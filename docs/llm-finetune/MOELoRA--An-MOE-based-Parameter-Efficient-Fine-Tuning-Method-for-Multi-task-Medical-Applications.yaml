- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://ar5iv.labs.arxiv.org/html/2310.18339](https://ar5iv.labs.arxiv.org/html/2310.18339)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Qidong Liu Xi‚Äôan Jiaotong University
  prefs: []
  type: TYPE_NORMAL
- en: City University of Hong Kong Xi‚Äôan, China
  prefs: []
  type: TYPE_NORMAL
- en: liuqidong@stu.xjtu.edu.cn ‚ÄÉ‚ÄÉ Xian Wu üñÇ¬†Corresponding Authors Jarvis Research
    Center, Tencent YouTu Lab Shenzhen, China
  prefs: []
  type: TYPE_NORMAL
- en: kevinxwu@tencent.com ‚ÄÉ‚ÄÉ Xiangyu Zhao üñÇ {@IEEEauthorhalign} Yuanshao Zhu
  prefs: []
  type: TYPE_NORMAL
- en: Feng Tian City University of Hong Kong Hong Kong, China
  prefs: []
  type: TYPE_NORMAL
- en: xianzhao@cityu.edu.hk Southern University of Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: City University of Hong Kong Shenzhen, China
  prefs: []
  type: TYPE_NORMAL
- en: zhuys2019@mail.sustech.edu.cn Xia‚Äôan Jiaotong University Xi‚Äôan, China
  prefs: []
  type: TYPE_NORMAL
- en: fengtian@mail.xjtu.edu.cn ‚ÄÉ‚ÄÉ Derong Xu
  prefs: []
  type: TYPE_NORMAL
- en: Yefeng Zheng University of Science and Technology of China
  prefs: []
  type: TYPE_NORMAL
- en: City University of Hong Kong Hefei, China
  prefs: []
  type: TYPE_NORMAL
- en: derongxu@mail.ustc.edu.cn Jarvis Research Center, Tencent YouTu Lab Shenzhen,
    China
  prefs: []
  type: TYPE_NORMAL
- en: yefengzheng@tencent.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The recent surge in the field of Large Language Models (LLMs) has gained significant
    attention in numerous domains. In order to tailor an LLM to a specific domain
    such as a web-based healthcare system, fine-tuning with domain knowledge is necessary.
    However, two issues arise during fine-tuning LLMs for medical applications. The
    first is the problem of task variety, where there are numerous distinct tasks
    in real-world medical scenarios. This diversity often results in suboptimal fine-tuning
    due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning
    can be prohibitive, impeding the application of LLMs. The large number of parameters
    in LLMs results in enormous time and computational consumption during fine-tuning,
    which is difficult to justify. To address these two issues simultaneously, we
    propose a novel parameter-efficient fine-tuning framework for multi-task medical
    applications called MOELoRA. The framework aims to capitalize on the benefits
    of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning.
    To unify MOE and LoRA, we devise multiple experts as the trainable parameters,
    where each expert consists of a pair of low-rank matrices to maintain a small
    number of trainable parameters. Additionally, we propose a task-motivated gate
    function for all MOELoRA layers that can regulate the contributions of each expert
    and generate distinct parameters for various tasks. To validate the effectiveness
    and practicality of the proposed method, we conducted comprehensive experiments
    on a public multi-task Chinese medical dataset. The experimental results demonstrate
    that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The
    implementation is available online for convenient reproduction of our experiments¬π¬π1https://github.com/liuqidong07/MOELoRA-peft.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Medical Applications; Large Language Model; Multi-task Learning;
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the impressive capabilities in language understanding and generation,
    the Large Language Models (LLMs) such as ChatGPT¬†[[1](#bib.bib1)] and ChatGLM¬†[[2](#bib.bib2)]
    have gained extensive interest from both academia and industry. Many efforts have
    been devoted to investigating the potential applications of LLMs across various
    domains¬†[[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]. One particularly suitable
    domain for LLMs is the medical domain, as the application of LLMs can benefit
    both patients and doctors. For patients, the LLM-enabled online Chatbot can provide
    convenient access to medical knowledge; For doctors, the LLM-enabled Clinical
    Decision Supporting Systems (CDSS) can relieve their heavy workload and improve
    diagnosis efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: However, the majority of LLMs are trained for general-purpose and are not customized
    for the medical domain. As a result, the general LLMs often fall short in medical
    tasks due to a lack of specialized medical knowledge¬†[[6](#bib.bib6)]. To empower
    LLMs with medical capabilities, a straightforward manner is to fine-tune LLMs
    with medical tasks. For large LLMs with more than $100$ billion parameters, they
    are usually closed-source and extremely costly for fine-tuning¬†[[7](#bib.bib7)].
    Therefore, in this paper, we focus on the open-source LLMs and fine-tuning them
    with medical knowledge and clinical tasks¬†[[8](#bib.bib8), [6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning LLMs for the medical domain usually involves two primary challenges:
    (i) Task Variety Problem: In real-world clinics, the LLMs can be applied to a
    large range of tasks, like doctor recommendation¬†[[9](#bib.bib9)], diagnosis prediction¬†[[10](#bib.bib10)],
    medicine recommendation¬†[[11](#bib.bib11)], medical named entity recognition¬†[[12](#bib.bib12),
    [13](#bib.bib13)], clinical report generation¬†[[14](#bib.bib14)] and *etc.* Since
    the input and output of these tasks are quite different, it is difficult to fine-tune
    a unified model for all tasks. Given the diversity of these tasks, fine-tuning
    a single model for each specific task is feasible but demands extensive expertise
    and labor. An integrated multi-task learning framework could potentially address
    this issue. However, much of the existing research on LLMs, as seen in studies
    like¬†[[8](#bib.bib8), [15](#bib.bib15)], predominantly centers on medical dialogue.
    Such over-attention ignores the variety of tasks, resulting in multi-task fine-tuning
    remains underexplored. (ii) High Tuning Cost: While fine-tuning all model parameters
    was a standard approach during the era of Bert¬†[[16](#bib.bib16)], it becomes
    challenging for LLMs due to their sheer size. The vast number of parameters in
    LLMs can lead to prohibitive time and computational expenses in practice¬†[[17](#bib.bib17)].
    As such, there is an urgent need for parameter efficient fine-tuning methodologies.
    To address these two challenges, the community urgently calls for developing a
    multi-tasking parameter efficient fine-tuning framework for LLM-driven medical
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For task variety problem, several multi-task learning frameworks have been
    proposed¬†[[18](#bib.bib18), [19](#bib.bib19)]. A standout among these is Mixture-of-Experts
    (MOE)¬†[[20](#bib.bib20)], which designs multiple separate experts to learn task-shared
    and -specific knowledge, and integrates a gate function to modulate the contributions
    of each expert. While existing frameworks adeptly consolidate multiple tasks for
    classical neural network architectures, they are primarily compatible with full
    fine-tuning, which is associated with high tuning costs. Correspondingly, the
    emergence of parameter efficient fine-tuning (PEFT) methods (P-Tuning¬†[[1](#bib.bib1)],
    LoRA¬†[[21](#bib.bib21)], etc.) has offered a potential solution to the problem
    of high tuning cost. These methods typically tune a limited number of parameters,
    keeping the pre-trained LLM parameters frozen. For instance, LoRA¬†[[21](#bib.bib21)]
    proposes to only train pairs of low-rank matrices for fitting the parameter updates
    of dense layers in LLMs. However, the existing PEFT is limited to fine-tuning
    either multiple sets of parameters for each task separately or a singular set
    across all tasks. Though separate training can fit each task well, this strategy
    is laborious and lacks task-shared knowledge. While fine-tuning a set of parameters
    is feasible, it may hurt performance due to issues such as data imbalance and
    seesaw effects¬†[[19](#bib.bib19), [22](#bib.bib22)]. For illustration, we analyze
    the data distribution of a Chinese medical dataset, PromptCBLUE, in Figure¬†[1](#S1.F1
    "Figure 1 ‚Ä£ I Introduction ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"). Our analysis reveals significant
    disparities: while some tasks boast nearly $5,000$. This imbalance can skew the
    uniquely fine-tuned parameters towards tasks with more samples, inadvertently
    undermining the performance on tasks with fewer samples. Therefore, parameter
    efficient fine-tuning of separate parameters for multi-task by a unique training
    process can alleviate both problems simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12316866ebbf0cb43a0c2444993760fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The illustration for data imbalance problem of various medical tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenges of task variety and high tuning costs, we propose
    a unified parameter efficient fine-tuning framework to learn separate parameters
    for various tasks, dubbed MOELoRA. Our framework follows the basic scheme of LoRA
    for the parameter efficiency, *i.e.,* only fine-tuning small size of parameters
    parallel to the dense layers in LLMs. However, as discussed previously, existing
    unified LoRA fine-tuning faces the challenge of a singular set of parameters across
    all tasks. Thus, in our approach, we first design several experts as the trainable
    part rather than a singular pair of low-rank matrices. On the one hand, inspired
    by MOE¬†[[20](#bib.bib20)], separate experts can help learn task-specific knowledge
    under one unique training process. On the other hand, such design gives the chance
    to produce several distinct sets of parameters. Besides, for parameter efficiency,
    we devise each expert as two low-rank matrices. Then, to learn separate sets of
    parameters for each task, we propose a task-motivated gate function. In specific,
    the gate function absorbs the task identity and outputs corresponding expert weights.
    By the expert weights for one specific task and the parameters of multiple experts,
    we can get the unique updated parameters for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the contributions of this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce MOELoRA, a novel multi-task PEFT framework that combines the strengths
    of both MOE and LoRA. Additionally, we design a task-motivated gate function to
    facilitate the tuning of distinct parameter sets for each task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct comprehensive experiments on a public multi-task Chinese medical
    dataset, with the results underscoring the superiority of the proposed MOELoRA
    framework.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To our knowledge, this research represents the first endeavor to delve into
    multi-task parameter efficient fine-tuning techniques for LLM-driven medical applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb2a16f93d6c3ab66c1094eaa402ac66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The medical name entity recognition example for illustration of how
    to use LLMs to complete medical tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first briefly introduce how LLMs are adopted for medical
    applications. Then, we give the problem definition of multi-task medical applications.
  prefs: []
  type: TYPE_NORMAL
- en: II-A LLMs for Medical Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Intelligent medical systems have become increasingly prevalent in contemporary
    web-based healthcare settings. Numerous studies have sought to standardize medical
    tasks by defining consistent input and output patterns, thereby streamlining the
    model design process. As the example of medical named entity recognition (NER)¬†[[12](#bib.bib12),
    [13](#bib.bib13)] illustrated in Figure¬†[2](#S2.F2 "Figure 2 ‚Ä£ II Preliminary
    ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"), traditional models typically process medical texts, denoted
    as $I_{M}$. However, the integration of LLMs into medical tasks introduces a distinct
    paradigm. Given that both the input and output of LLMs are typically linguistic
    in nature, there is a necessity to reformulate medical tasks to be compatible
    with LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To adapt medical tasks for LLMs, we need to restructure both the input and
    output patterns. Input Modification: We incorporate instruction templates into
    the original medical texts to guide LLMs in executing the relevant tasks¬†[[23](#bib.bib23)].
    Taking medical NER as an example, as depicted in Figure¬†[2](#S2.F2 "Figure 2 ‚Ä£
    II Preliminary ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method
    for Multi-task Medical Applications"), we employ the template: Please recognize
    the medical name entity in this sentence: ‚Äú[Medical Text]‚Äù, where ‚Äú[Medical Text]‚Äù
    serves as a placeholder for the raw medical text $I_{M}$ and $TP^{A}_{NER}$, respectively.
    With these modifications in place, the process by which LLMs undertake the NER
    task can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: After the task reformulation for LLMs, we can use the purely lingual data to
    fine-tune the foundation large language model, such as LlaMA¬†[[24](#bib.bib24)],
    ChatGLM¬†[[25](#bib.bib25)] and *etc.*. Then, the fine-tuned model completes the
    medical tasks by generating the regulated answers.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Multi-task Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As previously mentioned, medical applications often encompass a variety of tasks,
    such as name entity recognition, medical inquiry, etc. Our goal is to fine-tune
    LLMs to gain robust performance for each task and thus can also benefit the whole
    healthcare system. For multi-task fine-tuning, we consider a set of medical tasks
    represented as $\mathbb{T}=\{\mathcal{T}_{1},\ldots,\mathcal{T}_{j},\ldots,\mathcal{T}_{M}\}$
    and $LO$, optimize the parameters $\Phi$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the data from diverse tasks are standardized into a consistent linguistic
    format, we straightforwardly employ the conditional language modeling objectives¬†[[25](#bib.bib25)]
    for all training instances. Furthermore, with the intent to assimilate shared
    medical knowledge and be free from the labor of adjusting fine-tuning for several
    tasks, data from all tasks are incorporated into the unique optimization process.
    Consequently, the objective function for multi-task fine-tuning can be formulated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/ad777d5a480cfa5c2bafb6bdb57dffba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The overview of parameter efficient fine-tuning using MOELoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: III Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we provide a comprehensive description of our proposed framework.
    We begin with an overview in Section¬†[III-A](#S3.SS1 "III-A Overview ‚Ä£ III Method
    ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"). Subsequently, Section¬†[III-B](#S3.SS2 "III-B MOELoRA ‚Ä£
    III Method ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for
    Multi-task Medical Applications") delves into how MOELoRA seamlessly integrates
    the processes of MOE and LoRA, harnessing the strengths of both. The task-motivated
    gate function is detailed in Section¬†[III-C](#S3.SS3 "III-C Task Motivated Gate
    Function ‚Ä£ III Method ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"), where we also discuss the recovery
    of unique fine-tuned LLM parameters for each task. Lastly, Section¬†[III-D](#S3.SS4
    "III-D Optimization and Inference ‚Ä£ III Method ‚Ä£ MOELoRA: An MOE-based Parameter
    Efficient Fine-Tuning Method for Multi-task Medical Applications") elaborates
    on the optimization and inference processes.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‚Ä£ II-B Multi-task Fine-tuning ‚Ä£ II Preliminary
    ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications") provides a visual representation of the parameter efficient
    fine-tuning process of LLMs using MOELoRA. In the realm of parameter-efficient
    fine-tuning, LoRA¬†[[21](#bib.bib21)] introduces the concept of training only two
    low-rank matrices as a substitute for updates in dense layers. Building on this,
    our approach integrates MOELoRA layers into each dense layer, enabling them to
    acquire keys, queries, and values, as well as facilitating the feed-forward process.
    A significant advantage of our method is that we only fine-tune the parameters
    of the MOELoRA layers, keeping the rest of the original LLM parameters frozen.
    This approach substantially reduces the often prohibitive costs associated with
    tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, each MOELoRA layer incorporates multiple shared experts. These
    experts are designed to capture diverse knowledge across various medical domains,
    a concept we will delve deeper into in Section¬†[III-B](#S3.SS2 "III-B MOELoRA
    ‚Ä£ III Method ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for
    Multi-task Medical Applications"). We introduce a task-motivated gate function
    to ensure that unique parameter sets are learned for each task. This function
    determines the contribution weights of experts across all MOELoRA layers, enabling
    the generation of distinct updated parameters tailored to different tasks. It
    is worth noting that we employ a single gate function for all MOELoRA layers,
    rather than having a one-to-one correspondence between gates and MOELoRA layers.
    This design choice is intentional, aiming to reduce the number of tunable parameters
    and mitigate the risk of over-parameterization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f86160213bb6a0eb96dac8f07a008ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The architecture of the proposed MOELoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B MOELoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Low-rank Adaptation (LoRA)¬†[[21](#bib.bib21)] has demonstrated both its effectiveness
    and efficiency in fine-tuning LLMs. As our approach builds upon the foundational
    principles of LoRA for parameter efficiency, it is pertinent to provide a brief
    overview of its workings. LoRA is inspired by the low intrinsic dimension characteristic¬†[[26](#bib.bib26)],
    which reformulates the parameter fine-tuning process in LLMs as a low-rank decomposition.
    Specifically, the equation $\mathbf{W}_{0}+\Delta\mathbf{W}=\mathbf{W}+\mathbf{B}\mathbf{A}$
    and $\mathbf{A}\in\mathbb{R}^{r\times d_{out}}$ are low-rank and trainable. Given
    this setup, the forward process of a linear layer paired with a LoRA layer can
    be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{h}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\mathbf{B}\mathbf{A}\mathbf{x}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}$. The rank of the trainable low-rank matrices is denoted by
    $r$, $\mathbf{W}k$ and $\mathbf{B}$ and $\mathbf{B}$. Such characteristic results
    in achieving parameter efficiency for the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the integral parameters are fine-tuned for all tasks in the original
    LoRA, which causes difficulty in learning the various aspects of medical knowledge.
    A potential solution to this challenge is to segment the entire parameter set
    into several parts and derive various combinations. The Mixture-of-Expert (MOE)
    model¬†[[20](#bib.bib20)] suggests employing multiple expert networks to capture
    different facets of multi-task information, aligning with the combination concept.
    This insight leads us to design MOELoRA, which seamlessly integrates the advantages
    of both LoRA and MOE. To harmonize the distinct forward processes of LoRA and
    MOE, we introduce a set of experts, denoted as $\{E_{i}\}_{i=1}^{N}$ is expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{h}_{j}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}(\mathbf{x}_{j})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}\mathbf{x}_{j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{h}_{j}$ and $\mathbf{A}_{i}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$
    and $B$. This weight is determined by our proposed gate function, which we will
    detail in following section.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will discuss the number of trainable parameters for LoRA and MOELoRA.
    In terms of LoRA, the two low-rank matrices $\mathbf{B}\in\mathbb{R}^{d_{in}\times
    r}$ trainable experts and each expert own $\frac{r}{N}\times(d_{in}+d{out})$.
    As a conclusion, the MOELoRA has the same number of trainable parameters as LoRA,
    which indicates high efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Task Motivated Gate Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we detail the intricacies of our task-motivated gate function.
    As previously emphasized, the contribution of each expert should be tailored to
    specific tasks. To regulate these contributions, we introduce a gate function.
    Since these weights are inherently task-specific, our gate function is designed
    to take the task identity as input. To facilitate this, we employ a task embedding
    matrix, denoted as $\mathbf{E}\in\mathbb{R}^{|\mathbb{T}|\times d_{T}}$-th column
    of $\mathbf{E}$, we apply a linear transformation. This computation is captured
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\omega}_{j}={\rm Softmax}(\mathbf{W}_{T}\mathbf{e}_{j})$ |  | (5)
    |'
  prefs: []
  type: TYPE_TB
- en: Here, $\bm{\omega}_{j}\in\mathbb{R}^{|\mathbb{T}|}$. To prevent any disproportionately
    large weights, we employ a softmax operation to normalize the contribution weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we elucidate the mechanism to retrieve the distinct parameters learned
    for each task. While the conventional design of MOE directly feeds the input vector
    $\mathbf{x}$, the process can be articulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{W}_{j}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In contrast, if the gate function is driven by the input vector $\mathbf{x}$,
    leading to a sample-specific fine-tuned parameter matrix. This design would render
    the parameters non-retrievable on a per-task basis. The ability to retrieve parameters
    for each task offers two primary advantages: 1) Customization for Task: Each task
    is fine-tuned with a set of parameters, which can help learn more task-specific
    information and alleviate the problem of data imbalance. 2) Efficiency in Inference:
    The retrieved, fine-tuned LLM exhibits reduced inference latency. This is attributed
    to the elimination of the need for the additional forward computation associated
    with the LoRA layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Train and inference process of MOELoRA
  prefs: []
  type: TYPE_NORMAL
- en: 1:Indicate the LLM and the layers that need MOELoRA fine-tuning.2:Indicate the
    rank value $r$ of MOELoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Process
  prefs: []
  type: TYPE_NORMAL
- en: 4:Freeze all parameters in pre-trained LLM, *e.g.,* $\mathbf{W}_{q}$ in $\mathcal{D}$9:end¬†for
  prefs: []
  type: TYPE_NORMAL
- en: Inference Process
  prefs: []
  type: TYPE_NORMAL
- en: 10:for¬†$\mathcal{T}_{j}$, apply the corresponding parameters of LLM for pediction.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Optimization and Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we refer to the optimization and inference process of MOELoRA.
    For better readability, we also conclude the whole procedure in Algorithm¬†[1](#alg1
    "Algorithm 1 ‚Ä£ III-C Task Motivated Gate Function ‚Ä£ III Method ‚Ä£ MOELoRA: An MOE-based
    Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications").'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization. We first configure the MOELoRA according to the specified layers
    in LLM and several hyper-parameters (line 1-3). Then, for the parameter efficient
    fine-tuning, all pre-trained parameters in LLM (line 4) are frozen. During the
    optimization, we randomly sample a batch of data from all tasks iteratively, instead
    of grouping the samples from the same task into one batch as some multi-task researches¬†[[27](#bib.bib27),
    [28](#bib.bib28)] do. We choose the random sampling for batch by the performance
    comparison in experiments. Using the batch of data, we can conduct the forward
    process and compute the loss for training (line 6-7). For parameter update, it
    is worth noting that we only fine-tune the parameters of MOELoRA and task motivated
    gate function, *i.e.,* $\{\mathbf{A}_{i},\mathbf{B}_{i}\}_{i=1}^{N}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference. As discussed in Section¬†[III-C](#S3.SS3 "III-C Task Motivated Gate
    Function ‚Ä£ III Method ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"), the MOELoRA can recover the fine-tuned
    parameter matrices for each task by Equation¬†([6](#S3.E6 "In III-C Task Motivated
    Gate Function ‚Ä£ III Method ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")). For inference, we first recover
    the trained parameters in LLM for each task (line 10-13), which indicates that
    each task has its own LLM parameters. Then, we can apply the corresponding LLM
    to complete the specified task.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present comprehensive experiments conducted on a multi-task
    Chinese medical dataset. Through a detailed analysis of the experimental results,
    we seek to address the following Research Questions (RQ):'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ1: How does MOELoRA compare to other parameter-efficient fine-tuning strategies
    and cross-task generalization methods in terms of performance?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ2: What impact do the MOE architecture and the gate function have on the
    fine-tuning process? Additionally, how do different training strategies influence
    the performance of MOELoRA?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3: How do the number of experts and the rank of MOELoRA influence performance
    outcomes?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ4: Are the experts specialized in capturing specific aspects of knowledge?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-A Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IV-A1 Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our experiments are conducted on the PromptCBLUE dataset¬≤¬≤2https://tianchi.aliyun.com/competition/entrance/532084/information,
    a Chinese multi-task medical dataset recently made available on the Tianchi Competition
    Platform¬≥¬≥3https://tianchi.aliyun.com/competition/activeList. This dataset encompasses
    $16$ distinct medical tasks, each of which has been transformed into pure text
    format using specific prompts, ensuring compatibility with LLMs. To the best of
    our knowledge, PromptCBLUE is the only medical multi-task dataset tailored for
    LLMs. Specifically, the dataset includes tasks such as medical named entity recognition,
    diagnosis report generation, etc. Due to computational constraints, we have chosen
    eight tasks at random for our experiments. For pre-processing, we eliminated duplicate
    samples. Since the test set remains unreleased, we opt to use the development
    set as our test set. The validation set is derived from the training set, with
    its size matching that of the test set. The statistics of the pre-processed dataset
    are concluded in Table¬†[I](#S4.T1 "TABLE I ‚Ä£ IV-A1 Dataset ‚Ä£ IV-A Experimental
    Settings ‚Ä£ IV Experiment ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: The brief description and statistics of the dataset PromptCBLUE.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Description | # Train | # Validation | # Test |'
  prefs: []
  type: TYPE_TB
- en: '| CMeIE | Name Entity Recognition | 2,828 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-CDN | Normalization | 2,381 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-CDEE | Attribute Extraction | 1,562 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-MDCFNPC | Clinic Entity Discovery | 4,935 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| CHIP-CTC | Medical Text Classification | 3,622 | 1,100 | 1,100 |'
  prefs: []
  type: TYPE_TB
- en: '| KUAKE-QIC | Query Intention | 3,279 | 660 | 660 |'
  prefs: []
  type: TYPE_TB
- en: '| IMCS-V2-MRG | Report Generation | 1,799 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| MedDG | Doctor Dialogue | 4,964 | 600 | 600 |'
  prefs: []
  type: TYPE_TB
- en: IV-A2 Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our experiments, we benchmark against three distinct groups of baselines,
    namely: (i) LLM without Fine-tuning, (ii) LLM with Fine-tuning, and (iii) Cross-task
    Generalization. The latter two groups utilize ChatGLM-6B¬†[[25](#bib.bib25)], renowned
    for its prowess in Chinese text generation. A brief description of each baseline
    group is as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM without Fine-tuing: For this group, we employ In-Context Learning¬†[[29](#bib.bib29)]
    to guide the LLM in accomplishing the relevant tasks. Specifically, we provide
    a task description accompanied by $3$ randomly sampled examples for in-context
    learning. Models such as ChatGPT[[30](#bib.bib30)] and Huatuo¬†[[8](#bib.bib8)]
    serve as the foundational models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM with Fine-tuning: This group encompasses various fine-tuning strategies.
    One is P-Tuning¬†[[1](#bib.bib1)], which inserts continuous trainable prompt vectors
    to the start of the sequence. The other competitors are rooted in LoRA¬†[[21](#bib.bib21)].
    For the multi-task medical tasks, we implement two straightforward strategies:
    one that fine-tunes a single LoRA for all tasks (denoted as LoRA (Full)) and another
    that fine-tunes a distinct LoRA for each task (denoted as LoRA (Single)). Additionally,
    we introduce a variant of LoRA (Full) that incorporates a basic task description
    in the prompt, labeled as LoRA (Full+TP).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-task Generalization: To assess the applicability of cross-task generalization
    to multi-task fine-tuning, we also evaluate a recent approach, namely LoRAHub¬†[[31](#bib.bib31)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-A3 Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our experiments are simulated by PyTorch 1.12.0 and Python 3.6.5\. We run the
    code on Tesla V100 GPUs for acceleration. The LLM ChatGLM-6B¬†[[25](#bib.bib25)],
    recognized for its proficiency in Chinese language processing, serves as the foundational
    model for fine-tuning. To summarize, the configuration details are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Layers: For all LoRA fine-tuning baselines and the proposed MOELoRA,
    we designate trainable layers within the self-attention and linear layers of ChatGLM.
    These layers are identified as ‚Äúquery_key_value‚Äù, ‚Äúdense‚Äù, ‚Äúdense_h_to_4h‚Äù, and
    ‚Äúdense_4h_to_h‚Äù.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input/Output Length: The maximum input and output lengths were configured to
    $1,024$, respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameters: We set batch size to $64$, with a LoRA dropout $\alpha=0.1$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our MOELoRA implementation‚Å¥‚Å¥4https://github.com/liuqidong07/MOELoRA-peft is
    compatible with the PEFT package‚Åµ‚Åµ5https://github.com/huggingface/peft, facilitating
    easier adoption and utilization of MOELoRA.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A4 Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For our evaluations, we employ a variety of metrics tailored to the nature of
    each task. Specifically, the Micro-F1 score is used for CMeIE, CHIP-CDN, CHIP-CDEE
    and CHIP-MDCFNPC, while the Macro-F1 score is for CHIP-CTC and KUAKE-QIC. As for
    text generation tasks, *i.e.,* report generation and doctor dialogue, the Rouge-L¬†[[32](#bib.bib32)]
    is applied. Also, the average score across all tasks is used for evaluating the
    overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: The overall results of competing baselines and MOELoRA on PromptCBLUE.
    The boldface refers to the highest score and the underline indicates the best
    result of the baselines. ‚Äú*‚Äù indicates the statistically significant improvements
    (*i.e.,* two-sided t-test with $p<0.05$) over the best baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 0.3058 | 0.6069 | 0.2838 | 0.5854 | 0.5253 | 0.4851 | 0.3253 |
    0.1361 | 0.4067 |'
  prefs: []
  type: TYPE_TB
- en: '| Huatuo | 0.1826 | 0.3610 | 0.1658 | 0.3487 | 0.1909 | 0.1454 | 0.2401 | 0.1308
    | 0.2207 |'
  prefs: []
  type: TYPE_TB
- en: '| P-Tuning | 0.4552 | 0.8687 | 0.5256 | 0.7423 | 0.8275 | 0.8377 | 0.3155 |
    0.0901 | 0.5828 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA (Full) | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678
    | 0.1113 | 0.6155 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA (Single) | 0.4984 | 0.8882 | 0.5528 | 0.7765 | 0.8694 | 0.8524 | 0.3583
    | 0.1143 | 0.6138 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA (Full+TP) | 0.4933 | 0.8814 | 0.5450 | 0.7705 | 0.8755 | 0.8664 | 0.3556
    | 0.1160 | 0.6130 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRAHub | 0.4411 | 0.8442 | 0.5041 | 0.7177 | 0.8564 | 0.8502 | 0.3061 |
    0.1192 | 0.5799 |'
  prefs: []
  type: TYPE_TB
- en: '| MOELoRA | 0.5193* | 0.8928* | 0.5697* | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: The experimental results of ablation study for MOELoRA. The boldface
    refers to the highest score and the underline indicates the best result of the
    baselines. ‚Äú*‚Äù indicates the statistically significant improvements (*i.e.,* two-sided
    t-test with $p<0.05$) over the best baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| w/o MOE | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678 |
    0.1113 | 0.6155 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o gate | 0.5015 | 0.8840 | 0.5378 | 0.7789 | 0.8818 | 0.8699 | 0.3709 |
    0.1140 | 0.6174 |'
  prefs: []
  type: TYPE_TB
- en: '| w multiple gate | 0.4994 | 0.8840 | 0.5692 | 0.7842 | 0.8764 | 0.8675 | 0.3632
    | 0.1130 | 0.6196 |'
  prefs: []
  type: TYPE_TB
- en: '| w BT | 0.4817 | 0.8806 | 0.5712 | 0.7713 | 0.8682 | 0.8643 | 0.3522 | 0.1110
    | 0.6126 |'
  prefs: []
  type: TYPE_TB
- en: '| w RBT | 0.4769 | 0.8930 | 0.5600 | 0.7741 | 0.8636 | 0.8795 | 0.3541 | 0.1135
    | 0.6144 |'
  prefs: []
  type: TYPE_TB
- en: '| MOELoRA | 0.5193* | 0.8928* | 0.5697 | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
  prefs: []
  type: TYPE_TB
- en: IV-B Overall Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The comprehensive experimental results of MOELoRA and the competing baselines
    are presented in Table¬†[II](#S4.T2 "TABLE II ‚Ä£ IV-A4 Evaluation Metrics ‚Ä£ IV-A
    Experimental Settings ‚Ä£ IV Experiment ‚Ä£ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications"). Analyzing the average
    scores across all tasks, it is evident that MOELoRA consistently outperforms all
    other methods. A detailed analysis of the results is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLMs without Fine-tuning: The first group of baselines, which are LLMs without
    any fine-tuning, significantly lag behind the other groups. This highlights the
    importance of fine-tuning LLMs to incorporate task-specific medical knowledge.
    Notably, ChatGPT outperforms Huatuo on most tasks, suggesting that the LLMs only
    in large parameter scales can be well motivated by the in-context learning¬†[[29](#bib.bib29)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameter Efficient Fine-tuning Strategies: Among the parameter-efficient fine-tuning
    strategies, LoRA-based methods clearly surpass P-Tuning. While LoRA (Full) and
    LoRA (Full+TP) both utilize data from all tasks, LoRA (Full+TP) slightly underperforms.
    This might be attributed to the addition of task prompts, which extend the input
    texts, leading to potential truncation of informative words due to input length
    constraints. LoRA (Single), which fine-tunes for individual tasks, also does not
    match the performance of LoRA (Full), underscoring the value of shared knowledge
    across tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-task Generalization: We benchmark against a recent cross-task generalization
    method, LoRAHub. Despite its impressive performance in cross-task generalization
    settings, it requires a vast amount of task data, which conflicts with the multi-task
    setting. In our experiments, we only consider 8 tasks, which might explain its
    relative underperformance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Task-specific Observations: Performance variations are evident across tasks.
    For instance, LoRA (Full) and LoRA (Full+TP) excel in tasks with larger datasets,
    while LoRA (Single) shines in tasks with fewer samples, highlighting the data
    imbalance issue. MOELoRA consistently achieves optimal performance in most tasks,
    demonstrating its ability to effectively address this imbalance. For MedDG tasks,
    the inherent dialog capability of ChatGPT and Huatuo gives them an advantage over
    other approaches.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In response to RQ1, MOELoRA demonstrates superior performance compared to other
    parameter-efficient strategies and cross-task generalization methods.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To delve deeper into RQ2 and understand the contributions of each component
    in MOELoRA, we present the results of our ablation study in Table¬†[III](#S4.T3
    "TABLE III ‚Ä£ IV-A4 Evaluation Metrics ‚Ä£ IV-A Experimental Settings ‚Ä£ IV Experiment
    ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"). The variant w/o MOE (essentially reverts to LoRA (Full))
    excludes the MOE architecture. It demonstrates inferior performance compared to
    the full-fledged MOELoRA, underscoring the significance of the MOE architecture.
    Similarly, the w/o gate variant, which employs uniform expert weights bypassing
    the gate function, also lags behind MOELoRA, highlighting the gate function‚Äôs
    effectiveness. The w multiple gate variant, uses a unique gate function for each
    MOELoRA layer. We can see that it achieves comparable results on several tasks
    and is slightly outperformed by the single gate function design due to over-parameterization.
    Besides, multiple gate functions also incur a higher count of trainable parameters,
    leading to diminished efficiency compared to a single gate function setup.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we analyze the impact of different training strategies. Specifically,
    the w BT method¬†[[27](#bib.bib27)] consolidates samples from the same task into
    one batch. The w RBT approach¬†[[28](#bib.bib28)] randomly selects a task for each
    batch of data. Both of them prove to be less conducive for MOELoRA, resulting
    in performance degradation. These findings underscore the critical roles of both
    the MOE architecture and the gate function in the MOELoRA model, as well as the
    influence of specific training patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbb02dfe735ada014f5e9c4fc01e7105.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2b545cf1b24e845d33abc21ee43a131.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: The results of experiments for hyper-parameters, *i.e.,* expert number
    $N$.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Hyper-parameter Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To answer RQ3, we delve into the impact of hyper-parameters on the performance
    of MOELoRA. Specifically, we examine how variations in the expert number $M$ to
    $8$, ensuring that the size of trainable parameters remains relatively stable.
    Subsequently, we observe from Figure¬†[5b](#S4.F5.sf2 "In Figure 5 ‚Ä£ IV-C Ablation
    Study ‚Ä£ IV Experiment ‚Ä£ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications") that while an increase in $r$.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For RQ4, we present a visualization of the expert weights across four tasks
    in Figure¬†[6](#S4.F6 "Figure 6 ‚Ä£ IV-E Case Study ‚Ä£ IV Experiment ‚Ä£ MOELoRA: An
    MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications").
    For each task, the length of the bar in different colors represents the weights
    for the corresponding expert. Since the expert weights are normalized to 1, the
    lengths of the bar for each task are the same. At a macro level, it is evident
    that the contributions from each expert vary significantly, underscoring the idea
    that different experts specialize in distinct facets of medical knowledge. Moreover,
    the pronounced disparities in weights across tasks highlight the diverse nature
    of medical applications. Taking a closer look at the tasks CHIP-CDN and KUAKE-QIC,
    we observe that their expert weights are largely congruent, with exceptions in
    experts 3 and 4. Given that CHIP-CDN can be viewed as a precursor to KUAKE-QIC‚Äîsince
    diagnostic word normalization can bolster inquiry classification‚Äîthe similarity
    in expert weights suggests that MOELoRA is adept at harnessing shared knowledge
    to benefit intrinsically related tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6686ec5e867e1500ea35c838d4ce914d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The visualization of expert weights for various tasks. In each task,
    the length of the bar in different colors represents the weights for the corresponding
    expert.'
  prefs: []
  type: TYPE_NORMAL
- en: V Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-A LLM for Medical Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, there has been a surge of work in the medical field leveraging the
    powerful capabilities of LLM. For instance, Med-PaLM [[15](#bib.bib15)] proposes
    a new benchmark called MultiMedQA, which combines seven medical question-answering
    datasets to address the challenges of evaluating the clinical knowledge of LLM.
    Med-PaLM2 [[33](#bib.bib33)] has further improved upon Med-PaLM by introducing
    a new prompting strategy called ensemble refinement. This strategy is based on
    CoT [[34](#bib.bib34)] and self-consistency [[35](#bib.bib35)] and has shown significant
    improvements in MedQA. What‚Äôs more, ChatDoctor [[6](#bib.bib6)] constructs a dataset
    of 100,000 patient-doctor dialogues collected from a widely used online medical
    consultation platform. The dataset is fine-tuned on LLaMA and an automated information
    retrieval method is proposed to utilize online information, like Wikipedia. Besides,
    HuaTuo [[8](#bib.bib8)] is based on LLaMa¬†[[24](#bib.bib24)] and fine-tuned using
    Chinese medical knowledge from CMeKG [[36](#bib.bib36)]. Additionally, HuaTuo
    has introduced a new evaluation metric called SUS for Safety, Usability, and Smoothness.
    Previous studies have demonstrated the potential of LLMs in the medical field.
    However, they tend to focus on medical dialogue while neglecting other equally
    important tasks (such as medical NER and diagnosis report generation) and usually
    demand a significant fine-tuning cost for achieving generalization ability.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Parameter-Efficient Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parameter efficient fine-tuning (PEFT) aims to improve the performance of LLMs
    on new tasks by minimizing the number of fine-tuning parameters and computational
    complexity. Adapter Tuning [[37](#bib.bib37)] first introduces a lightweight adapter
    module, which has only a few trainable parameters and has shown comparable results
    to fine-tuning on the top layers of LLMs. On the other hand, prefix-tuning [[38](#bib.bib38)]
    and P-Tuning [[39](#bib.bib39)] both construct a task-specific virtual token that
    adds trainable, continuous prompts or embeddings to the original text sequence,
    making optimization more feasible than with discrete prompts. However, using prompts
    can be challenging for training and can also limit the available sequence length
    of the model. LoRA [[21](#bib.bib21)] is inspired by the discovery that low intrinsic
    dimension¬†[[26](#bib.bib26)] in the large parameters is the key role of LLMs,
    and introduces two trainable low-rank matrices into each dense layer. It has been
    shown to achieve comparable performance to full fine-tuning while requiring no
    additional computation during inference. Nevertheless, the LoRA fine-tuning performs
    inferiorly for multi-task medical applications. It can only learn integral updated
    parameters for all tasks, which loses the vital task-specific information. In
    recent times, a thread of research named cross-task generalization¬†[[31](#bib.bib31),
    [28](#bib.bib28), [40](#bib.bib40), [41](#bib.bib41)] emerges, which proposes
    various parameter efficient fine-tuning strategies for multi-task. However, different
    from the multi-task setting in this paper, they first train the model on too many
    tasks and aim to transfer the ability to unseen tasks. Due to the distinct setting,
    their method is difficult to be adapted to our problem. In a word, the multi-task
    parameter efficient fine-tuning for LLM-driven medical applications is still underexplored,
    and we take the first step.
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we first take the step to explore the multi-task parameter efficient
    fine-tuning method for LLM-driven medical applications. To satisfy the requirements
    of efficiency for fine-tuning and effectiveness for multi-task, we propose a novel
    multi-task fine-tuning framework. Specifically, we design the MOELoRA architecture,
    which consists of several low-rank experts as the trainable parameters to learn
    task-related knowledge and retain high efficiency. Besides, a task motivated gate
    function is devised to produce distinct fine-tuned parameters for various tasks.
    By the comprehensive experiments on a multi-task Chinese medical dataset, we verify
    the effectiveness of the proposed MOELoRA. In the future, we will further explore
    how to combine explicit medical knowledge, such as knowledge graphs, with LLMs
    by fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] X.¬†Liu, Y.¬†Zheng, Z.¬†Du, M.¬†Ding, Y.¬†Qian, Z.¬†Yang, and J.¬†Tang, ‚ÄúGpt understands,
    too,‚Äù *AI Open*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A.¬†Zeng, X.¬†Liu, Z.¬†Du, Z.¬†Wang, H.¬†Lai, M.¬†Ding, Z.¬†Yang, Y.¬†Xu, W.¬†Zheng,
    X.¬†Xia *et¬†al.*, ‚ÄúGlm-130b: An open bilingual pre-trained model,‚Äù in *The Eleventh
    International Conference on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] W.¬†Fan, Z.¬†Zhao, J.¬†Li, Y.¬†Liu, X.¬†Mei, Y.¬†Wang, J.¬†Tang, and Q.¬†Li, ‚ÄúRecommender
    systems in the era of large language models (llms),‚Äù *arXiv preprint arXiv:2307.02046*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] L.¬†Wang, C.¬†Ma, X.¬†Feng, Z.¬†Zhang, H.¬†Yang, J.¬†Zhang, Z.¬†Chen, J.¬†Tang,
    X.¬†Chen, Y.¬†Lin *et¬†al.*, ‚ÄúA survey on large language model based autonomous agents,‚Äù
    *arXiv preprint arXiv:2308.11432*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M.¬†U. Hadi, R.¬†Qureshi, A.¬†Shah, M.¬†Irfan, A.¬†Zafar, M.¬†Shaikh, N.¬†Akhtar,
    J.¬†Wu, and S.¬†Mirjalili, ‚ÄúA survey on large language models: Applications, challenges,
    limitations, and practical usage,‚Äù *TechRxiv*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L.¬†Yunxiang, L.¬†Zihan, Z.¬†Kai, D.¬†Ruilong, and Z.¬†You, ‚ÄúChatdoctor: A medical
    chat model fine-tuned on llama model using medical domain knowledge,‚Äù *arXiv preprint
    arXiv:2303.14070*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] OpenAI, ‚ÄúGpt-4 technical report,‚Äù *arXiv preprint arXiv:2303.08774*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] H.¬†Wang, C.¬†Liu, N.¬†Xi, Z.¬†Qiang, S.¬†Zhao, B.¬†Qin, and T.¬†Liu, ‚ÄúHuatuo:
    Tuning llama model with chinese medical knowledge,‚Äù 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Z.¬†Zheng, Z.¬†Qiu, H.¬†Xiong, X.¬†Wu, T.¬†Xu, E.¬†Chen, and X.¬†Zhao, ‚ÄúDdr: Dialogue
    based doctor recommendation for online medical service,‚Äù in *Proceedings of the
    28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, ser. KDD ‚Äô22.¬†¬†¬†New
    York, NY, USA: Association for Computing Machinery, 2022, p. 4592‚Äì4600\. [Online].
    Available: https://doi.org/10.1145/3534678.3539201'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Z.¬†Qiao, X.¬†Wu, S.¬†Ge, and W.¬†Fan, ‚ÄúMnn: Multimodal attentional neural
    networks for diagnosis prediction,‚Äù in *International Joint Conference on Artificial
    Intelligence*, 2019\. [Online]. Available: https://api.semanticscholar.org/CorpusID:199466261'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y.¬†Zhang, X.¬†Wu, Q.¬†Fang, S.¬†Qian, and C.¬†Xu, ‚ÄúKnowledge-enhanced attributed
    multi-task learning for medicine recommendation,‚Äù *ACM Trans. Inf. Syst.*, jan
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S.¬†Zhao, T.¬†Liu, S.¬†Zhao, and F.¬†Wang, ‚ÄúA neural multi-task learning framework
    to jointly model medical named entity recognition and normalization,‚Äù in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol.¬†33, no.¬†01, 2019, pp.
    817‚Äì824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S.¬†Rezayi, H.¬†Dai, Z.¬†Liu, Z.¬†Wu, A.¬†Hebbar, A.¬†H. Burns, L.¬†Zhao, D.¬†Zhu,
    Q.¬†Li, W.¬†Liu *et¬†al.*, ‚ÄúClinicalradiobert: Knowledge-infused few shot learning
    for clinical notes named entity recognition,‚Äù in *International Workshop on Machine
    Learning in Medical Imaging*.¬†¬†¬†Springer, 2022, pp. 269‚Äì278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y.¬†Miura, Y.¬†Zhang, E.¬†B. Tsai, C.¬†P. Langlotz, and D.¬†Jurafsky, ‚ÄúImproving
    factual completeness and consistency of image-to-text radiology report generation,‚Äù
    *arXiv preprint arXiv:2010.10042*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K.¬†Singhal, S.¬†Azizi, T.¬†Tu, S.¬†S. Mahdavi, J.¬†Wei, H.¬†W. Chung, N.¬†Scales,
    A.¬†Tanwani, H.¬†Cole-Lewis, S.¬†Pfohl *et¬†al.*, ‚ÄúLarge language models encode clinical
    knowledge,‚Äù *Nature*, vol. 620, no. 7972, pp. 172‚Äì180, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J.¬†D. M.-W.¬†C. Kenton and L.¬†K. Toutanova, ‚ÄúBert: Pre-training of deep
    bidirectional transformers for language understanding,‚Äù in *Proceedings of NAACL-HLT*,
    2019, pp. 4171‚Äì4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A.¬†Yang, B.¬†Xiao, B.¬†Wang, B.¬†Zhang, C.¬†Yin, C.¬†Lv, D.¬†Pan, D.¬†Wang, D.¬†Yan,
    F.¬†Yang *et¬†al.*, ‚ÄúBaichuan 2: Open large-scale language models,‚Äù *arXiv preprint
    arXiv:2309.10305*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y.¬†Zhang and Q.¬†Yang, ‚ÄúA survey on multi-task learning,‚Äù *IEEE Transactions
    on Knowledge and Data Engineering*, vol.¬†34, no.¬†12, pp. 5586‚Äì5609, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M.¬†Crawshaw, ‚ÄúMulti-task learning with deep neural networks: A survey,‚Äù
    *arXiv preprint arXiv:2009.09796*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N.¬†Shazeer, A.¬†Mirhoseini, K.¬†Maziarz, A.¬†Davis, Q.¬†Le, G.¬†Hinton, and
    J.¬†Dean, ‚ÄúOutrageously large neural networks: The sparsely-gated mixture-of-experts
    layer,‚Äù *arXiv preprint arXiv:1701.06538*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] E.¬†J. Hu, P.¬†Wallis, Z.¬†Allen-Zhu, Y.¬†Li, S.¬†Wang, L.¬†Wang, W.¬†Chen *et¬†al.*,
    ‚ÄúLora: Low-rank adaptation of large language models,‚Äù in *International Conference
    on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X.¬†Li, X.¬†Sun, Y.¬†Meng, J.¬†Liang, F.¬†Wu, and J.¬†Li, ‚ÄúDice loss for data-imbalanced
    nlp tasks,‚Äù in *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics*, 2020, pp. 465‚Äì476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S.¬†Zhang, L.¬†Dong, X.¬†Li, S.¬†Zhang, X.¬†Sun, S.¬†Wang, J.¬†Li, R.¬†Hu, T.¬†Zhang,
    F.¬†Wu *et¬†al.*, ‚ÄúInstruction tuning for large language models: A survey,‚Äù *arXiv
    preprint arXiv:2308.10792*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H.¬†Touvron, T.¬†Lavril, G.¬†Izacard, X.¬†Martinet, M.-A. Lachaux, T.¬†Lacroix,
    B.¬†Rozi√®re, N.¬†Goyal, E.¬†Hambro, F.¬†Azhar, A.¬†Rodriguez, A.¬†Joulin, E.¬†Grave,
    and G.¬†Lample, ‚ÄúLlama: Open and efficient foundation language models,‚Äù *arXiv
    preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Z.¬†Du, Y.¬†Qian, X.¬†Liu, M.¬†Ding, J.¬†Qiu, Z.¬†Yang, and J.¬†Tang, ‚ÄúGlm: General
    language model pretraining with autoregressive blank infilling,‚Äù in *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, 2022, pp. 320‚Äì335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A.¬†Aghajanyan, S.¬†Gupta, and L.¬†Zettlemoyer, ‚ÄúIntrinsic dimensionality
    explains the effectiveness of language model fine-tuning,‚Äù in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 7319‚Äì7328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X.-R. Sheng, L.¬†Zhao, G.¬†Zhou, X.¬†Ding, B.¬†Dai, Q.¬†Luo, S.¬†Yang, J.¬†Lv,
    C.¬†Zhang, H.¬†Deng *et¬†al.*, ‚ÄúOne model to serve all: Star topology adaptive recommender
    for multi-domain ctr prediction,‚Äù in *Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management*, 2021, pp. 4104‚Äì4113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T.¬†Sun, Z.¬†He, Q.¬†Zhu, X.¬†Qiu, and X.-J. Huang, ‚ÄúMultitask pre-training
    of modular prompt for chinese few-shot learning,‚Äù in *Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 2023, pp. 11‚Äâ156‚Äì11‚Äâ172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Q.¬†Dong, L.¬†Li, D.¬†Dai, C.¬†Zheng, Z.¬†Wu, B.¬†Chang, X.¬†Sun, J.¬†Xu, and
    Z.¬†Sui, ‚ÄúA survey for in-context learning,‚Äù *arXiv preprint arXiv:2301.00234*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] T.¬†Brown, B.¬†Mann, N.¬†Ryder, M.¬†Subbiah, J.¬†D. Kaplan, P.¬†Dhariwal, A.¬†Neelakantan,
    P.¬†Shyam, G.¬†Sastry, A.¬†Askell *et¬†al.*, ‚ÄúLanguage models are few-shot learners,‚Äù
    *Advances in neural information processing systems*, vol.¬†33, pp. 1877‚Äì1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C.¬†Huang, Q.¬†Liu, B.¬†Y. Lin, T.¬†Pang, C.¬†Du, and M.¬†Lin, ‚ÄúLorahub: Efficient
    cross-task generalization via dynamic lora composition,‚Äù *arXiv preprint arXiv:2307.13269*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] C.-Y. Lin and F.¬†J. Och, ‚ÄúAutomatic evaluation of machine translation
    quality using longest common subsequence and skip-bigram statistics,‚Äù in *Proceedings
    of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)*,
    2004, pp. 605‚Äì612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] K.¬†Singhal, T.¬†Tu, J.¬†Gottweis, R.¬†Sayres, E.¬†Wulczyn, L.¬†Hou, K.¬†Clark,
    S.¬†Pfohl, H.¬†Cole-Lewis, D.¬†Neal *et¬†al.*, ‚ÄúTowards expert-level medical question
    answering with large language models,‚Äù *arXiv preprint arXiv:2305.09617*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J.¬†Wei, X.¬†Wang, D.¬†Schuurmans, M.¬†Bosma, F.¬†Xia, E.¬†Chi, Q.¬†V. Le, D.¬†Zhou
    *et¬†al.*, ‚ÄúChain-of-thought prompting elicits reasoning in large language models,‚Äù
    *Advances in Neural Information Processing Systems*, vol.¬†35, pp. 24‚Äâ824‚Äì24‚Äâ837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] X.¬†Wang, J.¬†Wei, D.¬†Schuurmans, Q.¬†Le, E.¬†Chi, S.¬†Narang, A.¬†Chowdhery,
    and D.¬†Zhou, ‚ÄúSelf-consistency improves chain of thought reasoning in language
    models,‚Äù *arXiv preprint arXiv:2203.11171*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] O.¬†Byambasuren, Y.¬†Yang, Z.¬†Sui, D.¬†Dai, B.¬†Chang, S.¬†Li, and H.¬†Zan,
    ‚ÄúPreliminary study on the construction of chinese medical knowledge graph,‚Äù *Journal
    of Chinese Information Processing*, vol.¬†33, no.¬†10, pp. 1‚Äì9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] N.¬†Houlsby, A.¬†Giurgiu, S.¬†Jastrzebski, B.¬†Morrone, Q.¬†De¬†Laroussilhe,
    A.¬†Gesmundo, M.¬†Attariyan, and S.¬†Gelly, ‚ÄúParameter-efficient transfer learning
    for nlp,‚Äù in *International Conference on Machine Learning*.¬†¬†¬†PMLR, 2019, pp.
    2790‚Äì2799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X.¬†L. Li and P.¬†Liang, ‚ÄúPrefix-tuning: Optimizing continuous prompts for
    generation,‚Äù in *Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*.¬†¬†¬†Online: Association for Computational
    Linguistics, Aug. 2021, pp. 4582‚Äì4597\. [Online]. Available: https://aclanthology.org/2021.acl-long.353'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X.¬†Liu, K.¬†Ji, Y.¬†Fu, W.¬†Tam, Z.¬†Du, Z.¬†Yang, and J.¬†Tang, ‚ÄúP-tuning:
    Prompt tuning can be comparable to fine-tuning across scales and tasks,‚Äù in *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*.¬†¬†¬†Dublin, Ireland: Association for Computational Linguistics,
    May 2022, pp. 61‚Äì68\. [Online]. Available: https://aclanthology.org/2022.acl-short.8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A.¬†Asai, M.¬†Salehi, M.¬†E. Peters, and H.¬†Hajishirzi, ‚ÄúAttempt: Parameter-efficient
    multi-task tuning via attentional mixtures of soft prompts,‚Äù in *Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing*, 2022,
    pp. 6655‚Äì6672.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A.¬†√úst√ºn, A.¬†Bisazza, G.¬†Bouma, G.¬†van Noord, and S.¬†Ruder, ‚ÄúHyper-x:
    A unified hypernetwork for multi-task multilingual transfer,‚Äù *arXiv preprint
    arXiv:2205.12148*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
