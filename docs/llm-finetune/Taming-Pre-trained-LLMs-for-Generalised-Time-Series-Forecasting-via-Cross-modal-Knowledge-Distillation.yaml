- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:14'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.07300](https://ar5iv.labs.arxiv.org/html/2403.07300)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Peiyuan Liu^(1,) Equal Contribution    Hang Guo^(1,)¹¹footnotemark: 1    Tao
    Dai^(2,) Correspondence to: Tao Dai and Naiqi Li    Naiqi Li^(1,)²²footnotemark:
    2    Jigang Bao¹    Xudong Ren¹    Yong Jiang¹&Shu-Tao Xia¹ ¹Tsinghua Shenzhen
    International Graduate School, Tsinghua University'
  prefs: []
  type: TYPE_NORMAL
- en: ²College of Computer Science and Software Engineering, Shenzhen University
  prefs: []
  type: TYPE_NORMAL
- en: '{peiyuanliu.edu, cshguo, daitao.edu, linaiqi.thu}@google.com, {baojg19, rxd21}@mails.tsinghua.edu.cn,
    {jiangy, xiast}@sz.tsinghua.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multivariate time series forecasting has recently gained great success with
    the rapid growth of deep learning models. However, existing approaches usually
    train models from scratch using limited temporal data, preventing their generalization.
    Recently, with the surge of the Large Language Models (LLMs), several works have
    attempted to introduce LLMs into time series forecasting. Despite promising results,
    these methods directly take time series as the input to LLMs, ignoring the inherent
    modality gap between temporal and text data. In this work, we propose a novel
    Large Language Models and Time series Alignment framework, dubbed LLaTA, to fully
    unleash the potentials of LLMs in the time series forecasting challenge. Based
    on cross-modal knowledge distillation, the proposed method exploits both input-agnostic
    static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In
    this way, it empowers the forecasting model with favorable performance as well
    as strong generalization abilities. Extensive experiments demonstrate the proposed
    method establishes a new state of the art for both long- and short-term forecasting.
    Code is available at [https://github.com/Hank0626/LLaTA](https://github.com/Hank0626/LLaTA).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multivariate time series forecasting is an important task in time series analysis
    and plays an essential role in various real-world applications, such as forecasting
    on weather (Angryk et al., [2020](#bib.bib2)), energy (Demirel et al., [2012](#bib.bib12)),
    finance (Patton, [2013](#bib.bib27)), etc. To this end, numerous effective deep
    learning methods have been proposed in recent years (Wu et al., [2023](#bib.bib34);
    Zhang and Yan, [2023](#bib.bib37); Nie et al., [2023](#bib.bib25); Zeng et al.,
    [2023](#bib.bib36); Das et al., [2023](#bib.bib11); Wang et al., [2022](#bib.bib31);
    Dai et al., [2024](#bib.bib10)). Although existing methods have achieved promising
    results, they usually learn from scratch with scarce time series data, which in
    turn hampers their broader applicability.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, pre-trained Large Language Models (LLMs) have been introduced to time
    series analysis. Due to the large-scale pre-training, LLMs offer time series forecasting
    models with strong context modeling ability and also alleviate the data insufficiency
    challenge. For example, (Zhou et al., [2023](#bib.bib39)) proposed a unified time
    series analysis framework by adapting and fine-tuning LLMs. Building upon this,
    other works introduced additional enhancements to further refine and expand the
    capabilities of LLMs in time series forecasting, including refined fine-tuning
    methods (Chang et al., [2023](#bib.bib7)), sequence decomposition (Cao et al.,
    [2023](#bib.bib5)), and the incorporation of textual prompts (Jin et al., [2023a](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the promising results, most existing LLM-powered methods typically
    treat the pre-trained LLM as a well-initialized forecasting model, and directly
    use the time series data as the input. This straightforward way can lead to modality
    misalignment between pre-training and downstream tasks. As shown in [Fig. 1](#S1.F1
    "In 1 Introduction ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting
    via Cross-modal Knowledge Distillation"), the temporal tokens modeled by the current
    method can not align well with the word vectors of the pre-trained LLMs, which
    constrains the ability of LLMs for time series forecasting. Based on this observation,
    we ask a novel question: Is there a reasonable manner to narrow the modality gap
    between time series and language to further enhance the capability of LLMs-based
    time series forecasting models?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ca987ed30a0fd06baefdfe5b4f7d30f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The t-SNE visualization of LLM input tokens, featuring pre-trained
    word tokens with temporal tokens of GPT4TS (Left) (Zhou et al., [2023](#bib.bib39))
    and our method (Right). The integration of temporal tokens with pre-trained word
    tokens is notably more cohesive in our method, indicating effective modality alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: The key to answering the above question is to find an effective way to mitigate
    modality differences. Recently, cross-modal knowledge distillation has been proposed
    to achieve cross-model knowledge transfer. Some works have been proposed and achieved
    favorable performance in the domains of semantic segmentation (Vobecky et al.,
    [2022](#bib.bib30)), action detection (Dai et al., [2021](#bib.bib9)), speaker
    recognition (Jin et al., [2023b](#bib.bib19)), etc. However, existing advancements
    mainly focus on computer vision or natural language processing, while cross-modal
    knowledge distillation for temporal modality is still under-explored.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we propose LLaTA, a novel cross-modal knowledge distillation
    architecture, to overcome modality misalignment by distilling knowledge from pre-trained
    LLMs. Specifically, temporal modal tokens are projected into the same latent space
    of the textual modal tokens, and then knowledge distillation is used to align
    the two modalities. Therefore, the proposed LLaTA consists of two branches: (a)
    the temporal modal branch and (b) the textual modal branch. The former is used
    for processing time series information, and the latter is used to extract and
    adapt knowledge from pre-trained LLMs using aligned textual modal tokens. To ensure
    high-quality transfer, we consider two types of knowledge: input-agnostic static
    knowledge contained in frozen word embedding layers, and input-dependent dynamic
    knowledge encapsulated in forward parameters. The static knowledge is adequately
    exploited with the proposed principle word embedding extraction as well as the
    cross-attention mechanism, while the dynamic knowledge captures the modal consistency
    with the help of feature regularization loss and modal consistency loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the contributions of this paper are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce LLaTA, a novel cross-modal knowledge distillation framework to
    exploit the power of LLMs in time series forecasting. To the best of our knowledge,
    it is the first work attempting to address the modal-misalignment problem with
    knowledge distillation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We exploit both static and dynamic knowledge in the pre-trained LLMs and introduce
    well-designed modules/losses to adequately facilitate knowledge transfer between
    different modalities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive experiments on eight real-world datasets demonstrate that LLaTA achieves
    state-of-the-art performance on both long- and short-term time series forecasting
    tasks, with favorable generalization ability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbe198218955ecf0d356e51e188a8e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the proposed cross-modal distillation framework. The
    above is the textual modal branch, and the below is the temporal modal branch.
    Both static and dynamic knowledge are utilized to adapt LLMs for time series analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Time Series Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, deep learning has significantly revolutionized the field of
    time series forecasting, with a plethora of methods emerging to enhance predictive
    accuracy (Zeng et al., [2023](#bib.bib36); Das et al., [2023](#bib.bib11); Wu
    et al., [2023](#bib.bib34); Wang et al., [2022](#bib.bib31)). Among these, Transformer-based
    models have emerged as the frontrunners, offering unparalleled performance due
    to their exceptional ability to model complex dependencies in data  (Nie et al.,
    [2023](#bib.bib25); Zhang and Yan, [2023](#bib.bib37); Woo et al., [2022](#bib.bib32);
    Wu et al., [2021](#bib.bib33); Zhou et al., [2022](#bib.bib38)). However, they
    often have limitations due to the scarcity of training data and the necessity
    for intricate architectural designs.
  prefs: []
  type: TYPE_NORMAL
- en: In response to these challenges, the integration of LLMs into time series forecasting
    has emerged as a novel and promising direction. This approach leverages the extensive
    pre-training of LLMs to enhance the context-modeling capacity in time series analysis.
    A groundbreaking framework proposed by (Zhou et al., [2023](#bib.bib39)) first
    demonstrated the potential of adapting LLMs for time series analysis. Following
    this paradigm, subsequent research has introduced further refinements and innovations.
    For example, (Chang et al., [2023](#bib.bib7)) introduced a novel two-stage fine-tuning
    method and integrated time-series patching with additional temporal encoding into
    pre-trained LLMs. (Cao et al., [2023](#bib.bib5)) incorporated decomposition of
    time series and selection-based prompts for adapting to non-stationary data. (Jin
    et al., [2023a](#bib.bib18)) reprograms time series input with text prototypes
    and enriches it using context as a prefix for LLM alignment. However, these works
    directly input time series data into LLMs, overlooking the misalignment between
    time series and textual modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge distillation was first introduced by  (Hinton et al., [2015](#bib.bib15))
    to achieve model compression. Since then, this technique has been used to improve
    performance and efficiency across various tasks (Gou et al., [2021](#bib.bib13)),
    including transfer learning (Xu et al., [2020](#bib.bib35)), question answering (Hu
    et al., [2018](#bib.bib16)), text recognition (Guo et al., [2023](#bib.bib14))
    and so on. Recently, a few works (Dai et al., [2021](#bib.bib9); Jin et al., [2023b](#bib.bib19))
    have been proposed to address the cross-modal misalignment problem with knowledge
    distillation. However, these works do not focus on time series analysis tasks.
    Moreover, to the best of our knowledge, very little work has been done to investigate
    the solution to pre-trained LLMs cross-modal misalignment using knowledge distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in [Fig. 2](#S1.F2 "In 1 Introduction ‣ Taming Pre-trained LLMs for
    Generalised Time Series Forecasting via Cross-modal Knowledge Distillation"),
    we propose a novel knowledge distillation framework to transfer the knowledge
    from pre-trained LLMs for time series forecasting. The proposed framework consists
    of two branches: the textual modal branch and the temporal modal branch. In concrete,
    the textual modal branch takes the aligned text tokens $X_{text}$. A task-specific
    head is used to generate the output $Y_{text}$. The output of this branch is denoted
    as $Y_{time}$. During training, both static and dynamic knowledge from the textual
    modal branch is exploited to help the temporal branch achieve favorable performance
    on time series forecasting tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Static Knowledge Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As demonstrated in previous work (Mikolov et al., [2013](#bib.bib24)), the matrices
    of word embedding layers in pre-trained LLMs constitute a well-structured context
    representation space, e.g., semantic distances between different words can be
    quantized by vector similarity. We refer to this type of information as static
    knowledge, since this knowledge is input-agnostic. Despite this promising property,
    this static knowledge is usually neglected by previous LLMs-based time series
    methods (Zhou et al., [2023](#bib.bib39); Cao et al., [2023](#bib.bib5); Chang
    et al., [2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we attempt to exploit the static knowledge in pre-trained LLMs.
    The main challenge lies in the significant modal discrepancy between text and
    temporal series, which will lead to suboptimal results if we directly employ original
    word embedding as the input for the textual modal branch. For this reason, we
    propose a static knowledge distillation strategy to deal with this problem. Specifically,
    given a multivariate time series $I\in\mathbb{R}^{T\times C}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X_{time}={\rm MHSA(Embedding}(I))\in\mathbb{R}^{C\times M},$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: where $M$ is the feature dimension of pre-trained LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we consider using cross attention to align $X_{time}$ is usually
    huge, e.g., 50257 in GPT2 (Radford et al., [2019](#bib.bib28)), directly using
    cross attention will incur a significant cost. Observing that semantic-similar
    words will form “synonym clusters”, we thus propose a principal word embedding
    extraction strategy, which uses the cluster center to represent its surrounding
    words, to reduce the number of word entries. Specifically, we use Principal Component
    Analysis (PCA) to perform dimension reduction on $\mathcal{D}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathcal{D}}={\rm PCA}(\mathcal{D}),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $d$.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that this process needs to be done only once before model
    training and does not incur much training overhead. We then use Multi-head Cross
    Attention with $\hat{\mathcal{D}}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle X_{text}={\rm Softmax}(\frac{QK^{T}}{\sqrt{C}})V,$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle Q=X_{time}W_{q},K=\hat{\mathcal{D}}W_{k},V=\hat{\mathcal{D}}W_{v},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $W_{q}$), key ($K$), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Dynamic Knowledge Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pre-trained LLMs have powerful capability of modeling dynamic contexts with
    different inputs, and we refer to this input-specific feature from the textual
    modal branch as dynamic knowledge. We propose two distillation losses to migrate
    the pre-trained dynamic knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature Regularization Loss.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first transfer the dynamic knowledge from the hidden text feature $F^{l}_{text}$-th
    Transformer block in the text-modal and time-modal branches, respectively, the
    feature regularization loss is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma$ and $\phi^{text}_{l}(\cdot)$ to transform the features from temporal
    and textual modalities to the shared representation space.
  prefs: []
  type: TYPE_NORMAL
- en: Modal Consistency Loss.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We further add constraints to the output layer to supervise the learning of
    the temporal modal branch. Specifically, given the outputs $Y_{time}$ from the
    temporal modal branch and textual modal branch respectively, we use the following
    loss to ensure that the output is consistent across modalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{{output}}={\rm sim}({Y}_{{text}},Y_{time}),$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where ${\rm sim}(\cdot,\cdot)$ is a chosen similarity function.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | LLaTA | GPT4TS | PatchTST | Crossformer | FEDformer | TimesNet |
    MICN | DLinear | TiDE |'
  prefs: []
  type: TYPE_TB
- en: '|  (Ours)  |  ([2023](#bib.bib39))  |  ([2023](#bib.bib25))  |  ([2023](#bib.bib37))  |  ([2022](#bib.bib38))  |  ([2023](#bib.bib34))  |  ([2022](#bib.bib31))  |  ([2023](#bib.bib36))  |  ([2023](#bib.bib11))  |'
  prefs: []
  type: TYPE_TB
- en: '| Metric | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE
    | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| ETTm1 | 96 | 0.323 | 0.349 | 0.329 | 0.364 | 0.321 | 0.360 | 0.360 | 0.401
    | 0.379 | 0.419 | 0.338 | 0.375 | 0.316 | 0.362 | 0.345 | 0.372 | 0.352 | 0.373
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.374 | 0.375 | 0.368 | 0.382 | 0.362 | 0.384 | 0.402 | 0.440 | 0.426
    | 0.441 | 0.374 | 0.387 | 0.363 | 0.390 | 0.380 | 0.389 | 0.389 | 0.391 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.409 | 0.399 | 0.400 | 0.403 | 0.392 | 0.402 | 0.543 | 0.528 | 0.445
    | 0.459 | 0.410 | 0.411 | 0.408 | 0.426 | 0.413 | 0.413 | 0.423 | 0.413 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.477 | 0.438 | 0.460 | 0.439 | 0.450 | 0.435 | 0.704 | 0.642 | 0.543
    | 0.490 | 0.478 | 0.450 | 0.481 | 0.476 | 0.474 | 0.453 | 0.485 | 0.448 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.395 | 0.390 | 0.389 | 0.397 | 0.381 | 0.395 | 0.502 | 0.502 |
    0.448 | 0.452 | 0.400 | 0.406 | 0.392 | 0.413 | 0.403 | 0.407 | 0.412 | 0.406
    |'
  prefs: []
  type: TYPE_TB
- en: '| ETTm2 | 96 | 0.178 | 0.256 | 0.178 | 0.263 | 0.178 | 0.260 | 0.273 | 0.356
    | 0.203 | 0.287 | 0.187 | 0.267 | 0.179 | 0.275 | 0.193 | 0.292 | 0.181 | 0.264
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.242 | 0.297 | 0.245 | 0.306 | 0.249 | 0.307 | 0.426 | 0.487 | 0.269
    | 0.328 | 0.249 | 0.309 | 0.307 | 0.376 | 0.284 | 0.362 | 0.246 | 0.304 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.307 | 0.339 | 0.309 | 0.347 | 0.313 | 0.346 | 1.013 | 0.714 | 0.325
    | 0.366 | 0.321 | 0.351 | 0.325 | 0.388 | 0.369 | 0.427 | 0.307 | 0.341 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.397 | 0.393 | 0.409 | 0.408 | 0.400 | 0.398 | 3.154 | 1.274 | 0.421
    | 0.415 | 0.408 | 0.403 | 0.502 | 0.490 | 0.554 | 0.522 | 0.407 | 0.397 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.281 | 0.321 | 0.285 | 0.331 | 0.285 | 0.327 | 1.216 | 0.707 |
    0.305 | 0.349 | 0.291 | 0.333 | 0.328 | 0.382 | 0.350 | 0.401 | 0.289 | 0.326
    |'
  prefs: []
  type: TYPE_TB
- en: '| ETTh1 | 96 | 0.369 | 0.389 | 0.376 | 0.397 | 0.393 | 0.408 | 0.420 | 0.439
    | 0.376 | 0.419 | 0.384 | 0.402 | 0.421 | 0.431 | 0.386 | 0.400 | 0.384 | 0.393
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.427 | 0.423 | 0.438 | 0.426 | 0.445 | 0.434 | 0.540 | 0.519 | 0.420
    | 0.448 | 0.436 | 0.429 | 0.474 | 0.487 | 0.437 | 0.432 | 0.436 | 0.422 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.456 | 0.436 | 0.479 | 0.446 | 0.484 | 0.451 | 0.722 | 0.648 | 0.459
    | 0.465 | 0.491 | 0.469 | 0.569 | 0.551 | 0.481 | 0.459 | 0.480 | 0.445 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.479 | 0.467 | 0.495 | 0.476 | 0.480 | 0.471 | 0.799 | 0.685 | 0.506
    | 0.507 | 0.521 | 0.500 | 0.770 | 0.672 | 0.519 | 0.516 | 0.481 | 0.469 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.432 | 0.428 | 0.447 | 0.436 | 0.450 | 0.441 | 0.620 | 0.572 |
    0.440 | 0.460 | 0.458 | 0.450 | 0.558 | 0.535 | 0.456 | 0.452 | 0.445 | 0.432
    |'
  prefs: []
  type: TYPE_TB
- en: '| ETTh2 | 96 | 0.279 | 0.331 | 0.295 | 0.348 | 0.294 | 0.343 | 0.745 | 0.584
    | 0.358 | 0.397 | 0.340 | 0.374 | 0.299 | 0.364 | 0.333 | 0.387 | 0.400 | 0.440
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.353 | 0.380 | 0.386 | 0.404 | 0.377 | 0.393 | 0.877 | 0.656 | 0.429
    | 0.439 | 0.402 | 0.414 | 0.441 | 0.454 | 0.477 | 0.476 | 0.528 | 0.509 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.362 | 0.394 | 0.421 | 0.435 | 0.381 | 0.409 | 1.043 | 0.731 | 0.496
    | 0.487 | 0.452 | 0.452 | 0.654 | 0.567 | 0.594 | 0.541 | 0.643 | 0.571 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.404 | 0.426 | 0.422 | 0.445 | 0.412 | 0.433 | 1.104 | 0.763 | 0.463
    | 0.474 | 0.462 | 0.468 | 0.956 | 0.716 | 0.831 | 0.657 | 0.874 | 0.679 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.349 | 0.382 | 0.381 | 0.408 | 0.366 | 0.394 | 0.942 | 0.684 |
    0.437 | 0.449 | 0.414 | 0.427 | 0.587 | 0.525 | 0.559 | 0.515 | 0.611 | 0.550
    |'
  prefs: []
  type: TYPE_TB
- en: '| Weather | 96 | 0.164 | 0.204 | 0.182 | 0.223 | 0.177 | 0.218 | 0.158 | 0.230
    | 0.217 | 0.296 | 0.172 | 0.220 | 0.161 | 0.229 | 0.196 | 0.255 | 0.202 | 0.261
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.214 | 0.250 | 0.231 | 0.263 | 0.225 | 0.259 | 0.206 | 0.277 | 0.276
    | 0.336 | 0.219 | 0.261 | 0.220 | 0.281 | 0.237 | 0.296 | 0.242 | 0.298 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.269 | 0.291 | 0.283 | 0.300 | 0.278 | 0.297 | 0.272 | 0.335 | 0.339
    | 0.380 | 0.280 | 0.306 | 0.278 | 0.331 | 0.283 | 0.335 | 0.287 | 0.335 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.355 | 0.352 | 0.360 | 0.350 | 0.354 | 0.348 | 0.398 | 0.418 | 0.403
    | 0.428 | 0.365 | 0.359 | 0.311 | 0.356 | 0.345 | 0.381 | 0.351 | 0.386 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.250 | 0.274 | 0.264 | 0.284 | 0.258 | 0.280 | 0.259 | 0.315 |
    0.309 | 0.360 | 0.259 | 0.287 | 0.242 | 0.299 | 0.265 | 0.317 | 0.271 | 0.320
    |'
  prefs: []
  type: TYPE_TB
- en: '| ECL | 96 | 0.145 | 0.238 | 0.185 | 0.272 | 0.195 | 0.285 | 0.219 | 0.314
    | 0.193 | 0.308 | 0.168 | 0.272 | 0.164 | 0.269 | 0.197 | 0.282 | 0.237 | 0.329
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.161 | 0.252 | 0.189 | 0.276 | 0.199 | 0.289 | 0.231 | 0.322 | 0.201
    | 0.315 | 0.184 | 0.289 | 0.177 | 0.285 | 0.196 | 0.285 | 0.236 | 0.330 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.175 | 0.267 | 0.204 | 0.291 | 0.215 | 0.305 | 0.246 | 0.337 | 0.214
    | 0.329 | 0.198 | 0.300 | 0.193 | 0.304 | 0.209 | 0.301 | 0.249 | 0.344 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.222 | 0.303 | 0.245 | 0.324 | 0.256 | 0.337 | 0.280 | 0.363 | 0.246
    | 0.355 | 0.220 | 0.320 | 0.212 | 0.321 | 0.245 | 0.333 | 0.284 | 0.373 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.175 | 0.265 | 0.205 | 0.290 | 0.216 | 0.304 | 0.244 | 0.334 |
    0.214 | 0.327 | 0.192 | 0.295 | 0.186 | 0.294 | 0.212 | 0.300 | 0.251 | 0.344
    |'
  prefs: []
  type: TYPE_TB
- en: '| Traffic | 96 | 0.407 | 0.268 | 0.468 | 0.307 | 0.544 | 0.359 | 0.522 | 0.290
    | 0.587 | 0.366 | 0.593 | 0.321 | 0.519 | 0.309 | 0.650 | 0.396 | 0.805 | 0.493
    |'
  prefs: []
  type: TYPE_TB
- en: '| 192 | 0.430 | 0.278 | 0.476 | 0.311 | 0.540 | 0.354 | 0.530 | 0.293 | 0.604
    | 0.373 | 0.617 | 0.336 | 0.537 | 0.315 | 0.598 | 0.370 | 0.756 | 0.474 |'
  prefs: []
  type: TYPE_TB
- en: '| 336 | 0.444 | 0.281 | 0.488 | 0.317 | 0.551 | 0.358 | 0.558 | 0.305 | 0.621
    | 0.383 | 0.629 | 0.336 | 0.534 | 0.313 | 0.605 | 0.373 | 0.762 | 0.477 |'
  prefs: []
  type: TYPE_TB
- en: '| 720 | 0.477 | 0.300 | 0.521 | 0.333 | 0.586 | 0.375 | 0.589 | 0.328 | 0.626
    | 0.382 | 0.640 | 0.350 | 0.577 | 0.325 | 0.645 | 0.394 | 0.719 | 0.449 |'
  prefs: []
  type: TYPE_TB
- en: '| *Avg.* | 0.439 | 0.281 | 0.488 | 0.317 | 0.555 | 0.361 | 0.550 | 0.304 |
    0.610 | 0.376 | 0.620 | 0.336 | 0.541 | 0.315 | 0.625 | 0.383 | 0.760 | 0.473
    |'
  prefs: []
  type: TYPE_TB
- en: '| $1^{\text{st}}$  *Count* | 56 | 1 | 7 | 2 | 1 | 0 | 4 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Multivariate long-term forecasting results with different prediction
    lengths $H\in\{96,192,336,720\}$ *Count* indicates the number of times each method
    achieves the best results.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Parameter Efficient Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid catastrophic forgetting when tuning for downstream tasks, and to improve
    training efficiency, we employ the parameter-efficient training technique to fine-tune
    the pre-trained LLMs. Specifically, for the temporal modal branch, we introduce
    Low-rank Adaptation (LoRA) (Hu et al., [2021](#bib.bib17)) and fine-tune the positional
    encoding weights. The total loss during training is the weighted summation of
    the supervised loss $\mathcal{L}_{sup}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1}$ are hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate the effectiveness of the proposed LLaTA, we conduct extensive
    experiments on various time series forecasting tasks, including short/long-term
    forecasting and few/zero-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We carefully select representative baselines from the recent time series forecasting
    landscape, including the following categories: (1) LLMs-based models: GPT4TS (Zhou
    et al., [2023](#bib.bib39)); (2) Transformer-based models: PatchTST (Nie et al.,
    [2023](#bib.bib25)), Crossformer (Zhang and Yan, [2023](#bib.bib37)), ETSformer (Woo
    et al., [2022](#bib.bib32)), FEDformer (Zhou et al., [2022](#bib.bib38)) and Autoformer (Wu
    et al., [2021](#bib.bib33)); (3) CNN-based models: TCN (Bai et al., [2018](#bib.bib3)),
    MICN (Wang et al., [2022](#bib.bib31)) and TimesNet (Wu et al., [2023](#bib.bib34));
    (4) MLP-based models: DLinear (Zeng et al., [2023](#bib.bib36)) and TiDE (Das
    et al., [2023](#bib.bib11)). Besides, N-HiTS (Challu et al., [2022](#bib.bib6))
    and N-BEATS (Oreshkin et al., [2019](#bib.bib26)) are included for short-term
    forecasting.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following (Zhou et al., [2023](#bib.bib39)), we use pre-trained GPT2 based model
    (Radford et al., [2019](#bib.bib28)) with the first 6 Transformer layers as our
    backbone. Optimization is conducted using the Adam optimizer (Kingma and Ba, [2014](#bib.bib20)),
    with a learning rate of $0.0005$. In terms of loss functions for long-term forecasting,
    we apply L1 loss across all three loss types for ETT datasets, while for the other
    three datasets, smooth L1 loss is utilized. For short-term forecasting, we compute
    supervised loss with SMAPE, modal consistency loss with MASE, and feature regularization
    loss with smooth L1 loss, respectively. More details are provided in the supplementary
    material.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | LLaTA | GPT4TS | PatchTST | ETSformer | FEDformer | Autoformer |
    TimesNet | TCN | N-HiTS | N-BEATS | DLinear |'
  prefs: []
  type: TYPE_TB
- en: '|  (Ours)  |  ([2023](#bib.bib39))  |  ([2023](#bib.bib25))  |  ([2022](#bib.bib32))  |  ([2022](#bib.bib38))  |  ([2021](#bib.bib33))  |  ([2023](#bib.bib34))  |  ([2018](#bib.bib3))  |  ([2022](#bib.bib6))  |  ([2019](#bib.bib26))  |  ([2023](#bib.bib36))  |'
  prefs: []
  type: TYPE_TB
- en: '| Yearly |  SMAPE  | 13.351 | 13.531 | 13.477 | 18.009 | 13.728 | 13.974 |
    13.387 | 14.920 | 13.418 | 13.436 | 16.965 |'
  prefs: []
  type: TYPE_TB
- en: '|  MASE  | 3.003 | 3.015 | 3.019 | 4.487 | 3.048 | 3.134 | 2.996 | 3.364 |
    3.045 | 3.043 | 4.283 |'
  prefs: []
  type: TYPE_TB
- en: '|  OWA  | 0.786 | 0.793 | 0.792 | 1.115 | 0.803 | 0.822 | 0.786 | 0.880 | 0.793
    | 0.794 | 1.058 |'
  prefs: []
  type: TYPE_TB
- en: '| Quarterly |  SMAPE  | 9.990 | 10.177 | 10.380 | 13.376 | 10.792 | 11.338
    | 10.100 | 11.122 | 10.202 | 10.124 | 12.145 |'
  prefs: []
  type: TYPE_TB
- en: '|  MASE  | 1.164 | 1.194 | 1.233 | 1.906 | 1.283 | 1.365 | 1.182 | 1.360 |
    1.194 | 1.169 | 1.520 |'
  prefs: []
  type: TYPE_TB
- en: '|  OWA  | 0.878 | 0.898 | 0.921 | 1.302 | 0.958 | 1.012 | 0.890 | 1.001 | 0.899
    | 0.886 | 1.106 |'
  prefs: []
  type: TYPE_TB
- en: '| Monthly |  SMAPE  | 12.643 | 12.894 | 12.959 | 14.588 | 14.260 | 13.958 |
    12.679 | 15.626 | 12.791 | 12.677 | 13.514 |'
  prefs: []
  type: TYPE_TB
- en: '|  MASE  | 0.922 | 0.956 | 0.970 | 1.368 | 1.102 | 1.103 | 0.933 | 1.274 |
    0.969 | 0.937 | 1.037 |'
  prefs: []
  type: TYPE_TB
- en: '|  OWA  | 0.872 | 0.897 | 0.905 | 1.149 | 1.012 | 1.002 | 0.878 | 1.141 | 0.899
    | 0.880 | 0.956 |'
  prefs: []
  type: TYPE_TB
- en: '| Others |  SMAPE  | 4.552 | 4.940 | 4.952 | 7.267 | 4.954 | 5.485 | 4.891
    | 7.186 | 5.061 | 4.925 | 6.709 |'
  prefs: []
  type: TYPE_TB
- en: '|  MASE  | 3.092 | 3.228 | 3.347 | 5.240 | 3.264 | 3.865 | 3.302 | 4.677 |
    3.216 | 3.391 | 4.953 |'
  prefs: []
  type: TYPE_TB
- en: '|  OWA  | 0.967 | 1.029 | 1.049 | 1.591 | 1.036 | 1.187 | 1.035 | 1.494 | 1.040
    | 1.053 | 1.487 |'
  prefs: []
  type: TYPE_TB
- en: '| Average |  SMAPE  | 11.765 | 11.991 | 12.059 | 14.718 | 12.840 | 12.909 |
    11.829 | 13.961 | 11.927 | 11.851 | 13.639 |'
  prefs: []
  type: TYPE_TB
- en: '|  MASE  | 1.567 | 1.600 | 1.623 | 2.408 | 1.701 | 1.771 | 1.585 | 1.945 |
    1.613 | 1.599 | 2.095 |'
  prefs: []
  type: TYPE_TB
- en: '|  OWA  | 0.844 | 0.861 | 0.869 | 1.172 | 0.918 | 0.939 | 0.851 | 1.023 | 0.861
    | 0.855 | 1.051 |'
  prefs: []
  type: TYPE_TB
- en: '| $1^{\text{st}}$  *Count* | 14 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Short-term forecasting results on M4 dataset. The input length and
    prediction length are set to $[12,96]$ *Count* indicates the number of times each
    method achieves the best results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | ETTm1 | ETTm2 | ETTh1 | ETTh2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  TiDE ([2023](#bib.bib11))  | 0.515 | 0.469 | 0.303 | 0.337 | 0.779 | 0.604
    | 0.421 | 0.428 |'
  prefs: []
  type: TYPE_TB
- en: '|  DLinear ([2022](#bib.bib31))  | 0.567 | 0.499 | 0.329 | 0.382 | 0.647 |
    0.552 | 0.441 | 0.458 |'
  prefs: []
  type: TYPE_TB
- en: '|  MICN ([2022](#bib.bib31))  | 0.970 | 0.674 | 1.073 | 0.716 | 1.405 | 0.814
    | 2.533 | 1.158 |'
  prefs: []
  type: TYPE_TB
- en: '|  TimesNet ([2023](#bib.bib34))  | 0.673 | 0.534 | 0.321 | 0.354 | 0.865 |
    0.625 | 0.476 | 0.463 |'
  prefs: []
  type: TYPE_TB
- en: '|  FEDformer ([2022](#bib.bib38))  | 0.696 | 0.572 | 0.356 | 0.392 | 0.750
    | 0.607 | 0.553 | 0.525 |'
  prefs: []
  type: TYPE_TB
- en: '|  Crossformer ([2023](#bib.bib37))  | 1.340 | 0.848 | 1.985 | 1.048 | 1.744
    | 0.914 | 3.139 | 1.378 |'
  prefs: []
  type: TYPE_TB
- en: '|  PatchTST ([2023](#bib.bib25))  | 0.557 | 0.483 | 0.295 | 0.334 | 0.683 |
    0.546 | 0.550 | 0.487 |'
  prefs: []
  type: TYPE_TB
- en: '|  GPT4TS ([2023](#bib.bib39))  | 0.608 | 0.500 | 0.303 | 0.336 | 0.689 | 0.555
    | 0.579 | 0.497 |'
  prefs: []
  type: TYPE_TB
- en: '|  LLaTA (Ours)  | 0.504 | 0.462 | 0.302 | 0.330 | 0.644 | 0.541 | 0.419 |
    0.427 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Few-shot learning results on 10% training data of ETT datasets. All
    the results are averaged from 4 different prediction lengths $H\in\{96,192,336,720\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models |  h1 $\rightarrow$ m2  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  TiDE ([2023](#bib.bib11))  | 0.774 | 0.574 | 0.314 | 0.355 | 0.841 | 0.590
    | 0.321 | 0.364 |'
  prefs: []
  type: TYPE_TB
- en: '|  DLinear ([2023](#bib.bib11))  | 0.760 | 0.577 | 0.399 | 0.439 | 0.778 |
    0.594 | 0.496 | 0.496 |'
  prefs: []
  type: TYPE_TB
- en: '|  MICN ([2022](#bib.bib31))  | 1.439 | 0.780 | 2.428 | 1.236 | 0.764 | 0.601
    | 0.527 | 0.519 |'
  prefs: []
  type: TYPE_TB
- en: '|  TimesNet ([2023](#bib.bib34))  | 0.794 | 0.575 | 0.339 | 0.370 | 1.286 |
    0.705 | 0.361 | 0.390 |'
  prefs: []
  type: TYPE_TB
- en: '|  FEDformer ([2022](#bib.bib38))  | 0.765 | 0.588 | 0.357 | 0.403 | 0.741
    | 0.588 | 0.365 | 0.405 |'
  prefs: []
  type: TYPE_TB
- en: '|  Crossformer ([2023](#bib.bib37))  | 0.999 | 0.736 | 1.120 | 0.789 | 1.195
    | 0.711 | 2.043 | 1.124 |'
  prefs: []
  type: TYPE_TB
- en: '|  PatchTST ([2023](#bib.bib25))  | 0.894 | 0.610 | 0.318 | 0.362 | 0.871 |
    0.596 | 0.420 | 0.433 |'
  prefs: []
  type: TYPE_TB
- en: '|  GPT4TS ([2023](#bib.bib39))  | 0.798 | 0.574 | 0.317 | 0.359 | 0.920 | 0.610
    | 0.331 | 0.371 |'
  prefs: []
  type: TYPE_TB
- en: '|  LLaTA (Ours)  | 0.755 | 0.574 | 0.316 | 0.355 | 0.836 | 0.586 | 0.319 |
    0.360 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Zero-shot learning results on ETT datasets, where ‘h1’, ‘h2’, ‘m1’,
    and ‘m2’ denote ETTh1, ETTh2, ETTm1, and ETTm2 respectively. “${{\color[rgb]{1,0.65,0.3}\blacklozenge}}\to{{\color[rgb]{0.75,0.5,0.75}\bigstar}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Long-term Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setups.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct experiments on seven widely-used real-world datasets, including the
    Electricity Transformer Temperature (ETT) dataset with its four subsets (ETTh1,
    ETTh2, ETTm1, ETTm2), Weather, ECL, and Traffic (Wu et al., [2021](#bib.bib33)).
    Detailed descriptions of the implementation and datasets are provided in supplementary
    material. The input time series length $T$. Consistent with prior works, the Mean
    Square Error (MSE) and Mean Absolute Error (MAE) are chosen as evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Comprehensive long-term forecasting results are presented in Table [1](#S3.T1
    "Table 1 ‣ Modal Consistency Loss. ‣ 3.2 Dynamic Knowledge Learning ‣ 3 Methodology
    ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation"). Our method consistently delivers state-of-the-art performance,
    achieving the top results in 56 evaluations, in contrast to the nearest competing
    baseline which achieves top results only 7 times. Notably, our approach reduces
    MSE/MAE by 7.05%/6.53% compared to the state-of-the-art Transformer-based model
    PatchTST. In comparison with the LLM-powered method GPT4TS, we observe a reduction
    of 5.94%/5.14% in MSE/MAE. Moreover, our improvements are substantial against
    other baseline methods, exceeding 10% in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Short-term Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setups.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adopt the M4 datasets (Spyros Makridakis, [2018](#bib.bib29)), which comprise
    univariate marketing data collected yearly, quarterly, and monthly. Comprehensive
    details are available in the supplementary material. In this case, the prediction
    horizons are comparatively short, ranging in $[6,48]$. Correspondingly, the input
    lengths are set to be twice the size of the prediction horizons. The evaluation
    metrics are symmetric mean absolute percentage error (SMAPE), mean absolute scaled
    error (MSAE), and overall weighted average (OWA).
  prefs: []
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Table [2](#S4.T2 "Table 2 ‣ Implementation Details. ‣ 4 Experiments
    ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation"), our method demonstrates superior performance in short-term
    forecasting across various evaluation metrics. Notably, it achieves the best results
    in 14 out of 15 categories, markedly outperforming all baselines. In comparison
    with TimesNet, currently the leading method in short-term forecasting, our model
    achieves a 1% overall improvement in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Few/zero-shot Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have demonstrated remarkable performance in both few-shot and zero-shot
    tasks. The capabilities of few-shot and zero-shot learning are critically important
    for general time series forecasting models (Brown et al., [2020](#bib.bib4); Achiam
    et al., [2023](#bib.bib1); Liu et al., [2023a](#bib.bib22); Kojima et al., [2022](#bib.bib21)).
    To thoroughly assess the generalized ability of our method in time series forecasting,
    we conduct experiments under few-shot and zero-shot learning settings. In few-shot
    learning, only a small ratio of the training data is utilized. For zero-shot learning,
    the model trained on one dataset is directly employed for testing on another dataset
    without any additional training.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot Learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct few-shot experiments on four ETT datasets. Specifically, for each
    dataset, we utilize only the first 10% of the training data. This constrained
    data scenario presents a considerable challenge, testing the ability of the model
    to learn effectively with limited information. Table [3](#S4.T3 "Table 3 ‣ Implementation
    Details. ‣ 4 Experiments ‣ Taming Pre-trained LLMs for Generalised Time Series
    Forecasting via Cross-modal Knowledge Distillation") demonstrates that our method
    outperforms other baselines, highlighting its robustness in the few-shot setting.
    Compared with GPT4TS and PatchTST, our method achieves an average reduction of
    8% and 9%, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot Learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Going beyond few-shot scenarios, we further delve into zero-shot learning, where
    LLMs demonstrate their prowess as adept and intuitive reasoners. In this setting,
    models trained on one dataset $\blacklozenge$, without any further training. As
    shown in Table [4](#S4.T4 "Table 4 ‣ Implementation Details. ‣ 4 Experiments ‣
    Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation"), our method stands out for its exceptional performance,
    surpassing GPT4TS and PatchTST by 4% and 9% respectively. This indicates that
    our approach significantly enhances the model’s capability for effective learning
    transfer across different domains.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Model Insights.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is also noteworthy that the performances of two MLP-based models, DLinear
    and TiDE, show great performance under few-shot and zero-shot settings. This can
    be attributed to their linear simplicity and good generalization capabilities.
    Furthermore, the success of these models underscores the potential of linear approaches
    in extracting underlying patterns even in complex datasets (Zeng et al., [2023](#bib.bib36)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b086af3981f2eff9c92066062718b16e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Distribution of cross attention weights between different datasets
    and the principal word embeddings. For presentation clarity, we give the distribution
    of attention weights corresponding to the first 100 principal word embeddings
    since it includes over 99% weights. In addition, we also quantitatively give the
    index of the principal word embeddings corresponding to the Top10 weights of each
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Abaltion Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ablation on Different Loss Functions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The feature regularization loss transfers dynamic text modality knowledge, while
    the modal consistency loss ensures output coherence across modalities. The supervised
    loss directly guides learning with ground truth data. We analyze the specific
    effects of each proposed loss function as detailed in Table [5](#S5.T5 "Table
    5 ‣ Ablation on the Number of Principal Components. ‣ 5 Abaltion Study ‣ Taming
    Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge
    Distillation"). Employing only the supervised loss resulted in MSE/MAE of 0.436/0.428
    on ETTh1 and 0.253/0.276 on Weather, respectively. The addition of feature regularization
    loss or model consistency loss led to incremental improvements, with the best
    performance observed when all three losses were combined, achieving the lowest
    MSE and MAE on both datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation on the Number of Principal Components.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We employ PCA to conduct dimensional reduction on the original word embeddings
    for efficient training. Despite the reduced cost, however, PCA may inevitably
    lead to information loss. In this section, we ablate the number of principal components
    $d$, which can attain an explainable variance ratio of 88% while achieving satisfactory
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '| feature | consist | supervised | ETTh1 | Weather |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  MSE  |  MAE  |  MSE  |  MAE  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ | ✓ | 0.436 | 0.428 | 0.253 | 0.276 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | $-$ | ✓ | 0.434 | 0.431 | 0.254 | 0.276 |'
  prefs: []
  type: TYPE_TB
- en: '| $-$ | ✓ | ✓ | 0.438 | 0.426 | 0.258 | 0.283 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | ✓ | 0.432 | 0.428 | 0.250 | 0.274 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Ablation on different loss functions. ‘feature’ denotes feature regularization
    loss. ‘consist’ denotes modal consistency loss. ‘supervised’ denotes supervised
    loss. All the results are averaged from 4 different prediction lengths $H\in\{96,192,336,720\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why Implicit Textual Tokens.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we would like to discuss why implicit word embedding vectors are adopted
    instead of the more intuitive natural language description. As demonstrated in (Jin
    et al., [2023a](#bib.bib18)), it is hard to describe a time series losslessly
    through limited natural language, and it is also cumbersome to find accurate descriptions.
    Therefore, we use an implicit approach to learn static knowledge from frozen word
    embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Difference form Other Work.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition, one concurrent work (Jin et al., [2023a](#bib.bib18)) also considers
    cross attention to extracting knowledge from the word embedding layer, and we
    would like to clarify the difference to emphasize our contribution. First, the
    existing method uses cross attention to generate embeddings and combines them
    with prompt prefixes as input to frozen LLMs, while our LLaTA aims to generate
    aligned textual tokens as the input of the textual modal branch for subsequent
    cross-modal distillation. Second, previous work introduces linear weight $W\in\mathbb{R}^{|\mathcal{A}|\times
    d}$, this solution can lead to significant costs, while our approach uses an offline
    manner to generate synonym clusters, which guarantees efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability on Implicit Alignment.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To narrow the temporal-textual modality gap, we perform cross attention on word
    embedding weights to generate aligned text tokens instead of intuitive natural
    language. To understand this implicit alignment, we visualize the distribution
    of cross attention weights on different datasets in  [Fig. 3](#S4.F3 "In Linear
    Model Insights. ‣ 4.3 Few/zero-shot Learning ‣ 4 Experiments ‣ Taming Pre-trained
    LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation")
    and derive some interesting findings. Firstly, the distribution of attention weights
    is diverse across datasets, suggesting that different types of time series attend
    to different principal word embeddings. In addition, we also found an interesting
    phenomenon that similar datasets exhibit similar behavior, e.g., the ETT datasets
    ETTh1, ETTh2, and ETTm1 have similar distribution shapes, while the weather-related
    time series exhibit significantly different distributions, suggesting that similar
    domain may share some common implicit principal word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a6367df18b58834ae6ab92f8ff84fd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Ablation on different low dimension $d$ of PCA on (a) ETTh1 and (b)
    ETTh2 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose LLaTA, a novel cross-modal knowledge distillation framework
    that leverages the robust capabilities of Large Language Models (LLMs) for time
    series forecasting. Our approach effectively bridges the inherent modality gap
    between temporal data and the textual nature of LLMs. By distilling both input-agnostic
    static and input-dependent dynamic knowledge from LLMs, LLaTA not only improves
    time series forecasting performance but also substantially enhances generalization
    capabilities. Extensive experiments across several real-world datasets validate
    that LLaTA sets a new benchmark in both long- and short-term forecasting. This
    work paves the way for future research in integrating diverse modalities with
    LLMs, promising more versatile and powerful applications in time series analysis
    and beyond. Future efforts could focus on enriching LLMs with explicit time series
    knowledge and building multi-modal models with joint reasoning across modalities
    via advanced pre-training techniques.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angryk et al. [2020] Rafal A Angryk, Petrus C Martens, Berkay Aydin, Dustin
    Kempton, Sushant S Mahajan, Sunitha Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina
    Filali Boubrahimi, Shah Muhammad Hamdi, et al. Multivariate time series dataset
    for space weather data analytics. Scientific data, 7(1):227, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2018] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical
    evaluation of generic convolutional and recurrent networks for sequence modeling.
    arXiv preprint arXiv:1803.01271, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. Advances in Neural Information
    Processing Systems, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2023] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang
    Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer
    for time series forecasting. arXiv preprint arXiv:2310.04948, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Challu et al. [2022] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico
    Garza, Max Mergenthaler, and Artur Dubrawski. N-HiTs: Neural hierarchical interpolation
    for time series forecasting. arXiv preprint arXiv:2201.12886, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. [2023] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. LLM4TS: Two-stage
    fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint
    arXiv:2308.08469, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. A simple framework for contrastive learning of visual representations.
    In International Conference on Machine Learning, pages 1597–1607\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2021] Rui Dai, Srijan Das, and François Bremond. Learning an augmented
    rgb representation with cross-modal knowledge distillation for action detection.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    13053–13064, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2024] Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong
    Jiang, and Shu-Tao Xia. Periodicity decoupling framework for long-term series
    forecasting. In International Conference on Learning Representations, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Das et al. [2023] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and
    Rose Yu. Long-term forecasting with TiDE: Time-series dense encoder. arXiv preprint
    arXiv:2304.08424, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demirel et al. [2012] Ömer Fahrettin Demirel, Selim Zaim, Ahmet Çalişkan, and
    Pinar Özuyar. Forecasting natural gas consumption in istanbul using neural networks
    and multivariate time series methods. Turkish Journal of Electrical Engineering
    and Computer Sciences, 20(5):695–711, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. [2021] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. Knowledge distillation: A survey. International Journal of Computer Vision,
    129:1789–1819, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2023] Hang Guo, Tao Dai, Mingyan Zhu, Guanghao Meng, Bin Chen, Zhi
    Wang, and Shu-Tao Xia. One-stage low-resolution text recognition with high-resolution
    knowledge transfer. In Proceedings of the 31st ACM International Conference on
    Multimedia, pages 2189–2198, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. [2015] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2018] Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li,
    Nan Yang, and Ming Zhou. Attention-guided answer distillation for machine reading
    comprehension. arXiv preprint arXiv:1808.07644, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. arXiv preprint arXiv:2106.09685, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2023a] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-LLM:
    Time series forecasting by reprogramming large language models. arXiv preprint
    arXiv:2310.01728, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2023b] Yufeng Jin, Guosheng Hu, Haonan Chen, Duoqian Miao, Liang
    Hu, and Cairong Zhao. Cross-modal distillation for speaker recognition. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 37, pages 12977–12985,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. arXiv preprint arXiv:1412.6980, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances
    in Neural Information Processing Systems, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023a] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy,
    Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and
    Shwetak Patel. Large language models are few-shot health learners. arXiv preprint
    arXiv:2305.15525, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023b] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang,
    Lintao Ma, and Mingsheng Long. iTransformer: Inverted transformers are effective
    for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. [2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. [2023] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
    A time series is worth 64 words: Long-term forecasting with transformers. In International
    Conference on Learning Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oreshkin et al. [2019] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and
    Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time
    series forecasting. International Conference on Learning Representations, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patton [2013] Andrew Patton. Copula methods for forecasting multivariate time
    series. Handbook of economic forecasting, 2:899–960, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spyros Makridakis [2018] Spyros Makridakis. M4 dataset, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vobecky et al. [2022] Antonin Vobecky, David Hurych, Oriane Siméoni, Spyros
    Gidaris, Andrei Bursuc, Patrick Pérez, and Josef Sivic. Drive&segment: Unsupervised
    semantic segmentation of urban scenes via cross-modal distillation. In European
    Conference on Computer Vision, pages 478–495\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui
    Chen, and Yifei Xiao. MICN: Multi-scale local and global context modeling for
    long-term series forecasting. In International Conference on Learning Representations,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Woo et al. [2022] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and
    Steven C. H. Hoi. ETSformer: Exponential smoothing transformers for time-series
    forecasting. arXiv preprint arXiv:2202.01381, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2021] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer:
    Decomposition transformers with Auto-Correlation for long-term series forecasting.
    In Advances in Neural Information Processing Systems, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and
    Mingsheng Long. TimesNet: Temporal 2d-variation modeling for general time series
    analysis. In International Conference on Learning Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2020] Guodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge
    distillation meets self-supervision. In European Conference on Computer Vision,
    pages 588–604\. Springer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. [2023] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers
    effective for time series forecasting? In Proceedings of the AAAI conference on
    artificial intelligence, volume 37, pages 11121–11128, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Yan [2023] Yunhao Zhang and Junchi Yan. Crossformer: Transformer
    utilizing cross-dimension dependency for multivariate time series forecasting.
    In International Conference on Learning Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2022] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun,
    and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term
    series forecasting. In International Conference on Machine Learning, pages 27268–27286\.
    PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin.
    One Fits All: Power general time series analysis by pretrained lm. Advances in
    Neural Information Processing Systems, 36, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
