- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal
    Space Program'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08676](https://ar5iv.labs.arxiv.org/html/2408.08676)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Alejandro Carrasco Corresponding author. E-Mail:'
  prefs: []
  type: TYPE_NORMAL
- en: alejandro.carrasco.aragon@alumnos.upm.es Universidad Politécnica de Madrid,
    Madrid, Spain Victor Rodriguez-Fernandez Universidad Politécnica de Madrid, Madrid,
    Spain Richard Linares Massachussetts Institute of Technology, Massachussetts,
    USA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous
    agents that take actions based on the content of the user text prompt. This study
    explores the use of fine-tuned Large Language Models (LLMs) for autonomous spacecraft
    control, using the Kerbal Space Program Differential Games suite (KSPDG) as a
    testing environment. Traditional Reinforcement Learning (RL) approaches face limitations
    in this domain due to insufficient simulation capabilities and data. By leveraging
    LLMs, specifically fine-tuning models like GPT-3.5 and LLaMA, we demonstrate how
    these models can effectively control spacecraft using language-based inputs and
    outputs. Our approach integrates real-time mission telemetry into textual prompts
    processed by the LLM, which then generate control actions via an agent. The results
    open a discussion about the potential of LLMs for space operations beyond their
    nominal use for text-related tasks. Future work aims to expand this methodology
    to other space control tasks and evaluate the performance of different LLM families.
    The code is available at this URL: https://github.com/ARCLab-MIT/kspdg.'
  prefs: []
  type: TYPE_NORMAL
- en: \makeCustomtitle
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are, without a doubt, the last major breakthrough
    in the evolution of artificial intelligence systems. Since the release of ChatGPT
    [[1](#bib.bib1)] at the end of 2022, we have seen a plethora of applications and
    use cases emerge across various industries. From generating human-like text to
    aiding in code completion, LLMs have significantly impacted the way we interact
    with technology and the possibilities of what AI can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: In recent months, the use of LLMs is expanding beyond text-based applications
    to become language agents capable of taking actions based on the context of the
    system in which they are integrated. By leveraging the contextual information
    available to them, LLMs can make informed decisions and perform tasks autonomously.
    This new way of creating autonomous agents intersects with the usage of Reinforcement
    Learning (RL) algorithms, and provides a way to overcome some of its well-known
    limitations, such as the sample inefficiency, and the need for a well-defined
    reward function. Some recent studies have demonstrated how some powerful LLMs,
    such as GPT-4, can surpass state-of-the-art RL algorithms in complex games just
    through studying academics texts and reasoning [[2](#bib.bib2)], executing sophisticated
    trajectories and achieving good zero-shot performance.
  prefs: []
  type: TYPE_NORMAL
- en: This work is focused on the domain of space applications and the development
    of autonomous agents for guidance and control of spacecrafts. In this context,
    the creation of AI-based agents has mainly been tackled through RL during recent
    years, and in fact, we can find RL-based agents for different tasks such as sensor-tasking
    [[3](#bib.bib3)] and planetary landing [[4](#bib.bib4)]. However, unlike other
    AI research areas, the space domain lacks of publicly available simulation environments,
    which are crucial for training AI agents in complex space operations and providing
    a standard benchmark for evaluating different AI and autonomous control methods.
    To address this issue, Allen et al. introduced SpaceGym [[5](#bib.bib5)], a set
    non-cooperative game environments that are intended to spur development and act
    as proving grounds for autonomous and AI decision-makers in the orbital domain.
    Among the available environments in SpaceGym, in this work we focus on the Kerbal
    Space Program Differential Games suite (KSPDG). KSPDG is a suite of differential
    games, such as pursuit-evasion scenarios, encoded within the Kerbal Space Program
    (KSP) game engine ¹¹1https://www.privatedivision.com/portfolio/kerbal-space-program/
    and standardized with OpenAI Gym [[6](#bib.bib6)] and PettingZoo [[7](#bib.bib7)]
    interfaces, facilitating the use of diverse AI techniques, including multi-agent
    reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: While KSPDG presents an innovative framework for testing AI and autonomous control
    methods in space applications, it is unsuitable for RL training, due to technical
    and non-technical reasons. On the one hand, the KSP engine, which underpins KSPDG,
    lacks the capacity for the parallel, accelerated, and headless operations essential
    for extensive faster-than-real-time RL training. On the other hand, the principled
    stance of KSPDG’s creators to focus on evaluation rather than training emphasizes
    the need for a “true test set" environment where overfitting is minimized, and
    the genuine and unbiased capabilities of AI agents are tested. This approach diverges
    from the typical RL methodology that relies on iterative training and fine-tuning
    of agents within a specific simulation environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ca4a860c7d8229ecb21ed2c5f3827d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of the proposed approach to use a fine-tuned LLM (e.g. ChatGPT,
    LLaMA) as an autonomous spacecraft operator that gets, as user prompt, the current
    status of the mission from the KSDPG simulation environment (i.e., the state or
    observation in the RL jargon), and replies with a reasoned action to carry out,
    expressed as a function calling with the specific throttle vector and the textual
    justification behind the action.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the limitations of RL in creating autonomous agents for environments
    such as KSDPG, as well as for other space operations where numerous simulated
    data cannot be provided, we propose to adapt the current trend of LLM-based agents
    to develop an “intelligent" operator that controls a spacecraft based on the real-time
    telemetry of the environment, using language exclusively as the input and output
    of the system. As depicted in [Figure 1](#S1.F1 "In 1 Introduction ‣ Fine-tuning
    LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program"),
    we design the classic RL loop by interfacing the simulation environment (KSDPG)
    with a LLM, transforming the real-time observations (or state) of the mission
    as textual user prompts that are fed to the model. The LLM then processes the
    prompt and replies with an action that will be plugged in KSDPG to control the
    spacecraft. Our agent was ranked 2nd in the KSPDG challenge ²²2https://www.ll.mit.edu/conferences-events/2024/01/kerbal-space-program-differential-game-challenge,
    and was presented via a live demonstration during a special session at AIAA SciTech
    in January 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our previous study [[8](#bib.bib8)], prompt engineering was the primary
    focus, and LLM models demonstrated outstanding performance with zero-shot and
    few-shot prompts. That work utilized prompt engineering to effectively control
    a spacecraft in the Kerbal Space Program (KSP) simulation, achieving exceptional
    but non-generalizable results. Additionally, some fine-tuning experiments were
    conducted using the OpenAI fine-tuning API. At the time, the latest and most powerful
    model available for fine-tuning was gpt-3.5-turbo-0125. The customization of this
    fine-tuning process relied mainly on the data and three hyperparameters: number
    of epochs, batch size, and learning rate multiplier [[9](#bib.bib9)]. These experiments
    achieved moderate results due to the API’s limitations and its economical cost
    of fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the current study is to expand upon previous research by incorporating
    a set of open-source tools to overcome the limitations of the earlier framework.
    In contrast to the previous paper, this study focuses solely on the PE1_I3_E3
    scenario. PE refers to the pursuer-evader problem (rendezvous), I3 denotes the
    initial position (2.7 km of separation distance), and E3 represents a heuristic
    maneuvering technique. [[10](#bib.bib10)]. In the past two years, the landscape
    of LLMs has undergone significant transformations, characterized by frequent and
    substantial updates. Given these advancements, the "mighty" ChatGPT model is now
    rivaled by capable models such as Claude[[11](#bib.bib11)], Gemini[[12](#bib.bib12)],
    and open-source models like Mistral[[13](#bib.bib13)] and LLaMA[[14](#bib.bib14)].
    This research focuses on these latter models due to their enhanced flexibility
    and open source nature, which is crucial for pioneering research in this relatively
    unexplored area.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of a code agent with KSP is facilitated through a Remote Procedure
    Call (RPC) program that connects to the selected environment within the game.
    After each state update, the agent is able to execute an action from a defined
    set of continuous throttle commands. For a more verbose interaction with the LLM,
    the actions are verbal—forward, backward, right, left, up, and down—which are
    then converted into full throttle, full reverse throttle, or no action for each
    of the three thrusts. This discretization also allows the model to be more like
    a ‘human pilot’ instead of a control algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary challenge to address when fine tuning a model for KSPDG is the
    unavailability of varied mission scenarios, and the lack of expert gameplay logs
    completing those missions. In fact, this is one of the keys that prevented us
    from scaling up fine-tuning in [[8](#bib.bib8)]. [Figure 2](#S1.F2 "In 1 Introduction
    ‣ Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal
    Space Program") depicts a diagram of a program, alongside a bot that tracks the
    KSP navball’s information³³3https://wiki.kerbalspaceprogram.com/wiki/Navball and
    aligns the vessel to its prograde, to generate a number of pairs of randomized
    orbits for the pursuer and the evader problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The eccentricity, inclination, semimajor axis, and true anomaly of the pursuer’s
    orbit were randomly generated within the given constraints: eccentricity $\leq$
    3 km. The longitude of the ascending node and argument of periapsis were kept
    constant, ensuring the mission feasibility within a 4-minute span.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e458d1191d6335e6d69a1d63c7b11ad7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Diagram of the data generation process for fine-tuning the model.
    The sequence is as follows: (1) The orbit generator is invoked to create a new
    orbit. (2) The new orbit is saved into KSP. (3) The navball agent is activated
    to navigate the orbit and generate logs. (4) The logs are saved by the orbit generator.
    (5) After sufficient runs (e.g., 100), the script data parser converts the logs
    into text suitable for LLM processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the issue of data availability is resolved, the real focus of this research—fine-tuning—can
    be tackled. While OpenAI models, such as those used in previous studies, offer
    high-quality function calling and perform well with minimal customization, they
    present significant limitations for large-scale training due to cost and reduced
    flexibility. These constraints limited our previous experiments to using only
    1-2 files of human gameplay logs with discretized throttle actions to test the
    behavior of GPT models with limited data. Though these initial results were promising,
    they underscored the need for a more adaptable and cost-effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenges in this study, which demand specialized domain-specific
    knowledge for mission success, we transitioned to an open-source model capable
    of local training. LLaMA, the most renowned open-source model as of 2024, was
    chosen for its broad adoption and extensive research community support. The flexibility
    and adaptability of LLaMA allow for comprehensive fine-tuning tailored to our
    specific needs, moving beyond aggressive prompt engineering to a more data-driven
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fine-tune LLaMA efficiently⁴⁴4The LLaMA version used in this work is LLaMA-3-8B,
    we utilized a single workstation equipped with five RTX 4090 GPUs, employing several
    optimization techniques and tools to enhance efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low-Rank Adaptation (LoRA): LoRA reduces the number of trainable parameters
    by factorizing weight updates into low-rank matrices. Being more computational
    effective [[15](#bib.bib15)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Transformers Library: This library facilitated the management
    of model architecture and training processes [[16](#bib.bib16)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantization: To reduce model size and enhance inference speed, we applied
    quantization, converting weights and activations to lower precision without significantly
    impacting performance [[17](#bib.bib17)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLaMA Factory: This tool streamlined the fine-tuning process by efficiently
    managing datasets, pre-processing, and training tasks [[18](#bib.bib18)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training hyperparameters included a batch size of 2, a learning rate of 1e-4,
    a cosine learning rate scheduler, 3 epochs, a LoRA configuration (rank=16, alpha=8,
    dropout=0.05), Flash Attention 2 and Dora enabled, and gradient accumulation steps
    of 2. Fine-tuning only the last layers (2 in this case) and using a small batch
    size are recommended [[19](#bib.bib19), [20](#bib.bib20)]. The very small batch
    size here was due to hardware limitations. Extending the training beyond 3 epochs
    did not improve loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLaMA dataset consisted of 50 top-performing randomly generated orbit missions
    from the navball agent (see [Figure 2](#S1.F2 "In 1 Introduction ‣ Fine-tuning
    LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program")),
    chosen for their optimal distance (meters) and approach speed (seconds). These
    missions employed less aggressive prompting while still utilizing the chain-of-thought
    technique [[21](#bib.bib21)] to enable the model to learn and apply its own reasoning
    effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Table 1](#S2.T1 "In 2 Results ‣ Fine-tuning LLMs for Autonomous Spacecraft
    Control: A Case Study Using Kerbal Space Program"), we present the results of
    fine-tuned GPT models using human gameplays and LLaMA models using navball agent
    gameplays. GPT models show gradual improvement, surpassing the baseline after
    two training gameplays, indicating the potential of a fine-tuned model with more
    gameplays. Note that the simple fine-tuning in the GPT experiments used only one
    file, basic prompting, and the default hyperparameters selected by the OpenAI
    API. The LLaMA dataset was divided into subsets of 10, 25, and 50 gameplay files.
    One subset—the 10 files—utilized a sliding window technique, where previous actions
    were included to provide the model with additional context. LLaMA models consistently
    exceed their baseline performance, where the closest distance is ~120 meters better
    than GPT’s baseline⁵⁵5The GPT Model is 3.5, which is older than LLaMA 3. The best
    LLaMA models perform exceptionally well, demonstrating the potential benefits
    of larger datasets, indicating the potential of a fine-tuned model with more gameplays.
    Finally, the model utilizing the sliding window technique demonstrates great performance
    results leveraging the context of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Method Distance (m) Best       Average       Worst Failure Rate Average Latency
    (ms) baseline GPT 178.11 200.10 232.16 36.8% 840.42 simple fine-tuning 263.55
    265.89 271.51 0.0% 987.43 + hyperparameter tuning 188.90 202.08 210.62 0.1% 831.30
    + system prompt 197.41 214.87 227.67 0.0% 753.52 + two train gameplays 132.09
    159.78 200.47 0.2% 557.49 baseline LLaMA 52.69 140.68 267.32 9.09% 8580.43 fine-tune
    10 files 30.52 51.53 80.88 0.00% 3444.88 fine-tune 25 files 13.54 29.44 58.83
    0.00% 3316.89 fine-tune 50 files 11.86 29.76 48.81 0.00% 3455.29 fine-tune 10
    files win=3 23.08 40.03 49.28 0.00% 3292.44 human gameplays 5.97 6.25 6.54 - -
    navball agent 34.34 36.43 39.76 - -
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Performance of fine-tuned models for each technique, measured in distance
    (meters) and latency (milliseconds). Bold indicates best, underline indicates
    second best, and dashed underline indicates third best.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fine-tuning trajectories in [Figure 3](#S2.F3 "In 2 Results ‣ Fine-tuning
    LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program")
    indicate that the data ingested by the model aids in understanding the problem
    and determining appropriate actions. However, these trajectories also show that
    the model’s prior knowledge and reasoning still influence its performance. For
    instance, an incorrect hint, as depicted by the GPT trajectory, deteriorates the
    model’s performance and makes the agent recede from the evader once it overshoots
    (meaning when it goes past the evader). In contrast, an "agnostic" prompt that
    complements rather than dictates the model’s reasoning can even surpass the dataset
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e48df88b627283afa953d0649ad8265.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: This 3D plot depicts the trajectories of the best-performing fine-tuned
    models for GPT and LLaMA, along with the evader’s path. Due to an incorrect hint,
    the GPT model deviates significantly after overshooting its target, while the
    LLaMA model maintains a closer trajectory to the evader.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preliminary results of this study demonstrate that LLMs possess the capability
    to perform technical tasks beyond merely generating verbose text. Fine-tuning
    these models enhances their reasoning for autonomous space control missions without
    solely depending on the hints and reasoning provided in the prompt. This process
    yields a generalized model that can interact as an agent in KSP rendezvous missions.
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuned LLaMA model clearly surpasses the navball agent results, even
    in average distance, which poses a singular use case where the model outperforms
    the agent responsible of creating its own training data. The tests were run on
    validation sets, thus remarking the generalization capacity LLMs offer.
  prefs: []
  type: TYPE_NORMAL
- en: However, the complexity of a trajectory is not solely determined by distance.
    This highlights the necessity for more complex and dynamic scenarios that require
    additional metrics. This issue will be addressed in the next stages of this research.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the close distances achieved by the LLaMA models pave the way for
    exploring docking operations, where the increased complexity will test the robustness
    of the model’s chosen trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: We also plan on leveraging the advantages offered by multi-modal LLMs, such
    as the recently released GPT-4o [[22](#bib.bib22)] (where ’o’ stands for ’omni’)
    and the Phi-3 family [[23](#bib.bib23)], an open-source model incorporating multi-modal
    capabilities. Specifically, we intend to utilize vision capabilities in conjunction
    with language, as both modalities show the potential for creating an agent with
    human-like decisions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Introducing chatgpt. https://openai.com/blog/chatgpt. (Accessed on 03/29/2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov,
    Amos Azaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms
    by studying papers and reasoning. arXiv preprint arXiv:2305.15486, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Peng Mun Siew, Daniel Jang, Thomas G Roberts, and Richard Linares. Space-based
    sensor tasking using deep reinforcement learning. The Journal of the Astronautical
    Sciences, 69(6):1855–1892, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Brian Gaudet, Richard Linares, and Roberto Furfaro. Deep reinforcement
    learning for six degree-of-freedom planetary landing. Advances in Space Research,
    65(7):1723–1741, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ross E. Allen, Yaron Rachlin, Jessica Ruprecht, Sean Loughran, Jacob Varey,
    and Herbert Viggh. Spacegym: Discrete and differential games in non-cooperative
    space operations. In 2023 IEEE Aerospace Conference, pages 1–12, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
    Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth
    Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo
    Perez-Vicente, et al. Pettingzoo: Gym for multi-agent reinforcement learning.
    Advances in Neural Information Processing Systems, 34:15032–15043, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Victor Rodriguez-Fernandez, Alejandro Carrasco, Jason Cheng, Eli Scharf,
    Peng Mun Siew, and Richard Linares. Language models are spacecraft operators.
    arXiv preprint arXiv:2404.00413, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] OpenAI. Analyzing your fine-tuned model. https://platform.openai.com/docs/guides/fine-tuning/analyzing-your-fine-tuned-model,
    2024. Accessed: 2024-05-26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Ray Allen. spacegym-kspdg. https://github.com/mit-ll/spacegym-kspdg, 2023.
    Accessed: October 4, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Claude ai. https://claude.ai/. Accessed: 2024-05-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Gemini ai. https://gemini.google.com/. Accessed: 2024-05-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Tim Dettmers, Hugo Touvron, Antoine Joulin, Edouard Grave, Roberta Raileanu,
    and Myle Ott. Mistral 7b, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zhewei Yao, Weizhu Chen, Huan
    Wang, Zhen Dong, and Izhak Shafran. Lora: Low-rank adaptation of large language
    models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers:
    State-of-the-art natural language processing. In Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing: System Demonstrations, pages
    38–45, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. White
    noise: Regularizing neural networks through quantization. In International Conference
    on Machine Learning, pages 7197–7206\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and
    Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models.
    arXiv preprint arXiv:2403.13372, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Dominic Masters and Carlo Luschi. Revisiting small batch training for
    deep neural networks. ArXiv, abs/1804.07612, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman. Autofreeze: Automatically
    freezing model blocks to accelerate fine-tuning. ArXiv, abs/2102.01386, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Jason Wei et al. Chain of thought prompting elicits reasoning in large
    language models. arXiv preprint arXiv:2201.11903, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed:
    2024-05-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Misha Bilenko. New models added to the phi-3 family, available on microsoft
    azure. https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/,
    2024. Accessed: 2024-05-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
