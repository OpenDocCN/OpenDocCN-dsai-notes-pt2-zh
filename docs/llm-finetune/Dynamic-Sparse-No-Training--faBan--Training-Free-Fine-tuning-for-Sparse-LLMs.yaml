- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.08915](https://ar5iv.labs.arxiv.org/html/2310.08915)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \xpretocmd\@makefntextYuxin Zhang^(1†)  Lirui Zhao^(1†)  Mingbao Lin¹  Yunyun
    Sun²  Yiwu Yao²
  prefs: []
  type: TYPE_NORMAL
- en: Xingjia Han²  Jared Tanner³  Shiwei Liu^(4,5)  Rongrong Ji^(1,6‡)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  prefs: []
  type: TYPE_NORMAL
- en: Ministry of Education of China, Xiamen University
  prefs: []
  type: TYPE_NORMAL
- en: ²Huawei Technologies, ³University of Oxford, ⁴University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '⁵Eindhoven University of Technology, ⁶Institute of Artificial Intelligence,
    Xiamen University †Equal contribution   ‡Corresponding author: rrji@xmu.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ever-increasing large language models (LLMs), though opening a potential
    path for the upcoming artificial general intelligence, sadly drops a daunting
    obstacle on the way towards their on-device deployment. As one of the most well-established
    pre-LLMs approaches in reducing model complexity, network pruning appears to lag
    behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training)
    necessity under the massive volumes of model parameter and training data. To close
    this industry-academia gap, we introduce Dynamic Sparse No Training (DS\faBanT¹¹1
    Pronounced “DS No T”.), a training-free fine-tuning approach that slightly updates
    sparse LLMs without the expensive backpropagation and any weight updates. Inspired
    by the Dynamic Sparse Training, DS\faBanT minimizes the reconstruction error between
    the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing
    on top of sparse LLMs. To accomplish this purpose, DS\faBanT particularly takes
    into account the anticipated reduction in reconstruction error for pruning and
    growing, as well as the variance *w.r.t*. different input data for growing each
    weight. This practice can be executed efficiently in linear time since its obviates
    the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,
    Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DS\faBanT
    in enhancing the performance of sparse LLMs, especially at high sparsity levels.
    For instance, DS\faBanT is able to outperform the state-of-the-art Wanda by 26.79
    perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into
    how to fine-tune sparse LLMs in an efficient training-free manner and open new
    venues to scale the great potential of sparsity to LLMs. Codes are available at
    [https://github.com/zyxxmu/DSnoT](https://github.com/zyxxmu/DSnoT).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) (Zhang et al., [2022a](#bib.bib63); Touvron et al.,
    [2023a](#bib.bib49); Brown et al., [2020](#bib.bib2)) have recently emerged as
    the new favorite in various domains of natural language processing (NLP) (Wei
    et al., [2022b](#bib.bib54); [a](#bib.bib53); Bubeck et al., [2023](#bib.bib3)).
    Nevertheless, LLMs face a significant constraint: their extensive parameterization
    and computational demands present substantial challenges in terms of storage and
    deployment. For example, the GPT-175B model (Brown et al., [2020](#bib.bib2))
    eats up 320G of memory to load its parameters in FP16 precision, requiring at
    least five A100-80G GPUs for inference (Frantar & Alistarh, [2023](#bib.bib13)).
    In response to this issue, there has been a surge of interest in compressing LLMs,
    as it holds the promise of LLMs while remarkably reducing memory usage and computational
    costs. To date, the majority of current effort for LLM compression falls into
    quantization (Yao et al., [2022](#bib.bib59); Lin et al., [2023](#bib.bib31);
    Frantar et al., [2022](#bib.bib14); Dettmers et al., [2023](#bib.bib9); [2022](#bib.bib8);
    Xiao et al., [2023](#bib.bib58)), which compresses LLMs by diminishing the number
    of bits employed to represent weights or hidden states.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, network pruning (LeCun et al., [1989](#bib.bib28); Han et al.,
    [2015](#bib.bib18); Mocanu et al., [2018](#bib.bib39)), a technique that removes
    superfluous weights to create a sparse and lightweight model, has received relatively
    little attention (Frantar & Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)).
    The plausible reason is that, network pruning usually appreciates at least one,
    usually many, iterations of fine-tuning or re-training to guarantee top performance (Frankle
    & Carbin, [2019](#bib.bib12); Yin et al., [2023](#bib.bib60)). This fine-tuning
    step would cause a significant amount of compute and memory footprints due to
    the colossal model size and massive training data of modern LLMs, which even unnerves
    large corporations, let alone individual researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Two previous arts have explored the possibility to scale pruning to billion-level
    LLMs without any fine-tuning. SparseGPT (Frantar & Alistarh, [2023](#bib.bib13))
    formulates LLM pruning as a layer-wise weight reconstruction problem, where the
    target falls into mitigating the output discrepancy, *w.r.t.*, reconstruction
    error, between dense and sparse LLMs. To solve the row-Hessian challenge, *i.e.*,
    the need for calculating the expensive inversion of a huge matrix for each row
    individually, SparseGPT iteratively applies OBS (Hassibi et al., [1993](#bib.bib20))
    to individually prune and updates weights in a column-wise manner, ultimately
    reaching the same optimal solution as applying the closed-form regression reconstruction.
    Wanda (Sun et al., [2023](#bib.bib48)) proposes a new pruning metric that takes
    both weight magnitude and their corresponding input activations into consideration,
    performing on part with SparseGPT without the need for the expensive second-order
    information. The intuition behind Wanda lies in the existence of emergent outlier
    feature dimensions in large-scale LLMs which are significantly larger than typical
    features and meanwhile are essential for the optimal performance of LLMs (Dettmers
    et al., [2022](#bib.bib8)). While these two approaches enable LLM pruning without
    performing fine-tuning, their performance is still far from satisfactory, *e.g.*,
    starting to lose performance at 20% sparsity with LLaMA-30B. Therefore, it is
    imperative to enable fine-tuning for sparse LLMs to fully unlock the potential
    of sparsity to escalate the affordability of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d07301d98ea2a6c8c221d09a0b7e04ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Perplexity on WikiText-2 (left) and running time (right) of different
    methods for pruning LLaMA-V1 model family at 60% sparsity rate. Without any training,
    DS\faBanT consistently improves the performance of sparse LLMs, all within a linear
    time spectrum.'
  prefs: []
  type: TYPE_NORMAL
- en: In a parallel vein, Dynamic Sparse Training (DST), as outlined in previous research (Mocanu
    et al., [2018](#bib.bib39); Liu et al., [2019](#bib.bib32); Evci et al., [2020](#bib.bib11)),
    has garnered considerable attention recently due to its significant saving potentials
    in the context of neural network training. Instead of training an entire network,
    DST selectively updates and maintains a subset of the network throughout the training
    process, while allowing the sparse network topology to dynamically evolve via
    a weight operation (Mocanu et al., [2018](#bib.bib39)). Given its demonstrated
    efficacy in achieving efficient training, DST seems to be a promising candidate
    for efficient LLMs fine-tuning. However, it is essential to note that DST intrinsically
    requires the training of subnetworks via backpropagation, and the effectiveness
    of mask adaptation highly relies on a sufficient number of weight updates (Liu
    et al., [2021](#bib.bib33)). Moreover, prior studies have indicated its failure
    when employed for fine-tuning small-scale BERT-level language models (Liu et al.,
    [2023](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, it is noteworthy that the pruning-and-growing step employed in
    DST solely stands as a training-free methodology, enabling sparse mask adaptation
    based on certain weight status, *e.g.,* magnitude (Mocanu et al., [2018](#bib.bib39)).
    This offers an alternative perspective for addressing the aforementioned challenge: While
    fine-tuning sparse LLMs through backpropagation can result in substantial computational
    overhead, we can explore the possibility of iteratively updating sparse mask in
    a training-free fashion as a viable alternative. Based on this intuition, we introduce
    a training-free fine-tuning approach – Dynamic Sparse No Training (DS\faBanT).
    This approach empowers the further refinement of sparse LLMs without any weight
    updates. To facilitate mask adaptation in favor of the sparse reconstruction problem,
    we propose new criteria for mask pruning and growing, by considering both the
    expectation and variance of the reconstruction error reduction when recovering
    a specific weight. It is worth emphasizing that the DS\faBanT functions independently
    of the need for computationally intensive operations, such as gradient or Hessian
    matrices. Instead, it exclusively relies on a singular matrix multiplication operation
    to assess the reconstruction error.
  prefs: []
  type: TYPE_NORMAL
- en: We conduct comprehensive experiments to evaluate the effectiveness of DS\faBanT
    with a variety of LLMs, including LLaMa-V1 (Touvron et al., [2023a](#bib.bib49))
    and LLaMa-V2 (Zhang et al., [2022a](#bib.bib63)), Vicuna (Chiang et al., [2023](#bib.bib4)),
    and OPT families (Zhang et al., [2022a](#bib.bib63)), from 7 billion to 70 billion
    parameters. Our results demonstrate that DS\faBanT consistently improves the performance
    of sparse LLMs by a good margin, especially at high sparsity levels <math id="S1.p6.1.m1.1"
    class="ltx_Math" alttext="></math> 50%. For instance, DS\faBanT is able to improve
    the performance over Magnitude pruning, SparseGPT, and Wanda by 1.1e6, 4.31, and
    1.87 perplexity with OPT-13B on WikiText-2 at 60% sparsity only using 7.3s on
    a single NVIDIA A100 GPU. Our work provides fresh insights in efficient sparse
    LLM fine-tune without weight updates and we hope to encourage more research in
    exploring benefits of sparsity in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Network Sparsification. The process of eliminating redundant weights, known
    as network sparsification or network pruning, has served as a practical strategy
    to diminish the complexity of deep neural networks over the past decades (LeCun
    et al., [1989](#bib.bib28); Han et al., [2015](#bib.bib18)). Despite the substantial
    body of literature, network pruning can be roughly classified based on the granularity
    of sparsity and the dependency of the pre-trained dense models. I. Granularity
    of Sparsity: The granularity of sparsity varies from coarse grains to fine grains.
    The coarse-grained granularity can be a group of weights (Gray et al., [2017](#bib.bib17);
    Ding et al., [2017](#bib.bib10)), a complete neuron (Jiang et al., [2018](#bib.bib26));
    a filters/channels (Li et al., [2017](#bib.bib30)), or an attention head (Voita
    et al., [2019](#bib.bib51)), *etc*. On the other hand, fine-grained granularity
    eliminates the least important weights based on the selected criteria, regardless
    of where they are  (Gale et al., [2019](#bib.bib15)). The advantage of coarse-grained
    sparsity is its pronounced acceleration effect, which yet typically suffers from
    larger performance loss. Fine-grained sparsity enjoys performance superiority
    compared to other more structured forms of sparsity but receives limited support
    in common hardware. Nonetheless, recent advancements of dedicated fine-grained
    sparse patterns, such as N:M sparsity (Zhou et al., [2021](#bib.bib66); Zhang
    et al., [2022b](#bib.bib64)), can be effectively accelerated. As such, this paper
    focuses on fine-grained network pruning. II. Dependency of Pre-trained Networks:
    In parallel, sparsification techniques can be grouped into dense-to-sparse, and
    sparse-to-sparse methods based on the necessity of an over-parameterized dense
    network. The former entails embarking from a pre-trained dense model and discovering
    a sparse network (Han et al., [2015](#bib.bib18); Wen et al., [2016](#bib.bib55);
    Molchanov et al., [2017](#bib.bib40); Gale et al., [2019](#bib.bib15); Kurtic
    et al., [2022](#bib.bib27)), usually followed by a retraining process to recover
    the optimal accuracy. On the other hand, sparse-to-sparse methods aim to train
    sparse neural networks from scratch, omitting any preliminary steps involving
    dense pre-training (Mocanu et al., [2018](#bib.bib39); Lee et al., [2019](#bib.bib29);
    Evci et al., [2020](#bib.bib11); Wang et al., [2020](#bib.bib52); Liu et al.,
    [2021](#bib.bib33)). Among them, Dynamic Sparse Training (DST) (Mocanu et al.,
    [2018](#bib.bib39); Evci et al., [2020](#bib.bib11); Liu et al., [2021](#bib.bib33))
    stands out and receives upsurging interest due to its promise in saving both training
    and inference phases. In contrast to the conventional practices of pre-training
    followed by pruning, DST distinguishes itself by commencing with a randomly initialized
    sparse neural network. During a single training run, it dynamically adjusts the
    sparse network topology by such as pruning-and-growing, without the need for pre-training,
    while maintaining moderate training costs by, for example, keeping the similar
    sparsity ratios across all varying masks (Mostafa & Wang, [2019](#bib.bib41);
    Dettmers & Zettlemoyer, [2019](#bib.bib6); Yuan et al., [2021](#bib.bib61); Jayakumar
    et al., [2020](#bib.bib25)).'
  prefs: []
  type: TYPE_NORMAL
- en: While the crux of this paper focuses on the first category, *i.e.*, pruning
    a pre-trained LLM model, our proposed method is mainly inspired by the pruning-and-growing
    utilized in DST to iteratively refine the binary masks in a training-free manner,
    even though we do not conduct weight training as such. Another line of research,
    akin to our approach, demonstrates the existence of “supermasks” within randomly
    initialized network (Zhou et al., [2019](#bib.bib67); Ramanujan et al., [2020](#bib.bib46);
    Huang et al., [2022](#bib.bib22)) or pre-trained networks (Mallya et al., [2018](#bib.bib36);
    Wortsman et al., [2020](#bib.bib57); Zhang et al., [2023](#bib.bib65)), exhibiting
    the capacity to achieve commendable performance solely by seeking binary masks.
    However, it is imperative to note that these methods heavily rely on backpropagation,
    which is ill-suited for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning of LLMs. Compared to the well-established promise of pruning in pre-LLM
    small-scale models, the advancement of pruning in the context of LLMs appears
    to exhibit relatively modest progress. Firstly, traditional pruning generally
    requires at least one iteration of re-training to recover performance. Considering
    the substantial model size and massive datasets associated with LLMs, the prospect
    of conducting such resource-intensive re-training becomes a formidable challenge.
    To mitigate the above challenge, researchers have introduced pruning algorithms
    specifically devised for LLMs compression. Ma et al. ([2023](#bib.bib35)) explored
    structured sparse LLM by applying Taylor pruning (Molchanov et al., [2017](#bib.bib40))
    to remove entire weight rows, followed by the parameter efficient fine-tuning
    (PEFT) technique (Hu et al., [2021](#bib.bib21)) fine-tuning. However, the fine-tuning
    phase still demands a considerable amount of data while the performance suffers
    a significant degradation, attributed primarily to the coarse-grained level of
    sparsity. Recent research endeavours have evolved towards the direction of unstructured
    pruning in one-shot without fine-tuning, demonstrating significant progresses.
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) incorporates the Hessian inverse
    for pruning and subsequent residual weight updates, whereas Wanda (Sun et al.,
    [2023](#bib.bib48)) directly arrives at a sparse LLM model by a criterion depicted
    by the multiplication of the absolute values of weights and their activations
    with the aim to preserve outliers (Dettmers et al., [2022](#bib.bib8)) emerged
    in LLMs. DS\faBanT serves as an orthogonal perspective and can be organically
    integrated on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Dynamic Sparse No Training – DS\faBanT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Preliminary. LLM pruning entails the removal of a certain proportion of pre-trained
    weights to obtain a sparse LLM, with the objective of achieving minimal discrepancy
    between the output of the sparse and dense models (Hassibi et al., [1993](#bib.bib20)).
    Solving this problem can be very arduous given the immense scale of LLMs. Therefore,
    it is more practical to formalize LLM pruning as a layer-wise reconstruction problem (Hubara
    et al., [2021](#bib.bib23); Frantar & Alistarh, [2023](#bib.bib13)). Denote the
    weights of one dense LLM layer as $\mathbf{W}\in\mathbb{R}^{C_{\text{out}},C_{\text{in}}}$
    and $C_{\text{in}}$ calibration samples, the input activation can be represented
    as $\mathbf{A}\in\mathbb{R}^{C_{\text{in}},N\times L}$ be the sequence length.
    Pruning can be viewed as devising a binary mask $\mathbf{M}\in\{0,1\}^{C_{\text{out}},C_{\text{in}}}$
    can be formalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\mathbf{M},\mathbf{W}}\;&#124;&#124;\underbrace{(\mathbf{M}\odot\mathbf{W})*\mathbf{A}-\mathbf{W}*\mathbf{A}}_{\Delta}&#124;&#124;_{2},\;\;\emph{s.t.}\;\;1-\frac{\left\&#124;\mathbf{M}\right\&#124;_{0}}{C_{\text{out}}\cdot
    C_{\text{in}}}=p,$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $*$, $||\cdot||_{2}$ norm, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6412238c8beb43aee2a09aa292c88e70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Framework of DS\faBanT.'
  prefs: []
  type: TYPE_NORMAL
- en: Note we refer $\Delta\in\mathbb{R}^{C_{out},N\cdot L}$ as to the reconstruction
    error for ease of the following text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic Sparse No Training. The problem defined in Eq. ([1](#S3.E1 "In 3 Dynamic
    Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free
    Fine-tuning for Sparse LLMs")) can be addressed from two complementary perspectives.
    Firstly, it can be resolved through the initialization of sparse networks *i.e.*,
    devising criteria to prune weights that exhibit minimal impact on model output.
    For instance, SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) employs second-order
    Hessian inverses, while Wanda (Sun et al., [2023](#bib.bib48)) considers products
    of weight and activation norm as the guide for weight removal. Secondly, for the
    obtained sparse networks, the remaining weights can be naturally fine-tuned to
    further compensate for the reconstruction error (Han et al., [2015](#bib.bib18)).
    Unfortunately, this requires substantial training resources, which is not practical
    given the large volumes of LLMs. Therefore, SparseGPT adjusts the remaining weights
    via an iterative OBS update (Hassibi & Stork, [1992](#bib.bib19)), which as a
    consequence remarkably reduces the computing demands.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, our focus is on the second part, *i.e.*, how to efficiently reduce
    the reconstruction error of a given pruned sparse network to its dense counterpart?
    Instead of fully fine-tuning (Han et al., [2015](#bib.bib18)) or partially updating
    the pruned LLMs (Frantar & Alistarh, [2023](#bib.bib13)) to recover performance,
    we introduce an ultra-efficient yet effective alternative to refine the sparse
    mask after pruning based on their contribution to the reconstruction error. Our
    approach is inspired by the pruning-and-growing operation used in Dynamic Sparse
    Training (Mocanu et al., [2018](#bib.bib39); Evci et al., [2020](#bib.bib11)).
    DST incorporates the processes of weight pruning and weight growing within the
    framework of sparse network training, contributing to the discovery of improved
    sparse topologies. Note that this pruning-and-growing operation solely serves
    as a training-free approach that is able to adapt sparse masks towards a desirable
    perspective, *e.g.,* loss minimization. Based on this insight, we propose DS\faBanT,
    a training-free fine-tuning method for sparse LLMs that strips weights updating
    in DST and keeps the pruning-and-growing by converting the optimization objective
    to the reconstruction error of each weight row. We isolate pruning-and-growing
    from network training, and formulate it as an iterative approach to progressively
    optimize sparse masks towards the desirable ones achieving minimal reconstruction
    error represented by Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT
    ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: A sparse layer with weight $\mathbf{W}\odot$, update threshold $\epsilon$
    via Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs"))       for *$r=1$* do            
    for *$t=1$* do                   Obtain the growing index $i$ via Eq. ([3](#S3.E3
    "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan:
    Training-Free Fine-tuning for Sparse LLMs")).                   $\mathbf{M}_{r,i}=1$                  
    Update reconstruction error $\Delta_{r}$* then                         break      return
    Fine-tuned sparse weights $\mathbf{W}\odot\mathbf{M}$.* *Algorithm 1 Pseudocode
    of DS\faBanT.*  *Specifically, DS\faBanT starts with a sparse LLM which can be
    pruned by any existing criteria (Jaiswal et al., [2023](#bib.bib24); Sun et al.,
    [2023](#bib.bib48); Frantar & Alistarh, [2023](#bib.bib13)). Then, it performs
    iterative weight growing and pruning by looking at the reconstruction error as
    defined in Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")), with
    especially-designed criteria to decrease the output discrepancy between sparse
    LLMs and their dense counterparts. The framework of DS\faBanT is illustrated in
    Figure [2](#S3.F2 "Figure 2 ‣ 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs") and its
    main parts are detailedly described below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Growing Criterion. As each output neuron is computed independently, we use
    one weight row $\mathbf{W}_{r}$ for illustration. Given sparse weight row $\mathbf{M}_{r}\odot\mathbf{W}_{r}$
    across different input activations. Therefore, our growing criterion considers
    both the expectation and variance of the reconstruction error change when recovering
    a weight back. In particular, the index $i$ of the revived weights is derived
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{E}(\cdot)$ stand for the expectation and variance of given inputs
    across $N\times L$ represents the expected influence of weight growing on $\Delta_{r}$,
    we can determine which weight should be restored to approach the decrease of $\Delta_{r}$
    exhibits high variance across different inputs, restoring it may not result in
    stable error reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pruning Criterion. After choosing revived weights, we need to select another
    weight for pruning in order to maintain a fixed sparsity rate. However, the circumstances
    here are distinct: if we prune weights based on the impact of reconstruction error
    change as per Eq. ([2](#S3.E2 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")), there
    is a risk of removing weights that significantly influence the output. This concern
    becomes especially critical when pruning LLMs due to the presence of emergent
    large magnitude features within them (Dettmers et al., [2022](#bib.bib8); Wei
    et al., [2022a](#bib.bib53); Schaeffer et al., [2023](#bib.bib47)). To alleviate
    this, we utilize a transformed version of the Wanda metric (Sun et al., [2023](#bib.bib48)).
    In addition to its standard criterion for pruning weights, we mandate that the
    selected weights should also contribute positively towards the reduction of reconstruction
    error when being pruned. This helps in preserving critical weights from removal
    without compromising the stable decrease of reconstruction error during the training-free
    fine-tuning process. Therefore, the pruning index $j$ is obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Workflow. Given the criteria depicted above, the workflow of DS\faBanT is outlined
    in Algorithm [1](#alg1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs"). In particular,
    it iteratively performs weight growing and pruning with respect to Eq. ([2](#S3.E2
    "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan:
    Training-Free Fine-tuning for Sparse LLMs")) and Eq. ([3](#S3.E3 "In 3 Dynamic
    Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free
    Fine-tuning for Sparse LLMs")), with the reconstruction error updated until it
    reaches a pre-defined threshold. Meanwhile, we set a maximum pruning-and-growing
    cycle $T$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark. It’s noteworthy that Algorithm,[1](#alg1 "In 3 Dynamic Sparse No Training
    – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for
    Sparse LLMs") outlines the processing of each row in a sequential manner, primarily
    for the sake of simplicity. However, it’s imperative to acknowledge that each
    row can, in fact, undergo parallel processing by employing a binary indicator
    to assess whether a particular row has satisfied the termination condition. Furthermore,
    the DS\faBanT process eliminates the necessity for resource-intensive procedures
    such as backpropagation or the computation of gradient and Hessian matrices. Instead,
    it relies solely on several matrix multiplications to calculate the reconstruction
    error, a task that can be executed efficiently on GPUs. Subsequently, during each
    iteration of the DS\faBanT process, the only operation is to update the reconstruction
    error through straightforward addition and subtraction operations during the pruning-and-growing
    process. This approach effectively circumvents the introduction of additional
    algorithmic complexity. In summary, DS\faBanT preserves the simplicity associated
    with pruning LLMs, akin to the approaches employed in Wanda and Magnitude pruning.
    It’s important to note that while we share the same optimization objective with
    SparseGPT, our approach adopts a significantly more efficient pruning-and-growing
    operation to minimize the reconstruction error. As demonstrated in the next section,
    this efficiency operation can further improve the performance of SparseGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: WikiText-2 Perplexity comparison for pruning LLMs at 60% sparsity
    rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA-V1 | LLaMA-V2 | Vicuna | OPT |'
  prefs: []
  type: TYPE_TB
- en: '| Method | 7B | 13B | 30B | 65B | 7B | 13B | 70B | 13B | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 5.68 | 5.09 | 4.10 | 3.56 | 5.47 | 4.88 | 3.32 | 5.94 | 10.12 |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | 5.6e2 | 2.3e2 | 15.97 | 8.18 | 6.9e3 | 10.11 | 13.35 | 14.39
    | 1.1e6 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 66.70 | 30.71 | 10.81 | 7.37 | 40.01 | 9.41 | 6.77 | 12.02
    | 2.4e2 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 10.41 | 8.43 | 6.81 | 5.83 | 10.14 | 7.88 | 5.10 | 10.02 | 21.23
    |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 9.65 | 7.73 | 6.69 | 5.64 | 9.67 | 7.57 | 5.07 | 9.38 | 16.92
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 10.69 | 8.75 | 6.56 | 5.90 | 10.79 | 8.40 | 5.25 | 9.54 | 15.88 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 10.22 | 8.46 | 6.44 | 5.75 | 10.59 | 8.18 | 5.20 | 9.18 |
    14.01 |*  *## 4 Experimental Results'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementation details. The implementation details of our proposed DS\faBanT
    are presented as follows, mostly conforming to the existing setups (Frantar &
    Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)). In context to pruning
    configuration, we adhere to SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)),
    where a uniform sparsity is imposed for all layers with the first embedding layer
    and the final classification head skipped. Meanwhile, the calibration data consists
    of 128 segments, each with 2048 tokens. These segments are randomly selected from
    the first shard of the C4 dataset (Raffel et al., [2020](#bib.bib45)). For the
    hyper-parameter settings, we set the maximum cycle $T=50$ in all experiments.
    We implement DS\faBanT in PyTorch (Paszke et al., [2019](#bib.bib44)) and use
    the HuggingFace Transformers library (Wolf et al., [2019](#bib.bib56)) for handling
    models and datasets. All pruning experiments are conducted on NVIDIA A100 GPUs
    with 80GB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines. We principally work with the LLaMA-V1 (Touvron et al., [2023a](#bib.bib49)),
    LLaMA-V2 (Touvron et al., [2023b](#bib.bib50)), Vicuna (Chiang et al., [2023](#bib.bib4)),
    and OPT families (Zhang et al., [2022a](#bib.bib63)), from 7 billion to 70 billion
    parameters, which are among the most powerful and open-source Large Language Models
    (LLMs) in the field today. We run DS\faBanT on sparse LLMs pruned by various methods
    including (1) Magnitude-based pruning (Han et al., [2015](#bib.bib18)) that discards
    weights based on their magnitudes. (2) SparseGPT (Frantar & Alistarh, [2023](#bib.bib13))
    that utilizes second-order Hessian inverses to ascertain unimportant weights.
    (3) Wanda (Sun et al., [2023](#bib.bib48)) that removes weights with the smallest
    magnitudes multiplied by the corresponding input activation norms.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation. In accordance with prior studies (Frantar et al., [2022](#bib.bib14);
    Dettmers et al., [2023](#bib.bib9); Yao et al., [2022](#bib.bib59); Frantar &
    Alistarh, [2023](#bib.bib13)), we assess the performance of pruned models by calculating
    the perplexity of language generation experiments on separate validation sets
    derived from WikiText2 (Merity et al., [2016](#bib.bib37)). While perplexity has
    served as a stable and robust indicator of the generative performance of models (Dettmers
    & Zettlemoyer, [2023](#bib.bib7)), we also examined the zero-shot capabilities
    of pruned models. In detail, we report the accuracy in six zero-shot tasks including
    PIQA (Bisk et al., [2020](#bib.bib1)), StoryCloze (Mostafazadeh et al., [2017](#bib.bib42)),
    ARC Easy and Challenge (Clark et al., [2018](#bib.bib5)), HellaSwag (Zellers et al.,
    [2019](#bib.bib62)) and OpenBookQA (Mihaylov et al., [2018](#bib.bib38)). We implement
    the lm-eval-harness (Gao et al., [2021](#bib.bib16)) for the execution of all
    zero-shot tasks, with the report including both the accuracy results on each benchmark
    and overall average accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Language Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Quantitative results. The results for fine-tuning sparse LLM models at a uniform
    sparsity rate of 60% are presented in Table [1](#S3.T1 "Table 1 ‣ 3 Dynamic Sparse
    No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs"). Irrespective of the datasets used for evaluation, DS\faBanT
    consistently delivers performance improvement for sparse LLMs with their original
    sizes varying from 7B to 70B. For instance, when pruning LLaMA-V1 with 7B parameters,
    DS\faBanT is able to enhance the performance of Magnitude (Jaiswal et al., [2023](#bib.bib24)),
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)), and Wanda (Sun et al., [2023](#bib.bib48))
    by 4.94e2, 0.76, and 0.47 perplexity on the Wikitext-2 validation sets, respectively.
    It is worth noting that, without any weight updating, DS\faBanT consistently demonstrates
    better performance than SparseGPT, which requires expensive second-order Hessian
    inverses to update the sparse model. For larger models, the efficacy of DS\faBanT
    is still hold with performance gain from 13.35 to 6.77 perplexity when fine-tuning
    sparse LLaMA-V2-70B obtained by magnitude pruning (Han et al., [2015](#bib.bib18)).
    These findings suggest DS\faBanT’s versatility, being adaptable to boost the performance
    of sparse LLMs with different parameter budgets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: WikiText-2 perplexity performance of DS\faBanT for fine-tuning sparse
    LLaMA-V1-7B/65B pruned by the Wanda metric at varying sparsity rates.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA-V1-7B | LLaMA-V1-65B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsity | 50% | 60% | 70% | 80% | 90% | 50% | 60% | 70% | 80% | 90% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 7.26 | 10.69 | 88.84 | 4.80e3 | 6.41e5 | 4.57 | 5.90 | 15.24 | 2.06e3
    | 3.21e4 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 7.12 | 10.22 | 62.05 | 4.12e3 | 8.43e4 | 4.54 | 5.75 | 12.93
    | 1.82e3 | 2.09e4 |'
  prefs: []
  type: TYPE_TB
- en: 'Varying Sparsity Rates. We further investigate the efficacy of DS\faBanT when
    fine-tuning sparse LLMs with varying pruning rates. Table [2](#S4.T2 "Table 2
    ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No Training
    \faBan: Training-Free Fine-tuning for Sparse LLMs") shows that DS\faBanT offers
    effective performance enhancement across various pruning methods at different
    sparsity levels. Particularly, this improvement becomes increasingly evident as
    the sparsity level grows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Time overhead (in seconds) for pruning LLaMA-V1 model family.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | 7B | 13B | 30B | 65B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 209 | 337 | 721 | 1285 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 0.3 | 0.5 | 1.1 | 1.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda+DS\faBanT | 4.3 | 7.4 | 15.7 | 23.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparion with LoRA fine-tuning using 50% sparse LLaMA-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Time Cost | Perplexity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda+LoRA | 4h | 6.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda+DS\faBanT | 4.3s | 7.12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Wikitext-2 perplexity comparison for pruning LLaMA-V1 model family
    with N:M pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Sparsity | 7B | 13B | 30B | 65B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | - | 5.68 | 5.09 | 4.10 | 3.56 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 4:8 | 8.61 | 7.40 | 6.17 | 5.38 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 4:8 | 8.32 | 7.05 | 6.10 | 5.12 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 4:8 | 8.57 | 7.40 | 5.97 | 5.30 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 4:8 | 8.45 | 7.25 | 5.91 | 5.26 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 2:4 | 11.00 | 9.11 | 7.16 | 6.28 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 2:4 | 10.03 | 8.36 | 6.82 | 5.80 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 2:4 | 11.53 | 9.58 | 6.90 | 6.25 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 2:4 | 10.89 | 9.05 | 6.76 | 6.14 |'
  prefs: []
  type: TYPE_TB
- en: 'Computing efficiency. We further demonstrate the computing efficiency of DS\faBanT.
    Following Wanda (Sun et al., [2023](#bib.bib48)), we only report the total pruning
    time and exclude the forward pass process shared by all methods. Table [5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs") compares the quantitative
    wall-clock overhead evaluated on NVIDIA A100 GPUs. It is indeed encouraging to
    observe that, as a fine-tuning approach, DS\faBanT maintains a comparable computing
    time to Wanda, while demonstrating significantly higher efficiency compared to
    SparseGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison with LoRA Fine-tuning. To further demonstrate the ultra efficiency
    of DS\faBanT in terms of fine-tuning, we also compare DS\faBanT with parameter
    efficient fine-tuning (PEFT) method LoRA (Hu et al., [2021](#bib.bib21)). Table [5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs") presents a comparison
    of the time and performance of both methods in fine-tuning sparse LLaMA-7B. LoRA
    leverages the complete C4 dataset for a 5-hour fine-tuning and achieved a perplexity
    of 6.84\. In stark contrast, DS\faBanT only requires a brief duration of 4.3 s
    and 128 samples to deliver a comparable performance, 7.12 perplexity. Taking into
    consideration the additional network parameter burden incorporated by LoRA, the
    efficiency and practicality of DS\faBanT is hold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'N:M Fine-grained Sparsity. Compared with unstructured sparsity, N:M fine-grained
    sparsity offers more practical speedup on the NVIDIA Ampere sparse tensor core (Nvidia,
    [2020](#bib.bib43)). Thus, we also evaluate the effectiveness of DS\faBanT on
    N:M fine-grained sparsity. Given the unique pattern of N:M sparsity that stipulates
    N non-zero components within M consecutive weight block, our implementation of
    DS\faBanT involves a restriction on the position of pruning-and-growing weights.
    In particular, we select the pruned weight within the same block as the revived
    weight, thus the N:M characteristic is still maintained after fine-tuning. Table [5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs") lists the results
    for pruning LLaMA-V1 model family at 2:4 and 4:8 sparse patterns. Interestingly,
    even with the aforementioned extra restriction, DS\faBanT can achieve more significant
    performance improvement compared to previous methods. For instance, when pruning
    LLaMA-V1 with 7B parameters, DS\faBanT archives a perplexity of 10.89, enhancing
    Wanda (11.53) by a noticeable 0.64 ppl. Similar findings can be concluded when
    it comes to other models and sparse patterns. These results highlight the effectiveness
    of DS\faBanT in boosting the performance of sparse LLMs, even with more complex
    sparsity constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Zero-shot Accuracy comparison for pruning LLaMA-V1 model family at
    60% sparsity rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Params | Method | PIQA | HellaSwag | StoryCloze | ARC-e | ARC-c | OBQA |
    Mean |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | Dense | 78.7 | 56.9 | 76.8 | 75.3 | 41.8 | 34.0 | 60.6 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 73.1 | 44.8 | 71.5 | 62.6 | 30.2 | 24.4 | 51.1 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 73.7 | 47.2 | 72.3 | 62.8 | 30.9 | 29.4 | 52.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 73.0 | 43.6 | 69.7 | 62.8 | 30.3 | 25.0 | 50.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | w. DS\faBanT | 73.2 | 43.7 | 70.0 | 63.6 | 30.8 | 25.8 | 51.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | Dense | 79.1 | 59.9 | 78.4 | 77.4 | 46.5 | 33.2 | 62.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 75.6 | 49.0 | 74.8 | 68.4 | 36.2 | 27.6 | 55.2 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 75.8 | 51.5 | 75.8 | 69.8 | 36.3 | 28.8 | 56.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 74.9 | 48.9 | 74.5 | 68.9 | 34.9 | 27.6 | 54.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | w. DS\faBanT | 75.0 | 49.1 | 75.1 | 69.2 | 35.4 | 28.0 | 55.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 30B | Dense | 81.1 | 63.3 | 79.1 | 80.4 | 52.9 | 36.0 | 65.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 76.8 | 55.0 | 78.4 | 74.7 | 43.3 | 32.2 | 60.1 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 77.3 | 58.0 | 78.8 | 74.8 | 45.6 | 32.8 | 61.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 77.7 | 56.7 | 79.1 | 76.2 | 46.5 | 31.6 | 61.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | w. DS\faBanT | 78.1 | 56.7 | 79.7 | 76.8 | 46.6 | 32.6 | 61.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 65B | Dense | 81.2 | 64.6 | 80.2 | 81.3 | 52.9 | 38.2 | 66.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 79.6 | 58.3 | 80.5 | 77.4 | 46.6 | 33.4 | 62.6 |'
  prefs: []
  type: TYPE_TB
- en: '| w. DS\faBanT | 79.9 | 59.8 | 80.4 | 78.1 | 46.9 | 34.6 | 63.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 79.9 | 58.9 | 80.6 | 78.2 | 47.1 | 34.8 | 63.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | w. DS\faBanT | 80.9 | 59.6 | 80.2 | 78.2 | 47.7 | 36.0 | 63.7 |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Zero-shot Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following (Frantar & Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)),
    we also provided the accuracy performance of the LLaMA-V1 model family pruned
    at 50% sparsity rate on seven downstream zero-shot tasks. Averaging the accuracy
    over all tasks suggests DS\faBanT’s efficacy for enhancing sparse LLMs of any
    size. Particularly, DS\faBanT improves the average accuracy of SparseGPT by 1.6%
    when pruning LLaMA-V1-7B (52.7% for DS\faBanT and 51.1% for SparseGPT). For task-wise
    performance, DS\faBanT is beneficial on all tasks, while there is not a fixed
    superiority for fine-tuning models obtained by different pruning methods. This
    phenomenon may evidence the reported relatively noisy evaluation results from
    these zero-shot experiments (Dettmers et al., [2022](#bib.bib8)). However, the
    advantages of consistent performance improvement and efficiency of DS\faBanT for
    zero-shot tasks are obvious.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we investigate the influence of the components within DS\faBanT, unfolds
    as its update schedule, pruning-and-growing criteria, and robustness to calibration
    samples. All experimental setups are based on the LLaMA-7B model pruned by the
    Wanda metric (Sun et al., [2023](#bib.bib48)) with 60% sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83fc538bf46d6b044cec5280489f131f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: (left) Effect of the update schedule ($T,\epsilon$) and (right) number
    of calibration sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update schedule. In Figure [3](#S4.F3 "Figure 3 ‣ 4.4 Performance Analysis
    ‣ 4 Experimental Results ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs") (left), we examine the performance of DS\faBanT under different
    hyper-parameter setting for the update schedule, including the maximum cycle $C$.
    The best performance is obtained with 50 cycles and 0.1 updating threshold. To
    analyze, smaller $C$ both lead to an insufficient procedure for the decrease in
    reconstruction error. In contrast, running DS\faBanT without termination conditions
    also resulted in poor performance, most likely due to over-fitting of calibration
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Robustness to calibration samples. In Figure [3](#S4.F3 "Figure 3 ‣ 4.4 Performance
    Analysis ‣ 4 Experimental Results ‣ Dynamic Sparse No Training \faBan: Training-Free
    Fine-tuning for Sparse LLMs") (right), we show the performance of pruning methods
    with varying numbers of sampled sequences for calibration. As can be observed,
    SparseGPT suffers serious performance degradation when calibration samples are
    limited, mostly due to the difficulty in estimating Hessian inverses in such cases.
    Fortunately, DS\faBanT consistently the performance of SparseGPT, even if only
    very few samples are given. These results further highlight the robustness of
    DS\faBanT for mitigating the reconstruction error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Effect of the pruning and growing criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="24.3" width="102.5" overflow="visible"><g transform="translate(0,24.3)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="48.66" height="12.15" overflow="visible">Pruning</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(51.25,12.15)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="51.25" height="12.15" overflow="visible">Growing</foreignobject></g></g></g></svg>
    | $&#124;\mathbf{W}_{r,k}&#124;\cdot&#124;&#124;\mathbf{A}_{r}&#124;&#124;_{2}$
    |  Eq. ([3](#S3.E3 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) |   Eq. ([2](#S3.E2
    "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan:
    Training-Free Fine-tuning for Sparse LLMs")) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $&#124;\mathbf{W}_{r,k}&#124;\cdot&#124;&#124;\mathbf{A}_{r}&#124;&#124;_{2}$
    | 10.72 | 10.49 | 10.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Eq. ([2](#S3.E2 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) | 11.24 | 10.61
    | 10.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Eq. ([3](#S3.E3 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) | 10.52 | 10.37
    | 10.22 |'
  prefs: []
  type: TYPE_TB
- en: 'Pruning-and-growing criteria. We further investigate the influence on criteria
    for prune and grow in Table [7](#S4.T7 "Table 7 ‣ 4.4 Performance Analysis ‣ 4
    Experimental Results ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs"). Note that when we transfer Eq. ([2](#S3.E2 "In 3 Dynamic Sparse
    No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs")) to the prune criteria, the election of extreme values is also
    correspondingly reversed. As for the prune criterion, it can be seen that pruning
    weights that could bring the most reduction in reconstruction error actually led
    to a significant performance decrease. This indicates that while pursuing the
    reduction of reconstruction error, it is also essential to keep weights that exhibit
    an extremely large influence on the output, *e.g.*, weights within outlier channel.
    On the other hand, our proposed criteria based on the expectation and variance
    of the reconstruction error reduction achieved the best results among all growing
    criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we introduce DS\faBanT, a training-free fine-tuning approach that
    enhances the performance of sparse LLMs without the expensive backpropagation
    or any weight updates. Taking inspiration from the success of sparse training
    in the pre-LLM pruning age, DS\faBanT adapts iterative weights growing and pruning
    in a sparse LLM, with a transferred target for minimizing the reconstruction error
    between dense and sparse LLMs outputs. To furnish guidance in the selection of
    weights to be pruned and grown, we introduce novel criteria that take into account
    the expectation and variance of the reconstruction error reduction by growing
    each weight concerning different inputs. Extensive experiments on pruning representative
    LLMs across various language benchmarks demonstrate the efficiency and effectiveness
    of DS\faBanT in boosting the performance of sparse LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by National Key R&D Program of China (No.2022ZD0118202),
    the National Science Fund for Distinguished Young Scholars (No.62025603), the
    National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No.
    62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389,
    No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province
    of China (No.2021J01002, No.2022J06001).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence (AAAI)*, volume 34, pp.  7432–7439,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems (NeurIPs)*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers & Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. Sparse networks
    from scratch: Faster training without losing performance. *arXiv preprint arXiv:1907.04840*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers & Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. The case for
    4-bit precision: k-bit inference scaling laws. In *International Conference on
    Machine Learning (ICML)*, pp.  7750–7774\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems (NeurIPs)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *arXiv preprint arXiv:2306.03078*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2017) Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei
    Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, et al. Circnn: accelerating and
    compressing deep neural networks using block-circulant weight matrices. In *Proceedings
    of the 50th Annual IEEE/ACM International Symposium on Microarchitecture*, pp. 
    395–408, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning (ICML)*, pp.  2943–2952, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *International Conference
    on Learning Representations (ICLR)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Massive language models
    can be accurately pruned in one-shot. In *International Conference on Machine
    Learning (ICML)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training compression for generative pretrained transformers.
    In *International Conference on Learning Representations (ICLR)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gray et al. (2017) Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels
    for block-sparse weights. *arXiv preprint arXiv:1711.09224*, 3:2, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. Learning
    both weights and connections for efficient neural network. In *Advances in Neural
    Information Processing Systems (NeurIPS)*, pp.  1135–1143, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassibi & Stork (1992) Babak Hassibi and David Stork. Second order derivatives
    for network pruning: Optimal brain surgeon. In *Advances in Neural Information
    Processing Systems (NeurIPS)*, pp.  164–171, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal
    brain surgeon and general network pruning. In *IEEE international conference on
    neural networks*, pp.  293–299\. IEEE, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022) Tianjin Huang, Tianlong Chen, Meng Fang, Vlado Menkovski,
    Jiaxu Zhao, Lu Yin, Yulong Pei, Decebal Constantin Mocanu, Zhangyang Wang, Mykola
    Pechenizkiy, et al. You can have better graph neural networks by not training
    weights at all: Finding untrained gnns tickets. *arXiv preprint arXiv:2211.15335*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. (2021) Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph
    Naor, and Daniel Soudry. Accelerated sparse neural training: A provable and efficient
    method to find n: m transposable masks. *Advances in Neural Information Processing
    Systems (NeurIPs)*, 34:21099–21111, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jayakumar et al. (2020) Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon
    Osindero, and Erich Elsen. Top-kast: Top-k always sparse training. *Advances in
    Neural Information Processing Systems (NeurIPs)*, 33:20744–20754, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2018) Chunhui Jiang, Guiying Li, Chao Qian, and Ke Tang. Efficient
    dnn neuron pruning by minimizing layer-wise nonlinear reconstruction error. In
    *IJCAI*, volume 2018, pp.  2–2, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. Optimal brain damage.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, pp.  598–605,
    1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip:
    Single-shot network pruning based on connection sensitivity. In *International
    Conference on Learning Representations (ICLR)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. Pruning filters for efficient convnets. In *International Conference on
    Learning Representations (ICLR)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) S Liu, DC Mocanu, ARR Matavalam, Y Pei, and M Pechenizkiy.
    Sparse evolutionary deep learning with over one million artificial neurons on
    commodity hardware. arxiv. *arXiv preprint arXiv:1901.09181*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola
    Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization
    in sparse training. In *International Conference on Machine Learning*, pp.  6989–7000\.
    PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin
    Huang, Ajay Jaiswal, and Zhangyang Wang. Sparsity may cry: Let us fail (current)
    sparse neural networks together! *arXiv preprint arXiv:2303.02141*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mallya et al. (2018) Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback:
    Adapting a single network to multiple tasks by learning to mask weights. In *Proceedings
    of the European conference on computer vision (ECCV)*, pp.  67–82, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
    Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial
    neural networks with adaptive sparse connectivity inspired by network science.
    *Nature Communications*, 9:1–12, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molchanov et al. (2017) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. Pruning convolutional neural networks for resource efficient inference.
    In *International Conference on Learning Representations (ICLR)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mostafa & Wang (2019) Hesham Mostafa and Xin Wang. Parameter efficient training
    of deep convolutional neural networks by dynamic sparse reparameterization. In
    *International Conference on Machine Learning (ICML)*, pp.  4646–4655, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Michael Roth, Annie Louis,
    Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze
    test. In *Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
    and Discourse-level Semantics*, pp.  46–51, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvidia (2020) Nvidia. Nvidia a100 tensor core gpu architecture, 2020. [https://www.nvidia.com/content/dam/enzz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf](https://www.nvidia.com/content/dam/enzz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, pp.  8026–8037,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramanujan et al. (2020) Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi,
    Ali Farhadi, and Mohammad Rastegari. What’s hidden in a randomly weighted neural
    network? In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*,
    pp.  11893–11902, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaeffer et al. (2023) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are
    emergent abilities of large language models a mirage? *arXiv preprint arXiv:2304.15004*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. *arXiv preprint arXiv:1905.09418*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning
    tickets before training by preserving gradient flow. In *International Conference
    on Learning Representations (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems (NeurIPs)*, 35:24824–24837, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016) Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
    Learning structured sparsity in deep neural networks. *Advances in Neural Information
    Processing Systems (NeurIPs)*, 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wortsman et al. (2020) Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha
    Kembhavi, Mohammad Rastegari, Jason Yosinski, and Ali Farhadi. Supermasks in superposition.
    *Advances in Neural Information Processing Systems (NeurIPs)*, 33:15173–15184,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning (ICML)*,
    pp.  38087–38099\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems (NeurIPs)*, 35:27168–27183, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023) Lu Yin, Shiwei Liu, Meng Fang, Tianjin Huang, Vlado Menkovski,
    and Mykola Pechenizkiy. Lottery pools: Winning more by interpolating tickets without
    increasing training or inference cost. In *Proceedings of the AAAI Conference
    on Artificial Intelligence (AAAI)*, volume 37, pp.  10945–10953, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2021) Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong,
    Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate
    and fast memory-economic sparse training framework on the edge. *Advances in Neural
    Information Processing Systems (NeurIPs)*, 34:20838–20850, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022b) Yuxin Zhang, Mingbao Lin, Zhihang Lin, Yiting Luo, Ke Li,
    Fei Chao, Yongjian Wu, and Rongrong Ji. Learning best combination for efficient
    n: M sparsity. In *Advances in Neural Information Processing Systems (NeurIPS)*,
    2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Yuxin Zhang, Mingbao Lin, Fei Chao, Yan Wang, Ke Li, Yunhang
    Shen, Yongjian Wu, and Rongrong Ji. Lottery jackpots exist in pre-trained models.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: M fine-grained structured
    sparse neural networks from scratch. In *International Conference on Learning
    Representations (ICLR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
    Deconstructing lottery tickets: zeros, signs, and the supermask. In *Advances
    in Neural Information Processing Systems (NeurIPS)*, pp.  3597–3607, 2019.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
