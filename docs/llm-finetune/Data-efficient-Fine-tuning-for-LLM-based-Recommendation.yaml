- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Data-efficient Fine-tuning for LLM-based Recommendation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17197](https://ar5iv.labs.arxiv.org/html/2401.17197)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  prefs: []
  type: TYPE_NORMAL
- en: Xinyu Lin¹, Wenjie Wang¹, Yongqi Li², Shuo Yang³, Fuli Feng⁴, Yinwei Wei⁵, and
    Tat-Seng Chua¹ ¹National University of Singapore, ²The Hong Kong Polytechnic University,
    ³University of Technology Sydney, ⁴University of Science and Technology of China,
    ⁵Monash University  [xylin1028, wenjiewang96, liyongqi0@gmail.com, shuo.yang@student.uts.edu.au](mailto:xylin1028,%20wenjiewang96,%20liyongqi0@gmail.com,%20shuo.yang@student.uts.edu.au)
    [fulifeng93@gmail.com, weiyinwei@hotmail.com, dcscts@nus.edu.sg](mailto:fulifeng93@gmail.com,%20weiyinwei@hotmail.com,%20dcscts@nus.edu.sg)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Leveraging Large Language Models (LLMs) for recommendation has recently garnered
    considerable attention, where fine-tuning plays a key role in LLMs’ adaptation.
    However, the cost of fine-tuning LLMs on rapidly expanding recommendation data
    limits their practical application. To address this challenge, few-shot fine-tuning
    offers a promising approach to quickly adapt LLMs to new recommendation data.
    We propose the task of data pruning for efficient LLM-based recommendation, aimed
    at identifying representative samples tailored for LLMs’ few-shot fine-tuning.
    While coreset selection is closely related to the proposed task, existing coreset
    selection methods often rely on suboptimal heuristic metrics or entail costly
    optimization on large-scale recommendation data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle these issues, we introduce two primary objectives for the data pruning
    task in the context of LLM-based recommendation: 1) high accuracy aims to identify
    the influential samples that can lead to high overall performance; and 2) high
    efficiency underlines the low costs of the data pruning process. To pursue the
    two objectives, we propose a novel data pruning method incorporating two scores,
    namely influence score and effort score, to efficiently identify the influential
    samples. Particularly, the influence score is introduced to accurately estimate
    the influence of removing each sample on the overall performance. To achieve low
    costs of the data pruning process, we employ a small-sized surrogate model to
    replace LLMs to obtain the influence score. Considering the potential gap between
    the surrogate model and LLMs, we further propose an effort score to prioritize
    some hard samples specifically for LLMs. We instantiate the proposed method on
    two competitive LLM-based recommender models, and empirical results on three real-world
    datasets validate the effectiveness of our proposed method. In particular, the
    proposed method uses only 2% samples to surpass the full data fine-tuning, reducing
    time costs by 97%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Pruning, LLM-based Recommendation, Efficient Fine-tuning^†^†ccs: Information
    systems Recommender systems'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Leveraging Large Language Models (LLMs) for recommendation has demonstrated
    promising efficacy across various tasks, including Click-Through Rate (CTR) prediction (Bao
    et al., [2023b](#bib.bib5)), sequential recommendation (Rajput et al., [2023](#bib.bib41)),
    and explainable recommendation (Gao et al., [2023](#bib.bib15)). To build LLM-based
    recommender models, it is crucial to fine-tune LLMs on recommendation data for
    two primary reasons: 1) there exists a significant gap between previous LLMs’
    tuning tasks and the recommendation tasks (Bao et al., [2023b](#bib.bib5)), and
    2) the rapid and continuous update of recommendation data necessitates frequent
    fine-tuning of LLMs (Sachdeva et al., [2022](#bib.bib42)). For example, there
    are approximately 160 million new videos and 942 billion interactions emerging
    on TikTok per day¹¹1https://www.tiktok.com/transparency/.. Thus, frequent fine-tuning
    is imperative to incorporate up-to-date item information and enhance user behavior
    comprehension. However, fine-tuning LLMs on large-scale recommendation data demands
    substantial computational resources and time costs (Li et al., [2023b](#bib.bib31)),
    thereby diminishing the practicality of LLM-based recommender models in real-world
    applications. As such, it is essential to enhance the fine-tuning efficiency of
    LLM-based recommender models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the rich world knowledge encoded in LLMs offers a promising solution
    for efficient fine-tuning: few-shot fine-tuning. Previous studies have uncovered
    that LLMs have the potential to quickly adapt to recommendation tasks by fine-tuning
    on randomly sampled few-shot data (Bao et al., [2023b](#bib.bib5), [a](#bib.bib4);
    Lin et al., [2023](#bib.bib33)) (Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")(a)), significantly
    reducing training time and computational costs. Despite its efficiency, randomly
    sampled data may lack sufficient representativeness to enable LLMs to effectively
    comprehend new items and user behaviors. To combat this issue, we introduce the
    task of data pruning for efficient LLM-based recommendation, which aims to identify
    representative samples tailored for LLMs’ few-shot fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A closely related literature to this data pruning task is coreset selection (Guo
    et al., [2022](#bib.bib17)). It tries to select a small but representative subset
    from the full data, aiming to achieve comparable performance. Existing coreset
    selection methods generally fall into two categories²²2More detailed related work
    is discussed and compared in Section [4](#S4 "4\. Experiment ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation") and [5](#S5 "5\. Related Work ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation").: 1) Heuristic methods select hard
    or diverse samples based on pre-defined metrics (Paul et al., [2021](#bib.bib40);
    Wu et al., [2023b](#bib.bib54); Luo et al., [2023](#bib.bib37)). Such heuristic
    methods do not estimate the impact of selected samples on empirical risk, possibly
    leading to suboptimal coreset selection. 2) Optimization-based methods mainly
    optimize the selection of subsets to minimize the empirical risk (Borsos et al.,
    [2020](#bib.bib6); Yang et al., [2023b](#bib.bib56)). However, these methods are
    inapplicable to large-scale recommendation datasets due to the complex and costly
    bi-level or discrete optimization problem (He et al., [2023](#bib.bib21)). Worse
    still, both heuristic and optimization-based methods rely on the model well-trained
    by the full data to select the coreset, *e.g.,* calculating pre-defined scores
    or optimizing the data subset based on the well-trained model (*cf.* Section [2](#S2
    "2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    As such, it is infeasible to directly apply these methods for LLM-based recommendation
    because of the high training costs of LLMs on the large-scale full recommendation
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7af1145458995dae8de4a72fa335b713.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. (a) reveals that BIGRec achieves remarkable performance with only
    hundreds of samples. (b) shows the low costs of surrogate models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the above issues, we summarize two principal objectives for data
    pruning in the context of LLM-based recommendation: 1) high accuracy, which focuses
    on selecting the samples that can lead to low empirical risk; and 2) high efficiency,
    which emphasizes the low costs of the data pruning process, *i.e.,* eliminating
    the dependency of well-trained LLMs on the full data. Nevertheless, pursuing the
    two objectives faces two challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve high accuracy, it is essential to measure the influence of removing
    each training sample on the empirical risk. However, assessing the influence of
    all samples is costly, as it requires the leaving-one-out retraining for each
    sample (Tan et al., [2023](#bib.bib47)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve high efficiency, one possible solution is to train a surrogate model
    for sample selection, *e.g.,* using a small-sized traditional recommender model,
    which can drastically reduce the GPU memory usage and the training time compared
    to LLMs (see Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")(b)). However, there exists a gap between LLMs and
    surrogate models, attributable to their divergent capabilities in learning user
    behaviors (refer to Figure [5](#footnote5 "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence
    Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    As such, influential samples selected by surrogate models might deviate from the
    ones on LLMs, potentially hurting the adaptation of LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To address the challenges, we propose a novel Data pruning method, to Efficiently
    identify the influentiAl samples for LLM-based Recommender fine-tuning (shorted
    as DEALRec). DEALRec leverages two scores, namely influence score and effort score,
    to identify the influential samples. The influence score is formulated to estimate
    the influence of removing each training sample on the empirical risk. It is calculated
    by extending the influence function (Hampel, [1974](#bib.bib19)) via chain rules
    and second-order optimization techniques (Koh and Liang, [2017](#bib.bib29)).
    To efficiently calculate the influence score for all samples, DEALRec employs
    a simple yet effective symmetric property to accelerate the calculation, requiring
    only the estimation once for all samples (*cf.* Section [3.1](#S3.SS1 "3.1\. Influence
    Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    Thereafter, DEALRec uses a traditional recommender model as a surrogate model
    to obtain the influence score and introduces the effort score to mitigate the
    gap between the surrogate model and LLMs. The effort score is obtained by calculating
    the gradient norm of a sample loss *w.r.t.* the parameters of LLMs, intuitively
    measuring the effort of LLMs to fit a specific sample. By regularizing the influence
    score with the effort score, DEALRec identifies the influential samples that encompass
    both the representativeness of the full data and the significance to LLMs. We
    instantiate DEALRec on two LLM-based recommender models and conduct extensive
    experiments on three real-world datasets, validating the superiority of DEALRec
    in terms of both efficiency and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, this work offers three major contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a data pruning task to identify the influential samples tailored
    for efficient LLM-based recommender fine-tuning, unlocking the remarkable potential
    of applying LLM-based recommender models to real-world platforms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel data pruning method to discover the influential samples for
    LLM-based recommendation, which effectively and efficiently assesses the influence
    of removing a sample on empirical risk.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive experiments on three real-world datasets, demonstrating
    the effectiveness of DEALRec in achieving both high efficiency and accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Task Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce LLM-based recommender models and uncover
    the challenge of real-world applicability. Thereafter, we formulate the task of
    data pruning for LLM-based recommendation and compare the related work on coreset
    selection.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$ and $\mathcal{I}$, where $x=[i_{1},i_{2},\dots,i_{|x|}]$ is
    the next interacted item of the user³³3Our main focus lies in sequential recommendation,
    which holds notable practical significance by intricately considering the temporal
    aspect in real-world scenarios., where $\{i_{1},\dots,i_{|x|},y\}\subset\mathcal{I}$,
    the target is to fine-tune an LLM for recommendation tasks. The learnable parameters
    ($\phi\in\Phi$ conditioned on input $x$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $y_{t}$-th token of $y$ represents the token sequence preceding $y_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: While fine-tuning LLMs has demonstrated effectiveness in recommendation tasks (Liu
    et al., [2024](#bib.bib36)), its practical application is hindered by the high
    resource costs required by LLMs and the continuous influx of new recommendation
    data (Sachdeva et al., [2022](#bib.bib42)). Hence, it is essential to enhance
    the efficiency of LLM-based recommender fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet\quad$, the target of data pruning is to select a subset $\mathcal{S}\subset\mathcal{D}$
    can yield good performance on the testing set. The size of $\mathcal{S}$, *i.e.,*
    $|\mathcal{S}|=r|\mathcal{D}|$.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$Retrospect of coreset selection. As the closely related work
    to this data pruning task, coreset selection methods generally fall into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heuristic methods (Coleman et al., [2020](#bib.bib9); Toneva et al., [2018](#bib.bib48);
    Feldman and Zhang, [2020](#bib.bib13)) typically design some heuristic strategies
    to select samples based on an empirical minimizer:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle\mathcal{S}=H(\hat{\theta},\mathcal{D}),\quad\text{s.t.}\quad\hat{\theta}=\mathop{\arg\min}_{\theta\in\Theta}\mathcal{L}(\theta,\mathcal{D}),$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\mathcal{L}(\cdot)$ denotes the heuristic strategy such as selecting
    samples with larger prediction entropy (Coleman et al., [2020](#bib.bib9)), or
    clustering the samples based on the sample representations (Chai et al., [2023](#bib.bib7)).
    However, this group of methods designs the strategy $H(\cdot)$ intuitively and
    fails to explicitly consider the influence of a sample on the empirical risk.
    This might lead to suboptimal selection, thereby declining the performance of
    the model trained by the selected subset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimization-based methods (Borsos et al., [2020](#bib.bib6); Killamsetty et al.,
    [2021c](#bib.bib28), [b](#bib.bib27); Wu et al., [2023a](#bib.bib53)) mainly utilize
    bi-level optimization techniques to learn the best subset chosen for training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (3) |  | $\small\mathcal{S^{*}}=\mathop{\arg\min}_{\mathcal{S}\subset\mathcal{D}}\mathcal{L}(\hat{\theta},\mathcal{D}),\quad\text{s.t.}\quad\hat{\theta}=\mathop{\arg\min}_{\theta\in\Theta}\mathcal{L}(\theta,\mathcal{S}).$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Besides, there is also some work that employs discrete optimization problems
    based on the empirical minimizer $\hat{\theta}$ in Eq. ([2](#S2.E2 "In item 1)
    ‣ 2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    Nevertheless, they struggle to be applied to large-scale datasets *e.g.,* recommendation
    data, due to the complex solving of the optimization problem (He et al., [2023](#bib.bib21)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Furthermore, as shown in Eq. ([2](#S2.E2 "In item 1) ‣ 2\. Task Formulation
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")-[3](#S2.E3 "In item
    2) ‣ 2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based Recommendation")),
    previous coreset selection methods usually require the model to be trained over
    original training samples $\mathcal{D}$, which however is infeasible for LLM-based
    recommender models due to the continuous influx of data and the high resource
    costs of LLMs (*cf.* Section [1](#S1 "1\. Introduction ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")).
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$ Drawing upon the above insights, we consider two objectives
    for data pruning: 1) high accuracy emphasizes the low empirical risk of the model
    trained on the selected samples, and 2) high efficiency focuses on the low costs
    of the data pruning process, breaking free from the heavy fine-tuning of LLMs
    for data pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DEALRec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To pursue efficient LLM-based recommendation, we propose a novel data pruning
    method DEALRec, which involves two key components, *i.e.,* the influence score
    to estimate the influence on empirical risk, and the effort score as a regularization
    to mitigate the gap between surrogate model and LLMs. The overview of our method
    is presented in Figure [2](#S3.F2 "Figure 2 ‣ 3.1\. Influence Score ‣ 3\. DEALRec
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Influence Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To achieve good overall performance with the model trained on the pruned dataset
    $\mathcal{S}$ times. To overcome this challenge, we propose an efficient approximation
    of the influence for all samples by extending influence on parameter change (*i.e.,*
    a classic result from influence function (Koh and Liang, [2017](#bib.bib29)))
    via chain rule and second-order optimization techniques. We further utilize the
    symmetric property to speed up the calculation of the influence score.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/583797eaaaa183f8b35f905374e791a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Overview of DEALRec. DEALRec first trains a surrogate model on the
    full training samples. Subsequently, it calculates the influence score, which
    is then regularized by the effort score, to identify influential samples.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$ for training. Considering a training sample $s$, the empirical
    minimizer can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\hat{\theta}_{\epsilon,s}=\mathop{\arg\min}_{\theta\in\Theta}\frac{1}{n}\sum_{s_{i}\in\mathcal{D}}\mathcal{L}(s_{i},\theta)+\epsilon\mathcal{L}(s,\theta).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'According to (Ling, [1984](#bib.bib34)), the influence of upweighting a sample
    $s$ on the parameter change is then given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\mathcal{I}_{\text{param}}(s)=\frac{\mathrm{d}\hat{\theta}_{\epsilon,s}}{\mathrm{d}\epsilon}\bigg{&#124;}_{\epsilon=0}=-H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $H_{\hat{\theta}}=\frac{1}{n}\sum_{s_{i}\in\mathcal{D}}\nabla_{\theta}^{2}\mathcal{L}(s_{i},\hat{\theta})$,
    and $m$ to $\epsilon$ from training. As such, the parameter change of removing
    a training sample $s$ can be linearly approximated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\hat{\theta}_{-s}-\hat{\theta}\approx-\frac{1}{n}\mathcal{I}_{\text{param}}(s)=\frac{1}{n}H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{\theta}_{-s}=\mathop{\arg\min}_{\theta\in\Theta}\sum\nolimits_{s_{i}\in\mathcal{D},s_{i}\neq
    s}\mathcal{L}(s_{i},\theta)$.
  prefs: []
  type: TYPE_NORMAL
- en: Based on Eq. ([6](#S3.E6 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")), an intuitive approach to assess the
    sample influence for model training is to utilize the L2 norm of a sample’s influence
    on parameter change or an additional discrete optimization problem as proposed
    in (Yang et al., [2023b](#bib.bib56)). Nevertheless, large parameter changes do
    not necessarily lead to performance improvements. Besides, calculating Eq. ([6](#S3.E6
    "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")) for all training samples can be computationally costly (He et al.,
    [2023](#bib.bib21)) and is infeasible for recommendation data. To alleviate the
    issues, we propose an efficient approximation for the influence of removing a
    sample on the empirical risk.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$ by a small $\epsilon$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\displaystyle\mathcal{I}_{\text{upweight,loss}}(s,s^{\prime})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\nabla_{\theta}\mathcal{L}(s^{\prime},\hat{\theta})^{\mathrm{T}}\frac{\mathrm{d}{\hat{\theta}_{\epsilon,s}}}{\mathrm{d}\epsilon}\bigg{&#124;}_{\epsilon=0}\quad\quad\text{(chain
    rule)}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\nabla_{\theta}\mathcal{L}(s^{\prime},\hat{\theta})^{\mathrm{T}}H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Similarly, the influence of removing a training sample $s$ can be linearly
    approximated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: We can then obtain the influence of removing a sample $s$ on the empirical risk
    (*i.e.,* influence score) by
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\displaystyle\mathcal{I}_{\text{remove,loss}}(s,{\mathcal{D}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: However, it is non-trivial to directly obtain $H_{\hat{\theta}}^{-1}$ requires
    $\mathcal{O}(nm^{2}+m^{3})$ training samples and $\theta\in\mathbb{R}^{m}$. This
    results in cumbersome calculation of influence scores for all training samples.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Procedure of HVP Estimation
  prefs: []
  type: TYPE_NORMAL
- en: 1:Original training dataset $\mathcal{D}$, iteration number $T$ for $\forall
    i\in\{1,\dots,n\}$.4:for all $t\in\{1,\dots,T\}$;6:Calculate $\nabla_{\theta}^{2}\mathcal{L}(s_{t})$;7:    $\tilde{H}_{t}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]\leftarrow\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})+$;
    $\triangleright$.10:Unbiased estimation $\tilde{H}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$. The idea of stochastic-based HVP estimation is to iteratively
    obtain an unbiased estimator of $H_{\hat{\theta}}$. Specifically, we omit the
    $\hat{\theta}$ terms in Taylor expansion of $H^{-1}$, which can be further rewritten
    recursively as $H_{j}^{-1}=I+(I-H)H_{j-1}^{-1}$ as $j\rightarrow\infty$ as $v$
    at step $t$ can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\small\tilde{H}_{t}^{-1}v=v+\left(I-\nabla_{\theta}^{2}\mathcal{L}(s_{t})\right)\tilde{H}^{-1}_{t-1}v,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $s_{t}$, and $\nabla_{\theta}^{2}\mathcal{L}(s_{t})$ at step $t$ estimations
    of $H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta})$ (refer to
    Eq. ([9](#S3.E9 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation"))).
  prefs: []
  type: TYPE_NORMAL
- en: 'To further enhance the efficiency of acquiring influence scores for all samples,
    we use symmetric property to rewrite Eq. ([9](#S3.E9 "In 3.1\. Influence Score
    ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")) into:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $\displaystyle\mathcal{I}_{\text{remove,loss}}(s,{\mathcal{D}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The reformulation is based on the assumption that $\mathcal{L}(\cdot)$ is symmetric.
    Since $H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]\in\mathbb{R}^{m}$,
    we can efficiently obtain influence scores for all samples by only applying HVP
    estimation once for $H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$.
    The detailed HVP estimation process is illustrated in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49b29e1cb16246901ab9c2a500267c12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. (a) depicts the different learning ability due to the prior knowledge
    in LLMs. (b) presents the distributions of effort scores of LLM and surrogate
    model on Games dataset⁵⁵5We obtain the effort scores for surrogate model by calculating
    the gradient norm of the parameters of the surrogate model (Eq. ([12](#S3.E12
    "In 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")))..
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Gap Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in Eq. ([11](#S3.E11 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")), assessing the influence score of
    a sample requires the optimized parameters $\hat{\theta}$. Nevertheless, this
    poses challenges for LLM-based recommender models due to the continuous influx
    of large-scale new data in real-world scenarios. In this light, we propose to
    utilize a surrogate model to replace the LLMs and introduce an effort score as
    a gap regularization to complement the learning ability gap between LLMs and the
    surrogate models.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet\quad$Surrogate model. To reduce the costs, we propose utilizing a surrogate
    model, *e.g.,* a small-sized traditional recommender model, to compute the influence
    scores. Nevertheless, since LLMs acquire rich world knowledge during the pre-training
    stage, they intricately possess different learning abilities compared to the surrogate
    model (Figure [5](#footnote5 "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence Score ‣
    3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")(a)). Therefore,
    the influential samples on LLMs might deviate from the ones for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\delta_{s}=\lVert\nabla_{\phi}\mathcal{L}^{LLM}(s)\rVert_{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\phi$ is the learnable parameters of LLMs⁶⁶6The learnable parameters
    can be either the whole parameters of LLMs or the learnable parameters from parameter-efficient
    training, *e.g.,* LoRA (Hu et al., [2021](#bib.bib24)).. Intuitively, it measures
    the learning effort of LLMs to fit a specific user sequence, and a larger score
    indicates a harder sample for LLMs to learn. To elaborate, Eq. ([12](#S3.E12 "In
    3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")) measures the change in the model parameters, which can be interpreted
    as the discrepancy from the current knowledge encoded in LLMs’ parameters to the
    latest item knowledge or user behavior. As such, the effort score can emphasize
    significant samples particularly for LLMs, supplementing the different learning
    ability of the surrogate model (Figure [5](#footnote5 "footnote 5 ‣ Figure 3 ‣
    3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")(b)).
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Procedure of DEALRec
  prefs: []
  type: TYPE_NORMAL
- en: 1:Original training dataset $\mathcal{D}$, pre-trained parameters of LLM $\phi$.3:Obtain
    estimated ${H}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$ do5:    $I_{s_{i}}=\frac{1}{n^{2}}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})^{\mathrm{T}}H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{j}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{j},\hat{\theta})\right]+$;
    $\triangleright$ Split training samples $\mathcal{D}$ groups according to the
    final score $I_{s}$, $B\leftarrow\lfloor\frac{r|\mathcal{D}|}{K}\rfloor$ do10:    $k^{*}=\mathop{\arg\min}_{k}|G_{k}|$
    randomly select $\mathop{\min}\{B,|G_{k^{*}}|\}$;12:    $\mathcal{S}\leftarrow\mathcal{S}\cup\mathcal{S}_{k^{*}}$;13:    $B\leftarrow\lfloor\frac{r|\mathcal{D}|-|\mathcal{S}|}{|\mathcal{G}|}\rfloor$
    Update sampling budget14:Selected samples $\mathcal{S}$ for few-shot fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$Overall score. By injecting the signals of LLMs’ learning ability
    into the calculation of influence score, we can obtain the final score of each
    user sequence for LLM-based recommender fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is a hyper-parameter to balance the strength of the gap regularization.
    Notably, the gap regularization would suppress the easy samples with smaller effort
    scores while emphasizing the samples that are more difficult to learn, *i.e.,*
    larger effort scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, DEALRec identifies the influential samples with two key considerations:
    1) the influence score focuses on selecting the representative samples from the
    full dataset, capturing collaborative filtering information for low empirical
    risk; and 2) the effort score highlights the non-trivial samples that are significant
    to the learning of LLMs. The effectiveness of the two scores is empirically validated
    in Section [4.3.1](#S4.SS3.SSS1 "4.3.1\. Ablation Study (RQ2). ‣ 4.3\. In-depth
    Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Few-shot Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the final influential score obtained via Eq. ([13](#S3.E13 "In 3.2\.
    Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")),
    we can select a subset of data $\mathcal{S}$.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet\quad$ percentage of the training data. However, greedily selecting
    the samples with higher scores might result in very similar samples with low data
    coverage, which leads to: 1) Inadequacy of samples from other areas, thus hurting
    the bounded empirical risk (Zheng et al., [2022](#bib.bib61)) and lowering the
    overall performance (*cf.* Section [4.2](#S4.SS2 "4.2\. Overall Performance (RQ1)
    ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    2) Poor utilization of training samples because of the redundant samples with
    similar patterns, thereby causing suboptimal selection for few-shot fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet\quad$ groups according to their overall scores. We then iteratively
    sample $n_{s}$ is the average sampling budget for all groups and is initialized
    with $\lfloor\frac{r|\mathcal{D}|}{K}\rfloor$. If the group size is smaller than
    the average sampling budget, we select all users from this group and update the
    average sampling budget for the remaining groups (see Algorithm [2](#alg2 "Algorithm
    2 ‣ 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")).
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the selected few-shot samples $\mathcal{S}$) of LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $\hat{\phi}=\mathop{\arg\min}_{\phi\in\Phi}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{s_{i}\in\mathcal{S}}\mathcal{L}_{\phi}^{\text{LLM}}(s_{i}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: $\bullet\quad$ and calculate the influence score for all samples via Eq. ([11](#S3.E11
    "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")). We then obtain the effort score for LLMs via Eq. ([12](#S3.E12
    "In 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")), where $\phi$ can be the learnable parameters from any backend
    LLM-based recommender models. Eventually, we apply the stratified sampling to
    select the samples for LLMs’ few-shot fine-tuning. The detailed data pruning process
    of DEALRec is demonstrated in Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2\. Gap Regularization
    ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conduct extensive experiments on three real-world datasets to answer the
    following research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ1: How does our proposed DEALRec perform compared to the coreset selection
    baselines for LLM-based recommendation and the models trained with full data?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ2: How do the different components of DEALRec (*i.e.,* influence score, gap
    regularization, and stratified sampling) affect the performance, and is DEALRec
    generalizable to different surrogate models?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ3: How does DEALRec perform under different selection ratios and how does
    DEALRec improve the overall performance?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1\. Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1\. Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table 1\. Statistics of the three datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | # Users | # Items | # Interactions | Density |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Games | 49,156 | 17,332 | 342,329 | 0.04% |'
  prefs: []
  type: TYPE_TB
- en: '| MicroLens-50K | 49,887 | 19,217 | 359,048 | 0.04% |'
  prefs: []
  type: TYPE_TB
- en: '| Book | 88,263 | 86,272 | 5,303,707 | 0.07% |'
  prefs: []
  type: TYPE_TB
- en: 'We conduct experiments on three real-world recommendation datasets across different
    domains: 1) Games is from the Amazon review datasets⁷⁷7[https://jmcauley.ucsd.edu/data/amazon/.](https://jmcauley.ucsd.edu/data/amazon/.),
    which covers interactions between users and video games with rich textual features
    such as title and categories. 2) MicroLens-50K⁸⁸8[https://github.com/westlake-repl/MicroLens/.](https://github.com/westlake-repl/MicroLens/.)
    is a newly released micro-video recommendation dataset (Ni et al., [2023](#bib.bib39)).
    It contains 50$k$. For the three datasets, we sort all user-item interactions
    according to the global timestamps, and then split the interactions into training,
    validation, and testing sets with the ratio of 8:1:1. Besides, we consider two
    different fine-tuning settings as follows: 1) Few-shot fine-tuning fine-tunes
    LLM-based recommender models with limited samples at a fixed size, *e.g.,* 1024-shot,
    obtained via different data pruning methods. 2) Full fine-tuning utilizes all
    samples to fine-tune LLM-based recommender models without data pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare DEALRec with the random sampling and several competitive coreset
    selection methods, including difficulty-based methods and diversity-based methods:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random obtains the data subset via random sampling, which is a popular and strong
    baseline in data-efficient training (Guo et al., [2022](#bib.bib17)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GraNd (Paul et al., [2021](#bib.bib40)) is a representative coreset selection
    method that assumes hard samples are important. It uses the average gradient norm
    of each sample during training and selects the samples with larger gradient norms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EL2N (Paul et al., [2021](#bib.bib40)) proposes to select the samples with larger
    errors between the labels and the prediction from the model trained by the original
    dataset. This method is also difficulty-based.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CCS (Zheng et al., [2022](#bib.bib61)) is a competitive method that selects
    the samples considering both high data coverage and sample importance. We use
    EL2N as the importance metric for CCS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-DCon (Wu et al., [2023b](#bib.bib54)) is a recently proposed data pruning
    method for content-based recommendation, which clusters the user sequences based
    on the user representations obtained from both well-trained recommender models
    and LLMs for selection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RecRanker (Luo et al., [2023](#bib.bib37)) proposes a sampling strategy to select
    high-quality user sequences. It selects the users with more interactions for better
    user modeling and utilizes a cluster-based sampling strategy to enhance user diversity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We do not perform optimization-based methods for comparison because of the
    inapplicability of complex bi-level or discrete optimization for LLMs on large-scale
    recommendation data (*cf.* Section [2](#S2 "2\. Task Formulation ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")). We instantiate our proposed DEALRec
    and all baselines on two competitive backend LLM-based recommender models: 1)
    BIGRec (Bao et al., [2023a](#bib.bib4)) utilizes the item title to present the
    user sequence for recommendation generation; 2) TIGER (Rajput et al., [2023](#bib.bib41))
    learns extra tokens from item features to present items, and then converts the
    user sequence into the sequence of the new item token for next-item generation.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet\quad$ and NDCG@$K$ set to $10$ for Games, and $K=20$ for MicroLens-50K
    and Book.⁹⁹9We report metrics@$20$ because of the challenging modeling of user
    behavior on book and micro-video recommendations, where the temporal shifts of
    user interests and the item feature is stronger and thus more difficult to capture (Wang
    et al., [2023a](#bib.bib50), [b](#bib.bib51)). For the two backend LLM-based recommender
    models, we adopt full ranking protocal (He and Chua, [2017](#bib.bib22)) for BIGRec.
    Since TIGER does not support full ranking, we select the top-$K$ items from the
    generated items according to the probability scores for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Overall performance comparison between the baselines and DEALRec instantiated
    on two competitive LLM-based recommender models on three datasets. For each backend
    model, the bold results highlight the best results while the second-best ones
    are underlined. $*$-value ¡ 0.01) under one-sample t-tests. We run all experiments
    for 3 times with different random seeds and report the averaged results.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Games | MicroLens-50K | Book |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 1024-shot ($\bm{r}$=2%) | 1024-shot ($\bm{r}$=1%) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Methods | R@10 | R@20 | N@10 | N@20 | R@20 | R@50 | N@20 | N@50 | R@20
    | R@50 | N@20 | N@50 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TF-DCon | 0.0102 | 0.0157 | 0.0062 | 0.0078 | 0.0066 | 0.0099 | 0.0027
    | 0.0034 | 0.0104 | 0.0144 | 0.0083 | 0.0092 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RecRanker | 0.0112 | 0.0166 | 0.0074 | 0.0090 | 0.0024 | 0.0042 | 0.0011
    | 0.0014 | 0.0108 | 0.0145 | \ul0.0090 | \ul0.0097 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CCS | \ul0.0164 | 0.0246 | 0.0097 | 0.0122 | 0.0096 | 0.0131 | 0.0041
    | 0.0049 | \ul0.0110 | 0.0145 | 0.0088 | 0.0096 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GraNd | 0.0158 | 0.0250 | 0.0098 | 0.0125 | 0.0014 | 0.0032 | 0.0006 |
    0.0010 | 0.0102 | 0.0136 | 0.0080 | 0.0087 |'
  prefs: []
  type: TYPE_TB
- en: '|  | EL2N | 0.0154 | \ul0.0256 | 0.0098 | \ul0.0128 | 0.0096 | 0.0045 | 0.0041
    | 0.0016 | 0.0107 | \ul0.0149 | 0.0085 | 0.0094 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Random | 0.0163 | 0.0241 | \ul0.0100 | 0.0122 | \ul0.0108 | \ul0.0151
    | \ul0.0044 | \ul0.0054 | 0.0099 | 0.0134 | 0.0083 | 0.0090 |'
  prefs: []
  type: TYPE_TB
- en: '| BIGRec | DEALRec | 0.0181* | 0.0276* | 0.0115* | 0.0142* | 0.0124* | 0.0160*
    | 0.0055* | 0.0064* | 0.0117* | 0.0155* | 0.0096* | 0.0104* |'
  prefs: []
  type: TYPE_TB
- en: '|  | TF-DCon | 0.0051 | 0.0074 | 0.0033 | 0.0040 | 0.0006 | 0.0057 | 0.0002
    | 0.0013 | 0.0028 | 0.0051 | 0.0020 | 0.0027 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RecRanker | 0.0028 | 0.0045 | 0.0019 | 0.0024 | \ul0.0043 | \ul0.0064
    | \ul0.0011 | \ul0.0014 | 0.0027 | 0.0052 | 0.0018 | 0.0025 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CCS | 0.0050 | 0.0084 | 0.0031 | 0.0041 | 0.0026 | 0.0061 | 0.0010 | 0.0013
    | 0.0026 | 0.0048 | 0.0018 | 0.0024 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GraNd | 0.0042 | 0.0053 | 0.0027 | 0.0030 | 0.0006 | 0.0014 | 0.0003 |
    0.0005 | 0.0008 | 0.0020 | 0.0006 | 0.0010 |'
  prefs: []
  type: TYPE_TB
- en: '|  | EL2N | 0.0034 | 0.0048 | 0.0024 | 0.0029 | 0.0011 | 0.0016 | 0.0004 |
    0.0004 | 0.0005 | 0.0015 | 0.0004 | 0.0007 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Random | \ul0.0062 | \ul0.0102 | \ul0.0039 | \ul0.0051 | 0.0037 | 0.0059
    | 0.0011 | 0.0014 | \ul0.0033 | \ul0.0066 | \ul0.0022 | \ul0.0031 |'
  prefs: []
  type: TYPE_TB
- en: '| TIGER | DEALRec | 0.0074* | 0.0114* | 0.0062* | 0.0074* | 0.0058* | 0.0076*
    | 0.0020* | 0.0020* | 0.0039* | 0.0076* | 0.0026* | 0.0037* |'
  prefs: []
  type: TYPE_TB
- en: Table 3\. Performance comparison between DEALRec under 1024-shot fine-tuning
    and the full fine-tuning of the BIGRec in terms of both accuracy and time costs.
    “%Com.” denotes the performance achieved by DEALRec compared to the full fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Games | MicroLens-50K | Book |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | R@10$\uparrow$ | N@10$\uparrow$ | Time$\downarrow$ | R@50$\uparrow$ |
    N@50$\uparrow$ | R@20$\uparrow$ | N@20$\uparrow$ | Time$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 0.0169 | 0.0233 | 0.0102 | 0.0120 | 36.87h | 0.0081 | 0.0136 | 0.0038
    | 0.0053 | 66.64h | 0.0076 | 0.0108 | 0.0060 | 0.0068 | 84.77h |'
  prefs: []
  type: TYPE_TB
- en: '| DEALRec | 0.0181 | 0.0276 | 0.0115 | 0.0142 | 1.67h | 0.0124 | 0.0160 | 0.0055
    | 0.0064 | 1.23h | 0.0117 | 0.0155 | 0.0096 | 0.0104 | 1.93h |'
  prefs: []
  type: TYPE_TB
- en: '| % Com. | 107.10% | 118.45% | 112.75% | 118.33% | 4.53% | 153.09% | 117.65%
    | 144.74% | 120.75% | 1.85% | 153.95% | 143.52% | 160.00% | 152.94% | 2.28% |'
  prefs: []
  type: TYPE_TB
- en: 4.1.3\. Implementation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As for the two backend LLM-based recommender models, we follow the original
    settings in their paper for implementation. We employ LLaMA-7B for BIGRec and
    transformer-based encoder-decoder architecture for TIGER as introduced in their
    paper (Rajput et al., [2023](#bib.bib41)). All fine-tuning experiments are conducted
    on four NVIDIA RTX A5000 GPUs. Besides, we adopt the parameter-efficient fine-tuning
    technique LoRA (Hu et al., [2021](#bib.bib24)) to fine-tune BIGRec and fully fine-tune
    the parameters of TIGER. We utilize SASRec (Kang and McAuley, [2018](#bib.bib25)),
    a representative sequential recommender model, as the surrogate model in DEALRec.
    We set the iteration number $T$ in $\{0.1,0.3,0.5,1.0,2.0\}$ is explored in $\{25,50,75\}$.
    As for the coreset selection methods that require the training of LLMs, we consider
    a feasible implementation (Coleman et al., [2020](#bib.bib9)) by executing them
    on the same surrogate model as DEALRec.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Overall Performance (RQ1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The results of the baselines and DEALRec with two competitive backend LLM-based
    recommender models on three datasets under few-shot fine-tuning (1024 samples)
    are presented in Table [2](#S4.T2 "Table 2 ‣ 4.1.2\. Baselines. ‣ 4.1\. Experimental
    Settings ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation"),
    from which we have the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All methods with BIGRec typically yield better performance than those with
    TIGER, which is attributed to two reasons: 1) BIGRec employs a larger LLM (*i.e.,*
    LLaMA-7B) compared to TIGER, thereby benefiting from the stronger generalization
    ability of large-sized LLMs (Lin et al., [2023](#bib.bib33)); and 2) BIGRec leverages
    item titles to present the user sequence, leading to better utilization of world
    knowledge in LLMs. In contrast, TIGER learns extra item tokens for LLMs. This
    might result in cold-start item issues since only limited item tokens are learned
    while others are maintained randomly initialized under the few-shot fine-tuning
    setting.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among all coreset selection baselines, difficulty-based (GraNd, EL2N) methods
    generally perform better than diversity-based methods (TF-DCon, RecRanker). This
    is reasonable since diversity-based methods merely heuristically encourage selecting
    users with divergent preference, which lacks the assessments of their contributions
    to the model training. In contrast, GraNd and EL2N use pre-defined metrics to
    measure the sample difficulty and select the samples with larger scores, which
    encourages selecting the samples that are more informative for models’ optimization.
    Besides, CCS improves EL2N in most cases, as it maintains easy samples for selection,
    thus compensating the knowledge of recommendation data from high-density areas.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another interesting observation is that random sampling yields competitive
    performance or even outperforms other coreset selection methods in some cases,
    which might attributed to two possible reasons: 1) Uniformly selected user sequences
    preserve high coverage of the original training distribution compared to other
    baselines, which ensures a high probability of guaranteed bound for low empirical
    risk (Zheng et al., [2022](#bib.bib61)). This observation is also consistent with
    the findings in (Guo et al., [2022](#bib.bib17)). 2) The inferior performance
    of some coreset selection methods also might be caused by the implementation settings
    (Section [4.1.3](#S4.SS1.SSS3 "4.1.3\. Implementation. ‣ 4.1\. Experimental Settings
    ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")),
    where they may suffer from the learning ability gap between the surrogate model
    and LLMs. (*cf.* Section [3.2](#S3.SS2 "3.2\. Gap Regularization ‣ 3\. DEALRec
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DEALRec significantly outperforms all coreset selection methods across the
    three datasets. The consistent performance improvements on both backend models
    validate the superiority of DEALRec in identifying influential samples for LLMs’
    adaptation to the recommendation data. The superior performance is attributed
    to: 1) the accurate and efficient estimation of the influence on empirical risk,
    *i.e.,* overall performance by removing a sample in training; and 2) the gap regularization
    based on the effort score to penalize the easy samples for LLMs. By emphasizing
    the non-trivial samples specifically for LLMs, gap regularization alleviates the
    learning ability gap between the surrogate model and the LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '$\bullet\quad$Comparison with full fine-tuning. We further compare DEALRec
    with BIGRec under full training *w.r.t.* accuracy and efficiency, as presented
    in Table [3](#S4.T3 "Table 3 ‣ 4.1.2\. Baselines. ‣ 4.1\. Experimental Settings
    ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
    We can find that: 1) DEALRec achieves higher performance compared to the model
    trained by the full data, indicating the effectiveness of DEALRec for high accuracy.
    The inferior performance of BIGRec under full training also implies that not all
    user sequences are informative for model training, or even harmful to the training,
    *e.g.,* false negative interactions. This has also been observed in CTR prediction (Wu
    et al., [2023a](#bib.bib53)) and has been discussed in (Agarwal et al., [2020](#bib.bib3))
    from the view of data redundancy. 2) DEALRec significantly reduces the time costs
    for LLMs’ fine-tuning (97.11% reduction of fine-tuning costs on average). With
    the remarkably declined training costs, DEALRec has the potential to facilitate
    real-world applications of LLM-based recommender models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. In-depth Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We carry out further experiments^(10)^(10)10We conduct in-depth experiments
    using BIGRec as the backend model because of its better performance compared to
    TIGER. to analyze the effectiveness of each component of DEALRec and the robustness
    of surrogate model selection. Besides, we investigate how DEALRec performs under
    different selection ratios and explore how DEALRec performs over different user
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. Ablation Study (RQ2).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/537dfd4059a78d475e54fc2600d2bf87.png)![Refer to caption](img/8bda131f113acd2ba54bc1d60dd2bba2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Ablation study of the influence score, effort score, and coverage-enhanced
    sample selection strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To study the effectiveness of each component of DEALRec, *i.e.,* influence
    score, effort score, and coverage-enhanced sample selection strategy, we separately
    remove the Influence Score (IS) and effort score $\delta_{s}$”, respectively.
    Besides, we replace the coverage-enhanced sample selection strategy by greedily
    selecting the samples with higher scores, denoted as “Greedy”. From the results
    presented in Figure [4](#S4.F4 "Figure 4 ‣ 4.3.1\. Ablation Study (RQ2). ‣ 4.3\.
    In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation"), we can observe that: removing either the influence score or
    effort score will cause performance drops. This validates the effectiveness of
    1) the assessment of overall performance change caused by removing samples from
    training; 2) additional signals of learning ability captured from LLMs as regularization,
    alleviating the gap between the surrogate model and the LLMs. Moreover, simply
    selecting the samples with higher overall scores might weaken the learning of
    distinct user behaviors and item knowledge (inferior performance of “Greedy”),
    as discussed in Section [3.3](#S3.SS3 "3.3\. Few-shot Fine-tuning ‣ 3\. DEALRec
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. Robustness on different surrogate model (RQ2).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table 4\. Performance comparison between DEALRec with different surrogate models
    and the BIGRec under full training. “Time” presents the time costs for training
    the surrogate model on a single NVIDIA RTX A5000.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | R@10$\uparrow$ | N@10$\uparrow$ | Time$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Full | 0.0169 | 0.0233 | 0.0102 | 0.0120 | / |'
  prefs: []
  type: TYPE_TB
- en: '| BERT4Rec | 0.0175 | 0.0258 | 0.0103 | 0.0128 | 0.76h |'
  prefs: []
  type: TYPE_TB
- en: '| SASRec | 0.0181 | 0.0276 | 0.0115 | 0.0142 | 0.45h |'
  prefs: []
  type: TYPE_TB
- en: '| DCRec | 0.0211 | 0.0283 | 0.0117 | 0.0137 | 0.61h |'
  prefs: []
  type: TYPE_TB
- en: To further assess the generalization ability of DEALRec on different surrogate
    models, we employ three representative sequential recommender models, *i.e.,*
    BERT4Rec (Sun et al., [2019](#bib.bib45)), SASRec (Kang and McAuley, [2018](#bib.bib25)),
    and a recently proposed DCRec (Yang et al., [2023a](#bib.bib57)) as the surrogate
    models for data pruning, respectively. The results on Games are presented in Table [4](#S4.T4
    "Table 4 ‣ 4.3.2\. Robustness on different surrogate model (RQ2). ‣ 4.3\. In-depth
    Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation"),
    and we omit the results with similar observations on the other two datasets to
    save space.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the table, we can find that: 1) DEALRec with the three surrogate models
    consistently outperforms BIGRec under full fine-tuning. This demonstrates the
    strong robustness of DEALRec on different surrogate models. 2) Nonetheless, different
    surrogate models cause some fluctuations in the accuracy of the LLM-based recommender
    model. This is reasonable because different model architectures have different
    expressiveness of user behavior and item knowledge. As such, the selected samples
    possibly vary across different surrogate models, thus affecting the effectiveness
    of LLMs’ few-shot fine-tuning. 2) It is noted that DCRec surpasses SASRec and
    BERTRec by a large margin. The possible reason might be that the SOTA DCRec employs
    contrastive learning to enhance the learning of user representations, thus leading
    to better user modeling and yielding lower empirical risk. 3) SASRec exhibits
    the least time costs for training and achieves competitive performance among the
    three surrogate models. Therefore, based on the empirical results, SASRec could
    be a good choice of surrogate model for DEALRec to facilitate efficient LLM-based
    recommender fine-tuning in real-world deployments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Effect of selection ratio $\bm{r}$ (RQ3).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9dea565f63a4c12729187c4494df60fb.png)![Refer to caption](img/c111e07ab5f650ae8c81ef38a4a2da2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Performance of DEALRec with different selection ratio $r$ *w.r.t.*
    accuracy and efficiency on Games.
  prefs: []
  type: TYPE_NORMAL
- en: 'To investigate the effect of selection ratio $r$ from $0.2\%$ (4096-shot) and
    present the results in Figure [5](#S4.F5 "Figure 5 ‣ 4.3.3\. Effect of selection
    ratio 𝑟 (RQ3). ‣ 4.3\. In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation"). From the figures, it is observed that: 1) The
    recommendation accuracy rapidly improves as the number of selected samples increases
    from $0.2\%$, surpassing the full training when $r=1\%$ to $4\%$. 3) Empirically,
    setting $r=1\%$ is recommended to achieve comparable performance to full fine-tuning
    as well as achieving low costs for real-world deployments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4\. User group evaluation (RQ3).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ccf8917abad159f99ec9bfb8d878efcb.png)![Refer to caption](img/12cb4422c299f7171ef43274715a3ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Performance of DEALRec over easy to difficult samples (Group 1 to
    Group 3).
  prefs: []
  type: TYPE_NORMAL
- en: To study how DEALRec achieves superior overall performance, we test the DEALRec
    over user sequences of different difficulties. Specifically, we calculate the
    loss of each user sequence via the model trained by randomly selected few-shot
    samples; we then divide the users into three groups according to their loss values,
    from the easier samples with smaller loss (Group 1) to the harder samples with
    larger loss (Group 3). The results of each group of DEALRec and Random on Games
    are presented in Figure [6](#S4.F6 "Figure 6 ‣ 4.3.4\. User group evaluation (RQ3).
    ‣ 4.3\. In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation"). We can find that 1) the performance of both DEALRec and Random
    gradually declines from Group 1 to Group 3, because users with larger loss are
    more difficult to predict. Nevertheless, 2) DEALRec consistently outperforms Random
    in each group, which validates the effectiveness of DEALRec in considering the
    influence on overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5\. Effect of regularization strength $\bm{\lambda}$.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a95e1a4ca1f22e264e897957fbbece5.png)![Refer to caption](img/87589dc72705f29b2211882544c95675.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Performance of DEALRec with different $\lambda$.
  prefs: []
  type: TYPE_NORMAL
- en: To examine the impact of gap regularization strength, we vary $\lambda$ to $2$
    to balance between the performance-driven influential samples from the surrogate
    model and the difficult samples for the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1\. LLM-based Recommendation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Leveraging LLMs for recommendation has gained remarkable attention recently (Li
    et al., [2023a](#bib.bib32); Wu et al., [2023c](#bib.bib55); Feng et al., [2023](#bib.bib14);
    Wang et al., [2023c](#bib.bib49)), showcasing their potential across various recommendation
    tasks, including CTR predicton (Bao et al., [2023b](#bib.bib5)), sequential recommendation (Lin
    et al., [2023](#bib.bib33)), and cross-domain recommendation (Gong et al., [2023](#bib.bib16)).
    Some early studies explore the recommendation ability of powerful LLMs, *e.g.,*
    ChatGPT, through the in-context-learning ability (Liu et al., [2023](#bib.bib35);
    Dai et al., [2023](#bib.bib11); Sun et al., [2023](#bib.bib46)). Nevertheless,
    the performance of LLMs is limited without extra fine-tuning over the domain-specific
    recommendation data (Liu et al., [2023](#bib.bib35); Bao et al., [2023b](#bib.bib5)).
    To fully leverage the potential of LLMs for recommendation, a series of work studies
    various fine-tuning strategies tailored for recommendation tasks (Li et al., [2023b](#bib.bib31);
    Zhang et al., [2023](#bib.bib58); Gong et al., [2023](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: However, fine-tuning LLMs requires extensive computational resources and time
    costs, thus hindering the real-world applications of LLM-based recommender models.
    Therefore, it is crucial to enhance the fine-tuning efficiency of LLM-based recommender
    models. In this work, we propose the task of data pruning for efficient LLM-based
    recommendation, which aims to identify representative samples tailored for LLMs’
    few-shot fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Coreset Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coreset selection has been widely studied in both traditional machine learning (Wei
    et al., [2015](#bib.bib52); Chen et al., [2012](#bib.bib8); Feldman et al., [2011](#bib.bib12))
    and deep learning (Yang et al., [2023b](#bib.bib56)), benefiting many downstream
    tasks such as data-efficient learning (Toneva et al., [2018](#bib.bib48)), continual
    learning (Borsos et al., [2020](#bib.bib6)), neural architecture search (Shim
    et al., [2021](#bib.bib44)), and active learning (Sener and Savarese, [2018](#bib.bib43)).
    It aims to select a small but representative subset from the full data that can
    lead to comparable model performance. Previous work mainly falls into two groups:
    1) Heuristic methods (Coleman et al., [2020](#bib.bib9); Toneva et al., [2018](#bib.bib48);
    Feldman and Zhang, [2020](#bib.bib13)) typically assume difficult or diverse samples
    are informative for model training and use pre-defined metrics to compute a score
    for selection. 2) Optimization-based methods (Yang et al., [2023b](#bib.bib56);
    Mirzasoleiman et al., [2020](#bib.bib38); Killamsetty et al., [2021a](#bib.bib26);
    Kothawade et al., [2022](#bib.bib30)) leverages the bi-level or discrete optimization
    techniques to optimize the data subset that can minimize the empirical risk. However,
    heuristic methods do not estimate the impact of selected samples on empirical
    risk, thus might lead to suboptimal coreset selection. And optimization-based
    methods fail to be applied to LLM-based recommendation due to the cumbersome calculation
    for complex optimization. Furthermore, previous methods usually rely on the training
    of the model on full data for selection, which is infeasible for LLM-based recommendation
    (*cf.* Section [2](#S2 "2\. Task Formulation ‣ Data-efficient Fine-tuning for
    LLM-based Recommendation")).'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet\quad$Data Condensation (Zhao et al., [2020](#bib.bib60)) is another
    potential solution to achieve data-efficient training. However, it is intrinsically
    different from our proposed task of data pruning. While it aims to synthesize
    a small but informative dataset (Zhao and Bilen, [2023](#bib.bib59)), our proposed
    task targets to identify representative samples from the existing samples for
    LLM’s few-shot fine-tuning. Besides, previous work mainly designed for continuous
    data, which is not applicable to LLM-based recommendation (Wu et al., [2023a](#bib.bib53)).
    TF-DCon (Wu et al., [2023b](#bib.bib54)) is recently proposed for content-based
    recommendation and we compare it in Section [4.2](#S4.SS2 "4.2\. Overall Performance
    (RQ1) ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, we proposed the task of data pruning for efficient LLM-based
    recommendation, which aims to identify representative samples tailored for LLMs’
    few-shot fine-tuning. Furthermore, we posited two objectives for this data pruning
    task: 1) high accuracy targets to select the samples that can lead to low empirical
    risk; and 2) high efficiency strives to consume low costs for the data pruning
    process. To this end, we proposed a novel data pruning method, namely DEALRec,
    to efficiently identify the influential samples with two scores. 1) The influence
    score is formulated to estimate the influence of sample removal on empirical risk,
    where the calculation is extended from the influence function and is accelerated
    through the symmetric property. 2) We introduced a small-sized surrogate model
    to calculate the influence score efficiently and proposed the effort score to
    bridge the gap between the surrogate model and LLMs. Empirical results validate
    the effectiveness of DEALRec in achieving both high efficiency and high accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: This work proposes a data pruning task for LLM fine-tuning, opening up a new
    research direction for efficient LLM-based recommendation and leaving many promising
    directions for future work. 1) It is worthwhile to apply DEALRec to more LLM-based
    recommender models on more cross-domain datasets, improving fine-tuning performance
    with limited resources. 2) Due to the limited context window length of LLMs, it
    is promising to select the informative interacted items in users’ interaction
    sequences for LLMs’ fine-tuning. 3) Enhancing the inference efficiency of LLM-based
    recommender models is also a crucial problem for their real-world deployments.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. (2016) Naman Agarwal, Brian Bullins, and Elad Hazan. 2016. Second-order
    stochastic optimization in linear time. *stat* 1050 (2016), 15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. (2020) Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan
    Arora. 2020. Contextual diversity for active learning. In *ECCV*. Springer, 137–153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2023a) Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi
    Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023a. A bi-step grounding
    paradigm for large language models in recommendation systems. *arXiv:2308.08434*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2023b) Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng,
    and Xiangnan He. 2023b. Tallrec: An effective and efficient tuning framework to
    align large language model with recommendation. In *RecSys*. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borsos et al. (2020) Zalán Borsos, Mojmir Mutny, and Andreas Krause. 2020. Coresets
    via bilevel optimization for continual learning and streaming. *NeurIPS* 33 (2020),
    14879–14890.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chai et al. (2023) Chengliang Chai, Jiayi Wang, Nan Tang, Ye Yuan, Jiabin Liu,
    Yuhao Deng, and Guoren Wang. 2023. Efficient coreset selection with cluster-based
    methods. In *KDD*. ACM, 167–178.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2012) Yutian Chen, Max Welling, and Alex Smola. 2012. Super-samples
    from kernel herding. *arXiv:1203.3472* (2012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coleman et al. (2020) C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis,
    P Liang, J Leskovec, and M Zaharia. 2020. Selection via Proxy: Efficient Data
    Selection for Deep Learning. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cook (1977) R Dennis Cook. 1977. Detection of influential observation in linear
    regression. *Technometrics* 19, 1 (1977), 15–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2023) Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si,
    Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT’s Capabilities
    in Recommender Systems. *arXiv:2305.02182*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feldman et al. (2011) Dan Feldman, Matthew Faulkner, and Andreas Krause. 2011.
    Scalable training of mixture models via coresets. *NeurIPS* 24 (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feldman and Zhang (2020) Vitaly Feldman and Chiyuan Zhang. 2020. What neural
    networks memorize and why: Discovering the long tail via influence estimation.
    *NeurIPS* 33 (2020), 2881–2891.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2023) Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao
    Hu, Peng Jiang, Kun Gai, and Fei Sun. 2023. A Large Language Model Enhanced Conversational
    Recommender System. *arXiv:2308.06212* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang,
    and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented
    recommender system. *arXiv:2303.14524*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2023) Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu,
    and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model
    for Cold-Start Scenario. In *CIKM*. 4595–4601.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2022) Chengcheng Guo, Bo Zhao, and Yanbing Bai. 2022. Deepcore:
    A comprehensive library for coreset selection in deep learning. In *DEXA*. Springer,
    181–195.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2017) Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang
    He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction.
    In *IJCAI*. 1725–1731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hampel (1974) Frank R Hampel. 1974. The influence curve and its role in robust
    estimation. *Journal of the american statistical association* 69, 346 (1974),
    383–393.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *CVPR*. IEEE, 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2023) Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. 2023. Large-scale
    Dataset Pruning with Dynamic Uncertainty. *arXiv:2306.05175*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He and Chua (2017) Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization
    Machines for Sparse Predictive Analytics. In *SIGIR*. ACM, 355–364.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020) Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang,
    and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network
    for recommendation. In *SIGIR*. 639–648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv:2106.09685*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang and McAuley (2018) Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive
    sequential recommendation. In *ICDM*. IEEE, 197–206.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Killamsetty et al. (2021a) Krishnateja Killamsetty, Sivasubramanian Durga,
    Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. 2021a. Grad-match: Gradient matching
    based data subset selection for efficient deep model training. In *ICML*. PMLR,
    5464–5474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Killamsetty et al. (2021b) Krishnateja Killamsetty, Durga Sivasubramanian,
    Ganesh Ramakrishnan, and Rishabh Iyer. 2021b. Glister: Generalization based data
    subset selection for efficient and robust learning. In *AAAI*, Vol. 35\. 8110–8118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Killamsetty et al. (2021c) Krishnateja Killamsetty, Xujiang Zhao, Feng Chen,
    and Rishabh Iyer. 2021c. Retrieve: Coreset selection for efficient and robust
    semi-supervised learning. *NeurIPS* 34 (2021), 14488–14501.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koh and Liang (2017) Pang Wei Koh and Percy Liang. 2017. Understanding black-box
    predictions via influence functions. In *ICML*. PMLR, 1885–1894.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothawade et al. (2022) Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan,
    Jeff Bilmes, and Rishabh Iyer. 2022. PRISM: A Unified Framework of Parameterized
    Submodular Information Measures for Targeted Data Subset Selection and Summarization.
    In *AAAI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Lei Li, Yongfeng Zhang, and Li Chen. 2023b. Prompt distillation
    for efficient llm-based recommendation. In *CIKM*. 1348–1357.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao
    Xing. 2023a. E4SRec: An Elegant Effective Efficient Extensible Solution of Large
    Language Models for Sequential Recommendation. *arXiv:2312.02443*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2023) Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng,
    and Tat-Seng Chua. 2023. A multi-facet paradigm to bridge large language model
    and recommendation. *arXiv:2310.06491*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling (1984) Robert F Ling. 1984. Residuals and influence in regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang.
    2023. Is chatgpt a good recommender? a preliminary study. *arXiv:2304.10149*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024.
    ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source
    Large Language Models. In *WSDM*. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou,
    Zongpeng Li, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023. RecRanker: Instruction
    Tuning Large Language Model as Ranker for Top-k Recommendation. *arXiv:2312.16018*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirzasoleiman et al. (2020) Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
    2020. Coresets for data-efficient training of machine learning models. In *ICML*.
    PMLR, 6950–6960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ni et al. (2023) Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li,
    Xiangnan He, Yongfeng Zhang, and Fajie Yuan. 2023. A Content-Driven Micro-Video
    Recommendation Dataset at Scale. *arXiv:2309.15379* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paul et al. (2021) Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite.
    2021. Deep learning on a data diet: Finding important examples early in training.
    *NeurIPS* 34, 20596–20607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajput et al. (2023) Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H
    Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost,
    et al. 2023. Recommender Systems with Generative Retrieval. In *NeurIPS*. Curran
    Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sachdeva et al. (2022) Noveen Sachdeva, Mehak Dhaliwal, Carole-Jean Wu, and
    Julian McAuley. 2022. Infinite recommendation networks: a data-centric approach.
    *NeurIPS* 35, 31292–31305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener and Savarese (2018) Ozan Sener and Silvio Savarese. 2018. Active learning
    for convolutional neural networks: A core-set approach. (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shim et al. (2021) Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. 2021. Core-set
    sampling for efficient neural architecture search. *arXiv:2107.06869*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu
    Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional
    encoder representations from transformer. In *CIKM*. 1441–1450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language
    Models as Re-Ranking Agent. In *EMNLP*. ACL, 14918–14937.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2023) Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan
    Wang, and Xiaojuan Qi. 2023. Data Pruning via Moving-one-Sample-out. *arXiv:2310.14664*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toneva et al. (2018) Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes,
    Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. 2018. An empirical study
    of example forgetting during deep neural network learning. *arXiv:1812.05159*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, and Yong
    Wang. 2023c. LLM4Vis: Explainable visualization recommendation using ChatGPT.
    *arXiv:2310.07652* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yunshan
    Ma, and Tat-Seng Chua. 2023a. Causal Disentangled Recommendation Against User
    Preference Shifts. *TOIS* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yinwei Wei,
    and Tat-Seng Chua. 2023b. Equivariant Learning for Out-of-Distribution Cold-start
    Recommendation. In *MM*. 903–914.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2015) Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2015. Submodularity
    in data subset selection and active learning. In *ICML*. PMLR, 1954–1963.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023a) Jiahao Wu, Wenqi Fan, Shengcai Liu, Qijiong Liu, Rui He, Qing
    Li, and Ke Tang. 2023a. Dataset condensation for recommendation. *arXiv:2310.01038*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023b) Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai
    Liu, Qing Li, Xiao-Ming Wu, and Ke Tang. 2023b. Leveraging Large Language Models
    (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation.
    *arXiv:2310.09874*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023c) Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong
    Chen. 2023c. Exploring large language model for graph data understanding in online
    job recommendations. *arXiv:2307.05722*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023b) Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun,
    and Ping Li. 2023b. Dataset pruning: reducing training data by examining generalization
    influence. (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023a) Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da
    Luo, and Kangyi Lin. 2023a. Debiased Contrastive Learning for Sequential Recommendation.
    In *WWW*. 1063–1073.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao,
    Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large
    language model empowered recommendation approach. *arXiv:2305.07001*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Bilen (2023) Bo Zhao and Hakan Bilen. 2023. Dataset condensation with
    distribution matching. In *WACV*. IEEE, 6514–6523.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020) Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. Dataset
    Condensation with Gradient Matching. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2022) Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash. 2022.
    Coverage-centric Coreset Selection for High Pruning Rates. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
