- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:34:35'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.15300](https://ar5iv.labs.arxiv.org/html/2408.15300)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Maxim Zhelnin^♣ ¹, Viktor Moskvoretskii^♣ ^(1,3), Egor Shvetsov¹,
  prefs: []
  type: TYPE_NORMAL
- en: Egor Venediktov, Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev ^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Skolkovo Institute of Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: ² Artificial Intelligence Research Institute
  prefs: []
  type: TYPE_NORMAL
- en: ³ HSE University
  prefs: []
  type: TYPE_NORMAL
- en: 'Correspondence: [m.zhelnin@skol.tech](mailto:%20m.zhelnin@skol.tech) $\clubsuit$
    indicates equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized
    the usage of Large Language Models (LLMs). Recent studies have shown that a small
    subset of weights significantly impacts performance. Based on this observation,
    we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of
    Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting
    Gaussian noise into non-salient ones. To identify these columns, we developed
    a generalized sensitivity metric that extends and unifies metrics from previous
    studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full
    fine-tuning and modern PEFT methods under the same computational budget. Moreover,
    GIFT-SW offers practical advantages to recover performance of models subjected
    to mixed-precision quantization with keeping salient weights in full precision.
    Code is available in [our repository](https://github.com/On-Point-RND/GIFT_SW).
  prefs: []
  type: TYPE_NORMAL
- en: 'GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Maxim Zhelnin^♣ ¹, Viktor Moskvoretskii^♣ ^(1,3), Egor Shvetsov¹, Egor Venediktov,
    Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev ^(1,2) ¹ Skolkovo Institute of
    Science and Technology ² Artificial Intelligence Research Institute ³ HSE University
    Correspondence: [m.zhelnin@skol.tech](mailto:%20m.zhelnin@skol.tech) $\clubsuit$
    indicates equal contribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/429c6774423f92a2b210f31e12b31bb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Mean performance of different fine-tuning approaches for LLaMA models
    with scaling data budget. GIFT-SW shows superior performance with nearly all data
    budgets, also being as stable as full fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern LLMs demonstrate remarkable generalization capabilities on unseen tasks.
    However, fine-tuning remains crucial to enhance these models performance or to
    restore the performance after compression techniques like quantization Dettmers
    et al. ([2024](#bib.bib9)); Moskvoretskii et al. ([2024](#bib.bib28)), pruning Frantar
    and Alistarh ([2023](#bib.bib12)); Kim et al. ([2023](#bib.bib19)), or tensor
    decomposition have been applied. Given the large scale of modern LLMs, fine-tuning
    all parameters can be computationally and memory-intensive. To overcome this challenge,
    Parameter Efficient Fine-Tuning schemes have been developed, aimed to improve
    model performance while using limited computational and memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: To date, PEFT methods have not matched the accuracy of full fine-tuning Nikdan
    et al. ([2024](#bib.bib30)), highlighting the need for new approaches that can
    close this gap while still minimizing resource use. Additionally, most PEFT methods
    involve adding extra parameters, which increases computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address those issues and enhance the performance of efficiently trained
    LLMs, we introduce a novel PEFT method, GIFT-SW. This approach focuses on updating
    a small subset of salient weights while injecting noise into the non-salient weights.
    The development of this method is grounded in observations from previous studies
    and the related questions they raise, which we aim to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous research has shown that there is a small subset of salient weights
    which can significantly affect the effectiveness of post-training quantization
    (PTQ) Dettmers et al. ([2022](#bib.bib8), [2023](#bib.bib10)); Kim et al. ([2023](#bib.bib19))
    and pruning techniques Yin et al. ([2023](#bib.bib46)); Frantar and Alistarh ([2023](#bib.bib12));
    Sun et al. ([2023](#bib.bib40)). Moreover, [Gurnee et al.](#bib.bib15) identified
    a group of "universal neurons" that are critical to a model’s functionality, emphasizing
    the importance of selecting and updating these salient weights. Question 1: Does
    updating a small subset of salient weights is sufficient to adjust the model?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent studies have demonstrated that Perturbed Gradient Descent (PGD), with
    noise injections applied both before and after the gradient step, can stabilize
    convergence and help prevent overfitting Poole et al. ([2014](#bib.bib33)); Zhu
    et al. ([2018](#bib.bib48)); Jin et al. ([2021](#bib.bib18)). Question 2: Does
    Injecting Noise helps convergence?'
  prefs: []
  type: TYPE_NORMAL
- en: 'PGD is commonly employed to enhance model robustness by approximating the quantization
    process Shvetsov et al. ([2022](#bib.bib38)); Shin et al. ([2023](#bib.bib37));
    Défossez et al. ([2021](#bib.bib7)). This increased robustness can aid in maintaining
    the quality of the quantized model. Question 3: Does injecting noise helps robustness?'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting salient weights is a significant challenge, particularly in quantization
    and pruning, and it is central to our method. In our paper, we derive a general
    formulation for all previously established saliency metrics and present experiments
    to compare their effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of our work can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a novel PEFT method for pre-trained and quantized LLMs, called
    GIFT-SW. It is designed to fine-tune weights in salient columns while injecting
    Gaussian noise into non-salient weights, which are kept frozen during training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We generalize sensitivity metrics for identifying salient columns in pre-trained
    LLMs. We compare various novel and existing instances of the proposed general
    form and identify a new metric, which on average outperform previously studied
    in the literature metricsXiao et al. ([2023](#bib.bib44)); Lee et al. ([2024](#bib.bib22)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments demonstrate that GIFT-SW outperforms modern PEFT methods and full
    fine-tuning baselines across most zero-shot tasks. GIFT-SW for LLaMA models achieve
    comparable accuracy to the corresponding state-of-the-art TÜLU2 models, despite
    fine-tuning only 3% of the parameters and utilizing ten times less computational
    resources.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We demonstrate that GIFT-SW is more stable with respect to a size of training
    set compared with low-rank adapters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Parameter efficient fine-tuning of LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most popular method with high efficiency is LoRA Hu et al. ([2021](#bib.bib16)),
    which trains the low-rank adapters. Recent modifications to the method aim to
    improve the initialization of the adapters Liu et al. ([2024](#bib.bib26)) and
    enhance the low-rank representation of pre-trained weights by adding sparse adapters
    Nikdan et al. ([2024](#bib.bib30)). Another improvement of the learning capacity
    of LoRA is given by DoRA Liu et al. ([2024](#bib.bib26)), which fine-tunes magnitude
    and direction components of the pretrained weights. This method achieves considerable
    performance across various fine-tuning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Salient Weights in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The identification of salient weights¹¹1In our work, we use the terms salient
    weights and weight outliers interchangeably. is one of the main problems in weight
    pruning. Recently, several approaches have been proposed to identify such weights
    in LLMs, including SparseGPT Frantar and Alistarh ([2023](#bib.bib12)), Wanda Sun
    et al. ([2023](#bib.bib40)), and OWL Yin et al. ([2023](#bib.bib46)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Dettmers et al.](#bib.bib8)’s ([2022](#bib.bib8)) demonstrated that a small
    subset of outliers in input activations has a substantial impact on LLM performance,
    highlighting the relationship between the activation outliers and the salient
    weights. Many subsequent Post-Training Quantization (PTQ) methods used similar
    or identical pruning metrics to identify these salient weights Dettmers et al.
    ([2023](#bib.bib10)); Xiao et al. ([2023](#bib.bib44)); Lee et al. ([2024](#bib.bib22)).'
  prefs: []
  type: TYPE_NORMAL
- en: In our work, we generalize the identification metrics for salient weights by
    considering metrics from both the literature on pruning and quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Structured and Non-structured Salient Weights selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since salient weights account for only a few percent of all the weights, a straightforward
    approach to preserve them would be to store unstructured salient weights in a
    sparse matrix. Dettmers et al. ([2023](#bib.bib10)) demonstrated that this approach
    is computationally reasonable and leads to performance improvement. On the other
    hand, [Xiao et al.](#bib.bib44)’s ([2023](#bib.bib44)) revealed that outliers
    in activations are confined to a small fraction of weight channels, which was
    incorporated into SmoothQuant, where outlier columns are identified using a small
    calibration dataset. This concept is further developed in QUIK Ashkboos et al.
    ([2023](#bib.bib1)), where outlier columns are retained in full precision, while
    other columns are quantized using GPTQ (Frantar et al., [2022](#bib.bib13)). A
    similar procedure is used in OWQ Lee et al. ([2024](#bib.bib22)), but with an
    OBD-based metric LeCun et al. ([1989](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: Due to the lack of results in the literature on which approach brings better
    results, structured or unstructured salient weight selection, and motivated by
    computational efficiency mentioned in  Ashkboos et al. ([2023](#bib.bib1)), in
    our work we follow the second line of work with structured column-wise salient
    weight selection.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Noise Injections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we briefly describe Gaussian Noise Injections (GNI) and its
    benefits. Then, we show that the approximation of quantization noise and GNI are
    identical. Therefore, GNI can also benefit further model quantization. Therefor,
    to examine our third question, we sample noise relative to quantization levels,
    leaving other sampling options for future work.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Noise Injections (GNI). Perturbed Gradient Descent (PGD) is a family
    of methods that involve adding or multiplying weights with samples from some random
    distribution, during an optimization procedure. Gaussian noise injection (GNI)
    after the gradient step helps model to escape saddle points efficiently in non-convex
    optimization (Jin et al., [2021](#bib.bib18)). However, when Gaussian noise is
    injected before the gradient step, it helps model to escape from the spurious
    local optimum (Zhu et al., [2018](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta_{t+1}\leftarrow\theta_{t}-\tau(\nabla f(\theta_{t})+\xi)$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\ \theta_{t+1}\leftarrow\theta_{t}-\tau(\nabla f(\theta_{t}+\xi))$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\xi\sim\mathcal{N}(\mu,\,\sigma^{2})$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Moreover, practical benefits of noise injections are well documented in the
    literature and often can be discussed as regularization techniques Bishop ([1995](#bib.bib3));
    Srivastava et al. ([2014](#bib.bib39)); Camuto et al. ([2020](#bib.bib4)), methods
    to prompt adversarial robustenss Panda and Roy ([2021](#bib.bib32)) and to be
    used for data agumentation Moreno-Barea et al. ([2018](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: In our work we use GNI before evaluating the gradient. For this scenario, Orvieto
    et al. ([2023](#bib.bib31)) proposed to add noise only to one layer at training
    iteration to avoid variance explosion. It was empirically and theoretically demonstrated
    that GNI serves as a regularization. Liu et al. ([2023](#bib.bib25)) study fine-tuning
    of pre-trained Language Models with GNI. Authors propose first to learn layer-wise
    variance parameters for noise distributions and then to fine-tune the model by
    adding noise to all the weights. The obtained results showed that the approach
    is superior to independent layer-wise noise injections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization Noise Injections (QNI). Quantization aware training (QAT) of networks
    is applied to mitigate their accuracy degradation after quantization. However,
    uniform quantization ²²2For the reader not familiar with uniform quantization,
    we discuss it in more details in Section [A](#A1 "Appendix A Uniform quantization
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").
    $Q$ such that $\mathbf{\Omega}=Q(\mathbf{W})-\mathbf{W}$ Défossez et al. ([2021](#bib.bib7));
    Shvetsov et al. ([2022](#bib.bib38)); Shin et al. ([2023](#bib.bib37)). Thus,
    training models with QNI is exactly the same as employing PGD with GNI before
    evaluating the gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: Under some assumptions the noise $\mathbf{\Omega}$ typically yields improved
    outcomes Défossez et al. ([2021](#bib.bib7)); Shvetsov et al. ([2022](#bib.bib38)).
  prefs: []
  type: TYPE_NORMAL
- en: Although GNI is beneficial for model training there is no clear answer on how
    to choose noise parameters. Liu et al. ([2023](#bib.bib25)) determine noise parameters
    such that KL divergence between original and perturbed weights is minimized. Shin
    et al. ([2023](#bib.bib37)) identify parameters of the Gaussian distribution to
    resemble the weight distribution with a scale proportional to quantization step.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Straight Through Estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most popular QAT technique incorporating quantization operation into the
    traning process is Straight Through Estimation (STE)³³3More details on STE can
    be found in Section [C](#A3 "Appendix C Straight Through Estimator ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs"). Bengio et al.
    ([2013](#bib.bib2)); Shang et al. ([2023](#bib.bib36)), which basically re-parameterizes
    gradients. However, [Défossez et al.](#bib.bib7)’s ([2021](#bib.bib7)) demonstrated
    that STE has some disadvantages compared with QNI⁴⁴4Event though QNI and GNI are
    identical operations for consistency and clarity, in the case of quantization
    we will refer to this procedure as Quantization Noise Injections (QNI), as STE
    is biased and may cause weight oscillation between quantization steps. [Shin et al.](#bib.bib37)’s
    ([2023](#bib.bib37)) demonstrated that pretraining models for the following quantization
    with QNI instead of STE results in better performance. More technical details
    are provided in Section [C](#A3 "Appendix C Straight Through Estimator ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b69966161c71674487c72a55903caf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: GIFT-SW procedure follows Equation [2](#S2.E2 "In 2.4 Noise Injections
    ‣ 2 Related Work ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs"). We first sample some noise, relative to quantization levels, then,
    perform forward pass, and then update salient weights only. In GIFT-SW, quantization,
    pruning or tensor decomposition can be applied to non-salient weights and then,
    salient weights can be fine-tuned effectively without changing non-salient weights
    structure. In our experiments we select only 128 columns of salient weights, unless
    specified otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GIFT-SW consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify a fixed number of salient columns using a chosen sensitive metric,
    based on a small calibration set. This number remains consistent across all layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split columns of the matrices into subsets of salient columns and regular ones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, add noise to the weights in non-salient columns and update
    weights only in the salient columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Thus, the method depends on two main design choices: 1) how to choose salient
    columns and 2) the parameters of noise injections. We cover the choice of metrics
    in Section [3.1](#S3.SS1 "3.1 Generalizing parameter sensitivity metrics ‣ 3 Method
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").
    Noise injection details are provided in Section [3.2](#S3.SS2 "3.2 Quantization
    Noise Injection ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Generalizing parameter sensitivity metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several approaches have been proposed recently to identify weights sensitive
    to quantization Dettmers et al. ([2023](#bib.bib10)) or pruning Sun et al. ([2023](#bib.bib40)).
    We generalize them as metrics for sensitivity to perturbations, and by applying
    these metrics, we determine which columns are more susceptible to degradation.
    Therefore, we avoid adding noise to such columns and use them to fine-tune the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed sensitivity metric is written for a column $j$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{j}=\&#124;\mathbf{D}_{j}\&#124;_{\tau}\&#124;\mathbf{X}_{j}\&#124;_{\rho}^{\gamma},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{D}_{j}$ takes on one of the following values $1/2,1,2$ we utilize
    perturbations caused by quantization.⁵⁵5Optionally, one could use weight pruning
    as a source of perturbations or any other. That would lead to $\mathbf{D}_{j}=\mathbf{W}_{:,j}-Q(\mathbf{W}_{:,j})$
    corresponds to the weights subjected to uniform symmetric quantization (see Appendix
    [A](#A1 "Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input feature $\mathbf{X}$ are estimated for individual columns. Columns
    with the highest values are identified as the salient columns. Some details about
    the calibration dataset is described in Section [4.1](#S4.SS1 "4.1 Data ‣ 4 Experiments
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The metric given by Equation [4](#S3.E4 "In 3.1 Generalizing parameter sensitivity
    metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs") is closely related to those studied in the recent literature on quantization.
    In particular, the metric $\|\mathbf{X}\|_{\infty}$-th diagonal element of the
    Hessian matrix $\mathbf{H}$.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to Wanda, we use $l_{\infty}$ norm, the error for each channel includes
    all deviations between the quantized and original weights. Therefore, rare considerable
    errors can be mitigated by a large number of small deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Quantization Noise Injection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To improve our fine-tuning procedure with QNI, we avoid applying perturbations
    to sensitive weights. Therefore, after identifying columns that are sensitive
    to perturbations or salient during the fine-tuning stage, we inject quantization
    noise only into non-salient columns across all layers, as shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: The scale parameters of the Gaussian noise are determined by the quantization
    step sizes, which are computed for each layer prior to the training process.
  prefs: []
  type: TYPE_NORMAL
- en: For the weight matrix $\mathbf{W}$ is scaled with the quantization step size
    $\mathbf{\Delta}$ is given as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\mho(\mathbf{W})=\begin{cases}\mathbf{W_{[:,\textit{salient}]}},\\
    \mathbf{W_{[:,\textit{non-salient}]}}+\frac{1}{2}\mathrm{diag}(\mathbf{\Delta})\mathbf{\Omega}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases},$$ |  | (5) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathrm{diag}(\mathbf{\Delta})$.
  prefs: []
  type: TYPE_NORMAL
- en: Only weights of the salient columns $\mathbf{W_{[:,\textit{salient}]}}$ are
    frozen. We do not inject noise to salient weights since small perturbations in
    them can cause high model degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantization step size $\mathbf{\Delta}$-s row the scale factor $\Delta_{i}$
    is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Delta_{i}=\frac{\alpha_{i}}{2^{b-1}-1},$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $b$ is estimated by optimizing weight error through linear search as
    discussed in Appendix [A](#A1 "Appendix A Uniform quantization ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Equations [5](#S3.E5 "In 3.2 Quantization Noise Injection ‣ 3 Method
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs") and
    [6](#S3.E6 "In 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs"), the variance of the
    injected noise is determined by the distribution of non-salient weights across
    rows. We exclude salient columns from this distribution, as the salient weights
    may induce large quantization error and distort row-wise scale factors. This approach
    helps us to minimize the noise variance, which, in turn, leads to a reduction
    in the deviation of the non-salient weights during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By sampling noise in such way we can use it for quantization pre-training experiments
    discussed in Section [6.3](#S6.SS3 "6.3 Quantization Before and After Training
    ‣ 6 Ablation ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA2-7b | LLaMA2-13b | LLaMA3-8b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | TÜLU-V2-mix | OpenOrca | TÜLU-V2-mix | OpenOrca | TÜLU-V2-mix | OpenOrca
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FT | $71.97$ | $76.13$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $71.78$ | $75.91$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $72.03$ | $75.89$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{73.33}$ | $\mathbf{76.37}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Mean accuracy of LLaMA models fine-tuned with various instructive
    datasets and different methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bits | Method | LLaMA2-7b | LLaMA2-13b | LLaMA3-8b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4 bit | STE | $72.43$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $63.99$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{72.53}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 bit | STE | $69.82$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $62.91$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{71.00}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 bit | STE | $58.20$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $41.44$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{61.09}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Mean accuracy of quantized and then fine-tuned models. For fine-tuning
    we used TÜLU-V2-mix.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the experimental procedure used to test the performance
    of GIFT-SW compared to others.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following previous studies Nikdan et al. ([2024](#bib.bib30)); Hu et al. ([2021](#bib.bib16));
    Liu et al. ([2024](#bib.bib26)), we focus on the instruction tuning task. For
    this purpose, we use the TULU-V2-Mix as the main source of data Ivison et al.
    ([2023](#bib.bib17)), as it encompasses a wide range of instructions from different
    sources. This dataset has been filtered, contains a substantial amount of data
    without being too large, and models tuned to this set show superior performance.
    Additionally, we utilize the OpenOrca dataset Mukherjee et al. ([2023](#bib.bib29))
    to demonstrate that our method does not depend on a specific set of instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The sensitivity metrics to find salient columns are estimated based on 512 random
    sentences from the Pile validation dataset Xiao et al. ([2023](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We consider several baselines for both full precision and quantized experiments.
    All baselines are applied to LLaMA2-7b, LLaMA2-13b and LLaMA3-8b.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full precision version includes the choice of baselines, following recent studies
    Liu et al. ([2024](#bib.bib26)); Nikdan et al. ([2024](#bib.bib30)). We employ:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoRA is a widely used adapter-based method Hu et al. ([2021](#bib.bib16))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DoRA is modification of LoRA outperforming all current PEFT methods Liu et al.
    ([2024](#bib.bib26))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FT is full fine-tuning of all parameters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We do not include PEFT methods connected with prompt tuning, as they show worse
    performance compared to adapter-based methods Xu et al. ([2023](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantized version is presented by baselines of only weight quantization at
    $\{4,3,2\}$ bit-widths:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STE is quantization-aware fine-tuning of all parameters of a pre-trained model
    Bengio et al. ([2013](#bib.bib2)). During fine-tuning all parameters are trained,
    but 128 salient columns are updated in full-precision without quantization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QUIK + LoRA is an application of LoRA to the QUIK quantized model. Only low-rank
    adapters are trained, while the quantized weights and the salient weights are
    frozen.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: QUIK is a mixed-precision quantization method, that leverages GPTQ for quantization
    non-salient columns, while keeping the salient weight in full-precision Frantar
    et al. ([2022](#bib.bib13)); Ashkboos et al. ([2023](#bib.bib1)). Due to the techniques,
    QUIK achieves the highest performance among PTQ methods, such as GTPQ Frantar
    et al. ([2022](#bib.bib13)), AWQ Lin et al. ([2023](#bib.bib23)), SmoothQuant
    Xiao et al. ([2023](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation and Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We perform a comprehensive evaluation measuring zero-shot performance on HellaSwag
    Zellers et al. ([2019](#bib.bib47)), BoolQ Clark et al. ([2019](#bib.bib5)), WinoGrande
    Sakaguchi et al. ([2021](#bib.bib35)), PiQA Tata and Patel ([2003](#bib.bib41)),
    ARC-easy, and ARC-challenge Clark et al. ([2018](#bib.bib6)) using the LM Eval
    Harness Gao et al. ([2023](#bib.bib14)). The choice of baselines is similar to
    those in previous studies Egiazarian et al. ([2024](#bib.bib11)); Frantar et al.
    ([2022](#bib.bib13)); van Baalen et al. ([2024](#bib.bib42)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We demonstrate average accuracy across all the datasets, detailed per-dataset
    comparison can be found in Section [D](#A4 "Appendix D Detailed Benchmark Results
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Compute Budget
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In all experiments, the number of salient columns in the models is fixed at
    $128$ iterations are performed within one epoch with no instruction repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Training Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training was performed with 4 GPUs ( 40 GB each) for 500 iterations. The
    batch size is 128 for 7b models and 64 for 13b models. For baseline methods, the
    learning rate was set to $3\times 10^{-5}$ for the LLaMA3 model. We experimented
    with different learning rates and found these to be the most beneficial for baseline
    methods. We used a cosine annealing scheduler with the warmup ratio of 0.03\.
    The LoRA and DoRA alpha and dropout values were as specified in the original papers,
    and the rank was set to 64 to match the number of trainable parameters in our
    method. Thus, the number of trainable parameters is 160M for LLaMA2-7b, 250M for
    LLaMA2-13b, 167M for LLaMA3-8b.
  prefs: []
  type: TYPE_NORMAL
- en: For our method, the learning rate was set to $1\times 10^{-4}$ of the LLaMA3
    model. We fixed the number of salient columns at 128, such that the number of
    trainable parameters is 174M for LLaMA2-7b, 272M for LLaMA2-13b, and 176M for
    LLaMA3-8b.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of full fune-tuning with the noise injection, the learning rate
    was set to $3\times 10^{-5}$ for LLaMA2 & 3 models, correspondingly.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA2-7b | LLaMA2-13b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Performance | Compute^† | Performance | Compute^† |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2 | $73.49$ | $13$K |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-DPO | $73.8$ | $13$K |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $73.33$ | $272$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of Performance and Compute for LLaMA2 Models using our
    fine-tuning method versus original TÜLU2 models. Note: Compute values are represented
    as Trainable Parameters / Iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present the results of our computational experiments and
    answer the questions posed in Section [1](#S1 "1 Introduction ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs"). In short, our results
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results confirm that fine-tuning a subset of salient weights produces results
    comparable to those obtained using low-rank adapters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Q2:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noise injections lead to improved model performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Q3:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could not confirm that models trained with noise injections are more robust
    to further degradation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.1 Full Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The average performance across evaluation benchmarks for full precision models
    is presented in Table [1](#S3.T1 "Table 1 ‣ 3.2 Quantization Noise Injection ‣
    3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for
    LLMs"). GIFT-SW generally shows superior metrics across most models and instruction
    sets. However, we observe slight underperformance in LLaMA3 on the OpenOrca subset,
    where full training proves superior. This issue likely stems from the choice of
    learning rate and schedule, which can impact the tuning of outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Quantized Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present the averaged performance of models quantized with different precision
    (4, 3, 2) in Table [2](#S3.T2 "Table 2 ‣ 3.2 Quantization Noise Injection ‣ 3
    Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").
    For 4 and 3 bits GIFT-SW achieves comparable quality with STE, however, latter
    one requires significantly more compute. In the 2-bit setting, GIFT-SW shows a
    substantial quality improvement, surpassing the second-ranked model by over 5
    points.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Comparison with TÜLU2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compare GIFT-SW with TÜLU2 models Ivison et al. ([2023](#bib.bib17)), which
    are LLaMA2 models fine-tuned using a combination of instructions and DPO Rafailov
    et al. ([2023](#bib.bib34)). These models are among the top-performing LLaMA2
    modifications but demand significant computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [3](#S4.T3 "Table 3 ‣ 4.5 Training Details ‣ 4 Experiments ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs"), we show that
    by applying GIFT-SW with significantly lower computational budget (a smaller number
    of parameters and iterations) we achieve comparable results for LLaMA2-7b and
    outperform TÜLU2 for 13b.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Scaling Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We perform experiments to explore the performance of GIFT-SW and baselines
    with scaling data using LLaMA2 and LLaMA3 models. The results reported in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs") show that while LoRA and DoRA exhibit unstable performance
    with scaling data, our method and full fine-tuning are more stable. Moreover,
    our method consistently ranks first across nearly all data budgets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | 4 bit | 3 bit | 2 bit |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Salient FT | $72.82$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pre-GIFT-SW | $\mathbf{73.15}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Post-GIFT-SW | $72.53$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Mean performance for quantized models with or without applying GIFT-SW
    before or after quantization, results are demonstrated for LLaMA2-7b model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bits | Model | $\&#124;\mathbf{D}_{j}\&#124;_{2}^{2}\&#124;\mathbf{X}_{j}\&#124;_{2}^{2}$
    | $\&#124;\mathbf{D}_{j}\&#124;_{\infty}\&#124;\mathbf{X}_{j}\&#124;_{\infty}^{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4 bit | LLaMA2-7b | $\mathbf{69.86}$ | $69.52$ |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-7b | $72.94$ | $72.78$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | $72.92$ | $72.56$ |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-13b | $75.12$ | $75.17$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 bit | LLaMA2-7b | $67.50$ | $67.86$ |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-7b | $70.91$ | $70.88$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | $71.92$ | $71.45$ |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-13b | $\mathbf{74.33}$ | $74.31$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 bit | LLaMA2-7b | $45.86$ | $\mathbf{46.83}$ |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-7b | $\mathbf{54.84}$ | $48.20$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | $57.07$ | $56.73$ |'
  prefs: []
  type: TYPE_TB
- en: '| TÜLU2-13b | $59.62$ | $59.39$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Performance of LLaMA2 and TÜLU2 models after QUIK quantization with
    salient columns selected via various metrics. Weight perturbation is given by
    $\mathbf{D}_{j}=\mathbf{W}_{:,j}-Q(\mathbf{W}_{:,j})$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Outliers FT | Full FT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Noise | w/o Noise | w/ Noise | w/o Noise |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7b | $\mathbf{73.33}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | $\mathbf{75.93}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8b | $\mathbf{76.37}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Mean Performance of LLaMA models with and without Noise Injection
    for outlier fine-tuning and full model fine-tuning'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Ablation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Comparison sensitivity metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We study sensitivity metrics with respect to different noise levels (various
    perturbation magnitudes), which translate into varying quantization precision.
    In this experiment, the non-salient weights of LLaMA2 and TÜLU2 with 7B and 13B
    parameters. Models are quantized with QUIK, the salient weights are not updated.
    We select 128 columns of salient weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean results for zero-shot tasks in Table [5](#S5.T5 "Table 5 ‣ 5.4 Scaling
    Properties ‣ 5 Results ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs") show that for most precisions, the best performance is achieved
    with salient columns identified by Equation [4](#S3.E4 "In 3.1 Generalizing parameter
    sensitivity metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs") with $\gamma=1,\rho=\infty,\tau=\infty$ norm of
    the input feature (the OWQ metric) show better performance only for TÜLU2 quantized
    to 3 and 2 bits. Choosing salient columns solely by the input features (the QUIK
    metric) leads to underperformance, especially for 2 bit. Therefore, identifying
    salient columns sensitive to quantization noise requires considering both the
    weight quantization error and the maximum values of input activation.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the results, we chose the best-performing sensitivity metric with $\gamma=1,\rho=\infty,\tau=\infty$.
    However, the results do not reveal a clear rule for selecting the optimal sensitivity
    metric, as performance varies across different bit-widths and models with no discernible
    pattern. This remains an area for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Noise Injection Impact
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ablate the importance of QNI in the full-precision setting, we measure the
    mean performance of LLaMA2 models with and without noise injections for both salient
    columns fine-tuning and full fine-tuning. In the latter case, the noise is applied
    to the entire weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results in Table [6](#S5.T6 "Table 6 ‣ 5.4 Scaling Properties ‣ 5 Results
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs") show
    that QNI consistently enhances the performance of outlier fine-tuning. Although
    QNI can reduce performance when applied to the entire network, it still benefits
    LLaMA3-8b. Notably, outlier fine-tuning outperforms full fine-tuning, but only
    when QNI is used.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Quantization Before and After Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From studies related to QAT, it is known that pre-training a model with noise
    injection enables to improve its predictive capabilities after quantization Défossez
    et al. ([2021](#bib.bib7)); Shvetsov et al. ([2022](#bib.bib38)). Based on those
    observations, in this section we examine the performance of the quantized LLaMA2-7b
    after fine-tuning full precision salient columns in several settings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-GIFT-SW. Applying GIFT-SW prior to the quantization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-GIFT-SW. Applying GIFT-SW after the quantization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salient FT. Fine-tuning salient columns after quantization with no noise injected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the case of the pre-training, the bit-width for the model quantization corresponds
    to the noise level injected during the training. For the post-training, the noise
    injection is always performed at 4 bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.4 Scaling Properties ‣ 5 Results ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs") presents the average
    scores achieved by the models across evaluation benchmark. In the case of 4 bit
    quantization the Pre-GIFT-SW model considerable outperforms other models. But
    in the case of 3 and 2 bits, fine-tuning salient columns after quantization enables
    to achieve quantized models better generative capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: It can be explained by significant deviation of the quantized weights from their
    original values that is induced by the extremely low-bit quantization. As a result,
    the interrelations between the salient weights and the quantized weights are disrupted,
    and the positive effect of pre-training disappears. However, post-training of
    the salient weight enables to form them new relations with other weights, so the
    model partially recovers its generative capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Also it can be observed that application of Post-GIFT-SW and Salient FT to model
    quantized in 3 bit gives the similar scores. But in the case of 2 bit quantization,
    the noise injection improves the fine-tuning of the quantized model.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce GIFT-SW, a parameter-efficient fine-tuning method
    that trains only weights in a small subset of salient columns while injecting
    quantization noise into the frozen weights. GIFT-SW proves to be superior to previous
    fine-tuning strategies in both full precision and quantized settings, requiring
    less compute budget. In data scaling experiments, GIFT-SW demonstrates greater
    stability than previous PEFT methods and outperforms both PEFT and full fine-tuning
    across nearly all data budgets. Our ablation studies show that QNI is beneficial
    but only with salient weights. Although GIFT-SW outperforms previous methods,
    further research is needed to determine how to maximize its performance in quantized
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: We generalize the criterion for selecting salient columns from previous studies
    and empirically compare various parameters. Our experiments show that while some
    criteria perform better than others, none emerge as a clear dominant choice. This
    significant finding underscores the need for further research to refine these
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We find the main limitations of our work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We report results of GIFT-SW exclusively for LLaMA models. Currently, numerous
    open-source pre-trained LLMs with high generative capabilities are available.
    However, LLaMA models are the most commonly chosen for studying the efficiency
    of modern PEFT and quantization methods. Despite the architectural similarities
    among most LLMs, future experiments with different models are necessary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For quantizing models, we use only the GPTQ method, which is widely used for
    mixed-precision quantization of LLMs. This method improves the performance of
    quantized models by aggregating quantization error into columns stored in full
    precision. However, GIFT-SW can be easily integrated with other methods, such
    as conventional RTN or QuantEase.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Experiments with GIFT-SW report results for salient columns selected using
    the sensitivity metric ([4](#S3.E4 "In 3.1 Generalizing parameter sensitivity
    metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs")) with $\gamma=1$. Our proposed metric, based on our analysis, shows
    high sensitivity of the salient columns to quantization in most LLaMA2 cases.
    However, other sensitivity metrics may yield better performance for GIFT-SW and
    mixed-precision quantization in different LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Noise parameters for fine-tuning the salient weights are determined using the
    QNI approach. However, other noise distributions may also enhance the fine-tuning
    process. Identifying the optimal noise distribution is beyond the scope of this
    paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this study, we focus on developing the GIFT-SW algorithm for effective fine-tuning
    of LLMs, but we do not provide computationally efficient implementations of CUDA
    kernels for the algorithm. In the future, CUDA kernels for GIFT-SW can be developed
    based on the code from QUIK Ashkboos et al. ([2023](#bib.bib1)) and OWQ Lee et al.
    ([2024](#bib.bib22)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train GIFT-SW with only a few fine-tuning instruction sets, selected for
    their size and high benchmark results in previous studies. However, expanding
    the number of fine-tuning sets could make the experiments more comprehensive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We evaluate our method using six distinct benchmarks inherited from various
    previous studies. In future research, it would be beneficial to include more benchmarks
    to gain additional insights.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9 Potential Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GIFT-SW method poses risks similar to those of any PEFT method. For example,
    it omits explicit safety training measures, so could be applied to fine-tune LLMs
    for generating harmful content. Also it can be applied to tailor LLMs to tailor
    highly specific and potentially dangerous outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The work was supported by the Analytical center under the RF Government (subsidy
    agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ashkboos et al. (2023) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023. Towards
    end-to-end 4-bit inference on generative large language models. *arXiv preprint
    arXiv:2310.09259*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bishop (1995) Chris M Bishop. 1995. Training with noise is equivalent to tikhonov
    regularization. *Neural computation*, 7(1):108–116.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camuto et al. (2020) Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J
    Roberts, and Chris C Holmes. 2020. Explicit regularisation in gaussian noise injections.
    *Advances in Neural Information Processing Systems*, 33:16603–16614.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Défossez et al. (2021) Alexandre Défossez, Yossi Adi, and Gabriel Synnaeve.
    2021. Differentiable model compression via pseudo quantization noise. *arXiv preprint
    arXiv:2104.09987*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. *Advances in
    Neural Information Processing Systems*, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Ruslan A Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. 2023. Spqr: A sparse-quantized representation for near-lossless
    llm weight compression. In *The Twelfth International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egiazarian et al. (2024) Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. Extreme compression of large
    language models via additive quantization. *arXiv preprint arXiv:2401.06118*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    massive language models can be accurately pruned in one-shot. In *Proceedings
    of the 40th International Conference on Machine Learning*, pages 10323–10337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gurnee et al. (2024) Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah,
    Qinyi Sun, Will Hathaway, Neel Nanda, and Dimitris Bertsimas. 2024. Universal
    neurons in gpt2 language models. *arXiv preprint arXiv:2401.12181*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ivison et al. (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan
    Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith,
    Iz Beltagy, and Hannaneh Hajishirzi. 2023. [Camels in a changing climate: Enhancing
    lm adaptation with tulu 2](https://arxiv.org/abs/2311.10702). *Preprint*, arXiv:2311.10702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2021) Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and
    Michael I Jordan. 2021. On nonconvex optimization for machine learning: Gradients,
    stochasticity, and saddle points. *Journal of the ACM (JACM)*, 68(2):1–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Komatsuzaki (2019) Aran Komatsuzaki. 2019. One epoch is all you need. *arXiv
    preprint arXiv:1906.06669*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain
    damage. *Advances in neural information processing systems*, 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2024) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok
    Park. 2024. Owq: Outlier-aware weight quantization for efficient fine-tuning and
    inference of large language models. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, volume 38, pages 13355–13364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2023.
    Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017) Xiaofan Lin, Cong Zhao, and Wei Pan. 2017. Towards accurate
    binary convolutional neural network. *Advances in neural information processing
    systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson,
    and Rongrong Wang. 2023. Pac-tuning: Fine-tuning pretrained language models with
    pac-driven perturbed gradient descent. *arXiv preprint arXiv:2310.17588*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov,
    Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024. Dora: Weight-decomposed
    low-rank adaptation. *arXiv preprint arXiv:2402.09353*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreno-Barea et al. (2018) Francisco J Moreno-Barea, Fiammetta Strazzera, José M
    Jerez, Daniel Urda, and Leonardo Franco. 2018. Forward noise adjustment scheme
    for data augmentation. In *2018 IEEE symposium series on computational intelligence
    (SSCI)*, pages 728–734\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moskvoretskii et al. (2024) Viktor Moskvoretskii, Alexander Panchenko, and Irina
    Nikishina. 2024. [Are large language models good at lexical semantics? a case
    of taxonomy learning](https://aclanthology.org/2024.lrec-main.133). In *Proceedings
    of the 2024 Joint International Conference on Computational Linguistics, Language
    Resources and Evaluation (LREC-COLING 2024)*, pages 1498–1510, Torino, Italia.
    ELRA and ICCL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar,
    Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. [Orca: Progressive learning
    from complex explanation traces of gpt-4](https://arxiv.org/abs/2306.02707). *Preprint*,
    arXiv:2306.02707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nikdan et al. (2024) Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh. 2024.
    Rosa: Accurate parameter-efficient fine-tuning via robust adaptation. *arXiv preprint
    arXiv:2401.04679*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orvieto et al. (2023) Antonio Orvieto, Anant Raj, Hans Kersting, and Francis
    Bach. 2023. Explicit regularization in overparametrized models via noise injection.
    In *International Conference on Artificial Intelligence and Statistics*, pages
    7265–7287\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panda and Roy (2021) Priyadarshini Panda and Kaushik Roy. 2021. Implicit adversarial
    data augmentation and robustness with noise-based learning. *Neural Networks*,
    141:120–132.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poole et al. (2014) Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. 2014.
    Analyzing noise in autoencoders and deep networks. *arXiv preprint arXiv:1406.1831*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. [Direct preference optimization:
    Your language model is secretly a reward model](https://arxiv.org/abs/2305.18290).
    *Preprint*, arXiv:2305.18290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. (2023) Yuzhang Shang, Zhihang Yuan, and Zhen Dong. 2023. Pb-llm:
    Partially binarized large language models. In *The Twelfth International Conference
    on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2023) Juncheol Shin, Junhyuk So, Sein Park, Seungyeop Kang, Sungjoo
    Yoo, and Eunhyeok Park. 2023. Nipq: Noise proxy-based integrated pseudo-quantization.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 3852–3861.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shvetsov et al. (2022) Egor Shvetsov, Dmitry Osin, Alexey Zaytsev, Ivan Koryakovskiy,
    Valentin Buchnev, Ilya Trofimov, and Evgeny Burnaev. 2022. Quantnas for super
    resolution: searching for efficient quantization-friendly architectures against
    quantization noise. *arXiv preprint arXiv:2208.14839*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent
    neural networks from overfitting. *The journal of machine learning research*,
    15(1):1929–1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. In *The Twelfth
    International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tata and Patel (2003) Sandeep Tata and Jignesh M Patel. 2003. Piqa: An algebra
    for querying protein data sets. In *15th International Conference on Scientific
    and Statistical Database Management, 2003.*, pages 141–150\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van Baalen et al. (2024) Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter
    Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough.
    2024. Gptvq: The blessing of dimensionality for llm quantization. *arXiv preprint
    arXiv:2402.15319*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Widrow et al. (1996) Bernard Widrow, Istvan Kollar, and Ming-Chang Liu. 1996.
    Statistical theory of quantization. *IEEE Transactions on instrumentation and
    measurement*, 45(2):353–361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and
    Fu Lee Wang. 2023. Parameter-efficient fine-tuning methods for pretrained language
    models: A critical review and assessment. *arXiv preprint arXiv:2312.12148*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023) Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang,
    Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. 2023.
    Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms
    to high sparsity. In *Conference on Parsimony and Learning (Recent Spotlight Track)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2018) Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
    2018. The anisotropic noise in stochastic gradient descent: Its behavior of escaping
    from sharp minima and regularization effects. *arXiv preprint arXiv:1803.00195*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Uniform quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While non-uniform quantization may lead to higher compression rates, in our
    work we focus on uniform quantization since it widely used in efficient PTQ methods
    such as GPTQ, QUIK, OWQ Frantar et al. ([2022](#bib.bib13)); Ashkboos et al. ([2023](#bib.bib1));
    Lee et al. ([2024](#bib.bib22)). Quantization is a mapping that converts a range
    of full-precision values into a discrete range of values allowing usage of integer
    arithmetic and reduced memory consumption. For example, Fig. [3](#A1.F3 "Figure
    3 ‣ Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs") depicts a mapping with the quantization scale size
    $\Delta=\frac{1}{4}$ into integer values.'
  prefs: []
  type: TYPE_NORMAL
- en: In our work we apply uniform symmetric quantization with the row-wise quantization
    step size $\mathbf{\Delta}$ as below
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q_{\text{min}}=-2^{b-1},\quad q_{\text{max}}=2^{b-1}-1$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{clamp}(x;q_{\text{min}},q_{\text{max}})=\max(q_{\text{min}},\min(x,q_{\text{max}}))$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{\Delta}=(\Delta_{1},\ldots,\Delta_{n})^{\mathrm{T}},\quad\Delta_{i}=\frac{\alpha_{i}}{q_{\text{max}}}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{W}^{\text{int}}_{i,:}=\text{clamp}\left(\left\lfloor\frac{\mathbf{W}_{i,:}}{\Delta_{i}}\right\rfloor;q_{\text{min}},q_{\text{max}}\right)$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{W}\approx Q(\mathbf{W})=\mathrm{diag}(\mathbf{\Delta})\mathbf{W}^{\text{int}}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\Delta_{i}$ denotes the matrix of the quantized weights, $\mathrm{diag}(\mathbf{\Delta})$
    is found for each row by performing linear grid search over the interval $[0,\max(\mathbf{W}_{i,:})]$
    row . The search is conducted to minimize layer-wise mean squared error between
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{argmin}_{\mathbf{\Delta}}\&#124;\mathbf{W}-Q(\mathbf{W})\&#124;_{2}^{2},$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: <svg id="A1.F3.pic1" class="ltx_picture ltx_centering" height="254.6" overflow="visible"
    version="1.1" width="297.26"><g transform="translate(0,254.6) matrix(1 0 0 -1
    0 0) translate(19.37,0) translate(0,21.37) matrix(1.0 0.0 0.0 1.0 -19.37 -21.37)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(19.37,0) translate(0,21.37)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 54 -16.76)"
    fill="#000000" stroke="#000000"><foreignobject width="26.91" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0.25$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -14.76 51.58)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    clip-path="url(#pgfcp1)"><g transform="matrix(1.0 0.0 0.0 1.0 130.44 175.94)"
    fill="#0000FF" stroke="#0000FF" color="#FF0000"><foreignobject width="16.14" height="9.46"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\Delta$</foreignobject></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Uniform quantization step function with real valued one dimensional
    $w$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Details of LLMs quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For only weight quantization of LLaMA and TÜLU2 models models, we apply QUIK
    implementation of mixed-precision GPTQ method Ashkboos et al. ([2023](#bib.bib1));
    Frantar et al. ([2022](#bib.bib13)). We isolate 128 salient columns in full-precision.
    Non-salient columns are subjected to uniform symmetric quantization, as discussed
    in Appendix [A](#A1 "Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise
    Injected Fine-Tuning of Salient Weights for LLMs"). The salient columns are identified
    through sensitive metrics described in Section [3.1](#S3.SS1 "3.1 Generalizing
    parameter sensitivity metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs"). The Hessian matrix for the GPTQ method is computed
    on 128 random samples of the Wikitext-2 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Straight Through Estimator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'STE can be described in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain quantized weights $Q(\mathbf{W})$, which is usually is non differentiable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute gradients at quantized weights $Q(\mathbf{W})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: STE makes a particular choice of a quantization function to obtain the discrete
    weights from the real-valued weights. This approximation can be justified in some
    settings (Lin et al., [2017](#bib.bib24)) but in general the reasons behind its
    effectiveness are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Detailed Benchmark Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we report detailed benchmark results for LLaMA 2 & 3 after
    training with different methods. Tables [7](#A4.T7 "Table 7 ‣ Appendix D Detailed
    Benchmark Results ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs"), [8](#A4.T8 "Table 8 ‣ Appendix D Detailed Benchmark Results ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs") present accuracy
    metrics which are achieved by the full-precision models after fine-tuning on the
    TÜLU-V2-mix and OpenOrca subsets. Corresponding mean values are listed in Table
     [1](#S3.T1 "Table 1 ‣ 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs"). Tables present
    accuracy metrics which are achieved by quantized in 4, 3, 2 bits models after
    fine-tuning on the TÜLU-V2-mix subset. Corresponding mean values are listed in
    Table  [2](#S3.T2 "Table 2 ‣ 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7b | FP | $78.65$ | $48.63$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $80.28$ | $47.95$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $81.93$ | $48.89$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{82.63}$ | $\mathbf{49.91}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | FP | $83.27$ | $53.67$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $81.10$ | $51.28$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $81.01$ | $51.54$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{84.22}$ | $\mathbf{55.38}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8b | FP | $83.64$ | $55.72$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $83.30$ | $56.06$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $83.61$ | $55.63$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{83.88}$ | $\mathbf{57.00}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: LLaMA models performance fine-tuned with TÜLU-V2-mix subset'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7b | FT | $80.03$ | $48.72$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $78.81$ | $46.59$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $78.78$ | $46.93$ |'
  prefs: []
  type: TYPE_TB
- en: '| Our Best | $\mathbf{82.51}$ | $\mathbf{48.89}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | FT | $82.66$ | $54.78$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $81.68$ | $51.11$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $81.65$ | $51.19$ |'
  prefs: []
  type: TYPE_TB
- en: '| Our Best | $\mathbf{85.44}$ | $\mathbf{56.48}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8b | FT | $\mathbf{84.37}$ | $\mathbf{57.85}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | $82.84$ | $55.54$ |'
  prefs: []
  type: TYPE_TB
- en: '| DoRA | $82.63$ | $55.46$ |'
  prefs: []
  type: TYPE_TB
- en: '| Our Best | $84.34$ | $57.76$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: LLaMA models performance fine-tuned with OpenOrca'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bits | Model | Method | Benchmarks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4 bit | LLaMA2-7b | STE | $80.21$ | $48.55$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $68.96$ | $37.20$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{82.78}$ | $\mathbf{50.00}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | STE | $\mathbf{84.77}$ | $\mathbf{53.67}$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $74.89$ | $50.17$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $84.65$ | $53.50$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8b | STE | $81.59$ | $54.27$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $82.51$ | $51.62$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{83.15}$ | $\mathbf{55.20}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 bit | LLaMA2-7b | STE | $76.79$ | $45.65$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $63.88$ | $38.74$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{80.46}$ | $\mathbf{47.35}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | STE | $83.33$ | $\mathbf{53.24}$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $82.02$ | $48.21$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{85.44}$ | $51.54$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8b | STE | $75.87$ | $49.32$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $78.72$ | $50.60$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{80.31}$ | $\mathbf{52.99}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 bit | LLaMA2-7b | STE | $68.47$ | $32.17$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $62.11$ | $26.45$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{71.90}$ | $\mathbf{34.90}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13b | STE | $73.09$ | $36.09$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $59.36$ | $27.82$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{81.99}$ | $\mathbf{43.17}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8b | STE | $60.46$ | $27.65$ |'
  prefs: []
  type: TYPE_TB
- en: '| QUIK + LORA | $64.68$ | $32.17$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIFT-SW | $\mathbf{74.13}$ | $\mathbf{37.88}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Performance of quantized LLaMA models fine-tuned with TÜLU-V2-mix
    subset'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E TÜLU-V2-mix subset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [4](#A5.F4 "Figure 4 ‣ Appendix E TÜLU-V2-mix subset ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs") shows number of examples
    in datasets included in the TÜLU-V2-mix subset, which is used for fine-tuning
    experiments presented in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ab859181fb8781d74d000a1d7be6c04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Number of examples in datasets included in TÜLU-V2-mix subset'
  prefs: []
  type: TYPE_NORMAL
