- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11592](https://ar5iv.labs.arxiv.org/html/2402.11592)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yihua Zhang    Pingzhi Li    Junyuan Hong    Jiaxiang Li    Yimeng Zhang   
    Wenqing Zheng    Pin-Yu Chen    Jason D. Lee    Wotao Yin    Mingyi Hong    Zhangyang
    Wang    Sijia Liu    Tianlong Chen
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the evolving landscape of natural language processing (NLP), fine-tuning
    pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like
    SGD and Adam has become standard. Yet, as LLMs grow in size, the substantial memory
    overhead from back-propagation (BP) for FO gradient computation presents a significant
    challenge. Addressing this issue is crucial, especially for applications like
    on-device training where memory efficiency is paramount. This paper proposes a
    shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing
    memory costs during LLM fine-tuning, building on the initial concept introduced
    by Malladi et al. ([2023](#bib.bib52)). Unlike traditional ZO-SGD methods, our
    work expands the exploration to a wider array of ZO optimization techniques, through
    a comprehensive, first-of-its-kind benchmarking study across five LLM families
    (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning
    schemes. Our study unveils previously overlooked optimization principles, highlighting
    the importance of task alignment, the role of the forward gradient method, and
    the balance between algorithm complexity and fine-tuning performance. We further
    introduce novel enhancements to ZO optimization, including block-wise descent,
    hybrid training, and gradient sparsity. Our study offers a promising direction
    for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all
    our experiments are at [https://github.com/ZO-Bench/ZO-LLM](https://github.com/ZO-Bench/ZO-LLM).
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning pre-trained large language models (LLMs) has become the de-facto
    standard in the current paradigms of natural language processing (NLP) (Raffel
    et al., [2023](#bib.bib60); Sanh et al., [2022](#bib.bib65)). First-order (FO)
    optimizers, e.g., SGD (Amari, [1993](#bib.bib1)) and Adam (Kingma & Ba, [2014](#bib.bib41)),
    have been the predominant choices for LLM fine-tuning. However, as LLMs continue
    to scale, they encounter significant memory overhead due to the back-propagation
    (BP) required for FO gradient computation. For example, computing the gradient
    of the LLM OPT-13B requires $12\times$ more memory cost than the model inference.
    This leads to the challenge of achieving memory-efficient fine-tuning in LLMs.
    Advancements in addressing this challenge could also facilitate technological
    breakthroughs in related areas, such as on-device training, where memory efficiency
    is in high demand (Han et al., [2015](#bib.bib26); Zhu et al., [2023](#bib.bib88)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To enhance memory efficiency, an emerging solution is to replace a BP-required
    FO optimization method with a BP-free optimizer during LLM fine-tuning. This was
    initially proposed by Malladi et al. ([2023](#bib.bib52)), where the FO gradient
    is approximated using a finite difference of function values. Despite its new
    application to LLM fine-tuning, the underlying optimization principle used in
    Malladi et al. ([2023](#bib.bib52)) is commonly known as zeroth-order (ZO) optimization,
    and the function value-based gradient estimate is referred to as the ZO gradient
    estimate (Flaxman et al., [2005](#bib.bib20); Nesterov & Spokoiny, [2017](#bib.bib55);
    Duchi et al., [2015](#bib.bib18); Ghadimi & Lan, [2013](#bib.bib23); Liu et al.,
    [2020](#bib.bib47)). Malladi et al. ([2023](#bib.bib52)) employed the classical
    ZO stochastic gradient descent (ZO-SGD) algorithm (Ghadimi & Lan, [2013](#bib.bib23)),
    termed MeZO, to fine-tune the pre-trained LLMs and leveraged the BP-free characteristics
    of ZO optimization to reduce memory costs. However, from the perspective of ZO
    optimization, in addition to ZO-SGD, many other ZO optimization methods have not
    yet been explored in the context of LLM fine-tuning. Thus, it remains elusive
    whether there are potential improvements in accuracy and/or efficiency that can
    be achieved through a benchmarking study of ZO optimization for LLM fine-tuning.
    This yields the primary question to be explored:'
  prefs: []
  type: TYPE_NORMAL
- en: (Q) Can we establish a benchmark for ZO optimization in LLM fine-tuning, explore
    the overlooked optimization principles, and advance the current state of the art?
  prefs: []
  type: TYPE_NORMAL
- en: To address (Q), our work introduces several key innovations compared to the
    most relevant work (Malladi et al., [2023](#bib.bib52)). We explore a broader
    range of ZO optimization methods beyond ZO-SGD and examine various task and model
    types, and evaluation metrics. We conduct a detailed comparative analysis of different
    ZO optimization methods, shedding light on the often-overlooked forward gradient
    method (Ren et al., [2022](#bib.bib62)) and other ZO optimization techniques in
    LLM fine-tuning. This benchmarking study helps reveal the pros and cons of these
    methods in accuracy and efficiency. Extended from the gained insights, we propose
    to further improve ZO optimization-based LLM fine-tuning using techniques of block-wise
    descent, hybrid ZO and FO training, and gradient sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our key contributions are listed below.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ tasks of varying complexities, and $5$ fine-tuning schemes, covering
    both full-parameter and parameter-efficient fine-tuning (PEFT) approaches.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Assisted by our benchmark, we reveal a range of previously overlooked
    optimization principles and insights for LLM fine-tuning with ZO optimization.
    These include the significance of aligning tasks to enhance ZO optimization, the
    role of forward gradient as an LLM fine-tuning baseline, and the trade-offs between
    algorithm complexity, fine-tuning accuracy, query and memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ In addition to a holistic assessment of existing ZO optimization methods
    for LLM fine-tuning, we introduce novel enhancements to ZO optimization, including
    block-wise ZO optimization, hybrid ZO and FO fine-tuning, and sparsity-induced
    ZO optimization. These proposed techniques aim to improve the accuracy of ZO LLM
    fine-tuning while maintaining memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parameter-efficient fine-tuning (PEFT). Early efforts (Houlsby et al., [2019](#bib.bib30);
    Lin et al., [2020](#bib.bib44)) involved inserting trainable adapters, which are
    compact feed-forward networks, between the layers of the pre-trained model. More
    recently, various PEFT strategies have been proposed. For instance, Adapter-based
    methods (Houlsby et al., [2019](#bib.bib30); Chen et al., [2022](#bib.bib11);
    Luo et al., [2023](#bib.bib51); Karimi Mahabadi et al., [2021](#bib.bib37); Pfeiffer
    et al., [2020](#bib.bib58)) insert a few tunable yet highly compact modules into
    pre-trained models. LoRA (Hu et al., [2021a](#bib.bib31)) employs trainable low-rank
    weight perturbations to the pre-trained model, effectively reducing the required
    number of fine-tuning parameters. Prompt-based learning (Gao et al., [2020](#bib.bib22);
    Hu et al., [2021b](#bib.bib32); Tan et al., [2021](#bib.bib72)) has demonstrated
    effectiveness in various NLP tasks. Additionally, methods like prompt tuning (Lester
    et al., [2021](#bib.bib42)) and prefix tuning (Li & Liang, [2021](#bib.bib43))
    incorporate learnable continuous embeddings into the model’s hidden states to
    condition the frozen model for specific downstream tasks. The following work (Liu
    et al., [2021](#bib.bib48)) demonstrates its applicability on various model scales.
    While these state-of-the-art PEFT techniques have significantly reduced the number
    of parameters required for fine-tuning, they still incur memory costs associated
    with caching numerous activations due to the use of back-propagation (BP) (Malladi
    et al., [2023](#bib.bib52)).
  prefs: []
  type: TYPE_NORMAL
- en: Zeroth-order optimization. Zeroth-order (ZO) optimization is a technique that
    uses finite differences to estimate gradients. Such algorithms utilize function
    value oracle only, yet share similar algorithm structure with first-order (FO)
    gradient-based counterpart methods. ZO optimization usually enjoys provable (dimension-dependent)
    convergence guarantees, as discussed in various works (Flaxman et al., [2005](#bib.bib20);
    Nesterov & Spokoiny, [2017](#bib.bib55); Duchi et al., [2015](#bib.bib18); Ghadimi
    & Lan, [2013](#bib.bib23)). These methods have draw considerable attention due
    to its effectiveness in a wide range of modern machine learning (ML) challenges
    (Liu et al., [2020](#bib.bib47)), including the adversarial attack and defense
    (Chen et al., [2017](#bib.bib10); Tu et al., [2019](#bib.bib76); Ye et al., [2018](#bib.bib83);
    Ilyas et al., [2018](#bib.bib34); Zhang et al., [2022b](#bib.bib85); Verma et al.,
    [2023](#bib.bib78); Zhao et al., [2019](#bib.bib86); Hogan & Kailkhura, [2018](#bib.bib29);
    Shu et al., [2022](#bib.bib66)), model-agnostic contrastive explanations (Dhurandhar
    et al., [2019](#bib.bib17)), enhancing transfer learning through visual prompting
    (Tsai et al., [2020](#bib.bib74)), computational graph unrolling (Vicol et al.,
    [2023](#bib.bib79)), and optimizing automated ML processes (Gu et al., [2021a](#bib.bib24);
    Wang et al., [2022](#bib.bib81)). Beyond standard ML, it finds application in
    policy search in reinforcement learning (Vemula et al., [2019](#bib.bib77)), network
    resource management (Liu et al., [2018](#bib.bib45)), ML-based optimization of
    scientific workflows (Hoffman et al., [2022](#bib.bib28); Tsaknakis et al., [2022](#bib.bib75);
    Chen et al., [2024](#bib.bib9)), and on-chip learning enhancements (Gu et al.,
    [2021b](#bib.bib25)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite its wide range of use cases, the application of ZO optimization in ML
    has primarily been restricted to small model scales. This limitation is attributed
    to the high variance and slow convergence associated with ZO optimization, which
    are exacerbated by model dimensions. To scale up ZO optimization, several acceleration
    techniques have been proposed. These include the integration of historical data
    to refine ZO gradient estimators (Meier et al., [2019](#bib.bib54); Cheng et al.,
    [2021](#bib.bib15)), leveraging gradient structural information (Singhal et al.,
    [2023](#bib.bib68)) or sparsity to diminish the dependency of ZO optimization
    on problem size (Wang et al., [2017](#bib.bib82); Cai et al., [2022](#bib.bib7),
    [2021](#bib.bib6); Balasubramanian & Ghadimi, [2018](#bib.bib2); Ohta et al.,
    [2020](#bib.bib57); Gu et al., [2021b](#bib.bib25); Chen et al., [2024](#bib.bib9)),
    the reuse of intermediate features (Chen et al., [2024](#bib.bib9)) and random
    perturbation vectors (Malladi et al., [2023](#bib.bib52)) in the optimization
    process. These advancements suggest a growing potential for the application of
    ZO optimization in more complex and large-scale ML problems.
  prefs: []
  type: TYPE_NORMAL
- en: BP-free training for large models. Training large models, especially LLMs, is
    memory-consuming due to the involved large computation graphs for BP (Ren et al.,
    [2021](#bib.bib61); Kim et al., [2023](#bib.bib40)). Thus BP-free methods have
    become a recent focus in the deep learning (DL) community. Forward gradient learning (Baydin
    et al., [2022](#bib.bib3); Ren et al., [2022](#bib.bib62); Silver et al., [2021](#bib.bib67);
    Belouze, [2022](#bib.bib4)), built upon the forward-mode automatic differentiation
    (AD), provides an alternative to ZO optimization for BP-free training. Different
    from ZO optimization, it relies on the forward-mode AD to calculate a forward
    (directional) gradient. However, one main limitation of the forward gradient is
    its requirement of full access to the AD software and the deep model, making it
    less memory-efficient than ZO optimization and impractical for tackling black-box
    problems (Chen et al., [2024](#bib.bib9)). The specifications of BP-free methods
    include greedy layer-wise learning (Nøkland & Eidnes, [2019](#bib.bib56)), input-weight
    alignment (Boopathy & Fiete, [2022](#bib.bib5)), forward-forward algorithm (Hinton,
    [2022](#bib.bib27)), synthetic gradients (Jaderberg et al., [2017](#bib.bib35)),
    BBT/BBTv2 evolutionary algorithms (Sun et al., [2022a](#bib.bib70), [b](#bib.bib71)),
    gradient guessing using special low dimensional structure of neural networks (Singhal
    et al., [2023](#bib.bib68)) and other black-box methods which optimize the prompts (Prasad
    et al., [2023](#bib.bib59); Deng et al., [2022](#bib.bib16); Chai et al., [2022](#bib.bib8)).
    Many of these algorithms are also motivated through the lens of seeking DL’s biological
    interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Applying ZO optimization to fine-tune pre-trained LLMs is particularly intriguing
    because it combines the advantages of being BP-free and utilizing pre-training.
    This enables the scalability of ZO optimization to large-scale LLMs while maintaining
    memory efficiency. MeZO (Malladi et al., [2023](#bib.bib52)) introduced a scalable
    ZO-SGD algorithm to efficiently fine-tune LLMs with up to 60 billion parameters,
    achieving competitive results compared to first-order optimization methods and
    structured fine-tuning approaches like LoRA. They also provided theoretical insights
    into why ZO methods can be effective for LLMs. This opens the gateway for efficient
    BP-free LLM fine-tuning and largely motivates our ZO benchmark study.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Reviewing ZO Optimization and Beyond
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work’s core objective is to benchmark and harness the potential of ZO (zeroth-order)
    optimization in LLM finetuning, eliminating the need for (first-order) back-propagation
    (BP) during finetuning and thus achieving memory efficiency (Malladi et al., [2023](#bib.bib52)).
    It is worth noting that the ZO optimization technique utilized in (Malladi et al.,
    [2023](#bib.bib52)) is primarily the basic version, specifically, ZO stochastic
    gradient descent (ZO-SGD). There are more advanced ZO optimization methods available,
    as summarized in (Liu et al., [2020](#bib.bib47)). Thus, this section is dedicated
    to reviewing a broader range of ZO optimization approaches and shedding light
    on the previously overlooked principles for LLM fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of ZO optimization. ZO optimization serves as a gradient-free alternative
    to first-order (FO) optimization, approximating FO gradients through function
    value-based gradient estimates, which we call ZO gradient estimates, as discussed
    in (Flaxman et al., [2004](#bib.bib19); Nesterov & Spokoiny, [2017](#bib.bib55);
    Ghadimi & Lan, [2013](#bib.bib23); Duchi et al., [2015](#bib.bib18)). Thus, a
    ZO optimization method typically mirrors the algorithmic framework of its corresponding
    FO optimization counterpart. However, it substitutes the FO gradient with the
    ZO gradient estimate as the descent direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Various techniques exist for performing ZO gradient estimation. In this paper,
    we focus on the randomized gradient estimator (RGE) (Nesterov & Spokoiny, [2017](#bib.bib55);
    Duchi et al., [2015](#bib.bib18)), a method that relies on the finite difference
    of function values along a randomly chosen direction vector. RGE has also been
    used by Malladi et al. ([2023](#bib.bib52)) to achieve memory-efficient fine-tuning
    for LLMs. Its preference in LLM fine-tuning is attributed to its query efficiency,
    i.e., a low number of function queries. Given a scalar-valued function $f(\mathbf{x})$)
    is expressed using central difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (RGE) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{u}_{i}$ is a small perturbation stepsize (also known as smoothing
    parameter). Malladi et al. ([2023](#bib.bib52)) employed [RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") by setting $q=1$, where $O(\cdot)$
    signifies the Big O notation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rationale behind RGE stems from the concept of the directional derivative
    (Duchi et al., [2015](#bib.bib18)): As $\mu\to 0$ along the random direction $\mathbf{u}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{E_{\mathbf{u}}}[f^{\prime}(\mathbf{x},\mathbf{u})\mathbf{u}]=\mathbf{E_{\mathbf{u}}}[\mathbf{u}\mathbf{u}^{T}\nabla
    f(\mathbf{x})]=\nabla f(\mathbf{x}).$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: With the above background, the RGE $\hat{\nabla}f(\mathbf{x})$ using the directional
    derivative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forward gradient: A missing BP-free baseline in LLM fine-tuning.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a byproduct of connecting [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization
    and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark") to ([1](#S3.E1 "Equation 1 ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark")), we obtain the directional derivative-based gradient estimate, $\nabla
    f(\mathbf{x})\approx f^{\prime}(\mathbf{x},\mathbf{u})\mathbf{u}$, which is known
    as the forward gradient (Forward-Grad) (Baydin et al., [2022](#bib.bib3); Ren
    et al., [2022](#bib.bib62)). Different from [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing
    ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark") that relies solely on the finite difference of
    function values, Forward-Grad requires the use of forward mode automatic differentiation
    (AD) but eliminates the need for the backward evaluation in the implementation
    of deep model fine-tuning or training. In other words, Forward-Grad is BP-free
    and can serve as another alternative gradient estimation method that improves
    the memory efficiency of LLM fine-tuning. We stress that Forward-Grad is a possibly
    overlooked BP-free optimizer. Given its unbiasedness as shown in ([1](#S3.E1 "Equation
    1 ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark")), it could serve as an upper
    performance bound for ZO optimization in theory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A focused spectrum of ZO optimization methods. Next, we provide a brief overview
    of the ZO optimization methods to be focused on in this work. Specifically, we
    will include: ZO-SGD (Ghadimi & Lan, [2013](#bib.bib23)) that Malladi et al. ([2023](#bib.bib52))
    has employed for LLM fine-tuning, ZO-SGD using sign-based gradient estimation
    (ZO-SGD-Sign) (Liu et al., [2019a](#bib.bib46)), ZO-SGD with momentum (ZO-SGD-MMT)
    (Malladi et al., [2023](#bib.bib52)), ZO-SGD with conservative gradient update
    (ZO-SGD-Cons), and the ZO variant of the Adam optimizer (ZO-Adam) (Chen et al.,
    [2019](#bib.bib13)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned methods can be unified into the following generic optimization
    framework in solving $\min_{\mathbf{x}}f(\mathbf{x})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}h(\hat{\nabla}f(\mathbf{x}_{t})),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{x}_{t}$ is a certain descent direction post-processing operation.
    In ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A missing BP-free baseline in LLM
    fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")), we omit the
    inclusion of the stochastic mini-batch for empirical risk minimization for ease
    of presentation. For instance, ZO-SGD can be expressed as ([2](#S3.E2 "Equation
    2 ‣ Forward gradient: A missing BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing
    ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")) when $h(\hat{\nabla}f(\mathbf{x}))=\hat{\nabla}f(\mathbf{x})$.
    We refer readers to Appx. [B](#S2a "B Zeroth-Order Optimization Algorithms ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")
    for more algorithmic details of ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A
    missing BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and
    Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark")) as applied to the ZO optimization approaches we covered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our rationale for selecting the aforementioned ZO optimization approaches for
    LLM fine-tuning is based on two key considerations: (1) We prioritize ZO optimization
    methods that require minimal modifications to the existing FO optimizer, ensuring
    ease of implementation for LLM fine-tuning. (2) We focus on methods with distinct
    algorithmic characteristics, allowing us to explore a diverse range of optimization
    strategies for improving LLM performance. Regarding (2), we include ZO-SGD-Sign
    as it employs 1-bit gradient quantization and represents one of the simplest ZO
    optimization methods. Additionally, we include ZO-SGD-MMT and ZO-SGD-Cons as they
    incorporate certain forms of ‘adaptive learning’ into the descent step updates.
    The former utilizes momentum based on historical gradient information, while the
    latter allows for the heuristics-based selection of the descent direction. Furthermore,
    ZO-Adam is one of the most complex ZO optimizers due to its utilization of moving
    averages and adaptive learning rates.'
  prefs: []
  type: TYPE_NORMAL
- en: Task alignment in ZO optimization for LLM fine-tuning. Scaling up ZO optimization
    for deep model training, as discussed in (Chen et al., [2024](#bib.bib9)), is
    exceedingly challenging due to its high variance, which is dependent on the model
    size. Nevertheless, LLM pre-training offers a unique advantage by enabling the
    fine-tuner to start from a well-optimized pre-trained model state. This graceful
    model initialization makes ZO optimization potentially scalable to LLM fine-tuning
    tasks (Malladi et al., [2023](#bib.bib52)). Even in this pretraining-finetuning
    paradigm, another crucial factor, which we call ‘task alignment’, still plays
    a key role in achieving satisfactory ZO fine-tuning performance. The ‘task alignment’
    refers to aligning the fine-tuning task with the format of the pre-training task,
    given by the next token or sentence prediction. For example, Gao et al. ([2020](#bib.bib22));
    Malladi et al. ([2023](#bib.bib52)) have transformed downstream text classification
    tasks into next token prediction tasks by introducing well-crafted input prompts.
    These prompts serve as bridges to align the fine-tuning tasks with the pre-training
    ones, facilitating ZO optimization when initiated from the pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Test accuracy (%) of pretrained Roberta-Large model fine-tuned on
    SST2 and RTE tasks using ZO and FO optimization methods with (✓) and without (✗)
    text alignment. The evident performance degradation is highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | SST2 | RTE |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✗ | Difference | ✓ | ✗ | Difference |'
  prefs: []
  type: TYPE_TB
- en: '| FO-SGD | 91.6 | 91.5 | 0.1 | 70.9 | 61.4 | 9.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD | 89.4 | 79.2 | 10.2 | 68.7 | 60.4 | 8.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-Adam | 89.8 | 79.2 | 10.6 | 69.2 | 58.7 | 10.5 |'
  prefs: []
  type: TYPE_TB
- en: 'As a warm-up experiment, Tab. [1](#S3.T1 "Table 1 ‣ Forward gradient: A missing
    BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark") empirically justifies the importance of task alignment when applying
    ZO optimization to LLM fine-tuning on the simple binary classification task by
    comparing scenarios with and without the use of pre-defined prompts to achieve
    task alignment. We fine-tune the entire Roberta-Large (Liu et al., [2019b](#bib.bib50))
    model on SST2 (Socher et al., [2013](#bib.bib69)) and RTE (Wang et al., [2019](#bib.bib80))
    datasets with two selected ZO methods: ZO-SGD (i.e., MeZO in (Malladi et al.,
    [2023](#bib.bib52))) and ZO-Adam. We compare their performance with that of the
    FO method (FO-SGD). The task alignment is achieved with the template <CLS>SENTENCE.
    It was [terrible|great].<SEP> for SST dataset and another template <CLS>SENTENCE1?
    [Yes|No], SENTENCE2.<SEP> for RTE. As we can see, without prompt-based text alignment,
    there is a substantial performance drop across ZO fine-tuning methods. Both ZO-SGD
    and ZO-Adam yield about $10\%$ accuracy degradation on SST2 and RTE, respectively.
    In contrast, FO-SGD suffers less from the absence of task alignment. This suggests
    that the task alignment is particularly beneficial to ZO LLM fine-tuning. It is
    also worth noting that crafting effective prompts for task alignment can be non-trivial,
    as prompt design is context-dependent and can affect the fine-tuning performance.
    In this work, we follow (Gao et al., [2020](#bib.bib22)) and (Malladi et al.,
    [2023](#bib.bib52)) to align the fine-tuning tasks to the pretrained ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Benchmarking ZO Optimization for LLM Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we delve into the empirical performance of ZO optimization
    in LLM fine-tuning. Our benchmarking effort includes evaluating accuracy and efficiency,
    accounting for different downstream task complexities (ranging from simple classification
    to more complicated reasoning task), and considering various language model types.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Benchmark Setups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLM fine-tuning tasks, schemes, and models. We begin by introducing the tasks
    and the fine-tuning schemes. We focus on three tasks, considering their complexity
    from low to high, which include (1) the simplest binary classification task, Stanford
    Sentiment Treebank v2 (SST2) (Socher et al., [2013](#bib.bib69)), (2) the question-answering
    task, Choice Of Plausible Alternatives (COPA) (Roemmele et al., [2011](#bib.bib63)),
    and (3) the commonsense reasoning task, WinoGrande (Sakaguchi et al., [2021](#bib.bib64)).
    When evaluating memory efficiency, we also consider the task of multi-sentence
    reading comprehension (MultiRC) (Khashabi et al., [2018](#bib.bib38)). For LLM
    fine-tuning on these tasks, we explore four parameter-efficient fine-tuning (PEFT)
    schemes: full-tuning (FT) that fine-tunes the entire pre-trained model, low-rank
    adaptation (LoRA) by imposing low-rank weight perturbations (Hu et al., [2021a](#bib.bib31)),
    prefix-tuning (Prefix) by appending learnable parameters to token embedding (Li
    & Liang, [2021](#bib.bib43)), and prompt-tuning (Prompt) (Liu et al., [2022](#bib.bib49))
    by introducing a series of learnable tokens attached to the input to adapt the
    fixed model to downstream tasks. We refer readers to Appx. [A](#S1a "A Preliminaries
    of Parameter-Efficient Fine-Tuning (PEFT) ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") for details. Furthermore,
    we incorporate several representative language models, including Roberta-Large
    (Liu et al., [2019b](#bib.bib50)), OPT (Zhang et al., [2022a](#bib.bib84)), LLaMA2
    (Touvron et al., [2023](#bib.bib73)), Vicuna (Zheng et al., [2023](#bib.bib87)),
    and Mistral (Jiang et al., [2023](#bib.bib36)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setup and implementation of ZO optimization. To train the previously mentioned
    LLM fine-tuners, we utilize the ZO optimization methods introduced in Sec. [3](#S3
    "3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"). These include ZO-SGD (i.e.
    MeZO (Malladi et al., [2023](#bib.bib52))), ZO-SGD-Sign, ZO-SGD-MMT, ZO-SGD-Cons,
    and ZO-Adam. For comparison, we also present the performance of Forward-Grad,
    which relies on the forward mode auto-differentiation rather than BP. We also
    provide the performance of two FO optimizers: (FO-)SGD and (FO-)Adam. Before applying
    the aforementioned optimizers to the LLM fine-tuning tasks, we follow (Gao et al.,
    [2020](#bib.bib22); Malladi et al., [2023](#bib.bib52)) to align the fine-tuning
    task format with the token or sentence prediction-based pre-training task, as
    demonstrated in Tab. [1](#S3.T1 "Table 1 ‣ Forward gradient: A missing BP-free
    baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark").
    We run ZO (or BP-free) optimizers and FO optimizers for 20000 and 625 iterations
    respectively, as outlined in ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A missing
    BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark")). Note that ZO optimization takes a longer convergence time as shown
    in (Liu et al., [2020](#bib.bib47)). When implementing ([RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark")), unless specified otherwise,
    we set the query budget per gradient estimation to $q=1$. We determine the values
    of other hyperparameters, such as the smoothing parameter and learning rate, through
    a grid search for each method. Following (Malladi et al., [2023](#bib.bib52)),
    if not specified otherwise, we adopt the mixed precision (MP) training for FO
    optimizers, where the model is loaded with the full precision (i.e., 32 bits)
    but training is carried out in half precision with 16 bits (FP16). For ZO optimizers,
    half-precision training is adopted throughout the model loading and training.
    We also remark that neither MP nor FP16 can be applied to Forward-Grad. We refer
    readers for more details to Appx. [C](#S3a "C How to Implement Memory-Efficient
    ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation metrics. We evaluate ZO LLM fine-tuning using two sets of metrics:
    accuracy and efficiency. Accuracy measures the fine-tuned model’s test-time performance
    in specific tasks, such as test accuracy in classification tasks. Efficiency includes
    various measurements like memory efficiency (in terms of peak memory usage and
    GPU cost), query efficiency (i.e., the number of function queries required for
    ZO optimization ), and run-time efficiency. These metrics collectively provide
    insights into the resources needed for ZO LLM fine-tuning, helping assess its
    feasibility and cost-effectiveness in practical scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experiment Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Performance of LLM fine-tuning on SST2 over pretrained Roberta-Large
    and OPT/1.3B. Best performance among ZO methods (including Forward-Grad) is highlighted
    in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| SST2 | Roberta-Large | OPT-1.3B |'
  prefs: []
  type: TYPE_TB
- en: '| FT | LoRA | Prefix | Prompt | FT | LoRA | Prefix | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| FO-SGD | $91.4$ | $91.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Forward-Grad | $\mathbf{90.1}$ | $90.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD | $89.4$ | $\mathbf{90.8}$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-MMT | $89.6$ | $85.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-Cons | $89.6$ | $88.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-Sign | $52.5$ | $87.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-Adam | $89.8$ | $84.4$ |'
  prefs: []
  type: TYPE_TB
- en: 'ZO fine-tuning on SST2: A pilot study. In Tab. [2](#S4.T2 "Table 2 ‣ 4.2 Experiment
    Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"), experiments
    are conducted to compare the performance of different BP-free and BP-based (FO-SGD)
    methods on one of the simplest LLM downstream task: the binary classification
    task with SST2 dataset. We investigate two model architectures, the medium-sized
    Roberta-Large and the larger model OPT-1.3B. Several key results are summarized
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, ZO-Adam seems to be the most effective ZO method, achieving the best
    performance in 4 out of 8 fine-tuning settings. However, as will be shown later,
    this is achieved at the cost of additional memory consumption. This is not surprising
    considering that ZO-Adam has the highest algorithmic complexity, as explained
    in Sec. [3](#S3 "3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: Second, Forward-Grad is a competitive method compared to the ZO methods, especially
    in the FT (full-tuning) setting. This indicates that Forward-Grad may be suitable
    for problems of a larger scale, and make it a compelling baseline of ZO LLM fine-tuning.
    Additionally, when the complexity of the fine-tuning scheme decreases (e.g., Prompt),
    the advantage of Forward-Grad over function value-based ZO methods diminishes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, The performance of ZO methods exhibits high variance, as evidenced by
    the fluctuating relative rankings across different experimental scenarios, although
    extensive hyper-parameter search efforts have been made. For example, the effectiveness
    of ZO-Adam degrades dramatically in the (OPT-1.3B, Prompt) setting. In addition,
    the MeZO method (i.e., ZO-SGD) used in (Malladi et al., [2023](#bib.bib52)) does
    not always emerge as the top-performing ZO optimizer for LLM fine-tuning across
    various settings. This variance is not surprising and could be largely attributed
    to the high variance of [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization
    and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark") (Nesterov & Spokoiny, [2017](#bib.bib55); Duchi et al., [2015](#bib.bib18)).'
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, ZO-SGD-Cons and ZO-SGD-MMT also demonstrate strong performance as ZO
    optimizers in LLM fine-tuning. However, ZO-SGD-Sign, the simplest ZO optimization
    method, tends to be the weakest approach, except in the simplest fine-tuning setting
    Prompt. The above observations motivate us to extend our explorations, investigating
    the effectiveness of ZO methods across a broader spectrum of models and more complex
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'ZO fine-tuning on downstream tasks COPA and WinoGrande under OPT-13B. Extended
    from the experiments on SST2, Fig. [1](#S4.F1 "Figure 1 ‣ 4.2 Experiment Results
    ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") presents the
    fine-tuning performance on COPA and WinoGrande dataset using a larger model, OPT-13B.
    We summarize our key observations when the problem scales up and becomes more
    complicated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9529c00fc156c9a857e818c44e72334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Results of OPT-13B on the tasks COPA and WinoGrande fine-tuned using
    ZO/FO optimizers in different PEFT settings.'
  prefs: []
  type: TYPE_NORMAL
- en: First, compared to the previous results, the performance gap among different
    ZO methods are much enlarged. In the meantime, the performance gap between FO
    and ZO methods are also widened. For example, in the experiments with WinoGrande,
    the FO methods (FO-SGD and FO-Adam) outperform all the other ZO methods by a large
    margin. This observation shows the scalability bottleneck intrinsic to ZO methods,
    when dealing with larger models and/or more complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, certain ZO methods exhibit exceptional stability across varied conditions:
    Despite a general trend towards variability, specific ZO methods, e.g., ZO-Adam
    and ZO-SGD-MMT, demonstrate stability in their performance. The reason for this
    could be that these algorithms integrate variance-reduced optimization techniques
    (such as momentum and adaptive learning rate) to ZO optimization and become more
    adaptable and resilient to the variances of ZO gradient estimation (Chen et al.,
    [2019](#bib.bib13)).'
  prefs: []
  type: TYPE_NORMAL
- en: Third, the LoRA tuning method is consistently robust when paired with various
    ZO algorithms. This resilience across different ZO methods suggests that LoRA’s
    mechanism is inherently more adaptable to the variations in ZO optimization strategies,
    providing a stable and reliable tuning approach in diverse settings. We will peer
    into the performance of LoRA below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance of different LLMs finetuned with LoRA on COPA and WinoGrande
    using different ZO/FO methods. Table format is consistent with Tab. [2](#S4.T2
    "Table 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | OPT-13B | LLaMA2-7B | Vicuna-7B | Mistral-7B |'
  prefs: []
  type: TYPE_TB
- en: '| COPA |'
  prefs: []
  type: TYPE_TB
- en: '| FO-SGD | $88$ |'
  prefs: []
  type: TYPE_TB
- en: '| FO-Adam | $88$ |'
  prefs: []
  type: TYPE_TB
- en: '| Forward-Grad | $\mathbf{89}$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD | $87$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-CONS | $88$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-Adam | $\mathbf{89}$ |'
  prefs: []
  type: TYPE_TB
- en: '| WinoGrande |'
  prefs: []
  type: TYPE_TB
- en: '| FO-SGD | $66.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| FO-Adam | $68.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| Forward-Grad | $62.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD | $62.6$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-CONS | $63.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-Adam | $\mathbf{64.0}$ |'
  prefs: []
  type: TYPE_TB
- en: 'In Tab. [3](#S4.T3 "Table 3 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark"), we present how different ZO methods perform on
    (LoRA, COPA) and (LoRA, WinoGrande) across a range of widely-used LLM families.
    For ease of computation, we focus on a subset of ZO optimization methods, including
    ZO-SGD, ZO-SGD-CONS, and ZO-Adam. As we can see, in some scenarios with the COPA
    dataset, some BP-free methods exhibit effectiveness comparable to, or even superior
    to, that of FO methods (FO-SGD and FO-Adam). For example, Forward-Grad and ZO-Adam
    outperform the best FO method on model OPT-13B and Vicuna-7B. Conversely, for
    the more difficult task WinoGrande, a performance gap of $5\%\sim 6\%$ between
    FO and ZO methods still exists across different models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3fa4d493ac5d1b288c5cdd7faa49615.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Memory comparison between FO-SGD and ZO-SGD full fine-tuning across
    various sequence lengths with a fixed effective batch size of $2$), the memory
    usage of FO-SGD remains relatively stable since the memory consumption for storing
    gradients during BP surpasses that needed for activations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficiency analyses. In Tab. [4](#S4.T4 "Table 4 ‣ 4.2 Experiment Results ‣
    4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"), we present a comparison of
    the efficiency performance of various ZO/FO optimizers when fine-tuning the full
    OPT-13B model on the MultiRC dataset with a batch size $4$. We evaluate the efficiency
    in the following dimensions: memory cost (in GB), the consumption of GPU resources
    (number of GPUs), and runtime cost per optimization iteration (in seconds). All
    the experiments are carried out in the same environment. Several key observations
    can be made below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, almost all ZO methods (except ZO-Adam) demonstrate comparable levels
    of efficiency, requiring only a single GPU (A100) for LLM fine-tuning. This observation
    is expected as ZO methods entail relatively straightforward optimization steps,
    primarily based on function evaluations, as depicted in [RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"). Among the examined ZO optimization
    methods, ZO-Adam incurs higher memory costs due to its algorithmic complexity.
    Second, in comparison to FO methods, ZO optimization reduces runtime costs per
    iteration, e.g., by approximately $33.3\%$ for ZO-SGD compared to FO-SGD. Third,
    Forward-Grad appears to be the point at which ZO optimization methods lose their
    memory efficiency advantage over FO methods. Additionally, we note that the substantial
    runtime cost of Forward-Grad compared to FO optimizers (FO-SGD and FO-Adam) is
    possibly attributed to its incompatibility with MP and FP16.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we examine the memory cost of LLM fine-tuning vs. the input sequence
    length. In Fig. [2](#S4.F2 "Figure 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking
    ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for
    Memory-Efficient LLM Fine-Tuning: A Benchmark"), we compare the memory efficiency
    between ZO-SGD and FO-SGD across various sequence lengths (i.e. the token number
    per sample). We employed synthetic texts generated from random sequences with
    specified shapes. In all experiments, an effective batch size of $2$ as depicted
    in Fig. [2](#S4.F2 "Figure 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")), where the memory allocated for activations overwhelms
    that required for gradient storage. We provide a theoretical analysis of this
    phenomenon in Appx [C](#S3a "C How to Implement Memory-Efficient ZO/FO Optimizers?
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The peak memory cost (in GB), the required GPU resources, and the
    runtime cost (in seconds) of each optimizer when fine-tuning the full OPT-13B
    model on MultiRC with an averaged 400 context length. The order of included optimizers
    is ranked based on the memory cost. The per-iteration runtime in seconds (s) is
    averaged over 100 iterations. Notably, Forward-Grad is marked by $*$, indicating
    its incompatibility with efficiency-enhancing techniques such as MP (mixed-precision
    training) and FP16 (half-precision training).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer | Memory $\Downarrow$ | Runtime Cost |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD | $\mathbf{29}$s |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-Cons | $\mathbf{29}$s |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-Sign | $\mathbf{29}$s |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-MMT | $53$s |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-Adam | $80$s |'
  prefs: []
  type: TYPE_TB
- en: '| Forward-Grad^∗ | $138$s |'
  prefs: []
  type: TYPE_TB
- en: '| FO-SGD | $161$s |'
  prefs: []
  type: TYPE_TB
- en: '| FO-Adam | $257$s |'
  prefs: []
  type: TYPE_TB
- en: 'Ablation study on query budget. Recall from ([1](#S3.E1 "Equation 1 ‣ 3 Reviewing
    ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")) that Forward-Grad provides us with an unbiased
    gradient estimate with respect to the FO gradient, in contrast to the function
    value-based biased ZO gradient estimate. However, in the above experiments, we
    have not observed a significant advantage brought by Forward-Grad. We hypothesize
    that this is due to the smallest query budget $q=1$ we used, which, despite its
    query efficiency, may not fully showcase the unbiased benefit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea86a3f4ef7f840558d1786ff1ebc1a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: LoRA-based fine-tuning accuracy of OPT-1.3B on SST2 using ZO-SGD
    and Forward-Grad over different budgets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the above, Fig. [3](#S4.F3 "Figure 3 ‣ 4.2 Experiment Results ‣
    4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") explores the impact of varying
    query budget ($q$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Extended Study to Improve ZO Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond the benchmarking effort in Sec. [4](#S4 "4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark"), we will explore algorithmic advancements to further
    improve the effectiveness of ZO LLM fine-tuning. We will leverage the following
    techniques: (1) Block-wise ZO fine-tuning; (2) Hybrid ZO and FO fine-tuning; and
    (3)  sparsity-induced ZO gradient estimation. These designs aim to reduce the
    large variances in gradient estimation when using ZO algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Block-wise ZO optimization enhances fine-tuning performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It has been shown in (Liu et al., [2018](#bib.bib45)) that using a coordinate-wise
    deterministic gradient estimator can reduce ZO optimization variance, although
    this scheme is difficult to scale. In a similar vein, we ask if [RGE](#S3.Ex1
    "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") when estimating
    a FO gradient block-wise can also improve the performance of ZO optimization.
    The key idea is to split the LLM into different blocks and then apply the ZO gradient
    estimator to each block of parameters. For example, OPT-1.3B entails $p=26$ forward
    passes for ZO gradient estimation per fine-tuning step. Our rationale is that
    by conducting gradient estimation block-wise, the resulting gradient estimate’s
    variance will be reduced, thereby potentially improving the fine-tuning performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Tab. [5](#S5.T5 "Table 5 ‣ Block-wise ZO optimization enhances fine-tuning
    performance. ‣ 5 Extended Study to Improve ZO Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"), we compare the
    performance of the ZO fine-tuning baseline MeZO (Malladi et al., [2023](#bib.bib52))
    (corresponding to ZO-SGD with a query budget of $q=1$ as ZO-SGD-Block per iteration.
    Notably, ZO-SGD-Block outperforms ZO-SGD in different query budget settings across
    different fine-tuning tasks, showing the benefit of block-wise ZO tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Performance comparison of OPT-1.3B on the SST2 & WinoGrande tasks
    between ZO-SGD and ZO-SGD-Block. The # of parameter blocks in ZO-SGD-Block is
    set to $p=26$. Best performance for each task is highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer | Forward Pass # | SST2 | WinoGrande |'
  prefs: []
  type: TYPE_TB
- en: '| MeZO | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD ($q=26$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-Block | $26$ |'
  prefs: []
  type: TYPE_TB
- en: Trade-off between performance and memory efficiency via hybrid ZO-FO training.
    The primary source of memory cost in LLM fine-tuning arises from the BP process,
    which involves passing gradients from the deep layers to the shallow layers of
    the model. Hence, to enhance memory efficiency, a potential approach is to confine
    BP within the deep layers without propagating it to the shallow layers. Moreover,
    ZO optimization can be employed for the shallow layers without the need for BP.
    The above approach of combining FO optimization for deep layers and ZO optimization
    for shallow layers results in a hybrid ZO-FO fine-tuning scheme for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The trade-off between memory cost (in GB) v.s. fine-tuning accuracy
    (%) using the hybrid ZO-FO training on the OPT-1.3B model over the SST2 dataset.
    The memory or accuracy gap vs. that of the pure ZO-SGD method is noted by $\Delta$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Memory (GB) | Accuracy ($\%$) |'
  prefs: []
  type: TYPE_TB
- en: '| ZO Layer # | Memory | $\Delta$Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| $0$ | $1.98$ |'
  prefs: []
  type: TYPE_TB
- en: '| $4$ | $1.88$ |'
  prefs: []
  type: TYPE_TB
- en: '| $8$ | $1.55$ |'
  prefs: []
  type: TYPE_TB
- en: '| $12$ | $0.24$ |'
  prefs: []
  type: TYPE_TB
- en: '| $16$ | $0.18$ |'
  prefs: []
  type: TYPE_TB
- en: '| $20$ | $0.03$ |'
  prefs: []
  type: TYPE_TB
- en: '| $24$ | $0.00$ |'
  prefs: []
  type: TYPE_TB
- en: 'Tab. [6](#S5.T6 "Table 6 ‣ Block-wise ZO optimization enhances fine-tuning
    performance. ‣ 5 Extended Study to Improve ZO Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") presents the
    performance of using the hybrid ZO-FO fine-tuning scheme on (OPT-1.3B, SST2).
    We examine different variants of this hybrid scheme by deciding ‘where’ to split
    between ZO optimization (for shallow layers) and FO optimization (for deep layers).
    Suppose that the model consists of $n$. The results presented in Tab. [6](#S5.T6
    "Table 6 ‣ Block-wise ZO optimization enhances fine-tuning performance. ‣ 5 Extended
    Study to Improve ZO Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark") demonstrate that employing ZO optimization on only
    the first third of the model layers (i.e., $k\leq 8$), the performance achieved
    is similar to that of full ZO fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient pruning benefits performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We next explore gradient pruning, a technique known for accelerating model
    training without compromising convergence (McDanel et al., [2022](#bib.bib53)).
    Our key idea is to induce sparse parameter perturbations for reducing gradient
    estimation variance in [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization
    and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark"). We begin by leveraging magnitude-based pruning (Frankle & Carbin,
    [2018](#bib.bib21); Chen et al., [2020](#bib.bib12)) to obtain the layer-wise
    sparsity ratios. We then generate random pruning masks (following these layer-wise
    sparsity ratios) and apply them to the weight perturbations in [RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") per ZO fine-tuning step. Tab. [7](#S5.T7
    "Table 7 ‣ Gradient pruning benefits performance. ‣ 5 Extended Study to Improve
    ZO Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM
    Fine-Tuning: A Benchmark") shows the performance of the sparsity-induced ZO gradient
    estimation in LLM fine-tuning as a function of the overall sparsity ratio. It
    becomes evident that choosing a moderate sparsity ratio (e.g., $20\%$) can lead
    to improved performance over the vanilla ZO optimizer, ZO-SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Performance of fine-tuning OPT-1.3B on COPA & SST2 datasets using
    ZO-SGD at different sparsity ratios. A sparsity of $0\%$ sparsity) is highlighted
    in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| COPA |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsity ($\%$ | $30$ | $70$ |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy ($\%$ | $70.00$ | $7.000$ |'
  prefs: []
  type: TYPE_TB
- en: '| SST2 |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsity ($\%$ | $30$ | $70$ |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy ($\%$ | $\mathbf{92.32}$ | $\mathbf{92.20}$ |'
  prefs: []
  type: TYPE_TB
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work explores the application of zeroth-order (ZO) optimization in fine-tuning
    large language models (LLMs). ZO optimization approximates gradients using loss
    differences, eliminating the need for back-propagation and activation storage.
    While MeZO (Malladi et al., [2023](#bib.bib52)) has made strides in adapting ZO
    optimization for LLMs, understanding the full ZO landscape remains an open question.
    To achieve this, we broaden the scope by considering various ZO optimization methods,
    task types, and evaluation metrics. We conduct the first benchmark study of different
    ZO optimization techniques, shedding light on their accuracy and efficiency. We
    also uncover overlooked ZO optimization principles, such as task alignment and
    the role of forward gradient. Leveraging these insights, we propose techniques
    like block-wise descent, hybrid ZO and FO training, and gradient sparsity to enhance
    ZO optimization-based LLM fine-tuning. The proposed enhancements to ZO optimization
    enable us to further improve fine-tuning accuracy while maintaining memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Impact Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper aims to advance the optimization foundations of the memory-efficient
    fine-tuning of large language models (LLMs). Its potential impacts are contingent
    on how these fine-tuned LLMs are utilized. On the positive side, achieving memory
    efficiency during LLM fine-tuning could lead to significant reductions in energy
    consumption, contributing to the development of green AI and achieving improved
    performance in resource-constrained environments. However, there is a potential
    negative aspect in terms of misuse, as the fine-tuned models could be employed
    for generating misinformation, phishing attacks, or releasing copyrighted and
    private information. However, given the technical focus of this work, there are
    no specific societal consequences directly stemming from it that need to be highlighted
    here.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amari (1993) Amari, S.-i. Backpropagation and stochastic gradient descent method.
    *Neurocomputing*, 5(4-5):185–196, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balasubramanian & Ghadimi (2018) Balasubramanian, K. and Ghadimi, S. Zeroth-order
    (non)-convex stochastic optimization via conditional gradient and gradient updates.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baydin et al. (2022) Baydin, A. G., Pearlmutter, B. A., Syme, D., Wood, F.,
    and Torr, P. Gradients without backpropagation. *arXiv preprint arXiv:2202.08587*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belouze (2022) Belouze, G. Optimization without backpropagation. *arXiv preprint
    arXiv:2209.06302*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boopathy & Fiete (2022) Boopathy, A. and Fiete, I. How to train your wide neural
    network without backprop: An input-weight alignment perspective. In *International
    Conference on Machine Learning*, pp.  2178–2205\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2021) Cai, H., Lou, Y., McKenzie, D., and Yin, W. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. *arXiv
    preprint arXiv:2102.10707*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2022) Cai, H., Mckenzie, D., Yin, W., and Zhang, Z. Zeroth-order
    regularized optimization (zoro): Approximately sparse gradients and adaptive sampling.
    *SIAM Journal on Optimization*, 32(2):687–714, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chai et al. (2022) Chai, Y., Wang, S., Sun, Y., Tian, H., Wu, H., and Wang,
    H. Clip-tuning: Towards derivative-free prompt learning with a mixture of rewards,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2024) Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J.,
    Parasyris, K., Zhang, Y., Zhang, Z., Kailkhura, B., and Liu, S. Deepzero: Scaling
    up zeroth-order optimization for deep model training. *ICLR*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017) Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
    Zoo: Zeroth order optimization based black-box attacks to deep neural networks
    without training substitute models. In *Proceedings of the 10th ACM workshop on
    artificial intelligence and security*, pp.  15–26, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Chen, S., Ge, C., Tong, Z., Wang, J., Song, Y., Wang, J.,
    and Luo, P. Adaptformer: Adapting vision transformers for scalable visual recognition.
    *Advances in Neural Information Processing Systems*, 35:16664–16678, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,
    Z., and Carbin, M. The lottery ticket hypothesis for pre-trained bert networks.
    *Advances in neural information processing systems*, 33:15834–15846, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019) Chen, X., Liu, S., Xu, K., Li, X., Lin, X., Hong, M., and
    Cox, D. Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization.
    *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2020) Cheng, M., Singh, S., Chen, P. H., Chen, P.-Y., Liu, S.,
    and Hsieh, C.-J. Sign-opt: A query-efficient hard-label adversarial attack. In
    *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SklTQCNtvS](https://openreview.net/forum?id=SklTQCNtvS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2021) Cheng, S., Wu, G., and Zhu, J. On the convergence of prior-guided
    zeroth-order optimization algorithms. *Advances in Neural Information Processing
    Systems*, 34:14620–14631, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2022) Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu,
    T., Song, M., Xing, E. P., and Hu, Z. Rlprompt: Optimizing discrete text prompts
    with reinforcement learning, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhurandhar et al. (2019) Dhurandhar, A., Pedapati, T., Balakrishnan, A., Chen,
    P.-Y., Shanmugam, K., and Puri, R. Model agnostic contrastive explanations for
    structured data. *arXiv preprint arXiv:1906.00117*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duchi et al. (2015) Duchi, J. C., Jordan, M. I., Wainwright, M. J., and Wibisono,
    A. Optimal rates for zero-order convex optimization: The power of two function
    evaluations. *IEEE Transactions on Information Theory*, 61(5):2788–2806, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flaxman et al. (2004) Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online
    convex optimization in the bandit setting: gradient descent without a gradient.
    *arXiv preprint cs/0408007*, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flaxman et al. (2005) Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online
    convex optimization in the bandit setting: Gradient descent without a gradient.
    In *Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms*,
    pp.  385–394, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020) Gao, T., Fisch, A., and Chen, D. Making pre-trained language
    models better few-shot learners. *arXiv preprint arXiv:2012.15723*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghadimi & Lan (2013) Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order
    methods for nonconvex stochastic programming. *SIAM Journal on Optimization*,
    23(4):2341–2368, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2021a) Gu, B., Liu, G., Zhang, Y., Geng, X., and Huang, H. Optimizing
    large-scale hyperparameters via automated learning algorithm. *arXiv preprint
    arXiv:2102.09026*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2021b) Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., and Pan,
    D. Z. Efficient on-chip learning for optical neural networks through power-aware
    sparse zeroth-order optimization. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 35, pp.  7583–7591, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton (2022) Hinton, G. The forward-forward algorithm: Some preliminary investigations.
    *arXiv preprint arXiv:2212.13345*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffman et al. (2022) Hoffman, S. C., Chenthamarakshan, V., Wadhawan, K., Chen,
    P.-Y., and Das, P. Optimizing molecules using efficient queries from property
    evaluations. *Nature Machine Intelligence*, 4(1):21–31, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hogan & Kailkhura (2018) Hogan, T. A. and Kailkhura, B. Universal decision-based
    black-box perturbations: Breaking security-through-obscurity defenses. *arXiv
    preprint arXiv:1811.03733*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for nlp. In *International Conference on Machine Learning*,
    pp.  2790–2799\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021a) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models,
    2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021b) Hu, S., Ding, N., Wang, H., Liu, Z., Wang, J., Li, J., Wu,
    W., and Sun, M. Knowledgeable prompt-tuning: Incorporating knowledge into prompt
    verbalizer for text classification. *arXiv preprint arXiv:2108.02035*, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Huang, F., Gao, S., Pei, J., and Huang, H. Accelerated zeroth-order
    and first-order momentum methods from mini to minimax optimization. *The Journal
    of Machine Learning Research*, 23(1):1616–1685, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilyas et al. (2018) Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box
    adversarial attacks with limited queries and information. In *International conference
    on machine learning*, pp.  2137–2146\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2017) Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals,
    O., Graves, A., Silver, D., and Kavukcuoglu, K. Decoupled neural interfaces using
    synthetic gradients. In *International conference on machine learning*, pp.  1627–1635\.
    PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karimi Mahabadi et al. (2021) Karimi Mahabadi, R., Henderson, J., and Ruder,
    S. Compacter: Efficient low-rank hypercomplex adapter layers. *Advances in Neural
    Information Processing Systems*, 34:1022–1035, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khashabi et al. (2018) Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S.,
    and Roth, D. Looking beyond the surface:a challenge set for reading comprehension
    over multiple sentences. In *Proceedings of North American Chapter of the Association
    for Computational Linguistics (NAACL)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Kim, B., Cai, H., McKenzie, D., and Yin, W. Curvature-aware
    derivative-free optimization. *arXiv preprint arXiv:2109.13391*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Kim, T., Kim, H., Yu, G.-I., and Chun, B.-G. BPipe: Memory-balanced
    pipeline parallelism for training large language models. In Krause, A., Brunskill,
    E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings
    of the 40th International Conference on Machine Learning*, volume 202 of *Proceedings
    of Machine Learning Research*, pp.  16639–16653\. PMLR, 23–29 Jul 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of
    scale for parameter-efficient prompt tuning, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
    prompts for generation, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020) Lin, Z., Madotto, A., and Fung, P. Exploring versatile generative
    language model via parameter-efficient transfer learning, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S.,
    and Amini, L. Zeroth-order stochastic variance reduction for nonconvex optimization.
    In *Advances in Neural Information Processing Systems*, volume 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Liu, S., Chen, P.-Y., Chen, X., and Hong, M. signSGD via
    zeroth-order oracle. In *International Conference on Learning Representations*,
    2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III,
    A. O., and Varshney, P. K. A primer on zeroth-order optimization in signal processing
    and machine learning: Principals, recent advances, and applications. In *IEEE
    Signal Processing Magazine*, volume 37, pp.  43–54\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Liu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., and
    Tang, J. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally
    across scales and tasks. *arXiv preprint arXiv:2110.07602*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang,
    J. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and
    tasks. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pp.  61–68, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized
    bert pretraining approach. *arXiv preprint arXiv:1907.11692*, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023) Luo, G., Huang, M., Zhou, Y., Sun, X., Jiang, G., Wang, Z.,
    and Ji, R. Towards efficient visual adaption via structural re-parameterization.
    *arXiv preprint arXiv:2302.08106*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malladi et al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D.,
    Chen, D., and Arora, S. Fine-tuning language models with just forward passes.
    *arXiv preprint arXiv:2305.17333*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McDanel et al. (2022) McDanel, B., Dinh, H., and Magallanes, J. Accelerating
    dnn training with structured data gradient pruning. In *2022 26th International
    Conference on Pattern Recognition (ICPR)*, pp.  2293–2299\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meier et al. (2019) Meier, F., Mujika, A., Gauy, M. M., and Steger, A. Improving
    gradient estimation in evolutionary strategies with past descent directions. *arXiv
    preprint arXiv:1910.05268*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nesterov & Spokoiny (2017) Nesterov, Y. and Spokoiny, V. Random gradient-free
    minimization of convex functions. *Foundations of Computational Mathematics*,
    17:527–566, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nøkland & Eidnes (2019) Nøkland, A. and Eidnes, L. H. Training neural networks
    with local error signals. In *International conference on machine learning*, pp. 
    4839–4850\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ohta et al. (2020) Ohta, M., Berger, N., Sokolov, A., and Riezler, S. Sparse
    perturbations for improved convergence in stochastic zeroth-order optimization.
    In *Machine Learning, Optimization, and Data Science: 6th International Conference,
    LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II 6*,
    pp.  39–64\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfeiffer et al. (2020) Pfeiffer, J., Rücklé, A., Poth, C., Kamath, A., Vulić,
    I., Ruder, S., Cho, K., and Gurevych, I. Adapterhub: A framework for adapting
    transformers. *arXiv preprint arXiv:2007.07779*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al. (2023) Prasad, A., Hase, P., Zhou, X., and Bansal, M. Grips:
    Gradient-free, edit-based instruction search for prompting large language models,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2023) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2021) Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang,
    S., Zhang, M., Li, D., and He, Y. Zero-offload: Democratizing billion-scale model
    training, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2022) Ren, M., Kornblith, S., Liao, R., and Hinton, G. Scaling forward
    gradient with local losses. *arXiv preprint arXiv:2210.03310*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roemmele et al. (2011) Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice
    of plausible alternatives: An evaluation of commonsense causal reasoning. In *2011
    AAAI Spring Symposium Series*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanh et al. (2022) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika,
    L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari,
    M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani,
    G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen,
    S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma,
    A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Bers, T., Biderman, S.,
    Gao, L., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot
    task generalization, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. (2022) Shu, Y., Dai, Z., Sng, W., Verma, A., Jaillet, P., and Low,
    B. K. H. Zeroth-order optimization with trajectory-informed derivative estimation.
    In *The Eleventh International Conference on Learning Representations*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2021) Silver, D., Goyal, A., Danihelka, I., Hessel, M., and van
    Hasselt, H. Learning by directional gradient descent. In *International Conference
    on Learning Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singhal et al. (2023) Singhal, U., Cheung, B., Chandra, K., Ragan-Kelley, J.,
    Tenenbaum, J. B., Poggio, T. A., and Yu, S. X. How to guess a gradient. *arXiv
    preprint arXiv:2312.04709*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
    C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality
    over a sentiment treebank. In *Proceedings of the 2013 conference on empirical
    methods in natural language processing*, pp.  1631–1642, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2022a) Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X., and Qiu,
    X. Bbtv2: Towards a gradient-free future with large language models, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022b) Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box
    tuning for language-model-as-a-service. In *International Conference on Machine
    Learning*, pp.  20841–20855\. PMLR, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2021) Tan, Z., Zhang, X., Wang, S., and Liu, Y. Msp: Multi-stage
    prompting for making pre-trained language models better translators. *arXiv preprint
    arXiv:2110.06609*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsai et al. (2020) Tsai, Y.-Y., Chen, P.-Y., and Ho, T.-Y. Transfer learning
    without knowing: Reprogramming black-box machine learning models with scarce data
    and limited resources. In *International Conference on Machine Learning*, pp. 
    9614–9624\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsaknakis et al. (2022) Tsaknakis, I., Kailkhura, B., Liu, S., Loveland, D.,
    Diffenderfer, J., Hiszpanski, A. M., and Hong, M. Zeroth-order sciml: Non-intrusive
    integration of scientific software with deep learning. *arXiv preprint arXiv:2206.02785*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2019) Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi,
    J., Hsieh, C.-J., and Cheng, S.-M. Autozoom: Autoencoder-based zeroth order optimization
    method for attacking black-box neural networks. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, pp.  742–749, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vemula et al. (2019) Vemula, A., Sun, W., and Bagnell, J. Contrasting exploration
    in parameter and action space: A zeroth-order optimization perspective. In *The
    22nd International Conference on Artificial Intelligence and Statistics*, pp. 
    2926–2935\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verma et al. (2023) Verma, A., Bangar, S., Subramanyam, A., Lal, N., Shah, R. R.,
    and Satoh, S. Certified zeroth-order black-box defense with robust unet denoiser.
    *arXiv preprint arXiv:2304.06430*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vicol et al. (2023) Vicol, P., Kolter, Z., and Swersky, K. Low-variance gradient
    estimation in unrolled computation graphs with es-single. *arXiv preprint arXiv:2304.11153*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
    Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language
    understanding. In *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Wang, X., Guo, W., Su, J., Yang, X., and Yan, J. Zarts:
    On zero-order optimization for neural architecture search. *Advances in Neural
    Information Processing Systems*, 35:12868–12880, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Wang, Y., Du, S., Balakrishnan, S., and Singh, A. Stochastic
    zeroth-order optimization in high dimensions. *arXiv preprint arXiv:1710.10551*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2018) Ye, H., Huang, Z., Fang, C., Li, C. J., and Zhang, T. Hessian-aware
    zeroth-order optimization for black-box adversarial attack. *arXiv preprint arXiv:1812.11377*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Zhang, Y., Yao, Y., Jia, J., Yi, J., Hong, M., Chang, S.,
    and Liu, S. How to robustify black-box ml models? a zeroth-order optimization
    perspective. *ICLR*, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Zhao, P., Liu, S., Chen, P.-Y., Hoang, N., Xu, K., Kailkhura,
    B., and Lin, X. On the design of black-box adversarial examples by leveraging
    gradient-free optimization and operator splitting method. In *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, pp.  121–130, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,
    Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with
    mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Zhu, S., Voigt, T., Ko, J., and Rahimian, F. On-device training:
    A first overview on existing systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Preliminaries of Parameter-Efficient Fine-Tuning (PEFT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our benchmark, we consider three PEFT methods, including {LoRA, prefix tuning,
    prompt tuning}.
  prefs: []
  type: TYPE_NORMAL
- en: '*$1$ in a transformer model, LoRA decomposes it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{W}^{\prime}=\mathbf{W}+\mathbf{B}\mathbf{A}$ |  | (A1) |'
  prefs: []
  type: TYPE_TB
- en: where $W$ represents the rank. During fine-tuning, only $\mathbf{B}$ frozen.
  prefs: []
  type: TYPE_NORMAL
- en: '*$2$ is the length of the prompt and $d$ is the embedding dimension. The model
    input is then:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{x}}=[\mathbf{P};\mathbf{E}(\mathbf{x})]$ |  | (A2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{E}(\mathbf{x})$ are learned, with the rest of the model parameters
    kept frozen.
  prefs: []
  type: TYPE_NORMAL
- en: '*$3$ serving as keys and values in the attention mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}(\mathbf{K}+\mathbf{C}_{k})^{\text{T}}}{\sqrt{d_{k}}}\right)(\mathbf{V}+\mathbf{C}_{v})$
    |  | (A3) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{Q}$, $\mathbf{C}_{v}\in\mathbb{R}^{l\times d_{v}}$ are updated,
    and the original model parameters are frozen.
  prefs: []
  type: TYPE_NORMAL
- en: B Zeroth-Order Optimization Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Zeroth-order optimization addresses the minimization or maximization of an
    objective function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ without the use of
    derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\mathbf{x}\in\mathbb{R}^{n}}f(\mathbf{x})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'These methods are pivotal when the function is non-differentiable, gradient
    computation is expensive, or the function evaluations are noisy. Random gradient
    estimation (RGE) provides a surrogate for gradients in zeroth-order optimization
    by sampling function evaluations. The gradient $\nabla f(\mathbf{x})$ can be approximated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\nabla}f(\mathbf{x}):=\frac{f(\mathbf{x}+\mu\mathbf{u})-f(\mathbf{x}-\mu\mathbf{u})}{2\mu}\mathbf{u}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{u}$ is a small scalar. This estimation facilitates the use of
    gradient-based methods solely based on function evaluations. Utilizing this gradient
    estimator, we summarize the existing zeroth-other algorithms below.
  prefs: []
  type: TYPE_NORMAL
- en: '$\rhd$ ZO-SGD (Ghadimi & Lan, [2013](#bib.bib23)). The method directly update
    the parameters by the estimated gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\hat{\nabla}f(\mathbf{x}_{t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where <math id="S2a.p2.2.m1.1" class="ltx_Math" alttext="\eta_{t}></math> is
    the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: $\rhd$ ZO-SGD-Sign (Liu et al., [2019a](#bib.bib46); Cheng et al., [2020](#bib.bib14)).
    The intuition of ZO-SGD-Sign is to make the gradient estimation more robust to
    noise since the sign operation could mitigate the negative effect of (coordinate-wise)
    gradient noise of large variance. Yet, the downside of ZO-SGD-Sign is the lack
    of estimation precision. It is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\operatorname{sign}(\frac{f(\mathbf{x}+\mu\mathbf{u})-f(\mathbf{x}-\mu\mathbf{u})}{2\mu}\mathbf{u})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{u}$ is a standard Gaussian vector.
  prefs: []
  type: TYPE_NORMAL
- en: $\rhd$ ZO-SGD-MMT (Huang et al., [2022](#bib.bib33)). The stochastic gradient
    estimated from a batch may suffer from a large variance. Momentum uses a moving
    average to estimate the global gradient and update the parameters by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\mathbf{m}_{t},$ |  |'
  prefs: []
  type: TYPE_TB
- en: with the momentum defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{m}_{t}=\beta_{t}\mathbf{m}_{t-1}+\hat{\nabla}f(\mathbf{x}_{{t}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ZO-SGD-MMT adopts the momentum factor $\beta_{t}$ to control the average.
  prefs: []
  type: TYPE_NORMAL
- en: '$\rhd$ ZO-SGD-Cons (Kim et al., [2021](#bib.bib39)). This method is adapted
    from (Kim et al., [2021](#bib.bib39)) to update the parameter conservatively:
    we pick up the point corresponding to the smallest loss value. The update is given
    by'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: $\rhd$ ZO-Adam (Chen et al., [2019](#bib.bib13)). Similar to the ZO-SGD with
    momentum, ZO-Adam uses momentum to estimate the gradients. In addition, ZO-Adam
    adaptively penalizes the learning rate to reduce the noise. The update of ZO-Adam
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}}\mathbf{V}_{t}^{-1/2}\mathbf{m}_{t},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the momentum vector ${\mathbf{m}}$ are computed by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{m}_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{v}_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{V}_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: The hyperparameters $\beta_{1}$. We remove RMSProp since it increases the memory
    cost and could void the advantage of ZO methods compared to FO-SGD.
  prefs: []
  type: TYPE_NORMAL
- en: $\rhd$ Forward-Grad (Baydin et al., [2022](#bib.bib3)). Forward-Grad relies
    on the directional derivative along a random direction vector. Different from
    ZO optimization methods, it provides an unbiased stochastic gradient estimator
    of the FO gradient. The update of Forward-Grad is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}(\nabla f(\mathbf{x}_{t})^{\top}\mathbf{u})\mathbf{u},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{u}$, it employs the Jacobian-Vector Product during the forward
    pass of training to mitigate computation and memory requirements. As a result,
    the memory complexity remains relatively low compared to BP-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: C How to Implement Memory-Efficient ZO/FO Optimizers?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm A1 A General Pipeline for A FO/ZO Optimizer
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Model with forward function $f$: States of optimizer/model, $\bm{\tau}$;  Step
    2: Backward Pass: Calculate gradients w.r.t. ${\mathbf{x}}$ using gradients and
    utilize temporal state ${\mathbf{s}}_{\text{opt}}^{\prime}$.'
  prefs: []
  type: TYPE_NORMAL
- en: The memory efficiency of an optimizer heavily depends on its implementation
    details. This section will discuss these implementation details and provide a
    holistic memory profile of all the optimizers discussed above. We remark that
    the discussions in this section are based on the PyTorch framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the memory efficiency of an optimizer is defined by the peak memories
    consumed during training in a specific setting, which primarily depends on the
    maximum number of variables stored at a certain time point. To dissect the memory
    consumption of different optimizers, we summarized a general model pipeline for
    parameter updating in [Algorithm A1](#alg1 "In C How to Implement Memory-Efficient
    ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark"), which involves four main steps.'
  prefs: []
  type: TYPE_NORMAL
- en: First, the program needs to load the model with the full parameters ($\mathbf{x}$
    and temporarily storing $\bm{\tau}_{\text{bwd}}$ will be released. Typically,
    ${\mathbf{s}}_{\text{bwd}}$ also includes essential temporal variables that are
    only used in the step.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the memory life-cycle, the memory consumption will be summarized
    as two types: *constant memory* that exists through the training, and *dynamic
    allocation* that only exists in one iteration mainly for gradient computing. Though
    dynamic allocation only exists temporarily, it also contribute to the peak memory.
    Without losing generality, we summarize the primary components of the peak memory
    consumption as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underbrace{&#124;{\mathbf{x}}&#124;+&#124;{\mathbf{s}}_{\text{opt}}&#124;}_{\text{constant}}+\underbrace{\max\left\{&#124;{\mathbf{s}}_{\text{fwd}}&#124;,&#124;{\mathbf{s}}_{\text{bwd}}&#124;,&#124;{\mathbf{s}}_{\text{opt}}^{\prime}&#124;\right\}}_{\text{dynamic
    allocation}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $|\cdot|$ denotes the cardinality of a vector. Note that we concentrate
    on the primary factors in Big O notation, which, unless specified otherwise, was
    omitted for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Prevalent Efficiency-Enhancing Tricks Adopted in This Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior to devling into the dissection of the memory consumption of different
    optimizers, we first introduce the prevalent efficiency-enhancing tricks widely
    adopted in the state-of-the-art FO/ZO-optimizers. These methods are by default
    adopted in this work if compatible to achieve the state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: Half-precision training (FP16). Most modern LLMs, e.g. LLaMA2 (Touvron et al.,
    [2023](#bib.bib73)), operate at half-precision (i.e., 16-bit) for inference, denoted
    as FP16. With FP16, only half of the model memory is required for ZO methods.
    By default, ZO methods are loaded at half-precision if the methods do not necessitate
    differentiation. However, as the current auto-differentiation module in PyTorch
    does not support FP16, methods like Forward-Grad must run at full precision. Similarly,
    for FO methods, half-precision is not compatible for the same reason.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed-precision training (MP). Mixed precision is a common practice to speed
    up gradient computation by back-propagation. By default, MP is used in FO methods
    (FO-SGD and FO-Adam), where the model is loaded in full precision and the training
    is carried out in half-precision. Specifically, a half-precision replica of the
    model is used to conduct a forward pass yielding the intermediate results and
    gradients in half-precision. From the half-precision one, a full-precision gradient
    will be recovered for optimization. If the intermediate results consume more memory
    consumption than the model and gradients themselves, MP can also reduce memory
    complexity. We remark that MP is still operating on a full-precision model, which
    means MP and FP16 cannot be used together in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The ‘foreach’ implementation of Adam. The PyTorch implementation of Adam will
    use so-called foreach implementation to speed up the computation. At a high level,
    the foreach implementation will replicate and merge all the layers’ weight into
    one tensor during Adam updates. Operations on the unified tensor can be easily
    parallelized and therefore are faster. Though foreach can speed up computation,
    it demands extra memory to store all the weights.
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Unpacking Memory Costs of Different Optimizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we look into the optimizers studied in this work. For each
    algorithm, we will first analyze their theoretical memory consumption following
    the step $1\sim 3$ depicted in the framework in [Algorithm A1](#alg1 "In C How
    to Implement Memory-Efficient ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"). In the end, we provide a
    comparison between the theoretical and empirical results.'
  prefs: []
  type: TYPE_NORMAL
- en: In most backward algorithms, activation ${\mathbf{a}}$ and ${\mathbf{x}}$. Next,
    we will focus on the memory dynamics in Step 1-3.
  prefs: []
  type: TYPE_NORMAL
- en: FO-SGD (MP). In Step 1, the forward pass will first replicate the model in half-precision,
    i.e. $\bar{\mathbf{x}}$-bit gradient using activation per layer. The computation
    happens layer by layer. Therefore, the peak memory will be summed as $\sum\nolimits_{l}\max\{\frac{1}{2}|{\mathbf{a}}_{l}|,|{\mathbf{x}}_{l}|\}$.
    In Step 3, there will be no optimization state. We can approximate the dynamic
    memory as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\quad\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,\max\{\frac{1}{2}&#124;{\mathbf{a}}&#124;,&#124;{\mathbf{x}}&#124;\}\right\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,&#124;{\mathbf{x}}&#124;\right\}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The total memory consumption will be approximated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{3}{2}&#124;{\mathbf{x}}&#124;,2&#124;{\mathbf{x}}&#124;\right\}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'FO-Adam (MP). In Step 1 and 2, FO-Adam completes the forward pass in the same
    way as FO-SGD and thus consumes memory similarly. In Step 3, the PyTorch Adam
    utilizes the ‘foreach’ implementation to speed up state computation. The Adam
    optimizer will first replicate the gradient in an extra memory of size $|{\mathbf{x}}|$.
    Therefore, the peak memory will be the maximum of the dynamic memory in all of
    the three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\quad\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,\max\{\frac{1}{2}&#124;{\mathbf{a}}&#124;,&#124;{\mathbf{x}}&#124;\},2&#124;{\mathbf{x}}&#124;\right\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,2&#124;{\mathbf{x}}&#124;\right\}.\vspace*{-3mm}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In addition to the dynamic memory, the optimization step adds two states in
    memory for first-order and second-order momentum vectors, i.e., ${\mathbf{m}}_{t}$.
    We can sum up the memory of all steps to estimate the total memory as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{7}{2}&#124;{\mathbf{x}}&#124;,5&#124;{\mathbf{x}}&#124;\right\}.\vspace*{-3mm}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ZO-SGD (FP16). In Step 1-2, ZO-SGD estimates the gradient by two forward loss
    calculations based on the random direction vector ${\mathbf{u}}$ to reproduce
    the random direction vectors in the same order. Specifically, we initialize a
    random number generator by $\texttt{rng}=\text{RandomState}(\mathcal{S})$ by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathbf{u}}_{l}\sim\mathcal{N}_{\text{rng}}(\mathbf{0},\mathbf{I}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{N}_{\text{rng}}$. Without optimizer states, ZO-SGD consume a
    total memory as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{2}&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{u}}_{l}&#124;=\frac{1}{2}&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{x}}_{l}&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the second term is the maximal dynamic memory with the random seed trick.
  prefs: []
  type: TYPE_NORMAL
- en: ZO-SGD-MMT (FP16). ZO-SGD with Momentum is similar to ZO-SGD, but it consumes
    extra memory for momentum storage. The momentum shares the same size as the model
    parameter. According to the memory of ZO-SGD, we can get the memory estimation
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{x}}_{l}&#124;.\vspace*{-5mm}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ZO-Adam (FP16). Similar to ZO-SGD-MMT, ZO-Adam has extra optimizer states, first-order
    and second-order momentum. They have the same memory consumption as the parameters.
    Therefore, the total estimated memory is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{3}{2}&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{x}}_{l}&#124;.\vspace*{-5mm}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Forward-Grad. In Step 1 and 2, Forward-Grad estimates the gradient in one unified
    pass. Different from other ZO methods, Forward-Grad leverages the forward-mode
    auto-differentiation module¹¹1[https://pytorch.org/tutorials/intermediate/forward_ad_usage.html](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)
    in PyTorch to estimate the directional derivative, i.e., $\nabla f({\mathbf{x}})^{\top}{\mathbf{u}}$.
    In Step 3, Forward-Grad directly uses the gradient to update the parameters without
    extra memory costs. Thus, with the model memory, the total memory estimation can
    be summed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 2&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{a}}_{l}&#124;.\vspace*{-4mm}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table A1: Comparison of total memory complexity of different optimizers when
    fine-tuning the full model. $|{\mathbf{x}}|$ represents the parameter and intermediate
    memory of a specific layer $l$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer | Memory |'
  prefs: []
  type: TYPE_TB
- en: '| FO-SGD | $&#124;{\mathbf{x}}&#124;+\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,&#124;{\mathbf{x}}&#124;\right\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| FO-Adam | $3&#124;{\mathbf{x}}&#124;+\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,2&#124;{\mathbf{x}}&#124;\right\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Forward-Grad | $&#124;{\mathbf{x}}&#124;+&#124;{\mathbf{x}}&#124;+\max_{l}&#124;{\mathbf{a}}_{l}&#124;$
    |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD | $\frac{1}{2}&#124;{\mathbf{x}}&#124;+\max_{l}\frac{1}{2}&#124;{\mathbf{x}}_{l}&#124;$
    |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-SGD-MMT | $&#124;{\mathbf{x}}&#124;+\max_{l}\frac{1}{2}&#124;{\mathbf{x}}_{l}&#124;$
    |'
  prefs: []
  type: TYPE_TB
- en: '| ZO-Adam | $\frac{3}{2}&#124;{\mathbf{x}}&#124;+\max_{l}\frac{1}{2}&#124;{\mathbf{x}}_{l}&#124;$
    |'
  prefs: []
  type: TYPE_TB
- en: C.3 Theoretical Memory Cost Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we provide some analyisis based on the theoretical results
    discussed above. In particular, we first summarize the theoretical memory efficiency
    of different optimizers in Tab. [A1](#S3.T1a "Table A1 ‣ C.2 Unpacking Memory
    Costs of Different Optimizers ‣ C How to Implement Memory-Efficient ZO/FO Optimizers?
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark"). Several key insights can be summarized. First, compared to FO optimizers
    (including Forward-Grad), the memory efficiency merits of ZO optimizers are mainly
    in two aspects. On the one hand, ZO can avoid saving the intermediate results
    (model states) $\mathbf{a}_{l}$). In the meantime, Forward-Grad is not compatible
    with the modern efficiency enhancing tricks (such as FP16 or MP), which further
    reduces its memory efficiency. Last, although ZO-Adam is memory intensive compared
    to ZO-SGD, it can be greatly improved by using FP16, which shows a better efficiency
    than FO-SGD (MP).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretical analysis on the memory consumption change with sequence length.
    We remark that the empirical results ($|{\mathbf{a}}|$, it exhibits a much better
    efficiency advantage over FO-SGD, when the sequence length increases (e.g., $1504$).
    Therefore, the memory efficiency gap shown in Tab. [4](#S4.T4 "Table 4 ‣ 4.2 Experiment
    Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") will be further
    enlarged if a larger input sequence length is used.'
  prefs: []
  type: TYPE_NORMAL
