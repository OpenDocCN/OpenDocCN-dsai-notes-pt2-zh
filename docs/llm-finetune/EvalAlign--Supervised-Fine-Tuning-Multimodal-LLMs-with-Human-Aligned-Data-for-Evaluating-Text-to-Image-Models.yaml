- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16562](https://ar5iv.labs.arxiv.org/html/2406.16562)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhiyu Tan¹  Xiaomeng Yang¹  Luozheng Qin¹  Mengping Yang¹
  prefs: []
  type: TYPE_NORMAL
- en: Cheng Zhang²  Hao Li³
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Shanghai Academy of AI for Science  ² Carnegie Mellon University  ³ Fudan
    University
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/SAIS-FUXI/EvalAlign](https://github.com/SAIS-FUXI/EvalAlign)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The recent advancements in text-to-image generative models have been remarkable.
    Yet, the field suffers from a lack of evaluation metrics that accurately reflect
    the performance of these models, particularly lacking fine-grained metrics that
    can guide the optimization of the models. In this paper, we propose EvalAlign,
    a metric characterized by its accuracy, stability, and fine granularity. Our approach
    leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained
    on extensive datasets. We develop evaluation protocols that focus on two key dimensions:
    image faithfulness and text-image alignment. Each protocol comprises a set of
    detailed, fine-grained instructions linked to specific scoring options, enabling
    precise manual scoring of the generated images. We Supervised Fine-Tune (SFT)
    the MLLM to align closely with human evaluative judgments, resulting in a robust
    evaluation model. Our comprehensive tests across 24 text-to-image generation models
    demonstrate that EvalAlign not only provides superior metric stability but also
    aligns more closely with human preferences than existing metrics, confirming its
    effectiveness and utility in model assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-image models, such as DALL·E series [[38](#bib.bib38), [37](#bib.bib37),
    [3](#bib.bib3)], Imagen [[43](#bib.bib43)], and Stable Diffusion [[33](#bib.bib33)],
    have significantly impacted various domains such as entertainment, design, and
    education, by enabling high-quality image generation. These technologies not only
    advance the field of text-to-image generation but also enhance related applications
    such as video generation [[4](#bib.bib4), [60](#bib.bib60)], image editing [[46](#bib.bib46),
    [18](#bib.bib18), [59](#bib.bib59)], and personalized image generation [[13](#bib.bib13),
    [42](#bib.bib42)]. Despite this prevalence, in the practical use of generative
    models, we still lack evaluation methods to accurately evaluate existing text-to-image
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing benchmarks for text-to-image generation models are neither comprehensive
    nor accurate enough to meet current evaluation needs. The reasons for this include:
    (1) Limited model parameters: Current evaluation models have too few parameters,
    which restricts their ability to accurately represent images, leading to significant
    discrepancies compared to human evaluations. (2) Training data limitations: Some
    evaluation methods, such as Inception Score (IS) [[44](#bib.bib44)], Frechet Inception
    Distance (FID) [[15](#bib.bib15)], and CLIP Score [[14](#bib.bib14)], use models
    that have not been trained with synthesized images, which may introduce training
    bias and flaws the evaluation. (3) High annotation costs: Some methods, such as
    ImageReward [[56](#bib.bib56)], HPS [[55](#bib.bib55)] and HPS v2 [[54](#bib.bib54)],
    rely heavily on extensive human annotations, which significantly increases the
    cost of labeling. (4) Lack of detailed evaluation metric: The evaluation metrics
    do not provide fine-grained interpretability, preventing them from guiding model
    optimization effectively. (5) Computational inefficiency: The evaluation models
    require substantial computational resources, making them inefficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these issues, we propose EvalAlign, which offers low-cost, accurate,
    and efficient model evaluations while providing fine-grained, interpretable metrics.
    Specifically, we Supervised Fine-Tune (SFT) a Multimodal Large Language Model
    (MLLM) to align it with human annotations. Our focus is on two key evaluation
    aspects: image faithfulness and text-image alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to its pre-training on large-scale datasets and a large number of model
    parameters, the MLLM demonstrates excellent understanding and generalization capabilities
    for images and instructions. This makes it possible to design evaluation instruction
    based on MLLM to evaluate text-to-image models. However, since the pre-trained
    datasets do not include model-generated images (such as distorted body structures
    and human hands) or evaluation-related text instructions, using MLLM directly
    for model evaluation does not yield optimal results. Therefore, we employ SFT
    on a small amount of manually annotated data to align MLLM with human annotations
    for text-to-image generation. In summary, our main contributions can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a dataset specifically designed to address the evaluation challenges
    of text-to-image models. This dataset, derived from multiple data sources, has
    been thoroughly cleaned and systematically annotated by human. It enables precise
    evaluation of text-image alignment and image faithfulness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose the EvalAlign evaluation metric, which accurately aligns evaluation
    metrics with human preferences. This method is cost-effective in terms of annotation
    and training, computationally efficient, and provides interpretable metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct evaluations over 24 text-to-image models and compare EvalAlign with
    existing evaluation methods. Extensive experiments demonstrate that EvalAlign
    outperforms other methods in evaluating model performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluations and benchmarks of text-to-image generation. Despite the incredible
    progress achieved by text-to-image generation, evaluations and benchmarks in this
    area are far from flawless and contain critical limitations. For example, the
    most commonly used metrics, IS [[44](#bib.bib44)], FID [[15](#bib.bib15)], and
    CLIPScore [[14](#bib.bib14)] are broadly recognized as inaccurate for their inconsistency
    with human perception. To address, HPS series [[55](#bib.bib55), [54](#bib.bib54)],
    PickScore [[20](#bib.bib20)], and ImageReward [[56](#bib.bib56)] introduced human
    preference prior on image assessing to the benchmark, thereby allowing better
    correlation with image quality. However, with varying source and size of training
    data, these methods merely score the evaluated images in a coarse and general
    way, which cannot serve as an indication for model evolution. Meanwhile, HEIM [[21](#bib.bib21)]
    combined automatic and human evaluation and holistically evaluated text-to-image
    generation in 12 aspects, such as alignment, toxicity, and so on. As a consequence,
    HEIM relies heavily on human labour, limiting its application within budget-limited
    research groups severely. [[30](#bib.bib30)] standardized the protocol and settings
    of human evaluation, ensuring its verifiable and reproducible. There are also
    some works bear a resemble with us. For instance, TIFA [[16](#bib.bib16)], Gecko [[53](#bib.bib53)]
    and LLMScore [[27](#bib.bib27)] also formulate the evaluation as a set of visual
    question answering procedure and use LLMs as evaluation models. However, while
    they all mainly focus on text-image alignment, our approach takes both text-image
    alignment and image faithfulness into consideration. Moreover, the evaluation
    of LLMScore requires an object detection stage, which introduces significantly
    extra inference latency to the evaluation pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Table [1](#S2.T1 "Table 1 ‣ 2 Related Work ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"), existing text-to-image evaluation methods contains various limitations,
    making them incapable to serve as a fine-grained, comprehensive, and human-preference
    aligned automatic benchmark. While our work fills in this gap economically, and
    can be employed to indicate evolution direction and support thorough analysis
    of text-to-image generation models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of different evaluation metrics and frameworks for text-to-image
    generation. EvalAlign focuses on two key evaluation aspects, i.e., image faithfulness
    and text-image alignment, and supports human-aligned, fine-grained, and automatic
    evaluations. P: Prompt. I: Image. A: Annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Venue | Benchmark Feature | Dataset Size | Evaluation Aspect |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human-aligned | Fine-grained | Automatic | P | I | A | Faithfulness | Alignment
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Inception Score [[44](#bib.bib44)] | NeurIPS 2016 | ✗ | ✗ | ✓ | – | 1.3M
    | – | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| FID [[15](#bib.bib15)] | NeurIPS 2017 | ✗ | ✗ | ✓ | – | 1.3M | – | ✓ | ✗
    |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP-score [[14](#bib.bib14)] | EMNLP 2021 | ✗ | ✗ | ✓ | 400M | 400M | –
    | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HPS [[55](#bib.bib55)] | ICCV 2023 | ✓ | ✗ | ✓ | 25K | 98K | 25K | – | –
    |'
  prefs: []
  type: TYPE_TB
- en: '| TIFA [[16](#bib.bib16)] | ICCV 2023 | ✓ | ✓ | ✓ | 4K | – | 25K | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| TVRHE [[30](#bib.bib30)] | CVPR 2023 | ✓ | ✗ | ✗ | – | – | – | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ImageReward [[56](#bib.bib56)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 8.8K | 68K |
    137K | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PickScore [[20](#bib.bib20)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 35K | 1M | 500K
    | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| HPS v2 [[54](#bib.bib54)] | arXiv 2023 | ✓ | ✗ | ✓ | 107K | 430K | 645K |
    – | – |'
  prefs: []
  type: TYPE_TB
- en: '| HEIM [[21](#bib.bib21)] | NeurIPS 2023 | ✓ | ✓ | ✗ | – | – | – | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Gecko [[53](#bib.bib53)] | arXiv 2024 | ✓ | ✓ | ✓ | 2K | – | 108K | ✗ | ✓
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLMScore [[27](#bib.bib27)] | arXiv 2024 | ✓ | ✓ | ✓ | – | – | – | ✗ | ✓
    |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign (ours) | – | ✓ | ✓ | ✓ | 3K | 21K | 132K | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Multimodal Large Language Models (MLLMs). Pretrained on massive text-only and
    image-text data, MLLMs have exhibited exceptional image-text joint understanding
    and generalization abilities, facilitating a large spectrum of downstream applications.
    Among the works major in MLLMs, LLaVA [[26](#bib.bib26), [24](#bib.bib24)] and
    MiniGPT4 [[61](#bib.bib61), [5](#bib.bib5)] proposed to conduct visual instruction
    tuning during SFT, so that MLLMs can be easily aligned to human preference and
    precisely answer fine-grained questions on visual content. Meanwhile, Video-LLaMA [[58](#bib.bib58)]
    and VideoChat [[22](#bib.bib22)] utilized MLLMs for video understanding. VILA [[23](#bib.bib23)]
    quantitatively proved that involving text-only instruction-tuning data during
    SFT can further ameliorate model performance on text-only and multimodal downstream
    tasks. LLaVA-NeXT [[25](#bib.bib25)] extracted visual tokens for both the resized
    input image and the segmented sub-images to provide more detailed visual information
    for MLLMs, achieving significant performance bonus on tasks with high-resolution
    input images.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we manually annotate a minimal amount of high-quality visual
    instruction-tuning data to adapt MLLMs to perform human preference aligned and
    fine-grained evaluation. Owing to the generalization and multimodal understanding
    abilities of MLLMs, our experiments demonstrated that the finetuned MLLMs can
    accurately assess text-to-image generation models by following instructions regarding
    various fine-grained criteria of synthesised image.
  prefs: []
  type: TYPE_NORMAL
- en: 3 EvalAlign Dataset Construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train, validate and test the effectiveness of our evaluation models, we build
    EvalAlign benchmark. Specifically, EvalAlign dataset is a meticulously annotated
    collection featuring fine-grained annotations for images generated from text prompts.
    This dataset comprises 21k images, each accompanied by detailed instructions.
    The compilation process for the EvalAlign Dataset encompasses prompt collection,
    image generation, and precise instruction-based annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prompts and Images Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt collection. To assess the capabilities of our model in terms of text-image
    alignment and image faithfulness, we collect, filter, and clean prompts from existing
    evaluation datasets and generated prompts based on LLM. These prompts encompass
    a diverse range from real-world user prompts, prompts generated through rule-based
    templates with LLM, to manually crafted prompts. Specifically, the utilized prompts
    are soureced from HPS [[55](#bib.bib55)], HRS-Bench [[2](#bib.bib2)], HPSv2 [[54](#bib.bib54)],
    TIFA [[16](#bib.bib16)], DSG [[7](#bib.bib7)], T2I-Comp [[17](#bib.bib17)], Winoground [[50](#bib.bib50)],
    DALL-EVAL [[8](#bib.bib8)], DiffusionDB [[52](#bib.bib52)], PartiPrompts [[57](#bib.bib57)],
    DrawBench [[43](#bib.bib43)], and JourneryDB [[49](#bib.bib49)].
  prefs: []
  type: TYPE_NORMAL
- en: Prompt curation. The prompts are collected for facilitating fine-grained and
    comprehensive evaluation on text-to-image models in terms of image faithfulness
    and text-image alignment. Considering some of the collected prompt are unsuitable
    for these two evaluation tasks, we filter the collected prompts to ensure their
    quantity, quality and diversity. For the image faithfulness evaluation task, we
    prioritize prompts related to human, animals, and other tangible objects, as prompts
    depicting sci-fi scenarios are less suitable for this type of assessment. Our
    filtering process initially selects prompts that describe human, animals, and
    other real objects. After deduplicating these prompts, we carefully selected 1,500
    distinct prompts with varying topic, background and style for further evaluation.
    The selected prompts encompass 10k subjects across 15 categories. For the text-image
    alignment task, we refined our selection based on descriptions of style, color,
    quantity, and spatial relationships in the prompts. Only those prompts containing
    relevant descriptions and exceeding 15 words in length were considered, culminating
    in a final set of 1,500 prompts for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bff799c750b2bca9e6aac29a9c23f5fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of EvalAlign. We collect, filter and clean prompts from
    various sources to ensure their quantity, quality and diversity. We use 8 state-of-the-art
    text-to-image models to the generate images for evaluation. These synthesized
    images are then delegated to human annotators for thorough multi-turn annotation.
    Finally, the annotated data are used to SFT train a MLLM to align it with fine-grained
    human preference, thereby adapting the model to perform fine-grained human-aligned
    text-to-image evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: Image generation. To train and evaluate the MLLM, we use a diverse set of images
    generated by various models using the aforementioned prompts, allowing for detailed
    human annotation. For each prompt, multiple images are generated across different
    models. The dataset is enriched by images synthesized by models with varying architectures
    and scales, enhancing the diversity essential for a comprehensive evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: This variety not only tests the MLLM generalization capabilities but also aids
    in developing a model with broader applicability. The training dataset incorporates
    images from 8 models, whereas the test dataset spans 24 models. The inclusion
    of 16 unseen models in the test set is crucial for evaluating the MLLM to generalize
    beyond its training data. For detailed information on the inference setting of
    each model, please refer to the supplementary material. This structured approach
    ensures a robust framework for training and validating the MLLM, positioning it
    as a versatile and adaptive tool in the field of image generation from textual
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Data Annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prompt annotation. For text prompts focused on text-image alignment, we begin
    by annotating the entities and their attributes within the text, as illustrated
    in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Prompts and Images Collection ‣ 3 EvalAlign
    Dataset Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models"). Our annotators extract
    the entities mentioned in the prompts and label each entity with corresponding
    attributes, including quantity, color, spatial relationships, and actions. During
    the annotation, we also ask the annotators annotate the overall style of the image
    if described in the corresponding prompt and report prompts that contain toxic
    and NSFW content. These high-quality and detailed annotations facilitate the subsequent
    SFT training and evaluation of the MLLM. This meticulous annotation procedure
    ensures that the MLLM can accurately align and respond to the nuanced details
    specified in the prompts, enhancing both the training process and the model’s
    performance in generating images that faithfully reflect the described attributes
    and style.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image annotation. The images generated by text-to-image models often present
    challenges such as occluded human body parts, which can impede the effectiveness
    of SFT training and evaluation of the MLLM. Therefore, to address these challenges
    and enhance the model’s training and evaluative capabilities, specific annotations
    are applied to all images depicting human and animals. These annotations include:
    presence of human or animal faces; visibility of hands; visibility of limbs. By
    implementing these annotations, we ensure that the MLLM can more effectively learn
    from and assess the completeness and faithfulness of the generated images. This
    structured approach to annotation not only aids in identifying common generation
    errors but also optimizes the model’s ability to generate more accurate and realistic
    images, thereby improving both training outcomes and the model’s overall performance
    in generating coherent and contextually appropriate visual content.'
  prefs: []
  type: TYPE_NORMAL
- en: Instruction-Fine-tuning data annotation. To align the MLLM with human preference
    prior on fine-grained image assessing, we can train the model on a minimal amount
    of fine-grained human preference data through SFT training. As a consequence,
    we devise two sets of questions, each is concentrated on a specific fine-grained
    aspect of image faithfulness and image-text alignment, and ask human annotators
    to answer these questions to acquire the fine-grained human preference data. To
    aid the human annotators to understand the meaning of each question and its available
    answer options, thereby ensuring high annotation quality, we employ a thorough
    and comprehensive procedure of annotation preparation. First, we write a detailed
    annotation guideline and conduct a training for the annotators to explain the
    annotation guideline and answer their questions about the annotation (see supplementary
    materials for the annotation guideline). Then, we conduct a multi-turn trial annotation
    on another 50 synthesized images. After each trial, we calculate the Cohen’s kappa
    coefficient and interpret annotation guidelines for our annotators. In total,
    we conduct nine turns of trial annotation, and in the last turn of the trial,
    the Cohen’s kappa coefficient of our annotators reaches $0.681$, indicating high
    inter-annotator reliability and high annotation quality. More results about the
    trial annotation will be included in the supplementary materials.
  prefs: []
  type: TYPE_NORMAL
- en: After completing the aforementioned preparations, we delegate the images filtered
    during image annotation to 10 annotators and ask them to complete the annotation
    just as how they did in the trial annotation. Furthermore, during the whole annotation
    procedure, four text-to-image generation experts conduct random sampling quality
    inspection on the present annotated results, causing a second and a third re-annotation
    on 423 and 112 inspection-failed samples. Overall, owing to the valuable work
    of our human annotators and our fastidious annotation procedure, we get quality-sufficient
    instruction-tuning data required for the SFT training of the MLLM. More details
    of the annotation procedure will be introduced in the supplementary files.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Dataset Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To summarize, we generate 24k images from 3k prompts based on 8 text-to-image
    models, which includes DeepFloyd IF [[1](#bib.bib1)], SD15 [[40](#bib.bib40)],
    LCM [[28](#bib.bib28)], SD21 [[40](#bib.bib40)], SDXL [[33](#bib.bib33)], Wuerstchen [[32](#bib.bib32)],
    Pixart [[6](#bib.bib6)], and SDXL-Turbo [[48](#bib.bib48)]. After data filtering,
    4.5k images were selected as annotation data for task of text-image alignment.
    Subsequently, these images were carefully annotated to generate 13.5k text-image
    pairs, where 11.4k were used to the training dataset and 2.1k to the validation
    dataset. For the image faithfulness task, we select 12k images for annotate, producing
    36k text-image pairs, with 30k are used to the training dataset and 6.2k to the
    validation dataset. Additionally, we employed 24 text-to-image models to generate
    2.4k images from 100 prompts. After annotation, these images were used as testing
    dataset. Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign Dataset
    Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models") and Figure [3](#S3.F3 "Figure 3 ‣ 3.3
    Dataset Statistics ‣ 3 EvalAlign Dataset Construction ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models") show the distribution of objects in different categories within our prompts,
    demonstrating the diversity and balance of our prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17b5b4110ec3af14f95215057517430f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Statistics of prompts on evaluating text-to-image alignment. Prompts
    in our text-to-image alignment benchmark covers a broad range of concepts commonly
    used in text-to-image generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cac56fc5e2d00b8e486c6e7df212f128.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Statistics of prompts on evaluating image faithfulness. Prompts in
    our image faithfulness benchmark covers a broad range of objects and categories
    that related to image faithfulnes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Training and Evaluation Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Supervised Fine-Tuning the MLLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we mentioned above, we use MLLMs as the evaluation models and let it to
    answer a set of carefully-designed instructions, thereby achieving quantitative
    measurement of fine-grained text-to-image generation skills. Due to training bias,
    zero-shot MLLMs perform poorly when it comes to fine-grained evaluation on synthesized
    images, particularly in term of image faithfulness. To address, we conduct SFT
    training on the annotated data to align the MLLM with human preference prior on
    fine-grained image assessing. Formally, the SFT training sample can be denoted
    as a triplet, i.e., $(Q,M,A)$ denotes the question (or the instruction), the multimodal
    input and the answer, respectively. Specifically, $Q$ is mainly the image and
    necessary textual description, while $A$. Specifically, the loss function can
    be formulated as follows, where $N$ is the length of the ground truth answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(\theta)=\sum^{N}_{i=1}\log p(A_{i}&#124;Q,M,A_{<i};\theta).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Evaluation and Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During inference, the MLLM, parameterized by $\theta$, given the specific question
    $Q$ in an autoregressive way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R_{i}=f(Q,M,R_{<i};\theta).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'This autoregressive generation procedure is considered complete once the model
    generates a EOS token or the generated response exceeds the preset maximum generation
    length. After the generation, we use rule-based filtering and regular expression
    to extract the option chosen by the MLLM, where each option has an exclusive predefined
    score to quantitatively measure a fine-grained skill specified by the question
    $Q$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Score}(Q)=g(R)=g(f(Q,M;\theta)),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $g(\cdot)$ and $S_{a}$, that encompass every aspect of image faithfulness
    and text-image alignment, respectively. Consequently, Our metric EvalAlign can
    be defined by simply summing the scores of the two question sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\text{EvalAlign}_{\text{f}}$ indicates the image faithfulness score and
    image-text alignment score evaluated by EvalAlign.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We leverage the LLaVA-NeXT model and applied the LoRA fine-tuning method to
    align it with the training dataset as described previously. The entire training
    process was conducted on $32$ hours, using a learning rate of 5$\times$10^(-5).
    Additionally, we restricted the LoRA fine-tuning to the Q and K weights of the
    attention module, as extending the fine-tuning to the ViT and projection modules
    was found to cause overfitting. Importantly, our method does not necessitate any
    modifications to the MLLM, rendering it compatible with any MLLM model. For the
    ablation study, we evaluated the LLaVA-NeXT 13B model on the validation dataset.
    In the final experiment, we apply supervised fine-tuning to the LLaVA-NeXT 34B
    model on the testing dataset and compare against different evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we show results of EvalAlign on benchmarking 24 text-to-image
    generative models. We also compare with popular text-to-image metrics and frameworks,
    including Inception Score (IS) [[44](#bib.bib44)], Frechet Inception Distance
    (FID) [[15](#bib.bib15)], CLIP-score [[14](#bib.bib14)], and HPS v2 [[54](#bib.bib54)].
    Please refer to supplementary materials for detailed results and analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Results on image faithfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Human | EvalAlign |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) |'
  prefs: []
  type: TYPE_TB
- en: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) |'
  prefs: []
  type: TYPE_TB
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) |'
  prefs: []
  type: TYPE_TB
- en: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Results on text-to-image alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Human | EvalAlign |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) |'
  prefs: []
  type: TYPE_TB
- en: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) |'
  prefs: []
  type: TYPE_TB
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) |'
  prefs: []
  type: TYPE_TB
- en: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) |'
  prefs: []
  type: TYPE_TB
- en: 'Evaluation on image faithfullness. We conducted the image faithfullness evaluation
    on the testing dataset, as detailed in Table [3](#S5.T3 "Table 3 ‣ 5.1 Main Results
    ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models"). Our evaluation metric,
    EvalAlign, demonstrates close alignment with human preferences Specifically, the
    rankings of the top and bottom 10 models by both EvalAlign and human evaluation
    scores show remarkable consistency, confirming that our metric closely mirrors
    human evaluation. Importantly, most of the images from the 24 generative models
    were not present during training, showcasing the robust generalization capability
    of our evaluation method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation on text-image alignment. The evaluation of text-image alignment
    on the testing dataset was conducted follow the same manner of image faithfullness
    evaluation. Table [3](#S5.T3 "Table 3 ‣ 5.1 Main Results ‣ 5 Experimental Results
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models") reveals that the rankings based on the EvalAlign
    metric and human evaluation metric for the 24 models are generally consistent.
    This consistency likely stems from the proximity of MLLM’s pre-training tasks
    to the text-image alignment evaluation tasks, resulting in better performance
    on text-image alignment evaluation task.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Ablations and Analyses of EvalAlign
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Results on different prompt categories. SFT significantly enhances the performance
    of MLLMs on evaluation tasks. We conducted experiments comparing the LLava-Next
    13B model with and without SFT. As shown in Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations
    and Analyses of EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning
    Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models")
    and Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental
    Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models"), the results demonstrate that SFT considerably
    improves performance across all prompt categories in both image faithfulness and
    text-to-image alignment, closely aligning the MLLM’s predictions with human evaluations.
    Note that Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign
    ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models") shows that the baseline
    method without SFT performs poorly on the image faithfulness task, which suggests
    the necessity of SFT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Results of different prompt categories for evaluating image faithfulness.
    Baseline is the vanilla LLaVA-NeXT model without find-tuning with human-aligned
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Body | Hand | Face | Object | Common |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | 1.6701 | 1.0278 | 1.4107 | 2.2968 | 1.0637 |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 3.9950 | 3.9932 | 3.9867 | 2.6734 | 3.3476 |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 1.7305 | 0.9490 | 1.4393 | 2.3565 | 1.0903 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Results of different prompt categories for evaluating text-to-image
    alignment. Baseline is the vanilla LLaVA-NeXT model without find-tuning with human-aligned
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Object | Count | Color | Style | Spatial | Action |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | 1.6947 | 1.2032 | 1.8551 | 1.9796 | 1.5608 | 1.8015 |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 1.5602 | 1.0742 | 1.9275 | 1.1837 | 1.4118 | 1.1838 |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 1.6807 | 1.2516 | 1.8696 | 1.9592 | 1.5882 | 1.8382 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Ablation study on the size of training data. Results are reported
    on image faithfulness under different training data scale. We observe that a small
    number of annotated training data is sufficient for optimal results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Data Size | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5
    | SD v2.1 | LCM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 200 | 1.7443 | 1.8898 | 1.9278 | 1.1261 | 1.2977 | 1.5254 | 1.4309
    | 1.1204 |'
  prefs: []
  type: TYPE_TB
- en: '| 500 | 1.8890 | 1.9161 | 1.8586 | 1.2141 | 1.3109 | 1.3926 | 1.3815 | 0.9485
    |'
  prefs: []
  type: TYPE_TB
- en: '| 800 | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Ablation study on the size vision-language model. Results are reported
    on image faithfulness under different model scales of LLaVA-NeXT. We observe that
    model size is critical for reliable evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Model Size | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5
    | SD v2.1 | LCM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 7B | 1.9959 | 1.8615 | 1.8228 | 1.1708 | 1.2704 | 1.4031 | 1.3063
    | 1.0145 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    |'
  prefs: []
  type: TYPE_TB
- en: '| 34B | 2.1131 | 1.8621 | 1.8083 | 1.3906 | 1.3076 | 1.3921 | 1.2037 | 1.0143
    |'
  prefs: []
  type: TYPE_TB
- en: 'Effect of dataset size for vision-language model training. In order to explore
    the effects of data size, we train model on image faithfulness evaluation task
    under three different data size: 200, 500 and 800. As illustrated in Table [6](#S5.T6
    "Table 6 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), performance generally enhances with more training
    data. Notably, training with just 500 data nearly maximizes accuracy, with further
    increases to 800 data yielding only slight improvements. This result suggests
    that our method requires only a small amount of annotated data to achieve good
    performance, highlighting the cost-effectiveness of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effect of model size. Transformers have demonstrated a strong capacity to scale
    effectively across a variety of tasks, as noted in key studies [[9](#bib.bib9),
    [36](#bib.bib36)]. In Table [7](#S5.T7 "Table 7 ‣ 5.2 Ablations and Analyses of
    EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models"), we illustrate
    the advantages of scaling up the MLLM for image faithfulness evaluation. Specifically,
    increasing the MLLM model size from 7B to 34B results in notable improvements
    in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Comparison with Existing Evaluation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SFT with human-aligned data outperforms vanilla MLLMs. To validate the effectiveness
    of the MLLM after SFT, we use vanilla LLaVA-NeXT 13B as the baseline model for
    comparison. As shown in Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses
    of EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models") and Table [5](#S5.T5
    "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), the results of vanilla model suggest some correlations
    with human-annotated data. However, the alignment of the vanilla MLLM is relatively
    low due to the absence of images generated by model (such as distorted bodies
    and hands images) and issues related to evaluation in the MLLM’s pre-training
    dataset. After applying SFT on the LLaVA-Next 13B model using human annotated
    data, the model’s predictions on various fine-grained evaluation metrics are almost
    align to the human-annotated data and significantly surpass the evaluation results
    of all MLLM models that were not fine-tuned. This experimental results confirms
    that our SFT training enables the MLLM to be successfully applied to the task
    of evaluating text-to-image models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison with state-of-the-art methods. To verify the human preference alignment
    of our model, especially when compared with other baseline methods, we calculate
    Kendall rank [[19](#bib.bib19)] and Pearson [[12](#bib.bib12)] correlation coefficient
    and summarize the results in Table [8](#S5.T8 "Table 8 ‣ 5.3 Comparison with Existing
    Evaluation Methods ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning
    Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models").
    This comparison experiment encompasses images generated by 24 text-to-image models,
    covering an extensive range of synthesized image distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison with existing methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Faithfulness | Alignment |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Kendall$\uparrow$ | Kendall$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP-score | 0.1304 | 0.1765 | 0.6956 | 0.8800 |'
  prefs: []
  type: TYPE_TB
- en: '| HPSv2 | 0.4203 | 0.5626 | 0.5217 | 0.7113 |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 0.7464 | 0.8730 | 0.8043 | 0.9356 |'
  prefs: []
  type: TYPE_TB
- en: As can be concluded, compared with baseline methods, EvalAlign achieves significant
    higher alignment with fine-grained human preference on image faithfulness and
    image-text consistency, showcasing robust generalization ability. Although HPS
    v2 roughly aligns with human preference in some extent, the relative small model
    capacity and coarse ranking training limits its generalization to the fine-grained
    annotated data. Besides, since CLIP-s only cares the CLIP similarity of the generated
    image and its corresponding prompt, it behaves poorly in image faithfulness evaluation.
    The per-question alignment and the leaderboard of EvalAlign will be introduced
    in the supplementary materials.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we design an economic and efficient evaluation method that offers
    high accuracy, strong generalization capabilities, and provides fine-grained,
    interpretable metrics. We develop a comprehensive data annotation and cleaning
    process tailored for evaluation tasks, and established the EvalAlign benchmark
    for training and evaluating models on supervised fine-tuning tasks for MLLMs.
    Experimental results across 24 text-to-image models demonstrate that our evaluation
    metrics surpass the accuracy of all the state-of-art evaluation method. Additionally,
    we conduct a detailed empirical study on how MLLMs can be applied to model evaluation
    tasks. There are still many opportunities for further advancements and expansions
    based on our EvalAlign. We hope that our work can inspire and facilitate future
    research in this field.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Misha Konstantinov Alex Shonenkov and et al. Deepfloyd if. [https://github.com/deep-floyd/IF](https://github.com/deep-floyd/IF),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran
    Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark
    for text-to-image models. In Proceedings of the IEEE/CVF international conference
    on computer vision, pages 20041–20053, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation
    with better captions. https://openai.com/dall-e-3, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej
    Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al.
    Stable video diffusion: Scaling latent video diffusion models to large datasets.
    arXiv preprint arXiv:2311.15127, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang,
    Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
    Minigpt-v2: Large language model as a unified interface for vision-language multi-task
    learning. arXiv preprint arXiv:2310.09478, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of
    diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint
    arXiv:2310.00426, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Jaemin Cho, Yushi Hu, Jason Michael Baldridge, Roopal Garg, Peter Anderson,
    Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene
    graph: Improving reliability in fine-grained evaluation for text-image generation.
    In International conference on learning representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning
    skills and social biases of text-to-image generation models. In Proceedings of
    the IEEE/CVF international conference on computer vision, pages 3043–3054, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
    Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
    Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In
    International conference on machine learning, pages 7480–7512\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Dreamlike-diffusion v1.0. [https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Dreamlike-photoreal. [https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] David Freedman, Robert Pisani, and Roger Purves. Statistics (international
    student edition). Pisani, R. Purves, 4th edn. WW Norton & Company, New York, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano,
    Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image
    generation using textual inversion. In International conference on learning representations,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.
    CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings
    of the conference on empirical methods in natural language processing, pages 7514–7528,
    November 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
    and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to
    a local nash equilibrium. Advances in neural information processing systems, 30,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
    Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness
    evaluation with question answering. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 20406–20417, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench:
    A comprehensive benchmark for open-world compositional text-to-image generation.
    Advances in neural information processing systems, 36:78723–78747, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou.
    Composer: Creative and controllable image synthesis with composable conditions.
    arXiv preprint arXiv:2302.09778, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. G. KENDALL. A new measure of rank correlation. Biometrika, 30(1-2):81–93,
    1938.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna,
    and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image
    generation. Advances in neural information processing systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park,
    Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente,
    et al. Holistic evaluation of text-to-image models. Advances in neural information
    processing systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali
    Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv
    preprint arXiv:2305.06355, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi
    Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual
    language models. arXiv preprint arXiv:2312.07533, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines
    with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen,
    and Yong Jae Lee. Llava-next: improved reasoning, ocr, and world knowledge, January
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. Advances in neural information processing systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang.
    Llmscore: Unveiling the power of large language models in text-to-image synthesis
    evaluation. Advances in neural information processing systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency
    models: Synthesizing high-resolution images with few-step inference. arXiv preprint
    arXiv:2310.04378, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Bellagente Marco, Brack Manuel, Teufel Hannah, Friedrich Felix, and et al.
    Multifusion: fusing pre-trained models for multi-lingual, multi-modal image generation.
    arXiv preprint arXiv:2305.15296, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima,
    Esa Rahtu, Janne Heikkilä, and Shin’ichi Satoh. Toward verifiable and reproducible
    human evaluation for text-to-image generation. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 14277–14286, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Schramowski Patrick, Brack Manuel, and et al. Safe latent diffusion: Mitigating
    inappropriate degeneration in diffusion models. arXiv preprint arXiv:2211.05105,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Pablo Pernias, Dominic Rampas, and Marc Aubreville. Wuerstchen: Efficient
    pretraining of text-to-image models. arXiv preprint arXiv:2306.00637, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Openjourney. [https://huggingface.co/prompthero/openjourney](https://huggingface.co/prompthero/openjourney),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Openjourneyv2. [https://huggingface.co/ilkerc/openjourney-v2](https://huggingface.co/ilkerc/openjourney-v2),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. OpenAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. arXiv preprint
    arXiv:2204.06125, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,
    Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation.
    In International conference on machine learning, pages 8821–8831\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] redshift-diffusion. [https://huggingface.co/nitrosocke/redshift-diffusion](https://huggingface.co/nitrosocke/redshift-diffusion),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein,
    and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
    generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, pages 22500–22510, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    Advances in neural information processing systems, 35:36479–36494, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and Xi Chen. Improved techniques for training gans. Advances in neural information
    processing systems, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
    Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,
    et al. Laion-5b: An open large-scale dataset for training next generation image-text
    models. Advances in neural information processing systems, 35:25278–25294, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, and Ben Poole. Score-based generative modeling through stochastic differential
    equations. In International conference on learning representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Sdxl-refiner. [https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Sdxl-turbo. [https://stability.ai/research/adversarial-diffusion-distillation](https://stability.ai/research/adversarial-diffusion-distillation),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu,
    Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for
    generative image understanding. Advances in neural information processing systems,
    36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams,
    Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models
    for visio-linguistic compositionality. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 5238–5248, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] vintedois-diffusion v0.1. [https://huggingface.co/22h/vintedois-diffusion-v0-1](https://huggingface.co/22h/vintedois-diffusion-v0-1),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover,
    and Duen Horng Chau. Diffusiondb: A large-scale prompt gallery dataset for text-to-image
    generative models. In Proceedings of the annual meeting of the association for
    computational linguistics, pages 893–911, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajić, Su Wang,
    Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset,
    et al. Revisiting text-to-image evaluation with gecko: on metrics, prompts, and
    human ratings. arXiv preprint arXiv:2404.16820, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao,
    and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating
    human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference
    score: Better aligning text-to-image models with human preference. In Proceedings
    of the IEEE/CVF international conference on computer vision, pages 2096–2105,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie
    Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences
    for text-to-image generation. Advances in neural information processing systems,
    36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui
    Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling
    autoregressive models for content-rich text-to-image generation. Transactions
    on machine learning research, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned
    audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control
    to text-to-image diffusion models. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 3836–3847, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu
    Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video
    synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:
    Enhancing vision-language understanding with advanced large language models. arXiv
    preprint arXiv:2304.10592, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multimodal LLMs. Since EvalAlign evaluation models are fine-tuned MLLMs, they
    also suffer from multimodal hallucination, where models may generate content that
    seems plausible but actually incorrect or fabricated, and cannot be inferred from
    the input images and texts. Moreover, due to the possible harmful content in the
    pretraining data of the utilized base MLLMs, the model may inherit these biases
    and generate inappropriate response. Although we carefully curate the SFT training
    data of the EvalAlign evaluation models, the problems of hallucination and biased
    pre-training is alleviated but not fully addressed. Other than the these issues,
    EvalAlign evaluation models also suffer from opacity and interpretability, context
    limitation, as well as sensitivity to input formatting, like most multimodal LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Human Annotations. Human annotation is naturally subjective and influenced by
    individual perspectives, biases, and preferences. During the annotation, annotators
    can make mistakes, leading to incorrect or noisy labels. Regarding these challenges,
    we conduct 9 rounds of trial annotation and 2 rounds of random sampling quality
    inspection to ensure the inter-annotator consistency and overall annotation quality.
    We also design easy-to-understand annotation guidelines, instructions and platform
    to lower the annotation difficulty and benefit the annotation accuracy. Despite
    all these efforts, conducting human annotation with different annotators, user
    interface and annotation guidelines may lead to different result, making our annotation
    somewhat limited. Furthermore, human annotation can be time-consuming and resource-intensive,
    limiting the scale at which we can afford.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Ethical and Social Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar with other MLLMs, EvalAlign may potentially generate responses contain
    offensive, inappropriate, or harmful content. Since the base MLLMs of EvalAlign
    are pretrained on large datasets scraped from the web that might contain private
    information and harmful content, they may inadvertently generate or expose sensitive
    information, raising ethical and privacy concerns. MLLMs are also susceptible
    to adversarial attacks, where inputs are intentionally crafted to deceive the
    model. This vulnerability can be exploited to manipulate model outputs, posing
    security and ethic risks. To alleviate these safety limitation, we create dedicated
    evaluation sets for bias detection and mitigation, and conducted adversarial testing
    through hours of redteaming. Besides, EvalAlign is designed for fine-grained,
    human-aligned automatic text-to-image evaluations. We believe that with appropriate
    use, it could provide users with interesting experiences for detailed synthesized
    image evaluation, and inspires more appealing research works about text-to-image
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0140bf909e36463bbe51f4462720598.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Demonstration of our user interface. Each time, our specially designed
    user interface will provide one sample to the annotators. We incorporated four
    distinct icons to signify various functionalities of the user interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Annotation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before performing the final human annotation, we made a series of efforts to
    guarantee its quantity, quality and efficiency. To begin with, we select appropriate
    candidates to perform the annotation and hold a training meeting for them. Then,
    we design a user-friendly user interface and a comprehensive annotation procedure.
    We write a detailed annotation guidelines to explain every aspect and precaution
    of the annotation. As mentioned above, we conduct 9 rounds of trial annotation
    on another 50 synthesized images and 2 turns of random sampling quality inspection
    to further ensure inter-annotator consistency and annotation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Annotator selection. The accuracy and reliability of the annotated data depend
    heavily on the capabilities of the human annotators involved in the annotation
    process. As a consequence, at the beginning of the annotation, We first conduct
    annotator selection to build an appropriate and unbiased annotation team, and
    train this annotation team with our meticulously prepared annotation guidelines.
    For annotator selection, we let the candidates to accomplish a test concentrating
    on 10 factors, domain expertise, resistance to visually disturbing content, attention
    to detail, communication skills, reliability, cultural and linguistic competence,
    technical skills, ethical considerations, aesthetic cognition, and motivation.
    Notably, since the evaluated models may generate images with uncomfortable and
    inappropriate visual content, the candidates are notified with this inconvenience
    before the test. Only those agreed with this inconvenience are eligible to participate
    in the test, and they are welcome to withdraw at any time if they choose to do
    so. Based on the test results and candidate backgrounds, We try our best to ensure
    that the selected annotators are well-balanced in background and have a generally
    competitive abilities of the 10 mentioned factors. To summarize, our annotation
    team includes 10 annotators carefully selected from 29 candidates, 5 males and
    5 females, all have a bachelor’s degree. We interview the annotators and ensure
    they are adequate for the annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Annotation training and guidelines. After the selection, we conduct a training
    meeting over our comprehensive user guidelines to make the annotation team aware
    of our purpose and standard. During the training meeting, we explain the purpose,
    precaution, standard, workload and wage of the annotation. Besides, we have formally
    informed the annotators that the human annotation is solely for research purposes,
    and the data they have annotated may potentially be released to the public in
    the future. We, and the annotators reached consensus on the standard, workload,
    wage and intended usage of the annotated data. The rules for recognising image
    faithfulness and text-image are universal, and thus each individual’s standards
    should not differ significantly. As a consequence, we annotate a few samples using
    our meticulously developed annotation platform for the annotators to ensure inter-annotator
    consistency. The overall snapshot of the developed annotation paltform is exhibited
    in [fig. 4](#A2.F4 "In Appendix B Ethical and Social Impacts ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"). With this training, we also equip the annotators with necessary knowledge
    for unbiased detailed human evaluation on image faithfulness and text-image alignment.
    Specifically, the employed annotation guidelines involve the instructions for
    using the annotation platform and detailed guidelines about the annotation procedure,
    and we demonstrate them in Table [9](#A3.T9 "Table 9 ‣ Appendix C Annotation Details
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: User Guidelines of the Human Annotation. Considering that our annotators
    are native Chinese speakers while our readers may not be, each user is actually
    provided with a copy of Chinese version of the user guidelines. Meanwhile, we
    demonstrate its translated English version as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Guidelines |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Part I Introduction Welcome to the annotation platform. This platform is
    designed to simplify the annotation process and enhance annotation efficiency.
    Before the detailed introduction, we want to claim again that you may feel inconvenient
    as the evaluated models may generate images with uncomfortable and inappropriate
    visual content. Now, you are still welcomed if you want to withdraw your consent
    The annotation process is conducted on a sample-by-sample basis, with a question-by-question
    approach. Thus, you are supposed to answer all the questions raised for the present
    sample to accomplish its annotation. Once all the delegated samples are accomplished,
    your job is finished and we are thankful for your contribution to the project.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Part II Guidelines of the User Interface 1.User Login: To access the annotation
    platform, you are required to login as a user. Please navigate to the login page,
    enter the username and password provided by us, and click the “Login” button.
    2.Dashboard: Once you complete the login, you will be jumped into the dashborad
    page. The dashboard will list the overview of the samples assigned to you to annotate.
    Besides, we list the status of each sample for you to freely check your annotation
    progress (e.g., pending, completed). 3.Annotation Interface: Click on the “Start”
    button or an assigned image through the dashboard interface, you will jump into
    the annotation interface. annotation interface is made up of three components:
    1) Image Display: View the image to be annotated and its conditioned prompts;
    2) Question Panel: List of single-choice questions related to the image; 3) Navigation
    Buttons: "Next" and "Previous" buttons to navigate through questions and images.
    4.Answering Questions: Each time, the annotation interface will provide you a
    sample for annotation, please view the image and read the associated question,
    select the appropriate answer from the available options, and repeat the processs
    for all questions related to the question. 5.Saving and Submitting Annotations:
    To save progress and submit completed annotations, you can click the “Save” button
    to save your progress. If you finish the assigned sample and ensure the accuracy
    and confidence of its, you can click the “Submit” button to submit this annotation.
    6.Review and Edit Annotations: If you want to review and edit your submission,
    you can navigate the completed tasks section, and select the image to review.
    You will jump into its annotation interface with the previously submitted annotations
    and are allowed to do any modification. 7.Report and contact: If you find any
    problem about the assigned sample, such as witnessing NSFW or biased content,
    assigned visually abnormal sample, feel free to click the “Report” button and
    fill a form to report this sample. If you have any question about the standard
    of the annotation or have suggestions for improvement, please do not hesitate
    to contact us through phone, we will be glad to help you. |'
  prefs: []
  type: TYPE_TB
- en: '| Part III General Guidelines of the Human Annotation 1.In general, you are
    supposed to answer all the questions raised for the present sample to accomplish
    its annotation. This annotation only involves single-choice question. 2.Before
    answering the question, please ensure that the question is applicable to this
    prompt. If it is not applicable, please select option 0 directly—this is the predefined
    option for this particular scenario. 3.If you are answering a question about the
    image faithfulness, you may find the question is applicable to multiple objects
    within the image. you need to answer the question regarding to every applicable
    object and its role in the image. A straightforward way for this is to solely
    score every applicable object and choose the option closest to the calculated
    weighted average score. 4.If you are answering the object faithfulness question
    on the image faithfulness annotation, you need to drop and report for the encountered
    image with no clear main object. 5.If you are answering the commonsense question
    on the image faithfulness annotation, you need to drop and report for the encountered
    surreal and sci-fi image. 6.You are required to first annotate 30 samples to form
    a stable and reasonable assessment standard. Then, accomplish the annotation in
    progress. 7.This annotation is for evaluating image faithfulness and text-image
    alignment, as a consequence, the standard of the annotation is universal. 8.If
    you feel confused at anything about the human annotation, feel free to contact
    us through phone, we will be glad to help you. 9\. Once you have submitted your
    annotation results, we are very thankful to inform you that you have finished
    your job. Thank you once again for your contribution to our project. 10.If you
    have submitted your annotation but want to withdraw your submission and review
    the annotation results, you can contact us through phone, we will send it back
    to you. |'
  prefs: []
  type: TYPE_TB
- en: Trial Annotation Even with the above preparation, there is no quantitative evidence
    to verify the quality, the efficiency, and the inter-annotator consistency of
    the human annotation. Additionally, the standard for assessing image faithfulness
    and text-image are universal, which further emphasize the significant role of
    high inter-annotator consistency. Considering that, we conduct a multi-turn trial
    annotation on another 50 synthesized images. After each trial, we calculate the
    Cohen’s kappa coefficient and conduct a meeting for our annotators to explain
    annotation standards, rules and guidelines, thereby ensuring high inter-annotator
    reliability. In total, we conduct nine turns of trial annotation, and in the last
    turn of the trial, the Cohen’s kappa coefficient of our annotators reaches $0.681$,
    indicating high inter-annotator reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Random Sampling Quality Inspection Upon reaching the milestone percentages of
    25%, 50%, 75%, and 100% in the annotation progress, we conducted a series of random
    sampling quality inspections on the present annotation results at each milestone,
    totally four turns of random sampling quality inspection. The random sampling
    quality inspection by four experts in text-to-image generation selected from our
    group on 1,000 randomly sampled annotated images. For the first two turn of quality
    inspection, there are totally 423 and 112 annotated samples that failed the inspection.
    The failed samples are re-annotated and re-inspected. As for the last two turns
    of quality inspection, they both revealed zero failed samples due to the thoughtful
    and rigorous annotation preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Additional Details of the Evaluated Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the details of the evaluated text-to-image generative
    models in this work.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stable Diffusion {v1.4, v1.5, v2 base, v2.0, v2.1}. Stable Diffusion (SD) is
    a series of 1B text-to-image generative models based on latent diffusion model [[41](#bib.bib41)]
    and is trained on LAION-5B [[45](#bib.bib45)]. Specifically, the SD series includes
    SD v1.1, SD v1.2, SD v1.4, SD v1.5, SD v2 base, SD v2.0, and SD v2.1 respectively.
    Among them, we choose the most commonly-employed SD v1.4, SD v1.5, SD v2.0 and
    SD v2.1 for EvalAlign evaluation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SD v1.1 was trained at a resolution of 256x256 on laion2B-en for 237k steps,
    followed by training at a resolution of 512x512 on laion-high-resolution ((170M
    examples from LAION-5B with resolution >= 1024x1024) for the subsequent 194k steps.
    While, SD v1.2 was initialized from v1.1 and further finetuned for 515k steps
    at resolution 512x512 on laion-aesthetics v2 5+ (a subset of laion2B-en, filtered
    to images with an original size >= 512x512, estimated aesthetics score > 5.0,
    and an estimated watermark probability < 0.5). SD v1.4 is initialized from v1.2
    and subsequently finetuned for 225k steps at resolution 512x512 on laion-aesthetics
    v2 5+. This version incorporates a 10% dropping of the text-conditioning to improve
    classifier-free guidance sampling. Similar to SD v1.4, SD v1.5 is resumed from
    SD v1.2 and trained 595k steps at resolution 512x512 on laion-aesthetics v2 5+,
    with 10% dropping of the text-conditioning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SD v2 base is trained from scratch for 550k steps at resolution 256x256 on a
    subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW
    classifier with punsafe = 0.1 and an aesthetic score >= 4.5\. Then it is further
    trained for 850k steps at resolution 512x512 on the same dataset on images with
    resolution >= 512x512\. SD v2.0 is resumed from stable-diffusion v2 base and trained
    for 150k steps using a v-objective on the same dataset. After that, it is further
    finetuned for another 140k steps on 768x768 images. SD v2.1 is finetuned from
    SD v2.0 with an additional 55k steps on the same dataset (with punsafe=0.1), and
    then finetuned for another 155k extra steps with punsafe=0.98.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable Diffusion XL {v1.0, Refiner v1.0}. Stable Diffusion XL (SDXL) is a powerful
    text-to-image generation model that iterates on the previous Stable Diffusion
    models in three key ways: (1) its UNet is 3x larger and SDXL combines a second
    text encoder (OpenCLIP ViT-bigG/14) with the original text encoder to significantly
    increase the number of parameters; (2) it introduces size and crop-conditioning
    to preserve training data from being discarded and gain more control over how
    a generated image should be cropped; (3) it introduces a two-stage model process;
    the base model (can also be run as a standalone model) generates an image as an
    input to the refiner model which adds additional high-quality details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pixart-Alpha. Pixart-Alpha is a model that can be used to generate and modify
    images based on text prompts. It is a Transformer Latent Diffusion Model that
    uses one fixed, pretrained text encoders (T5)) and one latent feature encoder
    (VAE).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Consistency Model Stable Diffusion XL Latent Consistency Model Stable
    Diffusion XL (LCM SDXL) [[28](#bib.bib28)] enables SDXL for swift inference with
    minimal steps. Viewing the guided reverse diffusion process as solving an augmented
    probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution
    of such ODE in latent space, mitigating the need for numerous iterations and allowing
    rapid, high-fidelity sampling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dreamlike Diffusion 1.0. Dreamlike Diffusion 1.0 [[10](#bib.bib10)] is a SD
    v1.5 model finetuned on high-quality art images by dreamlike.art.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dreamlike Photoreal 2.0. Dreamlike Photoreal 2.0 [[11](#bib.bib11)] is a photorealistic
    text-to-image latent diffusion model resumed from SD v1.5 by dreamlike art. This
    model was finetuned on 768x768 images, it works pretty good with resolution 768x768,
    640x896, 896x640 and higher resolution such as 768x1024.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Openjourney v1, v2. Openjourney [[34](#bib.bib34)] is an open-source text-to-image
    generation model resumed from SD v1.5 and finetuned on Midjourney images by PromptHero.
    Openjourney v2 [[35](#bib.bib35)] was further finetuned using another 124000 images
    for 12400 steps, about 4 epochs and 32 training hours.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redshift Diffusion. Redshift Diffusion [[39](#bib.bib39)] is a Stable Diffusion
    model finetuned on high-resolution 3D artworks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vintedois Diffusion. Vintedois Diffusion [[51](#bib.bib51)] is a Stable Diffusion
    v1.5 model finetuned on a large number of high-quality images with simple prompts
    to generate beautiful images without a lot of prompt engineering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Safe Stable Diffusion {Weak, Medium, Strong, Max}. Safe Stable Diffusion [[31](#bib.bib31)]
    is an enhanced version of the SD v1.5 model by mitigating inappropriate degeneration
    caused by pretraining on unfiltered web-crawled datasets. For instance SD may
    unexpectedly generate nudity, violence, images depicting self-harm, and otherwise
    offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that
    drastically reduces this type of content. Specifically, it has an additional safety
    guidance mechanism that aims to suppress and remove inappropriate content (hate,
    harassment, violence, self-harm, sexual content, shocking images, and illegal
    activity) during image generation. The strength levels for inappropriate content
    removal are categorized as: {Weak, Medium, Strong, Max}.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MultiFusion. MultiFusion [[29](#bib.bib29)] is a multimodal, multilingual diffusion
    model that extends the capabilities of SD v1.4 by integrating various modules
    to transfer capabilities to the downstream model. This combination results in
    novel decoder embeddings, which enable prompting of the image generation model
    with interleaved multimodal, multilingual inputs, despite being trained solely
    on monomodal data in a single language.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepFloyd-IF { M, L, XL } v1.0. DeepFloyd-IF [[1](#bib.bib1)] is a novel state-of-the-art
    open-source text-to-image model with a high degree of photorealism and language
    understanding. It is a modular composed of a frozen text encoder and three cascaded
    pixel diffusion modules: a base model that generates 64x64 image based on text
    prompt and two super-resolution models, each designed to generate images of increasing
    resolution: 256x256 and 1024x1024, respectively. All stages of the model utilize
    a frozen text encoder based on the T5 transformer to extract text embeddings,
    which are then fed into a UNet architecture enhanced with cross-attention and
    attention pooling. Besides, it underscores the potential of larger UNet architectures
    in the first stage of cascaded diffusion models and depicts a promising future
    for text-to-image synthesis. The model is available in three different sizes:
    M, L, and XL. M has 0.4B parameters, L has 0.9B parameters, and XL has 4.3B parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout all of the conducted experiments, we use the default inference settings
    for each evaluated models.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Instruction Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we present every instruction used for EvalAlign evaluation on image faithfuleness
    and text-image alignment. The templates contain some placeholders set for filling
    in the corresponding attributes of the input images during the evaluation. For
    example, a specific “<ObjectHere>” and “<NumberHere>” can be “people, laptop,
    scissors.” and “plate: 1, turkey sandwich: 3, lettuce: 1.”, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For EvalAlign evaluation on image faithfulness, we devise 5 questions concentrate
    on the faithfulness of the generated body structure, generated face, generated
    hand, generated objects, as well as generation adherence to commonsense and logic.
    The instruction templates for these fine-grained criteria are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A5.p3.pic1" class="ltx_picture" height="172.69" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,172.69) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="145.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q1]:Are
    there any issues with the [human/animals] body structure in the image, such as
    multiple arms, missing limbs or legs when not obscured, multiple heads, limb amputations,
    and etc? [OPTIONS]: 0.There are no human or animal body in the picture; 1.The
    body structure of the people or animals in the picture has a very grievous problem
    that is unbearable; 2.The body structure of the people or animals in the picture
    has some serious problems and is not acceptable; 3.The body structure of the people
    or animals in the picture has a slight problem that does not affect the senses;
    4.The body structure of the people or animals in the picture is basically fine,
    with only a few flaws; 5.The body structure of the people or animals in the picture
    is completely fine and close to reality.</foreignobject></g></g></svg><svg id="A5.p4.pic1"
    class="ltx_picture" height="156.09" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,156.09) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65
    13.78)"><foreignobject width="556.69" height="128.53" transform="matrix(1 0 0
    -1 0 16.6)" overflow="visible" color="#000000">[Q2]:Are there any issues with
    the [human/animals] hands in the image, such as having more or less than five
    fingers when not obscured, broken fingers, disproportionate finger sizes, abnormal
    nail size proportions, and etc? [OPTIONS]: 0.No human or animal hands are shown
    in the picture; 1.The hand in the picture has a very grievous problem that is
    unbearable; 2.The hand in the picture has some serious problems and is not acceptable;
    3.The hand in the picture has a slight problem that does not affect the senses;
    4.The hand in the picture is basically fine, with only a few flaws; 5.The hands
    in the picture are completely fine and close to reality.</foreignobject></g></g></svg><svg
    id="A5.p5.pic1" class="ltx_picture" height="156.09" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,156.09) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="128.53" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q3]:Are there any issues with
    [human/animals] face in the image, such as facial distortion, asymmetrical faces,
    abnormal facial features, unusual expressions in the eyes, and etc? [OPTIONS]:
    0.There is no face of any person or animal in the picture; 1.The face of the person
    or animal in the picture has a very grievous problem that is unbearable; 2.The
    face of the person or animal in the picture has some serious problems and is not
    acceptable; 3.The face of the person or animal in the picture has a slight problem
    that does not affect the senses; 4.The face of the person or animal in the picture
    is basically fine, with only a few flaws; 5.The face of the person or animal in
    the picture is completely fine and close to reality.</foreignobject></g></g></svg><svg
    id="A5.p6.pic1" class="ltx_picture" height="139.48" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,139.48) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="111.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q4]:Are there any issues or
    tentative errors with objects in the image that do not correspond with the real
    world, such as distortion of items, and etc? [OPTIONS]: 0.There are objects in
    the image that completely do not match the real world, which is very serious and
    intolerable; 1.There are objects in the image that do not match the real world,
    which is quite serious and unacceptable; 2.There are slightly unrealistic objects
    in the image that do not affect the senses; 3.There are basically no objects in
    the image that do not match the real world, only some flaws; 4.All objects in
    the image match the real world, no problem.</foreignobject></g></g></svg><svg
    id="A5.p7.pic1" class="ltx_picture" height="189.3" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,189.3) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="161.74" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q5]:Does the generated image
    contain elements that violate common sense or logical rules, such as animal/human
    with inconsistent anatomy, object-context mismatch, impossible physics, scale
    and proportion issues, temporal and spatial inconsistencies, hybrid objects, and
    etc? [OPTIONS]: 0.The image contains elements that violate common sense or logical
    rules, which is very grievous and intolerable; 1.The presence of elements in the
    image that seriously violate common sense or logical rules is unacceptable; 2.The
    image contains elements that violate common sense or logical rules, which is slightly
    problematic and does not affect the senses; 3.There are basically no elements
    in the image that violate common sense or logical rules, only some flaws; 4.There
    are no elements in the image that violate common sense or logical rules, and they
    are close to reality.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: The templates of EvalAlign evaluation on text-image alignment are as follows.
    We select 6 common aspects of text-image alignment, object, number, color, style,
    spatial relationship and action. For images that do not involve the specified
    attribute, the corresponding question template is not filled in and subsequently
    input into EvalAlign.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A5.p9.pic1" class="ltx_picture" height="73.07" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,73.07) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q1]:Does the given image contain
    all the objects (<ObjectHere>) presented in the corresponding prompts? [OPTIONS]:
    1.None objects are included; 2.Some objects are missing; 3.All objects are included.</foreignobject></g></g></svg><svg
    id="A5.p10.pic1" class="ltx_picture" height="73.07" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q2]:Does the given image correctly
    reflect the numbers (<NumberHere>) of each object presented in the corresponding
    prompts? [OPTIONS]: 1.All counting numbers are wrong; 2.Some of them are wrong;
    3.All counting numbers are right.</foreignobject></g></g></svg><svg id="A5.p11.pic1"
    class="ltx_picture" height="73.07" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65
    13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[Q3]:Does the given image correctly
    reflect the colors of each object (<ColorHere>) presented in the corresponding
    prompts? [OPTIONS]: 1.All colors are wrong; 2.Some of them are wrong; 3.All corresponding
    colors numbers are right.</foreignobject></g></g></svg><svg id="A5.p12.pic1" class="ltx_picture"
    height="73.07" overflow="visible" version="1.1" width="600"><g transform="translate(0,73.07)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="45.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[Q4]:Does the given image correctly reflect the style (<StyleHere>)
    described in the corresponding prompts? [OPTIONS]: 1.All styles are wrong; 2.Some
    of them are wrong; 3.All styles are right.</foreignobject></g></g></svg><svg id="A5.p13.pic1"
    class="ltx_picture" height="73.07" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65
    13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[Q5]:Does the given image correctly
    reflect the spatial relationship (<SpatialHere>) of each object described in the
    corresponding prompts? [OPTIONS]: 1.All spatial relationships are wrong; 2.Some
    of them are wrong; 3.All spatial relationships are right.</foreignobject></g></g></svg><svg
    id="A5.p14.pic1" class="ltx_picture" height="73.07" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q6]:Does the given image correctly
    reflect the action of each object (<ActionHere>) described in the corresponding
    prompts? [OPTIONS]: 1.All actions are wrong; 2.Some of them are wrong; 3.All actions
    are right.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Additional Quantitative Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: F.1 Generalization Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To verify the generalization capability of our evaluation model, We compared
    MLLM’s SFT using different training datasets: one with images generated by all
    8 text-to-image models and another with images generated by only 4 of these models,
    while the final evaluation was conducted on images generated by the other 4 models.
    As shown in Table [10](#A6.T10 "Table 10 ‣ F.1 Generalization Experiments ‣ Appendix
    F Additional Quantitative Analysis ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models") and Table [11](#A6.T11
    "Table 11 ‣ F.1 Generalization Experiments ‣ Appendix F Additional Quantitative
    Analysis ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models"), We observed that MLLMs trained on
    images from a subset of text-to-image models can effectively generalize to images
    generated by unseen text-to-image models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Ablation study on the number of different text-to-image models used
    to generate the training data for evaluating image faithfulness. We observe that
    EvalAlign exhibits strong generalization capability.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | T2I models | body | hand | face | object | common | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | - | 1.4988 | 0.8638 | 1.1648 | 2.2096 | 0.8710 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 8 | 1.6058 | 0.7901 | 1.1974 | 2.2783 | 0.8871 | 0.0596 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.6522 | 0.9588 | 1.2355 | 2.3032 | 0.9516 | 0.0987 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Ablation study on the number of different text-to-image models used
    to generate the training data for evaluating text-to-image alignment. We observe
    that EVALALIGN exhibits strong generalization capability.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | T2I models | Object | Count | Color | Style | Spatial | Action |
    MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | - | 1.7373 | 1.3131 | 2.0000 | 1.9333 | 1.5952 | 1.8837 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | 8 | 1.7203 | 1.3232 | 1.9565 | 1.9333 | 1.6547 | 1.8605 | 0.0256
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.7832 | 1.3526 | 1.9637 | 1.9876 | 1.6891 | 1.8954 | 0.0469 |'
  prefs: []
  type: TYPE_TB
- en: F.2 Instruction Enhancement Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 12: Ablation study on the enhancement of instructions. Results are reported
    on image faithfulness under different instructions. We observe that enhanced instructions
    can significantly improves the evaluation metrics. MAE: mean absolute error.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Instruction | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD
    v1.5 | SD v2.1 | LCM | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign | ✗ | 1.9565 | 1.9286 | 1.8565 | 1.1818 | 1.3419 | 1.4801 | 1.4078
    | 1.1051 | 0.1201 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
  prefs: []
  type: TYPE_TB
- en: 'Providing more contextual information for instructions enhances the performance
    of MLLMs. To further improve MLLM evaluation performance, we enhanced the prompts
    for both SFT and inference stages. As shown in Table  [12](#A6.T12 "Table 12 ‣
    F.2 Instruction Enhancement Experiments ‣ Appendix F Additional Quantitative Analysis
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), our experiments demonstrate that the enhanced
    prompts significantly increase evaluation accuracy. Specifically, the evaluation
    using enhanced instructions reduced the MAE metric by half, from 0.120 to 0.006,
    compared to the original instructions. Additionally, this approach consistently
    improved evaluation performance across different text-to-image models.'
  prefs: []
  type: TYPE_NORMAL
- en: F.3 Mulit-scaling Resolutions Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 13: Ablation study on multi-scale input. Results are reported on image
    faithfulness under different input strategy. We observe that input with multi-scale
    resolution images can improves the evaluation metrics. MAE: mean absolute error.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Multi Scale | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD
    v1.5 | SD v2.1 | LCM | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EvalAlign |  | 1.8105 | 1.9238 | 1.9325 | 1.2078 | 1.2247 | 1.4540 | 1.3012
    | 1.0554 | 0.1358 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
  prefs: []
  type: TYPE_TB
- en: 'In the design of LLaVA-Next, using multi-scale resolution images as input helps
    address the issue of detail information loss, which significantly impacts the
    evaluation of image faithfulness, such as assessing deformations in hands and
    faces. We conducted a multi-scale image training comparison experiment to validate
    this approach. The baseline was the 13B LLaVA model with 336$\times$336, 672$\times$1008)
    as input. As shown in Table [13](#A6.T13 "Table 13 ‣ F.3 Mulit-scaling Resolutions
    Experiments ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"), training with multi-scale inputs significantly enhanced the model’s
    understanding of image and achieved better evaluation performance.'
  prefs: []
  type: TYPE_NORMAL
- en: F.4 Full Comparison with Existing Evaluation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 14: Results on faithfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ | 31.6226${}^{1~{}~{}}$
    | 0.9696 ${}^{1~{}~{}}$ | 5.2583^(21) |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ | 29.2322${}^{6~{}~{}}$
    | 5.6452^(14) |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ | 29.8197${}^{3~{}~{}}$
    | 0.7245 ${}^{2~{}~{}}$ | 5.3985^(16) |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ | 29.0620${}^{7~{}~{}}$
    | 0.7043 ${}^{3~{}~{}}$ | 5.3774^(18) |'
  prefs: []
  type: TYPE_TB
- en: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ | 30.6622${}^{2~{}~{}}$
    | 5.9751${}^{4~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ | 29.3588${}^{5~{}~{}}$
    | 21.6532${}^{4~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) | 26.3475^(13)
    | 0.8196^(15) | 0.1478 ^(16) | 20.8637^(10) | 5.7368^(10) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ | 25.7396^(17) | 0.7555^(24)
    | -0.0507^(22) | 20.4594^(21) | 5.5428^(15) |'
  prefs: []
  type: TYPE_TB
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ | 27.3299^(10) |
    0.8364${}^{8~{}~{}}$ | 21.4824${}^{5~{}~{}}$ | 5.6575^(12) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) | 25.5764^(18)
    | 0.8165^(18) | -0.1022^(23) | 20.6211^(18) | 5.8643${}^{6~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) | 26.2798^(14)
    | 0.8101^(20) | 0.2042 ^(12) | 20.7880^(12) | 6.1760${}^{1~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) | 26.1180^(15)
    | 0.7809^(23) | -0.1264^(24) | 20.3873^(24) | 5.3861^(17) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) | 26.5823^(12) | 0.8377${}^{7~{}~{}}$
    | 21.0502${}^{9~{}~{}}$ | 5.3073^(19) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) | 25.3481^(21) | 0.8170^(17)
    | 0.0872 ^(18) | 20.7529^(13) | 5.9060${}^{5~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) | 24.6984^(23)
    | 0.7958^(22) | -0.0415^(21) | 20.4088^(22) | 5.1995^(22) |'
  prefs: []
  type: TYPE_TB
- en: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) | 25.1572^(22)
    | 0.8101^(21) | 0.0218 ^(20) | 20.6155^(19) | 5.7657${}^{8~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    | 0.8543${}^{3~{}~{}}$ | 21.2664${}^{7~{}~{}}$ | 5.6614^(11) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) | 25.4972^(19) | 0.8214^(13)
    | 0.1686 ^(14) | 20.7143^(16) | 6.0535${}^{2~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) | 27.4512${}^{9~{}~{}}$
    | 0.6087 ${}^{5~{}~{}}$ | 20.7474^(14) | 5.3012^(20) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) | 25.3697^(20) | 0.8190^(16)
    | 0.1050 ^(17) | 20.6535^(17) | 5.8571${}^{7~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |
    26.5901^(11) | 0.8341${}^{9~{}~{}}$ | 0.3562 ^(10) | 20.8358^(11) | 5.0181^(23)
    |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) | 27.4836${}^{8~{}~{}}$
    | 0.4463 ${}^{8~{}~{}}$ | 20.7170^(15) | 5.6502^(13) |'
  prefs: []
  type: TYPE_TB
- en: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) | 23.8133^(24)
    | 0.8151^(19) | 0.0695 ^(19) | 20.4780^(20) | 4.3824^(24) |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) | 25.9522^(16) |
    0.8329^(11) | 0.1637 ^(15) | 20.4035^(23) | 5.7451${}^{9~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Results on text-to-image alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ | 32.5477^(10) | 0.8579${}^{2~{}~{}}$
    | 21.1998^(10) | 7.1864${}^{5~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ | 32.7140${}^{9~{}~{}}$
    | 0.3820 ${}^{6~{}~{}}$ | 21.1284^(12) | 6.6571^(20) |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ | 35.6465${}^{3~{}~{}}$
    | 0.4738 ${}^{2~{}~{}}$ | 7.1101${}^{8~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ | 33.8011${}^{6~{}~{}}$
    | 0.3833 ${}^{5~{}~{}}$ | 7.0067^(14) |'
  prefs: []
  type: TYPE_TB
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ | 37.0493${}^{1~{}~{}}$
    | 0.6542 ${}^{1~{}~{}}$ | 6.9293^(16) |'
  prefs: []
  type: TYPE_TB
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ | 31.0951^(14) | 0.8434${}^{8~{}~{}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ | 32.7752${}^{8~{}~{}}$
    | 21.7627${}^{6~{}~{}}$ | 6.8389^(19) |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ | 35.1593${}^{4~{}~{}}$
    | 0.4322 ${}^{4~{}~{}}$ | 7.2762${}^{1~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ | 36.4632${}^{2~{}~{}}$
    | 0.2513 ${}^{7~{}~{}}$ | 7.1280${}^{7~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) | 31.1495^(12)
    | 0.8173^(16) | -0.0867^(14) | 21.1163^(13) | 6.2729^(22) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) | 31.1017^(13) | 0.8278^(14)
    | -0.0453^(12) | 21.2093${}^{9~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) | 28.7957^(24)
    | 0.8264^(15) | -0.1337^(15) | 20.9625^(17) | 4.9593^(24) |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |
    34.8196${}^{5~{}~{}}$ | 0.2295 ${}^{8~{}~{}}$ | 6.8581^(18) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) | 30.6153^(17) | 0.8298^(13)
    | -0.1424^(16) | 21.1905^(11) | 7.0124^(13) |'
  prefs: []
  type: TYPE_TB
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |
    31.9503^(11) | 0.8319^(12) | -0.0222^(11) | 21.1141^(14) | 6.1987^(23) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) | 30.6615^(16)
    | 0.7751^(23) | -0.5028^(22) | 20.7491^(21) | 7.2743${}^{2~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |
    33.7712${}^{7~{}~{}}$ | 6.9363^(15) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) | 30.5644^(18)
    | 0.8140^(18) | -0.2728^(18) | 20.9899^(16) | 6.4238^(21) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) | 29.9149^(20) | 0.8048^(20)
    | -0.3438^(19) | 20.8462^(19) | 7.0150^(12) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) | 30.1673^(19) | 0.8142^(17)
    | -0.2213^(17) | 20.8640^(18) | 7.1642${}^{6~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) | 30.7820^(15)
    | 0.7974^(21) | -0.3591^(20) | 21.0257^(15) | 7.0709^(10) |'
  prefs: []
  type: TYPE_TB
- en: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) | 29.2865^(22)
    | 0.8066^(19) | -0.4172^(21) | 20.6327^(23) | 7.0610^(11) |'
  prefs: []
  type: TYPE_TB
- en: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) | 29.8126^(21)
    | 0.7601^(24) | -0.6095^(24) | 20.7046^(22) | 7.2273${}^{4~{}~{}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) | 29.2389^(23)
    | 0.7851^(22) | -0.6051^(23) | 20.5973^(24) | 6.8613^(17) |'
  prefs: []
  type: TYPE_TB
- en: 'Table [14](#A6.T14 "Table 14 ‣ F.4 Full Comparison with Existing Evaluation
    Methods ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models") and Table [15](#A6.T15 "Table 15 ‣ F.4 Full Comparison with Existing
    Evaluation Methods ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign:
    Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating
    Text-to-Image Models") present the comparison results between our proposed method
    and existing alternatives. Owing to the powerful aligned understanding of images
    and text by MLLMs, our model achieves the best alignment with human performance
    across all tested benchmarks for 24 text-to-image models, along with a significant
    improvement in image faithfulness. This indicates that our model excels in both
    comprehending and evaluating the intricate details of generated images, closely
    mirroring human judgment and setting a new standard for image-text alignment and
    faithfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Qualitative Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in Figure [5](#A7.F5 "Figure 5 ‣ Appendix G Qualitative Analysis ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), we present a comparison of different evaluation
    metrics on images generated by four models, including human annotated scores,
    EvalAlign, ImageReward [[56](#bib.bib56)], HPSv2 [[54](#bib.bib54)], and PickScore [[20](#bib.bib20)].
    The digits in the figure represent the ranking for each evaluation metric, with
    darker colors indicating higher rankings. From the figure, it is evident that
    our proposed EvalAlign metric closely matches the human rankings across two evaluation
    dimensions, demonstrating excellent consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, Figure [6](#A7.F6 "Figure 6 ‣ Appendix G Qualitative Analysis
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models") showcases the EvalAlign evaluation metric across
    different fine-grained results. The numbers in the figure represent EvalAlign
    scores for the corresponding evaluation aspect, with darker colors indicating
    higher scores and better generation performance. Note that if the text prompt
    does not specify a particular style, the style consistency score defaults to 0.
    From these results, it is evident that the same text-to-image model exhibits significant
    performance variation across different evaluation aspects.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7680590ff159f32f823ec69fde57cd7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Qualitative results of EvalAlign benchmark. As can be concluded,
    EvalAlign is consistently aligned with fine-grained human preference in terms
    of image faithfulness and text-image alignment, while other methods fail to do
    so.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/735d8f6edbed6474fdd91e3d2241f9fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Qualitative results of EvalAlign benchmark. As can be inferred, EvalAlign
    distinctively provides multiple fine-grained scores covering every aspect of image
    faithfulness and text-image alignment. Additionally, we noticed that, in general,
    a model cannot perform well on every fine-grained aspect, which is consistent
    with [[21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
