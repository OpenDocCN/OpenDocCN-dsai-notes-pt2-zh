- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:35:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:35:52'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EvalAlign: 监督微调的多模态LLM与人类对齐数据用于评估文本生成图像模型'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16562](https://ar5iv.labs.arxiv.org/html/2406.16562)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16562](https://ar5iv.labs.arxiv.org/html/2406.16562)
- en: Zhiyu Tan¹  Xiaomeng Yang¹  Luozheng Qin¹  Mengping Yang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 谭智宇¹  杨晓萌¹  邱洛铮¹  杨梦萍¹
- en: Cheng Zhang²  Hao Li³
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 程章²  郝力³
- en: ¹ Shanghai Academy of AI for Science  ² Carnegie Mellon University  ³ Fudan
    University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 上海人工智能科学研究院  ² 卡内基梅隆大学  ³ 复旦大学
- en: '[https://github.com/SAIS-FUXI/EvalAlign](https://github.com/SAIS-FUXI/EvalAlign)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/SAIS-FUXI/EvalAlign](https://github.com/SAIS-FUXI/EvalAlign)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The recent advancements in text-to-image generative models have been remarkable.
    Yet, the field suffers from a lack of evaluation metrics that accurately reflect
    the performance of these models, particularly lacking fine-grained metrics that
    can guide the optimization of the models. In this paper, we propose EvalAlign,
    a metric characterized by its accuracy, stability, and fine granularity. Our approach
    leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained
    on extensive datasets. We develop evaluation protocols that focus on two key dimensions:
    image faithfulness and text-image alignment. Each protocol comprises a set of
    detailed, fine-grained instructions linked to specific scoring options, enabling
    precise manual scoring of the generated images. We Supervised Fine-Tune (SFT)
    the MLLM to align closely with human evaluative judgments, resulting in a robust
    evaluation model. Our comprehensive tests across 24 text-to-image generation models
    demonstrate that EvalAlign not only provides superior metric stability but also
    aligns more closely with human preferences than existing metrics, confirming its
    effectiveness and utility in model assessment.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在文本生成图像模型方面的进展非常显著。然而，该领域仍然缺乏能够准确反映这些模型性能的评估指标，特别是缺乏可以指导模型优化的细粒度指标。本文提出了EvalAlign，这是一种以其准确性、稳定性和细粒度为特征的指标。我们的方法利用了在广泛数据集上预训练的多模态大语言模型（MLLMs）的能力。我们制定了关注两个关键维度的评估协议：图像真实性和文本-图像对齐。每个协议包含一组详细的、细粒度的说明与特定评分选项相关联，使生成的图像能够进行精准的手动评分。我们对MLLM进行监督微调（SFT），使其与人类评估判断紧密对齐，从而形成一个强健的评估模型。我们在24个文本生成图像模型上的综合测试表明，EvalAlign不仅提供了更优的指标稳定性，而且比现有指标更贴近人类偏好，证实了其在模型评估中的有效性和实用性。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Text-to-image models, such as DALL·E series [[38](#bib.bib38), [37](#bib.bib37),
    [3](#bib.bib3)], Imagen [[43](#bib.bib43)], and Stable Diffusion [[33](#bib.bib33)],
    have significantly impacted various domains such as entertainment, design, and
    education, by enabling high-quality image generation. These technologies not only
    advance the field of text-to-image generation but also enhance related applications
    such as video generation [[4](#bib.bib4), [60](#bib.bib60)], image editing [[46](#bib.bib46),
    [18](#bib.bib18), [59](#bib.bib59)], and personalized image generation [[13](#bib.bib13),
    [42](#bib.bib42)]. Despite this prevalence, in the practical use of generative
    models, we still lack evaluation methods to accurately evaluate existing text-to-image
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成图像模型，如DALL·E系列 [[38](#bib.bib38), [37](#bib.bib37), [3](#bib.bib3)]、Imagen
    [[43](#bib.bib43)] 和Stable Diffusion [[33](#bib.bib33)]，通过实现高质量图像生成，已在娱乐、设计和教育等各个领域产生了显著影响。这些技术不仅推动了文本生成图像领域的发展，还提升了相关应用，如视频生成
    [[4](#bib.bib4), [60](#bib.bib60)]、图像编辑 [[46](#bib.bib46), [18](#bib.bib18), [59](#bib.bib59)]
    和个性化图像生成 [[13](#bib.bib13), [42](#bib.bib42)]。尽管如此，在生成模型的实际应用中，我们仍然缺乏准确评估现有文本生成图像模型的方法。
- en: 'Existing benchmarks for text-to-image generation models are neither comprehensive
    nor accurate enough to meet current evaluation needs. The reasons for this include:
    (1) Limited model parameters: Current evaluation models have too few parameters,
    which restricts their ability to accurately represent images, leading to significant
    discrepancies compared to human evaluations. (2) Training data limitations: Some
    evaluation methods, such as Inception Score (IS) [[44](#bib.bib44)], Frechet Inception
    Distance (FID) [[15](#bib.bib15)], and CLIP Score [[14](#bib.bib14)], use models
    that have not been trained with synthesized images, which may introduce training
    bias and flaws the evaluation. (3) High annotation costs: Some methods, such as
    ImageReward [[56](#bib.bib56)], HPS [[55](#bib.bib55)] and HPS v2 [[54](#bib.bib54)],
    rely heavily on extensive human annotations, which significantly increases the
    cost of labeling. (4) Lack of detailed evaluation metric: The evaluation metrics
    do not provide fine-grained interpretability, preventing them from guiding model
    optimization effectively. (5) Computational inefficiency: The evaluation models
    require substantial computational resources, making them inefficient.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的文本到图像生成模型的基准测试既不全面也不准确，无法满足当前的评估需求。这些原因包括：（1）模型参数有限：目前的评估模型参数过少，限制了其准确表示图像的能力，导致与人类评估相比存在显著差异。（2）训练数据限制：一些评估方法，如Inception
    Score (IS) [[44](#bib.bib44)]，Frechet Inception Distance (FID) [[15](#bib.bib15)]，和CLIP
    Score [[14](#bib.bib14)]，使用的模型没有用合成图像进行训练，这可能引入训练偏差并使评估出现缺陷。（3）高标注成本：一些方法，如ImageReward
    [[56](#bib.bib56)]，HPS [[55](#bib.bib55)] 和HPS v2 [[54](#bib.bib54)]，过度依赖大量人工标注，显著增加了标注成本。（4）缺乏详细的评估指标：评估指标没有提供细粒度的可解释性，阻碍了其对模型优化的有效指导。（5）计算效率低：评估模型需要大量计算资源，效率低下。
- en: 'To address these issues, we propose EvalAlign, which offers low-cost, accurate,
    and efficient model evaluations while providing fine-grained, interpretable metrics.
    Specifically, we Supervised Fine-Tune (SFT) a Multimodal Large Language Model
    (MLLM) to align it with human annotations. Our focus is on two key evaluation
    aspects: image faithfulness and text-image alignment.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们提出了EvalAlign，它提供低成本、准确和高效的模型评估，同时提供细粒度的可解释指标。具体而言，我们对多模态大语言模型（MLLM）进行**监督微调**（SFT），使其与人工标注对齐。我们关注两个关键评估方面：图像忠实性和文本-图像对齐。
- en: 'Due to its pre-training on large-scale datasets and a large number of model
    parameters, the MLLM demonstrates excellent understanding and generalization capabilities
    for images and instructions. This makes it possible to design evaluation instruction
    based on MLLM to evaluate text-to-image models. However, since the pre-trained
    datasets do not include model-generated images (such as distorted body structures
    and human hands) or evaluation-related text instructions, using MLLM directly
    for model evaluation does not yield optimal results. Therefore, we employ SFT
    on a small amount of manually annotated data to align MLLM with human annotations
    for text-to-image generation. In summary, our main contributions can be summarized
    as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其在大规模数据集上的预训练和大量的模型参数，MLLM展现了对图像和指令的出色理解和泛化能力。这使得基于MLLM设计评估指令以评估文本到图像模型成为可能。然而，由于预训练数据集不包含模型生成的图像（如扭曲的身体结构和人手）或评估相关的文本指令，直接使用MLLM进行模型评估无法获得**最佳**结果。因此，我们在少量手动标注的数据上进行SFT，以使MLLM与人工标注对齐，用于文本到图像生成。总之，我们的主要贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a dataset specifically designed to address the evaluation challenges
    of text-to-image models. This dataset, derived from multiple data sources, has
    been thoroughly cleaned and systematically annotated by human. It enables precise
    evaluation of text-image alignment and image faithfulness.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一个专门设计用于应对文本到图像模型评估挑战的数据集。该数据集来源于多个数据源，经过彻底清理和系统标注。它使得对文本-图像对齐和图像忠实性的准确评估成为可能。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the EvalAlign evaluation metric, which accurately aligns evaluation
    metrics with human preferences. This method is cost-effective in terms of annotation
    and training, computationally efficient, and provides interpretable metrics.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了EvalAlign评估指标，它准确地将评估指标与人类偏好对齐。这种方法在标注和训练方面具有成本效益，计算上高效，并提供了可解释的指标。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct evaluations over 24 text-to-image models and compare EvalAlign with
    existing evaluation methods. Extensive experiments demonstrate that EvalAlign
    outperforms other methods in evaluating model performance.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对 24 个文本到图像模型进行了评估，并将 EvalAlign 与现有评估方法进行了比较。大量实验表明，EvalAlign 在评估模型性能方面优于其他方法。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Evaluations and benchmarks of text-to-image generation. Despite the incredible
    progress achieved by text-to-image generation, evaluations and benchmarks in this
    area are far from flawless and contain critical limitations. For example, the
    most commonly used metrics, IS [[44](#bib.bib44)], FID [[15](#bib.bib15)], and
    CLIPScore [[14](#bib.bib14)] are broadly recognized as inaccurate for their inconsistency
    with human perception. To address, HPS series [[55](#bib.bib55), [54](#bib.bib54)],
    PickScore [[20](#bib.bib20)], and ImageReward [[56](#bib.bib56)] introduced human
    preference prior on image assessing to the benchmark, thereby allowing better
    correlation with image quality. However, with varying source and size of training
    data, these methods merely score the evaluated images in a coarse and general
    way, which cannot serve as an indication for model evolution. Meanwhile, HEIM [[21](#bib.bib21)]
    combined automatic and human evaluation and holistically evaluated text-to-image
    generation in 12 aspects, such as alignment, toxicity, and so on. As a consequence,
    HEIM relies heavily on human labour, limiting its application within budget-limited
    research groups severely. [[30](#bib.bib30)] standardized the protocol and settings
    of human evaluation, ensuring its verifiable and reproducible. There are also
    some works bear a resemble with us. For instance, TIFA [[16](#bib.bib16)], Gecko [[53](#bib.bib53)]
    and LLMScore [[27](#bib.bib27)] also formulate the evaluation as a set of visual
    question answering procedure and use LLMs as evaluation models. However, while
    they all mainly focus on text-image alignment, our approach takes both text-image
    alignment and image faithfulness into consideration. Moreover, the evaluation
    of LLMScore requires an object detection stage, which introduces significantly
    extra inference latency to the evaluation pipeline.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像生成的评估和基准。尽管文本到图像生成取得了令人难以置信的进展，但该领域的评估和基准远非完美，存在关键的局限性。例如，最常用的指标，IS [[44](#bib.bib44)]、FID [[15](#bib.bib15)]
    和 CLIPScore [[14](#bib.bib14)] 被广泛认为在与人类感知不一致方面不准确。为了解决这一问题，HPS 系列 [[55](#bib.bib55),
    [54](#bib.bib54)]、PickScore [[20](#bib.bib20)] 和 ImageReward [[56](#bib.bib56)]
    引入了图像评估的人类偏好先验，从而实现了与图像质量更好的相关性。然而，由于训练数据的来源和规模各异，这些方法仅以粗略和一般的方式对评估图像进行打分，这不能作为模型演变的指示。同时，HEIM [[21](#bib.bib21)]
    结合了自动和人工评估，从12个方面，如对齐、毒性等，全面评估了文本到图像生成。因此，HEIM 在很大程度上依赖于人工劳动，严重限制了预算有限的研究组的应用。[[30](#bib.bib30)]
    标准化了人工评估的协议和设置，确保其可验证和可重复性。还有一些工作与我们类似。例如，TIFA [[16](#bib.bib16)]、Gecko [[53](#bib.bib53)]
    和 LLMScore [[27](#bib.bib27)] 也将评估表述为一组视觉问答过程，并使用 LLMs 作为评估模型。然而，虽然它们主要关注文本-图像对齐，但我们的方法同时考虑了文本-图像对齐和图像真实性。此外，LLMScore
    的评估需要一个物体检测阶段，这在评估流程中引入了显著的额外推理延迟。
- en: 'As illustrated in Table [1](#S2.T1 "Table 1 ‣ 2 Related Work ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"), existing text-to-image evaluation methods contains various limitations,
    making them incapable to serve as a fine-grained, comprehensive, and human-preference
    aligned automatic benchmark. While our work fills in this gap economically, and
    can be employed to indicate evolution direction and support thorough analysis
    of text-to-image generation models.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格[1](#S2.T1 "Table 1 ‣ 2 Related Work ‣ EvalAlign: Supervised Fine-Tuning
    Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models")所示，现有的文本到图像评估方法存在各种局限性，使其无法作为一个细粒度、全面且符合人类偏好的自动基准。虽然我们的工作经济地填补了这一空白，并可以用于指示演变方向以及支持对文本到图像生成模型的深入分析。'
- en: 'Table 1: Comparison of different evaluation metrics and frameworks for text-to-image
    generation. EvalAlign focuses on two key evaluation aspects, i.e., image faithfulness
    and text-image alignment, and supports human-aligned, fine-grained, and automatic
    evaluations. P: Prompt. I: Image. A: Annotation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 1：不同评估指标和框架的比较，针对文本到图像生成。EvalAlign 关注两个关键评估方面，即图像真实性和文本-图像对齐，并支持符合人类偏好、细粒度和自动评估。P:
    提示。I: 图像。A: 注释。'
- en: '| Method | Venue | Benchmark Feature | Dataset Size | Evaluation Aspect |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 会议 | 基准特征 | 数据集大小 | 评估方面 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Human-aligned | Fine-grained | Automatic | P | I | A | Faithfulness | Alignment
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 人类对齐 | 细粒度 | 自动化 | P | I | A | 可信度 | 对齐 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Inception Score [[44](#bib.bib44)] | NeurIPS 2016 | ✗ | ✗ | ✓ | – | 1.3M
    | – | ✓ | ✗ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Inception Score [[44](#bib.bib44)] | NeurIPS 2016 | ✗ | ✗ | ✓ | – | 1.3M
    | – | ✓ | ✗ |'
- en: '| FID [[15](#bib.bib15)] | NeurIPS 2017 | ✗ | ✗ | ✓ | – | 1.3M | – | ✓ | ✗
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| FID [[15](#bib.bib15)] | NeurIPS 2017 | ✗ | ✗ | ✓ | – | 1.3M | – | ✓ | ✗
    |'
- en: '| CLIP-score [[14](#bib.bib14)] | EMNLP 2021 | ✗ | ✗ | ✓ | 400M | 400M | –
    | ✗ | ✓ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| CLIP-score [[14](#bib.bib14)] | EMNLP 2021 | ✗ | ✗ | ✓ | 400M | 400M | –
    | ✗ | ✓ |'
- en: '| HPS [[55](#bib.bib55)] | ICCV 2023 | ✓ | ✗ | ✓ | 25K | 98K | 25K | – | –
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| HPS [[55](#bib.bib55)] | ICCV 2023 | ✓ | ✗ | ✓ | 25K | 98K | 25K | – | –
    |'
- en: '| TIFA [[16](#bib.bib16)] | ICCV 2023 | ✓ | ✓ | ✓ | 4K | – | 25K | ✗ | ✓ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| TIFA [[16](#bib.bib16)] | ICCV 2023 | ✓ | ✓ | ✓ | 4K | – | 25K | ✗ | ✓ |'
- en: '| TVRHE [[30](#bib.bib30)] | CVPR 2023 | ✓ | ✗ | ✗ | – | – | – | ✓ | ✗ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| TVRHE [[30](#bib.bib30)] | CVPR 2023 | ✓ | ✗ | ✗ | – | – | – | ✓ | ✗ |'
- en: '| ImageReward [[56](#bib.bib56)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 8.8K | 68K |
    137K | – | – |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ImageReward [[56](#bib.bib56)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 8.8K | 68K |
    137K | – | – |'
- en: '| PickScore [[20](#bib.bib20)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 35K | 1M | 500K
    | – | – |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| PickScore [[20](#bib.bib20)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 35K | 1M | 500K
    | – | – |'
- en: '| HPS v2 [[54](#bib.bib54)] | arXiv 2023 | ✓ | ✗ | ✓ | 107K | 430K | 645K |
    – | – |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| HPS v2 [[54](#bib.bib54)] | arXiv 2023 | ✓ | ✗ | ✓ | 107K | 430K | 645K |
    – | – |'
- en: '| HEIM [[21](#bib.bib21)] | NeurIPS 2023 | ✓ | ✓ | ✗ | – | – | – | ✓ | ✓ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| HEIM [[21](#bib.bib21)] | NeurIPS 2023 | ✓ | ✓ | ✗ | – | – | – | ✓ | ✓ |'
- en: '| Gecko [[53](#bib.bib53)] | arXiv 2024 | ✓ | ✓ | ✓ | 2K | – | 108K | ✗ | ✓
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Gecko [[53](#bib.bib53)] | arXiv 2024 | ✓ | ✓ | ✓ | 2K | – | 108K | ✗ | ✓
    |'
- en: '| LLMScore [[27](#bib.bib27)] | arXiv 2024 | ✓ | ✓ | ✓ | – | – | – | ✗ | ✓
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| LLMScore [[27](#bib.bib27)] | arXiv 2024 | ✓ | ✓ | ✓ | – | – | – | ✗ | ✓
    |'
- en: '| EvalAlign (ours) | – | ✓ | ✓ | ✓ | 3K | 21K | 132K | ✓ | ✓ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign（我们的） | – | ✓ | ✓ | ✓ | 3K | 21K | 132K | ✓ | ✓ |'
- en: Multimodal Large Language Models (MLLMs). Pretrained on massive text-only and
    image-text data, MLLMs have exhibited exceptional image-text joint understanding
    and generalization abilities, facilitating a large spectrum of downstream applications.
    Among the works major in MLLMs, LLaVA [[26](#bib.bib26), [24](#bib.bib24)] and
    MiniGPT4 [[61](#bib.bib61), [5](#bib.bib5)] proposed to conduct visual instruction
    tuning during SFT, so that MLLMs can be easily aligned to human preference and
    precisely answer fine-grained questions on visual content. Meanwhile, Video-LLaMA [[58](#bib.bib58)]
    and VideoChat [[22](#bib.bib22)] utilized MLLMs for video understanding. VILA [[23](#bib.bib23)]
    quantitatively proved that involving text-only instruction-tuning data during
    SFT can further ameliorate model performance on text-only and multimodal downstream
    tasks. LLaVA-NeXT [[25](#bib.bib25)] extracted visual tokens for both the resized
    input image and the segmented sub-images to provide more detailed visual information
    for MLLMs, achieving significant performance bonus on tasks with high-resolution
    input images.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大型语言模型（MLLMs）。在大规模文本和图像-文本数据上进行预训练的 MLLMs 展现了卓越的图像-文本联合理解和泛化能力，促进了广泛的下游应用。在主要研究
    MLLMs 的工作中，LLaVA [[26](#bib.bib26), [24](#bib.bib24)] 和 MiniGPT4 [[61](#bib.bib61),
    [5](#bib.bib5)] 提出了在 SFT 期间进行视觉指令调优，以使 MLLMs 能够轻松地与人类偏好对齐，并准确回答关于视觉内容的细粒度问题。同时，Video-LLaMA [[58](#bib.bib58)]
    和 VideoChat [[22](#bib.bib22)] 利用 MLLMs 进行视频理解。VILA [[23](#bib.bib23)] 定量证明了在
    SFT 期间涉及文本-only 指令调优数据可以进一步改善模型在文本-only 和多模态下游任务上的表现。LLaVA-NeXT [[25](#bib.bib25)]
    提取了调整大小的输入图像和分割子图像的视觉标记，以提供更多详细的视觉信息，从而在处理高分辨率输入图像的任务中取得了显著的性能提升。
- en: In this paper, we manually annotate a minimal amount of high-quality visual
    instruction-tuning data to adapt MLLMs to perform human preference aligned and
    fine-grained evaluation. Owing to the generalization and multimodal understanding
    abilities of MLLMs, our experiments demonstrated that the finetuned MLLMs can
    accurately assess text-to-image generation models by following instructions regarding
    various fine-grained criteria of synthesised image.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们手动标注了一小部分高质量的视觉指令调优数据，以使多模态大型语言模型（MLLMs）适应执行与人类偏好对齐的细粒度评估。由于 MLLMs 的泛化能力和多模态理解能力，我们的实验表明，经过微调的
    MLLMs 可以通过遵循关于合成图像各种细粒度标准的指令，准确评估文本到图像生成模型。
- en: 3 EvalAlign Dataset Construction
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 EvalAlign 数据集构建
- en: To train, validate and test the effectiveness of our evaluation models, we build
    EvalAlign benchmark. Specifically, EvalAlign dataset is a meticulously annotated
    collection featuring fine-grained annotations for images generated from text prompts.
    This dataset comprises 21k images, each accompanied by detailed instructions.
    The compilation process for the EvalAlign Dataset encompasses prompt collection,
    image generation, and precise instruction-based annotation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练、验证和测试我们评估模型的有效性，我们构建了 EvalAlign 基准测试。具体而言，EvalAlign 数据集是一个经过精细标注的集合，包含了从文本提示生成的图像的细粒度注释。该数据集包括
    21k 张图像，每张图像都附有详细的说明。EvalAlign 数据集的编制过程包括提示收集、图像生成和基于指令的精确注释。
- en: 3.1 Prompts and Images Collection
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 提示和图像收集
- en: Prompt collection. To assess the capabilities of our model in terms of text-image
    alignment and image faithfulness, we collect, filter, and clean prompts from existing
    evaluation datasets and generated prompts based on LLM. These prompts encompass
    a diverse range from real-world user prompts, prompts generated through rule-based
    templates with LLM, to manually crafted prompts. Specifically, the utilized prompts
    are soureced from HPS [[55](#bib.bib55)], HRS-Bench [[2](#bib.bib2)], HPSv2 [[54](#bib.bib54)],
    TIFA [[16](#bib.bib16)], DSG [[7](#bib.bib7)], T2I-Comp [[17](#bib.bib17)], Winoground [[50](#bib.bib50)],
    DALL-EVAL [[8](#bib.bib8)], DiffusionDB [[52](#bib.bib52)], PartiPrompts [[57](#bib.bib57)],
    DrawBench [[43](#bib.bib43)], and JourneryDB [[49](#bib.bib49)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 提示收集。为了评估我们模型在文本与图像对齐和图像真实性方面的能力，我们从现有评估数据集中收集、过滤和清理提示，并基于 LLM 生成提示。这些提示涵盖了从现实用户提示、通过规则模板与
    LLM 生成的提示，到手工制作的提示等多种范围。具体而言，所使用的提示来源于 HPS [[55](#bib.bib55)]、HRS-Bench [[2](#bib.bib2)]、HPSv2
    [[54](#bib.bib54)]、TIFA [[16](#bib.bib16)]、DSG [[7](#bib.bib7)]、T2I-Comp [[17](#bib.bib17)]、Winoground
    [[50](#bib.bib50)]、DALL-EVAL [[8](#bib.bib8)]、DiffusionDB [[52](#bib.bib52)]、PartiPrompts
    [[57](#bib.bib57)]、DrawBench [[43](#bib.bib43)] 和 JourneryDB [[49](#bib.bib49)]。
- en: Prompt curation. The prompts are collected for facilitating fine-grained and
    comprehensive evaluation on text-to-image models in terms of image faithfulness
    and text-image alignment. Considering some of the collected prompt are unsuitable
    for these two evaluation tasks, we filter the collected prompts to ensure their
    quantity, quality and diversity. For the image faithfulness evaluation task, we
    prioritize prompts related to human, animals, and other tangible objects, as prompts
    depicting sci-fi scenarios are less suitable for this type of assessment. Our
    filtering process initially selects prompts that describe human, animals, and
    other real objects. After deduplicating these prompts, we carefully selected 1,500
    distinct prompts with varying topic, background and style for further evaluation.
    The selected prompts encompass 10k subjects across 15 categories. For the text-image
    alignment task, we refined our selection based on descriptions of style, color,
    quantity, and spatial relationships in the prompts. Only those prompts containing
    relevant descriptions and exceeding 15 words in length were considered, culminating
    in a final set of 1,500 prompts for evaluation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提示策划。收集这些提示是为了在文本到图像模型的图像真实性和文本与图像对齐方面进行细粒度和全面的评估。考虑到一些收集到的提示不适合这两种评估任务，我们过滤了收集到的提示，以确保其数量、质量和多样性。对于图像真实性评估任务，我们优先考虑与人类、动物及其他有形物体相关的提示，因为描述科幻场景的提示不太适合这种评估。我们的过滤过程最初选择描述人类、动物和其他真实物体的提示。在去重这些提示后，我们仔细挑选了
    1,500 个主题、背景和风格各异的提示用于进一步评估。所选提示涵盖了 15 个类别的 10k 个主题。对于文本与图像对齐任务，我们根据提示中的风格、颜色、数量和空间关系的描述来精炼我们的选择。只有那些包含相关描述且长度超过
    15 个词的提示被考虑，最终得出 1,500 个提示用于评估。
- en: '![Refer to caption](img/bff799c750b2bca9e6aac29a9c23f5fe.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bff799c750b2bca9e6aac29a9c23f5fe.png)'
- en: 'Figure 1: Overview of EvalAlign. We collect, filter and clean prompts from
    various sources to ensure their quantity, quality and diversity. We use 8 state-of-the-art
    text-to-image models to the generate images for evaluation. These synthesized
    images are then delegated to human annotators for thorough multi-turn annotation.
    Finally, the annotated data are used to SFT train a MLLM to align it with fine-grained
    human preference, thereby adapting the model to perform fine-grained human-aligned
    text-to-image evaluations.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：EvalAlign概览。我们从各种来源收集、过滤和清理提示，以确保其数量、质量和多样性。我们使用8种最先进的文本到图像模型生成图像用于评估。这些合成图像随后交由人工注释员进行彻底的多轮注释。最终，注释后的数据用于对MLLM进行SFT训练，使其与细致的人类偏好对齐，从而使模型能够进行细粒度的人类对齐的文本到图像评估。
- en: Image generation. To train and evaluate the MLLM, we use a diverse set of images
    generated by various models using the aforementioned prompts, allowing for detailed
    human annotation. For each prompt, multiple images are generated across different
    models. The dataset is enriched by images synthesized by models with varying architectures
    and scales, enhancing the diversity essential for a comprehensive evaluation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成。为了训练和评估MLLM，我们使用了通过各种模型生成的多样化图像，利用上述提示进行详细的人类注释。对于每个提示，多个模型生成的图像被纳入数据集中。数据集通过具有不同架构和规模的模型合成的图像得到了丰富，增强了全面评估所需的多样性。
- en: This variety not only tests the MLLM generalization capabilities but also aids
    in developing a model with broader applicability. The training dataset incorporates
    images from 8 models, whereas the test dataset spans 24 models. The inclusion
    of 16 unseen models in the test set is crucial for evaluating the MLLM to generalize
    beyond its training data. For detailed information on the inference setting of
    each model, please refer to the supplementary material. This structured approach
    ensures a robust framework for training and validating the MLLM, positioning it
    as a versatile and adaptive tool in the field of image generation from textual
    descriptions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样性不仅测试了MLLM的泛化能力，还帮助开发具有更广泛适用性的模型。训练数据集包含来自8个模型的图像，而测试数据集涵盖24个模型。测试集中的16个未见过的模型对于评估MLLM是否能够超越其训练数据进行泛化至关重要。有关每个模型推理设置的详细信息，请参阅补充材料。这一结构化的方法确保了对MLLM的训练和验证框架的稳健性，使其成为文本描述生成图像领域中的一种多功能和适应性强的工具。
- en: 3.2 Data Annotation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据注释
- en: 'Prompt annotation. For text prompts focused on text-image alignment, we begin
    by annotating the entities and their attributes within the text, as illustrated
    in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Prompts and Images Collection ‣ 3 EvalAlign
    Dataset Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models"). Our annotators extract
    the entities mentioned in the prompts and label each entity with corresponding
    attributes, including quantity, color, spatial relationships, and actions. During
    the annotation, we also ask the annotators annotate the overall style of the image
    if described in the corresponding prompt and report prompts that contain toxic
    and NSFW content. These high-quality and detailed annotations facilitate the subsequent
    SFT training and evaluation of the MLLM. This meticulous annotation procedure
    ensures that the MLLM can accurately align and respond to the nuanced details
    specified in the prompts, enhancing both the training process and the model’s
    performance in generating images that faithfully reflect the described attributes
    and style.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '提示注释。对于关注文本-图像对齐的文本提示，我们首先对文本中的实体及其属性进行注释，如图[1](#S3.F1 "图1 ‣ 3.1 提示和图像收集 ‣
    3 EvalAlign数据集构建 ‣ EvalAlign: 通过人类对齐数据对多模态LLMs进行监督微调以评估文本到图像模型")所示。我们的注释员提取提示中提到的实体，并标记每个实体的对应属性，包括数量、颜色、空间关系和动作。在注释过程中，我们还要求注释员在提示中描述的情况下注释图像的整体风格，并报告包含有害和NSFW内容的提示。这些高质量和详细的注释有助于后续的MLLM的SFT训练和评估。这一精细的注释过程确保了MLLM能够准确对齐并响应提示中指定的细微细节，从而提高训练过程和模型生成忠实反映描述的属性和风格的图像的表现。'
- en: 'Image annotation. The images generated by text-to-image models often present
    challenges such as occluded human body parts, which can impede the effectiveness
    of SFT training and evaluation of the MLLM. Therefore, to address these challenges
    and enhance the model’s training and evaluative capabilities, specific annotations
    are applied to all images depicting human and animals. These annotations include:
    presence of human or animal faces; visibility of hands; visibility of limbs. By
    implementing these annotations, we ensure that the MLLM can more effectively learn
    from and assess the completeness and faithfulness of the generated images. This
    structured approach to annotation not only aids in identifying common generation
    errors but also optimizes the model’s ability to generate more accurate and realistic
    images, thereby improving both training outcomes and the model’s overall performance
    in generating coherent and contextually appropriate visual content.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标注。文本到图像模型生成的图像经常出现诸如遮挡的人体部位等挑战，这可能会妨碍SFT训练和MLLM的评估。因此，为了解决这些挑战并提升模型的训练和评估能力，对所有描绘人类和动物的图像应用了特定的标注。这些标注包括：人脸或动物脸的存在；手部的可见性；肢体的可见性。通过实施这些标注，我们确保MLLM能更有效地从生成图像中学习并评估其完整性和忠实度。这种结构化的标注方法不仅有助于识别常见的生成错误，还优化了模型生成更准确和真实图像的能力，从而提升训练效果和模型在生成连贯且上下文适当的视觉内容方面的整体表现。
- en: Instruction-Fine-tuning data annotation. To align the MLLM with human preference
    prior on fine-grained image assessing, we can train the model on a minimal amount
    of fine-grained human preference data through SFT training. As a consequence,
    we devise two sets of questions, each is concentrated on a specific fine-grained
    aspect of image faithfulness and image-text alignment, and ask human annotators
    to answer these questions to acquire the fine-grained human preference data. To
    aid the human annotators to understand the meaning of each question and its available
    answer options, thereby ensuring high annotation quality, we employ a thorough
    and comprehensive procedure of annotation preparation. First, we write a detailed
    annotation guideline and conduct a training for the annotators to explain the
    annotation guideline and answer their questions about the annotation (see supplementary
    materials for the annotation guideline). Then, we conduct a multi-turn trial annotation
    on another 50 synthesized images. After each trial, we calculate the Cohen’s kappa
    coefficient and interpret annotation guidelines for our annotators. In total,
    we conduct nine turns of trial annotation, and in the last turn of the trial,
    the Cohen’s kappa coefficient of our annotators reaches $0.681$, indicating high
    inter-annotator reliability and high annotation quality. More results about the
    trial annotation will be included in the supplementary materials.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 指令-细化数据标注。为了使MLLM与人类在细致图像评估上的偏好保持一致，我们可以通过SFT训练在最少量的细化人类偏好数据上训练模型。因此，我们设计了两组问题，每组集中于图像忠实度和图像-文本对齐的特定细致方面，并请人工标注者回答这些问题以获取细化的人类偏好数据。为了帮助人工标注者理解每个问题及其可选答案，从而确保高质量的标注，我们采用了全面细致的标注准备程序。首先，我们编写了详细的标注指南，并对标注者进行了培训，以解释标注指南并回答他们有关标注的问题（详见附录材料中的标注指南）。然后，我们在另外50张合成图像上进行多轮试验标注。每轮试验后，我们计算Cohen的kappa系数并为我们的标注者解读标注指南。总共进行了九轮试验标注，在最后一轮试验中，我们的标注者的Cohen
    kappa系数达到了$0.681$，表明标注者之间的一致性和标注质量都很高。更多关于试验标注的结果将包含在附录材料中。
- en: After completing the aforementioned preparations, we delegate the images filtered
    during image annotation to 10 annotators and ask them to complete the annotation
    just as how they did in the trial annotation. Furthermore, during the whole annotation
    procedure, four text-to-image generation experts conduct random sampling quality
    inspection on the present annotated results, causing a second and a third re-annotation
    on 423 and 112 inspection-failed samples. Overall, owing to the valuable work
    of our human annotators and our fastidious annotation procedure, we get quality-sufficient
    instruction-tuning data required for the SFT training of the MLLM. More details
    of the annotation procedure will be introduced in the supplementary files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上述准备工作后，我们将图像注释期间过滤的图像分配给10位注释员，并要求他们按照试注释时的方式完成注释。此外，在整个注释过程中，四位文本生成图像的专家会对当前注释结果进行随机抽样质量检查，这导致了对423个和112个检查失败样本的第二次和第三次重新注释。总体而言，由于我们人类注释员的宝贵工作和我们严格的注释程序，我们获得了SFT训练MLLM所需的质量充分的指令调整数据。注释程序的更多细节将在补充文件中介绍。
- en: 3.3 Dataset Statistics
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据集统计
- en: 'To summarize, we generate 24k images from 3k prompts based on 8 text-to-image
    models, which includes DeepFloyd IF [[1](#bib.bib1)], SD15 [[40](#bib.bib40)],
    LCM [[28](#bib.bib28)], SD21 [[40](#bib.bib40)], SDXL [[33](#bib.bib33)], Wuerstchen [[32](#bib.bib32)],
    Pixart [[6](#bib.bib6)], and SDXL-Turbo [[48](#bib.bib48)]. After data filtering,
    4.5k images were selected as annotation data for task of text-image alignment.
    Subsequently, these images were carefully annotated to generate 13.5k text-image
    pairs, where 11.4k were used to the training dataset and 2.1k to the validation
    dataset. For the image faithfulness task, we select 12k images for annotate, producing
    36k text-image pairs, with 30k are used to the training dataset and 6.2k to the
    validation dataset. Additionally, we employed 24 text-to-image models to generate
    2.4k images from 100 prompts. After annotation, these images were used as testing
    dataset. Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign Dataset
    Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models") and Figure [3](#S3.F3 "Figure 3 ‣ 3.3
    Dataset Statistics ‣ 3 EvalAlign Dataset Construction ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models") show the distribution of objects in different categories within our prompts,
    demonstrating the diversity and balance of our prompts.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，我们基于8个文本生成图像模型，从3k个提示生成了24k个图像，这些模型包括DeepFloyd IF [[1](#bib.bib1)]，SD15 [[40](#bib.bib40)]，LCM [[28](#bib.bib28)]，SD21 [[40](#bib.bib40)]，SDXL [[33](#bib.bib33)]，Wuerstchen [[32](#bib.bib32)]，Pixart [[6](#bib.bib6)]，和SDXL-Turbo [[48](#bib.bib48)]。经过数据过滤，选择了4.5k个图像作为文本-图像对齐任务的注释数据。随后，这些图像经过精心注释，生成了13.5k个文本-图像对，其中11.4k用于训练数据集，2.1k用于验证数据集。对于图像真实性任务，我们选择了12k个图像进行注释，生成了36k个文本-图像对，其中30k用于训练数据集，6.2k用于验证数据集。此外，我们使用24个文本生成图像模型从100个提示生成了2.4k个图像。经过注释后，这些图像被用作测试数据集。图 [3](#S3.F3
    "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign Dataset Construction ‣ EvalAlign:
    Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating
    Text-to-Image Models")和图 [3](#S3.F3 "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign
    Dataset Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models")展示了我们提示中不同类别对象的分布，体现了我们提示的多样性和平衡性。'
- en: '![Refer to caption](img/17b5b4110ec3af14f95215057517430f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17b5b4110ec3af14f95215057517430f.png)'
- en: 'Figure 2: Statistics of prompts on evaluating text-to-image alignment. Prompts
    in our text-to-image alignment benchmark covers a broad range of concepts commonly
    used in text-to-image generation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 评估文本生成图像对齐的提示统计。我们文本生成图像对齐基准中的提示涵盖了文本生成图像中常用的广泛概念。'
- en: '![Refer to caption](img/cac56fc5e2d00b8e486c6e7df212f128.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cac56fc5e2d00b8e486c6e7df212f128.png)'
- en: 'Figure 3: Statistics of prompts on evaluating image faithfulness. Prompts in
    our image faithfulness benchmark covers a broad range of objects and categories
    that related to image faithfulnes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 评估图像真实性的提示统计。我们图像真实性基准中的提示涵盖了与图像真实性相关的广泛对象和类别。'
- en: 4 Training and Evaluation Methods
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 训练和评估方法
- en: 4.1 Supervised Fine-Tuning the MLLM
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 对MLLM的监督微调
- en: 'As we mentioned above, we use MLLMs as the evaluation models and let it to
    answer a set of carefully-designed instructions, thereby achieving quantitative
    measurement of fine-grained text-to-image generation skills. Due to training bias,
    zero-shot MLLMs perform poorly when it comes to fine-grained evaluation on synthesized
    images, particularly in term of image faithfulness. To address, we conduct SFT
    training on the annotated data to align the MLLM with human preference prior on
    fine-grained image assessing. Formally, the SFT training sample can be denoted
    as a triplet, i.e., $(Q,M,A)$ denotes the question (or the instruction), the multimodal
    input and the answer, respectively. Specifically, $Q$ is mainly the image and
    necessary textual description, while $A$. Specifically, the loss function can
    be formulated as follows, where $N$ is the length of the ground truth answer:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用 MLLMs 作为评估模型，并让其回答一组精心设计的指令，从而实现对细粒度文本到图像生成技能的定量测量。由于训练偏差，零-shot MLLMs
    在合成图像的细粒度评估中表现不佳，特别是在图像真实性方面。为了解决这一问题，我们在标注数据上进行 SFT 训练，以使 MLLM 与人类对细粒度图像评估的偏好对齐。形式上，SFT
    训练样本可以表示为一个三元组，即 $(Q,M,A)$ 表示问题（或指令）、多模态输入和答案。具体来说，$Q$ 主要是图像和必要的文本描述，而 $A$。具体来说，损失函数可以如下公式化，其中
    $N$ 是真实答案的长度：
- en: '|  | $\displaystyle L(\theta)=\sum^{N}_{i=1}\log p(A_{i}&#124;Q,M,A_{<i};\theta).$
    |  | (1) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(\theta)=\sum^{N}_{i=1}\log p(A_{i}&#124;Q,M,A_{<i};\theta).$
    |  | (1) |'
- en: 4.2 Evaluation and Metrics
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估和指标
- en: 'During inference, the MLLM, parameterized by $\theta$, given the specific question
    $Q$ in an autoregressive way:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，参数化为 $\theta$ 的 MLLM 以自回归的方式给定特定问题 $Q$：
- en: '|  | $\displaystyle R_{i}=f(Q,M,R_{<i};\theta).$ |  | (2) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{i}=f(Q,M,R_{<i};\theta).$ |  | (2) |'
- en: 'This autoregressive generation procedure is considered complete once the model
    generates a EOS token or the generated response exceeds the preset maximum generation
    length. After the generation, we use rule-based filtering and regular expression
    to extract the option chosen by the MLLM, where each option has an exclusive predefined
    score to quantitatively measure a fine-grained skill specified by the question
    $Q$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该自回归生成程序在模型生成 EOS 令牌或生成的响应超过预设的最大生成长度时被视为完成。在生成后，我们使用基于规则的过滤和正则表达式来提取 MLLM 选择的选项，其中每个选项都有一个专门的预定义分数，用于定量衡量由问题
    $Q$ 指定的细粒度技能：
- en: '|  | $\displaystyle\text{Score}(Q)=g(R)=g(f(Q,M;\theta)),$ |  | (3) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Score}(Q)=g(R)=g(f(Q,M;\theta)),$ |  | (3) |'
- en: 'where $g(\cdot)$ and $S_{a}$, that encompass every aspect of image faithfulness
    and text-image alignment, respectively. Consequently, Our metric EvalAlign can
    be defined by simply summing the scores of the two question sets:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g(\cdot)$ 和 $S_{a}$，分别涵盖了图像真实性和文本-图像对齐的各个方面。因此，我们的指标 EvalAlign 可以通过简单地将两个问题集的分数相加来定义：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $\text{EvalAlign}_{\text{f}}$ indicates the image faithfulness score and
    image-text alignment score evaluated by EvalAlign.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\text{EvalAlign}_{\text{f}}$ 表示图像真实性分数和 EvalAlign 评估的图像-文本对齐分数。
- en: 4.3 Implementation Details
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实现细节
- en: We leverage the LLaVA-NeXT model and applied the LoRA fine-tuning method to
    align it with the training dataset as described previously. The entire training
    process was conducted on $32$ hours, using a learning rate of 5$\times$10^(-5).
    Additionally, we restricted the LoRA fine-tuning to the Q and K weights of the
    attention module, as extending the fine-tuning to the ViT and projection modules
    was found to cause overfitting. Importantly, our method does not necessitate any
    modifications to the MLLM, rendering it compatible with any MLLM model. For the
    ablation study, we evaluated the LLaVA-NeXT 13B model on the validation dataset.
    In the final experiment, we apply supervised fine-tuning to the LLaVA-NeXT 34B
    model on the testing dataset and compare against different evaluation metrics.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了 LLaVA-NeXT 模型，并应用了 LoRA 微调方法将其与前述训练数据集对齐。整个训练过程持续了 $32$ 小时，使用了 $5\times10^{-5}$
    的学习率。此外，我们将 LoRA 微调限制在注意力模块的 Q 和 K 权重上，因为将微调扩展到 ViT 和投影模块会导致过拟合。重要的是，我们的方法不需要对
    MLLM 进行任何修改，使其与任何 MLLM 模型兼容。对于消融研究，我们在验证数据集上评估了 LLaVA-NeXT 13B 模型。在最后的实验中，我们对测试数据集上的
    LLaVA-NeXT 34B 模型进行监督微调，并与不同的评估指标进行比较。
- en: 5 Experimental Results
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: In this section, we show results of EvalAlign on benchmarking 24 text-to-image
    generative models. We also compare with popular text-to-image metrics and frameworks,
    including Inception Score (IS) [[44](#bib.bib44)], Frechet Inception Distance
    (FID) [[15](#bib.bib15)], CLIP-score [[14](#bib.bib14)], and HPS v2 [[54](#bib.bib54)].
    Please refer to supplementary materials for detailed results and analyses.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了EvalAlign在24个文本到图像生成模型的基准测试结果。我们还与流行的文本到图像评估指标和框架进行比较，包括Inception
    Score (IS) [[44](#bib.bib44)]、Frechet Inception Distance (FID) [[15](#bib.bib15)]、CLIP-score [[14](#bib.bib14)]和HPS
    v2 [[54](#bib.bib54)]。详细的结果和分析请参见补充材料。
- en: 5.1 Main Results
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主要结果
- en: 'Table 2: Results on image faithfulness.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 图像忠实度结果。'
- en: '| Model | Human | EvalAlign |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 人类 | EvalAlign |'
- en: '| --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ |'
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ |'
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ |'
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ |'
- en: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ |'
- en: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ |'
- en: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) |'
- en: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ |'
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ |'
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) |'
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) |'
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) |'
- en: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) |'
- en: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) |'
- en: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) |'
- en: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) |'
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    |'
- en: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) |'
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) |'
- en: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) |'
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |'
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) |'
- en: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) |'
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) |'
- en: 'Table 3: Results on text-to-image alignment.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 文本到图像对齐结果。'
- en: '| Model | Human | EvalAlign |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 人类 | EvalAlign |'
- en: '| --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ |'
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ |'
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ |'
- en: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ |'
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ |'
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ |'
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ |'
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ |'
- en: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ |'
- en: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) |'
- en: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) |'
- en: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) |'
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |'
- en: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) |'
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |'
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) |'
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |'
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) |'
- en: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) |'
- en: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) |'
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) |'
- en: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) |'
- en: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) |'
- en: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) |'
- en: 'Evaluation on image faithfullness. We conducted the image faithfullness evaluation
    on the testing dataset, as detailed in Table [3](#S5.T3 "Table 3 ‣ 5.1 Main Results
    ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models"). Our evaluation metric,
    EvalAlign, demonstrates close alignment with human preferences Specifically, the
    rankings of the top and bottom 10 models by both EvalAlign and human evaluation
    scores show remarkable consistency, confirming that our metric closely mirrors
    human evaluation. Importantly, most of the images from the 24 generative models
    were not present during training, showcasing the robust generalization capability
    of our evaluation method.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '关于图像真实性的评估。我们在测试数据集上进行了图像真实性评估，具体细节见表[3](#S5.T3 "Table 3 ‣ 5.1 Main Results
    ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models")。我们的评估指标EvalAlign显示与人工偏好高度一致。具体而言，EvalAlign和人工评估分数中排名前十和最后十的模型的排名显示出显著的一致性，确认了我们的指标与人工评估高度吻合。重要的是，大多数来自24个生成模型的图像在训练期间并未出现，展示了我们评估方法的强大泛化能力。'
- en: 'Evaluation on text-image alignment. The evaluation of text-image alignment
    on the testing dataset was conducted follow the same manner of image faithfullness
    evaluation. Table [3](#S5.T3 "Table 3 ‣ 5.1 Main Results ‣ 5 Experimental Results
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models") reveals that the rankings based on the EvalAlign
    metric and human evaluation metric for the 24 models are generally consistent.
    This consistency likely stems from the proximity of MLLM’s pre-training tasks
    to the text-image alignment evaluation tasks, resulting in better performance
    on text-image alignment evaluation task.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '关于文本-图像对齐的评估。对测试数据集上文本-图像对齐的评估采用了与图像真实性评估相同的方法。表[3](#S5.T3 "Table 3 ‣ 5.1 Main
    Results ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models")显示，基于EvalAlign指标和人工评估指标的24个模型的排名一般是一致的。这种一致性可能源于MLLM的预训练任务与文本-图像对齐评估任务的接近，从而在文本-图像对齐评估任务中表现更佳。'
- en: 5.2 Ablations and Analyses of EvalAlign
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 EvalAlign的消融实验和分析
- en: 'Results on different prompt categories. SFT significantly enhances the performance
    of MLLMs on evaluation tasks. We conducted experiments comparing the LLava-Next
    13B model with and without SFT. As shown in Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations
    and Analyses of EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning
    Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models")
    and Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental
    Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models"), the results demonstrate that SFT considerably
    improves performance across all prompt categories in both image faithfulness and
    text-to-image alignment, closely aligning the MLLM’s predictions with human evaluations.
    Note that Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign
    ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models") shows that the baseline
    method without SFT performs poorly on the image faithfulness task, which suggests
    the necessity of SFT.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '不同提示类别的结果。SFT显著提高了MLLMs在评估任务上的表现。我们进行了LLava-Next 13B模型在有无SFT的比较实验。如表 [5](#S5.T5
    "表 5 ‣ 5.2 EvalAlign的消融和分析 ‣ 5 实验结果 ‣ EvalAlign: 通过人类对齐数据对多模态LLMs进行监督微调以评估文本到图像模型")和表 [5](#S5.T5
    "表 5 ‣ 5.2 EvalAlign的消融和分析 ‣ 5 实验结果 ‣ EvalAlign: 通过人类对齐数据对多模态LLMs进行监督微调以评估文本到图像模型")所示，结果表明SFT在所有提示类别中的表现都有显著提升，无论是在图像真实性还是文本到图像对齐方面，都与人类评估高度一致。注意，表 [5](#S5.T5
    "表 5 ‣ 5.2 EvalAlign的消融和分析 ‣ 5 实验结果 ‣ EvalAlign: 通过人类对齐数据对多模态LLMs进行监督微调以评估文本到图像模型")显示，未进行SFT的基线方法在图像真实性任务上的表现较差，这表明SFT的必要性。'
- en: 'Table 4: Results of different prompt categories for evaluating image faithfulness.
    Baseline is the vanilla LLaVA-NeXT model without find-tuning with human-aligned
    data.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同提示类别下的图像真实性评估结果。基线是未经人类对齐数据微调的原始LLaVA-NeXT模型。'
- en: '| Method | Body | Hand | Face | Object | Common |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 身体 | 手 | 脸 | 对象 | 常见 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Human | 1.6701 | 1.0278 | 1.4107 | 2.2968 | 1.0637 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 1.6701 | 1.0278 | 1.4107 | 2.2968 | 1.0637 |'
- en: '| Baseline | 3.9950 | 3.9932 | 3.9867 | 2.6734 | 3.3476 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 3.9950 | 3.9932 | 3.9867 | 2.6734 | 3.3476 |'
- en: '| EvalAlign | 1.7305 | 0.9490 | 1.4393 | 2.3565 | 1.0903 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 1.7305 | 0.9490 | 1.4393 | 2.3565 | 1.0903 |'
- en: 'Table 5: Results of different prompt categories for evaluating text-to-image
    alignment. Baseline is the vanilla LLaVA-NeXT model without find-tuning with human-aligned
    data.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 不同提示类别下的文本到图像对齐评估结果。基线是未经人类对齐数据微调的原始LLaVA-NeXT模型。'
- en: '| Method | Object | Count | Color | Style | Spatial | Action |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 对象 | 数量 | 颜色 | 风格 | 空间 | 动作 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | 1.6947 | 1.2032 | 1.8551 | 1.9796 | 1.5608 | 1.8015 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 1.6947 | 1.2032 | 1.8551 | 1.9796 | 1.5608 | 1.8015 |'
- en: '| Baseline | 1.5602 | 1.0742 | 1.9275 | 1.1837 | 1.4118 | 1.1838 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 1.5602 | 1.0742 | 1.9275 | 1.1837 | 1.4118 | 1.1838 |'
- en: '| EvalAlign | 1.6807 | 1.2516 | 1.8696 | 1.9592 | 1.5882 | 1.8382 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 1.6807 | 1.2516 | 1.8696 | 1.9592 | 1.5882 | 1.8382 |'
- en: 'Table 6: Ablation study on the size of training data. Results are reported
    on image faithfulness under different training data scale. We observe that a small
    number of annotated training data is sufficient for optimal results.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 关于训练数据规模的消融研究。结果报告了不同训练数据规模下的图像真实性。我们观察到少量标注的训练数据已足以获得最佳结果。'
- en: '| Method | Data Size | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5
    | SD v2.1 | LCM |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据规模 | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5 | SD v2.1
    | LCM |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766 | 1.0066
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | 200 | 1.7443 | 1.8898 | 1.9278 | 1.1261 | 1.2977 | 1.5254 | 1.4309
    | 1.1204 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 200 | 1.7443 | 1.8898 | 1.9278 | 1.1261 | 1.2977 | 1.5254 | 1.4309
    | 1.1204 |'
- en: '| 500 | 1.8890 | 1.9161 | 1.8586 | 1.2141 | 1.3109 | 1.3926 | 1.3815 | 0.9485
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 1.8890 | 1.9161 | 1.8586 | 1.2141 | 1.3109 | 1.3926 | 1.3815 | 0.9485
    |'
- en: '| 800 | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 800 | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    |'
- en: 'Table 7: Ablation study on the size vision-language model. Results are reported
    on image faithfulness under different model scales of LLaVA-NeXT. We observe that
    model size is critical for reliable evaluation.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：关于视觉-语言模型大小的消融研究。结果在不同规模的 LLaVA-NeXT 模型下报告图像真实性评估。我们观察到，模型大小对于可靠评估至关重要。
- en: '| Method | Model Size | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5
    | SD v2.1 | LCM |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 模型大小 | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5 | SD v2.1
    | LCM |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | 7B | 1.9959 | 1.8615 | 1.8228 | 1.1708 | 1.2704 | 1.4031 | 1.3063
    | 1.0145 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 7B | 1.9959 | 1.8615 | 1.8228 | 1.1708 | 1.2704 | 1.4031 | 1.3063
    | 1.0145 |'
- en: '| 13B | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    |'
- en: '| 34B | 2.1131 | 1.8621 | 1.8083 | 1.3906 | 1.3076 | 1.3921 | 1.2037 | 1.0143
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 34B | 2.1131 | 1.8621 | 1.8083 | 1.3906 | 1.3076 | 1.3921 | 1.2037 | 1.0143
    |'
- en: 'Effect of dataset size for vision-language model training. In order to explore
    the effects of data size, we train model on image faithfulness evaluation task
    under three different data size: 200, 500 and 800. As illustrated in Table [6](#S5.T6
    "Table 6 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), performance generally enhances with more training
    data. Notably, training with just 500 data nearly maximizes accuracy, with further
    increases to 800 data yielding only slight improvements. This result suggests
    that our method requires only a small amount of annotated data to achieve good
    performance, highlighting the cost-effectiveness of our approach.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集大小对视觉-语言模型训练的影响。为了探究数据量的效果，我们在三种不同数据量（200、500 和 800）下进行图像真实性评估任务的模型训练。如表[6](#S5.T6
    "Table 6 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models")所示，性能通常随着训练数据的增加而提升。值得注意的是，仅使用500个数据进行训练几乎可以最大化准确性，而进一步增加到800个数据仅带来轻微的改善。这个结果表明，我们的方法只需要少量的标注数据即可实现良好的性能，突显了我们方法的成本效益。'
- en: 'Effect of model size. Transformers have demonstrated a strong capacity to scale
    effectively across a variety of tasks, as noted in key studies [[9](#bib.bib9),
    [36](#bib.bib36)]. In Table [7](#S5.T7 "Table 7 ‣ 5.2 Ablations and Analyses of
    EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models"), we illustrate
    the advantages of scaling up the MLLM for image faithfulness evaluation. Specifically,
    increasing the MLLM model size from 7B to 34B results in notable improvements
    in performance.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '模型大小的影响。Transformers 在各种任务中表现出有效扩展的强大能力，如关键研究中所述[[9](#bib.bib9), [36](#bib.bib36)]。在表[7](#S5.T7
    "Table 7 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models")中，我们展示了在图像真实性评估中扩大 MLLM 的优势。具体来说，将 MLLM 模型的大小从
    7B 增加到 34B 结果显著提高了性能。'
- en: 5.3 Comparison with Existing Evaluation Methods
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 与现有评估方法的比较
- en: 'SFT with human-aligned data outperforms vanilla MLLMs. To validate the effectiveness
    of the MLLM after SFT, we use vanilla LLaVA-NeXT 13B as the baseline model for
    comparison. As shown in Table [5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses
    of EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models") and Table [5](#S5.T5
    "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), the results of vanilla model suggest some correlations
    with human-annotated data. However, the alignment of the vanilla MLLM is relatively
    low due to the absence of images generated by model (such as distorted bodies
    and hands images) and issues related to evaluation in the MLLM’s pre-training
    dataset. After applying SFT on the LLaVA-Next 13B model using human annotated
    data, the model’s predictions on various fine-grained evaluation metrics are almost
    align to the human-annotated data and significantly surpass the evaluation results
    of all MLLM models that were not fine-tuned. This experimental results confirms
    that our SFT training enables the MLLM to be successfully applied to the task
    of evaluating text-to-image models.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '使用人类对齐数据的SFT优于原始MLLMs。为了验证SFT后的MLLM的有效性，我们使用原始LLaVA-NeXT 13B作为比较的基线模型。如表[5](#S5.T5
    "Table 5 ‣ 5.2 Ablations and Analyses of EvalAlign ‣ 5 Experimental Results ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models")和表[5](#S5.T5 "Table 5 ‣ 5.2 Ablations and Analyses
    of EvalAlign ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models")所示，原始模型的结果与人类标注数据有一些相关性。然而，由于缺乏模型生成的图像（例如扭曲的身体和手部图像）以及与MLLM预训练数据集中的评估相关的问题，原始MLLM的对齐相对较低。在使用人类标注数据对LLaVA-Next
    13B模型应用SFT后，该模型在各种细粒度评估指标上的预测几乎与人类标注数据对齐，并显著超越了所有未经过微调的MLLM模型的评估结果。这些实验结果确认了我们的SFT训练使MLLM成功应用于文本到图像模型的评估任务。'
- en: 'Comparison with state-of-the-art methods. To verify the human preference alignment
    of our model, especially when compared with other baseline methods, we calculate
    Kendall rank [[19](#bib.bib19)] and Pearson [[12](#bib.bib12)] correlation coefficient
    and summarize the results in Table [8](#S5.T8 "Table 8 ‣ 5.3 Comparison with Existing
    Evaluation Methods ‣ 5 Experimental Results ‣ EvalAlign: Supervised Fine-Tuning
    Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models").
    This comparison experiment encompasses images generated by 24 text-to-image models,
    covering an extensive range of synthesized image distribution.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '与最先进方法的比较。为了验证我们模型的人类偏好对齐，特别是与其他基线方法相比，我们计算了Kendall排名[[19](#bib.bib19)]和Pearson[[12](#bib.bib12)]相关系数，并在表[8](#S5.T8
    "Table 8 ‣ 5.3 Comparison with Existing Evaluation Methods ‣ 5 Experimental Results
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models")中总结了结果。此比较实验涵盖了24种文本生成图像模型生成的图像，涵盖了广泛的合成图像分布。'
- en: 'Table 8: Comparison with existing methods.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：与现有方法的比较。
- en: '| Method | Faithfulness | Alignment |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 真实性 | 对齐 |'
- en: '| --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kendall$\uparrow$ | Kendall$\uparrow$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Kendall$\uparrow$ | Kendall$\uparrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| CLIP-score | 0.1304 | 0.1765 | 0.6956 | 0.8800 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CLIP-score | 0.1304 | 0.1765 | 0.6956 | 0.8800 |'
- en: '| HPSv2 | 0.4203 | 0.5626 | 0.5217 | 0.7113 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| HPSv2 | 0.4203 | 0.5626 | 0.5217 | 0.7113 |'
- en: '| EvalAlign | 0.7464 | 0.8730 | 0.8043 | 0.9356 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 0.7464 | 0.8730 | 0.8043 | 0.9356 |'
- en: As can be concluded, compared with baseline methods, EvalAlign achieves significant
    higher alignment with fine-grained human preference on image faithfulness and
    image-text consistency, showcasing robust generalization ability. Although HPS
    v2 roughly aligns with human preference in some extent, the relative small model
    capacity and coarse ranking training limits its generalization to the fine-grained
    annotated data. Besides, since CLIP-s only cares the CLIP similarity of the generated
    image and its corresponding prompt, it behaves poorly in image faithfulness evaluation.
    The per-question alignment and the leaderboard of EvalAlign will be introduced
    in the supplementary materials.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 可以得出结论，与基线方法相比，EvalAlign在图像真实性和图像-文本一致性方面与细粒度人类偏好的对齐显著更高，展示了强大的泛化能力。虽然HPS v2在某种程度上大致与人类偏好对齐，但相对较小的模型容量和粗略的排名训练限制了其对细粒度标注数据的泛化。此外，由于CLIP-s仅关注生成图像及其对应提示的CLIP相似度，它在图像真实性评估方面表现较差。EvalAlign的每个问题对齐和排行榜将在补充材料中介绍。
- en: 6 Conclusion and Discussion
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与讨论
- en: In this work, we design an economic and efficient evaluation method that offers
    high accuracy, strong generalization capabilities, and provides fine-grained,
    interpretable metrics. We develop a comprehensive data annotation and cleaning
    process tailored for evaluation tasks, and established the EvalAlign benchmark
    for training and evaluating models on supervised fine-tuning tasks for MLLMs.
    Experimental results across 24 text-to-image models demonstrate that our evaluation
    metrics surpass the accuracy of all the state-of-art evaluation method. Additionally,
    we conduct a detailed empirical study on how MLLMs can be applied to model evaluation
    tasks. There are still many opportunities for further advancements and expansions
    based on our EvalAlign. We hope that our work can inspire and facilitate future
    research in this field.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们设计了一种经济高效的评估方法，提供了高准确性、强泛化能力，并提供了精细的、可解释的指标。我们开发了一个针对评估任务的全面数据注释和清理过程，并建立了
    EvalAlign 基准，用于训练和评估 ML 模型的监督微调任务。跨24个文本到图像模型的实验结果表明，我们的评估指标超越了所有最先进的评估方法的准确性。此外，我们还进行了关于
    ML 模型如何应用于模型评估任务的详细实证研究。基于我们的 EvalAlign，仍有许多进一步发展和扩展的机会。我们希望我们的工作能够激发和促进该领域未来的研究。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Misha Konstantinov Alex Shonenkov and et al. Deepfloyd if. [https://github.com/deep-floyd/IF](https://github.com/deep-floyd/IF),
    2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Misha Konstantinov Alex Shonenkov 等人. Deepfloyd if. [https://github.com/deep-floyd/IF](https://github.com/deep-floyd/IF),
    2023。'
- en: '[2] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran
    Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark
    for text-to-image models. In Proceedings of the IEEE/CVF international conference
    on computer vision, pages 20041–20053, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li
    Erran Li, 和 Mohamed Elhoseiny. Hrs-bench: 全面的、可靠的和可扩展的文本到图像模型基准. 见于 IEEE/CVF 国际计算机视觉会议论文集,
    页20041–20053, 2023。'
- en: '[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation
    with better captions. https://openai.com/dall-e-3, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, 等人. 通过更好的标题提升图像生成. [https://openai.com/dall-e-3](https://openai.com/dall-e-3),
    2023。'
- en: '[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej
    Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al.
    Stable video diffusion: Scaling latent video diffusion models to large datasets.
    arXiv preprint arXiv:2311.15127, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej
    Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, 等人.
    稳定的视频扩散: 将潜在的视频扩散模型扩展到大数据集. arXiv 预印本 arXiv:2311.15127, 2023。'
- en: '[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang,
    Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
    Minigpt-v2: Large language model as a unified interface for vision-language multi-task
    learning. arXiv preprint arXiv:2310.09478, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang,
    Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, 和 Mohamed Elhoseiny.
    Minigpt-v2: 大型语言模型作为视觉-语言多任务学习的统一接口. arXiv 预印本 arXiv:2310.09478, 2023。'
- en: '[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of
    diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint
    arXiv:2310.00426, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu, 等人. Pixart-alpha: 快速训练用于真实感文本到图像合成的扩散变换器.
    arXiv 预印本 arXiv:2310.00426, 2023。'
- en: '[7] Jaemin Cho, Yushi Hu, Jason Michael Baldridge, Roopal Garg, Peter Anderson,
    Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene
    graph: Improving reliability in fine-grained evaluation for text-image generation.
    In International conference on learning representations, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jaemin Cho, Yushi Hu, Jason Michael Baldridge, Roopal Garg, Peter Anderson,
    Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, 和 Su Wang. Davidsonian 场景图: 提升文本-图像生成的精细评估可靠性.
    见于 国际学习表征会议, 2023。'
- en: '[8] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning
    skills and social biases of text-to-image generation models. In Proceedings of
    the IEEE/CVF international conference on computer vision, pages 3043–3054, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Jaemin Cho, Abhay Zala, 和 Mohit Bansal. Dall-eval: 探测文本到图像生成模型的推理能力和社会偏见.
    见于 IEEE/CVF 国际计算机视觉会议论文集, 页3043–3054, 2023。'
- en: '[9] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
    Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
    Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In
    International conference on machine learning, pages 7480–7512\. PMLR, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
    Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
    Alabdulmohsin, 等。将视觉变换器扩展到220亿参数。在国际机器学习会议论文集中，第7480–7512页。PMLR，2023年。'
- en: '[10] Dreamlike-diffusion v1.0. [https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0),
    2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Dreamlike-diffusion v1.0。 [https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)，2022年。'
- en: '[11] Dreamlike-photoreal. [https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0),
    2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Dreamlike-photoreal。 [https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)，2023年。'
- en: '[12] David Freedman, Robert Pisani, and Roger Purves. Statistics (international
    student edition). Pisani, R. Purves, 4th edn. WW Norton & Company, New York, 2007.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] David Freedman, Robert Pisani, 和 Roger Purves。《统计学（国际学生版）》。Pisani, R.
    Purves，第4版。WW Norton & Company，纽约，2007年。'
- en: '[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano,
    Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image
    generation using textual inversion. In International conference on learning representations,
    2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano,
    Gal Chechik, 和 Daniel Cohen-or。一张图片胜过一个词：使用文本倒置个性化文本到图像生成。在学习表征国际会议，2022年。'
- en: '[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.
    CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings
    of the conference on empirical methods in natural language processing, pages 7514–7528,
    November 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, 和 Yejin Choi。CLIPScore：一个无参考评估指标用于图像描述。在自然语言处理经验方法会议论文集中，第7514–7528页，2021年11月。'
- en: '[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
    and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to
    a local nash equilibrium. Advances in neural information processing systems, 30,
    2017.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
    和 Sepp Hochreiter。通过两时间尺度更新规则训练的生成对抗网络收敛到局部纳什均衡。《神经信息处理系统进展》，30，2017年。'
- en: '[16] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
    Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness
    evaluation with question answering. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 20406–20417, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
    Krishna, 和 Noah A Smith。Tifa：通过问答的准确和可解释的文本到图像忠实度评估。在IEEE/CVF国际计算机视觉会议论文集中，第20406–20417页，2023年。'
- en: '[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench:
    A comprehensive benchmark for open-world compositional text-to-image generation.
    Advances in neural information processing systems, 36:78723–78747, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, 和 Xihui Liu。T2i-compbench：一个开放世界组合文本到图像生成的综合基准。《神经信息处理系统进展》，36:78723–78747，2023年。'
- en: '[18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou.
    Composer: Creative and controllable image synthesis with composable conditions.
    arXiv preprint arXiv:2302.09778, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, 和 Jingren Zhou。Composer：具有可组合条件的创意和可控图像合成。arXiv
    预印本 arXiv:2302.09778，2023年。'
- en: '[19] M. G. KENDALL. A new measure of rank correlation. Biometrika, 30(1-2):81–93,
    1938.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. G. KENDALL。新的等级相关度度量。《生物统计学》，30(1-2)：81–93，1938年。'
- en: '[20] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna,
    and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image
    generation. Advances in neural information processing systems, 36, 2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna,
    和 Omer Levy。Pick-a-pic：一个关于文本到图像生成的用户偏好开放数据集。《神经信息处理系统进展》，36，2024年。'
- en: '[21] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park,
    Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente,
    et al. Holistic evaluation of text-to-image models. Advances in neural information
    processing systems, 36, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park,
    Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente,
    等。对文本到图像模型的全面评估。《神经信息处理系统进展》，36，2024年。'
- en: '[22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali
    Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv
    preprint arXiv:2305.06355, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] KunChang Li、Yinan He、Yi Wang、Yizhuo Li、Wenhai Wang、Ping Luo、Yali Wang、Limin
    Wang 和 Yu Qiao。Videochat：以聊天为中心的视频理解。arXiv预印本 arXiv:2305.06355，2023。'
- en: '[23] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi
    Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual
    language models. arXiv preprint arXiv:2312.07533, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Ji Lin、Hongxu Yin、Wei Ping、Yao Lu、Pavlo Molchanov、Andrew Tao、Huizi Mao、Jan
    Kautz、Mohammad Shoeybi 和 Song Han。Vila：关于视觉语言模型的预训练。arXiv预印本 arXiv:2312.07533，2023。'
- en: '[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines
    with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Haotian Liu、Chunyuan Li、Yuheng Li 和 Yong Jae Lee。通过视觉指令调优改进基线。arXiv预印本
    arXiv:2310.03744，2023。'
- en: '[25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen,
    and Yong Jae Lee. Llava-next: improved reasoning, ocr, and world knowledge, January
    2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Haotian Liu、Chunyuan Li、Yuheng Li、Bo Li、Yuanhan Zhang、Sheng Shen 和 Yong
    Jae Lee。Llava-next：改进的推理、OCR 和世界知识，2024年1月。'
- en: '[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. Advances in neural information processing systems, 36, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Haotian Liu、Chunyuan Li、Qingyang Wu 和 Yong Jae Lee。视觉指令调优。神经信息处理系统进展，36，2024。'
- en: '[27] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang.
    Llmscore: Unveiling the power of large language models in text-to-image synthesis
    evaluation. Advances in neural information processing systems, 36, 2024.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Yujie Lu、Xianjun Yang、Xiujun Li、Xin Eric Wang 和 William Yang Wang。Llmscore：揭示大型语言模型在文本到图像合成评估中的力量。神经信息处理系统进展，36，2024。'
- en: '[28] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency
    models: Synthesizing high-resolution images with few-step inference. arXiv preprint
    arXiv:2310.04378, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Simian Luo、Yiqin Tan、Longbo Huang、Jian Li 和 Hang Zhao。潜在一致性模型：用少量步骤推理合成高分辨率图像。arXiv预印本
    arXiv:2310.04378，2023。'
- en: '[29] Bellagente Marco, Brack Manuel, Teufel Hannah, Friedrich Felix, and et al.
    Multifusion: fusing pre-trained models for multi-lingual, multi-modal image generation.
    arXiv preprint arXiv:2305.15296, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Bellagente Marco、Brack Manuel、Teufel Hannah、Friedrich Felix 等人。Multifusion：融合预训练模型以进行多语言、多模态图像生成。arXiv预印本
    arXiv:2305.15296，2023。'
- en: '[30] Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima,
    Esa Rahtu, Janne Heikkilä, and Shin’ichi Satoh. Toward verifiable and reproducible
    human evaluation for text-to-image generation. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 14277–14286, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Mayu Otani、Riku Togashi、Yu Sawai、Ryosuke Ishigami、Yuta Nakashima、Esa Rahtu、Janne
    Heikkilä 和 Shin’ichi Satoh。朝着可验证和可重复的人类评估进行文本到图像生成。在IEEE/CVF计算机视觉与模式识别会议论文集，页码14277–14286，2023。'
- en: '[31] Schramowski Patrick, Brack Manuel, and et al. Safe latent diffusion: Mitigating
    inappropriate degeneration in diffusion models. arXiv preprint arXiv:2211.05105,
    2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Schramowski Patrick、Brack Manuel 等人。安全潜在扩散：减轻扩散模型中的不适当退化。arXiv预印本 arXiv:2211.05105，2022。'
- en: '[32] Pablo Pernias, Dominic Rampas, and Marc Aubreville. Wuerstchen: Efficient
    pretraining of text-to-image models. arXiv preprint arXiv:2306.00637, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Pablo Pernias、Dominic Rampas 和 Marc Aubreville。Wuerstchen：高效预训练文本到图像模型。arXiv预印本
    arXiv:2306.00637，2023。'
- en: '[33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Dustin Podell、Zion English、Kyle Lacey、Andreas Blattmann、Tim Dockhorn、Jonas
    Müller、Joe Penna 和 Robin Rombach。Sdxl：提升潜在扩散模型以实现高分辨率图像合成。arXiv预印本 arXiv:2307.01952，2023。'
- en: '[34] Openjourney. [https://huggingface.co/prompthero/openjourney](https://huggingface.co/prompthero/openjourney),
    2022.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Openjourney。 [https://huggingface.co/prompthero/openjourney](https://huggingface.co/prompthero/openjourney)，2022。'
- en: '[35] Openjourneyv2. [https://huggingface.co/ilkerc/openjourney-v2](https://huggingface.co/ilkerc/openjourney-v2),
    2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Openjourneyv2. [https://huggingface.co/ilkerc/openjourney-v2](https://huggingface.co/ilkerc/openjourney-v2),
    2023。'
- en: '[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. OpenAI, 2018.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever 等人。通过生成预训练提升语言理解。OpenAI，2018。'
- en: '[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. arXiv preprint
    arXiv:2204.06125, 2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Aditya Ramesh、Prafulla Dhariwal、Alex Nichol、Casey Chu 和 Mark Chen。使用clip潜变量进行层次化文本条件图像生成。arXiv预印本
    arXiv:2204.06125，2022。'
- en: '[38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,
    Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation.
    In International conference on machine learning, pages 8821–8831\. PMLR, 2021.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,
    Alec Radford, Mark Chen, 和 Ilya Sutskever. 零样本文本到图像生成。国际机器学习会议论文集，页码 8821–8831。PMLR，2021。'
- en: '[39] redshift-diffusion. [https://huggingface.co/nitrosocke/redshift-diffusion](https://huggingface.co/nitrosocke/redshift-diffusion),
    2022.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] redshift-diffusion. [https://huggingface.co/nitrosocke/redshift-diffusion](https://huggingface.co/nitrosocke/redshift-diffusion)，2022。'
- en: '[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695,
    2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn
    Ommer. 使用潜在扩散模型的高分辨率图像合成。IEEE/CVF 计算机视觉与模式识别会议论文集，页码 10684–10695，2022。'
- en: '[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695,
    2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn
    Ommer. 使用潜在扩散模型的高分辨率图像合成。IEEE/CVF 计算机视觉与模式识别会议论文集，页码 10684–10695，2022。'
- en: '[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein,
    and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
    generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, pages 22500–22510, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein,
    和 Kfir Aberman. Dreambooth：针对主题驱动生成的文本到图像扩散模型的微调。IEEE/CVF 计算机视觉与模式识别会议论文集，页码 22500–22510，2023。'
- en: '[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    Advances in neural information processing systems, 35:36479–36494, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
    L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans
    等人。具有深度语言理解的 photorealistic 文本到图像扩散模型。神经信息处理系统进展，35:36479–36494，2022。'
- en: '[44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and Xi Chen. Improved techniques for training gans. Advances in neural information
    processing systems, 29, 2016.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    和 Xi Chen. 改进的 GAN 训练技术。神经信息处理系统进展，29，2016。'
- en: '[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
    Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,
    et al. Laion-5b: An open large-scale dataset for training next generation image-text
    models. Advances in neural information processing systems, 35:25278–25294, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
    Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman
    等人。Laion-5b：用于训练下一代图像-文本模型的开放大规模数据集。神经信息处理系统进展，35:25278–25294，2022。'
- en: '[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, and Ben Poole. Score-based generative modeling through stochastic differential
    equations. In International conference on learning representations, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, 和 Ben Poole. 基于评分的生成建模通过随机微分方程。国际学习表示会议，2021。'
- en: '[47] Sdxl-refiner. [https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0),
    2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Sdxl-refiner. [https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)，2023。'
- en: '[48] Sdxl-turbo. [https://stability.ai/research/adversarial-diffusion-distillation](https://stability.ai/research/adversarial-diffusion-distillation),
    2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Sdxl-turbo. [https://stability.ai/research/adversarial-diffusion-distillation](https://stability.ai/research/adversarial-diffusion-distillation)，2023。'
- en: '[49] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu,
    Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for
    generative image understanding. Advances in neural information processing systems,
    36, 2024.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu,
    Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang 等人。Journeydb：用于生成图像理解的基准测试。神经信息处理系统进展，36，2024。'
- en: '[50] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams,
    Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models
    for visio-linguistic compositionality. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 5238–5248, 2022.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Tristan Thrush、Ryan Jiang、Max Bartolo、Amanpreet Singh、Adina Williams、Douwe
    Kiela 和 Candace Ross. Winoground: 探究视觉和语言模型的视觉-语言组成性. 见于 IEEE/CVF 计算机视觉与模式识别会议论文集，第
    5238–5248 页，2022 年。'
- en: '[51] vintedois-diffusion v0.1. [https://huggingface.co/22h/vintedois-diffusion-v0-1](https://huggingface.co/22h/vintedois-diffusion-v0-1),
    2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] vintedois-diffusion v0.1. [https://huggingface.co/22h/vintedois-diffusion-v0-1](https://huggingface.co/22h/vintedois-diffusion-v0-1)，2023
    年。'
- en: '[52] Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover,
    and Duen Horng Chau. Diffusiondb: A large-scale prompt gallery dataset for text-to-image
    generative models. In Proceedings of the annual meeting of the association for
    computational linguistics, pages 893–911, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Zijie J Wang、Evan Montoya、David Munechika、Haoyang Yang、Benjamin Hoover
    和 Duen Horng Chau. Diffusiondb: 一个大规模的提示库数据集用于文本到图像生成模型. 见于计算语言学协会年会论文集，第 893–911
    页，2023 年。'
- en: '[53] Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajić, Su Wang,
    Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset,
    et al. Revisiting text-to-image evaluation with gecko: on metrics, prompts, and
    human ratings. arXiv preprint arXiv:2404.16820, 2024.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Olivia Wiles、Chuhan Zhang、Isabela Albuquerque、Ivana Kajić、Su Wang、Emanuele
    Bugliarello、Yasumasa Onoe、Chris Knutsen、Cyrus Rashtchian、Jordi Pont-Tuset 等. 使用
    Gecko 重新审视文本到图像评估: 关于度量、提示和人类评分. arXiv 预印本 arXiv:2404.16820，2024 年。'
- en: '[54] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao,
    and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating
    human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341,
    2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Xiaoshi Wu、Yiming Hao、Keqiang Sun、Yixiong Chen、Feng Zhu、Rui Zhao 和 Hongsheng
    Li. 人类偏好分数 v2: 一个评估文本到图像合成的人类偏好的可靠基准. arXiv 预印本 arXiv:2306.09341，2023 年。'
- en: '[55] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference
    score: Better aligning text-to-image models with human preference. In Proceedings
    of the IEEE/CVF international conference on computer vision, pages 2096–2105,
    2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Xiaoshi Wu、Keqiang Sun、Feng Zhu、Rui Zhao 和 Hongsheng Li. 人类偏好分数: 更好地对齐文本到图像模型与人类偏好.
    见于 IEEE/CVF 国际计算机视觉会议论文集，第 2096–2105 页，2023 年。'
- en: '[56] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie
    Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences
    for text-to-image generation. Advances in neural information processing systems,
    36, 2024.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Jiazheng Xu、Xiao Liu、Yuchen Wu、Yuxuan Tong、Qinkai Li、Ming Ding、Jie Tang
    和 Yuxiao Dong. Imagereward: 学习和评估人类对文本到图像生成的偏好. 神经信息处理系统进展，第 36 卷，2024 年。'
- en: '[57] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui
    Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling
    autoregressive models for content-rich text-to-image generation. Transactions
    on machine learning research, 2022.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Jiahui Yu、Yuanzhong Xu、Jing Yu Koh、Thang Luong、Gunjan Baid、Zirui Wang、Vijay
    Vasudevan、Alexander Ku、Yinfei Yang、Burcu Karagol Ayan 等. 扩展自回归模型用于内容丰富的文本到图像生成.
    机器学习研究杂志，第 2022 年。'
- en: '[58] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned
    audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858,
    2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Hang Zhang、Xin Li 和 Lidong Bing. Video-llama: 一种指令调整的视听语言模型用于视频理解. arXiv
    预印本 arXiv:2306.02858，2023 年。'
- en: '[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control
    to text-to-image diffusion models. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 3836–3847, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Lvmin Zhang、Anyi Rao 和 Maneesh Agrawala. 向文本到图像扩散模型添加条件控制. 见于 IEEE/CVF
    国际计算机视觉会议论文集，第 3836–3847 页，2023 年。'
- en: '[60] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu
    Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video
    synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Shiwei Zhang、Jiayu Wang、Yingya Zhang、Kang Zhao、Hangjie Yuan、Zhiwu Qin、Xiang
    Wang、Deli Zhao 和 Jingren Zhou. I2vgen-xl: 通过级联扩散模型进行高质量图像到视频合成. arXiv 预印本 arXiv:2311.04145，2023
    年。'
- en: '[61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:
    Enhancing vision-language understanding with advanced large language models. arXiv
    preprint arXiv:2304.10592, 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Deyao Zhu、Jun Chen、Xiaoqian Shen、Xiang Li 和 Mohamed Elhoseiny. Minigpt-4:
    通过先进的大型语言模型增强视觉-语言理解. arXiv 预印本 arXiv:2304.10592，2023 年。'
- en: Appendix A Limitations
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 限制
- en: Multimodal LLMs. Since EvalAlign evaluation models are fine-tuned MLLMs, they
    also suffer from multimodal hallucination, where models may generate content that
    seems plausible but actually incorrect or fabricated, and cannot be inferred from
    the input images and texts. Moreover, due to the possible harmful content in the
    pretraining data of the utilized base MLLMs, the model may inherit these biases
    and generate inappropriate response. Although we carefully curate the SFT training
    data of the EvalAlign evaluation models, the problems of hallucination and biased
    pre-training is alleviated but not fully addressed. Other than the these issues,
    EvalAlign evaluation models also suffer from opacity and interpretability, context
    limitation, as well as sensitivity to input formatting, like most multimodal LLMs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态 LLMs。由于 EvalAlign 评估模型是微调过的 MLLMs，它们也会遭遇多模态幻觉，即模型可能生成看似可信但实际上不正确或捏造的内容，这些内容无法从输入的图像和文本中推断出来。此外，由于所用基础
    MLLMs 的预训练数据中可能包含有害内容，模型可能继承这些偏见并生成不适当的回应。尽管我们仔细筛选了 EvalAlign 评估模型的 SFT 训练数据，但幻觉和偏见预训练的问题得到了缓解，但尚未完全解决。除此之外，EvalAlign
    评估模型还存在不透明性和可解释性、上下文限制以及对输入格式敏感等问题，像大多数多模态 LLMs 一样。
- en: Human Annotations. Human annotation is naturally subjective and influenced by
    individual perspectives, biases, and preferences. During the annotation, annotators
    can make mistakes, leading to incorrect or noisy labels. Regarding these challenges,
    we conduct 9 rounds of trial annotation and 2 rounds of random sampling quality
    inspection to ensure the inter-annotator consistency and overall annotation quality.
    We also design easy-to-understand annotation guidelines, instructions and platform
    to lower the annotation difficulty and benefit the annotation accuracy. Despite
    all these efforts, conducting human annotation with different annotators, user
    interface and annotation guidelines may lead to different result, making our annotation
    somewhat limited. Furthermore, human annotation can be time-consuming and resource-intensive,
    limiting the scale at which we can afford.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 人工注释。人工注释本质上是主观的，受到个人观点、偏见和偏好的影响。在注释过程中，注释者可能会犯错误，导致标签不正确或有噪声。针对这些挑战，我们进行了 9
    轮试注和 2 轮随机抽样质量检查，以确保注释者之间的一致性和整体注释质量。我们还设计了易于理解的注释指南、说明和平台，以降低注释难度并提高注释准确性。尽管做出了这些努力，与不同的注释者、用户界面和注释指南进行人工注释可能会导致不同的结果，使我们的注释有些局限。此外，人工注释可能耗时且资源密集，限制了我们能够承受的规模。
- en: Appendix B Ethical and Social Impacts
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 伦理和社会影响
- en: Similar with other MLLMs, EvalAlign may potentially generate responses contain
    offensive, inappropriate, or harmful content. Since the base MLLMs of EvalAlign
    are pretrained on large datasets scraped from the web that might contain private
    information and harmful content, they may inadvertently generate or expose sensitive
    information, raising ethical and privacy concerns. MLLMs are also susceptible
    to adversarial attacks, where inputs are intentionally crafted to deceive the
    model. This vulnerability can be exploited to manipulate model outputs, posing
    security and ethic risks. To alleviate these safety limitation, we create dedicated
    evaluation sets for bias detection and mitigation, and conducted adversarial testing
    through hours of redteaming. Besides, EvalAlign is designed for fine-grained,
    human-aligned automatic text-to-image evaluations. We believe that with appropriate
    use, it could provide users with interesting experiences for detailed synthesized
    image evaluation, and inspires more appealing research works about text-to-image
    generation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他 MLLMs 相似，EvalAlign 可能生成包含冒犯、不适当或有害内容的回应。由于 EvalAlign 的基础 MLLMs 是在从网络抓取的大型数据集上预训练的，这些数据集可能包含私人信息和有害内容，它们可能会无意中生成或暴露敏感信息，引发伦理和隐私问题。MLLMs
    还容易受到对抗性攻击，即输入被故意设计以欺骗模型。这种脆弱性可能被利用来操控模型输出，带来安全和伦理风险。为了缓解这些安全限制，我们创建了专门的评估集以检测和缓解偏见，并通过数小时的红队测试进行对抗性测试。此外，EvalAlign
    设计用于细粒度的人类对齐自动文本到图像评估。我们相信，适当使用的情况下，它可以为用户提供有趣的细化图像评估体验，并激发更多关于文本到图像生成的有吸引力的研究工作。
- en: '![Refer to caption](img/d0140bf909e36463bbe51f4462720598.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d0140bf909e36463bbe51f4462720598.png)'
- en: 'Figure 4: Demonstration of our user interface. Each time, our specially designed
    user interface will provide one sample to the annotators. We incorporated four
    distinct icons to signify various functionalities of the user interface.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：我们用户界面的演示。每次，我们特别设计的用户界面将向注释者提供一个样本。我们设计了四个不同的图标，以表示用户界面的各种功能。
- en: Appendix C Annotation Details
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 注释详情
- en: Before performing the final human annotation, we made a series of efforts to
    guarantee its quantity, quality and efficiency. To begin with, we select appropriate
    candidates to perform the annotation and hold a training meeting for them. Then,
    we design a user-friendly user interface and a comprehensive annotation procedure.
    We write a detailed annotation guidelines to explain every aspect and precaution
    of the annotation. As mentioned above, we conduct 9 rounds of trial annotation
    on another 50 synthesized images and 2 turns of random sampling quality inspection
    to further ensure inter-annotator consistency and annotation accuracy.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行最终的人类注释之前，我们进行了系列努力以确保注释的数量、质量和效率。首先，我们选择合适的候选人进行注释，并为他们举行培训会议。然后，我们设计了一个用户友好的用户界面和一个全面的注释程序。我们编写了详细的注释指南，以解释注释的每个方面和注意事项。如上所述，我们对另外50张合成图像进行了9轮试注释，并进行了2轮随机抽样质量检查，以进一步确保注释者间的一致性和注释准确性。
- en: Annotator selection. The accuracy and reliability of the annotated data depend
    heavily on the capabilities of the human annotators involved in the annotation
    process. As a consequence, at the beginning of the annotation, We first conduct
    annotator selection to build an appropriate and unbiased annotation team, and
    train this annotation team with our meticulously prepared annotation guidelines.
    For annotator selection, we let the candidates to accomplish a test concentrating
    on 10 factors, domain expertise, resistance to visually disturbing content, attention
    to detail, communication skills, reliability, cultural and linguistic competence,
    technical skills, ethical considerations, aesthetic cognition, and motivation.
    Notably, since the evaluated models may generate images with uncomfortable and
    inappropriate visual content, the candidates are notified with this inconvenience
    before the test. Only those agreed with this inconvenience are eligible to participate
    in the test, and they are welcome to withdraw at any time if they choose to do
    so. Based on the test results and candidate backgrounds, We try our best to ensure
    that the selected annotators are well-balanced in background and have a generally
    competitive abilities of the 10 mentioned factors. To summarize, our annotation
    team includes 10 annotators carefully selected from 29 candidates, 5 males and
    5 females, all have a bachelor’s degree. We interview the annotators and ensure
    they are adequate for the annotation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注释者选择。注释数据的准确性和可靠性在很大程度上依赖于参与注释过程的人类注释者的能力。因此，在注释开始时，我们首先进行注释者选择，以建立一个适当且公正的注释团队，并用我们精心准备的注释指南对该注释团队进行培训。对于注释者选择，我们让候选人完成一个集中于10个因素的测试，这些因素包括领域专长、对视觉干扰内容的抵抗能力、注意细节、沟通技巧、可靠性、文化和语言能力、技术技能、伦理考量、美学认知和动机。值得注意的是，由于评估模型可能生成具有不适和不当视觉内容的图像，候选人在测试前会被告知这一不便之处。只有那些同意这一不便的候选人才有资格参加测试，如果他们选择退出，他们随时都可以退出。根据测试结果和候选人背景，我们尽力确保所选注释者在背景上均衡，并在上述10个因素的能力上具有总体竞争力。总之，我们的注释团队包括从29名候选人中精心挑选的10名注释者，其中5名男性和5名女性，均具有本科学历。我们面试了这些注释者，并确保他们适合进行注释。
- en: 'Annotation training and guidelines. After the selection, we conduct a training
    meeting over our comprehensive user guidelines to make the annotation team aware
    of our purpose and standard. During the training meeting, we explain the purpose,
    precaution, standard, workload and wage of the annotation. Besides, we have formally
    informed the annotators that the human annotation is solely for research purposes,
    and the data they have annotated may potentially be released to the public in
    the future. We, and the annotators reached consensus on the standard, workload,
    wage and intended usage of the annotated data. The rules for recognising image
    faithfulness and text-image are universal, and thus each individual’s standards
    should not differ significantly. As a consequence, we annotate a few samples using
    our meticulously developed annotation platform for the annotators to ensure inter-annotator
    consistency. The overall snapshot of the developed annotation paltform is exhibited
    in [fig. 4](#A2.F4 "In Appendix B Ethical and Social Impacts ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"). With this training, we also equip the annotators with necessary knowledge
    for unbiased detailed human evaluation on image faithfulness and text-image alignment.
    Specifically, the employed annotation guidelines involve the instructions for
    using the annotation platform and detailed guidelines about the annotation procedure,
    and we demonstrate them in Table [9](#A3.T9 "Table 9 ‣ Appendix C Annotation Details
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 标注培训和指南。在选择之后，我们召开了关于我们综合用户指南的培训会议，以使标注团队了解我们的目的和标准。在培训会议中，我们解释了标注的目的、注意事项、标准、工作量和工资。此外，我们正式告知标注员，人工标注仅用于研究目的，他们标注的数据可能会在未来公开发布。我们与标注员就标注数据的标准、工作量、工资和预期用途达成了一致。识别图像真实性和文本-图像的一般规则是普遍适用的，因此每个人的标准不应有显著差异。因此，我们使用我们精心开发的标注平台对一些样本进行标注，以确保标注员之间的一致性。开发的标注平台的总体快照展示在[图
    4](#A2.F4 "在附录 B 伦理和社会影响 ‣ EvalAlign：使用与人类对齐的数据进行监督微调的多模态 LLM，用于评估文本到图像模型")中。通过此次培训，我们还为标注员提供了进行无偏详细人工评估图像真实性和文本-图像对齐所需的知识。具体来说，使用的标注指南包括使用标注平台的说明和关于标注程序的详细指南，我们在表格[9](#A3.T9
    "表格 9 ‣ 附录 C 标注细节 ‣ EvalAlign：使用与人类对齐的数据进行监督微调的多模态 LLM，用于评估文本到图像模型")中展示了这些内容。
- en: 'Table 9: User Guidelines of the Human Annotation. Considering that our annotators
    are native Chinese speakers while our readers may not be, each user is actually
    provided with a copy of Chinese version of the user guidelines. Meanwhile, we
    demonstrate its translated English version as follows.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：人工标注用户指南。考虑到我们的标注员是以中文为母语的人，而我们的读者可能不是，所以每位用户实际上都会提供一份中文版本的用户指南。同时，我们展示其翻译后的英文版本如下。
- en: '| User Guidelines |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 用户指南 |'
- en: '| --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Part I Introduction Welcome to the annotation platform. This platform is
    designed to simplify the annotation process and enhance annotation efficiency.
    Before the detailed introduction, we want to claim again that you may feel inconvenient
    as the evaluated models may generate images with uncomfortable and inappropriate
    visual content. Now, you are still welcomed if you want to withdraw your consent
    The annotation process is conducted on a sample-by-sample basis, with a question-by-question
    approach. Thus, you are supposed to answer all the questions raised for the present
    sample to accomplish its annotation. Once all the delegated samples are accomplished,
    your job is finished and we are thankful for your contribution to the project.
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 第一部分 介绍 欢迎使用标注平台。该平台旨在简化标注过程并提高标注效率。在详细介绍之前，我们再次声明，您可能会感到不便，因为评估的模型可能生成带有不适和不恰当视觉内容的图像。如果您希望撤回同意，您仍然受到欢迎。标注过程是逐样本进行的，每个问题逐一解决。因此，您应该回答当前样本提出的所有问题以完成其标注。一旦所有指定的样本完成，您的工作就结束了，我们对您对项目的贡献表示感谢。
    |'
- en: '| Part II Guidelines of the User Interface 1.User Login: To access the annotation
    platform, you are required to login as a user. Please navigate to the login page,
    enter the username and password provided by us, and click the “Login” button.
    2.Dashboard: Once you complete the login, you will be jumped into the dashborad
    page. The dashboard will list the overview of the samples assigned to you to annotate.
    Besides, we list the status of each sample for you to freely check your annotation
    progress (e.g., pending, completed). 3.Annotation Interface: Click on the “Start”
    button or an assigned image through the dashboard interface, you will jump into
    the annotation interface. annotation interface is made up of three components:
    1) Image Display: View the image to be annotated and its conditioned prompts;
    2) Question Panel: List of single-choice questions related to the image; 3) Navigation
    Buttons: "Next" and "Previous" buttons to navigate through questions and images.
    4.Answering Questions: Each time, the annotation interface will provide you a
    sample for annotation, please view the image and read the associated question,
    select the appropriate answer from the available options, and repeat the processs
    for all questions related to the question. 5.Saving and Submitting Annotations:
    To save progress and submit completed annotations, you can click the “Save” button
    to save your progress. If you finish the assigned sample and ensure the accuracy
    and confidence of its, you can click the “Submit” button to submit this annotation.
    6.Review and Edit Annotations: If you want to review and edit your submission,
    you can navigate the completed tasks section, and select the image to review.
    You will jump into its annotation interface with the previously submitted annotations
    and are allowed to do any modification. 7.Report and contact: If you find any
    problem about the assigned sample, such as witnessing NSFW or biased content,
    assigned visually abnormal sample, feel free to click the “Report” button and
    fill a form to report this sample. If you have any question about the standard
    of the annotation or have suggestions for improvement, please do not hesitate
    to contact us through phone, we will be glad to help you. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 第二部分 用户界面指南 1. 用户登录：要访问注释平台，您需要以用户身份登录。请导航至登录页面，输入我们提供的用户名和密码，然后点击“登录”按钮。
    2. 控制面板：完成登录后，您将跳转到控制面板页面。控制面板将列出分配给您的样本概览。除此之外，我们还列出了每个样本的状态，以便您自由检查您的注释进度（例如，待处理、已完成）。
    3. 注释界面：通过控制面板界面点击“开始”按钮或分配的图像，您将跳转到注释界面。注释界面由三个组件组成：1) 图像显示：查看待注释的图像及其条件提示；2)
    问题面板：列出与图像相关的单选问题；3) 导航按钮：“下一步”和“上一步”按钮，用于在问题和图像之间导航。 4. 回答问题：每次，注释界面将提供一个样本进行注释，请查看图像并阅读相关问题，从可用选项中选择适当的答案，并对所有相关问题重复此过程。
    5. 保存和提交注释：要保存进度和提交已完成的注释，您可以点击“保存”按钮以保存您的进度。如果您完成了分配的样本并确保其准确性和自信度，您可以点击“提交”按钮以提交此注释。
    6. 审查和编辑注释：如果您想审查和编辑您的提交，您可以导航到已完成任务部分，选择要审查的图像。您将跳转到其注释界面，并可以对之前提交的注释进行任何修改。
    7. 举报和联系：如果您发现分配的样本有任何问题，例如发现NSFW或偏见内容，或分配的样本视觉异常，请随时点击“举报”按钮并填写表单以报告此样本。如果您对注释标准有任何问题或有改进建议，请通过电话与我们联系，我们将很高兴为您提供帮助。
    |'
- en: '| Part III General Guidelines of the Human Annotation 1.In general, you are
    supposed to answer all the questions raised for the present sample to accomplish
    its annotation. This annotation only involves single-choice question. 2.Before
    answering the question, please ensure that the question is applicable to this
    prompt. If it is not applicable, please select option 0 directly—this is the predefined
    option for this particular scenario. 3.If you are answering a question about the
    image faithfulness, you may find the question is applicable to multiple objects
    within the image. you need to answer the question regarding to every applicable
    object and its role in the image. A straightforward way for this is to solely
    score every applicable object and choose the option closest to the calculated
    weighted average score. 4.If you are answering the object faithfulness question
    on the image faithfulness annotation, you need to drop and report for the encountered
    image with no clear main object. 5.If you are answering the commonsense question
    on the image faithfulness annotation, you need to drop and report for the encountered
    surreal and sci-fi image. 6.You are required to first annotate 30 samples to form
    a stable and reasonable assessment standard. Then, accomplish the annotation in
    progress. 7.This annotation is for evaluating image faithfulness and text-image
    alignment, as a consequence, the standard of the annotation is universal. 8.If
    you feel confused at anything about the human annotation, feel free to contact
    us through phone, we will be glad to help you. 9\. Once you have submitted your
    annotation results, we are very thankful to inform you that you have finished
    your job. Thank you once again for your contribution to our project. 10.If you
    have submitted your annotation but want to withdraw your submission and review
    the annotation results, you can contact us through phone, we will send it back
    to you. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 第三部分 人工标注的一般指南 1. 一般来说，你需要回答当前样本提出的所有问题以完成标注。此标注仅涉及单项选择题。 2. 在回答问题之前，请确保问题适用于此提示。如果不适用，请直接选择选项
    0——这是特定场景的预定义选项。 3. 如果你在回答关于图像真实性的问题，你可能会发现该问题适用于图像中的多个对象。你需要对每个适用的对象及其在图像中的角色进行回答。一个直接的方法是仅对每个适用的对象进行评分，并选择最接近计算加权平均分的选项。
    4. 如果你在图像真实性标注中回答对象真实性的问题，你需要弃标并报告没有明确主要对象的图像。 5. 如果你在图像真实性标注中回答常识性问题，你需要弃标并报告遇到的超现实和科幻图像。
    6. 你需要首先标注 30 个样本，以形成稳定且合理的评估标准。然后，完成正在进行的标注工作。 7. 此标注用于评估图像真实性和文本-图像对齐，因此标注的标准是普遍的。
    8. 如果你对人工标注有任何疑问，请随时通过电话联系我们，我们将很高兴为你提供帮助。 9. 一旦你提交了标注结果，我们非常感谢地通知你，你已经完成了你的工作。再次感谢你对我们项目的贡献。
    10. 如果你已经提交了标注但想撤回提交并查看标注结果，你可以通过电话联系我们，我们将把它发还给你。 |'
- en: Trial Annotation Even with the above preparation, there is no quantitative evidence
    to verify the quality, the efficiency, and the inter-annotator consistency of
    the human annotation. Additionally, the standard for assessing image faithfulness
    and text-image are universal, which further emphasize the significant role of
    high inter-annotator consistency. Considering that, we conduct a multi-turn trial
    annotation on another 50 synthesized images. After each trial, we calculate the
    Cohen’s kappa coefficient and conduct a meeting for our annotators to explain
    annotation standards, rules and guidelines, thereby ensuring high inter-annotator
    reliability. In total, we conduct nine turns of trial annotation, and in the last
    turn of the trial, the Cohen’s kappa coefficient of our annotators reaches $0.681$,
    indicating high inter-annotator reliability.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 试验标注 尽管有上述准备，但仍没有定量证据来验证人工标注的质量、效率和标注者间的一致性。此外，评估图像真实性和文本-图像对齐的标准是普遍的，这进一步强调了高标注者间一致性的重要性。考虑到这一点，我们对另外
    50 张合成图像进行了多轮试验标注。在每轮试验后，我们计算 Cohen 的 kappa 系数，并召开会议向标注者解释标注标准、规则和指南，从而确保高标注者间的可靠性。总共进行了九轮试验标注，在最后一轮试验中，我们的标注者的
    Cohen 的 kappa 系数达到 $0.681$，表明标注者间的一致性很高。
- en: Random Sampling Quality Inspection Upon reaching the milestone percentages of
    25%, 50%, 75%, and 100% in the annotation progress, we conducted a series of random
    sampling quality inspections on the present annotation results at each milestone,
    totally four turns of random sampling quality inspection. The random sampling
    quality inspection by four experts in text-to-image generation selected from our
    group on 1,000 randomly sampled annotated images. For the first two turn of quality
    inspection, there are totally 423 and 112 annotated samples that failed the inspection.
    The failed samples are re-annotated and re-inspected. As for the last two turns
    of quality inspection, they both revealed zero failed samples due to the thoughtful
    and rigorous annotation preparation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 随机抽样质量检查 在注释进度达到25%、50%、75%和100%的里程碑百分比时，我们在每个里程碑对当前的注释结果进行了系列随机抽样质量检查，共进行了四轮随机抽样质量检查。四位来自我们团队的文本到图像生成专家对1,000张随机抽样的注释图像进行了质量检查。前两轮质量检查中，共有423和112个注释样本未通过检查。未通过的样本进行了重新注释和重新检查。至于最后两轮质量检查，由于精心和严格的注释准备，未发现任何失败的样本。
- en: Appendix D Additional Details of the Evaluated Models
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 评估模型的附加细节
- en: In this section, we introduce the details of the evaluated text-to-image generative
    models in this work.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了本工作中评估的文本到图像生成模型的详细信息。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Stable Diffusion {v1.4, v1.5, v2 base, v2.0, v2.1}. Stable Diffusion (SD) is
    a series of 1B text-to-image generative models based on latent diffusion model [[41](#bib.bib41)]
    and is trained on LAION-5B [[45](#bib.bib45)]. Specifically, the SD series includes
    SD v1.1, SD v1.2, SD v1.4, SD v1.5, SD v2 base, SD v2.0, and SD v2.1 respectively.
    Among them, we choose the most commonly-employed SD v1.4, SD v1.5, SD v2.0 and
    SD v2.1 for EvalAlign evaluation.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定扩散 {v1.4, v1.5, v2 base, v2.0, v2.1}。稳定扩散（SD）是一系列基于潜在扩散模型的1B文本到图像生成模型[[41](#bib.bib41)]，并在LAION-5B[[45](#bib.bib45)]上进行训练。具体来说，SD系列包括SD
    v1.1、SD v1.2、SD v1.4、SD v1.5、SD v2 base、SD v2.0和SD v2.1。我们选择了最常用的SD v1.4、SD v1.5、SD
    v2.0和SD v2.1进行EvalAlign评估。
- en: SD v1.1 was trained at a resolution of 256x256 on laion2B-en for 237k steps,
    followed by training at a resolution of 512x512 on laion-high-resolution ((170M
    examples from LAION-5B with resolution >= 1024x1024) for the subsequent 194k steps.
    While, SD v1.2 was initialized from v1.1 and further finetuned for 515k steps
    at resolution 512x512 on laion-aesthetics v2 5+ (a subset of laion2B-en, filtered
    to images with an original size >= 512x512, estimated aesthetics score > 5.0,
    and an estimated watermark probability < 0.5). SD v1.4 is initialized from v1.2
    and subsequently finetuned for 225k steps at resolution 512x512 on laion-aesthetics
    v2 5+. This version incorporates a 10% dropping of the text-conditioning to improve
    classifier-free guidance sampling. Similar to SD v1.4, SD v1.5 is resumed from
    SD v1.2 and trained 595k steps at resolution 512x512 on laion-aesthetics v2 5+,
    with 10% dropping of the text-conditioning.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SD v1.1在laion2B-en上以256x256的分辨率训练了237k步，随后在laion-high-resolution上以512x512的分辨率进行了194k步训练（来自LAION-5B的170M样本，分辨率>=1024x1024）。而SD
    v1.2从v1.1初始化，并在laion-aesthetics v2 5+（laion2B-en的一个子集，过滤为原始大小>=512x512、估计美学分数>5.0和估计水印概率<0.5的图像）上以512x512的分辨率进一步微调了515k步。SD
    v1.4从v1.2初始化，随后在laion-aesthetics v2 5+上以512x512的分辨率进行了225k步微调。该版本通过降低10%的文本条件以改进无分类器引导采样。类似于SD
    v1.4，SD v1.5从SD v1.2恢复，并在laion-aesthetics v2 5+上以512x512的分辨率训练了595k步，也降低了10%的文本条件。
- en: SD v2 base is trained from scratch for 550k steps at resolution 256x256 on a
    subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW
    classifier with punsafe = 0.1 and an aesthetic score >= 4.5\. Then it is further
    trained for 850k steps at resolution 512x512 on the same dataset on images with
    resolution >= 512x512\. SD v2.0 is resumed from stable-diffusion v2 base and trained
    for 150k steps using a v-objective on the same dataset. After that, it is further
    finetuned for another 140k steps on 768x768 images. SD v2.1 is finetuned from
    SD v2.0 with an additional 55k steps on the same dataset (with punsafe=0.1), and
    then finetuned for another 155k extra steps with punsafe=0.98.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SD v2 基础模型从头开始训练 550k 步，分辨率为 256x256，数据集为经过筛选的 LAION-5B，使用 LAION-NSFW 分类器（punsafe
    = 0.1）和美学评分 >= 4.5。然后在同一数据集上的图像（分辨率 >= 512x512）上，进一步训练 850k 步，分辨率为 512x512。SD
    v2.0 从 stable-diffusion v2 基础模型恢复，并在同一数据集上使用 v-objective 训练 150k 步。之后，再进一步微调 140k
    步，分辨率为 768x768。SD v2.1 从 SD v2.0 微调，再额外训练 55k 步，数据集相同（punsafe=0.1），然后再额外训练 155k
    步，punsafe=0.98。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stable Diffusion XL {v1.0, Refiner v1.0}. Stable Diffusion XL (SDXL) is a powerful
    text-to-image generation model that iterates on the previous Stable Diffusion
    models in three key ways: (1) its UNet is 3x larger and SDXL combines a second
    text encoder (OpenCLIP ViT-bigG/14) with the original text encoder to significantly
    increase the number of parameters; (2) it introduces size and crop-conditioning
    to preserve training data from being discarded and gain more control over how
    a generated image should be cropped; (3) it introduces a two-stage model process;
    the base model (can also be run as a standalone model) generates an image as an
    input to the refiner model which adds additional high-quality details.'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Stable Diffusion XL {v1.0, Refiner v1.0}。Stable Diffusion XL (SDXL) 是一个强大的文本到图像生成模型，相比之前的
    Stable Diffusion 模型有三个关键改进：(1) 它的 UNet 大小增加了 3 倍，SDXL 结合了第二个文本编码器 (OpenCLIP ViT-bigG/14)
    和原始文本编码器，显著增加了参数数量；(2) 它引入了尺寸和裁剪条件，以防止训练数据被丢弃，并增加了对生成图像裁剪方式的控制；(3) 它引入了两阶段模型过程；基础模型（也可以作为独立模型运行）生成图像作为
    refiner 模型的输入，后者添加了额外的高质量细节。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pixart-Alpha. Pixart-Alpha is a model that can be used to generate and modify
    images based on text prompts. It is a Transformer Latent Diffusion Model that
    uses one fixed, pretrained text encoders (T5)) and one latent feature encoder
    (VAE).
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pixart-Alpha。Pixart-Alpha 是一个可以根据文本提示生成和修改图像的模型。它是一个 Transformer 潜在扩散模型，使用一个固定的、预训练的文本编码器
    (T5) 和一个潜在特征编码器 (VAE)。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Latent Consistency Model Stable Diffusion XL Latent Consistency Model Stable
    Diffusion XL (LCM SDXL) [[28](#bib.bib28)] enables SDXL for swift inference with
    minimal steps. Viewing the guided reverse diffusion process as solving an augmented
    probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution
    of such ODE in latent space, mitigating the need for numerous iterations and allowing
    rapid, high-fidelity sampling.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Latent Consistency Model Stable Diffusion XL Latent Consistency Model Stable
    Diffusion XL (LCM SDXL) [[28](#bib.bib28)] 使 SDXL 能够快速推断，步骤最少。通过将引导的反向扩散过程视为解决增强的概率流
    ODE (PF-ODE)，LCM 旨在直接预测这种 ODE 在潜在空间中的解决方案，从而减少大量迭代并允许快速、高保真度的采样。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dreamlike Diffusion 1.0. Dreamlike Diffusion 1.0 [[10](#bib.bib10)] is a SD
    v1.5 model finetuned on high-quality art images by dreamlike.art.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dreamlike Diffusion 1.0。Dreamlike Diffusion 1.0 [[10](#bib.bib10)] 是一个基于 dreamlike.art
    高质量艺术图像微调的 SD v1.5 模型。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dreamlike Photoreal 2.0. Dreamlike Photoreal 2.0 [[11](#bib.bib11)] is a photorealistic
    text-to-image latent diffusion model resumed from SD v1.5 by dreamlike art. This
    model was finetuned on 768x768 images, it works pretty good with resolution 768x768,
    640x896, 896x640 and higher resolution such as 768x1024.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dreamlike Photoreal 2.0。Dreamlike Photoreal 2.0 [[11](#bib.bib11)] 是一个从 SD v1.5
    恢复的逼真文本到图像潜在扩散模型，由 dreamlike art 制作。该模型经过 768x768 图像的微调，在 768x768、640x896、896x640
    及更高分辨率如 768x1024 的图像中效果很好。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Openjourney v1, v2. Openjourney [[34](#bib.bib34)] is an open-source text-to-image
    generation model resumed from SD v1.5 and finetuned on Midjourney images by PromptHero.
    Openjourney v2 [[35](#bib.bib35)] was further finetuned using another 124000 images
    for 12400 steps, about 4 epochs and 32 training hours.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Openjourney v1, v2。Openjourney [[34](#bib.bib34)] 是一个从 SD v1.5 恢复的开源文本到图像生成模型，由
    PromptHero 基于 Midjourney 图像进行了微调。Openjourney v2 [[35](#bib.bib35)] 进一步通过 124000
    张图像的 12400 步（约 4 个周期和 32 个训练小时）进行了微调。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Redshift Diffusion. Redshift Diffusion [[39](#bib.bib39)] is a Stable Diffusion
    model finetuned on high-resolution 3D artworks.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Redshift Diffusion。Redshift Diffusion [[39](#bib.bib39)] 是一个在高分辨率3D艺术作品上经过微调的Stable
    Diffusion模型。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Vintedois Diffusion. Vintedois Diffusion [[51](#bib.bib51)] is a Stable Diffusion
    v1.5 model finetuned on a large number of high-quality images with simple prompts
    to generate beautiful images without a lot of prompt engineering.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Vintedois Diffusion。Vintedois Diffusion [[51](#bib.bib51)] 是一个在大量高质量图像上经过微调的Stable
    Diffusion v1.5模型，能够用简单的提示生成美丽的图像，而无需大量的提示工程。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Safe Stable Diffusion {Weak, Medium, Strong, Max}. Safe Stable Diffusion [[31](#bib.bib31)]
    is an enhanced version of the SD v1.5 model by mitigating inappropriate degeneration
    caused by pretraining on unfiltered web-crawled datasets. For instance SD may
    unexpectedly generate nudity, violence, images depicting self-harm, and otherwise
    offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that
    drastically reduces this type of content. Specifically, it has an additional safety
    guidance mechanism that aims to suppress and remove inappropriate content (hate,
    harassment, violence, self-harm, sexual content, shocking images, and illegal
    activity) during image generation. The strength levels for inappropriate content
    removal are categorized as: {Weak, Medium, Strong, Max}.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Safe Stable Diffusion {Weak, Medium, Strong, Max}。Safe Stable Diffusion [[31](#bib.bib31)]
    是对SD v1.5模型的增强版本，通过减少由于在未经过滤的网络爬取数据集上预训练而引起的不适当内容的生成。例如，SD可能会意外生成裸露、暴力、自我伤害以及其他令人反感的内容。Safe
    Stable Diffusion 是Stable Diffusion的扩展，极大地减少了这类内容。具体而言，它具有额外的安全指导机制，旨在在图像生成过程中压制和移除不适当的内容（仇恨、骚扰、暴力、自我伤害、色情内容、令人震惊的图像以及非法活动）。不适当内容移除的强度级别分类为：{Weak,
    Medium, Strong, Max}。
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MultiFusion. MultiFusion [[29](#bib.bib29)] is a multimodal, multilingual diffusion
    model that extends the capabilities of SD v1.4 by integrating various modules
    to transfer capabilities to the downstream model. This combination results in
    novel decoder embeddings, which enable prompting of the image generation model
    with interleaved multimodal, multilingual inputs, despite being trained solely
    on monomodal data in a single language.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MultiFusion。MultiFusion [[29](#bib.bib29)] 是一个多模态、多语言的扩散模型，通过集成各种模块扩展了SD v1.4的能力，将能力传递到下游模型。这种组合产生了新型解码器嵌入，使得图像生成模型能够使用交错的多模态、多语言输入进行提示，尽管该模型仅在单语言的单模态数据上进行训练。
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DeepFloyd-IF { M, L, XL } v1.0. DeepFloyd-IF [[1](#bib.bib1)] is a novel state-of-the-art
    open-source text-to-image model with a high degree of photorealism and language
    understanding. It is a modular composed of a frozen text encoder and three cascaded
    pixel diffusion modules: a base model that generates 64x64 image based on text
    prompt and two super-resolution models, each designed to generate images of increasing
    resolution: 256x256 and 1024x1024, respectively. All stages of the model utilize
    a frozen text encoder based on the T5 transformer to extract text embeddings,
    which are then fed into a UNet architecture enhanced with cross-attention and
    attention pooling. Besides, it underscores the potential of larger UNet architectures
    in the first stage of cascaded diffusion models and depicts a promising future
    for text-to-image synthesis. The model is available in three different sizes:
    M, L, and XL. M has 0.4B parameters, L has 0.9B parameters, and XL has 4.3B parameters.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepFloyd-IF { M, L, XL } v1.0。DeepFloyd-IF [[1](#bib.bib1)] 是一种新型的最先进开源文本到图像模型，具有高度的真实感和语言理解能力。它是一个模块化系统，由一个冻结的文本编码器和三个级联像素扩散模块组成：一个基础模型根据文本提示生成64x64的图像，以及两个超分辨率模型，分别设计用于生成256x256和1024x1024分辨率的图像。模型的所有阶段都使用基于T5变换器的冻结文本编码器来提取文本嵌入，然后输入到一个经过交叉注意力和注意力池化增强的UNet架构中。此外，它强调了在级联扩散模型的第一阶段中较大UNet架构的潜力，并描绘了文本到图像合成的美好前景。该模型提供三种不同的尺寸：M、L和XL。M有0.4B参数，L有0.9B参数，XL有4.3B参数。
- en: Throughout all of the conducted experiments, we use the default inference settings
    for each evaluated models.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有进行的实验中，我们使用了每个评估模型的默认推理设置。
- en: Appendix E Instruction Templates
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 指令模板
- en: 'Here, we present every instruction used for EvalAlign evaluation on image faithfuleness
    and text-image alignment. The templates contain some placeholders set for filling
    in the corresponding attributes of the input images during the evaluation. For
    example, a specific “<ObjectHere>” and “<NumberHere>” can be “people, laptop,
    scissors.” and “plate: 1, turkey sandwich: 3, lettuce: 1.”, respectively.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们展示了用于 EvalAlign 评估图像真实性和文本-图像对齐的每一条指令。这些模板包含一些占位符，用于在评估过程中填充输入图像的相应属性。例如，特定的“<ObjectHere>”和“<NumberHere>”可以是“people,
    laptop, scissors.”和“plate: 1, turkey sandwich: 3, lettuce: 1.”，分别表示。'
- en: 'For EvalAlign evaluation on image faithfulness, we devise 5 questions concentrate
    on the faithfulness of the generated body structure, generated face, generated
    hand, generated objects, as well as generation adherence to commonsense and logic.
    The instruction templates for these fine-grained criteria are as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 EvalAlign 在图像真实性方面的评估，我们设计了 5 个问题，集中在生成的身体结构、生成的面孔、生成的手部、生成的对象以及生成是否遵循常识和逻辑上。这些细化标准的指令模板如下：
- en: '<svg id="A5.p3.pic1" class="ltx_picture" height="172.69" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,172.69) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="145.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q1]:Are
    there any issues with the [human/animals] body structure in the image, such as
    multiple arms, missing limbs or legs when not obscured, multiple heads, limb amputations,
    and etc? [OPTIONS]: 0.There are no human or animal body in the picture; 1.The
    body structure of the people or animals in the picture has a very grievous problem
    that is unbearable; 2.The body structure of the people or animals in the picture
    has some serious problems and is not acceptable; 3.The body structure of the people
    or animals in the picture has a slight problem that does not affect the senses;
    4.The body structure of the people or animals in the picture is basically fine,
    with only a few flaws; 5.The body structure of the people or animals in the picture
    is completely fine and close to reality.</foreignobject></g></g></svg><svg id="A5.p4.pic1"
    class="ltx_picture" height="156.09" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,156.09) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65
    13.78)"><foreignobject width="556.69" height="128.53" transform="matrix(1 0 0
    -1 0 16.6)" overflow="visible" color="#000000">[Q2]:Are there any issues with
    the [human/animals] hands in the image, such as having more or less than five
    fingers when not obscured, broken fingers, disproportionate finger sizes, abnormal
    nail size proportions, and etc? [OPTIONS]: 0.No human or animal hands are shown
    in the picture; 1.The hand in the picture has a very grievous problem that is
    unbearable; 2.The hand in the picture has some serious problems and is not acceptable;
    3.The hand in the picture has a slight problem that does not affect the senses;
    4.The hand in the picture is basically fine, with only a few flaws; 5.The hands
    in the picture are completely fine and close to reality.</foreignobject></g></g></svg><svg
    id="A5.p5.pic1" class="ltx_picture" height="156.09" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,156.09) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="128.53" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q3]:Are there any issues with
    [human/animals] face in the image, such as facial distortion, asymmetrical faces,
    abnormal facial features, unusual expressions in the eyes, and etc? [OPTIONS]:
    0.There is no face of any person or animal in the picture; 1.The face of the person
    or animal in the picture has a very grievous problem that is unbearable; 2.The
    face of the person or animal in the picture has some serious problems and is not
    acceptable; 3.The face of the person or animal in the picture has a slight problem
    that does not affect the senses; 4.The face of the person or animal in the picture
    is basically fine, with only a few flaws; 5.The face of the person or animal in
    the picture is completely fine and close to reality.</foreignobject></g></g></svg><svg
    id="A5.p6.pic1" class="ltx_picture" height="139.48" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,139.48) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="111.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q4]:Are there any issues or
    tentative errors with objects in the image that do not correspond with the real
    world, such as distortion of items, and etc? [OPTIONS]: 0.There are objects in
    the image that completely do not match the real world, which is very serious and
    intolerable; 1.There are objects in the image that do not match the real world,
    which is quite serious and unacceptable; 2.There are slightly unrealistic objects
    in the image that do not affect the senses; 3.There are basically no objects in
    the image that do not match the real world, only some flaws; 4.All objects in
    the image match the real world, no problem.</foreignobject></g></g></svg><svg
    id="A5.p7.pic1" class="ltx_picture" height="189.3" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,189.3) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="161.74" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q5]:Does the generated image
    contain elements that violate common sense or logical rules, such as animal/human
    with inconsistent anatomy, object-context mismatch, impossible physics, scale
    and proportion issues, temporal and spatial inconsistencies, hybrid objects, and
    etc? [OPTIONS]: 0.The image contains elements that violate common sense or logical
    rules, which is very grievous and intolerable; 1.The presence of elements in the
    image that seriously violate common sense or logical rules is unacceptable; 2.The
    image contains elements that violate common sense or logical rules, which is slightly
    problematic and does not affect the senses; 3.There are basically no elements
    in the image that violate common sense or logical rules, only some flaws; 4.There
    are no elements in the image that violate common sense or logical rules, and they
    are close to reality.</foreignobject></g></g></svg>'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The templates of EvalAlign evaluation on text-image alignment are as follows.
    We select 6 common aspects of text-image alignment, object, number, color, style,
    spatial relationship and action. For images that do not involve the specified
    attribute, the corresponding question template is not filled in and subsequently
    input into EvalAlign.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: EvalAlign对文本-图像对齐的评估模板如下。我们选择了6个文本-图像对齐的常见方面，包括对象、数量、颜色、风格、空间关系和动作。对于不涉及指定属性的图像，相应的问题模板不填写，然后输入到EvalAlign中。
- en: '<svg id="A5.p9.pic1" class="ltx_picture" height="73.07" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,73.07) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q1]:Does the given image contain
    all the objects (<ObjectHere>) presented in the corresponding prompts? [OPTIONS]:
    1.None objects are included; 2.Some objects are missing; 3.All objects are included.</foreignobject></g></g></svg><svg
    id="A5.p10.pic1" class="ltx_picture" height="73.07" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q2]:Does the given image correctly
    reflect the numbers (<NumberHere>) of each object presented in the corresponding
    prompts? [OPTIONS]: 1.All counting numbers are wrong; 2.Some of them are wrong;
    3.All counting numbers are right.</foreignobject></g></g></svg><svg id="A5.p11.pic1"
    class="ltx_picture" height="73.07" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65
    13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[Q3]:Does the given image correctly
    reflect the colors of each object (<ColorHere>) presented in the corresponding
    prompts? [OPTIONS]: 1.All colors are wrong; 2.Some of them are wrong; 3.All corresponding
    colors numbers are right.</foreignobject></g></g></svg><svg id="A5.p12.pic1" class="ltx_picture"
    height="73.07" overflow="visible" version="1.1" width="600"><g transform="translate(0,73.07)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="45.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">[Q4]:Does the given image correctly reflect the style (<StyleHere>)
    described in the corresponding prompts? [OPTIONS]: 1.All styles are wrong; 2.Some
    of them are wrong; 3.All styles are right.</foreignobject></g></g></svg><svg id="A5.p13.pic1"
    class="ltx_picture" height="73.07" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65
    13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1 0 0 -1
    0 16.6)" overflow="visible" color="#000000">[Q5]:Does the given image correctly
    reflect the spatial relationship (<SpatialHere>) of each object described in the
    corresponding prompts? [OPTIONS]: 1.All spatial relationships are wrong; 2.Some
    of them are wrong; 3.All spatial relationships are right.</foreignobject></g></g></svg><svg
    id="A5.p14.pic1" class="ltx_picture" height="73.07" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,73.07) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="45.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">[Q6]:Does the given image correctly
    reflect the action of each object (<ActionHere>) described in the corresponding
    prompts? [OPTIONS]: 1.All actions are wrong; 2.Some of them are wrong; 3.All actions
    are right.</foreignobject></g></g></svg>'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Additional Quantitative Analysis
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 附加定量分析
- en: F.1 Generalization Experiments
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 泛化实验
- en: 'To verify the generalization capability of our evaluation model, We compared
    MLLM’s SFT using different training datasets: one with images generated by all
    8 text-to-image models and another with images generated by only 4 of these models,
    while the final evaluation was conducted on images generated by the other 4 models.
    As shown in Table [10](#A6.T10 "Table 10 ‣ F.1 Generalization Experiments ‣ Appendix
    F Additional Quantitative Analysis ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models") and Table [11](#A6.T11
    "Table 11 ‣ F.1 Generalization Experiments ‣ Appendix F Additional Quantitative
    Analysis ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models"), We observed that MLLMs trained on
    images from a subset of text-to-image models can effectively generalize to images
    generated by unseen text-to-image models.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证我们评估模型的泛化能力，我们比较了使用不同训练数据集的MLLM的SFT：一个是由所有 8 个文本到图像模型生成的图像，另一个是由其中 4 个模型生成的图像，而最终评估是在由其他
    4 个模型生成的图像上进行的。如表 [10](#A6.T10 "表 10 ‣ F.1 泛化实验 ‣ 附录 F 附加定量分析 ‣ EvalAlign: 用于评估文本到图像模型的人工对齐数据的监督微调多模态
    LLMs")和表 [11](#A6.T11 "表 11 ‣ F.1 泛化实验 ‣ 附录 F 附加定量分析 ‣ EvalAlign: 用于评估文本到图像模型的人工对齐数据的监督微调多模态
    LLMs")所示，我们观察到基于部分文本到图像模型生成的图像训练的MLLM能够有效泛化到由未见过的文本到图像模型生成的图像。'
- en: 'Table 10: Ablation study on the number of different text-to-image models used
    to generate the training data for evaluating image faithfulness. We observe that
    EvalAlign exhibits strong generalization capability.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：对用于生成训练数据以评估图像真实度的不同文本到图像模型数量的消融研究。我们观察到**EvalAlign**展现了强大的泛化能力。
- en: '| Method | T2I models | body | hand | face | object | common | MAE |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | T2I 模型 | 身体 | 手 | 脸 | 物体 | 常见 | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | - | 1.4988 | 0.8638 | 1.1648 | 2.2096 | 0.8710 | 0 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | - | 1.4988 | 0.8638 | 1.1648 | 2.2096 | 0.8710 | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | 8 | 1.6058 | 0.7901 | 1.1974 | 2.2783 | 0.8871 | 0.0596 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 8 | 1.6058 | 0.7901 | 1.1974 | 2.2783 | 0.8871 | 0.0596 |'
- en: '| 4 | 1.6522 | 0.9588 | 1.2355 | 2.3032 | 0.9516 | 0.0987 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.6522 | 0.9588 | 1.2355 | 2.3032 | 0.9516 | 0.0987 |'
- en: 'Table 11: Ablation study on the number of different text-to-image models used
    to generate the training data for evaluating text-to-image alignment. We observe
    that EVALALIGN exhibits strong generalization capability.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：对用于生成训练数据以评估文本到图像对齐的不同文本到图像模型数量的消融研究。我们观察到**EVALALIGN**展现了强大的泛化能力。
- en: '| Method | T2I models | Object | Count | Color | Style | Spatial | Action |
    MAE |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | T2I 模型 | 物体 | 数量 | 颜色 | 风格 | 空间 | 动作 | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | - | 1.7373 | 1.3131 | 2.0000 | 1.9333 | 1.5952 | 1.8837 | 0 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | - | 1.7373 | 1.3131 | 2.0000 | 1.9333 | 1.5952 | 1.8837 | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | 8 | 1.7203 | 1.3232 | 1.9565 | 1.9333 | 1.6547 | 1.8605 | 0.0256
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 8 | 1.7203 | 1.3232 | 1.9565 | 1.9333 | 1.6547 | 1.8605 | 0.0256
    |'
- en: '| 4 | 1.7832 | 1.3526 | 1.9637 | 1.9876 | 1.6891 | 1.8954 | 0.0469 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.7832 | 1.3526 | 1.9637 | 1.9876 | 1.6891 | 1.8954 | 0.0469 |'
- en: F.2 Instruction Enhancement Experiments
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 指令增强实验
- en: 'Table 12: Ablation study on the enhancement of instructions. Results are reported
    on image faithfulness under different instructions. We observe that enhanced instructions
    can significantly improves the evaluation metrics. MAE: mean absolute error.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：对指令增强的消融研究。结果报告了在不同指令下图像真实度的评估。我们观察到，增强的指令可以显著改善评估指标。MAE：均值绝对误差。
- en: '| Method | Instruction | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD
    v1.5 | SD v2.1 | LCM | MAE |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 指令 | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5 | SD v2.1
    | LCM | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 | 0 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766 | 1.0066
    | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | ✗ | 1.9565 | 1.9286 | 1.8565 | 1.1818 | 1.3419 | 1.4801 | 1.4078
    | 1.1051 | 0.1201 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | ✗ | 1.9565 | 1.9286 | 1.8565 | 1.1818 | 1.3419 | 1.4801 | 1.4078
    | 1.1051 | 0.1201 |'
- en: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
- en: 'Providing more contextual information for instructions enhances the performance
    of MLLMs. To further improve MLLM evaluation performance, we enhanced the prompts
    for both SFT and inference stages. As shown in Table  [12](#A6.T12 "Table 12 ‣
    F.2 Instruction Enhancement Experiments ‣ Appendix F Additional Quantitative Analysis
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), our experiments demonstrate that the enhanced
    prompts significantly increase evaluation accuracy. Specifically, the evaluation
    using enhanced instructions reduced the MAE metric by half, from 0.120 to 0.006,
    compared to the original instructions. Additionally, this approach consistently
    improved evaluation performance across different text-to-image models.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '提供更多上下文信息来增强指令可以提高 MLLMs 的性能。为了进一步提高 MLLM 评估性能，我们增强了 SFT 和推理阶段的提示。正如表 [12](#A6.T12
    "表 12 ‣ F.2 指令增强实验 ‣ 附录 F 额外定量分析 ‣ EvalAlign: 使用人类对齐数据进行监督微调的多模态 LLM 评估文本到图像模型")
    所示，我们的实验表明，增强的提示显著提高了评估准确性。具体而言，使用增强指令的评估将 MAE 指标从 0.120 减少到 0.006，相比于原始指令。此外，这种方法在不同的文本到图像模型中始终提高了评估性能。'
- en: F.3 Mulit-scaling Resolutions Experiments
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 多尺度分辨率实验
- en: 'Table 13: Ablation study on multi-scale input. Results are reported on image
    faithfulness under different input strategy. We observe that input with multi-scale
    resolution images can improves the evaluation metrics. MAE: mean absolute error.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：多尺度输入的消融研究。结果报告了不同输入策略下图像忠实度的表现。我们观察到，使用多尺度分辨率图像可以改善评估指标。MAE：平均绝对误差。
- en: '| Method | Multi Scale | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD
    v1.5 | SD v2.1 | LCM | MAE |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 多尺度 | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5 | SD v2.1
    | LCM | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 | 0 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766 | 1.0066
    | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign |  | 1.8105 | 1.9238 | 1.9325 | 1.2078 | 1.2247 | 1.4540 | 1.3012
    | 1.0554 | 0.1358 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign |  | 1.8105 | 1.9238 | 1.9325 | 1.2078 | 1.2247 | 1.4540 | 1.3012
    | 1.0554 | 0.1358 |'
- en: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
- en: 'In the design of LLaVA-Next, using multi-scale resolution images as input helps
    address the issue of detail information loss, which significantly impacts the
    evaluation of image faithfulness, such as assessing deformations in hands and
    faces. We conducted a multi-scale image training comparison experiment to validate
    this approach. The baseline was the 13B LLaVA model with 336$\times$336, 672$\times$1008)
    as input. As shown in Table [13](#A6.T13 "Table 13 ‣ F.3 Mulit-scaling Resolutions
    Experiments ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"), training with multi-scale inputs significantly enhanced the model’s
    understanding of image and achieved better evaluation performance.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '在 LLaVA-Next 的设计中，使用多尺度分辨率图像作为输入有助于解决细节信息丢失的问题，这对图像忠实度的评估有显著影响，例如评估手部和面部的变形。我们进行了多尺度图像训练对比实验来验证这种方法。基准模型是
    13B LLaVA 模型，输入分辨率为 336$\times$336 和 672$\times$1008。正如表 [13](#A6.T13 "表 13 ‣
    F.3 多尺度分辨率实验 ‣ 附录 F 额外定量分析 ‣ EvalAlign: 使用人类对齐数据进行监督微调的多模态 LLM 评估文本到图像模型") 所示，使用多尺度输入进行训练显著增强了模型对图像的理解，并且取得了更好的评估性能。'
- en: F.4 Full Comparison with Existing Evaluation Methods
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 与现有评估方法的全面比较
- en: 'Table 14: Results on faithfulness.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：忠实度结果。
- en: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 人类 | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore | IS
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ | 31.6226${}^{1~{}~{}}$
    | 0.9696 ${}^{1~{}~{}}$ | 5.2583^(21) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ | 31.6226${}^{1~{}~{}}$
    | 0.9696 ${}^{1~{}~{}}$ | 5.2583^(21) |'
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ | 29.2322${}^{6~{}~{}}$
    | 5.6452^(14) |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ | 29.2322${}^{6~{}~{}}$
    | 5.6452^(14) |'
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ | 29.8197${}^{3~{}~{}}$
    | 0.7245 ${}^{2~{}~{}}$ | 5.3985^(16) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ | 29.8197${}^{3~{}~{}}$
    | 0.7245 ${}^{2~{}~{}}$ | 5.3985^(16) |'
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ | 29.0620${}^{7~{}~{}}$
    | 0.7043 ${}^{3~{}~{}}$ | 5.3774^(18) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ | 29.0620${}^{7~{}~{}}$
    | 0.7043 ${}^{3~{}~{}}$ | 5.3774^(18) |'
- en: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ | 30.6622${}^{2~{}~{}}$
    | 5.9751${}^{4~{}~{}}$ |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ | 30.6622${}^{2~{}~{}}$
    | 5.9751${}^{4~{}~{}}$ |'
- en: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ | 29.3588${}^{5~{}~{}}$
    | 21.6532${}^{4~{}~{}}$ |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ | 29.3588${}^{5~{}~{}}$
    | 21.6532${}^{4~{}~{}}$ |'
- en: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) | 26.3475^(13)
    | 0.8196^(15) | 0.1478 ^(16) | 20.8637^(10) | 5.7368^(10) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) | 26.3475^(13)
    | 0.8196^(15) | 0.1478 ^(16) | 20.8637^(10) | 5.7368^(10) |'
- en: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ | 25.7396^(17) | 0.7555^(24)
    | -0.0507^(22) | 20.4594^(21) | 5.5428^(15) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ | 25.7396^(17) | 0.7555^(24)
    | -0.0507^(22) | 20.4594^(21) | 5.5428^(15) |'
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ | 27.3299^(10) |
    0.8364${}^{8~{}~{}}$ | 21.4824${}^{5~{}~{}}$ | 5.6575^(12) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ | 27.3299^(10) |
    0.8364${}^{8~{}~{}}$ | 21.4824${}^{5~{}~{}}$ | 5.6575^(12) |'
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) | 25.5764^(18)
    | 0.8165^(18) | -0.1022^(23) | 20.6211^(18) | 5.8643${}^{6~{}~{}}$ |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) | 25.5764^(18)
    | 0.8165^(18) | -0.1022^(23) | 20.6211^(18) | 5.8643${}^{6~{}~{}}$ |'
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) | 26.2798^(14)
    | 0.8101^(20) | 0.2042 ^(12) | 20.7880^(12) | 6.1760${}^{1~{}~{}}$ |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) | 26.2798^(14)
    | 0.8101^(20) | 0.2042 ^(12) | 20.7880^(12) | 6.1760${}^{1~{}~{}}$ |'
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) | 26.1180^(15)
    | 0.7809^(23) | -0.1264^(24) | 20.3873^(24) | 5.3861^(17) |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) | 26.1180^(15)
    | 0.7809^(23) | -0.1264^(24) | 20.3873^(24) | 5.3861^(17) |'
- en: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) | 26.5823^(12) | 0.8377${}^{7~{}~{}}$
    | 21.0502${}^{9~{}~{}}$ | 5.3073^(19) |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) | 26.5823^(12) | 0.8377${}^{7~{}~{}}$
    | 21.0502${}^{9~{}~{}}$ | 5.3073^(19) |'
- en: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) | 25.3481^(21) | 0.8170^(17)
    | 0.0872 ^(18) | 20.7529^(13) | 5.9060${}^{5~{}~{}}$ |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) | 25.3481^(21) | 0.8170^(17)
    | 0.0872 ^(18) | 20.7529^(13) | 5.9060${}^{5~{}~{}}$ |'
- en: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) | 24.6984^(23)
    | 0.7958^(22) | -0.0415^(21) | 20.4088^(22) | 5.1995^(22) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) | 24.6984^(23)
    | 0.7958^(22) | -0.0415^(21) | 20.4088^(22) | 5.1995^(22) |'
- en: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) | 25.1572^(22)
    | 0.8101^(21) | 0.0218 ^(20) | 20.6155^(19) | 5.7657${}^{8~{}~{}}$ |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) | 25.1572^(22)
    | 0.8101^(21) | 0.0218 ^(20) | 20.6155^(19) | 5.7657${}^{8~{}~{}}$ |'
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    | 0.8543${}^{3~{}~{}}$ | 21.2664${}^{7~{}~{}}$ | 5.6614^(11) |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    | 0.8543${}^{3~{}~{}}$ | 21.2664${}^{7~{}~{}}$ | 5.6614^(11) |'
- en: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) | 25.4972^(19) | 0.8214^(13)
    | 0.1686 ^(14) | 20.7143^(16) | 6.0535${}^{2~{}~{}}$ |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) | 25.4972^(19) | 0.8214^(13)
    | 0.1686 ^(14) | 20.7143^(16) | 6.0535${}^{2~{}~{}}$ |'
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) | 27.4512${}^{9~{}~{}}$
    | 0.6087 ${}^{5~{}~{}}$ | 20.7474^(14) | 5.3012^(20) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) | 27.4512${}^{9~{}~{}}$
    | 0.6087 ${}^{5~{}~{}}$ | 20.7474^(14) | 5.3012^(20) |'
- en: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) | 25.3697^(20) | 0.8190^(16)
    | 0.1050 ^(17) | 20.6535^(17) | 5.8571${}^{7~{}~{}}$ |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) | 25.3697^(20) | 0.8190^(16)
    | 0.1050 ^(17) | 20.6535^(17) | 5.8571${}^{7~{}~{}}$ |'
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |
    26.5901^(11) | 0.8341${}^{9~{}~{}}$ | 0.3562 ^(10) | 20.8358^(11) | 5.0181^(23)
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |
    26.5901^(11) | 0.8341${}^{9~{}~{}}$ | 0.3562 ^(10) | 20.8358^(11) | 5.0181^(23)
    |'
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) | 27.4836${}^{8~{}~{}}$
    | 0.4463 ${}^{8~{}~{}}$ | 20.7170^(15) | 5.6502^(13) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) | 27.4836${}^{8~{}~{}}$
    | 0.4463 ${}^{8~{}~{}}$ | 20.7170^(15) | 5.6502^(13) |'
- en: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) | 23.8133^(24)
    | 0.8151^(19) | 0.0695 ^(19) | 20.4780^(20) | 4.3824^(24) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) | 23.8133^(24)
    | 0.8151^(19) | 0.0695 ^(19) | 20.4780^(20) | 4.3824^(24) |'
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) | 25.9522^(16) |
    0.8329^(11) | 0.1637 ^(15) | 20.4035^(23) | 5.7451${}^{9~{}~{}}$ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) | 25.9522^(16) |
    0.8329^(11) | 0.1637 ^(15) | 20.4035^(23) | 5.7451${}^{9~{}~{}}$ |'
- en: 'Table 15: Results on text-to-image alignment.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '表 15: 文本到图像对齐的结果。'
- en: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ | 32.5477^(10) | 0.8579${}^{2~{}~{}}$
    | 21.1998^(10) | 7.1864${}^{5~{}~{}}$ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ | 32.5477^(10) | 0.8579${}^{2~{}~{}}$
    | 21.1998^(10) | 7.1864${}^{5~{}~{}}$ |'
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ | 32.7140${}^{9~{}~{}}$
    | 0.3820 ${}^{6~{}~{}}$ | 21.1284^(12) | 6.6571^(20) |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ | 32.7140${}^{9~{}~{}}$
    | 0.3820 ${}^{6~{}~{}}$ | 21.1284^(12) | 6.6571^(20) |'
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ | 35.6465${}^{3~{}~{}}$
    | 0.4738 ${}^{2~{}~{}}$ | 7.1101${}^{8~{}~{}}$ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ | 35.6465${}^{3~{}~{}}$
    | 0.4738 ${}^{2~{}~{}}$ | 7.1101${}^{8~{}~{}}$ |'
- en: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ | 33.8011${}^{6~{}~{}}$
    | 0.3833 ${}^{5~{}~{}}$ | 7.0067^(14) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ | 33.8011${}^{6~{}~{}}$
    | 0.3833 ${}^{5~{}~{}}$ | 7.0067^(14) |'
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ | 37.0493${}^{1~{}~{}}$
    | 0.6542 ${}^{1~{}~{}}$ | 6.9293^(16) |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ | 37.0493${}^{1~{}~{}}$
    | 0.6542 ${}^{1~{}~{}}$ | 6.9293^(16) |'
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ | 31.0951^(14) | 0.8434${}^{8~{}~{}}$
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ | 31.0951^(14) | 0.8434${}^{8~{}~{}}$
    |'
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ | 32.7752${}^{8~{}~{}}$
    | 21.7627${}^{6~{}~{}}$ | 6.8389^(19) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ | 32.7752${}^{8~{}~{}}$
    | 21.7627${}^{6~{}~{}}$ | 6.8389^(19) |'
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ | 35.1593${}^{4~{}~{}}$
    | 0.4322 ${}^{4~{}~{}}$ | 7.2762${}^{1~{}~{}}$ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ | 35.1593${}^{4~{}~{}}$
    | 0.4322 ${}^{4~{}~{}}$ | 7.2762${}^{1~{}~{}}$ |'
- en: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ | 36.4632${}^{2~{}~{}}$
    | 0.2513 ${}^{7~{}~{}}$ | 7.1280${}^{7~{}~{}}$ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ | 36.4632${}^{2~{}~{}}$
    | 0.2513 ${}^{7~{}~{}}$ | 7.1280${}^{7~{}~{}}$ |'
- en: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) | 31.1495^(12)
    | 0.8173^(16) | -0.0867^(14) | 21.1163^(13) | 6.2729^(22) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) | 31.1495^(12)
    | 0.8173^(16) | -0.0867^(14) | 21.1163^(13) | 6.2729^(22) |'
- en: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) | 31.1017^(13) | 0.8278^(14)
    | -0.0453^(12) | 21.2093${}^{9~{}~{}}$ |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) | 31.1017^(13) | 0.8278^(14)
    | -0.0453^(12) | 21.2093${}^{9~{}~{}}$ |'
- en: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) | 28.7957^(24)
    | 0.8264^(15) | -0.1337^(15) | 20.9625^(17) | 4.9593^(24) |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) | 28.7957^(24)
    | 0.8264^(15) | -0.1337^(15) | 20.9625^(17) | 4.9593^(24) |'
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |
    34.8196${}^{5~{}~{}}$ | 0.2295 ${}^{8~{}~{}}$ | 6.8581^(18) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |
    34.8196${}^{5~{}~{}}$ | 0.2295 ${}^{8~{}~{}}$ | 6.8581^(18) |'
- en: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) | 30.6153^(17) | 0.8298^(13)
    | -0.1424^(16) | 21.1905^(11) | 7.0124^(13) |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) | 30.6153^(17) | 0.8298^(13)
    | -0.1424^(16) | 21.1905^(11) | 7.0124^(13) |'
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |
    31.9503^(11) | 0.8319^(12) | -0.0222^(11) | 21.1141^(14) | 6.1987^(23) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |
    31.9503^(11) | 0.8319^(12) | -0.0222^(11) | 21.1141^(14) | 6.1987^(23) |'
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) | 30.6615^(16)
    | 0.7751^(23) | -0.5028^(22) | 20.7491^(21) | 7.2743${}^{2~{}~{}}$ |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) | 30.6615^(16)
    | 0.7751^(23) | -0.5028^(22) | 20.7491^(21) | 7.2743${}^{2~{}~{}}$ |'
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |
    33.7712${}^{7~{}~{}}$ | 6.9363^(15) |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |
    33.7712${}^{7~{}~{}}$ | 6.9363^(15) |'
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) | 30.5644^(18)
    | 0.8140^(18) | -0.2728^(18) | 20.9899^(16) | 6.4238^(21) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) | 30.5644^(18)
    | 0.8140^(18) | -0.2728^(18) | 20.9899^(16) | 6.4238^(21) |'
- en: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) | 29.9149^(20) | 0.8048^(20)
    | -0.3438^(19) | 20.8462^(19) | 7.0150^(12) |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) | 29.9149^(20) | 0.8048^(20)
    | -0.3438^(19) | 20.8462^(19) | 7.0150^(12) |'
- en: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) | 30.1673^(19) | 0.8142^(17)
    | -0.2213^(17) | 20.8640^(18) | 7.1642${}^{6~{}~{}}$ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) | 30.1673^(19) | 0.8142^(17)
    | -0.2213^(17) | 20.8640^(18) | 7.1642${}^{6~{}~{}}$ |'
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) | 30.7820^(15)
    | 0.7974^(21) | -0.3591^(20) | 21.0257^(15) | 7.0709^(10) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) | 30.7820^(15)
    | 0.7974^(21) | -0.3591^(20) | 21.0257^(15) | 7.0709^(10) |'
- en: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) | 29.2865^(22)
    | 0.8066^(19) | -0.4172^(21) | 20.6327^(23) | 7.0610^(11) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) | 29.2865^(22)
    | 0.8066^(19) | -0.4172^(21) | 20.6327^(23) | 7.0610^(11) |'
- en: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) | 29.8126^(21)
    | 0.7601^(24) | -0.6095^(24) | 20.7046^(22) | 7.2273${}^{4~{}~{}}$ |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) | 29.8126^(21)
    | 0.7601^(24) | -0.6095^(24) | 20.7046^(22) | 7.2273${}^{4~{}~{}}$ |'
- en: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) | 29.2389^(23)
    | 0.7851^(22) | -0.6051^(23) | 20.5973^(24) | 6.8613^(17) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) | 29.2389^(23)
    | 0.7851^(22) | -0.6051^(23) | 20.5973^(24) | 6.8613^(17) |'
- en: 'Table [14](#A6.T14 "Table 14 ‣ F.4 Full Comparison with Existing Evaluation
    Methods ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models") and Table [15](#A6.T15 "Table 15 ‣ F.4 Full Comparison with Existing
    Evaluation Methods ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign:
    Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating
    Text-to-Image Models") present the comparison results between our proposed method
    and existing alternatives. Owing to the powerful aligned understanding of images
    and text by MLLMs, our model achieves the best alignment with human performance
    across all tested benchmarks for 24 text-to-image models, along with a significant
    improvement in image faithfulness. This indicates that our model excels in both
    comprehending and evaluating the intricate details of generated images, closely
    mirroring human judgment and setting a new standard for image-text alignment and
    faithfulness.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '表[14](#A6.T14 "表 14 ‣ F.4 与现有评估方法的全面比较 ‣ 附录 F 附加定量分析 ‣ EvalAlign: 使用与人类对齐的数据对多模态
    LLM 进行监督微调以评估文本到图像模型")和表[15](#A6.T15 "表 15 ‣ F.4 与现有评估方法的全面比较 ‣ 附录 F 附加定量分析 ‣
    EvalAlign: 使用与人类对齐的数据对多模态 LLM 进行监督微调以评估文本到图像模型")展示了我们提出的方法与现有替代方法之间的比较结果。由于MLLMs对图像和文本的强大对齐理解，我们的模型在24个文本到图像模型的所有测试基准中，与人类表现的对齐最佳，并且在图像真实性方面有显著提升。这表明我们的模型在理解和评估生成图像的复杂细节方面表现优异，紧密地反映了人类判断，为图像与文本对齐和真实性设立了新标准。'
- en: Appendix G Qualitative Analysis
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 定性分析
- en: 'As shown in Figure [5](#A7.F5 "Figure 5 ‣ Appendix G Qualitative Analysis ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), we present a comparison of different evaluation
    metrics on images generated by four models, including human annotated scores,
    EvalAlign, ImageReward [[56](#bib.bib56)], HPSv2 [[54](#bib.bib54)], and PickScore [[20](#bib.bib20)].
    The digits in the figure represent the ranking for each evaluation metric, with
    darker colors indicating higher rankings. From the figure, it is evident that
    our proposed EvalAlign metric closely matches the human rankings across two evaluation
    dimensions, demonstrating excellent consistency.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[5](#A7.F5 "图 5 ‣ 附录 G 定性分析 ‣ EvalAlign: 使用与人类对齐的数据对多模态 LLM 进行监督微调以评估文本到图像模型")所示，我们展示了四种模型生成的图像在不同评估指标上的比较，包括人工标注分数、EvalAlign、ImageReward [[56](#bib.bib56)]、HPSv2 [[54](#bib.bib54)]和PickScore [[20](#bib.bib20)]。图中的数字代表每个评估指标的排名，颜色越深表示排名越高。从图中可以明显看出，我们提出的EvalAlign指标在两个评估维度上与人工排名高度一致，表现出色的一致性。'
- en: 'Furthermore, Figure [6](#A7.F6 "Figure 6 ‣ Appendix G Qualitative Analysis
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models") showcases the EvalAlign evaluation metric across
    different fine-grained results. The numbers in the figure represent EvalAlign
    scores for the corresponding evaluation aspect, with darker colors indicating
    higher scores and better generation performance. Note that if the text prompt
    does not specify a particular style, the style consistency score defaults to 0.
    From these results, it is evident that the same text-to-image model exhibits significant
    performance variation across different evaluation aspects.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图 [6](#A7.F6 "图 6 ‣ 附录 G 定性分析 ‣ EvalAlign：用人类对齐数据对文本到图像模型进行监督微调的多模态大模型")
    展示了 EvalAlign 评价指标在不同细粒度结果中的表现。图中的数字表示对应评价方面的 EvalAlign 分数，颜色越深表示分数越高，生成性能越好。请注意，如果文本提示没有指定特定风格，则风格一致性分数默认为
    0。从这些结果中，可以明显看出相同的文本到图像模型在不同评价方面表现出显著的性能差异。
- en: '![Refer to caption](img/7680590ff159f32f823ec69fde57cd7c.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7680590ff159f32f823ec69fde57cd7c.png)'
- en: 'Figure 5: Qualitative results of EvalAlign benchmark. As can be concluded,
    EvalAlign is consistently aligned with fine-grained human preference in terms
    of image faithfulness and text-image alignment, while other methods fail to do
    so.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：EvalAlign 基准的定性结果。可以得出结论，EvalAlign 在图像忠实度和文本-图像对齐方面与细粒度的人类偏好始终保持一致，而其他方法则未能做到这一点。
- en: '![Refer to caption](img/735d8f6edbed6474fdd91e3d2241f9fb.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/735d8f6edbed6474fdd91e3d2241f9fb.png)'
- en: 'Figure 6: Qualitative results of EvalAlign benchmark. As can be inferred, EvalAlign
    distinctively provides multiple fine-grained scores covering every aspect of image
    faithfulness and text-image alignment. Additionally, we noticed that, in general,
    a model cannot perform well on every fine-grained aspect, which is consistent
    with [[21](#bib.bib21)].'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：EvalAlign 基准的定性结果。如可以推测的，EvalAlign 显著地提供了多个细粒度的评分，涵盖了图像忠实度和文本-图像对齐的各个方面。此外，我们注意到，一般来说，一个模型不能在每个细粒度方面表现出色，这与[[21](#bib.bib21)]一致。
