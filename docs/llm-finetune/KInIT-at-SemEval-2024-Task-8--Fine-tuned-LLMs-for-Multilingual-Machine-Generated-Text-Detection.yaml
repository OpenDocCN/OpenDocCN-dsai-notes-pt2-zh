- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13671](https://ar5iv.labs.arxiv.org/html/2402.13671)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michal Spiegel^(1,2)    Dominik Macko¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Kempelen Institute of Intelligent Technologies
  prefs: []
  type: TYPE_NORMAL
- en: ² Faculty of Informatics, Masaryk University
  prefs: []
  type: TYPE_NORMAL
- en: michal.spiegel@intern.kinit.sk, dominik.macko@kinit.sk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual
    black-box machine-generated text detection. Such a detection is important for
    preventing a potential misuse of large language models (LLMs), the newest of which
    are very capable in generating multilingual human-like texts. We have coped with
    this task in multiple ways, utilizing language identification and parameter-efficient
    fine-tuning of smaller LLMs for text classification. We have further used the
    per-language classification-threshold calibration to uniquely combine fine-tuned
    models predictions with statistical detection metrics to improve generalization
    of the system detection performance. Our submitted method achieved competitive
    results, ranking at the fourth place, just under 1 percentage point behind the
    winner.
  prefs: []
  type: TYPE_NORMAL
- en: 'KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection'
  prefs: []
  type: TYPE_NORMAL
- en: Michal Spiegel^(1,2)  and Dominik Macko¹ ¹ Kempelen Institute of Intelligent
    Technologies ² Faculty of Informatics, Masaryk University michal.spiegel@intern.kinit.sk,
    dominik.macko@kinit.sk
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent large language models (LLMs) are able to generate high-quality texts
    that are not easily detectable by human readers. A problem arises when such generated
    texts are misused for academic exams (Achiam et al., [2023](#bib.bib1)), plagiarism
    (Wahle et al., [2022](#bib.bib17)), disinformation spreading (Vykopal et al.,
    [2023](#bib.bib16)), etc. Therefore, it is crucial to develop automated means
    to detect machine-generated texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'SemEval-2024 Task 8 (Wang et al., [2024](#bib.bib18)) consists of three subtasks:
    A) binary human-written vs. machine-generated text classification, B) multi-way
    machine-generated text classification, and C) human-machine mixed text detection.
    In our work, we have focused on subtask A, especially its multilingual track.
    It covered 8 known languages for training (Arabic, Bulgarian, Chinese, English,
    German, Indonesian, Russian, Urdu), multiple domains (e.g., Wikipedia, news, abstracts),
    and multiple text generators (e.g., GPT-3, ChatGPT, BLOOMZ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'During our participation in the shared task, we have explored various alternatives.
    Our best submitted solution (illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection")) combines two fine-tuned LLMs (green-colored) with statistical
    detection (orange-colored) using a two-step majority voting (purple-colored) based
    ensemble method. Such a system achieved fourth place in the final leaderboard,
    with a performance of 95% in accuracy, within 1 percentage point range behind
    the winning system. We have published the source code for easier replication purposes¹¹1[https://github.com/kinit-sk/semeval-2024-task-8-machine-text-detection](https://github.com/kinit-sk/semeval-2024-task-8-machine-text-detection).
    We have used the statistical detection methods implemented in the recently published
    IMGTB framework²²2[https://github.com/michalspiegel/IMGTB](https://github.com/michalspiegel/IMGTB),
    which will be extended to also support all the fine-tunning options that we have
    used in this work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: System components overview.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our key observations and contributions include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have proposed a unique way of combining the statistical and fine-tuned detection
    methods using a two-way majority voting and a per-language threshold calibration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have proposed and compared three ensemble system alternatives to cope with
    multilingual machine-generated text detection (additional two in the post-deadline
    study).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have experienced a remarkably good performance of fine-tuned LLMs of 7B parameters
    in this task. We have not used such “a big hammer” for the classification task
    before.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have proposed the best-performing single-model system called rMistral (Mistral-7B
    fine-tuned in a robust way – using both the train and dev sets and obfuscating
    20% of the train data), achieving 0.97 AUC ROC on the test data. Although our
    per-language threshold calibration method would not bring the best accuracy on
    the test set (0.93), the threshold fixed to 1.0 (only predictions with a probability
    of 1, i.e. 100% confident, are marked as machine-generated) would won the competition
    (accuracy of 0.97). Nevertheless, we have noticed such a threshold performance
    only after the deadline and we considered the model being over-fitted (we would
    not submitted the results) which turned-out to be false.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the machine-generated text detection task, three main groups of methods
    are nowadays used (Uchendu et al., [2023](#bib.bib14)). The first one is a stylometric
    detection, which uses linguistic features (e.g., n-grams) to differentiate between
    human and machine writing styles (Fröhling and Zubiaga, [2021](#bib.bib4); Kumarage
    et al., [2023](#bib.bib9)). The second group is a statistical detection, which
    uses statistical distribution based on a pre-trained language model (e.g., GPT2)
    to calculate various metrics (e.g., entropy) that can be used even without training
    (i.e., better generalization) to differentiate machine and human written texts
    (Mitchell et al., [2023](#bib.bib13); Hans et al., [2024](#bib.bib7)). The last
    group is a fine-tuned detection, which further trains an already pre-trained language
    model for the detection task (Uchendu et al., [2020](#bib.bib15); Macko et al.,
    [2023](#bib.bib11)).
  prefs: []
  type: TYPE_NORMAL
- en: In the SemEval2024 Task 8 (Wang et al., [2024](#bib.bib18)), we have focused
    on the multilingual track of Subtask A, which aimed at a binary classification
    to differentiate between human-written and machine-generated texts. The provided
    dataset (not allowing additional training data) contained the predefined splits
    of train, dev, and test sets. The train and dev sets officially contained 8 languages
    (3 languages in the dev set only), while unknown number of languages is contained
    in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Due to a multilingual nature of the data and our previous experience in multilingual
    machine-generated text detection (Macko et al., [2023](#bib.bib11)), we wanted
    to try-out something new in this shared task. Our initial idea was to experiment
    with a per-language “mixture-of-experts”, which would consist of multiple models,
    fine-tuned in a monolingual way per each official language in the train and dev
    sets. Since it was expected that surprise languages will be present in the test
    set, we would have used an additional multilingually fine-tuned model for other
    languages. However, we have started the experiments only few weeks before the
    deadline, which gave us little time to cope with the problems such as over-fitting
    and hyper-parameter optimisation (shown as severe towards the deadline).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, while training these per-language models, we also started to fine-tune
    the Falcon-7B model (Almazrouei et al., [2023](#bib.bib2)) for the machine-generated
    text detection task, inspired by the winning system (Gagiano and Tian, [2023](#bib.bib5))
    of the recent ALTA 2023 shared task (although English monolingual). Since Falcon-7B
    is pre-trained on two languages only (English and French), we did not want to
    use it in a standalone way due to uncertain cross-lingual capability. Therefore,
    we have similarly fine-tuned the Mistral-7B model (Jiang et al., [2023](#bib.bib8)),
    which is similarly sized generative model outperforming even some 13B parameters
    models in common benchmarks. We have not previously experimented with such a “big
    hammer” for the task; therefore, it was a new interesting experience for us. We
    have further combined these LLMs with statistical detectors to ensure better generalization
    of the system, which is described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 3 System Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| System | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\ast$LLM2S3 | The system described in this paper. It is an ensemble using
    two-step majority voting for predictions, consisting of 2 LLMs (Falcon-7B and
    Mistral-7B) fine-tuned using the train set only, 3 zero-shot statistical methods
    (Entropy, Rank, Binoculars) using Falcon-7B and Falcon-7B-Instruct for calculation
    of the metrics, utilizing language identification and per-language threshold calibration.
    |'
  prefs: []
  type: TYPE_TB
- en: '| PLMoE | Our initial idea representing a per-language mixture of experts.
    It uses Electra-Large-Discriminator for English and XML-RoBERTa-Large for each
    of other languages officially present in the train and dev sets. Models for languages
    present in the dev set only are trained using the dev set. For unknown languages
    the Mistral-7B fine-tuned using the whole train set is used. |'
  prefs: []
  type: TYPE_TB
- en: '| rLLM2S3 | The same ensemble system as LLM2S3; however, the LLMs are fine-tuned
    using both the train and dev sets. Also, to increase the robustness of the system,
    we have obfuscated 20% of the train samples during fine-tuning, by using HomoglyphAttack
    and inserting zero-width-joiner character, inspired by our recent work (Macko
    et al., [2024](#bib.bib12)). |'
  prefs: []
  type: TYPE_TB
- en: '| rLLM2B-ES | The post-deadline ensemble system similar to rLLM2S3; however,
    the Llama-2-7B is used instead of Falcon-7B and Binoculars is used solely in the
    statistical part (instead of a combination of 3 methods). Moreover, the fine-tuning
    process used the early stopping mechanism to alleviate the over-fitting. |'
  prefs: []
  type: TYPE_TB
- en: '| LLM2B1 | The post-deadline ensemble system using the original LLM2S3 fine-tuned
    Falcon-7B and Mistral-7B models; however, classification thresholds are not calibrated,
    but only predictions with a probability of 1 (i.e., 100% confident) are marked
    as machine-generated. Such predictions are combined with Binoculars zero-shot
    prediction using the per-language threshold calibration. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Description of system alternatives. The main system described in this
    paper is denoted by $\ast$. The last two alternatives were evaluated post-deadline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our best system (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ KInIT at
    SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection"))
    combines the predictions of two fine-tuned LLMs (Falcon-7B and Mistral-7B) with
    the selected statistical metrics (Entropy, Rank, Binoculars) by using a two-step
    majority voting. Firstly, a single majority-voted prediction results out of the
    three statistical metrics. Then, the final majority-voted prediction is a combination
    of the previous one with the Falcon and Mistral predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Each prediction uses a separate classification-decision threshold, which is
    applied on prediction probabilities and statistical metrics. These thresholds
    are calibrated in a per-language way, meaning that separate thresholds are used
    for each language officially present in the train and dev sets, plus an additional
    threshold for unknown languages (i.e., not officially present in the train and
    dev sets). The thresholds are calibrated based on the machine-class prediction
    probabilities and statistical metrics for samples in the train and dev sets combined.
    The calibration maximized the difference between true positive rate (TPR) and
    false positive rate (FPR) based on the ROC (receiver operating characteristic)
    curve. The texts with probabilities (or statistical metrics) outreaching the thresholds
    are considered machine-generated, otherwise they are considered human-written.
    The thresholds are saved and used for prediction of test samples.
  prefs: []
  type: TYPE_NORMAL
- en: Due to unknown languages in the test set and using the per-language threshold
    calibration, we have utilized the FastText³³3[https://pypi.org/project/fasttext-langdetect/](https://pypi.org/project/fasttext-langdetect/)
    language identification. Since it is not fully accurate, we have used such language
    information only if the prediction probability was greater than 0.5, otherwise
    the language was handled as unknown.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the system includes two fine-tuned LLMs, namely Falcon-7B and
    Mistral-7B. For the fine-tuning process, we have used a parameter efficient fine-tuning
    (PEFT) technique called QLoRA (Dettmers et al., [2023](#bib.bib3)) to minimize
    the computational costs of our system training.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the system performance generalization, we have integrated a statistical
    part of the system, which is based on the three statistical metrics, namely Entropy
    (Lavergne et al., [2008](#bib.bib10)), Rank (Gehrmann et al., [2019](#bib.bib6)),
    and recently proposed Binoculars (Hans et al., [2024](#bib.bib7)). The statistical
    metrics are calculated using the Falcon-7B as a base model. Since Binoculars requires
    two models, it uses also Falcon-7B-Instruct (as a performer model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the described best submitted system, we have tried multiple system
    alternatives, which are briefly described in Table [1](#S3.T1 "Table 1 ‣ 3 System
    Overview ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection"). In addition to those ensembles, we have evaluated single detectors,
    namely Falcon, Mistral, S5 (a combination of 5 statistical metrics – likelihood,
    entropy, rank, log-rank, and llm-deviation), and Binoculars. After the deadline,
    we have also finished fine-tuning of Llama-2-7B and retrained the detectors using
    the early stopping (patience of 5) to prevent over-fitting. Also, when knowing
    the gold labels of the test set, we have evaluated various combinations of the
    trained detectors to see whether we have done the right decision for the submission.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the experimental purpose, we have used the defaults splits of the provided
    dataset, namely the train and dev sets in the pre-deadline experiments, and the
    gold labels of the test set for the post-deadline evaluation of the pre-deadline
    system alternatives. The main system described in this paper uses only the train
    set in the training process; however, uses both the train and dev sets for the
    classification threshold calibration. Some of the system alternatives used both
    the train and dev sets in the training process, as described in Table [1](#S3.T1
    "Table 1 ‣ 3 System Overview ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for
    Multilingual Machine-Generated Text Detection").'
  prefs: []
  type: TYPE_NORMAL
- en: As the key evaluation metric in the shared task is accuracy, we have also used
    this metric for the preliminary system evaluation and selection of the alternative
    for submission. Since classification task is sensitive to the used classification
    threshold, we have also used AUC ROC (area under curve of the receiver operating
    characteristic) as a threshold independent metric, providing better information
    about the classification capability.
  prefs: []
  type: TYPE_NORMAL
- en: For the fine-tuning process, we have used the official baseline script⁴⁴4[https://github.com/mbzuai-nlp/SemEval2024-task8/blob/main/subtaskA/baseline/transformer_baseline.py](https://github.com/mbzuai-nlp/SemEval2024-task8/blob/main/subtaskA/baseline/transformer_baseline.py),
    modified to export machine-class prediction probabilities in addition to the predictions.
    Since, it was not clear which version of the XLM-RoBERTa model was marked as a
    baseline in the multilingual track (with the known accuracy of 0.72), we have
    trained both the base (XLM-R-B) and large (XLM-R-L) versions. In addition, we
    have also included mDeBERTa-v3-base (mDeBERTa) model in our baselines, since it
    performed the best in our previous work (Macko et al., [2023](#bib.bib11)).
  prefs: []
  type: TYPE_NORMAL
- en: To perform per-language models fine-tuning, we have used the source field of
    the train and dev data to select data only for the specific language. Other parameters
    of the fine-tuning process remained unchanged. The FastText language identification
    is used for a prediction, which uses the machine-class probability of the corresponding
    language-specific model.
  prefs: []
  type: TYPE_NORMAL
- en: The used QLoRA PEFT fine-tuning process used the binary cross entropy with logits
    for loss calculations and 4-bit quantization using BitsAndBytes⁵⁵5[https://pypi.org/project/bitsandbytes](https://pypi.org/project/bitsandbytes).
    The LoRA configuration⁶⁶6[https://pypi.org/project/peft](https://pypi.org/project/peft)
    used an alpha of 16, a dropout of 0.1, r of 64, and the task type of sequence
    classification. Unlike the baseline fine-tuning, this version used half-precision
    training, gradient accumulation of 4 steps, and evaluation each 1,000 steps. Other
    parameters were the same.
  prefs: []
  type: TYPE_NORMAL
- en: Due to time constraints, we have not done any hyper-parameter optimization;
    thus, further improvements of the system are very likely possible.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The experimental results are provided in Table [5](#S5 "5 Results ‣ KInIT at
    SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection").
    It must be noted that the results in the bottom part of the table are not part
    of the competition, since those experiments were performed after the submission
    deadline of the shared task. Also, the performance results using the test set
    were not known before the deadline; gold labels has been released only afterwards.
    Therefore, the design decisions could be made purely using the dev set.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Accuracy | AUC ROC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | System | Dev | Test | Dev | Test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baselines | XLM-R-B | 0.7158 | 0.7935 | 0.8262 | 0.9040 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-R-L | 0.7275 | 0.8841 | 0.8187 | 0.9063 |'
  prefs: []
  type: TYPE_TB
- en: '| mDeBERTa | 0.6968 | 0.8943 | 0.7952 | 0.9832 |'
  prefs: []
  type: TYPE_TB
- en: '| System Alternatives | $\ast$ | 0.9035 | 0.9501 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| PLMoE$\bullet$ | 0.9878 | 0.5819 | 0.9943 | 0.6268 |'
  prefs: []
  type: TYPE_TB
- en: '| rLLM2S3$\bullet$ | 0.9965 | 0.9560 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Ablation Study | Falcon | 0.8043 | 0.9102 | 0.8775 | 0.9492 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | 0.8560 | 0.9027 | 0.9138 | 0.9579 |'
  prefs: []
  type: TYPE_TB
- en: '| rFalcon | 0.9905 | 0.8843 | 0.9991 | 0.9395 |'
  prefs: []
  type: TYPE_TB
- en: '| rMistral | 0.9980 | 0.9268 | 0.9997 | 0.9713 |'
  prefs: []
  type: TYPE_TB
- en: '| S3$\bullet$ | 0.7248 | 0.8328 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| S5$\bullet$ | 0.5880 | 0.4763 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Binoculars | 0.5430 | 0.7979 | 0.6304 | 0.8777 |'
  prefs: []
  type: TYPE_TB
- en: '| Binoculars$\bullet$ | 0.6240 | 0.8434 | 0.6304 | 0.8777 |'
  prefs: []
  type: TYPE_TB
- en: '| Post-Deadline Study | PLMoE-ES$\bullet$ | 0.9885 | 0.8417 | 0.9947 | 0.9635
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | 0.7335 | 0.7587 | 0.9342 | 0.7400 |'
  prefs: []
  type: TYPE_TB
- en: '| rLlama-2 | 0.8903 | 0.8907 | 0.8416 | 0.9400 |'
  prefs: []
  type: TYPE_TB
- en: '| rLlama-2-ES | 0.9838 | 0.8805 | 0.9960 | 0.9108 |'
  prefs: []
  type: TYPE_TB
- en: '| rFalcon-ES | 0.9410 | 0.8672 | 0.9872 | 0.9503 |'
  prefs: []
  type: TYPE_TB
- en: '| rMistral-ES | 0.9863 | 0.9412 | 0.9984 | 0.9834 |'
  prefs: []
  type: TYPE_TB
- en: '| rLLM2B-ES$\bullet$ | 0.9915 | 0.9700 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM2B1 | 0.8668 | 0.9708 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '|  | rMistral1 | 0.9975 | 0.9675 | 0.9997 | 0.9713 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Detection performance evaluated using the dev (pre-deadline) and test
    (post-deadline) splits separately. The main system described in this paper is
    denoted by $\ast$”. “-ES” denotes using of early-stopping mechanism to prevent
    over-fitting. “N/A” denotes not available values due to prediction-based majority
    voting (i.e., no probabilities to calculate AUC ROC). The gray color denotes unrepresentative
    performance values due to training on the dev set.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to high accuracy and high AUC ROC metrics using the dev set, we considered
    rFalcon and rMistral over-fitted; therefore, we decided not to submit our rLLM2S3
    system. This turned-out to be a mistake, since it performed slightly better than
    the submitted LLM2S3 on the test set. On the other hand, our suspicion of over-fitting
    PLMoE (due to the similar observations) turned-out to be valid, since it performed
    much worse using the test set. Therefore, it seems that per-language monolingually
    fine-tuned (i.e., lower amount of samples) models require optimization of hyper-parameters
    to prevent over-fitting and to better generalize to unseen texts.
  prefs: []
  type: TYPE_NORMAL
- en: As an ablation study, we also provide the results for individual components
    of our system alternatives. As the results show, the ensembling into more complex
    systems of LLM2S3 and rLLM2S3 helped generalization of the classification performance.
    Individual methods would not outperform the submitted system.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Post-Deadline Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the post-deadline experiments (already knowing the gold labels of the test
    set for evaluation), we have finished Llama-2-7B model fine-tuning and retraining
    all three robust-version LLMs using the early stopping (to minimize the over-fitting).
    The results revealed that the rLlama-2 model does not suffer by over-fitting as
    much. Based on the test set evaluation and by examining various combinations,
    the retrained rLlama-2-ES and rMistral-ES seemed like good candidates to combine
    with Binoculars (rLLM2B-ES), outperforming the winning system in the competition.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping helped a lot in boosting performance generalization (i.e., reducing
    over-fitting) of our per-language mixture-of-experts ensemble system (PLMoE-ES),
    achieving one of the highest AUC ROC using the test set. Nevertheless, in the
    accuracy as an official metric, it would not outperform the other system alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we have noticed that optimal thresholds for fine-tuned LLMs are
    often set to 1.0 by using purely the dev set samples machine-class probabilities.
    Therefore, we have fixed the thresholds to 1.0 for the LLM2B1 system (containing
    only models we have trained before the deadline), meaning that the machine-class
    predictions of the LLMs are used only when having 100% confidence (otherwise considered
    human-written). Such predictions, when combined with Binoculars, achieved even
    higher performance using the test set data (0.9708). Thus, we had such a system
    trained before the deadline; however, we have not noticed such a threshold bringing
    the best performance in time. Moreover, when looking at the accuracy for the dev
    set, we do not see why we would select such a system for the submission. It can
    be just a coincidence that it performs so well using the test set data. Further
    experiments are required to examine this phenomenon using independent out-of-distribution
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Also, even when our rLLM2B-ES system alternative or the rMistral1 single-model
    system would won the competition, we are now not sure that we would be confident
    enough (about not being over-fitted) to submit it as the final system without
    evaluation on the external dataset. Thus, we have submitted best what we could
    at the time.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Per-language Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a deeper insight of the proposed system (LLM2S3) performance, we have performed
    an analysis per each language identified in the test set. The results are provided
    in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Per-language Analysis ‣ 5.1 Post-Deadline
    Study ‣ 5 Results ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual
    Machine-Generated Text Detection"). Interesting is that it achieved the highest
    accuracy for the Italian surprise language (it). Lower accuracy is evident for
    German and Arabic languages, although present in the dev set. It must be noted
    that this version of the system was not trained using the dev set, only the classification
    threshold calibration used such data. Therefore, the robust versions of system
    alternatives are expected to provide higher performance especially in those languages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9fe444ed2e459cb39092dc0204f0f7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Per-language test-set performance (it is a surprise language, de
    and ar are in the dev set only). Axis scale for Accuracy is shown from 0.9 to
    1.0\. The per-class samples counts are provided in the top-right table.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To cope with the problem of multilingual, multidomain, and multigenerator machine-generated
    text detection, we have proposed an ensemble system using 2 LLMs (Falcon-7B and
    Mistral-7B) fine-tuned for the binary sequence classification task. We have further
    combined the predictions with the statistical metrics of Entropy, Rank, and Binoculars
    using a two-stage majority voting. The classification thresholds in our system
    have been calibrated in a per-language manner, for which we have utilized the
    FastText language identification. A combination of fine-tuned LLMs and statistical
    detection seems to be the right way to cope with generalization of the detection
    performance. Out of the evaluated single-model systems, Mistral-7B is the best
    candidate for fine-tuning, which by itself can bring a remarkable classification
    performance. Further improvements of the system could be easily achievable by
    hyper-parameters optimization, which we have not done in the submitted system
    due to lack of time.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work was partially supported by the projects funded by the European Union
    under the Horizon Europe: AI-CODE, a project funded by the European Union under
    the Horizon Europe, GA No. [101135437](https://cordis.europa.eu/project/id/101135437),
    and VIGILANT, GA No. [101073921](https://doi.org/10.3030/101073921). Part of the
    research results was obtained using the computational resources procured in the
    national project National competence centre for high performance computing (project
    code: 311070AKF2) funded by European Regional Development Fund, EU Structural
    Funds Informatization of Society, Operational Program Integrated Infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and
    Guilherme Penedo. 2023. Falcon-40B: An open large language model with state-of-the-art
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [QLoRA: Efficient finetuning of quantized llms](http://arxiv.org/abs/2305.14314).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fröhling and Zubiaga (2021) Leon Fröhling and Arkaitz Zubiaga. 2021. Feature-based
    detection of automated language models: tackling GPT-2, GPT-3 and Grover. *PeerJ
    Computer Science*, 7:e443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gagiano and Tian (2023) Rinaldo Gagiano and Lin Tian. 2023. A prompt in the
    right direction: Prompt based classification of machine-generated text detection.
    In *Proceedings of ALTA*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gehrmann et al. (2019) Sebastian Gehrmann, Hendrik Strobelt, and Alexander
    Rush. 2019. GLTR: Statistical detection and visualization of generated text. In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics:
    System Demonstrations*. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hans et al. (2024) Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova,
    Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
    2024. [Spotting LLMs with binoculars: Zero-shot detection of machine-generated
    text](http://arxiv.org/abs/2401.12070).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumarage et al. (2023) Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee,
    Kirill Trapeznikov, Scott Ruston, and Huan Liu. 2023. Stylometric detection of
    ai-generated text in twitter timelines. *arXiv preprint arXiv:2303.03697*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lavergne et al. (2008) Thomas Lavergne, Tanguy Urvoy, and François Yvon. 2008.
    Detecting fake content with relative entropy scoring. In *Proceedings of the 2008
    International Conference on Uncovering Plagiarism, Authorship and Social Software
    Misuse - Volume 377*, PAN’08, page 27–31, Aachen, DEU. CEUR-WS.org.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Macko et al. (2023) Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas,
    Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko,
    and Maria Bielikova. 2023. [MULTITuDE: Large-scale multilingual machine-generated
    text detection benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.616). In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 9960–9987, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Macko et al. (2024) Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel
    Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, and
    Maria Bielikova. 2024. [Authorship obfuscation in multilingual machine-generated
    text detection](http://arxiv.org/abs/2401.07867). *arXiv preprint arXiv:2401.07867*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D.
    Manning, and Chelsea Finn. 2023. [DetectGPT: Zero-shot machine-generated text
    detection using probability curvature](https://arxiv.org/abs/2301.11305). *arXiv
    preprint arXiv:2301.11305*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Uchendu et al. (2023) Adaku Uchendu, Thai Le, and Dongwon Lee. 2023. Attribution
    and obfuscation of neural text authorship: A data mining perspective. *ACM SIGKDD
    Explorations Newsletter*, 25(1):1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uchendu et al. (2020) Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020.
    [Authorship attribution for neural text generation](https://doi.org/10.18653/v1/2020.emnlp-main.673).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 8384–8395, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vykopal et al. (2023) Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro,
    Dominik Macko, and Maria Bielikova. 2023. [Disinformation capabilities of large
    language models](http://arxiv.org/abs/2311.08838). *arXiv preprint arXiv:2311.08838*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wahle et al. (2022) Jan Philip Wahle, Terry Ruas, Frederic Kirstein, and Bela
    Gipp. 2022. [How large language models are transforming machine-paraphrase plagiarism](https://doi.org/10.18653/v1/2022.emnlp-main.62).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem
    Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud,
    Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych,
    and Preslav Nakov. 2024. SemEval-2024 Task 8: Multigenerator, multidomain, and
    multilingual black-box machine-generated text detection. In *Proceedings of the
    18th International Workshop on Semantic Evaluation*, SemEval 2024, Mexico, Mexico.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Computational Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For experiments regarding model fine-tuning and inference processes, we have
    used $1\times$ A100 40GB GPU, cumulatively consuming around 10,000 GPU-core hours.
    For combining the results and analysis, we have used Jupyter Lab running on 4
    CPU cores, without the GPU acceleration.
  prefs: []
  type: TYPE_NORMAL
