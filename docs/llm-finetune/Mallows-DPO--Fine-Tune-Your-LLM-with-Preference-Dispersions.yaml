- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:36:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14953](https://ar5iv.labs.arxiv.org/html/2405.14953)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Haoxian Chen^∗ ,  Hanyang Zhao^∗ ,  Henry Lam ,  David D. Yao  and  Wenpin Tang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Direct Preference Optimization (DPO) has recently emerged as a popular approach
    to improve reinforcement learning with human feedback (RLHF), leading to better
    techniques to fine-tune large language models (LLM). A weakness of DPO, however,
    lies in its lack of capability to characterize the diversity of human preferences.
    Inspired by Mallows’ theory of preference ranking, we develop in this paper a
    new approach, the Mallows-DPO. A distinct feature of this approach is a dispersion
    index, which reflects the dispersion of human preference to prompts. We show that
    existing DPO models can be reduced to special cases of this dispersion index,
    thus unified with Mallows-DPO. More importantly, we demonstrate (empirically)
    how to use this dispersion index to enhance the performance of DPO in a broad
    array of benchmark tasks, from synthetic bandit selection to controllable generations
    and dialogues, while maintaining great generalization capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '^∗Equal Contribution. Emails: {hc3136,hz2684,khl2114,ddy1,wt2319}@columbia.edu,
    Department of Industrial Engineering and Operations Research, Columbia University.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key words: Dispersion, direct preference optimization, fine-tuning, large language
    models (LLMs), Mallows models, reinforcement learning with human feedback (RLHF).'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning with Human Feeback (RLHF, [[28](#bib.bib28), [33](#bib.bib33),
    [44](#bib.bib44)]) has made significant contributions to the success of modern
    Large Language Models (LLMs) such as ChatGPT and GPT4 [[1](#bib.bib1)]. More recently,
    Direct Preference Optimization (DPO) [[30](#bib.bib30)] solves essentially the
    same problem as RLHF, but bypasses the training of the reward model, and thus
    leading to faster speed and better resource efficiency. More importantly, DPO
    also achieves comparable or superior performance against RLHF in downstream tasks
    such as fine-tuning LLMs in Llama3, Zephyr [[37](#bib.bib37)], Neural Chat, BTLM-DPO
    [[18](#bib.bib18)], etc. DPO’s success has attracted much research attention,
    leading to variants beyond pairwise ranking [[32](#bib.bib32)], and unified perspectives
    on loss parameterization [[2](#bib.bib2), [15](#bib.bib15), [36](#bib.bib36),
    [40](#bib.bib40)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notwithstanding the successes achieved by RLHF and DPO, both are limited by
    the restrictive assumption that the underlying preference follows the Bradley-Terry
    (BT) model [[5](#bib.bib5)]. In particular, the degree of possible agreement or
    disagreement in response to different prompts is not accounted for in the objective
    function. For instance, people are more likely to agree on “$1+1=$.” as opposed
    to “What is the best city to live in the U.S.? // New York.” In the context of
    language models, this concerns the issue of dispersion of the next-token prediction,
    which was also observed in [[9](#bib.bib9), [16](#bib.bib16)]. See Section [5.1](#S5.SS1
    "5.1\. Evidence of preference dispersion ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions") for dispersion analysis of IMDB and Anthropic
    HH datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this paper is to formalize the idea of prompt dispersion in
    the design of DPO. We adapt Mallows’ preference ranking theory [[12](#bib.bib12),
    [24](#bib.bib24)], a family of ranking models that provide a natural carrier for
    prompt dispersion, and propose the following decomposition/factorization of the
    (latent) reward function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mbox{reward}(\mbox{prompt, completion})=\mbox{dispersion}(\mbox{prompt})\times\mbox{scaled
    reward}(\mbox{completion}\mid\mbox{prompt}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where “prompt” and “completion” correspond, respectively, to question and answer.
    This decomposition allows to specify the diverse level of prompt dispersions hidden
    in the DPO, which is translated into a prompt-dependent factor – the dispersion
    index in the preference likelihood. The scaled reward is given by the relative
    rank of the (possible) completions, which further enhances the model interpretability.
    We then leverage the change of variables technique to propose two models, Mallows-$\theta$-DPO.
  prefs: []
  type: TYPE_NORMAL
- en: The main contributions of this paper are as follows. First, we formalize the
    idea of prompt dispersion in DPO, and develop the Mallows-DPO approach to implement
    this idea, so as to improve and generalize DPO. Second, we propose approximations
    to the dispersion index so as to facilitate computation. We also provide various
    analytical results for Mallows-DPO, which lead to various new insights on existing
    DPO models, including a generalized $\Psi$PO model that unifies all DPO models
    (including Mallows-DPO). Finally, our experiments on bandit, IMDB and Anthropic
    HH dataset all show clear advantages of Mallows-DPO over (BT-)DPO.
  prefs: []
  type: TYPE_NORMAL
- en: Other Related Works. Existing literature studying personalization in dialogue
    generation such as [[16](#bib.bib16), [21](#bib.bib21)] have also paid attention
    to the diversity of human preferences (“there are a thousand Hamlets in a thousand
    people’s eyes” [[16](#bib.bib16)]); and [[27](#bib.bib27)] proposes a Nash game
    model to incorporate this diversity. There are also recent works that propose
    learning the online preferences [[8](#bib.bib8), [34](#bib.bib34)] or learning
    from AI feedbacks [[4](#bib.bib4), [10](#bib.bib10), [20](#bib.bib20)]. Studies
    to improve the design and capabilities of RLHF include [[14](#bib.bib14), [19](#bib.bib19),
    [38](#bib.bib38), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the paper is organized as follows. Background materials on
    RLHF and DPO are highlighted in Section [2](#S2 "2\. Preliminaries ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions"). Section [3](#S3 "3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    focuses on the development of Mallows-DPO, followed by more analytical results
    and various perspectives in Section [4](#S4 "4\. Perspectives on Mallows-DPO ‣
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"). Experimental results
    are detailed in Section [5](#S5 "5\. Experiments ‣ Mallows-DPO: Fine-Tune Your
    LLM with Preference Dispersions"), and concluding remarks in Section [6](#S6 "6\.
    Conclusion ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Supervised fine-tuning (SFT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both RLHF and DPO (reviewed below) start with fine-tuning a pre-trained large
    language model by supervised learning on high-quality data for some downstream
    tasks of interest (e.g. dialogue, summarization, etc.), to acquire a model $\pi^{\mathrm{SFT}}$.
    This step is referred to as the SFT phase. For instance, for training InstructGPT
    [[28](#bib.bib28)], GPT-3 [[6](#bib.bib6)] is first fine-tuned on the given input
    prompt distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]. On top of $\pi^{\mathrm{SFT}}$
    to produce pairs of answers (or, “completions”), $\left\{y_{1},y_{2}\right\}\sim\pi(y\mid
    x)$. The preferences are assumed to be generated by some latent reward model $r^{*}(x,y)$
    in which the prompt $x$ is the action.'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Reward model. To capture the underlying human preferences, RLHF assumes
    the Bradley-Terry model [[5](#bib.bib5)] that stipulates the pairwise preference
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma(s):=\frac{1}{1+e^{-s}}$ is the sigmoid function. Given access
    to a static dataset of comparisons
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1,\ldots,N},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'RLHF seeks to approximate the latent reward $r^{*}(x,y)$, and estimate the
    parameters by minimizing the (negative) log-likelihood loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Denote by $r_{\psi_{*}}(x,y)$ the solution to the problem in ([2](#S2.E2 "In
    2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")).'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) RL. The learned reward function $r_{\psi_{*}}(x,y)$ is then used to provide
    feedback to the language model. More precisely, the following KL-regularized RL
    problem is considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(y\mid
    x)}\left[r_{\psi_{*}}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right]$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $$\beta></math>. The regularization is important as it prevents deviating
    too far from the SFT model that is trained to conform to the true preference,
    while maintaining the generation diversity to avoid mode-collapsing to a single
    high-reward answer. In terms of optimization, RLHF leverages RL algorithms due
    to the discrete nature of the language and the associated non-differentiable property
    of the objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In view of ([3](#S2.E3 "In 2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")), RLHF uses the reward function $r(x,y)=r_{\psi}(x,y)-\beta\left(\log\pi(y\mid
    x)-\log\pi_{\text{ref }}(y\mid x)\right)$, and solves the RL problem by proximal
    policy optimization [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. DPO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[30](#bib.bib30)]. One disadvantage of RLHF is that the RL step often requires
    substantial computational effort (e.g., to carry out the proximal policy optimization).
    The idea of DPO is to combine the two steps (a)–(b) in RLHF into a single one,
    bypassing the computation in the RL step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea is that given a reward function $r(x,y)$, the problem in ([3](#S2.E3
    "In 2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")) has a closed-form solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $Z(x)=\sum_{y}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)$
    is a normalizing constant. Rewrite the above as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid x)}+\beta\log
    Z(x).$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Through this change of variables, the latent reward $r^{*}(x,y)$. Substituting
    this $r^{*}$ expression into ([1](#S2.E1 "In 2.2\. RLHF ‣ 2\. Preliminaries ‣
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")) yields:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $Z^{*}(x)$.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The expression in ([6](#S2.E6 "In 2.3\. DPO ‣ 2\. Preliminaries ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")) motivates the DPO objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: which is a supervised learning problem, requiring much less computation than
    the RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DPO based on Mallows Ranking Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Mallows ranking models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For $n\geq 1$. Consider the following preference probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{P}_{\phi,\mu_{0},d}(\mu):=\frac{1}{Z(\phi,d)}\phi^{d\left(\mu,\mu_{0}\right)}\quad\text{
    for }\mu\in\mathfrak{S}_{n},$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi\in(0,1]$ is a discrepancy function that is right invariant: $d(\mu_{1},\mu_{2})=d\left(\mu_{1}\circ\mu_{2}^{-1},id\right)$,
    the distribution ([8](#S3.E8 "In 3.1\. Mallows ranking models ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    is concentrated on $\mu_{0}$ items) with pairwise preferences, Mallows [[24](#bib.bib24)]
    considered two specific cases of the discrepancy function in ([8](#S3.E8 "In 3.1\.
    Mallows ranking models ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mallows-$\theta$ is the Spearman’s rho,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mallows-$\phi$ is the Kendall’s tau,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where $\operatorname{inv}(\mu):=\#\left\{(i,j)\in[n]^{2}:i<j\right.$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general form in ([8](#S3.E8 "In 3.1\. Mallows ranking models ‣ 3\. DPO
    based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")) was suggested by [[12](#bib.bib12)] along with other discrepancy
    functions (e.g. Cayley, Hamming, Ulam distances, etc.) See [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)] for the related group representation approach
    to ranked, or partially ranked data. Note that the Mallows-$\theta$ (see [[26](#bib.bib26),
    [29](#bib.bib29), [35](#bib.bib35)].) In the context of language models, this
    conforms to a possibly infinite number of completions given a prompt, and allows
    interpreting unseen completions conceptually.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Mallows-DPO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we adapt Mallows ranking models highlighted above to the setting of language
    models. First, denote by $\mu(\cdot\mid x)$, such that the preference distribution
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=\mathbb{P}\left(\mu(y_{1}\mid
    x)<\mu\left(y_{2}\mid x\right)\right).$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Next, for the preference probability in ([8](#S3.E8 "In 3.1\. Mallows ranking
    models ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your
    LLM with Preference Dispersions")), given an input prompt $x$ may be computationally
    hard. Similar to RLHF in ([3](#S2.E3 "In 2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")), our goal here is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[-\mu_{0}(y\mid x)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $r^{*}(x,y)$—note that a smaller rank is preferred as per ([9](#S3.E9
    "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions"))—and hence providing a natural
    candidate for the scaled reward that enhances model interpretation. This yields
    the change of variables similar to ([5](#S2.E5 "In 2.3\. DPO ‣ 2\. Preliminaries
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: which “cleverly" avoids estimating $\mu_{0}(\cdot\mid x)$. We then derive the
    two versions of Mallows-DPO.
  prefs: []
  type: TYPE_NORMAL
- en: Mallows-$\theta$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\log\phi(x)\in(-\infty,0)$. The change of variables in ([10](#S3.E10
    "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")) leads to the objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathcal{L}_{\mathrm{MDPO}}\left(\pi;\pi_{\mathrm{ref}}\right):=$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'In comparison with the objective of the BT-DPO (Bradley-Terry based DPO) in
    ([7](#S2.E7 "In 2.3\. DPO ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions")), the objective of Mallows-$\theta$-DPO can be viewed
    as DPO with prompt dispersion. (In contrast, prompt dispersion is not present
    at all in the BT-DPO.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mallows-$\phi$ model, it was shown in [[24](#bib.bib24)] (see also [[7](#bib.bib7),
    [25](#bib.bib25)]):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: or compactly,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'is the link function. Similar to Mallows-$\theta$-DPO:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'In comparison with the BT-DPO in ([7](#S2.E7 "In 2.3\. DPO ‣ 2\. Preliminaries
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")), Mallows-$\phi$.
    Refer to Table [1](#S3.T1 "Table 1 ‣ 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows
    Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for a summary of the key features of the different models.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. Key Features of DPO Models
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Class | Model Name | Dispersion | Link function |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | DPO [[30](#bib.bib30)] | ✗ | sigmoid $\sigma(\cdot)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-DPO | Mallows-$\theta$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$ in ([15](#S3.E15 "In 3.2\. Mallows-DPO ‣ 3\. DPO based on
    Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    |'
  prefs: []
  type: TYPE_TB
- en: 3.3\. Approximate the dispersion index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the dispersion index $\phi(x)$ to the empirical output distribution of the
    pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the preference follows Mallows-$\phi$ model. There are two extreme
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When $-\log(\phi(x))\rightarrow\infty$, we have:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Thus, the probability distribution of the next token will concentrate on a point
    mass.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When $-\log(\phi(x))\rightarrow 0$ so the next token will be uniformly distributed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The above observation motivates us to use Shannon’s entropy, for a discrete
    random variable $X\in\{x_{1},\ldots,x_{n}\}$ when $X$ is uniformly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given constant <math id=$$, we propose:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $-\phi^{*}\log\left(\frac{H(\pi(\cdot\mid x))}{\log(n)}\right),$ |  |
    (17) |'
  prefs: []
  type: TYPE_TB
- en: as a proxy to $-\log\phi(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the hyperparameter $\phi^{*}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(\pi(\cdot\mid x))\approx\frac{1}{2}\sum_{i=1}^{N-1}\left[H(X_{i+1}\mid
    X_{i}=x^{w}_{i})+H(X_{i+1}\mid X_{i}=x^{l}_{i})\right],$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: which can be directly computed by the logits of the model given the output (preference)
    data. This is also closely related to the predictive entropy [[17](#bib.bib17),
    [23](#bib.bib23)] of the next-token predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Unify Mallows-$\theta$ for computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the link function $g_{x}$, with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: For computational purposes, we propose two smooth approximations to $g_{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sigmoid approximation: Since $g_{x}(1)=\frac{1}{1+\phi(x)}$. See Figure [3](#S3.F3
    "Figure 3 ‣ 3.4\. Unify Mallows-𝜃 and Mallows-ϕ for computation ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for an illustration of this approximation. With this approximation, Mallows-$\phi$).
    Thus, Mallows-$\theta$-DPO with sigmoid approximation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Polynomial fitting: We use a polynomial of form $P(x)=a_{3}x^{3}+a_{1}x+a_{0}$
    being a hyperparameter. We choose $\epsilon$ for $\phi(x)=0.5$). See Figures [3](#S3.F3
    "Figure 3 ‣ 3.4\. Unify Mallows-𝜃 and Mallows-ϕ for computation ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[3](#S3.F3
    "Figure 3 ‣ 3.4\. Unify Mallows-𝜃 and Mallows-ϕ for computation ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for an illustration.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4089e75f667b4937a6f747d63ff862b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Sigmoid approximation
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c2da4454b7107de2ad5ea0dacb552f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. Poly-fitting on $\pm\epsilon$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ef9060eb0d2d9f488ca4422be0a6a4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3. Poly-fitting on $\pm 2\log\phi$
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Perspectives on Mallows-DPO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we provide several alternative perspectives on Mallows-DPO
    in ([12](#S3.E12 "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")) and ([16](#S3.E16
    "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")), with the proofs given in Appendix
    A. We say a DPO is directed by $g(\cdot)$ if the preference distribution can be
    expressed as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=g\left(r^{*}(x,y_{1})-r^{*}\left(x,y_{2}\right)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: for some reward function $r^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Dispersion weighted objectives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following result shows that Mallows-$\theta$-DPO can be viewed as a DPO
    with either the reward or the KL-regularizer weighted by the dispersion index.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1  (Mallows-$\theta$-DPO as dispersion weighted DPO).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $c(x)=-2\log\phi(x)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: A similar result holds for Mallows-$\phi$-DPO.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2  (Mallows-$\phi$-DPO as dispersion weighted DPO).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Setting $\phi(x)=e$ in ([15](#S3.E15 "In 3.2\. Mallows-DPO ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    yields'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $g(s):=\frac{1-\operatorname{sgn}(s)}{2}+\operatorname{sgn}(s)\left(\frac{&#124;s&#124;+1}{1-e^{&#124;s&#124;+1}}-\frac{&#124;s&#124;}{1-e^{&#124;s&#124;}}\right).$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'Let $c(x)=-2\log\phi(x)$ or the KL-regularizer weighted by $\beta c(x)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 4.2\. Connection to $\Psi$PO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The objective of $\Psi$PO [[2](#bib.bib2)] is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: where $\Psi:[0,1]\to\mathbb{R}$PO to the Bradley-Terry based DPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'Roughly speaking, the function $\Psi$. Assume such a function exists, which
    we denote as $\Psi^{M}(\cdot)$ model in ([13](#S3.E13 "In 3.2\. Mallows-DPO ‣
    3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with
    Preference Dispersions")), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid x)}\left[\Psi^{M}\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\neq r(x,y_{1})-\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid
    x)}\left[r\left(x,y_{2}\right)\right],$ |  |'
  prefs: []
  type: TYPE_TB
- en: i.e., for any $\Psi^{M}(\cdot)$PO to take into account prompt dispersion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalized $\Psi$. The generalized $\Psi$PO takes the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\tilde{\Psi}\left(x,p^{*}(y\succ
    y^{\prime}\mid x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: 'A special instance is when $\tilde{\Psi}(x,p)=f(x)\Psi(p)$ is separable:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[f(x)\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: Theorem 3  (Mallows-DPO as generalized $\Psi$PO).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: (i) Mallows-$\theta$ and $f(x)=-\frac{1}{2\log\phi(x)}$.
  prefs: []
  type: TYPE_NORMAL
- en: (ii) Mallows-$\phi$ and $f(x)=-\frac{1}{\log\phi(x)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate the capability of our proposed Mallows-DPO to learn
    the preferences in comparison with DPO. First, we use the preferences dataset
    of IMDB [[22](#bib.bib22)] datasets and Anthropic Helpful and Harmless dialogue
    [[3](#bib.bib3)] dataset to provide evidence that human preferences may be diversed.
    Next, we consider a synthetic bandit problem to demonstrate the effectiveness
    of our proposed Mallows-$\phi$-DPO, even without prompt dispersions. We further
    conduct experiments on tasks such as conditional generation (IMDB) and dialogue
    (Anthropic HH). Our findings show that Mallows-DPO outperforms DPO with an evident
    margin, both for in-distribution performance and out-of-distribution generalization
    capability.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Evidence of preference dispersion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A first natural question is: are human preferences dispersed? To verify this
    key motivation for our work, we plot the distribution of the dispersion estimators
    given the SFT model and pairwise preferences. Recall from Section [3](#S3 "3\.
    DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") that the dispersion estimator is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and we take the hyperparameter <math id="S5.SS1.p1.1.m1.1" class="ltx_Math"
    alttext="\phi^{*}></math> such that the empirical mean is equal to 1, so we do
    not need to tune this scaling constant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/892ecba31376f2f4fe90627f7b4cd48d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) IMDB preference dispersion
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/397713bda291ca043c63c48d526dda52.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Anthropic-HH preference dispersion.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4. LEFT. Distribution of our dispersion estimator on IMDB. RIGHT. Anthropic-HH
    perference dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that for the task of conditional generation such as IMDB, the human
    preferences are not quite diverse: the dispersion estimators are located near
    $1$. However, for tasks such as single dialogue, our plot shows that human preferences
    may be dispersed: the distribution is both skewed and of high variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Mallows-$\phi$-DPO mitigates reward collapse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We study Mallows-DPO in a synthetic bandit experiment where there is no contextual
    information $x$. Second, the limited data availability tests the ability of the
    approaches to produce diversified policies and avoid reward collapse.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we consider five arms, each associated with a random reward drawn
    from a probability distribution. Preference between any two picked arms is determined
    by the random reward realizations, with larger reward being preferred. In the
    experiment, we collect $16$, and (2) across different epochs. The details are
    provided in Appendix B.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.2\. Mallows-ϕ-DPO mitigates reward collapse
    ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    displays the efficient frontiers for Mallows-$\phi$ is small. (2) Over all possible
    $\beta$ gets smaller. That is, Mallows-$\phi$-DPO leads to the policies that have
    both high rewards and small KL divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ebd4e666fdc6751cb771f3e560f5d24e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2e0370ae681d81084341acf5b1eec0c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5. Efficient frontiers: reward vs KL. LEFT. generated by measuring KL
    and reward for the policy trained with different $\beta$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68770a2f80f7991cc30bb188271cc9b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. Training curves of Mallows-$\phi$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Mallows-DPO yields better tradeoff between accuracy and regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct the conditional generation for IMDB dataset. In this task, $x$ with
    positive sentiment. Following the setting in [[30](#bib.bib30)], we first fine-tune
    GPT-2-large on the training split of IMDB datasets until convergence to get the
    SFT model. Next, we use the pairwise preference data from [[39](#bib.bib39)] to
    fine-tune the SFT model by DPO and Mallows-DPO.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e109a6c199521d77942962c0c857b67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7. Efficient frontiers: accuracy vs KL achieved by Mallows-DPO and BT-DPO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S5.F7 "Figure 7 ‣ 5.3\. Mallows-DPO yields better tradeoff between
    accuracy and regularization ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") displays the efficient frontiers (during the training
    process) for BT-DPO, Mallows-$\theta$-DPO outperforms both, achieving the same
    accuracy (evaluated by the reward model) at a smaller KL divergence to the SFT
    model/policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.4\. Dispersion matters: Mallows-DPO enhancing both in-distribution and out-of-distribution
    performances'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare the performances of Mallows-DPO and BT-DPO in terms of the win rate
    evaluated by GPT4, and generalization capability on the out-of-distribution datasets.
    In the experiment, we choose $\beta$ value leads to a drop both in performance
    and per-input diversity of RLHF and DPO. Results are shown in Figure [8](#S5.F8
    "Figure 8 ‣ 5.4\. Dispersion matters: Mallows-DPO enhancing both in-distribution
    and out-of-distribution performances ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-distribution test. We first fine-tune a pretrained Pythia-2.8B model on
    the training set of Anthropic HH dataset using Mallows-DPO and BT-DPO, and then
    evaluate the responses on a subset of its test split, generated by these fine-tuned
    models. GPT-4 serves as the evaluator, and compares pairs of responses: one from
    the model fine-tuned with Mallows-$\theta$-DPO have an edge over BT-DPO. In particular,
    Mallows-$\phi$ for $\beta=0.1$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4dc0d4b95f28003f2f295ed466962f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8. Win rates computed by GPT-4 evaluations for responses on both the
    in-distribution dataset (Anthropic HH) and out-of-distribution datasets (H4 Stack
    Exchange and Stanford Human Preferences), generated by the models fine-tuned on
    the Anthropic HH training set.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-distribution test. We apply the models, fine-tuned on the train split
    of the Antropic HH dataset, to other datasets with different input distributions.
    The H4 Stack Exchange Preferences Dataset, collected from the Stack Overflow,
    and Stanford Human Preferences (SHP), containing questions or instructions in
    various subject areas, are used for evaluation. In these out-of-distribution tasks,
    the advantage of dispersion on generalization becomes apparent, as Mallows-$\theta$,
    the performance of Mallows-DPO is consistently above 55%, with Mallows-$\phi$-DPO
    achieving a notable improvement on the H4 Stack Exchange dataset, reaching a win
    rate with more than 60%.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have developed in this paper a novel approach, the Mallows-DPO, to fine-tune
    LLM. A distinct feature of this approach is a dispersion index, which naturally
    captures the dispersion of human preference to prompts, and can be systematically
    incorporated into the reward function as a weight factor, thus ushering in a new
    class of dispersion-weighted DPO models. We demonstrate empirically how Mallows-DPO
    achieves improved performance in a broad array of benchmark tasks, including synthetic
    bandit selection, controllable generation, and dialogues.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few issues that we have yet to address in this study, for instance,
    to explore why Mallows-DPO outperforms BT-DPO, how the dispersion index contributes
    to performance improvement, what guidelines to follow to set the $\beta$-divergence
    [[39](#bib.bib39)]). These will be pursued in our future works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Acknowledgement: Hanyang Zhao and Wenpin Tang are supported by NSF grants DMS-2113779
    and DMS-2206038, and by a start-up grant at Columbia University. The works of
    Haoxian Chen, Hanyang Zhao, Henry Lam and David Yao are part of a Columbia-CityU/HK
    collaborative project that is supported by InnotHK Initiative, The Government
    of the HKSAR and the AIFT Lab. Additionally, Haoxian Chen is supported by the
    Amazon CAIT fellowship. Henry Lam and Wenpin Tang receive support from the Columbia
    Innovation Hub grant.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and Shyamal Anadkat. GPT-4
    technical report. 2023. arXiv:2303.08774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm
    to understand learning from human preferences. In AISTATS, pages 4447–4455, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. 2022.
    arXiv:2204.05862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.
    Constitutional AI: Harmlessness from AI feedback. 2022. arXiv:2212.08073.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block
    designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda
    Askell. Language models are few-shot learners. In Neurips, volume 33, pages 1877–1901,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Róbert Busa-Fekete, Eyke Hüllermeier, and Balázs Szörényi. Preference-based
    rank elicitation using statistical models: The case of Mallows. In ICML, pages
    1071–1079, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang,
    Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko,
    Tianqi Liu, et al. Human alignment of large language models through online preference
    optimisation. arXiv preprint arXiv:2403.08635, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] David M Chan, Yiming Ni, David A Ross, Sudheendra Vijayanarasimhan, Austin
    Myers, and John Canny. Distribution aware metrics for conditional natural language
    generation. 2022. arXiv:2209.07518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play
    fine-tuning converts weak language models to strong language models. 2024. arXiv:2401.01335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Douglas Critchlow. Metric methods for analyzing partially ranked data,
    volume 34. Lecture notes in Statistics, Springer, 1985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Persi Diaconis. Group representations in probability and statistics, volume 11.
    Lecture Notes-Monograph Series, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Persi Diaconis. A generalization of spectral analysis with application
    to ranked data. Ann. Stat., pages 949–979, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm:
    A simulation framework for methods that learn from human feedback. In Neurips,
    volume 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. Human-aware
    loss functions (halos). Technical report, Contextual AI, 2023. https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There
    are a thousand hamlets in a thousand people’s eyes: Enhancing knowledge-grounded
    dialogue with personal memory. 2022. arXiv:2204.02624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani.
    Predictive entropy search for efficient global optimization of black-box functions.
    In NIPS, volume 27, pages 918––926, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, and Iz Beltagy.
    Camels in a changing climate: Enhancing LM adaptation with Tulu 2. 2023. arXiv:2311.10702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina,
    Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects
    of RLHF on LLM generalisation and diversity. arXiv preprint arXiv:2310.06452,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,
    Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement
    learning from human feedback with AI feedback. 2023. arXiv:2309.00267.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan
    Jurafsky. Deep reinforcement learning for dialogue generation. 2016. arXiv:1606.01541.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and
    Christopher Potts. Learning word vectors for sentiment analysis. In ACL, pages
    142–150, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] David JC MacKay. Information-based objective functions for active data
    selection. Neural computation, 4(4):590–604, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Colin L Mallows. Non-null ranking models. I. Biometrika, 44(1/2):114–130,
    1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Cheng Mao and Yihong Wu. Learning mixtures of permutations: groups of
    pairwise comparisons and combinatorial method of moments. Ann. Statist., 50(4):2231–2255,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Marina Meila and Le Bao. An exponential model for infinite rankings. J.
    Mach. Learn. Res., 11:3481–3518, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar,
    Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    and Andrea Michi. Nash learning from human feedback. 2023. arXiv:2312.00886.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, and Alex Ray. Training
    language models to follow instructions with human feedback. In Neurips, volume 35,
    pages 27730–27744, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Jim Pitman and Wenpin Tang. Regenerative random permutations of integers.
    Ann. Probab., 47(3):1378–1416, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model. In Neurips, volume 36, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. 2017. arXiv:1707.06347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li,
    and Houfeng Wang. Preference ranking optimization for human alignment. In AAAI,
    volume 38, pages 18990–18998, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize
    with human feedback. In Neurips, volume 33, pages 3008–3021, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider,
    Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning
    of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Wenpin Tang. Mallows ranking models: maximum likelihood estimate and regeneration.
    In ICML, pages 6125–6134, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi
    Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires,
    and Bilal Piot. Generalized preference optimization: A unified approach to offline
    alignment. 2024. arXiv:2402.05749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif
    Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier,
    and Nathan Habib. Zephyr: Direct distillation of LM alignment. 2023. arXiv:2310.16944.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang,
    Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language
    models part II: Reward modeling. 2024. arXiv:2401.06080.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond
    reverse KL: Generalizing direct preference optimization with diverse divergence
    constraints. 2023. arXiv:2309.16240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin
    Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization:
    Pushing the boundaries of LLM performance in machine translation. 2024. arXiv:2401.08417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee,
    and Yi Ma. Investigating the catastrophic forgetting in multimodal large language
    models. 2023. arXiv:2309.10313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and
    Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. 2023.
    arXiv:2305.10425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
    Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of RLHF in large language
    models part I: PPO. 2023. arXiv:2307.04964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford,
    Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models
    from human preferences. 2019. arXiv:1909.08593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix / Supplemental Material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Proofs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Proof of Theorem 1. For ([20](#S4.E20 "In Theorem 1 (Mallows-𝜃-DPO as dispersion
    weighted DPO). ‣ 4.1\. Dispersion weighted objectives ‣ 4\. Perspectives on Mallows-DPO
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")), the proof follows
    from the derivation of the equivalence between RLHF and DPO, as now the optimal
    policy satisfies'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $c(x)^{-1}r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid
    x)}+\beta\log Z(x),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'leading to the objective in ([12](#S3.E12 "In 3.2\. Mallows-DPO ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")).
    The same argument establishes ([21](#S4.E21 "In Theorem 1 (Mallows-𝜃-DPO as dispersion
    weighted DPO). ‣ 4.1\. Dispersion weighted objectives ‣ 4\. Perspectives on Mallows-DPO
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")). $\blacksquare$'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Theorem 3. (i) With the Bradley-Terry connection as mentioned above,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\Psi\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\left(r(x,y_{1})-r\left(x,y_{2}\right)\right)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which is a weighted reward of DPO, up to an additive constant. It follows that
    the optimal policy of the generalized $\Psi$
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1\. Bandit Experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the bandit experiment detailed in Section [5.2](#S5.SS2 "5.2\. Mallows-ϕ-DPO
    mitigates reward collapse ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions"), we conduct two sub-experiments to compute the efficient
    frontiers using Mallow-$\phi$ values required to compute the full efficient frontier,
    and for each $\beta$ among the four policies for every $100$ training steps. Given
    that we know the real reward distribution, all these quantities can be computed
    analytically.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the training details, we use all 16 data in a single batch and adopts
    SGD as the optimizer, with learning rate of 5e-3. To ensure convergence, we run
    the optimization for a large number of epochs, set to 500,000. For Mallows-$\phi$
    to be 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. Reward distributions of the five arms.
  prefs: []
  type: TYPE_NORMAL
- en: '| Arm 1 | Arm 2 | Arm 3 | Arm 4 | Arm 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Reward | Prob. | Reward | Prob. | Reward | Prob. | Reward | Prob. | Reward
    | Prob. |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.5 | 30 | 0.5 | 18 | 0.5 | 15 | 0.99 | 1 | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.5 | 3 | 0.5 | 15 | 0.5 | 10 | 0.01 | 4 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: Table 3. 16 pairs of sampled preference data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Win | 3 | 2 | 2 | 1 | 3 | 1 | 1 | 1 | 4 | 2 | 2 | 2 | 1 | 3 | 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Lose | 5 | 5 | 5 | 2 | 5 | 5 | 4 | 5 | 5 | 4 | 1 | 5 | 3 | 5 | 4 | 2 |'
  prefs: []
  type: TYPE_TB
- en: B.2\. Controllable Generation Experiment Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We follow the training setup in [[30](#bib.bib30)], and first fine-tune GPT-2-large
    on the training split of IMDB datasets until convergence to get the SFT model.
    The next step is different from [[30](#bib.bib30)] in that we directly utilize
    the (offline) preference dataset from [[39](#bib.bib39)] instead of generating
    pairwise preferences from the trained SFT model, as in DPO. The rest is the same:
    we use the pairwise preference data to fine-tune the SFT model by either DPO or
    Mallows-DPO. The evaluation metric: accuracy is obtained from a prior sentiment
    classifier as the ground truth reward. By default, we use RMSprop optimizer with
    a learning rate of 1e-6, with a linear learning rate warmup from 0 to 1e-6 over
    the first 150 steps. The training batch size is 64.'
  prefs: []
  type: TYPE_NORMAL
- en: B.3\. Language Modeling Experiment Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We follow the training setup in [[30](#bib.bib30)]. By default, we use RMSprop
    optimizer with a learning rate of 1e-6, with a linear learning rate warmup from
    0 to 1e-6 over the first 150 steps. The training batch size is 32.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.1\. GPT-4 Judgement Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Response quality evaluation is completed by GPT-4\. The prompt for instructing
    GPT-4 to evaluate which response is better is particularly important. Thus, we
    use the fastchat package for GPT-4 evaluation, and we used their well-written
    pair-v2 judge prompt. The prompt is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please act as an impartial judge and evaluate the quality of the responses
    provided by two AI assistants to the user question displayed below. You should
    choose the assistant that follows the user’s instructions and answers the user’s
    question better. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their responses.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Avoid any position biases and ensure that the order in which the responses were
    presented does not influence your decision. Do not allow the length of the responses
    to influence your evaluation. Do not favor certain names of the assistants. Be
    as objective as possible. After providing your explanation, output your final
    verdict by strictly following this format: \‘‘ [[A]]\’’ if assistant A is better,
    \‘‘[[B]]\’’ if assistant B is better, and \‘‘[[C]]\’’ for a tie."'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure fairness and unbiasedness, for each pairwise input $(x,y_{1},y_{2})$
    wins if and only if it wins both comparisons, or wins one comparison while the
    other is tied. We compute win rate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Win rate (Model A)}=\frac{\text{Number of samples where Model A
    wins}}{\text{Total number of test samples}}+0.5\times\frac{\text{Number of tied
    samples}}{\text{Total number of test samples}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Qualitative Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present a series of comparisons between Mallows-DPO variants
    and BT-DPO, as shown in Tables [4](#A3.T4 "Table 4 ‣ Appendix C Qualitative Examples
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[15](#A3.T15 "Table
    15 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions"). These tables demonstrate the qualitative examples of responses
    to in-distribution inputs from the Anthropic-HH test set, to out-of-distribution
    inputs from the SHP test set, and to out-of-distribution inputs from the SE dataset
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To interpret, Tables [4](#A3.T4 "Table 4 ‣ Appendix C Qualitative Examples
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[5](#A3.T5 "Table
    5 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") show that models fine-tuned by Mallows-$\theta$-DPO and Mallows-$\phi$-DPO,
    their responses do not deviate too much from the user prompt and remain meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, the examples in Tables [8](#A3.T8 "Table 8 ‣ Appendix C Qualitative
    Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[11](#A3.T11
    "Table 11 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") indicate that models of Mallows-$\theta$-DPO in
    Tables [8](#A3.T8 "Table 8 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")–[9](#A3.T9 "Table 9 ‣ Appendix C Qualitative
    Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions") include
    peer feedback and additional suggestions, like double majoring, regarding a question
    of major transferring. Tables [10](#A3.T10 "Table 10 ‣ Appendix C Qualitative
    Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[11](#A3.T11
    "Table 11 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") show that models fine-tuned by Mallows-$\theta$-DPO
    demonstrate strong knowledge in history and philosophy, by providing more related
    details and supportive arguments, compared to BT-DPO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we also present examples for answering questions regarding coding
    in Tables [12](#A3.T12 "Table 12 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")–[15](#A3.T15 "Table 15 ‣ Appendix
    C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"),
    among which Tables [12](#A3.T12 "Table 12 ‣ Appendix C Qualitative Examples ‣
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[13](#A3.T13 "Table
    13 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") show that Mallows-$\theta$-DPO in Table [15](#A3.T15 "Table 15 ‣
    Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") provides additional code to support the user, complementing the
    suggestions by BT-DPO.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses Mallows-$\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\theta$-DPO | Grandma loves receiving special gifts from family
    members, so an elegant, quality, personalized hand knit sweater is a great idea.
    She loves warm, cozy clothing, so a cashmere sweater is a nice gesture, or a cozy
    pullover knit sweater, for a cozy fall or winter wardrobe. Some other ideas are
    a personalized coffee mug, a knitting patterns, and a hand knit tea cozy. Also
    a small ornament from a special hand knit sweater is a nice gift. If you know
    your grandma’s interests, you can even surprise her by shopping together, so you
    can pick up special gifts for her such as a cookbook for baking, or a knitting
    pattern book, or a coffee mug, or a tea cozy. |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | Grandma loves receiving special gifts from family members, so an
    elegant, quality, personalized hand knit sweater is a great idea. She might enjoy
    a good coffee and a tasty meal, or a book and flowers. If you have a special skill,
    you could knit her a sweater for Christmas or even a sweater for her birthday.
    You can choose a different special occasion, like Valentine’s Day. You could knit
    her a Valentine’s gift like, a Valentine’s ring or bracelet. Or you could make
    a holiday sweater and knit it for Christmas, so she can relax and unwind at the
    end of the year. If she has a lot of friends, you could make her a special hand
    knit sweater and knit it for all her friends to enjoy. If you know her from church
    or play sports, you could knit her a sports themed sweater. Or you could make
    a sweater with a Bible verse on the front, like the sweater Myron did for his
    grandmother. When you make the sweater, you make it all up, so you could even
    make a sweater with a Bible verse on the front, like the sweater that Myron did
    for his grandmother. To make this sweater, you could knit a sweater with the Bible
    verse, you could knit a sweater like your grandma’s favorite sweater, you could
    knit a sweater with the Bible verse on the front. So Myron made his grandmother
    a sweater with the Bible verse. You could make your grandma a sweater with the
    Bible verse. Grandma loves giving special gifts to family and friends, so I recommend
    a sweater with a Bible verse on the front. Or you could make her a sweater with
    your own special gift. Would you like me to help you with this? Let’s see, you
    want a hand knit sweater, knit by a knitting machine, you want only one sweater
    and you want to knit it for your grandma, how about that? Let’s say you want to
    knit up a sweater and hand it to her. Is that ok? Let me make a list of possibilities
    for your sweater, we can pick one or a few that might be special. Here’s a list
    of items that are the best match: Bible verse Glover Shawnee Grandma would like
    to know more about the Bible verse, so let’s get specific. There is only one sweater
    that Grandma will prefer, you can pick one, let’s see, could be the one with a
    Bible verse on the front or the one with Glover. So it’s a hand knit sweater with
    a Bible verse on the front. Here’s a list of specific Bible verses that people
    would like Grandma to learn: “I shall tell my lord the king all the prosperity
    that I have. I shall say, I shall say to you of peace. I shall declare the good
    news to the poor. I shall declare the gospel of grace. Here is a list of specific
    Bible verses that people would like Grandma to learn: Let there be light! Let
    there be light! Let there be light! - - - Let there be light! Let [maximum generation
    length reached] |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Assistant A’s response is more relevant and helpful to the user’s
    statement. The assistant suggests personalized gifts that align with the grandmother’s
    interests, such as a hand knit sweater, a personalized coffee mug, a knitting
    pattern book, and a tea cozy. The assistant also suggests shopping together as
    a way to spend quality time and pick out special gifts. On the other hand, Assistant
    B’s response is repetitive and confusing. It focuses heavily on knitting a sweater
    with a Bible verse, which was not mentioned in the user’s statement. The assistant
    also suggests knitting a sweater for the grandmother’s friends and for people
    from church or sports, which is not relevant to the user’s statement. The assistant’s
    suggestions for Bible verses are also not relevant to the user’s statement. Therefore,
    Assistant A’s response is more helpful, relevant, and accurate. It provides a
    variety of gift ideas that align with the grandmother’s interests and suggests
    spending quality time together. Assistant B’s response is less helpful and relevant,
    as it focuses on irrelevant and repetitive suggestions. |'
  prefs: []
  type: TYPE_TB
- en: Table 5. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses Mallows-$\phi$.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$-DPO | Grandma loves receiving special gifts from family members,
    so an elegant, quality bingo or knitting set is a great idea. She also loves coffee,
    so a nice coffee table book with coffee-themed photos and a small mug is a nice
    gift. Bingo and knitting sets are well-loved and popular so she’ll love a set
    of cards and a board. A photo album is a great idea too, so she can look at pictures
    of her family. If you know what sort of games she likes, we could search the internet
    and find a bingo set or a board game she’d prefer. It also doesn’t hurt to give
    her coffee while you make your gift, so you could leave a cup of hot coffee. |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | Grandma loves receiving special gifts from family members, so an
    elegant, quality, personalized hand knit sweater is a great idea. She might enjoy
    a good coffee and a tasty meal, or a book and flowers. If you have a special skill,
    you could knit her a sweater for Christmas or even a sweater for her birthday.
    You can choose a different special occasion, like Valentine’s Day. You could knit
    her a Valentine’s gift like, a Valentine’s ring or bracelet. Or you could make
    a holiday sweater and knit it for Christmas, so she can relax and unwind at the
    end of the year. If she has a lot of friends, you could make her a special hand
    knit sweater and knit it for all her friends to enjoy. If you know her from church
    or play sports, you could knit her a sports themed sweater. Or you could make
    a sweater with a Bible verse on the front, like the sweater Myron did for his
    grandmother. When you make the sweater, you make it all up, so you could even
    make a sweater with a Bible verse on the front, like the sweater that Myron did
    for his grandmother. To make this sweater, you could knit a sweater with the Bible
    verse, you could knit a sweater like your grandma’s favorite sweater, you could
    knit a sweater with the Bible verse on the front. So Myron made his grandmother
    a sweater with the Bible verse. You could make your grandma a sweater with the
    Bible verse. Grandma loves giving special gifts to family and friends, so I recommend
    a sweater with a Bible verse on the front. Or you could make her a sweater with
    your own special gift. Would you like me to help you with this? Let’s see, you
    want a hand knit sweater, knit by a knitting machine, you want only one sweater
    and you want to knit it for your grandma, how about that? Let’s say you want to
    knit up a sweater and hand it to her. Is that ok? Let me make a list of possibilities
    for your sweater, we can pick one or a few that might be special. Here’s a list
    of items that are the best match: Bible verse Glover Shawnee Grandma would like
    to know more about the Bible verse, so let’s get specific. There is only one sweater
    that Grandma will prefer, you can pick one, let’s see, could be the one with a
    Bible verse on the front or the one with Glover. So it’s a hand knit sweater with
    a Bible verse on the front. Here’s a list of specific Bible verses that people
    would like Grandma to learn: “I shall tell my lord the king all the prosperity
    that I have. I shall say, I shall say to you of peace. I shall declare the good
    news to the poor. I shall declare the gospel of grace. Here is a list of specific
    Bible verses that people would like Grandma to learn: Let there be light! Let
    there be light! Let there be light! - - - Let there be light! Let [maximum generation
    length reached] |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Assistant A’s response is more relevant and helpful to the user’s
    statement. The user mentioned that the person in question likes bingo, knitting,
    shopping, her grandkids, and coffee. Assistant A took all these interests into
    account and suggested appropriate gifts such as a bingo or knitting set, a coffee
    table book, a mug, and a photo album. On the other hand, Assistant B focused solely
    on knitting and suggested making a hand-knit sweater with a Bible verse on it.
    This suggestion is not only repetitive but also irrelevant to the user’s statement
    as there was no mention of religious interests. Furthermore, Assistant B’s response
    is confusing and lacks coherence, making it less helpful to the user. Therefore,
    Assistant A provided a better response. |'
  prefs: []
  type: TYPE_TB
- en: Table 6. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses BT-DPO over Mallows-$\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\theta$-DPO | This is a difficult question! Let me start by saying
    it is true that an elderly person can often enjoy some or all of these things,
    and they might enjoy doing them for many reasons. However, it can sometimes be
    difficult for them to enjoy them for very long, since being able to do them is
    dependent on so many things that can be difficult to take for granted. For example,
    a person can’t get used to the fact that their body is breaking down, or that
    they might not be able to do something at high level for the next few years, so
    they might get bored with some of the things they used to like. However, it is
    still important to have a range of things that they can do and enjoy. If you could
    suggest a Christmas gift that would suit her interests, I could look at some ideas.
    The best gift would have some sort of physical thing that she could use to do,
    like knitting or bingo. Maybe an app would be more like a book that she could
    read? You can help her find games and things like that to play, so it is not just
    about reading? |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | Well, I’d say a good gift might be a subscription to a magazine
    or an online magazine, or an online bingo game or an online knitting community.
    Or you could give her a subscription to a physical bingo card, or some yarn, and
    a pattern for a knitting project. Or you could buy her coffee, and put her name
    on her bingo card, or a knitting project. Or you could give her a book about bingo,
    or bingo, or a knitting project. Or you could just make her a present, like a
    pottery bowl. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Assistant B’s response is more relevant and helpful to the user’s
    question. Although the user’s question is not explicitly stated, it can be inferred
    that the user is looking for gift suggestions for a person who enjoys bingo, knitting,
    shopping, her grandkids, and coffee. Assistant B provides a variety of gift suggestions
    that align with the person’s interests, such as a subscription to an online magazine,
    a physical bingo card, yarn for knitting, coffee, and a book about bingo. On the
    other hand, Assistant A’s response is less focused and does not provide as many
    concrete gift suggestions. Therefore, Assistant B’s response is more helpful and
    relevant. On the other hand, Assistant A’s response is less focused and does not
    provide as many concrete gift suggestions. Therefore, Assistant B’s response is
    more helpful and relevant. |'
  prefs: []
  type: TYPE_TB
- en: Table 7. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses BT-DPO over Mallows-$\phi$.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$-DPO | This is a good list of categories to check when buying
    for an elderly person, but there are many more options. You could check out whether
    she enjoys gardening and flowers, or if she likes baking and cooking. If you have
    a special skill you can try to learn more about her hobby or skills, and make
    her a special item that she can use to improve her skills. For example, are your
    grandma into sports? Does she like to play a musical instrument? If so, you can
    make a special instrument for her, and the instrument can be a tool to help her
    develop her passion for music. Similarly, if she enjoys baking, you could make
    something for her to make, to use in her kitchen. There are all kinds of ways
    you could find out more about her interests, and make them something she’ll enjoy.
    |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | Well, I’d say a good gift might be a subscription to a magazine
    or an online magazine, or an online bingo game or an online knitting community.
    Or you could give her a subscription to a physical bingo card, or some yarn, and
    a pattern for a knitting project. Or you could buy her coffee, and put her name
    on her bingo card, or a knitting project. Or you could give her a book about bingo,
    or bingo, or a knitting project. Or you could just make her a present, like a
    pottery bowl. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Assistant B’s response is more relevant and helpful to the user’s
    question. The user didn’t ask a question, but it seems like they’re looking for
    gift ideas for a woman who likes bingo, knitting, shopping, her grandkids, and
    coffee. Assistant B provides specific gift ideas that align with the woman’s interests,
    such as a subscription to an online bingo game or knitting community, a physical
    bingo card, yarn and a pattern for a knitting project, coffee, or a book about
    bingo or knitting. On the other hand, Assistant A suggests finding out more about
    the woman’s interests, which doesn’t directly address the user’s implied question.
    Therefore, Assistant B’s response is more helpful and relevant. |'
  prefs: []
  type: TYPE_TB
- en: Table 8. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I don’t want to be an engineer, I like physics. Hi there, im currently
    in high school and have some financial problems in the family, my parents are
    not satisfied with me doing physics (i mean get a degree and pursue higher studies
    in) as im also considering doing engineering as it has become a need. But i don’t
    want to leave physics, i love atomic physics and cosmology so much. I want to
    ask here, can i continue my passion for physics while I’m doing engineering (IT)
    , will there be time for my passion. Or else is there any scope to follow my passion
    and take out my family of this? |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\theta$-DPO | There is a way you can pursue your interest in physics
    while continuing to do engineering! Let’s take a step back and understand more
    about what motivates you to study physics, what are you learning that you enjoy,
    and what career would you like to go into? It’s quite possible that you could
    pursue your interest in physics even while working with technologies like computers
    or electronics and electronics, but these fields don’t always allow for the kind
    of hands-on work or the level of interactivity that you are looking for in physics
    and astrophysics. Here’s some feedback from my peers who have made the transition:
    "I initially studied physics in school to be a medical physicist. When I realized
    I only wanted to pursue a career in physics, I worked full-time in the physics
    field and went to graduate school in another field, to focus on physics." "I studied
    Physics A in university, then switched to engineering to do physics B. Now I want
    to come back to physics but take a break to have more time with my kids. I love
    Physics!" "For me, physics is primarily about learning concepts and techniques
    from a particular area. After taking my first physics exam in university, I fell
    in love with the process and started doing physics outside of school, with friends
    and family and working." You can do these things. So maybe don’t let your family
    hold you back from continuing your passion. You could take courses in basic quantum
    mechanics and relativity and cosmology to learn about the universe and why it’s
    the way it is. Just think about what you want to do and how you could combine
    physics with your interests! |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | Yes, you can definitely do both physics and engineering. Many people
    do both. At the end of the day it’s about what you like to do with your life,
    and most people find that their passion and work is what they want to do. It’s
    a great thing, if you’re interested in physics, to be able to pursue it while
    doing something else. I’m always happy to be a resource and answer any specific
    questions you might have if you don’t yet know how to work it out. Here’s some
    things to consider: -If you love physics you should start out with an introductory
    course in physics, this is essential for the study of physics. You might go on
    to physics for a year or two, then try physics and engineering for a later year
    or two. But it is also really important to keep coming back to physics - it’s
    like a food for the mind - so if you can’t make it all the way to advanced physics,
    eventually you should aim to do things in the field of physics or engineering
    that you are interested in. -The second thing to consider is that this does take
    up a lot of time - and a lot of money - but we should expect that. I wish I could
    give you some free advice here but I am not a full-time professor and there are
    lots of people with the same questions. You might try asking around at your school
    and perhaps someone can help you find a mentor who can give you a little more
    advice. Just keep in mind that the time you devote to a certain thing - for many
    young people - is mostly because of the pressure of other people and their expectations
    around them. So if there is the feeling that you aren [maximum generation length
    reached] |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Both Assistant A and Assistant B provide helpful and encouraging
    responses to the user’s question. They both suggest that it is possible to pursue
    a passion for physics while studying engineering. However, Assistant A’s response
    is more detailed and provides more practical advice. It includes feedback from
    people who have made the transition from physics to engineering, which could be
    very helpful for the user. Assistant B’s response, while supportive, is less specific
    and ends abruptly, leaving the user without a complete answer. Therefore, Assistant
    A’s response is more helpful and complete. |'
  prefs: []
  type: TYPE_TB
- en: Table 9. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I don’t want to be an engineer, I like physics. Hi there, im currently
    in high school and have some financial problems in the family, my parents are
    not satisfied with me doing physics (i mean get a degree and pursue higher studies
    in) as im also considering doing engineering as it has become a need. But i don’t
    want to leave physics, i love atomic physics and cosmology so much. I want to
    ask here, can i continue my passion for physics while I’m doing engineering (IT)
    , will there be time for my passion. Or else is there any scope to follow my passion
    and take out my family of this? |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$-DPO | Yes, you can definitely do both physics and engineering.
    Many people do both. At the end of the day, physics is about the mind-brain interaction
    of the fundamental particles in the universe, whereas engineering is about the
    world-building of technologies and products. So it’s possible to pursue both!
    If you enjoy physics, you may be able to learn about and participate in research
    in engineering, and vice versa. Or, you can teach yourself engineering by reading
    technical papers and following online tutorials and then pick a project to work
    on. Most engineering schools have classes or tutorials for self-study! So that’s
    the way it’s usually done. It’s a choice of whether you like physics, or of whether
    you go into engineering! Hope that helps! You could pursue a "dual degree", where
    you have some physics courses, and also some engineering course work, as an option.
    The physics courses could be in an engineering lab, or physics courses before
    your engineering classes. The more common thing is to just take a class in physics
    before taking an engineering class, either in another department at your university,
    or via a separate institution. And that might be a good place to take a class.
    They might also have online physics courses, or they might make it a requirement
    to take a physics course for an engineering degree. Hope that helps. |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | Yes, you can definitely do both physics and engineering. Many people
    do both. At the end of the day it’s about what you like to do with your life,
    and most people find that their passion and work is what they want to do. It’s
    a great thing, if you’re interested in physics, to be able to pursue it while
    doing something else. I’m always happy to be a resource and answer any specific
    questions you might have if you don’t yet know how to work it out. Here’s some
    things to consider: -If you love physics you should start out with an introductory
    course in physics, this is essential for the study of physics. You might go on
    to physics for a year or two, then try physics and engineering for a later year
    or two. But it is also really important to keep coming back to physics - it’s
    like a food for the mind - so if you can’t make it all the way to advanced physics,
    eventually you should aim to do things in the field of physics or engineering
    that you are interested in. -The second thing to consider is that this does take
    up a lot of time - and a lot of money - but we should expect that. I wish I could
    give you some free advice here but I am not a full-time professor and there are
    lots of people with the same questions. You might try asking around at your school
    and perhaps someone can help you find a mentor who can give you a little more
    advice. Just keep in mind that the time you devote to a certain thing - for many
    young people - is mostly because of the pressure of other people and their expectations
    around them. So if there is the feeling that you aren [maximum generation length
    reached] |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Both Assistant A and Assistant B provide helpful and encouraging
    responses to the user’s question. They both affirm that it’s possible to pursue
    both physics and engineering, and they both suggest taking introductory physics
    courses. However, Assistant A goes a step further by suggesting the possibility
    of a “dual degree” and mentioning that some engineering schools offer self-study
    classes or tutorials. Assistant B, on the other hand, provides more emotional
    support and acknowledges the time and financial constraints the user might face.
    Assistant B also suggests seeking a mentor for further advice. Both responses
    are valuable, but Assistant A’s response is more detailed and practical, providing
    more concrete steps the user can take to pursue both interests. Therefore, Assistant
    A’s response is better. |'
  prefs: []
  type: TYPE_TB
- en: Table 10. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Are there any examples of famous philosophers grossly misinterpreting/misunderstanding
    other philosophers? I haven’t read it myself, but I’ve heard that Bertrand Russell’s
    characterisation of Hegel’s philosophy in his *History of Western Philosophy*
    is quite bad. I was wondering if there are any other examples of philosophers
    misunderstanding other philosophers in a big way. |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\theta$-DPO | According to the book *What Is Philosophy?: A Historical
    Survey* by George Murdock, Bertrand Russell actually read and criticized Hegel’s
    philosophy in his own writings in the early 20th century, but he didn’t have a
    clear enough idea about the history of philosophy to understand Hegel. At the
    time of writing the book, Russell believed that Hegel’s philosophy was a philosophy
    of religion, and that he denied freedom and happiness because of this. However,
    Hegel’s own ideas were quite incompatible with certain parts of Christianity.
    Russell also had problems understanding Kant, and he thought Kant gave a simplified
    view of the history of philosophy. He took Kant at face value, when in reality
    Kant himself was very critical of Hegel, describing him as an atheist. The book
    does also have an extensive list of references which Russell could have checked
    to be sure that he wasn’t making a mistake himself, and I think the biggest mistake
    he made was to take Hegel’s ideas at face value when he thought they were religious.
    As far as other philosophers having been misunderstood, Plato and Kant are the
    most famous examples of this in the Western tradition, because they were so influential.
    The problem with both of these was that they weren’t just philosophers but also
    scholars, who studied other philosophers’ ideas and came up with their own versions
    of them. But philosophers weren’t able to interpret ideas of other philosophers
    until the 20th century, because people had not thought about philosophy like they
    do now, but rather as a practical skill where the only real goals were to understand
    the world more and to make people behave better. There are many examples of famous
    philosophers, like Socrates, Hobbes, and Descartes, saying things that other philosophers
    said. These statements are usually based on the historical context of the times,
    so they’re not false statements. Even people who are aware of this situation,
    and have not given up their prejudices, will still form incorrect assumptions
    about philosophers. But that’s not what philosophers do. They don’t need to prove
    all their points in order to prove them, which is what people who study history
    do, so they are usually much more accurate in their thinking. |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | According to Wikipedia, as of 2012 Bertrand Russell was accused
    of “misrepresenting the views of Hegel and Marx”. It seems like Bertrand Russell’s
    understanding of Hegel was not entirely accurate, but rather a caricature. Since
    the book is a history of philosophy, I was wondering if you’re interested in the
    idea of “bad philosophy” in which a philosopher is unfairly caricatured by others?
    But Russell’s book is widely regarded as having historical importance, and for
    the most part I don’t think his interpretation of Hegel is considered to be controversial.
    He does have some critics. In addition, his history of Western philosophy was
    enormously influential at the time, and much has changed since then. But I do
    think there is value to his book, and I think Russell and Hegel are not unfairly
    caricatured by each other. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Assistant A provides a more detailed response, explaining how
    Bertrand Russell misunderstood Hegel’s philosophy and also mentioning that Russell
    had problems understanding Kant. Assistant A also provides examples of other philosophers
    who have been misunderstood, such as Plato and Kant, and explains why this misunderstanding
    often occurs. On the other hand, Assistant B’s response is less detailed and does
    not provide any examples of other philosophers who have been misunderstood. Assistant
    B also seems to contradict itself by stating that Russell’s interpretation of
    Hegel is not considered controversial, but then mentioning that Russell does have
    critics. Therefore, Assistant A’s response is more helpful, relevant, accurate,
    and detailed. |'
  prefs: []
  type: TYPE_TB
- en: Table 11. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Are there any examples of famous philosophers grossly misinterpreting/misunderstanding
    other philosophers? I haven’t read it myself, but I’ve heard that Bertrand Russell’s
    characterisation of Hegel’s philosophy in his *History of Western Philosophy*
    is quite bad. I was wondering if there are any other examples of philosophers
    misunderstanding other philosophers in a big way. |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$-DPO | According to the book *Bertrand Russell on Philosophy:
    The Philosopher’s Role in Life*, Russell had a major problem with the German philosopher
    Hegel’s understanding of social and political order, because at the time Russell
    was growing up Hegel’s writings appealed to him for insights about human nature,
    society, and politics. Hegel believed that human consciousness developed toward
    a certain ultimate goal, and that this ideal was a “goal” and a “principle” that
    humans could strive to reach. Russell thought Hegel was wrong because he believed
    that humanity’s final goal would be one based on some kind of scientific understanding.
    In addition, Russell thinks that Hegel’s philosophy isn’t compatible with human
    experience, and that it’s kind of a “dream world” kind of philosophy, and that
    it’s too abstract for people to be able to understand it. He thought that Hegel’s
    philosophy wasn’t a very useful perspective in the world, even though he was very
    big in Europe at the time. His philosophy is called “theistic idealism” in philosophy.
    Here are some points about the Hegel’s system of philosophy from Russell: - Hegel
    believed in the “dialectical unity of nature and history,” meaning that the universe
    consists of two parts: One which is called nature, and the other which is called
    spirit. These terms describe each other. - Hegel believed that human reality is
    a kind of “moral drama” in which human consciousness has a goal, and where the
    only real “moral actors” are human beings, and people are real actors who can
    make choices and make a difference. People can choose how to respond to social
    interactions, and people can choose how to respond to their own lives. But since
    humans have free will, their values and their moral choices are not necessarily
    predictable. And humans are not a deterministic system, since humans have free
    will, and humans can make moral choices and make a difference in the world. -
    The Hegelian version of the dialectical process is this: In “nature” there are
    two distinct concepts: the concept of “consciousness” and the concept of “matter,”
    that are both independent and distinct. But then “consciousness” and “matter”
    are in a “state of development” called “spirit,” and when the development in “spirit”
    is complete, then the dialectical relationship is complete. Human reality is basically
    this kind of two system-like dialectical relationship. - Hegel believed in a kind
    of “synthesis of opposites”: the concept of “matter” is really not “matter” by
    itself, since the “world” is really a thing, and “nature” is not “primitive nature”
    in itself, but a “self-developing whole” that is “spirit” in a special sense.
    When “spirit” develops fully, then the “whole” becomes “pure spirit”. - Russell
    thought this “synthesis of opposites” Hegelian system is not a “scientific” system,
    since humanity cannot have a “scientific” understanding of how the “world” is,
    and the idea that they could reach an ultimate goal is not scientifically possible.
    |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | According to Wikipedia, as of 2012 Bertrand Russell was accused
    of “misrepresenting the views of Hegel and Marx”. It seems like Bertrand Russell’s
    understanding of Hegel was not entirely accurate, but rather a caricature. Since
    the book is a history of philosophy, I was wondering if you’re interested in the
    idea of “bad philosophy” in which a philosopher is unfairly caricatured by others?
    But Russell’s book is widely regarded as having historical importance, and for
    the most part I don’t think his interpretation of Hegel is considered to be controversial.
    He does have some critics. In addition, his history of Western philosophy was
    enormously influential at the time, and much has changed since then. But I do
    think there is value to his book, and I think Russell and Hegel are not unfairly
    caricatured by each other. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Assistant A provides a detailed explanation of Bertrand Russell’s
    interpretation of Hegel’s philosophy, highlighting the points of disagreement
    and misunderstanding. This response is not only relevant to the user’s question
    but also provides a comprehensive understanding of the topic. On the other hand,
    Assistant B’s response is less detailed and does not provide any other examples
    of philosophers misunderstanding other philosophers, which was the user’s main
    question. Therefore, Assistant A’s response is more helpful and informative. |'
  prefs: []
  type: TYPE_TB
- en: Table 12. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I’m attempting to get a (in my opinion) very simple scenario working
    with Google Tag Manager and Analytics, and running into problems. Now we would
    like to track events using custom JavaScript, via the . For example: <code>$(’a’).click(function()
    dataLayer.push(’event’: ’linkclickedtest’); ); </code> This just isn’t working,
    at all. If I watch Chrome’s console/network tabs, no data seems to get sent to
    Google. I’ve also installed for Chrome and it doesn’t show the event firing either.
    I must be missing something simple. Any guidance would be appreciated. Thanks!
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\theta$-DPO | There are a few possibilities here: - You are defining
    the event before the link has even been added to the DOM, so your code will never
    run. - The code you are running also won’t run in that scenario because the event
    has not yet been added, so the code you have won’t get executed. - You have to
    wait for the JavaScript to run before the code can be evaluated, so the first
    part of the code above is going to run after the code and the link you’ve just
    added to the DOM, but before the event has been loaded. If you change the code
    in your question to be more along these lines, it’ll work: <script> // Wait for
    the JavaScript to load, before executing the code below. setTimeout(function()
    document.querySelector(’a’).onclick = function() dataLayer.push(’event’: ’linkclickedtest’);
    , 0); |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | There are a few possibilities here: You’re using the latest version
    of Google Tag Manager, and you haven’t configured it correctly. You’re using custom
    JavaScript in a page other than the one being tracked. You’re using analytics
    tags in other parts of your code, which are interfering with your tracking code.
    For instance, if you were using <script> tags at the start of <body>, those would
    interfere with Google Tag Manager script. To resolve issue 1 here, you’ll need
    to update the version of Google Tag Manager you’re using. In the Google Tag Manager
    Settings page, click "Download & Install" to download and install the latest version
    of the Google Tag Manager. You’ll also need to make sure it’s configured correctly
    in your environment, and also set it up correctly in the header of the page you’re
    tracking. If you’re using a different version of Google Tag Manager than the one
    you downloaded, you’ll need to install Google Tag Manager version 1.5 or higher.
    For example |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Both assistants provide potential reasons why the user’s code
    might not be working and offer solutions. However, Assistant A’s response is more
    relevant and directly addresses the user’s question. Assistant A suggests that
    the user’s event might be defined before the link has been added to the DOM, and
    provides a code snippet to resolve this issue. On the other hand, Assistant B’s
    response is less specific and doesn’t provide a direct solution to the user’s
    problem. Therefore, Assistant A’s response is more helpful and accurate. |'
  prefs: []
  type: TYPE_TB
- en: Table 13. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I’m attempting to get a (in my opinion) very simple scenario working
    with Google Tag Manager and Analytics, and running into problems. Now we would
    like to track events using custom JavaScript, via the . For example: <code>$(’a’).click(function()
    dataLayer.push(’event’: ’linkclickedtest’); ); </code> This just isn’t working,
    at all. If I watch Chrome’s console/network tabs, no data seems to get sent to
    Google. I’ve also installed for Chrome and it doesn’t show the event firing either.
    I must be missing something simple. Any guidance would be appreciated. Thanks!
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$-DPO | There are a few possibilities here: - You are defining
    the event before the link has even been added to the DOM, so your code will never
    run. Also, check whether your code is running at all or not, that is, whether
    you see any result in the Chrome console. - Your selector is incorrect in the
    code you shared here. - The code is not firing at all on the click event of the
    link because analytics script is not being run at all in the browser. If this
    is the case, you could verify that a click event is actually created from the
    element before trying to push the dataLayer. Have you checked these things? If
    you find out why the code runs on your computer but not on the computer of the
    person you are testing on, can you test it on their computer and tell us what
    happens? If you are just trying to make sure your code works from the example
    you gave above, I’m afraid I’m not sure what other details to go over. I’m |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | There are a few possibilities here: You’re using the latest version
    of Google Tag Manager, and you haven’t configured it correctly. You’re using custom
    JavaScript in a page other than the one being tracked. You’re using analytics
    tags in other parts of your code, which are interfering with your tracking code.
    For instance, if you were using <script> tags at the start of <body>, those would
    interfere with Google Tag Manager script. To resolve issue 1 here, you’ll need
    to update the version of Google Tag Manager you’re using. In the Google Tag Manager
    Settings page, click "Download & Install" to download and install the latest version
    of the Google Tag Manager. You’ll also need to make sure it’s configured correctly
    in your environment, and also set it up correctly in the header of the page you’re
    tracking. If you’re using a different version of Google Tag Manager than the one
    you downloaded, you’ll need to install Google Tag Manager version 1.5 or higher.
    For example |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Both assistants provide useful suggestions for troubleshooting
    the user’s issue with Google Tag Manager and Analytics. However, Assistant A’s
    response is more directly related to the user’s specific code example and the
    issues that might be causing it not to work, such as the timing of the event definition,
    the accuracy of the selector, and the firing of the analytics script. On the other
    hand, Assistant B’s response is more general and focuses on potential issues with
    the Google Tag Manager version and configuration, which may not be directly related
    to the user’s problem. Therefore, Assistant A’s response is more helpful and relevant
    to the user’s question. |'
  prefs: []
  type: TYPE_TB
- en: Table 14. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses BT-DPO over Mallows-$\theta$ on Anthropic-HH
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I have an Ajax function which takes some time to respond, due to
    large file. How do I interrupt the response function if I want to stop the Ajax
    request? <code>$("#startAjaxButton").click(function(){ $.ajax({ url:"ajaxRequest.txt",
    success:function(result){ $("#div1").html(result); } }); }); </code> |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\theta$-DPO | That code should pause the Ajax request until you
    click the “Start” button. Another approach is to use “setTimeout” to have JavaScript
    keep the Ajax request active until it’s finished. setTimeout($.ajax(..), 5000);
    Note: If you want to continue on executing the Ajax request after you’ve stopped
    the request, you’ll need to “un-set-timeout” the request. “setTimeout(function(),
    0)” will un-set the timeout and start the Ajax request again. |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | That code should pause the Ajax request until you click the “Start”
    button. Another approach is to use “setTimeout” to have JavaScript keep the Ajax
    request active until it’s finished. setTimeout($.ajax(..), 5000); Note: If you
    want to interrupt a long request for which you have not waited, I would recommend
    using the “setTimeout” function to run the Ajax before the long request process
    has finished. To interrupt the request, use the “clearTimeout” function. As an
    alternative, you can also just use JavaScript’s default wait loop (“setTimeout”
    and “clearTimeout”) without having to wait for a specific event to trigger the
    function. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Both Assistant A and Assistant B provided similar responses initially,
    suggesting the use of "setTimeout" to keep the Ajax request active until it’s
    finished. However, Assistant B went a step further by explaining how to interrupt
    a long request using the "clearTimeout" function. This directly addresses the
    user’s question about how to interrupt the response function if they want to stop
    the Ajax request. Therefore, Assistant B’s response is more helpful and relevant
    to the user’s question. |'
  prefs: []
  type: TYPE_TB
- en: Table 15. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I have an Ajax function which takes some time to respond, due to
    large file. How do I interrupt the response function if I want to stop the Ajax
    request? <code>$("#startAjaxButton").click(function(){ $.ajax({ url:"ajaxRequest.txt",
    success:function(result){ $("#div1").html(result); } }); }); </code> |'
  prefs: []
  type: TYPE_TB
- en: '| Mallows-$\phi$-DPO | That code should pause the Ajax request until you click
    the “Start” button. Another approach is to use “setTimeout” to have JavaScript
    keep the Ajax request active until it’s finished. setTimeout($.ajax(..), 5000);
    Note: If you want to interrupt a long request for which you have not waited a
    long time, you may need to perform a "clearTimeout" on your variable holding the
    time to wait, because the code in the function will not execute until after that
    time has passed. You can do this using: clearTimeout($("#id").data("timeOut"));
    //remove the timeout reference from your element $("#id").data("timeOut", null);
    //and clear all timeout references from your element |'
  prefs: []
  type: TYPE_TB
- en: '| BT-DPO | That code should pause the Ajax request until you click the “Start”
    button. Another approach is to use “setTimeout” to have JavaScript keep the Ajax
    request active until it’s finished. setTimeout($.ajax(..), 5000); Note: If you
    want to interrupt a long request for which you have not waited, I would recommend
    using the “setTimeout” function to run the Ajax before the long request process
    has finished. To interrupt the request, use the “clearTimeout” function. As an
    alternative, you can also just use JavaScript’s default wait loop (“setTimeout”
    and “clearTimeout”) without having to wait for a specific event to trigger the
    function. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgment | Both Assistant A and Assistant B provided similar responses, suggesting
    the use of “setTimeout” to keep the Ajax request active until it’s finished. However,
    Assistant A’s response is more accurate and detailed. Assistant A correctly explains
    that to interrupt a long request, you need to perform a “clearTimeout” on your
    variable holding the time to wait. Assistant A also provides the code to do this.
    On the other hand, Assistant B’s explanation of using “setTimeout” to run the
    Ajax before the long request process has finished is not clear and could be misleading.
    Therefore, Assistant A’s response is more helpful and accurate. |'
  prefs: []
  type: TYPE_TB
