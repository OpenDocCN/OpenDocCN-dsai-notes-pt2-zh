- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:35:19'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14088](https://ar5iv.labs.arxiv.org/html/2407.14088)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Joy Mahapatra joymahapatra90@gmail.com Indian Statistical Institute Kolkata
    Utpal Garain utpal@isical.ac.in Indian Statistical Institute Kolkata
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Data-to-text (D2T) generation aims to generate human-readable text from semi-structured
    data, such as tables and graphs. The recent success of D2T is largely attributed
    to advancements in LLMs. Despite the success of LLMs, no research has been conducted
    to illustrate the impact of model size on the performance of fine-tuned LLMs for
    D2T tasks. D2T model performance is typically assessed based on three key qualities:
    readability (indicates fluency and coherence), informativeness (measures content
    similarity), and faithfulness (assesses consistency of factual information). It
    is currently uncertain whether increasing the size of LLMs effectively improves
    performance in D2T tasks across these three qualities. The objective of this study
    is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of
    model size. Through extensive comparative analysis, we aim to elucidate both the
    advantages and limitations of scaling model sizes across five widely used D2T
    datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art
    LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM,
    and Llama 2). To comprehensively cover all the three essential qualities of D2T
    models, we incorporate six widely recognized automatic metrics—BLEU, METEOR, BERTScore,
    MoverScore, Parent, and BARTScore. We also provide an in-depth analysis of LLM
    performance concerning model size in the presence of source-reference divergence,
    a critical aspect of D2T tasks. Our investigation reveals that increasing LLM
    size enhances readability and informativeness in D2T tasks, but larger (in terms
    of size) LLMs may sacrifice faithfulness. Moreover, small-sized LLMs show more
    resilience than larger ones when source-reference divergence is present.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since their inception, large language models (LLMs) have emerged as dominant
    models in text generations, surpassing their predecessor models in terms of generality
    and performance (Raffel et al., [2020](#bib.bib88); Brown et al., [2020](#bib.bib11);
    Chowdhery et al., [2023](#bib.bib15); Touvron et al., [2023](#bib.bib103)). Current
    LLMs have demonstrated exceptional performance in a multitude of text generation
    tasks, including but not limited to machine translation, automatic summarization,
    and text simplification (Lewis et al., [2020](#bib.bib51); Zhang et al., [2022](#bib.bib121);
    Taori et al., [2023](#bib.bib100)). The remarkable effectiveness of LLMs in these
    tasks has led to a clear trend of increasing their sizes (numbers of model parameters)
    in existing LLMs to further enhance performance (Petroni et al., [2019](#bib.bib83);
    Roberts et al., [2020](#bib.bib91)). Consequently, there has been an emergence
    of numerous new LLM families (Touvron et al., [2023](#bib.bib103); Chowdhery et al.,
    [2023](#bib.bib15); Scao et al., [2022](#bib.bib92)) characterized by substantially
    larger sizes, which emphasizes the ongoing progress within the field of text generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data-to-text (D2T) generation (Lin et al., [2024](#bib.bib56)), an essential
    facet of text generation, where goal is to produce human-readable text from semi-structured
    data sources, including slot-value paired meaning representations (MR) (Novikova
    et al., [2017](#bib.bib75)), tables (Bao et al., [2018](#bib.bib5)), or graphs (Nan
    et al., [2021](#bib.bib70)). Based on the source data types, three major types
    of D2T tasks are shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"). D2T has diverse applications across various domains, spanning
    dialogue generation (Wen et al., [2015](#bib.bib110)), sports reporting (Wiseman
    et al., [2017](#bib.bib111)), weather forecasting (Belz, [2008](#bib.bib6)), business
    intelligence (Gatt and Krahmer, [2018](#bib.bib27)), and many more. In critical
    fields like healthcare, D2T plays a vital role in automatic diagnostic reporting (Hommes
    et al., [2019](#bib.bib36); Yermakov et al., [2021](#bib.bib117)), underscoring
    its relevance in critical decision-making. The recent success of D2T tasks can
    be largely attributed to the advancement of LLMs, which are leveraged through
    several effective inference and training methods such as parameter-efficient fine-tuning (Li
    and Liang, [2021](#bib.bib55); Lester et al., [2021](#bib.bib50); Dettmers et al.,
    [2023](#bib.bib16)). The synergy between LLMs and D2T generation marks a significant
    advancement (Li et al., [2024b](#bib.bib53); Jing et al., [2024](#bib.bib41);
    Kasner and Dusek, [2024](#bib.bib45)) across various domains and critical safety
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da8ffed2591c5bfa953ff83c68b7e038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of data-to-text (D2T) generation with three major types:
    graph-to-text (left), table-to-text (middle), MR (meaning representation)-to-text
    (right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite advancements in LLMs for D2T tasks, there exists a siggnificant gap
    in analyzing the impact of model size on the performance of fine-tuned LLMs as
    D2T models. The assessment of D2T models commonly revolves around three fundamental
    performance qualities: readability, informativeness, and faithfulness (see Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")). Readability (Chen
    et al., [2020](#bib.bib14); Li et al., [2022](#bib.bib54)) mainly concerns the
    fluency and coherence of generated text from the model, whereas informativeness (Shen
    et al., [2019](#bib.bib96); Li et al., [2022](#bib.bib54)) determines whether
    the D2T model is effective in terms of transforming essential information from
    given data to generated text. Faithfulness (Tian et al., [2019](#bib.bib102);
    Wang et al., [2020](#bib.bib109)) is an important performance indicator for D2T,
    assessing whether the generated text presents any incorrect or irrelevant facts
    w.r.t. the given source data. Considering the huge potential of LLMs in various
    critical D2T domains (Hommes et al., [2019](#bib.bib36); Pauws et al., [2019](#bib.bib79)),
    it is crucial to understand the impact of model size over fine-tuned LLMs in terms
    of D2T performance. However, almost no existing literature has investigated the
    impact of model size on the performance of fine-tuned LLMs across all three performance
    qualities of D2T.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4fe95f2853d0693e9c4e736312e943c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Three key qualities to assess the performance of a D2T model are:
    readability (focusing on fluency and coherence), informativeness (evaluating the
    ability to generate essential content), and faithfulness (indicating the consistency
    of the generated text by measuring the presence of irrelevant facts).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper aims to address the research gap by providing a comparative analysis
    to demonstrate the impact of model size (number of parameters) on the performance
    of fine-tuned LLMs for D2T tasks. We conduct the comprehensive performance comparison
    of twelve LLMs with varying sizes, drawn from five widely used LLM families: BART (Lewis
    et al., [2020](#bib.bib51)), T5 (Raffel et al., [2020](#bib.bib88)), OPT (Zhang
    et al., [2022](#bib.bib121)), BLOOM (Scao et al., [2022](#bib.bib92)), and Llama
    2 (Touvron et al., [2023](#bib.bib103)). We measure the performances of LLMs on
    D2T tasks across three key qualities of D2T model—readability, informativeness,
    and faithfulness. To cover a broad spectrum, we include three primary types of
    D2T tasks—table-to-text, graph-to-text, and MR-to-text. All of these D2T are depicted
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation").
    We incorporate five state-of-the-art datasets in our experiments: E2E (Novikova
    et al., [2017](#bib.bib75)) (MR-to-text), ViGGo (Juraska et al., [2019](#bib.bib43))
    (MR-to-text), WikiTableText (Bao et al., [2018](#bib.bib5)) (table-to-text), DART (Nan
    et al., [2021](#bib.bib70)) (graph-to-text), and WebNLG (Gardent et al., [2017](#bib.bib26))
    (graph-to-text). Through comparative analyses, we aim to investigate both the
    advantages and limitations of scaling up the parameters in LLMs (or size of LLMs)
    within the domain of D2T. Additionally, we provide insights into the performance
    of LLMs in terms of their size with considering the factual/information divergence
    between the source and reference data—known as source-reference divergence. Through
    considering source-reference divergence, we aim to assess LLMs’ generalization
    capability in D2T tasks and evaluate if increasing LLM size contributes to quality
    generation when references diverge from the source data in D2T.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The organization of the remainder of the paper is as follows. Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation") outlines
    the research questions and motivations behind this paper. Prevalent related work
    concerning D2T and application of LLMs is presented in Section [3](#S3 "3 Related
    Work ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation"). Section [4](#S4 "4 Preliminaries ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") offers a concise overview of conditional text generation, (large)
    language models, D2T, and source-reference divergence. All details about the models,
    datasets, and experimental settings are provided in Section [5](#S5 "5 Models,
    Datasets and Experimental Settings ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation"). In Section [6](#S6
    "6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation"), we
    present a comparative performance evaluation of LLMs with different model sizes
    for D2T tasks. Section [7](#S7 "7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") analyzes the impact of model size on LLM performance
    in the presence of source-reference divergence in D2T tasks. Section [8](#S8 "8
    Case Studies ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") includes case studies to illustrate
    the outcomes of LLMs more clearly. Finally, we conclude our findings in Section [9](#S9
    "9 Conclusion ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Research Questions and Motivations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper aims to empirically address the following research questions regarding
    the performance of fine-tuned LLMs for D2T, focusing impact of model sizes (i.e.,
    parameter counts) across the three key qualities of readability, informativeness,
    and faithfulness.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the impacts of model size within a family of fine-tuned LLMs on the
    performance of data-to-text (D2T) tasks, in terms of the readability, informativeness,
    and faithfulness?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do larger LLM families (such as OPT, BLOOM, Llama 2, etc.) convincingly outperform
    smaller LLM families (such as BART, T5, etc.) in terms of D2T task performance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the presence of source-reference divergence influence the performance of
    LLMs for D2T tasks? If so, does increasing the model size of LLM aid in mitigating
    the effects of source-reference divergence?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The motivation behind our objectives stems from several key considerations.
    Firstly, the recent trend of increasing number of parameters in LLMs comes with
    significant computational costs and longer inference times (Zheng et al., [2023](#bib.bib125);
    Li et al., [2024a](#bib.bib52); Jiang et al., [2024](#bib.bib40)). If this escalation
    in model size does not yield substantial improvements (Aghajanyan et al., [2021](#bib.bib1);
    Tulchinskii et al., [2023](#bib.bib104)) in D2T, it raises questions about the
    necessity of bearing such computational burdens (Luccioni et al., [2023](#bib.bib64);
    Faiz et al., [2023](#bib.bib22)). Secondly, despite the widespread use of LLMs,
    their performance in D2T tasks has not been comprehensively explored. This research
    gap hinders the effective application of LLMs in D2T scenarios. Lastly, LLMs are
    known for their generalization capabilities (Ge et al., [2023](#bib.bib28); Zhao
    et al., [2024](#bib.bib123); Yang et al., [2024](#bib.bib115)), as evidenced by
    their success in various generative text tasks. Therefore, it is pertinent to
    investigate whether this generalization extends to enhancing LLM performance for
    D2T tasks in the presence of source-reference divergence.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Traditionally, D2T models have relied on rule-based methodologies (Reiter and
    Dale, [1997](#bib.bib90)), meticulously crafted by domain experts. However, these
    rule-based approaches have been hindered by two primary challenges: scalability
    and the scarcity of domain experts (Gatt and Krahmer, [2018](#bib.bib27); Angeli
    et al., [2010](#bib.bib2)). As the demand for D2T models continues to rise across
    various applications (Lu and Ng, [2011](#bib.bib63); Gatt and Krahmer, [2018](#bib.bib27)),
    there has been a notable shift towards the development of automatic D2T models.
    Statistical n-gram based language models (Brown et al., [1992](#bib.bib10); Bickel
    et al., [2005](#bib.bib9); Pauls and Klein, [2011](#bib.bib78)) laid the foundation
    for probabilistic D2T models (Chen and Mooney, [2008](#bib.bib13); Angeli et al.,
    [2010](#bib.bib2); Lu and Ng, [2011](#bib.bib63)). While these initial probabilistic
    models have gained traction due to their scalability, they still struggle to emulate
    human-like text generation for D2T tasks (Belz and Reiter, [2006](#bib.bib7);
    Reiter, [2018](#bib.bib89)). The emergence of recurrent neural network-based (RNN)
    language models (Mikolov et al., [2010](#bib.bib69)) marked a significant milestone
    in the realm of D2T tasks (Lebret et al., [2016](#bib.bib49)), enabling D2T models
    to exhibit extreme fluency. With the introduction of encoder-decoder framework-based
    sequence-to-sequence neural networks (Sutskever et al., [2014](#bib.bib99)), D2T
    generation experienced a surge in popularity compared to existing methods (Graves
    et al., [2013](#bib.bib31)). These models (Wen et al., [2015](#bib.bib110); Shang
    et al., [2015](#bib.bib95)) excel in readability, representing a significant advancement
    not only in D2T but also in several text generation tasks. Nevertheless, these
    RNN-based encoder-decoder D2T models still struggle to ensure the faithfulness
    and informativeness of generated text (Wiseman et al., [2017](#bib.bib111)). With
    the integration of attention mechanisms (Bahdanau et al., [2015](#bib.bib3); Luong
    et al., [2015](#bib.bib66); Xu et al., [2015](#bib.bib114)) into deep encoder-decoder
    networks and the introduction of pre-training/fine-tuning mechanisms (Howard and
    Ruder, [2018](#bib.bib37)), numerous D2T models (Mei et al., [2016](#bib.bib68);
    Liu et al., [2018](#bib.bib57); Sha et al., [2018](#bib.bib94); Nema et al., [2018](#bib.bib71);
    Budzianowski et al., [2018](#bib.bib12); Juraska et al., [2018](#bib.bib42); Nie
    et al., [2019a](#bib.bib72); Puduppully et al., [2019](#bib.bib86); Gong et al.,
    [2019](#bib.bib30)) have emerged. The advent of the transformer models (Vaswani
    et al., [2017](#bib.bib107)) has led to their widespread adoption across various
    NLP tasks, including D2T generation. Almost all recent language models, built
    upon the transformer architecture, play pivotal roles in text generation tasks,
    including D2T (Erdem et al., [2022](#bib.bib21); Ji et al., [2023](#bib.bib39);
    Lin et al., [2024](#bib.bib56)). These transformer-based pre-trained language
    models serve as foundational models for D2T tasks, benefiting from pre-training
    on large text datasets using self-supervised learning (Taylor, [1953](#bib.bib101);
    Bickel et al., [2005](#bib.bib9)). Through fine-tuning strategies, these language
    models have gained immense popularity in D2T tasks (Devlin et al., [2019](#bib.bib17);
    Lewis et al., [2020](#bib.bib51); Radford et al., [2019](#bib.bib87); Brown et al.,
    [2020](#bib.bib11)). D2T models built upon these language models often achieve
    higher rankings compared to earlier D2T models (Ge et al., [2023](#bib.bib28);
    Zhang et al., [2024](#bib.bib119)). Utilizing language models in D2T tasks also
    streamlines the training process, as they require only a small amount of task-specific
    data for fine-tuning (Erdem et al., [2022](#bib.bib21)). While language model-led
    D2T systems show improvements in informativeness, they still struggle to generate
    faithful content.'
  prefs: []
  type: TYPE_NORMAL
- en: Several recent studies (Petroni et al., [2019](#bib.bib83); Heinzerling and
    Inui, [2021](#bib.bib32); Roberts et al., [2020](#bib.bib91)) on language models,
    suggest that the parameters within these models are responsible for encoding knowledge.
    To enhance the knowledge capacity of these models, recent efforts have significantly
    increased the number of parameters (Raffel et al., [2020](#bib.bib88); Scao et al.,
    [2022](#bib.bib92); Touvron et al., [2023](#bib.bib103)). These large, parameter-rich
    language models (i.e., LLMs) demonstrate remarkable performance across various
    generative tasks, such as text generation, and discriminative tasks, like text
    classification (Devlin et al., [2019](#bib.bib17); Lewis et al., [2020](#bib.bib51);
    Radford et al., [2019](#bib.bib87)). Even in few-shot and zero-shot training setups (Brown
    et al., [2020](#bib.bib11)), these LLMs consistently excel in numerous NLP tasks.
    The trend of augmenting parameters within LLMs has significantly shaped the landscape
    of text generation, with LLMs significantly outperforming earlier deep learning-based
    models. This phenomenon has also impacted D2T tasks, as the majority of recent
    D2T models utilize LLMs (Kasner and Dusek, [2024](#bib.bib45); Li et al., [2024b](#bib.bib53);
    Jing et al., [2024](#bib.bib41); Lorandi and Belz, [2024](#bib.bib60)). Recent
    advancements in parameter-efficient fine-tuning approaches for LLMs, such as prefix-tuning (Li
    and Liang, [2021](#bib.bib55)), P-tuning (Liu et al., [2022](#bib.bib58)), and
    prompt tuning (Lester et al., [2021](#bib.bib50)), along with popular adapter-based
    fine-tuning methods like LoRA (Hu et al., [2022](#bib.bib38)), have made LLMs
    more suitable for D2T tasks. Given that LLMs are pre-trained on vast text corpora (Raffel
    et al., [2020](#bib.bib88); Touvron et al., [2023](#bib.bib103)), they excel in
    capturing rich contextual information, making them well-suited for generating
    text in D2T scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the widespread application of LLMs in D2T, several important questions
    persist regarding their performance, particularly considering their sizes (numbers
    of parameters) (Luccioni et al., [2023](#bib.bib64); Jiang et al., [2024](#bib.bib40)).
    Achieving human-like readability in generated text remains a challenge for LLMs
    in D2T (Jing et al., [2024](#bib.bib41)). Regarding informativeness, a crucial
    quality of D2T models, LLMs may overlook essential information when generating
    from non-textual data due to underlying biases stemming from their large number
    of parameters (Wu and Aji, [2023](#bib.bib113)). Recent studies highlight that
    even with an increase in parameters, the effective or extrinsic parameters of
    LLMs remain low (Aghajanyan et al., [2021](#bib.bib1); Tulchinskii et al., [2023](#bib.bib104)).
    Concerns also persist regarding the faithfulness of LLMs in D2T (Ji et al., [2023](#bib.bib39)).
    Recent investigations have sought to highlight the limitations of LLMs as D2T
    models. However, these studies have primarily consisted of surveys discussing
    the progress of D2T tasks (Lin et al., [2024](#bib.bib56)) or outlining D2T models
    focusing on single key attributes (Gatt and Krahmer, [2018](#bib.bib27); Erdem
    et al., [2022](#bib.bib21); Ji et al., [2023](#bib.bib39)). None of these studies
    offers a comprehensive analysis of D2T models considering all three essential
    qualities (readability, informativeness, and faithfulness), particularly in relation
    to LLMs. Our study aims to address this research gap.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Conditional Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text generation or natural language generation (Lu et al., [2018](#bib.bib62);
    Erdem et al., [2022](#bib.bib21)), both fundamental and crucial task in NLP, encompasses
    various applications such as machine translation (Gatt and Krahmer, [2018](#bib.bib27)),
    D2T (Ye et al., [2020](#bib.bib116)), dialogue generation (Wen et al., [2015](#bib.bib110)),
    image captioning (Karpathy and Fei-Fei, [2017](#bib.bib44)), summarization (See
    et al., [2017](#bib.bib93); ul Islam et al., [2023](#bib.bib105)), story generation (Fan
    et al., [2018](#bib.bib23)), report writing (Lu and Ng, [2011](#bib.bib63); Lin
    et al., [2024](#bib.bib56)), etc. These text generation tasks can be broadly categorized
    into two main types: open-ended text generation and conditional (or controlled)
    text generation (Holtzman et al., [2018](#bib.bib34); Li et al., [2022](#bib.bib54)).
    In open-ended text generation, models produce text without any constraint from
    prior source information. For illustration, generating story narratives after
    a given context is an open-ended generation (Fan et al., [2018](#bib.bib23)).
    Completing text from a text fragment is also an example of open-ended generation.
    On the other hand, in conditional text generation, the model generates text based
    on a constraint given through prior source information and instructions. Examples
    of conditional text generation tasks include automatic summarization, machine
    translation, and task-oriented dialogue generation, all of which rely on specific
    input (source) conditions for text generation. D2T falls within conditional text
    generation (Puduppully et al., [2019](#bib.bib86); Upadhyay and Massie, [2023](#bib.bib106)),
    as it involves generating text from structured data inputs, further emphasizing
    its significance in this category. Conditional text generation tasks are always
    represented through conditional probability distributions (Graves et al., [2013](#bib.bib31);
    Gatt and Krahmer, [2018](#bib.bib27); Erdem et al., [2022](#bib.bib21)). To generate
    a text, $w=w_{1},w_{2},w_{3},\dots,w_{n}$ means empty word) from a source, $s$,
    we can be explicitly written as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w_{i}\sim\left.Pr\left(\cdot\Big{&#124;}{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\underbrace{s}_{\text{Given
    source}}},{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\underbrace{w_{1},w_{2},w_{3},\dots,w_{i-1}}_{\text{Previous
    generated context}}}\right)\right&#124;_{i=1}^{n-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/7e5733d139a0a18ca687dd8a4a0b3219.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Two prevalent types of language models based on transformer (Vaswani
    et al., [2017](#bib.bib107)) architecture are depicted here: bidirectional and
    unidirectional language models. The red lines represent attention mechanisms.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 (Large) Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most of the recent advancement of generative text generation tasks are direct
    outcomes of predictive language modelling (Pauls and Klein, [2011](#bib.bib78);
    Mikolov et al., [2010](#bib.bib69); Devlin et al., [2019](#bib.bib17); Radford
    et al., [2019](#bib.bib87)). With the advent of transformers (Vaswani et al.,
    [2017](#bib.bib107)) and pre-training paradigm (Howard and Ruder, [2018](#bib.bib37)),
    language models have demonstrated unparalleled dominance across various NLG tasks.
    Primarily there are mainly two type of language modelling (Figure [3](#S4.F3 "Figure
    3 ‣ 4.1 Conditional Text Generation ‣ 4 Preliminaries ‣ Impact of Model Size on
    Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"))—
    bidirectional and uni-directional language modelling (Lu et al., [2018](#bib.bib62)).
    In bidirectional language model, both left and right contexts are considered in
    predicting a context. Masked language modelling (Taylor, [1953](#bib.bib101)),
    where the objective is to predict mask word in a text, paved the path for bidirectional
    language model. With the appearance of BERT (Devlin et al., [2019](#bib.bib17)),
    bi-directional language model show performance enhancement both in terms of semantic
    and contextual informativeness in several NLP task. Objective of language model
    is to predict masked words ($w_{i}$), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w_{i}\leftarrow\underset{w}{\text{argmax}}\ p(w&#124;w_{1}\dots
    w_{i-1}w_{\text{mask}}w_{i+1}\dots w_{n})$ |  |'
  prefs: []
  type: TYPE_TB
- en: On the other hand, unidirectional language models predicts target words from
    one direction, mostly left-to-right directional are followed (Radford et al.,
    [2019](#bib.bib87); Brown et al., [2020](#bib.bib11); Raffel et al., [2020](#bib.bib88)).
    Earlier, unidirectional language model seems to have less powerful compare to
    the bi-directional language model. However several recent studies (Li and Liang,
    [2021](#bib.bib55); Liu et al., [2022](#bib.bib58)) have shown that unidirectional
    language models can also be powerful as bi-direction language model in terms of
    language understanding. Currently most of the popular language models are uni-directional
    language models. Objective of unidirectional language model is to predict next
    words ($w_{n}$),
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w_{n}\leftarrow\underset{w}{\text{argmax}}\ p(w&#124;w_{1}w_{2}\dots
    w_{n-1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/97c062a360ab22e56480f1bc38907800.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Two of the most popular architectures used for implementing language
    models: the encoder-decoder architecture (left) and the decoder-only architecture
    (right). The sequence $e_{1}e_{2}\dots e_{m}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two primary architectures are commonly used for implementing language models:
    encoder-decoder architecture (Sutskever et al., [2014](#bib.bib99); Vinyals et al.,
    [2016](#bib.bib108)) and decoder-only architecture (Radford et al., [2019](#bib.bib87);
    Brown et al., [2020](#bib.bib11)) (Figure [4](#S4.F4 "Figure 4 ‣ 4.2 (Large) Language
    Model ‣ 4 Preliminaries ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation")). In the encoder-decoder
    architecture, the encoder part encodes an input sequence, and the decoder generates
    a new sequence from the encoded representation. On the other hand, in the decoder-only
    architecture, the decoder auto-regressively completes a sequence from a given
    context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contemporary times, large language models (LLMs) have become omnipresent
    in nearly all NLP tasks, owing to their remarkable performance and versatility (Devlin
    et al., [2019](#bib.bib17); Brown et al., [2020](#bib.bib11); Touvron et al.,
    [2023](#bib.bib103)). LLMs stand out from previous language models primarily due
    to two factors: their enormous size (Chowdhery et al., [2023](#bib.bib15)), characterized
    by a vast number of underlying parameters, and their extensive self-supervised
    pre-training on massive text corpora (Raffel et al., [2020](#bib.bib88); Penedo
    et al., [2023](#bib.bib80)). Compared to earlier language models (Graves et al.,
    [2013](#bib.bib31); Mikolov et al., [2010](#bib.bib69); Peters et al., [2018](#bib.bib82)),
    contemporary LLMs boast billions, and even trillions, of parameters, enabling
    them to achieve exceptional language understanding (Roberts et al., [2020](#bib.bib91);
    Aghajanyan et al., [2021](#bib.bib1)). Additionally, LLMs demonstrate impressive
    results through supervised fine-tuning, also known as instruction tuning (Si et al.,
    [2023](#bib.bib97); Zhang et al., [2023](#bib.bib120)), and training aided by
    reinforcement learning techniques (Fernandes et al., [2023](#bib.bib24); Ouyang
    et al., [2022](#bib.bib76)), incorporating human-like feedback through preference
    models (Touvron et al., [2023](#bib.bib103); Chowdhery et al., [2023](#bib.bib15)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Data-to-Text (D2T) Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The aim of Data-to-Text generation (D2T) (Lin et al., [2024](#bib.bib56)) is
    to convert non-textual, semi-structured source data, such as tables, graphs, or
    slot-value pairs (meaning representation, MR), into human-readable text output.
    D2T encompasses various types based on source representation, including graph-to-text,
    table-to-text, and MR-to-text (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")). In graph-to-text (Gardent et al., [2017](#bib.bib26);
    Nan et al., [2021](#bib.bib70)), structured graph data or knowledge triple sets
    are transformed into coherent narratives, while table-to-text (Liu et al., [2018](#bib.bib57);
    Bao et al., [2018](#bib.bib5); Gong et al., [2019](#bib.bib30)) involves converting
    tabular data into fluent textual descriptions. MR-to-text focuses on generating
    textual reports from slot-value pairs, also known as Meaning Representation (MR) (Novikova
    et al., [2017](#bib.bib75); Juraska et al., [2019](#bib.bib43); Sha et al., [2018](#bib.bib94);
    Kedzie and McKeown, [2020](#bib.bib47)). While D2T shares similarities with other
    text generation tasks like machine translation and summarization, its primary
    distinction lies in the use of semi-structured non-textual data as input (Ji et al.,
    [2023](#bib.bib39)). As the demand for automated text generation from semi-structured
    data grows, D2T remains a crucial research area with promising implications for
    enhancing data accessibility and decision-making processes (Gatt and Krahmer,
    [2018](#bib.bib27)). Numerous predictive models have been developed for D2T generation,
    with recent advancements largely attributed to the use of predictive language
    models (Pauls and Klein, [2011](#bib.bib78); Mikolov et al., [2010](#bib.bib69);
    Devlin et al., [2019](#bib.bib17); Radford et al., [2019](#bib.bib87)). These
    models, utilizing recurrent neural networks (RNNs) Hochreiter and Schmidhuber,
    [1997](#bib.bib33); Graves et al., [2013](#bib.bib31); Mikolov et al., [2010](#bib.bib69)
    and transformer architecture (Vaswani et al., [2017](#bib.bib107)), exhibit significant
    potential across various D2T tasks. Transformers have attracted significant attention
    and demonstrated remarkable success in D2T applications (Su et al., [2021](#bib.bib98);
    Wang et al., [2020](#bib.bib109)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'D2T is typically learned from a dataset consisting of a set of source-reference
    pairs, $\mathcal{D}=\left.{\{s,r\}}\right|_{\in\mathcal{D}}$, i.e., $r_{i}\in\mathcal{V}$.
    These predictive D2T models (Sha et al., [2018](#bib.bib94); Liu et al., [2018](#bib.bib57);
    Dusek et al., [2018](#bib.bib20); Gong et al., [2019](#bib.bib30)) often follow
    either encoder-decoder or decoder-only paradigms, with transformer-based architectures
    being commonly used for both encoder and decoder components. In training or learning
    phase, the predictive D2T model $\mathcal{M}_{\theta}$, typically using maximum
    likelihood estimation (Gkatzia, [2016](#bib.bib29); Holtzman et al., [2018](#bib.bib34);
    Puduppully and Lapata, [2021](#bib.bib85); Luo et al., [2023](#bib.bib65)) strategies
    (equation [1](#S4.E1 "In 4.3 Data-to-Text (D2T) Generation ‣ 4 Preliminaries ‣
    Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{*}\leftarrow\underset{\theta}{\text{argmax}}\sum\limits_{(s,r)\in\mathcal{D}}\mathcal{L}(\mathcal{M}_{\theta}(s),r)$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathcal{L}$) from a given source data, $s$ (Fan et al., [2018](#bib.bib23)),
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g\leftarrow\underset{\mathcal{V}^{*}}{\text{decoding}}\
    \mathcal{M}_{\theta^{*}}(s)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Source-Reference Divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous studies (Dhingra et al., [2019](#bib.bib18); Tian et al., [2019](#bib.bib102);
    ul Islam et al., [2023](#bib.bib105)) have noted that across various D2T tasks (Lebret
    et al., [2016](#bib.bib49)), there often exists a discrepancy between the information
    or facts present in the source data ($s$). This disparity is known as source-reference
    divergence (Dhingra et al., [2019](#bib.bib18); Li et al., [2022](#bib.bib54)).
    An illustrative example of such divergence is depicted in Figure [5](#S4.F5 "Figure
    5 ‣ 4.4 Source-Reference Divergence ‣ 4 Preliminaries ‣ Impact of Model Size on
    Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6867828ff8fa0e467e0999736c3820b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example of source-reference divergence, taken from the WikiTableText (Bao
    et al., [2018](#bib.bib5)) dataset. The reference text includes two additional
    facts (bolded and underlined) - ‘scrolling performer’ and ‘fun game’ - that are
    absent in the corresponding source data.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary causes of such divergence stem from the processes of data collection
    and the inherent nature of the tasks (Lebret et al., [2016](#bib.bib49); Gardent
    et al., [2017](#bib.bib26); Dusek et al., [2018](#bib.bib20)). In many cases,
    source and reference texts are drawn from disparate origins, resulting in a weak
    factual correlation between them. Additionally, D2T datasets are often curated
    by multiple human annotators who interpret the source data before generating corresponding
    textual references (Nie et al., [2019b](#bib.bib73)). Each annotator brings their
    unique background knowledge and comprehension abilities to the task, leading to
    generated references that may include additional or misaligned facts. Another
    significant factor contributing to source-reference divergence in D2T tasks is
    the inherent nature of D2T tasks (Li et al., [2022](#bib.bib54)). Source data
    of D2T task are typically more compact and specific, whereas references tend to
    be more generalized. Consequently, source-reference divergence is common and challenging
    to eliminate entirely, particularly in D2T contexts (Dhingra et al., [2019](#bib.bib18);
    Nie et al., [2019b](#bib.bib73)). Therefore, evaluating D2T model performance
    in the presence of such divergence is of utmost importance.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Models, Datasets and Experimental Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the large language models (LLMs) and data-to-text
    (D2T) datasets employed in our subsequent experiments. Additionally, we include
    a subsection outlining the experimental settings, where we provide detailed information
    regarding the experimental setup.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We include 12 popular LLMs from five widely recognized LLM families (BART,
    T5, BLOOM, OPT, and Llama 2), all of which are widely utilized in the text generation
    field. We select LLMs of varying model sizes from each of the five model families.
    Considering the model sizes, among these five LLM families, we categorize BART
    and T5 as families of smaller LLMs, while the remaining three are families of
    larger LLMs. Figure [6](#S5.F6 "Figure 6 ‣ 5.1 Models ‣ 5 Models, Datasets and
    Experimental Settings ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation") depicts all 12 incorporated
    LLMs along with their sizes, i.e., the number of parameters. All these LLMs are
    sourced from Hugging Face’s Model Hub (Wolf et al., [2020](#bib.bib112)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/435fa74b32b00f102c6101723183d505.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Sizes (number of parameters) of the 12 LLMs from five LLM families
    included in our experiments. BART, T5, BLOOM, OPT, and Llama 2 families are indicated
    with blue, orange, green, red, and purple respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: BART.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'BART (Lewis et al., [2020](#bib.bib51)) is an encoder-decoder architecture-based
    language model family. The encoder encodes the given context through bidirectional
    language modeling, while the decoder performs auto-regressive generation. BART
    is pre-trained using a self-supervised approach involving text denoising, which
    includes methods like token masking, token deletion, text infilling, etc. In our
    experiments, we use two models of the BART family, which differ in terms of their
    sizes: BART-base (139 million parameters) and BART-large (406 million parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: T5.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similar to BART, T5 (Raffel et al., [2020](#bib.bib88)) follows the encoder-decoder
    paradigm. However, the main motivation behind T5 is to unify several NLP tasks
    and train them together as a single task. T5 is pre-trained using BERT-like span-corruption
    techniques (Devlin et al., [2019](#bib.bib17)) on a large crawled dataset called
    C4 (Colossal Clean Crawled Corpus) (Raffel et al., [2020](#bib.bib88)). T5 has
    demonstrated state-of-the-art results on several natural language understanding
    tasks. In our experiments, we utilize two models of T5 family with different model
    sizes: T5-base (223 million parameters) and T5-large (738 million parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: BLOOM.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'BLOOM (Scao et al., [2022](#bib.bib92)) is a publicly available decoder-only
    language model family trained on a large dataset—ROOT dataset, which includes
    46 natural languages and 13 programming languages. Similar to BART and T5, BLOOM
    is based on a transformer architecture. In our study, we consider three different
    sizes for BLOOM LLMs: BLOOM-1B (1 billion parameters), BLOOM-2.7B (2.7 billion
    parameters), and BLOOM-6.7B (6.7 billion parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: OPT.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Like BLOOM, OPT (Zhang et al., [2022](#bib.bib121)) is an auto-regressive decoder-based
    language model family that utilizes a transformer decoder architecture. OPT is
    pre-trained on three datasets: the RoBERTa dataset (Liu et al., [2019](#bib.bib59)),
    the Pile, and PushShift.io Reddit (Zhang et al., [2022](#bib.bib121)). These corpora
    were previously collected or filtered. OPT has demonstrated performance comparable
    to popular GPT models on several state-of-the-art benchmarks. Unlike GPT (Brown
    et al., [2020](#bib.bib11)), OPT is fully publicly available. In this paper, we
    use three OPT models with different model sizes: OPT-2.7B (2.7 billion parameters),
    OPT-6.7B (6.7 billion parameters), and OPT-13B (13 billion parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: Llama 2.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Llama 2 family (Touvron et al., [2023](#bib.bib103)) is the successor of Llama
    from [Meta AI](https://ai.meta.com/). It is a decoder-based LLM trained through
    three stages: traditional pre-training, supervised fine-tuning, and human feedback
    endorsed by reinforcement learning. Llama 2 is pre-trained with almost 2 trillion
    tokens and is ranked higher in terms of human safety and usability compared to
    other LLMs. It shows comparable results with several proprietary LLMs like ChatGPT.
    In our experiments, we utilize two Llama 2 models with different sizes: Llama2-6B
    (6 billion parameters) and Llama2-13B (13 billion parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We included five state-of-the-art D2T datasets covering all three major types
    of D2T tasks: E2E and ViGGO for MR-to-text, WikiTableText for table-to-text, and
    DART and WebNLG for graph-to-text. Table [1](#S5.T1 "Table 1 ‣ WikiTableText dataset.
    ‣ 5.2 Datasets ‣ 5 Models, Datasets and Experimental Settings ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") presents key statistics for all five D2T datasets. All these dataset
    are downloaded from Kasner et al. ([2023](#bib.bib46)) and Wolf et al. ([2020](#bib.bib112)).'
  prefs: []
  type: TYPE_NORMAL
- en: E2E dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset contains information from the restaurant domain and is frequently
    utilized in D2T tasks (Novikova et al., [2017](#bib.bib75); Dusek et al., [2018](#bib.bib20)).
    It consists of input data in the form of slot-value pairs, where each slot represents
    an attribute and its corresponding value indicates the attribute’s realization.
    Known for its lexical richness and diverse syntactical structures, this dataset
    is highly esteemed in D2T research. Comprising over 2 million tokens (including
    both references and source) and almost 37K data-text pairs, with approximately
    4.5K unique words, it provides abundant data for both training and evaluation
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: ViGGo dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ViGGO dataset (Juraska et al., [2018](#bib.bib42)), focusing on the video
    game domain, is ideal for training open-domain D2T models. With a diverse range
    of dialogue acts and forms drawn from nearly a hundred video games, ViGGO offers
    extensive coverage and high text diversity. Containing around 7K instances, ViGGO
    exhibits greater lexical richness compared to other datasets like E2E. Each meaning
    representation in ViGGO corresponds to one of nine distinct conversational dialogue
    acts, providing nuanced dialogue interactions. Overall, ViGGO excels in prioritizing
    lexical richness and text diversity, making it a valuable resource for D2T tasks.
  prefs: []
  type: TYPE_NORMAL
- en: WikiTableText dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: WikiTableText (Bao et al., [2018](#bib.bib5)) is designed for the table-to-text
    form of Data-to-Text (D2T) task. Unlike the E2E and ViGGO datasets, it belongs
    to the open-domain category. It consists of numerous domain Wikipedia tables,
    randomly selected and manually annotated to generate corresponding text. With
    a total of $\sim 13$K data-text pairs, each pair contains an average of 13.9 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '| dataset | D2T types | domain | dataset size | source (linearized) | reference
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (# instances) | average length | unique tokens | total tokens | average length
    | unique tokens | total tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| E2E | mr-to-text | closed | 36,856 | 27.3 | 125 | 1M | 20.8 | 4.5K | 885K
    |'
  prefs: []
  type: TYPE_TB
- en: '| ViGGo | mr-to-text | closed | 6,900 | 29.9 | 618 | 206K | 21.5 | 4.4K | 148K
    |'
  prefs: []
  type: TYPE_TB
- en: '| WikiTableText | table-to-text | open | 13,318 | 35.2 | 29K | 469K | 13.9
    | 24K | 185K |'
  prefs: []
  type: TYPE_TB
- en: '| DART | graph-to-text | open | 70,524 | 34.8 | 44K | 2.5M | 19.3 | 45K | 1.5M
    |'
  prefs: []
  type: TYPE_TB
- en: '| WebNLG | graph-to-text | open | 38,872 | 30.4 | 7K | 1.2M | 20.1 | 19K |
    905K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Summary of key statistics, D2T types, and domains for the five incorporated
    datasets. The table presents average length (number of tokens in text), unique
    tokens, and total tokens for both sources (linearized to text) and references.'
  prefs: []
  type: TYPE_NORMAL
- en: DART dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DART dataset (Nan et al., [2021](#bib.bib70)) is integral for open-domain
    structured D2T generation, similar to the WikiTableText dataset. It pertains specifically
    to the graph-to-text D2T task, aiming to generate text from knowledge graph triplets.
    Each instance in the dataset covers a variety of domains, with inputs consisting
    of semantic triplet sets accompanied by detailed sentence descriptions. These
    descriptions are curated from a multitude of datasets. On average, the reference
    text sets contain approximately 19.3 tokens each.
  prefs: []
  type: TYPE_NORMAL
- en: WebNLG dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The underlying task of this dataset involves generating text from a series of
    knowledge graph entries, also known as RDF (Resource Description Format)-to-text
    generation, a type of graph-to-text generation (Gardent et al., [2017](#bib.bib26)).
    The dataset focuses on generating text from knowledge graph triplets sourced mainly
    from DBPedia across six domains (Perez-Beltrachini and Gardent, [2017](#bib.bib81);
    Gardent et al., [2017](#bib.bib26)). It’s gathered via crowdsourcing, ensuring
    diverse and validated outputs. Notably, it exhibits greater lexical and semantic
    diversity compared to similar datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In all our experiments in this study, we fined-tune LLMs for D2T task. We fine-tune
    each LLM on all five datasets for 3 epochs. Because of smaller sizes, T5 and BART
    models underwent full fine-tuning with a learning rate of 1e-05. Both T5 and BART
    were loaded with torch.bfloat16 precision. However, due to the large number of
    parameters in BLOOM, OPT, and Llama 2, we employed the QLoRA (Quantized version
    of Low Rank Adapter) (Dettmers et al., [2023](#bib.bib16)) technique as a parameter-efficient
    fine-tuning approach, utilizing a learning rate of 1e-04. All these three larger
    LLM families’ models were loaded with 4-bit precision. The peft library are used
    for implementing QLoRA in our experiments (Mangrulkar et al., [2022](#bib.bib67)).
    As previously mentioned, all 12 LLMs and the five D2T datasets were sourced from
    HuggingFace (Wolf et al., [2020](#bib.bib112)). We maintained a consistent text
    length of 256 for all generations, encompassing both source data and generated
    text. For optimization purposes, we utilized the popular AdamW optimizer (Loshchilov
    and Hutter, [2019](#bib.bib61)), which is primarily based on the Adam optimizer (Kingma
    and Ba, [2015](#bib.bib48)) with $L^{2}$-norm over the weight space. In QLoRA
    set-ups, we continue AdamW optimizer’s operation at torch.bfloat16 precision.
    Decoding strategies play a crucial role in text generation. We compared all generated
    text using the beam-search decoding strategy with a beam size of 5. All experiments
    were conducted on a local system equipped with an NVIDIA RTX A6000 (48 GB) graphics
    card to ensure adequate GPU access.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Significant Testing.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We aim to determine the statistical significance of the performance evaluations
    in all our comparative analyses across the three key qualities of D2T (readability,
    informativeness, and faithfulness). To achieve this, we conduct Welch’s t-test (Dror
    et al., [2018](#bib.bib19)) comparing the results of each LLM to the best-performing
    LLM within the same family. We apply a significance level of $p<0.05$ for these
    tests, with each Welch’s t-test based on samples size of 6.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Performance Evaluations and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we analyze the performance of LLMs in D2T using popularly
    used evaluation metrics across three aspects: Readability, Informativeness, and
    Faithfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Readability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The readability (Reiter and Dale, [1997](#bib.bib90); Belz and Reiter, [2006](#bib.bib7);
    Chen et al., [2020](#bib.bib14)) of a D2T model mainly considers two factors:
    fluency and coherence. Fluency determines whether the generated text is grammatically
    correct and easy to read, enhancing its naturalness in terms of reading. Coherence
    assesses whether the information, topics, and facts in the generated text are
    well-ordered and chronologically expressed. To measure readability, we incorporate
    two automatic evaluation metrics: Bleu and Meteor. We explain these two metrics
    briefly below.'
  prefs: []
  type: TYPE_NORMAL
- en: '| family | model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; size (number of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; parameters in billion) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  prefs: []
  type: TYPE_TB
- en: '| BART | BART-base | 0.1 | 0.399 | 0.281 | 0.421 | 0.423 | 0.481 |'
  prefs: []
  type: TYPE_TB
- en: '| BART-large | 0.4 | 0.403 | 0.283 | 0.419 | 0.413 | 0.503 |'
  prefs: []
  type: TYPE_TB
- en: '| T5 | T5-base | 0.2 | 0.398 | 0.268 | 0.408 | 0.461 | 0.527 |'
  prefs: []
  type: TYPE_TB
- en: '| T5-large | 0.7 | 0.411 | 0.302 | 0.431 | 0.479 | 0.546 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT | OPT-2.7B | 2.7 | 0.350 | 0.262 | 0.421 | 0.441 | 0.521 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-6.7B | 6.7 | 0.369 | 0.269 | 0.426 | 0.448 | 0.538 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13B | 13.0 | 0.347 | 0.269 | 0.412 | 0.463 | 0.549 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.374 | 0.255 | 0.411 | 0.437 | 0.491 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM-3B | 3.0 | 0.380 | 0.260 | 0.396 | 0.446 | 0.520 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM-7B | 7.0 | 0.379 | 0.274 | 0.423 | 0.444 | 0.530 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.419 | 0.248 | 0.436 | 0.494 | 0.532 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13B | 13.0 | 0.408 | 0.288 | 0.451 | 0.51 | 0.563 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Analysis of readability across 12 LLMs from five LLM families (BERT,
    T5, BLOOM, OPT, and Llama 2) on all five D2T datasets, measured using the Bleu
    metric. The best scoring results in terms of model size for each family are highlighted
    in bold. Overall, the Bleu score consistently tends to increase with larger model
    sizes, except for a few exceptions. All results are statistically significant
    at the $p<0.05$ level.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bleu. Bleu (Papineni et al., [2002](#bib.bib77)) evaluates the readability
    between generated and reference texts using n-gram matching. It measures both
    precision and recall based on n-gram overlap. Despite facing criticism, Bleu remains
    widely used for assessing textual fluency. Essentially, Bleu computes the geometric
    mean of n-gram precision of generated text with respect to the reference. Additionally,
    a brevity penalty term is incorporated into Bleu to address length discrepancies
    between the generated and reference texts. We can represent Bleu as the following
    equation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textsc{Bleu}=\beta\cdot\text{exp}\left(\sum_{n}\text{log}(w_{n}\cdot\text{Precision}_{n})\right)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'where $\text{Precision}_{n}$ represents the brevity penalty (defines through
    equation [3](#S6.E3 "In 1st item ‣ 6.1 Readability ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | <math id="S6.E3.m1.4" class="ltx_Math" alttext="\displaystyle\beta=\begin{cases}1&amp;\text{if
    $&#124;g&#124;></math> |  | (3) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Where $|r|$) text, respectively. In most cases, $n$ is set to 4, which is also
    known as Bleu-4. In our evaluation also we employed Bleu-4. We used SacreBLEU (Post,
    [2018](#bib.bib84)) implementation to obtain our blue results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meteor. Meteor (Banerjee and Lavie, [2005](#bib.bib4)), similar to Bleu, relies
    on n-gram string matching and finds predominant use in machine translation tasks.
    While Bleu focuses solely on n-gram precision, it overlooks the recall component.
    Meteor addresses this by leveraging unigram-based string overlaps to compute both
    recall and precision. Ultimately, Meteor distinguishes itself as a weighted variant
    of the F1-measure, as expressed by the following equation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textsc{Meteo}_{\text{F1-measure}}=\frac{10\cdot\text{Precison}\cdot\text{Recall}}{\text{Recall}+9\text{Precison}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Where, Precision and Recall denote precision and recall based on unigram-based
    matching. To account for higher-order $n$-grams, Meteor applies a penalty factor
    to the F1-measure. The penalty factor is calculated as the ratio of the number
    of chunks concatenated through matched unigrams to the number of matched unigrams
    (equation [4](#S6.E4 "In 2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{penalty}=0.5\times\frac{\text{number of chunks}}{\text{number
    of matched unigrams}}$ |  | (4) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Finally, Meteor score is calculated through,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textsc{Meteor}=(1-\text{penalty})\cdot\textsc{Meteo}_{\text{F1-measure}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: In addition to the above formulation, unlike Bleu, Meteor integrates synonym
    matching (via WordNet synsets) and word stemming (via Porter’s stemmer) features
    to capture textual similarity between candidates and references.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| family | model |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '&#124; size (number of &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '&#124; parameters in billion) &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART | BART-base | 0.1 | 0.368 | 0.315 | 0.367 | 0.365 | 0.377 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART-large | 0.4 | 0.371 | 0.321 | 0.367 | 0.381 | 0.408 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5 | T5-base | 0.2 | 0.369 | 0.312 | 0.361 | 0.390 | 0.410 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5-large | 0.7 | 0.377 | 0.324 | 0.370 | 0.400 | 0.420 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT | OPT-2.7B | 2.7 | 0.355 | 0.307 | 0.366 | 0.382 | 0.403 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-6.7B | 6.7 | 0.366 | 0.305 | 0.371 | 0.387 | 0.406 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-13B | 13.0 | 0.355 | 0.308 | 0.361 | 0.391 | 0.411 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.369 | 0.304 | 0.367 | 0.379 | 0.386 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-3B | 3.0 | 0.371 | 0.305 | 0.356 | 0.386 | 0.403 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-7B | 7.0 | 0.369 | 0.314 | 0.375 | 0.385 | 0.412 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.376 | 0.307 | 0.375 | 0.403 | 0.412 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama2-13B | 13.0 | 0.37 | 0.318 | 0.38 | 0.407 | 0.419 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 3: Analysis of the readability of 12 LLMs from five LLM families (BERT,
    T5, BLOOM, OPT, and Llama 2) across all five D2T datasets, evaluated using the
    Meteor metric. The best-performing results in terms of model size for each family
    are highlighted in bold. Similar to the Bleu results, the Meteor score tends to
    increase with larger model sizes, with a few exceptions. All results are statistically
    significant at the $p<0.05$ level.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Takeaways.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From our empirical results on readability based on Table [2](#S6.T2 "Table
    2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") and Table [3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation"), we can derive
    insights to address the readability part of first question posed in Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation") .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: What are the impacts of model size within a family of fine-tuned
    LLMs on the performance of data-to-text (D2T) tasks, in terms of the readability
    , informativeness, and faithfulness?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: Undoubtedly, the parameter count of LLMs significantly influences the
    readability of D2T tasks. Across all three categories of D2T tasks, it becomes
    apparent from Table [2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") and Table [3](#S6.T3 "Table 3 ‣
    2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") that augmenting the model parameters substantially enhances the
    readability of D2T tasks, with only minor discrepancies observed. Nearly all LLMs
    demonstrate superior performance compared to their lower-sized counterparts within
    the same LLM family. Consequently, the Bleu and Meteor scores consistently indicate
    improved readability across all LLMs within their respective families. If we consider
    the sum of all results across the five datasets, it is evident that in a family
    of large language models (LLMs), the readability scores consistently improve with
    an increase in the number of parameters, i.e., model size. As both Table [2](#S6.T2
    "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") and Table [3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation") show statistically
    significant results at the $p<0.05$ level, thereby reinforcing our findings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.2 Informativeness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Informativeness of D2T models concerns the content similarity between reference
    text and the generated text from the model (Shen et al., [2019](#bib.bib96); Erdem
    et al., [2022](#bib.bib21); Belz et al., [2020](#bib.bib8)). A highly informative
    D2T model indicates its ability to generate genuinely helpful information. Semantic
    similarity between reference and generated text contributes to content similarity.
    To measure informativeness based on content similarity, we incorporate two contextual
    similarity-based metrics—BertScore and MoverScore. It’s important to note that,
    similar to reference text, source text also contains valuable information, and
    in the presence of source-reference divergence, it becomes crucial to assess the
    similarity between source and generated text, similar to reference text. For measuring
    informativeness in the presence of source-reference divergence, we use the Parent
    metric, which considers both reference and source text to compute content similarity
    with respect to the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BertScore. BertScore (Zhang et al., [2020](#bib.bib122)) leverages contextual
    representations to assess text similarity. Unlike models based on n-gram matching,
    which may struggle to capture contextual nuances, BertScore utilizes contextual
    representations derived from the BERT model. BERT is trained using masked language
    modeling. Contextual representations of words within a text can be extracted from
    either the hidden layers or output layers of BERT. BertScore utilizes these contextual
    representations and employs the cosine similarity function to estimate the similarity
    between two contextual representations. It calculates an F1-score based on contextual
    matching between the generated text ($g$ words—$g_{1}g_{2}\dots g_{m}$, Contextual
    representation of BERT of $g$ is,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{BERT}(g)=h[g_{1}]h[g_{2}]\dots h[g_{m}]$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Likewise we can obtained $\text{BERT}(r)$. To compute the recall of BertScore,
    we perform the following steps:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Recall}_{\textsc{BertScore}}=\frac{1}{&#124;r&#124;}\sum_{r_{i}\in
    r}\underset{g_{j}\in g}{\text{max}}(h[r_{i}]^{\mathsf{T}}h[g_{j}])$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Similarly, the precision of BertScore is measured as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Precision}_{\textsc{BertScore}}=\frac{1}{&#124;g&#124;}\sum_{g_{j}\in
    g}\underset{r_{i}\in r}{\text{max}}(h[r_{i}]^{\mathsf{T}}h[g_{j}])$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Finally, BertScore is obtained by calculating the F1-measure of $\text{Precision}_{\textsc{BertScore}}$.
    Widely regarded as a superior metric for semantic similarity, BERTScore outperforms
    several other metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| family | model |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '&#124; size (number of &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '&#124; parameters in billion) &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART | BART-base | 0.1 | 0.918 | 0.889 | 0.899 | 0.908 | 0.919 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART-large | 0.4 | 0.918 | 0.890 | 0.896 | 0.914 | 0.927 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5 | T5-base | 0.2 | 0.917 | 0.884 | 0.895 | 0.925 | 0.932 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5-large | 0.7 | 0.919 | 0.895 | 0.899 | 0.927 | 0.936 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT | OPT-2.7B | 2.7 | 0.908 | 0.879 | 0.896 | 0.919 | 0.927 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-6.7B | 6.7 | 0.910 | 0.874 | 0.898 | 0.919 | 0.927 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-13B | 13.0 | 0.908 | 0.879 | 0.897 | 0.920 | 0.928 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.911 | 0.884 | 0.895 | 0.915 | 0.917 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-3B | 3.0 | 0.912 | 0.883 | 0.887 | 0.916 | 0.925 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-7B | 7.0 | 0.911 | 0.886 | 0.896 | 0.917 | 0.928 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.918 | 0.882 | 0.897 | 0.931 | 0.934 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama2-13B | 13.0 | 0.918 | 0.889 | 0.900 | 0.931 | 0.938 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 4: Comparative evaluation of Informativeness for 12 LLMs from five families
    (BERT, T5, BLOOM, OPT, and Llama 2) using the BertScore metric on five D2T datasets.
    Bold highlights indicate the best performing model sizes within each family. Generally,
    BertScore increases with larger model sizes, with minor exceptions. All results
    are statistically significant at $p<0.05$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MoverScore. Similar to BertScore, MoverScore (Zhao et al., [2019](#bib.bib124))
    prioritizes contextual similarity between texts. It quantifies text embeddings
    derived from the contextual representation of all underlying words using power
    law. To compute the semantic distance between two texts, it utilizes Word Mover’s
    Distance (WMD) with the underlying $n$-grams of the texts (equation [5](#S6.E5
    "In 2nd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{WMD}(g,r)=\underset{F\in\mathbb{R}^{&#124;g&#124;\times&#124;r&#124;}}{\text{min}}\sum
    C\odot F$ |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Where $C$ denote cost matrix and flow matrix. In WMD, each entry of the cost
    matrix is determined by the contextual representation of the n-grams obtained
    after applying the power law. The flow metrics are obtained by determining the
    weight of each n-gram through inverse document frequency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| family | model |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '&#124; size (number of &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '&#124; parameters in billion) &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART | BART-base | 0.1 | 0.667 | 0.658 | 0.688 | 0.666 | 0.673 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART-large | 0.4 | 0.667 | 0.664 | 0.683 | 0.671 | 0.684 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5 | T5-base | 0. | 0.666 | 0.650 | 0.679 | 0.688 | 0.685 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5-large | 0.7 | 0.672 | 0.669 | 0.683 | 0.692 | 0.691 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT | OPT-2.7B | 2.7 | 0.655 | 0.645 | 0.682 | 0.673 | 0..676 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-6.7B | 6.7 | 0.659 | 0.639 | 0.687 | 0.673 | 0.678 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-13B | 13.0 | 0.654 | 0.646 | 0.684 | 0.676 | 0.679 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.658 | 0.649 | 0.676 | 0.669 | 0.663 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-3B | 3.0 | 0.658 | 0.651 | 0.670 | 0.670 | 0.673 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-7B | 7.0 | 0.658 | 0.655 | 0.678 | 0.671 | 0.679 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.671 | 0.651 | 0.686 | 0.691 | 0.689 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama2-13B | 13.0 | 0.668 | 0.66 | 0.69 | 0.692 | 0.691 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 5: Assessment of Informativeness across 12 LLMs from five families (BERT,
    T5, BLOOM, OPT, and Llama 2) using the MoverScore metric on five D2T datasets.
    The highest-performing model sizes within each family are highlighted in bold.
    Consistent with the BertScore metric, MoverScore generally increases with model
    size, reflecting improved informativeness. All results are statistically significant
    at $p<0.05$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parent. Compared to BertScore and MoverScore, Parent (Dhingra et al., [2019](#bib.bib18))
    assesses the generated text using both reference and source texts. It is particularly
    valuable in scenarios with source-reference divergence and to verify if the generated
    text aligns with the source data. Parent correlates strongly with human judgments,
    as it considers both reference and source data. It calculates an F1-measure based
    on recall and precision, computed using lexical entailment techniques such as
    word overlap and co-occurrence. Let’s assume that the lexical entailment of $g_{i}$,
    i.e., $g_{i}\in\text{n-gram}(g)$ and ${E}_{n}(g_{i},r)$) is defined as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Precision}_{{E}_{n}}(g,s,r)\propto\ \frac{\sum\limits_{g_{i}\in\text{n-gram}(g)}{E}_{n}(g_{i},r)+(1-{E}_{n}(g_{i},r)){E}_{n}(g_{i},s)}{&#124;g_{i}\in\text{n-gram}(g)&#124;}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Likewise, the entailed recall of $g$ is calculated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Recall}_{{E}_{n}}(g,s,r)=\text{Recall}_{{E}_{n}}(g,s)^{\lambda}\cdot\text{Recall}_{{E}_{n}}(g,r)^{1-\lambda}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\lambda$) often is in semi-structured data. Finally, Parent is measure
    through F1-measure of $\text{Precision}_{{E}_{n}}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| family | model |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '&#124; size (number of &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '&#124; parameters in billion) &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART | BART-base | 0.1 | 0.613 | 0.420 | 0.526 | 0.538 | 0.567 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART-large | 0.4 | 0.600 | 0.413 | 0.514 | 0.563 | 0.586 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5 | T5-base | 0.2 | 0.599 | 0.430 | 0.533 | 0.614 | 0.618 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5-large | 0.738 | 0.603 | 0.436 | 0.547 | 0.622 | 0.644 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT | OPT-2.7B | 2.7 | 0.528 | 0.395 | 0.510 | 0.557 | 0.589 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-6.7B | 6.7 | 0.549 | 0.401 | 0.507 | 0.559 | 0.600 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-13B | 13.0 | 0.529 | 0.399 | 0.499 | 0.565 | 0.612 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.565 | 0.405 | 0.502 | 0.561 | 0.566 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-3B | 3.0 | 0.569 | 0.400 | 0.476 | 0.556 | 0.585 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-7B | 7.0 | 0.565 | 0.404 | 0.504 | 0.557 | 0.594 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.602 | 0.390 | 0.531 | 0.621 | 0.621 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama2-13B | 13.0 | 0.599 | 0.418 | 0.531 | 0.622 | 0.640 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 6: Assessment of informativeness across 12 LLMs from five LLM families
    on all five D2T datasets, using the Parent metric. The best-performing model sizes
    within each family are highlighted in bold. Unlike BertScore and MoverScore, Parent
    shows a mixed response, with inconsistent behavior observed in some LLM families
    such as BART, OPT, and BLOOM. However, in most cases, Parent exhibits a positive
    correlation with increasing model size. All results are statistically significant
    at $p<0.05$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Takeaways.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Except some negligible exceptions, the empirical findings from Tables [4](#S6.T4
    "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation"), [5](#S6.T5 "Table 5 ‣ 2nd item ‣ 6.2 Informativeness
    ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation"), and
    [6](#S6.T6 "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") consistently validate the relationship
    between model parameters and the informativeness of D2T models. As the model parameters
    increase, there is a clear and noticeable improvement in the LLM’s informativeness.
    With this comprehensive understanding, we are now ready to address informativeness
    part the first question posed in Section [2](#S2 "2 Research Questions and Motivations
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: What are the impacts of model size within a family of fine-tuned
    LLMs on the performance of data-to-text (D2T) tasks, in terms of the readability,
    informativeness, and faithfulness?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: Both Table [4](#S6.T4 "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation") and Table [5](#S6.T5
    "Table 5 ‣ 2nd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") present compelling evidence supporting the
    notion that increasing model size enhances the informativeness of LLMs in the
    same family, particularly concerning the alignment between reference and generated
    text. Across all three primary D2T tasks, a consistent improvement in informativeness
    is observed with the size of LLMs. While isolated cases of decreased informativeness
    with increasing LLM parameters are noted, such as with BART family (in the case
    of the WikiTableText dataset using both BertScore and MoverScore), and Llama 2
    family (in the case of the E2E dataset using MoverScore), these instances exhibit
    minimal degradation. Consequently, they do not significantly challenge our conclusion
    that increasing number of parameters enhances the informativeness of LLMs inside
    of a LLM family for D2T tasks. Furthermore, Table [6](#S6.T6 "Table 6 ‣ 3rd item
    ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") also indicates that increasing size often leads to enhanced informativeness
    between source and generated text. However, compared to BertScore and MoverScore,
    discrepancies are more pronounced. Some LLMs from BART, OPT, and BLOOM families
    exhibit instances where increasing parameters may degrade Parent scores (see Table [6](#S6.T6
    "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")). Therefore, it is apparent that enlarging
    model size may occasionally diminish the informativeness inside of a LLM family
    when informativeness evaluated between source and generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.3 Faithfulness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Faithfulness is a crucial aspect of D2T model (Wang et al., [2020](#bib.bib109);
    Ji et al., [2023](#bib.bib39)), primarily assessing factual consistency or accuracy
    of the generated text compared to the provided source data. It can be characterized
    by two cases: i) the model generating incorrect facts compared to the given source
    data, and ii) the model generating irrelevant facts that do not appear in the
    source data. Figure [7](#S6.F7 "Figure 7 ‣ 6.3 Faithfulness ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") illustrates an instance of unfaithful
    generation on the E2E dataset. To quantify faithfulness of a D2T model, we employ
    the BartScore metric, which utilizes the likelihood of a BART model for estimating
    faithfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e28e3921a6932cdd6081f5592baca82e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An illustration of unfaithful generation on the E2E dataset. The
    generated text contains the phrase “Highly price”, which is irrelevant with respect
    to the information provided in the source data.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BartScore. BartScore (Yuan et al., [2021](#bib.bib118)) evaluates the faithfulness
    between two texts by estimating the likelihood of text generation based on a sequence-to-sequence
    model like BART. Its premise is that if two pieces of text are factually equivalent,
    the likelihood of generating one from the other will be higher. Faithfulness in
    BartScore is determined by the probability (equation [6](#S6.E6 "In 1st item ‣
    6.3 Faithfulness ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"))
    of transforming from the source text ($s$).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Faithfulness}(s,g)$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (6) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: BartScore has exhibited superior performance compared to several earlier evaluation
    metrics. To calculate BartScore, we employ a pre-trained BARTScore model trained
    on the PARABANK dataset (Yuan et al., [2021](#bib.bib118)). It’s essential to
    note that when dealing with source ($s$ through text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| family | model |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '&#124; size (number of &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '&#124; parameters in billion) &#124;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART | BART-base | 0.1 | 0.088 | 0.047 | 0.066 | 0.061 | 0.097 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BART-large | 0.4 | 0.092 | 0.046 | 0.066 | 0.065 | 0.094 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5 | T5-base | 0.2 | 0.091 | 0.052 | 0.072 | 0.067 | 0.093 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| T5-large | 0.7 | 0.093 | 0.048 | 0.072 | 0.068 | 0.095 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT | OPT-2.7B | 2.7 | 0.057 | 0.045 | 0.068 | 0.063 | 0.089 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-6.7B | 6.7 | 0.053 | 0.046 | 0.066 | 0.062 | 0.087 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| OPT-13B | 13.0 | 0.050 | 0.043 | 0.065 | 0.063 | 0.087 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.089 | 0.042 | 0.057 | 0.064 | 0.090 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-3B | 3.0 | 0.083 | 0.041 | 0.054 | 0.064 | 0.089 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| BLOOM-7B | 7.0 | 0.086 | 0.040 | 0.056 | 0.063 | 0.088 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.094 | 0.05 | 0.062 | 0.073 | 0.106 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Llama2-13B | 13.0 | 0.089 | 0.048 | 0.061 | 0.074 | 0.105 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 7: Assessment of faithfulness across 12 LLMs from five families (BERT,
    T5, BLOOM, OPT, and Llama 2) on the five D2T datasets, using probabilities from
    the BartScore metric (noting that some studies report BartScore using logits).
    The best-performing model sizes within each family are highlighted in bold. Unlike
    readability and informativeness, BartScore shows an inverse trend for faithfulness.
    In most LLM families, increasing model size correlates with a decrease in BartScore,
    indicating reduced faithfulness in larger models. All results are statistically
    significant at $p<0.05$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Takeaways.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Upon scrutinizing the empirical findings presented in Table [7](#S6.T7 "Table
    7 ‣ 1st item ‣ 6.3 Faithfulness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"), we are now equipped to address faithfulness part of the first
    question posed in Section [2](#S2 "2 Research Questions and Motivations ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") concerning the faithfulness of D2T models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: What are the impacts of model size within a family of fine-tuned
    LLMs on the performance of data-to-text (D2T) tasks, in terms of the readability,
    informativeness, and faithfulness?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: No, an increase in the number of parameters (i.e., model size) will
    not necessarily improve the faithfulness of LLMs inside of a LLM family for D2T
    tasks. In fact, increasing parameters can degrade the performance of LLMs in terms
    of faithfulness. Table [7](#S6.T7 "Table 7 ‣ 1st item ‣ 6.3 Faithfulness ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation") clearly suggests that
    for LLMs from BLOOM, OPT, and Llama 2 families, increasing their parameters adversely
    affects their faithfulness aspect for all D2T datasets. Although we have seen
    a very small number of discrepancies (five cases with a very low margin) in T5
    and BART families (for the E2E and DART datasets) and Llama 2 family (for the
    DART dataset), they still indicate that faithfulness often degrades with increasing
    parameters on other datasets. Hence, we can certainly claim that increasing the
    number of parameters will not be helpful for enhancing the faithfulness of LLMs
    toward D2T.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.4 Comparative Performance Analysis across LLM Families
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Thus far, our focus has been primarily on analyzing the performance of LLMs
    within individual LLM families. Now, our attention shifts towards the comparative
    performance analysis of LLMs across different LLM families. Considering all six
    metrics’ tables, Table  [2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation"),  [3](#S6.T3 "Table
    3 ‣ 2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"),  [4](#S6.T4 "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation"),  [5](#S6.T5 "Table
    5 ‣ 2nd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"),  [6](#S6.T6 "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation"), and  [7](#S6.T7 "Table
    7 ‣ 1st item ‣ 6.3 Faithfulness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"), we can effectively address the second question raised in Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Do larger LLM families (such as OPT, BLOOM, Llama 2, etc.) convincingly
    outperform smaller LLM families (such as BART, T5, etc.) in terms of D2T task
    performance?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: From the results of all six automatic metrics, there is no significant
    evidence to conclusively state that larger LLM families (from BLOOM, OPT, and
    Llama 2 families) outperform smaller LLM families (from BART and T5). While Llama
    2 family shows significant improvement over T5 and BART families in terms of readability
    and informativeness, it falls short compared to T5-large model of T5 family in
    terms of faithfulness. On the other hand, both OPTs and BLOOMs families perform
    lower than the T5 family across all three quality aspects. Consequently, it can
    be inferred that LLM from smaller model sized family, such as T5-large model,
    can surprisingly perform quite well in D2T tasks. Moreover, their main advantage
    lies in their lower computational cost compared to larger LLM families like Llama
    2. Especially for faithful D2T tasks (Wang et al., [2020](#bib.bib109); Ji et al.,
    [2023](#bib.bib39)), it is preferable to utilize such smaller LLM families.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.5 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the empirical results of all six automatic metrics on three important qualities
    (readability, informativeness, and faithfulness), we have seen that increasing
    the model parameters in LLMs inside of an LLM family can boost readability and
    informativeness, but it tends to reduce the faithfulness of the D2T task. Therefore,
    in scenarios where faithfulness is crucial, such as in safety-critical applications
    like medical report generation (Pauws et al., [2019](#bib.bib79); Hommes et al.,
    [2019](#bib.bib36); Nishino et al., [2020](#bib.bib74)), it is advisable to use
    a LLM from smaller model-sized LLM family in D2T applications. However, in cases
    where readability (fluency and coherence) and informativeness are paramount, increasing
    parameters is a viable approach.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Analyzing the Effect of Source-Reference Divergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in Subsection [4.4](#S4.SS4 "4.4 Source-Reference Divergence ‣
    4 Preliminaries ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation"), source-reference divergence is
    a common phenomenon in D2T tasks and is unavoidable in D2T datasets (Dhingra et al.,
    [2019](#bib.bib18); Wiseman et al., [2017](#bib.bib111); ul Islam et al., [2023](#bib.bib105)).
    In this analysis, we examine two aspects: how LLMs perform in D2T tasks when source-reference
    divergence is present, and whether the number of parameters in LLMs affects their
    performance in the context of such source-reference divergence. For this analysis,
    we consider three LLM families—T5, BLOOM, and Llama 2—along with six LLMs: T5-base,
    T5-large, BLOOM-3B, BLOOM-7B, Llama2-7B, and Llama2-13B. Additionally, we incorporate
    all three types of D2T tasks, using one dataset for each type: E2E for MR-to-Text,
    WikiTableText for Table-to-Text, and DART for Graph-to-Text. We evaluate the performance
    of these models based on the three important qualities of D2T: readability (measured
    by Bleu), informativeness (measured by MoverScore and Parent), and faithfulness
    (measured by BartScore). To provide a clearer illustration of the impact of source-reference
    divergence, we categorize the test partitions of each dataset into three groups
    based on their average source-reference divergence: low, medium, and high. The
    low group consists of instances with low source-reference divergences, while the
    high group comprises instances with high source-reference divergences. Source-reference
    divergence ($\text{div}(s,r)$) at the unigram level, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{div}(s,r)=1-\frac{\big{&#124;}\text{unigram}(s)\cap\text{unigram}(r)\big{&#124;}}{\big{&#124;}\text{unigram}(s)\cup\text{unigram}(r)\big{&#124;}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Where $|x|$ often consists of semi-structured non-textual data, where higher-order
    grams may overlook significant portions of information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/923c614c190eb0aefdaf32f768e72020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Analysis of performance of T5 in context of source reference divergence
    based on three datasets (E2E, WikiTableText and DART) in terms of Bleu, Parent,
    BartScore and MoverScore. First row represents performance of T5-base, and second
    row represents performance of T5-large.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Influence of Source-Reference Divergence on LLM Performance for D2T
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part of analysis, we investigated the performance of LLMs under source-reference
    divergence. Figures [8](#S7.F8 "Figure 8 ‣ 7 Analyzing the Effect of Source-Reference
    Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation"),  [8](#S7.F8 "Figure 8 ‣ 7 Analyzing
    the Effect of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"),
    and [10](#S7.F10 "Figure 10 ‣ 7.1 Influence of Source-Reference Divergence on
    LLM Performance for D2T ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") illustrate the performance of T5, BLOOM, and
    Llama 2 across three D2T datasets in the context of source-reference divergence.
    These figures provide insights to address the first part of the third question
    raised in Section [2](#S2 "2 Research Questions and Motivations ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eebb319f82b94ba54a9fcf63ad1eb3f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Analysis of performance of BLOOM in context of source reference divergence
    based on three datasets (E2E, WikiTableText and DART) in terms of Bleu, Parent,
    BartScore and MoverScore. First row represents performance of BLOOM-3B, and second
    row represents performance of BLOOM-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Does the presence of source-reference divergence influence the performance
    of LLMs for D2T tasks? If so, does increasing the model size of LLM aid in mitigating
    the effects of source-reference divergence?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: The analysis of Figures [8](#S7.F8 "Figure 8 ‣ 7 Analyzing the Effect
    of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation"),  [9](#S7.F9 "Figure
    9 ‣ 7.1 Influence of Source-Reference Divergence on LLM Performance for D2T ‣
    7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size on
    Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"),
    and  [10](#S7.F10 "Figure 10 ‣ 7.1 Influence of Source-Reference Divergence on
    LLM Performance for D2T ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") reveals a significant influence of source-reference
    divergence on the performance of LLMs across all three LLM families. Across the
    low, medium, and high divergence groups, distinct performance trends of LLMs emerge.
    In the low divergence group, characterized by minimal source-reference discrepancies,
    all LLM models consistently exhibit higher scores across the four evaluated metrics
    (Bleu, Parent, MoverScore, and BartScore). This suggests that when the source-reference
    divergence is low, LLMs tend to perform better in terms of readability, informativeness,
    and faithfulness. Conversely, in the high divergence group where source-reference
    discrepancies are more pronounced, all LLMs demonstrate notably lower performance
    across the metrics compared to the low and medium groups. This observation underscores
    the significant impact of source-reference divergence on LLM performance, with
    higher levels of divergence leading to decreased performance across the evaluated
    qualities. In summary, these findings confirm that LLM performance is indeed influenced
    by source-reference divergence, with lower levels of divergence consistently associated
    with superior performance across the evaluated metrics. So, despite the claims
    of LLMs to possess greater generalization and knowledge, their performance in
    D2T tasks remains inadequate when source-reference divergence is present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b76824bec3ebeb3f679eb2630e63124c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Analysis of performance of Llama 2 in context of source reference
    divergence based on three datasets (E2E, WikiTableText and DART) in terms of Bleu,
    Parent, BartScore and MoverScore. First row represents performance of Llama2-7B,
    and second row represents performance of Llama2-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Impact of model Size on LLM Performance for D2T in the Presence of Source-Reference
    Divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we conduct an analysis to determine the influence of underlying
    parameter numbers (or simply size) on the performance of LLMs in the context of
    source-reference divergence. To facilitate this analysis, we follow a similar
    methodology as in our previous analysis, segmenting the entire test set into three
    groups (low, medium, and high) based on source-reference divergence. For comparison
    purposes, we juxtapose two LLMs in a single plot, specifically focusing on three
    comparisons: Llama2-7B vs T5-base, Llama2-13B vs T5-base, and BLOOM-7B vs T5-base.
    These comparisons enable us to juxtapose the performance of three larger LLMs
    (BLOOM-7B, Llama2-7B, and Llama2-13B) against that of a relatively smaller LLM,
    T5-base. We analyze these comparisons across four metrics (Bleu, Parent, MoverScore,
    and BartScore), encompassing all three key qualities of D2T models over the three
    D2T datasets—E2E, WikiTableText, and DART. Through these comparative analyses,
    we aim to address the final part of the third question raised in Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation"), shedding
    light on the relationship between LLM parameter numbers and performance in the
    presence of source-reference divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/40ce5e6dfbece3644f6b4d8fb5189327.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Comparative performance analysis between T5-base and BLOOM-7B in
    context of source reference divergence based on three datasets (E2E, WikiTableText
    and DART) in terms of Bleu, Parent, BartScore and MoverScore.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: Does the presence of source-reference divergence influence the performance
    of LLMs for D2T tasks? If so, does increasing the model size of LLM aid in mitigating
    the effects of source-reference divergence?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd50c9e963133f82ddc8000e3c129c22.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 12: Comparative performance analysis between T5-base and Llama2-7B in
    context of source reference divergence based on three datasets (E2E, WikiTableText
    and DART) in terms of Bleu, Parent, BartScore and MoverScore.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6b15937300b7b160a110a27bb13e5f0.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 13: Comparative performance analysis between T5-base and Llama2-13B
    in context of source reference divergence based on three datasets (E2E, WikiTableText
    and DART) in terms of Bleu, Parent, BartScore and MoverScore.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Answer: From the comparison plots [11](#S7.F11 "Figure 11 ‣ 7.2 Impact of model
    Size on LLM Performance for D2T in the Presence of Source-Reference Divergence
    ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"),
     [12](#S7.F12 "Figure 12 ‣ 7.2 Impact of model Size on LLM Performance for D2T
    in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference
    Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation"), and  [13](#S7.F13 "Figure 13 ‣
    7.2 Impact of model Size on LLM Performance for D2T in the Presence of Source-Reference
    Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"), it becomes apparent that LLMs with larger parameters (Llama2-13B,
    Llama2-7B, and BLOOM-7B) tend to outperform low-parameter models like T5-base
    in D2T tasks when source-reference divergence is low. However, there are some
    exceptions, particularly in comparisons with the BLOOM-7B and T5-base models.
    As the source-reference divergence rate rises (i.e., in the medium group of source-reference
    divergence), T5-base, being a low-parameter model, begins to perform well and
    competes with the larger LLMs. Finally, when the source-reference divergence becomes
    high, T5-base often either outperforms the larger LLMs (Llama2-13B, Llama2-7B,
    and BLOOM-7B) in almost all metrics or significantly narrows the performance gap
    with them. Although there are some exceptional cases where T5-base cannot outperform
    LLMs, this is primarily observed with the WikiTableText dataset. Nevertheless,
    in the medium group of source-reference divergence in the WikiTableText dataset,
    the T5-base model often performs better than all three larger LLMs. These observations
    indicate that when source-reference divergence is high, it is advisable to use
    smaller LLMs like T5-base for better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| source | reference | LLM | generated text | remarks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| name=The Vaults; eatType = pub; food=French; priceRange=less than £20; area=city
    centre; familyFriendly=yes | Near Raja Cuisine, in the city centre is a pub offering
    French food. The Vaults is cheap and family-friendly. | T5-base |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Vaults is a family-friendly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; French pub in the city centre &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with a price range of less than £20. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| All LLMs are successfully generated similar texts, however T5-base transformed
    with ‘food=French’ into ‘French pub’, mistakenly. |'
  prefs: []
  type: TYPE_TB
- en: '| T5-large |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Vaults is a family-friendly pub &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in the city centre that &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; serves French food for less than £20. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama2-7B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Vaults is a family-friendly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pub that serves French food. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It is located in the city centre &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and has a price range of less than £20. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama2-13B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Vaults is a family-friendly pub that &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; serves French food for less than £20. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It is located in the city centre. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| name=The Waterman; eatType=restaurant; priceRange=high; area=riverside; familyFriendly=yes;
    near=Raja Indian Cuisine | The Waterman is a restaurant near to Raja Indian Cuisine
    that is child-friendly. It has a high price range but is located in riverside.
    | T5-base |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Waterman is a high priced, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; child friendly restaurant in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the riverside area near Raja &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Indian Cuisine. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| In this example, there is a discrepancy between the source data and the reference.
    The source data mentions familyFriendly=yes, while the reference uses the term
    ‘child-friendly’. Interestingly, all three LLMs, except Llama2-13B, successfully
    identify this discrepancy. This case highlights the possibility that larger LLMs
    may overlook certain divergences in the data. |'
  prefs: []
  type: TYPE_TB
- en: '| T5-large |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Waterman is a child friendly &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; restaurant in the riverside area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; near Raja Indian Cuisine. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It has a high price range. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama2-7B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Waterman is a high priced &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; restaurant in the riverside area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; near Raja Indian Cuisine. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It is child friendly. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama2-13B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The Waterman is a high priced family &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; friendly restaurant in the riverside area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; near Raja Indian Cuisine. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Case study on the E2E dataset featuring four LLMs (T5-base, T5-large,
    Llama2-7B, and Llama2-13B), illustrating errors made by smaller LLMs such as T5-base
    and instances where larger LLMs like Llama2-13B fail to identify source-reference
    divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '| source | reference | LLM | generated text | remarks |'
  prefs: []
  type: TYPE_TB
- en: '| [[’subjtitle’, ’history of the jews in malaysia’], [’subjsubtitle’, ’history’],
    [’year’, ’1881’], [’males’, ’14’], [’females’, ’9’], [’boys’, ’5’], [’girls’,
    ’4’], [’total’, ’32’]] | in 1988 , there were 32 the jews in malaysia in which
    there were 14 males , 9 females , 5 boys and 4 girls . | T5-base | in 1881, there
    were 32 jews in malaysia. | All LLMs are experiencing issues with informativeness
    as they struggle to generate all the source content accurately. In this example,
    larger LLMs show improved informativeness, with Llama2-13B achieving comparatively
    better results. |'
  prefs: []
  type: TYPE_TB
- en: '| T5-large |'
  prefs: []
  type: TYPE_TB
- en: '&#124; in 1881, there were 14 males and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 9 females in malaysia. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama2-7B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; there were 14 males and 9 females &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in the jews in malaysia in 1881 . &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Llama2-13B |'
  prefs: []
  type: TYPE_TB
- en: '&#124; there were 14 males , 9 females , &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 5 boys and 4 girls in the jews &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in malaysia in 1881 . &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[’subjtitle’, ’bath rugby’], [’subjsubtitle’, ’current squad’], [’player’,
    ’jack wilson’], [’position’, ’wing’], [’union’, ’new zealand’]] | in bath rugby
    , wing jack wilson was a member of new zealand union . | T5-base | jack wilson
    was a wing in new zealand. | While T5-base and T5-large provide faithful information,
    Llama2-7B and Llama2-13B outperform them in terms of readability. |'
  prefs: []
  type: TYPE_TB
- en: '| T5-large | jack wilson was from union new zealand. |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7B | jack wilson was from new zealand . |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13B | jack wilson was from new zealand . |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Case study on the WikiTableText dataset featuring four LLMs (T5-base,
    T5-large, Llama2-7B, and Llama2-13B), demonstrating how increasing LLM size enhances
    informativeness, while smaller LLMs like T5-base and T5-large generate faithful
    but less readable content compared to larger LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Case Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We present two case studies (Table [8](#S7.T8 "Table 8 ‣ 7.2 Impact of model
    Size on LLM Performance for D2T in the Presence of Source-Reference Divergence
    ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")
    and [9](#S7.T9 "Table 9 ‣ 7.2 Impact of model Size on LLM Performance for D2T
    in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference
    Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")) conducted on the E2E and WikiTableText
    datasets, each showcasing two samples and their corresponding generations from
    four LLMs: T5-base, T5-large, Llama2-7B, and Llama2-13B. In the first case (Table [8](#S7.T8
    "Table 8 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")) study using the E2E dataset, one example illustrates
    how smaller LLMs like T5-base may fail to accurately convey information. Another
    example highlights a source-reference divergence phenomenon, where larger LLMs
    like Llama2-13B fail to capture the divergence, while other LLMs successfully
    handle it. Therefore, Table [8](#S7.T8 "Table 8 ‣ 7.2 Impact of model Size on
    LLM Performance for D2T in the Presence of Source-Reference Divergence ‣ 7 Analyzing
    the Effect of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")
    demonstrates that smaller-sized LLMs may sacrifice informativeness, while larger
    LLMs frequently struggle to maintain source-reference divergence. The second case
    study (Table [9](#S7.T9 "Table 9 ‣ 7.2 Impact of model Size on LLM Performance
    for D2T in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect
    of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")) on the WikiTableText
    dataset demonstrates how increasing LLM size enhances informativeness as a D2T
    model. Additionally, it reveals that smaller LLMs such as T5-base and T5-large
    tend to prioritize faithfulness, while larger LLMs prioritize readability. Table [9](#S7.T9
    "Table 9 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") reveals that smaller-sized LLMs prioritize
    faithfulness, rendering them more suitable for applications requiring safety-critical
    considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper investigates the impact of model size (number of model parameters)
    on the performance of fine-tuned LLMs in D2T generation tasks, focusing on three
    key qualities: readability, informativeness, and faithfulness. Despite its significant
    importance, this investigation has been entirely overlooked in the existing literature
    on LLMs for D2T applications. We conducted a comprehensive assessment involving
    twelve LLMs drawn from five popular LLM families: T5, BART, OPT, BLOOM, and Llama
    2. Our analysis encompasses three primary D2T task categories: graph-to-text,
    table-to-text, and MR-to-text. Six widely recognized evaluation metrics to ensure
    a thorough examination of D2T models: BLEU, METEOR, BERTScore, MoverScore, Parent,
    and BARTScore. Given the inherent risks associated with human evaluation (Freitag
    et al., [2021](#bib.bib25)), such as inter-annotator knowledge disparities and
    cognitive biases, we have opted not to include human evaluation results in our
    study to ensure unbiased comparisons. Nonetheless, two case studies are presented
    to enhance clarity in our analyses. Moreover, we examine a crucial aspect of D2T
    tasks: source-reference divergence, which presents challenges in generating a
    reference from the source data. Our primary findings can be summarized into three
    main points. Firstly, we find that increasing the parameters of large language
    models (LLMs) within a given LLM family generally improves readability and informativeness
    in data-to-text (D2T) tasks. However, this often comes at the expense of faithfulness
    in such tasks, suggesting a preference for smaller LLMs in safety-critical D2T
    domains. Secondly, we demonstrate that larger-sized LLM families do not consistently
    outperform smaller ones in D2T task performance, challenging the notion that bigger
    models always yield better results. Lastly, in the context of source-reference
    divergence, we meticulously observe that the performance of all LLMs is negatively
    affected. However, smaller LLMs exhibit greater resilience compared to their larger
    counterparts in dealing with source-reference divergence. We firmly believe that
    our research offers a profound understanding of the nuanced interplay between
    model size and LLM performance in D2T tasks, furnishing valuable insights to optimize
    the utilization of LLMs across diverse D2T applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is partially supported by the Indo-French Centre for the Promotion
    of Advanced Research (IFCPAR/CEFIPRA) through CSRP Project No. 6702-2.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aghajanyan et al. (2021) Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.
    Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
    In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, *Proceedings
    of the ACL/IJCNLP*, pages 7319–7328\. Association for Computational Linguistics,
    2021. URL [https://doi.org/10.18653/v1/2021.acl-long.568](https://doi.org/10.18653/v1/2021.acl-long.568).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angeli et al. (2010) Gabor Angeli, Percy Liang, and Dan Klein. A simple domain-independent
    probabilistic approach to generation. In *Proceedings of the EMNLP*, pages 502–512\.
    Association for Computational Linguistics, 2010. URL [https://aclanthology.org/D10-1049/](https://aclanthology.org/D10-1049/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
    machine translation by jointly learning to align and translate. In Yoshua Bengio
    and Yann LeCun, editors, *Proceedings of the ICLR*, 2015. URL [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. METEOR: an automatic
    metric for MT evaluation with improved correlation with human judgments. In Jade
    Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss, editors, *Proceedings
    of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
    and/or Summarization*, pages 65–72\. Association for Computational Linguistics,
    2005. URL [https://aclanthology.org/W05-0909/](https://aclanthology.org/W05-0909/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2018) Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming
    Zhou, and Tiejun Zhao. Table-to-text: Describing table region with natural language.
    In Sheila A. McIlraith and Kilian Q. Weinberger, editors, *Proceedings of the
    AAAI*, pages 5020–5027\. AAAI Press, 2018. URL [https://doi.org/10.1609/aaai.v32i1.11944](https://doi.org/10.1609/aaai.v32i1.11944).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belz (2008) Anja Belz. Automatic generation of weather forecast texts using
    comprehensive probabilistic generation-space models. *Natural Language Engineering*,
    14(4):431–455, 2008. URL [https://doi.org/10.1017/S1351324907004664](https://doi.org/10.1017/S1351324907004664).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belz and Reiter (2006) Anja Belz and Ehud Reiter. Comparing automatic and human
    evaluation of NLG systems. In Diana McCarthy and Shuly Wintner, editors, *Proceedings
    of the EACL*. The Association for Computer Linguistics, 2006. URL [https://aclanthology.org/E06-1040/](https://aclanthology.org/E06-1040/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Belz et al. (2020) Anya Belz, Simon Mille, and David M. Howcroft. Disentangling
    the properties of human evaluation methods: A classification system to support
    comparability, meta-evaluation and reproducibility testing. In Brian Davis, Yvette
    Graham, John D. Kelleher, and Yaji Sripada, editors, *Proceedings of the INLG*,
    pages 183–194\. Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.inlg-1.24](https://doi.org/10.18653/v1/2020.inlg-1.24).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bickel et al. (2005) Steffen Bickel, Peter Haider, and Tobias Scheffer. Predicting
    sentences using n-gram language models. In *Proceedings of the EMNLP-HLT*, pages
    193–200\. The Association for Computational Linguistics, 2005. URL [https://aclanthology.org/H05-1025/](https://aclanthology.org/H05-1025/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (1992) Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
    Jennifer C. Lai, and Robert L. Mercer. Class-based n-gram models of natural language.
    *Computational Linguistics*, 18(4):467–479, 1992. URL [https://aclanthology.org/J92-4003.pdf](https://aclanthology.org/J92-4003.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio
    Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, *Proceedings
    of the NeurIPS*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budzianowski et al. (2018) Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng,
    Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. Multiwoz - A large-scale
    multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Ellen
    Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, *Proceedings
    of the EMNLP*, pages 5016–5026. Association for Computational Linguistics, 2018.
    URL [https://aclanthology.org/D18-1547/](https://aclanthology.org/D18-1547/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen and Mooney (2008) David L. Chen and Raymond J. Mooney. Learning to sportscast:
    A test of grounded language acquisition. In William W. Cohen, Andrew McCallum,
    and Sam T. Roweis, editors, *Proceedings of the ICML*, volume 307, pages 128–135\.
    Association for Computing Machinery, 2008. URL [https://doi.org/10.1145/1390156.1390173](https://doi.org/10.1145/1390156.1390173).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang,
    Sairam Sundaresan, and William Yang Wang. Logic2text: High-fidelity natural language
    generation from logical forms. In Trevor Cohn, Yulan He, and Yang Liu, editors,
    *Proceedings of the EMNLP Findings*, volume EMNLP 2020, pages 2096–2111\. Association
    for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.findings-emnlp.190](https://doi.org/10.18653/v1/2020.findings-emnlp.190).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways. *Journal of Machine Learning Research*, 24:240:1–240:113, 2023.
    URL [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,
    *Proceedings of the NeurIPS*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, *Proceedings
    of the NAACL-HLT*, pages 4171–4186\. Association for Computational Linguistics,
    2019. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhingra et al. (2019) Bhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh, Ming-Wei
    Chang, Dipanjan Das, and William W. Cohen. Handling divergent reference texts
    when evaluating table-to-text generation. In Anna Korhonen, David R. Traum, and
    Lluís Màrquez, editors, *Proceedings of the ACL*, pages 4884–4895\. Association
    for Computational Linguistics, 2019. URL [https://doi.org/10.18653/v1/p19-1483](https://doi.org/10.18653/v1/p19-1483).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dror et al. (2018) Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart.
    The hitchhiker’s guide to testing statistical significance in natural language
    processing. In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings of the ACL*,
    pages 1383–1392\. Association for Computational Linguistics, 2018. URL [https://aclanthology.org/P18-1128/](https://aclanthology.org/P18-1128/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dusek et al. (2018) Ondrej Dusek, Jekaterina Novikova, and Verena Rieser. Findings
    of the E2E NLG challenge. In Emiel Krahmer, Albert Gatt, and Martijn Goudbeek,
    editors, *Proceedings of the INLG*, pages 322–328\. Association for Computational
    Linguistics, 2018. URL [https://doi.org/10.18653/v1/w18-6539](https://doi.org/10.18653/v1/w18-6539).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Erdem et al. (2022) Erkut Erdem, Menekse Kuyu, Semih Yagcioglu, Anette Frank,
    Letitia Parcalabescu, Barbara Plank, Andrii Babii, Oleksii Turuta, Aykut Erdem,
    Iacer Calixto, Elena Lloret, Elena Simona Apostol, Ciprian-Octavian Truica, Branislava
    Sandrih, Sanda Martincic-Ipsic, Gábor Berend, Albert Gatt, and Grazina Korvel.
    Neural natural language generation: A survey on multilinguality, multimodality,
    controllability and learning. *Journal of Artificial Intelligence Research*, 73:1131–1207,
    2022. URL [https://doi.org/10.1613/jair.1.12918](https://doi.org/10.1613/jair.1.12918).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Faiz et al. (2023) Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek
    Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint
    of large language models. *CoRR*, abs/2309.14393, 2023. URL [https://doi.org/10.48550/arXiv.2309.14393](https://doi.org/10.48550/arXiv.2309.14393).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2018) Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical
    neural story generation. In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings
    of the ACL*, pages 889–898\. Association for Computational Linguistics, 2018.
    URL [https://aclanthology.org/P18-1082/](https://aclanthology.org/P18-1082/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Wu, Graham Neubig, and André F. T. Martins. Bridging the Gap: A Survey on Integrating
    (Human) Feedback for Natural Language Generation. *Transactions of the Association
    for Computational Linguistics*, 11:1643–1668, 2023. ISSN 2307-387X. URL [https://doi.org/10.1162/tacl_a_00626](https://doi.org/10.1162/tacl_a_00626).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freitag et al. (2021) Markus Freitag, George F. Foster, David Grangier, Viresh
    Ratnakar, Qijun Tan, and Wolfgang Macherey. Experts, errors, and context: A large-scale
    study of human evaluation for machine translation. *Transactions of the Association
    for Computational Linguistics*, 9:1460–1474, 2021. URL [https://doi.org/10.1162/tacl_a_00437](https://doi.org/10.1162/tacl_a_00437).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gardent et al. (2017) Claire Gardent, Anastasia Shimorina, Shashi Narayan,
    and Laura Perez-Beltrachini. The webnlg challenge: Generating text from RDF data.
    In José Maria Alonso, Alberto Bugarín, and Ehud Reiter, editors, *Proceedings
    of the INLG*, pages 124–133\. Association for Computational Linguistics, 2017.
    URL [https://doi.org/10.18653/v1/w17-3518](https://doi.org/10.18653/v1/w17-3518).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gatt and Krahmer (2018) Albert Gatt and Emiel Krahmer. Survey of the state
    of the art in natural language generation: Core tasks, applications and evaluation.
    *Journal of Artificial Intelligence Research*, 61:65–170, 2018. URL [https://doi.org/10.1613/jair.5477](https://doi.org/10.1613/jair.5477).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. (2023) Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. Openagi: When LLM meets domain experts.
    In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey
    Levine, editors, *Proceedings of the NeurIPS*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gkatzia (2016) Dimitra Gkatzia. Content selection in data-to-text systems:
    A survey. *CoRR*, abs/1610.08375, 2016. URL [http://arxiv.org/abs/1610.08375](http://arxiv.org/abs/1610.08375).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2019) Heng Gong, Xiaocheng Feng, Bing Qin, and Ting Liu. Table-to-text
    generation with effective hierarchical encoder on three dimensions (row, column
    and time). In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
    *Proceedings of the EMNLP*, pages 3141–3150\. Association for Computational Linguistics,
    2019. URL [https://doi.org/10.18653/v1/D19-1310](https://doi.org/10.18653/v1/D19-1310).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves et al. (2013) Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton.
    Speech recognition with deep recurrent neural networks. In *Proceedings of the
    ICASSP*, pages 6645–6649\. IEEE, 2013. URL [https://doi.org/10.1109/ICASSP.2013.6638947](https://doi.org/10.1109/ICASSP.2013.6638947).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinzerling and Inui (2021) Benjamin Heinzerling and Kentaro Inui. Language
    models as knowledge bases: On entity representations, storage capacity, and paraphrased
    queries. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty, editors, *Proceedings
    of the EACL*, pages 1772–1791\. Association for Computational Linguistics, 2021.
    URL [https://doi.org/10.18653/v1/2021.eacl-main.153](https://doi.org/10.18653/v1/2021.eacl-main.153).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural Computation*, 9(8):1735–1780, 1997. URL [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holtzman et al. (2018) Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut,
    David Golub, and Yejin Choi. Learning to write with cooperative discriminators.
    In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings of the ACL*, pages 1638–1649\.
    Association for Computational Linguistics, 2018. URL [https://aclanthology.org/P18-1152/](https://aclanthology.org/P18-1152/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
    Choi. The curious case of neural text degeneration. In *Proceedings of the ICLR*.
    OpenReview.net, 2020. URL [https://openreview.net/forum?id=rygGQyrFvH](https://openreview.net/forum?id=rygGQyrFvH).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hommes et al. (2019) Saar Hommes, Chris van der Lee, Felix J. Clouth, Jeroen K.
    Vermunt, Xander Verbeek, and Emiel Krahmer. A personalized data-to-text support
    tool for cancer patients. In Kees van Deemter, Chenghua Lin, and Hiroya Takamura,
    editors, *Proceedings of the INLG*, pages 443–452\. Association for Computational
    Linguistics, 2019. URL [https://aclanthology.org/W19-8656/](https://aclanthology.org/W19-8656/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. Universal language
    model fine-tuning for text classification. In Iryna Gurevych and Yusuke Miyao,
    editors, *Proceedings of the ACL*, pages 328–339\. Association for Computational
    Linguistics, 2018. URL [https://aclanthology.org/P18-1031/](https://aclanthology.org/P18-1031/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. In *Proceedings of the ICLR*. OpenReview.net, 2022. URL
    [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination
    in natural language generation. *ACM Computing Surveys*, 55(12):248:1–248:38,
    2023. URL [https://doi.org/10.1145/3571730](https://doi.org/10.1145/3571730).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Peng Jiang, Christian Sonne, Wangliang Li, Fengqi You, and
    Siming You. Preventing the immense increase in the life-cycle energy and carbon
    footprints of llm-powered intelligent chatbots. *Engineering*, 2024. ISSN 2095-8099.
    URL [https://www.sciencedirect.com/science/article/pii/S2095809924002315](https://www.sciencedirect.com/science/article/pii/S2095809924002315).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jing et al. (2024) Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou Zhao,
    Wei Zhou, and Liqiang Nie. Stylized data-to-text generation: A case study in the
    e-commerce domain. *ACM Transactions on Information Systems*, 42(1):25:1–25:24,
    2024. URL [https://doi.org/10.1145/3603374](https://doi.org/10.1145/3603374).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Juraska et al. (2018) Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, and
    Marilyn A. Walker. A deep ensemble model with slot alignment for sequence-to-sequence
    natural language generation. In Marilyn A. Walker, Heng Ji, and Amanda Stent,
    editors, *Proceedings of the NAACL-HLT*, pages 152–162\. Association for Computational
    Linguistics, 2018. URL [https://doi.org/10.18653/v1/n18-1014](https://doi.org/10.18653/v1/n18-1014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Juraska et al. (2019) Juraj Juraska, Kevin Bowden, and Marilyn A. Walker. Viggo:
    A video game corpus for data-to-text generation in open-domain conversation. In
    Kees van Deemter, Chenghua Lin, and Hiroya Takamura, editors, *Proceedings of
    the INLG*, pages 164–172\. Association for Computational Linguistics, 2019. URL
    [https://aclanthology.org/W19-8623/](https://aclanthology.org/W19-8623/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpathy and Fei-Fei (2017) Andrej Karpathy and Li Fei-Fei. Deep visual-semantic
    alignments for generating image descriptions. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 39(4):664–676, 2017. URL [https://doi.org/10.1109/TPAMI.2016.2598339](https://doi.org/10.1109/TPAMI.2016.2598339).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kasner and Dusek (2024) Zdenek Kasner and Ondrej Dusek. Beyond reference-based
    metrics: Analyzing behaviors of open llms on data-to-text generation. *CoRR*,
    abs/2401.10186, 2024. URL [https://doi.org/10.48550/arXiv.2401.10186](https://doi.org/10.48550/arXiv.2401.10186).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kasner et al. (2023) Zdenek Kasner, Ekaterina Garanina, Ondrej Plátek, and
    Ondrej Dusek. Tabgenie: A toolkit for table-to-text generation. In Danushka Bollegala,
    Ruihong Huang, and Alan Ritter, editors, *Proceedings of the ACL*, pages 444–455\.
    Association for Computational Linguistics, 2023. URL [https://doi.org/10.18653/v1/2023.acl-demo.42](https://doi.org/10.18653/v1/2023.acl-demo.42).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kedzie and McKeown (2020) Chris Kedzie and Kathleen R. McKeown. Controllable
    meaning representation to text generation: Linearization and data augmentation
    strategies. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings
    of the EMNLP*, pages 5160–5185\. Association for Computational Linguistics, 2020.
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.419](https://doi.org/10.18653/v1/2020.emnlp-main.419).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In Yoshua Bengio and Yann LeCun, editors, *Proceedings of the ICLR*,
    2015. URL [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lebret et al. (2016) Rémi Lebret, David Grangier, and Michael Auli. Neural text
    generation from structured data with application to the biography domain. In Jian
    Su, Xavier Carreras, and Kevin Duh, editors, *Proceedings of the EMNLP*, pages
    1203–1213\. Association for Computational Linguistics, 2016. URL [https://doi.org/10.18653/v1/d16-1128](https://doi.org/10.18653/v1/d16-1128).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing
    Huang, Lucia Specia, and Scott Wen-tau Yih, editors, *Proceedings of the EMNLP*,
    pages 3045–3059\. Association for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.emnlp-main.243](https://doi.org/10.18653/v1/2021.emnlp-main.243).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
    denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault, editors, *Proceedings of the ACL*, pages 7871–7880\. Association for
    Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024a) Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari.
    Toward sustainable genai using generation directives for carbon-friendly large
    language model inference. *CoRR*, abs/2403.12900, 2024a. URL [https://doi.org/10.48550/arXiv.2403.12900](https://doi.org/10.48550/arXiv.2403.12900).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024b) Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu
    Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, et al. Unifying structured data
    as graph for data-to-text pre-training. *Transactions of the Association for Computational
    Linguistics*, 12:210–228, 2024b. ISSN 2307-387X. URL [https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and
    Hua Wu. Faithfulness in natural language generation: A systematic survey of analysis,
    evaluation and optimization methods. *CoRR*, abs/2203.05227, 2022. URL [https://doi.org/10.48550/arXiv.2203.05227](https://doi.org/10.48550/arXiv.2203.05227).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli, editors, *Proceedings of the ACL/IJCNLP*, pages 4582–4597\. Association
    for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.acl-long.353](https://doi.org/10.18653/v1/2021.acl-long.353).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2024) Yupian Lin, Tong Ruan, Jingping Liu, and Haofen Wang. A survey
    on neural data-to-text generation. *IEEE Transactions on Knowledge and Data Engineering*,
    36(4):1431–1449, 2024. URL [https://doi.org/10.1109/TKDE.2023.3304385](https://doi.org/10.1109/TKDE.2023.3304385).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang
    Sui. Table-to-text generation by structure-aware seq2seq learning. In Sheila A.
    McIlraith and Kilian Q. Weinberger, editors, *Proceedings of the AAAI*, pages
    4881–4888\. AAAI Press, 2018. URL [https://doi.org/10.1609/aaai.v32i1.11925](https://doi.org/10.1609/aaai.v32i1.11925).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning
    across scales and tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,
    editors, *Proceedings of the ACL*, pages 61–68\. Association for Computational
    Linguistics, 2022. URL [https://doi.org/10.18653/v1/2022.acl-short.8](https://doi.org/10.18653/v1/2022.acl-short.8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized BERT pretraining approach. *CoRR*, abs/1907.11692, 2019.
    URL [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lorandi and Belz (2024) Michela Lorandi and Anya Belz. High-quality data-to-text
    generation for severely under-resourced languages with out-of-the-box large language
    models. In Yvette Graham and Matthew Purver, editors, *Proceedings of the EACL
    Findings*, pages 1451–1461\. Association for Computational Linguistics, 2024.
    URL [https://aclanthology.org/2024.findings-eacl.98](https://aclanthology.org/2024.findings-eacl.98).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *Proceedings of the ICLR*. OpenReview.net, 2019. URL
    [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2018) Sidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, and Yong Yu.
    Neural text generation: Past, present and beyond. *CoRR*, abs/1803.07133, 2018.
    URL [http://arxiv.org/abs/1803.07133](http://arxiv.org/abs/1803.07133).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu and Ng (2011) Wei Lu and Hwee Tou Ng. A probabilistic forest-to-string model
    for language generation from typed lambda calculus expressions. In *Proceedings
    of the EMNLP*, pages 1611–1622\. Association for Computational Linguistics, 2011.
    URL [https://aclanthology.org/D11-1149/](https://aclanthology.org/D11-1149/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luccioni et al. (2023) Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure
    Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model.
    *Journal of Machine Learning Research*, 24:253:1–253:15, 2023. URL [http://jmlr.org/papers/v24/23-0069.html](http://jmlr.org/papers/v24/23-0069.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023) Wei Er Luo, Xi Yue, and Chao Tie Zhong. Data-to-text generation
    with data control and multi-loss fusion. In *Proceedings of the FAIML*, pages
    244–247\. Association for Computing Machinery, 2023. URL [https://doi.org/10.1145/3616901.3616995](https://doi.org/10.1145/3616901.3616995).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luong et al. (2015) Thang Luong, Hieu Pham, and Christopher D. Manning. Effective
    approaches to attention-based neural machine translation. In Lluís Màrquez, Chris
    Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, *Proceedings
    of the EMNLP*, pages 1412–1421\. Association for Computational Linguistics, 2015.
    URL [https://doi.org/10.18653/v1/d15-1166](https://doi.org/10.18653/v1/d15-1166).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient
    fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mei et al. (2016) Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. What to
    talk about and how? selective generation using lstms with coarse-to-fine alignment.
    In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, *Proceedings of the NAACL-HLT*,
    pages 720–730\. Association for Computational Linguistics, 2016. URL [https://doi.org/10.18653/v1/n16-1086](https://doi.org/10.18653/v1/n16-1086).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2010) Tomás Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký,
    and Sanjeev Khudanpur. Recurrent neural network based language model. In Takao
    Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, *Proceedings of the
    Interspeech*, pages 1045–1048\. ISCA, 2010. URL [https://doi.org/10.21437/Interspeech.2010-343](https://doi.org/10.21437/Interspeech.2010-343).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nan et al. (2021) Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand
    Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,
    Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia
    Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming
    Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: open-domain structured
    data record to text generation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,
    Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
    and Yichao Zhou, editors, *Proceedings of the NAACL-HLT*, pages 432–447\. Association
    for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.naacl-main.37](https://doi.org/10.18653/v1/2021.naacl-main.37).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nema et al. (2018) Preksha Nema, Shreyas Shetty, Parag Jain, Anirban Laha, Karthik
    Sankaranarayanan, and Mitesh M. Khapra. Generating descriptions from structured
    data using a bifocal attention mechanism and gated orthogonalization. In Marilyn A.
    Walker, Heng Ji, and Amanda Stent, editors, *Proceedings of the NAACL-HLT*, pages
    1539–1550\. Association for Computational Linguistics, 2018. URL [https://doi.org/10.18653/v1/n18-1139](https://doi.org/10.18653/v1/n18-1139).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2019a) Feng Nie, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. An encoder
    with non-sequential dependency for neural data-to-text generation. In Kees van
    Deemter, Chenghua Lin, and Hiroya Takamura, editors, *Proceedings of the INLG*,
    pages 141–146\. Association for Computational Linguistics, 2019a. URL [https://aclanthology.org/W19-8619/](https://aclanthology.org/W19-8619/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2019b) Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew
    Lin. A simple recipe towards reducing hallucination in neural surface realisation.
    In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, *Proceedings of
    the ACL*, pages 2673–2679\. Association for Computational Linguistics, 2019b.
    URL [https://doi.org/10.18653/v1/p19-1256](https://doi.org/10.18653/v1/p19-1256).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nishino et al. (2020) Toru Nishino, Ryota Ozaki, Yohei Momoki, Tomoki Taniguchi,
    Ryuji Kano, Norihisa Nakano, Yuki Tagawa, Motoki Taniguchi, Tomoko Ohkuma, and
    Keigo Nakamura. Reinforcement learning with imbalanced dataset for data-to-text
    medical report generation. In Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings
    of the EMNLP Findings*, volume EMNLP 2020, pages 2223–2236\. Association for Computational
    Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.findings-emnlp.202](https://doi.org/10.18653/v1/2020.findings-emnlp.202).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novikova et al. (2017) Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
    The E2E dataset: New challenges for end-to-end generation. In Kristiina Jokinen,
    Manfred Stede, David DeVault, and Annie Louis, editors, *Proceedings of the SIGDIAL*,
    pages 201–206\. Association for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/w17-5525](https://doi.org/10.18653/v1/w17-5525).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training
    language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed,
    A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, *Proceedings of the
    NeurIPS*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: A method for automatic evaluation of machine translation. In *Proceedings
    of the ACL*, pages 311–318\. Association for Computational Linguistics, 2002.
    URL [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pauls and Klein (2011) Adam Pauls and Dan Klein. Faster and smaller n-gram language
    models. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, *Proceedings
    of the ACL-HLT*, pages 258–267\. Association for Computer Linguistics, 2011. URL
    [https://aclanthology.org/P11-1027/](https://aclanthology.org/P11-1027/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pauws et al. (2019) Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter.
    Making effective use of healthcare data using data-to-text technology. In Sergio
    Consoli, Diego Reforgiato Recupero, and Milan Petkovic, editors, *Data Science
    for Healthcare - Methodologies and Applications*, pages 119–145\. Springer, 2019.
    URL [https://doi.org/10.1007/978-3-030-05249-2_4](https://doi.org/10.1007/978-3-030-05249-2_4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated
    corpora with web data only. In Alice Oh, Tristan Naumann, Amir Globerson, Kate
    Saenko, Moritz Hardt, and Sergey Levine, editors, *Proceedings of the NeurIPS*,
    2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perez-Beltrachini and Gardent (2017) Laura Perez-Beltrachini and Claire Gardent.
    Analysing data-to-text generation benchmarks. In José Maria Alonso, Alberto Bugarín,
    and Ehud Reiter, editors, *Proceedings of the INLG*, pages 238–242\. Association
    for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/w17-3537](https://doi.org/10.18653/v1/w17-3537).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word
    representations. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, *Proceedings
    of the NAACL-HLT*, pages 2227–2237\. Association for Computational Linguistics,
    2018. URL [https://doi.org/10.18653/v1/n18-1202](https://doi.org/10.18653/v1/n18-1202).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language models
    as knowledge bases? In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan,
    editors, *Proceedings of the EMNLP*, pages 2463–2473\. Association for Computational
    Linguistics, 2019. URL [https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post (2018) Matt Post. A call for clarity in reporting BLEU scores. In Ondrej
    Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry
    Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, Christof Monz, Matteo
    Negri, Aurélie Névéol, Mariana L. Neves, Matt Post, Lucia Specia, Marco Turchi,
    and Karin Verspoor, editors, *Proceedings of the WMT*, pages 186–191\. Association
    for Computational Linguistics, 2018. URL [https://doi.org/10.18653/v1/w18-6319](https://doi.org/10.18653/v1/w18-6319).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Puduppully and Lapata (2021) Ratish Puduppully and Mirella Lapata. Data-to-text
    generation with macro planning. *Transactions of the Association for Computational
    Linguistics*, 9:510–527, 2021. URL [https://doi.org/10.1162/tacl_a_00381](https://doi.org/10.1162/tacl_a_00381).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Puduppully et al. (2019) Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text
    generation with content selection and planning. In *Proceedings of the AAAI*,
    pages 6908–6915\. AAAI Press, 2019. URL [https://doi.org/10.1609/aaai.v33i01.33016908](https://doi.org/10.1609/aaai.v33i01.33016908).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019. URL [https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21:140:1–140:67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reiter (2018) Ehud Reiter. A structured review of the validity of BLEU. *Computational
    Linguistics*, 44(3), 2018. URL [https://doi.org/10.1162/coli_a_00322](https://doi.org/10.1162/coli_a_00322).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reiter and Dale (1997) Ehud Reiter and Robert Dale. Building applied natural
    language generation systems. *Natural Language Engineering*, 3(1):57–87, 1997.
    URL [https://doi.org/10.1017/S1351324997001502](https://doi.org/10.1017/S1351324997001502).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. How much
    knowledge can you pack into the parameters of a language model? In Bonnie Webber,
    Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings of the EMNLP*, pages
    5418–5426\. Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert
    Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff,
    Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina
    McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz
    Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell,
    Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy,
    Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher
    Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM:
    A 176b-parameter open-access multilingual language model. *CoRR*, abs/2211.05100,
    2022. URL [https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    to the point: Summarization with pointer-generator networks. In Regina Barzilay
    and Min-Yen Kan, editors, *Proceedings of the ACL*, pages 1073–1083\. Association
    for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/P17-1099](https://doi.org/10.18653/v1/P17-1099).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sha et al. (2018) Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li,
    Baobao Chang, and Zhifang Sui. Order-planning neural text generation from structured
    data. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, *Proceedings of
    the AAAI*, pages 5414–5421\. AAAI Press, 2018. URL [https://doi.org/10.1609/aaai.v32i1.11947](https://doi.org/10.1609/aaai.v32i1.11947).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2015) Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding
    machine for short-text conversation. In *Proceedings of the ACL/IJCNLP*, pages
    1577–1586. Association for Computational Linguistics, 2015. URL [https://doi.org/10.3115/v1/p15-1152](https://doi.org/10.3115/v1/p15-1152).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2019) Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. Pragmatically
    informative text generation. In Jill Burstein, Christy Doran, and Thamar Solorio,
    editors, *Proceedings of the NAACL-HLT*, pages 4060–4067\. Association for Computational
    Linguistics, 2019. URL [https://doi.org/10.18653/v1/n19-1410](https://doi.org/10.18653/v1/n19-1410).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Si et al. (2023) Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping
    Wang. An empirical study of instruction-tuning large language models in chinese.
    In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings of the EMNLP
    Findings*, pages 4086–4107\. Association for Computational Linguistics, 2023.
    URL [https://doi.org/10.18653/v1/2023.findings-emnlp.269](https://doi.org/10.18653/v1/2023.findings-emnlp.269).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2021) Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, and Nigel
    Collier. Plan-then-generate: Controlled data-to-text generation via planning.
    In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih,
    editors, *Proceedings of the EMNLP Findings*, pages 895–909\. Association for
    Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.findings-emnlp.76](https://doi.org/10.18653/v1/2021.findings-emnlp.76).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence
    to sequence learning with neural networks. In Zoubin Ghahramani, Max Welling,
    Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, *Proceedings
    of the NeurIPS*, pages 3104–3112, 2014. URL [https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taylor (1953) Wilson L. Taylor. ‘cloze procedure’: A new tool for measuring
    readability. *Journalism Quarterly*, 30(4):415–433, 1953. ISSN 0022-5533. URL
    [https://doi.org/10.1177/107769905303000401](https://doi.org/10.1177/107769905303000401).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2019) Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P.
    Parikh. Sticking to the facts: Confident decoding for faithful data-to-text generation.
    *CoRR*, abs/1910.08684, 2019. URL [http://arxiv.org/abs/1910.08684](http://arxiv.org/abs/1910.08684).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. *CoRR*, abs/2307.09288, 2023. URL
    [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tulchinskii et al. (2023) Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva,
    Daniil Cherniavskii, Sergey I. Nikolenko, Evgeny Burnaev, Serguei Barannikov,
    and Irina Piontkovskaya. Intrinsic dimension estimation for robust detection of
    ai-generated texts. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
    Moritz Hardt, and Sergey Levine, editors, *Proceedings of the NeurIPS*, 2023.
    URL [http://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ul Islam et al. (2023) Saad Obaid ul Islam, Iza Skrjanec, Ondrej Dusek, and
    Vera Demberg. Tackling hallucinations in neural chart summarization. In C. Maria
    Keet, Hung-Yi Lee, and Sina Zarrieß, editors, *Proceedings of the INLG*, pages
    414–423\. Association for Computational Linguistics, 2023. URL [https://doi.org/10.18653/v1/2023.inlg-main.30](https://doi.org/10.18653/v1/2023.inlg-main.30).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upadhyay and Massie (2023) Ashish Upadhyay and Stewart Massie. CBR assisted
    context-aware surface realisation for data-to-text generation. In Stewart Massie
    and Sutanu Chakraborti, editors, *Proceedings of the ICCBR*, volume 14141, pages
    34–49\. Springer, 2023. URL [https://doi.org/10.1007/978-3-031-40177-0_3](https://doi.org/10.1007/978-3-031-40177-0_3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, *Proceedings of
    the NeurIPS*, pages 5998–6008, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. (2016) Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order
    matters: Sequence to sequence for sets. In Yoshua Bengio and Yann LeCun, editors,
    *Proceedings of the ICLR*, 2016. URL [http://arxiv.org/abs/1511.06391](http://arxiv.org/abs/1511.06391).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou
    Chen. Towards faithful neural table-to-text generation with content-matching constraints.
    In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,
    *Proceedings of the ACL*, pages 1072–1086\. Association for Computational Linguistics,
    2020. URL [https://doi.org/10.18653/v1/2020.acl-main.101](https://doi.org/10.18653/v1/2020.acl-main.101).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2015) Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-hao Su,
    David Vandyke, and Steve J. Young. Semantically conditioned lstm-based natural
    language generation for spoken dialogue systems. In Lluís Màrquez, Chris Callison-Burch,
    Jian Su, Daniele Pighin, and Yuval Marton, editors, *Proceedings of the EMNLP*,
    pages 1711–1721\. Association for Computational Linguistics, 2015. URL [https://doi.org/10.18653/v1/d15-1199](https://doi.org/10.18653/v1/d15-1199).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiseman et al. (2017) Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush.
    Challenges in data-to-document generation. In Martha Palmer, Rebecca Hwa, and
    Sebastian Riedel, editors, *Proceedings of the EMNLP*, pages 2253–2263\. Association
    for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/d17-1239](https://doi.org/10.18653/v1/d17-1239).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
    In Qun Liu and David Schlangen, editors, *Proceedings of the EMNLP*, pages 38–45\.
    Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.emnlp-demos.6](https://doi.org/10.18653/v1/2020.emnlp-demos.6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu and Aji (2023) Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation
    biases for large language models. *CoRR*, abs/2307.03025, 2023. URL [https://doi.org/10.48550/arXiv.2307.03025](https://doi.org/10.48550/arXiv.2307.03025).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville,
    Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell:
    Neural image caption generation with visual attention. In Francis R. Bach and
    David M. Blei, editors, *Proceedings of the ICML*, volume 37, pages 2048–2057\.
    Proceedings of Machine Learning Research, 2015. URL [http://proceedings.mlr.press/v37/xuc15.html](http://proceedings.mlr.press/v37/xuc15.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2024) Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann
    Heng, and Wai Lam. Unveiling the generalization power of fine-tuned large language
    models. *CoRR*, abs/2403.09162, 2024. URL [https://doi.org/10.48550/arXiv.2403.09162](https://doi.org/10.48550/arXiv.2403.09162).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2020) Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational
    template machine for data-to-text generation. In *Proceedings of the ICLR*. OpenReview.net,
    2020. URL [https://openreview.net/forum?id=HkejNgBtPB](https://openreview.net/forum?id=HkejNgBtPB).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yermakov et al. (2021) Ruslan Yermakov, Nicholas Drago, and Angelo Ziletti.
    Biomedical data-to-text generation via fine-tuning transformers. In Anya Belz,
    Angela Fan, Ehud Reiter, and Yaji Sripada, editors, *Proceedings of the INLG*,
    pages 364–370\. Association for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.inlg-1.40](https://doi.org/10.18653/v1/2021.inlg-1.40).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2021) Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore:
    Evaluating generated text as text generation. In Marc’Aurelio Ranzato, Alina Beygelzimer,
    Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, *Proceedings
    of the NeurIPS*, pages 27263–27277, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024) Min Zhang, Jianfeng He, Shuo Lei, Murong Yue, Linhan Wang,
    and Chang-Tien Lu. Can llm find the green circle? investigation and human-guided
    tool manipulation for compositional generalization. In *Proceedings of the ICASSP*,
    pages 11996–12000, 2024. URL [https://www.doi.org/10.1109/ICASSP48485.2024.10446355](https://www.doi.org/10.1109/ICASSP48485.2024.10446355).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei
    Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction
    tuning for large language models: A survey. *CoRR*, abs/2308.10792, 2023. URL
    [https://doi.org/10.48550/arXiv.2308.10792](https://doi.org/10.48550/arXiv.2308.10792).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria
    Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained
    transformer language models. *CoRR*, abs/2205.01068, 2022. URL [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In *Proceedings
    of the ICLR*. OpenReview.net, 2020. URL [https://openreview.net/forum?id=SkeHuCVFDr](https://openreview.net/forum?id=SkeHuCVFDr).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2024) Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin
    Liu, and Gao Huang. Expel: LLM agents are experiential learners. In Michael J.
    Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, *Proceedings of the
    AAAI*, pages 19632–19642\. AAAI Press, 2024. URL [https://doi.org/10.1609/aaai.v38i17.29936](https://doi.org/10.1609/aaai.v38i17.29936).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019) Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M.
    Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized
    embeddings and earth mover distance. In Kentaro Inui, Jing Jiang, Vincent Ng,
    and Xiaojun Wan, editors, *Proceedings of the EMNLP*, pages 563–578\. Association
    for Computational Linguistics, 2019. URL [https://doi.org/10.18653/v1/D19-1053](https://doi.org/10.18653/v1/D19-1053).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang,
    and Yang You. Response length perception and sequence scheduling: An llm-empowered
    LLM inference pipeline. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
    Moritz Hardt, and Sergey Levine, editors, *Proceedings of the NeurIPS*, 2023.
    URL [http://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
