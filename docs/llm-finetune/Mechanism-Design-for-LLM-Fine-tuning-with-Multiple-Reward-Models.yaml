- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:36:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Mechanism Design for LLM Fine-tuning with Multiple Reward Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16276](https://ar5iv.labs.arxiv.org/html/2405.16276)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Haoran Sun
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: sunhaoran0301@stu.pku.edu.cn    Yurong Chen
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: chenyurong@pku.edu.cn    Siwei Wang
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research Asia
  prefs: []
  type: TYPE_NORMAL
- en: siweiwang@microsoft.com    Wei Chen
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research Asia
  prefs: []
  type: TYPE_NORMAL
- en: weic@microsoft.com    Xiaotie Deng
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: xiaotie@pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent research on fine-tuning large language models (LLMs) through the aggregation
    of multiple preferences has attracted considerable attention. However, the existing
    literature predominantly focuses on the empirical performance of aggregation algorithms,
    while neglecting the underlying motivation for agents to misreport their preferences.
    In this paper, we formalize this as a multi-parameter mechanism design problem,
    where an LLM provider designs both training and payment rules to achieve specific
    objectives and promote the truthful reporting of preferences. Firstly, we claim
    the necessity of a payment scheme by demonstrating that without payments, truth-telling
    is a strictly dominated strategy under a wide range of training rules. Then, we
    introduce the affine maximizer payment scheme for the social welfare maximizing
    training rules that are widely used in practice, which ensures both dominant-strategy
    incentive compatibility (DSIC) and individual rationality (IR). Furthermore, we
    prove that under mild conditions, any other payment rule that also implements
    these training rules in DSIC can be converted to the affine maximizer payment
    by adding a factor irrelevant to the agents’ own reports. We also show that this
    mechanism satisfies approximate DSIC when the input of the mechanism is a biased
    version of the reported preferences, showcasing its robustness in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pre-training and fine-tuning paradigm is fundamental in developing language
    models (Devlin et al. ([2018](#bib.bib16)); Radford et al. ([2018](#bib.bib49));
    Liu et al. ([2019](#bib.bib38)); Touvron et al. ([2023](#bib.bib60))). During
    pre-training, the model is fed with vast amounts of data to acquire a general
    capability to understand and generate language through self-supervised learning.
    The subsequent fine-tuning phase customizes these pre-trained models for specific
    downstream tasks using smaller, task-oriented datasets, ensuring that the model
    outputs are more closely aligned with particular requirements. As LLMs gain increasing
    popularity, there is a growing demand for fine-tuning basic LLMs, as basic models
    often fail to meet users’ demands, especially in catering to individual preferences.
  prefs: []
  type: TYPE_NORMAL
- en: The process of fine-tuning an LLM to align with certain human preferences is
    challenging to achieve through supervision (Ji et al. ([2023](#bib.bib34)); Köpf
    et al. ([2024](#bib.bib35)); Wang et al. ([2023b](#bib.bib64)); Shen et al. ([2023](#bib.bib57))),
    primarily due to the difficulty in constructing datasets with a substantial number
    of valid question-answer pairs for supervised training. Reinforcement learning
    from human feedback (RLHF) (Ouyang et al. ([2022](#bib.bib46)); Christiano et al.
    ([2017](#bib.bib10))) offers a promising solution to this problem. In RLHF, a
    reward model is first trained to be used as a proxy for human judgment. This model
    then provides reward signals for the standard reinforcement learning process.
    This technique of fine-tuning with a reward model has proven effective in encoding
    human preferences into models and has become a fundamental component of the training
    process for most advanced LLMs. With the advancement of RLHF, numerous studies
    have investigated efficient methods for aggregating multiple preferences into
    a single fine-tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: However, most of these studies focus primarily on improving empirical performance
    across various metrics (Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Coste et al. ([2023](#bib.bib14)); Zhang et al.
    ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62)); Eisenstein et al. ([2023](#bib.bib21))).
    They often implicitly assume that we are accessible to real preferences, neglecting
    the possibility of agents’ misreporting their preferences. This problem becomes
    more crucial when we consider a real-world scenario, where different agents provide
    their preferences for the aggregation. In such cases, agents may engage in strategic
    misreporting to increase their utility. An intuitive example is that if an agent
    knows beforehand that the fine-tuning process aims to neutralize all preferences,
    it might pretend to have a more polarized preference as a beneficial strategy.
    These strategic behaviors can distort the final training results, even if the
    trained algorithm is highly effective. Nevertheless, this issue has not attracted
    sufficient attention in the existing literature, particularly concerning the fine-tuning
    process of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Our Contribution.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this paper, we mainly study the incentive design in such scenarios. First,
    we formalize this as a multi-parameter mechanism design problem between a fine-tuning
    service provider and groups of agents seeking fine-tuning services. The provider
    proposes a mechanism that includes a *training rule* for integrating different
    groups’ preferences into a fine-tuned model and a *payment rule* to charge the
    groups. After observing the mechanism, each group strategically reports its preference
    to maximize its utility. We consider that the subsequent fine-tuning process is
    implemented using RLHF, a standard method for aligning a model with human preference.
    Therefore, we abstract the preference of each group to be reward models, and term
    the whole scenario the *RLHF Game*.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we demonstrate the profitability of misreporting a polarized preference
    under a wide range of mechanisms that include only a training rule ([Theorem 3.3](#S3.Thmtheorem3
    "Theorem 3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). This
    underscores the necessity of a payment rule to address incentive issues.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, we focus on a representative set of training rules, termed the SW-Maximizing
    training rules, in which the provider aims to maximize social welfare while incorporating
    different regularization measures. For SW-Maximizing training rules, we propose
    the affine maximizer payment scheme, a weighted version of the Vickrey-Clarke-Groves
    (VCG) payment (Vickrey ([1961](#bib.bib61)); Clarke ([1971](#bib.bib11)); Groves
    ([1973](#bib.bib26))). We prove that agents truthfully reporting their preferences
    constitutes a dominant strategy in such mechanisms ([Theorem 4.2](#S4.Thmtheorem2
    "Theorem 4.2\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). Utilizing
    the notion of payment equivalence, we prove that under a mild condition, any other
    payment rule that also implements these training rules in dominant-strategy incentive
    compatibility (DSIC) can be converted to the affine maximizer payment by adding
    a factor irrelevant to agents’ own reports ([Theorem 4.5](#S4.Thmtheorem5 "Theorem
    4.5\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣
    Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). We validate
    this condition for many commonly used regularization terms like KL-divergence ([Proposition 4.4](#S4.Thmtheorem4
    "Proposition 4.4\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
    Consequently, we derive the revenue-maximizing payment rule that implements SW-Maximizing
    training rules in both DSIC and individual rationality (IR) ([Corollary 4.6](#S4.Thmtheorem6
    "Corollary 4.6\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
    Furthermore, we show that this mechanism remains approximately DSIC when the input
    of the mechanism is a biased version of the reported preferences, which is an
    abstraction modeling for the inevitable errors that occur in practice. This showcases
    the robustness of the proposed mechanisms in real-world applications ([Theorem 4.9](#S4.Thmtheorem9
    "Theorem 4.9\. ‣ 4.2 Approximate Valuation ‣ 4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
  prefs: []
  type: TYPE_NORMAL
- en: Primary Related Work.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several studies have investigated similar scenarios. Among them, Duetting et al.
    ([2023](#bib.bib20)) and Soumalias et al. ([2024](#bib.bib58)) are most related
    to ours. Duetting et al. ([2023](#bib.bib20)) examines the problem of designing
    a mechanism to aggregate multiple agents’ preferences based on each agent’s bids
    and determine their payments. However, they exclude the case where preferences
    can be misreported, which is the primary concern in our study. The concurrent
    work by Soumalias et al. ([2024](#bib.bib58)) also considers the mechanism design
    for aggregating multiple preferences. Their focus is mainly on the practical implementation
    of SW-Maximizing training rule with KL-divergence and the payment scheme that
    obtains both DSIC and interpretability. However, in this scenario, we are more
    concerned with the theoretical properties of more general mechanisms, including
    the implementability and the property of payment equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are works studying other scenarios related to LLMs from
    the perspective of algorithmic game theory. Laufer et al. ([2023](#bib.bib36))
    abstracts the fine-tuning process as a bargaining game and characterizes the perfect
    sub-game equilibria. Dubey et al. ([2024](#bib.bib19)) proposes an auction where
    bidders compete to place their content within a summary generated by an LLM. Conitzer
    et al. ([2024](#bib.bib13)) considers incorporating social choice theory in LLM
    alignment. Feizi et al. ([2023](#bib.bib23)) explores the potential for leveraging
    LLMs in online advertising systems.
  prefs: []
  type: TYPE_NORMAL
- en: Paper Organization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Section 2](#S2 "2 Preliminaries and Model ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), we provide the preliminaries and the formal description
    of the RLHF Game. In [Section 3](#S3 "3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"), we study
    the incentive design for general training rules in the RLHF Game. We demonstrate
    the properties of mechanisms that consist of SW-Maximizing training rules and
    payment rules in [Section 4](#S4 "4 Social Welfare Maximizing Mechanism ‣ Mechanism
    Design for LLM Fine-tuning with Multiple Reward Models"). Further related work
    is provided in [Section 5](#S5 "5 Further Related Work ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), and we conclude in [Section 6](#S6
    "6 Discussion and Conclusion ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries and Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Preliminaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large Language Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Large language models (LLMs) function as mappings from a sequence of tokens
    to a probability distribution over the next token. The input sequence is usually
    constrained by a maximum length $K$.
  prefs: []
  type: TYPE_NORMAL
- en: 'An LLM parameterized by $\theta\in\Theta$. For practical purposes, the output
    sequence is also required to be of finite length. We assume the maximum output
    length is also $K$ generated by $g_{\theta}$ is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{LLM}_{\theta}({\bm{x}})=\prod_{t=1}^{&#124;\bm{x}&#124;}g_{\theta}(x_{t}\mid{\bm{x}}_{<t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where ${\bm{x}}_{<t}$. $\text{LLM}_{\theta}$ the probability of ${\bm{x}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Reward Modeling.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reward modeling is instrumental for aligning LLMs with human preferences, particularly
    within the context of RLHF. In this process, a reward model $\text{rm}:T^{\ast}\to\mathbb{R}$
    over $T^{\ast}$. Unless otherwise stated, we use $\mathcal{R}$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Formulation of the RLHF Game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we present the formal description of the RLHF Game. There is one
    LLM *provider* and $n$ for all ${\bm{x}}\in T^{\ast}$. Let $\mathcal{R}$. The
    exact reward model $\text{rm}_{i}$, the valuation when it receives a model $\text{LLM}_{\theta}$
    is known by both the provider and the agents.
  prefs: []
  type: TYPE_NORMAL
- en: The provider first announces the mechanism, including a training rule ${\psi}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\psi}:\mathcal{R}^{n}\times\mathcal{W}^{n}\times\Theta\to\Theta,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Both rules take $n$. In this case, the model coincides with most previous work,
    where agents’ incentives are not considered (Ramé et al. ([2024](#bib.bib51));
    Wu et al. ([2024](#bib.bib65)); Jang et al. ([2023](#bib.bib32)); Coste et al.
    ([2023](#bib.bib14)); Zhang et al. ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62));
    Eisenstein et al. ([2023](#bib.bib21))). Specifically, the training rule seeks
    to find the model that maximizes a certain objective function $f$. That is,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})\in\arg\max_{\theta\in\Theta}f(v_{1}(\theta;\text{rm}_{1}),\cdots,v_{n}(\theta;\text{rm}_{n}),\vec{w},D(\text{LLM}_{\theta}&#124;&#124;\text{LLM}_{\theta_{\text{init}}})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $D$ has a unique global optimal point for any possible inputs. Hence,
    in the rest of the paper, the “$\in$ is substituted by “=”.
  prefs: []
  type: TYPE_NORMAL
- en: After observing the announced mechanism (${\psi}$, and its group size $\tilde{w}_{i}$
    is optimal, i.e., the final parameter satisfies $\theta_{\text{final}}={\psi}(\overrightarrow{\widetilde{\text{rm}}},\vec{\tilde{w}},\theta_{\text{init}})$,
    so the valuation for group $i$’s utility is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The groups may strategically report, thus $\overrightarrow{\widetilde{\text{rm}}}$.
    The goal of the LLM provider is to achieve its training objective based on the
    group’s true preferences, taking into account that the misreporting may distort
    the training outcome. To this end, it is crucial to incentivize all groups to
    report their information truthfully so that the provider is accessible to the
    groups’ private information. We formally define these desiderata of a mechanism
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A mechanism $({\psi},p)$, $\text{rm}^{\prime}_{i}$, $\theta_{\text{init}}$,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (DSIC) |'
  prefs: []
  type: TYPE_TB
- en: Definition 2.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A mechanism $({\psi},p)$, $\overrightarrow{\text{rm}}_{-i}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i})\geq
    0.$ |  | (IR) |'
  prefs: []
  type: TYPE_TB
- en: DSIC means that for any group, truthfully reporting the reward model and the
    group size yields the highest utility, regardless of other groups’ reports. IR
    means that truthfully reporting always yields non-negative utilities. Only when
    both DSIC and IR are satisfied, all groups are incentivized to participate in
    this game and report truthfully. When a mechanism $({\psi},p)$ in DSIC, IR or
    both DSIC and IR. Especially, when we say the implementability of a training rule,
    we refer to the property of DSIC.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Incentives for General Training Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the incentive design within the RLHF Game framework.
    As a warm-up, we consider a simplified scenario where all group sizes are equal
    to $1$. Unless stated otherwise, the results directly apply to the more general
    case where $\vec{w}$ is also private information.
  prefs: []
  type: TYPE_NORMAL
- en: For the valuation function in this section, we consider a reasonable form $v(\cdot;\cdot)$
    defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 3.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For any agent with preference represented by reward model rm, its valuation
    on model $\text{LLM}_{\theta}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $v(\theta;\text{rm})=\mathbb{E}_{{\bm{x}}\sim\text{LLM}_{\theta}}\text{rm}({\bm{x}})=\sum_{{\bm{x}}\in
    T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\text{rm}({\bm{x}}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: In practice, this can be obtained by averaging the reward of the sequences sampled
    from an LLM. We discuss the influence of possible errors in this process in [Section 4](#S4
    "4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Necessity of Payment Rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We begin by demonstrating the necessity of payment rules to ensure incentive
    compatibility for training rules under the following assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 3.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: (1) For all $i\in[n]$ exists and $\partial f/\partial D<0$ exists and is positive.
    (3) For all $\overrightarrow{\text{rm}}$ for all ${\bm{x}}\in T^{\ast}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rationale of these assumptions is as follows: (1) is that we assume the
    training process aims to find a model $\text{LLM}_{\theta}$. And (3) is to exclude
    some extreme training rules that the training outcome remains the same for most
    input and changes drastically. In practice, (1) is satisfied for most training
    functions $f$ and the strength of regularization. At least, they are satisfied
    by the commonly used KL-divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining these three conditions, we show that when the preference for some
    ${\bm{x}}$ for the ${\bm{x}}$ at all.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [3.2](#S3.Thmtheorem2 "Assumption 3.2\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), when the payment rule $p\equiv 0$, such that $\text{rm}_{i}({\bm{x}})=1/|S|$.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we call a strategy strongly dominated when another strategy yields strictly
    higher utility regardless of others’ reports. [Theorem 3.3](#S3.Thmtheorem3 "Theorem
    3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") tells us
    that truthful reporting is strongly dominated with only training rules, and thus
    will not be adopted by rational agents.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Characteristics of Payment Rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having established the necessity of payment rules in this scenario, we mainly
    address two questions in the remainder of this section: First, *given a training
    rule ${\psi}$. Second, *for an implementable training rule ${\psi}$.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We resolve the first question primarily by utilizing the notion of *cycle monotonicity*,
    first proposed by Rochet ([1987](#bib.bib53)). Cycle monotonicity generalizes
    monotonicity defined in a single-parameter scenario ((Myerson, [1981](#bib.bib43))).
    In the RLHF Game, we define a function as $l(\text{rm}^{\prime},\text{rm};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})\coloneqq
    v_{i}({\psi}((\text{rm},\overrightarrow{\text{rm}}_{-i}),\theta_{\text{init}});\text{rm})-v_{i}({\psi}((\text{rm}^{\prime},\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}}));\text{rm})$)
    under $\overrightarrow{\text{rm}}_{-i}$. The cycle monotonicity is defined based
    on this function:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3.4  (Cycle Monotonicity).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The training rule ${\psi}$ ($k\geq 0$ we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For general training rules, cycle monotonicity is a sufficient and necessary
    condition for implementability.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.5  (Rochet ([1987](#bib.bib53))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A training rule ${\psi}$ is implementable if and only if it satisfies cycle
    monotonicity.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the proof of [Theorem 3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)).
    ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") is constructive.
    However, for general implementable training rules, the calculation of the payment
    rules is too complex to be practical.
  prefs: []
  type: TYPE_NORMAL
- en: The second question is more general, so we primarily consider the concept of
    *payment equivalence* ((Ashlagi et al., [2010](#bib.bib2))) for an implementable
    training rule.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3.6  (Payment Equivalence).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An implementable training rule ${\psi}$ such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Or equivalently, when fixing $\overrightarrow{\text{rm}}_{-i}$ for all $\text{rm}_{i}\in\mathcal{R}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Payment equivalence indicates that the only way to modify a DSIC mechanism $({\psi},p)$’s
    payment function $p_{i}$ satisfies payment equivalence and we can figure out one
    mechanism $({\psi},p)$ among all these payment rules that implement ${\psi}$ in
    both DSIC and IR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Payment equivalence is influenced by the domain of the types: reward models
    and group sizes in the RLHF Game. When $\vec{w}\equiv 1$, which is a connected
    set in the Euclidean space. Thus, we can directly apply the result in Nisan et al.
    ([2007](#bib.bib44)) and get the following theorem.'
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 3.7.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When $\vec{w}\equiv 1$ is public information and the agents only report the
    reward models, all implementable training rules satisfy payment equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: However, when the group sizes $\vec{w}$. Thus, payment equivalence may not be
    satisfied for general training rules, and we will study this for a representative
    set of training rules in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Social Welfare Maximizing Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we consider the scenario where group $i$ to truthfully report
    both $\text{rm}_{i}$, though it is possible to adopt the method used in the constructive
    proof for [Theorem 3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics
    of Payment Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design
    for LLM Fine-tuning with Multiple Reward Models") to derive the payment rule,
    the resulting payment rule can be complex and impractical.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this section, our primary focus is on a subset of training rules
    designed to maximize social welfare under regularization constraints, which is
    commonly used in practice to aggregate various preferences (Boyd and Vandenberghe
    ([2004](#bib.bib5)); Nocedal and Wright ([1999](#bib.bib45))), balancing efficiency
    and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.1  (SW-Maximizing Training Rules).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Given the reports $\overrightarrow{\text{rm}}$. Formally, it is represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{i=1}^{n}w_{i}v_{i}(\theta;\text{rm}_{i})-\lambda
    D(\text{LLM}_{\theta}&#124;&#124;\text{LLM}_{\theta_{\text{init}}}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\lambda$ is a hyperparameter that adjusts regularization strength.
  prefs: []
  type: TYPE_NORMAL
- en: Note that SW-Maximizing training rules constitute a set of training rules. We
    use ${\psi}\in\Psi^{SW}$ and $\theta_{\text{init}}$. One simple way to achieve
    it is to set a large $\lambda$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Affine Maximizer Payment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We introduce the affine maximizer payment rule (Roberts ([1979](#bib.bib52)))
    $p^{AFF}$, a weighted version of VCG payment (Vickrey ([1961](#bib.bib61)); Clarke
    ([1971](#bib.bib11)); Groves ([1973](#bib.bib26))):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The notations $ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$,
    the reported number of agents are $\vec{w}$. The affine social welfare consists
    of both the groups’ valuations and the regularization term. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle ASW_{-j}(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We show that $p^{AFF}$ implements SW-Maximizing training rules in both DSIC
    and IR, which implies that truthfully reporting both reward models and group sizes
    constitutes a dominant Nash Equilibrium in this mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any ${\psi}\in\Psi^{SW}$ satisfies DSIC and IR.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding payment equivalence, as we have mentioned in the previous section,
    the domain $\mathcal{W}\times\mathcal{R}$, the results in Nisan et al. ([2007](#bib.bib44))
    can not be directly applied. However, we show that under the following assumption,
    SW-Maximizing training rules satisfy payment equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 4.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $\epsilon></math>, <math id=$, then $\max_{{\bm{x}}\in T^{\ast}}|\text{LLM}_{\theta}({\bm{x}})-\text{LLM}_{\theta^{\prime}}({\bm{x}})|\leq\epsilon$.
  prefs: []
  type: TYPE_NORMAL
- en: This assumption is reasonable for most measures $D$ are sufficiently close,
    the training outcomes $\theta$ should also be close. Specifically, we validate
    this assumption for some widely used distance measures.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 4.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4
    Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models") holds for SW-Maximizing training rules with regularizations
    KL-divergence, $D_{\mathrm{KL}}(p||q)=\sum_{{\bm{x}}\in T^{\ast}}p({\bm{x}})\log
    p({\bm{x}})/q({\bm{x}})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under this assumption, we derive the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), each training rule ${\psi}\in\Psi^{SW}$ satisfies payment equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: With the property of payment equivalence, we further investigate the revenue-maximizing
    payment rule that implements SW-Maximizing training rules in both DSIC and IR.
    Finding the revenue-maximizing multi-parameter mechanism is a challenging problem
    in classic mechanism design theory. However, since we have proved the payment
    equivalence for SW-Maximizing training rules, we can utilize the necessary condition
    defined in [Definition 3.6](#S3.Thmtheorem6 "Definition 3.6 (Payment Equivalence).
    ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") to formulate
    it as a optimization problem. Solving this problem provides the optimal payment
    rule under the same conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Corollary 4.6.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), for each training rule ${\psi}\in\Psi^{SW}$ in both DSIC and
    IR is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{*}_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The relationship between the domains $\mathcal{R}\times\mathcal{W}$ includes
    all normalized reward models. Second, based on payment equivalence, finding the
    revenue-maximizing mechanism satisfying IR also needs information on the exact
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Approximate Valuation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we discuss the influence of error generated in practice on the
    incentive property in the RLHF Game. We abstract it as an approximate valuation
    problem (Chiesa et al. ([2012](#bib.bib9))). Formally, when group $i$ with a conditional
    distribution $F_{i}(\cdot|\text{rm}_{i})$ can be considered as the optimal for
    the deviated reward models.
  prefs: []
  type: TYPE_NORMAL
- en: We assume that agent groups are aware of the noise when feeding preferences
    into the mechanism. Therefore, their utilities will take it into account and have
    a different form. We use the capital letter $U_{i}$ and group size $w_{i}$ is
    given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $U_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i})=\mathbb{E}_{\widehat{\text{rm}}_{i}\sim
    F_{i}(\cdot&#124;\text{rm}^{\prime}_{i})}u_{i}((\widehat{\text{rm}}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that in defining $U_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only consider the case when the noised input to the mechanism and the reported
    reward models are close:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 4.7  (Bounded Error).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any profile of reported reward models $\overrightarrow{\text{rm}}$s with
    non-zero probability satisfies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{{\bm{x}}\in T^{\ast}}&#124;\widehat{\text{rm}}_{i}({\bm{x}})-\text{rm}_{i}({\bm{x}})&#124;\leq\epsilon\quad\forall
    i\in[n].$ |  |'
  prefs: []
  type: TYPE_TB
- en: We first show that by directly applying results in [Section 4.1](#S4.SS1 "4.1
    Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design
    for LLM Fine-tuning with Multiple Reward Models") to the noised input, the loss
    in the social welfare is upper-bounded by $2\epsilon\sum_{i=1}^{n}w_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 4.8.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.7](#S4.Thmtheorem7 "Assumption 4.7 (Bounded Error). ‣ 4.2 Approximate Valuation
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), when the training rule ${\psi}\in\Psi^{SW}$, the
    loss in social welfare is bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For training rule ${\psi}\in\Psi^{SW}$. Therefore, we can derive the following
    theorem based on [Lemma 4.8](#S4.Thmtheorem8 "Lemma 4.8\. ‣ 4.2 Approximate Valuation
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models").
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.9.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.7](#S4.Thmtheorem7 "Assumption 4.7 (Bounded Error). ‣ 4.2 Approximate Valuation
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), when the training rule ${\psi}\in\Psi^{SW}$, $\overrightarrow{\text{rm}}_{-i}$,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: In other words, when $\vec{w}$-DSIC mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: This means that for any group $i$.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Further Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RLHF with Multiple Reward Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Research involving multiple reward models primarily focuses on developing algorithms
    to enhance practical performance. Some studies design methods to simultaneously
    satisfy multiple preferences (Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Park et al. ([2024](#bib.bib47))). Additionally,
    there is a body of work that trains multiple models for a single preference and
    then ensembles them to improve the robustness of RLHF (Coste et al. ([2023](#bib.bib14));
    Zhang et al. ([2024](#bib.bib68))), mitigate the influence of incorrect and ambiguous
    preferences in the dataset (Wang et al. ([2024](#bib.bib62))), and reduce reward
    hacking (Eisenstein et al. ([2023](#bib.bib21))). Unlike these approaches, our
    work considers how to collect misaligned preferences truthfully from different
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-parameter Auctions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several studies have explored the properties relevant to our paper in various
    multi-parameter auction scenarios, such as implementability (Rochet ([1987](#bib.bib53));
    Miyake ([1998](#bib.bib41)); Conitzer and Sandholm ([2004](#bib.bib12)); Saks
    and Yu ([2005](#bib.bib55)); Bikhchandani et al. ([2006](#bib.bib4)); Ashlagi
    et al. ([2010](#bib.bib2))) and payment equivalence (Ivanova-Stenzel and Salmon
    ([2008](#bib.bib30)); Heydenreich et al. ([2009](#bib.bib29)); Bergemann and Välimäki
    ([2010](#bib.bib3)); Pavan et al. ([2014](#bib.bib48))). Another central topic
    in auction theory is to design mechanisms that satisfy DSIC and IR while maximizing
    the expected revenue for the auctioneer. Although the single-parameter scenario
    has been resolved by Myerson ([1981](#bib.bib43)), the optimal auction design
    for multi-parameter settings remains an open question. Therefore, there is a stream
    of research focusing on a specific subset, affine maximizer auctions, which inherently
    satisfy DSIC and IR (Sandholm and Likhodedov ([2015](#bib.bib56)); Roberts ([1979](#bib.bib52));
    Likhodedov and Sandholm ([2004](#bib.bib37)); Briest et al. ([2010](#bib.bib7));
    Tang and Sandholm ([2012](#bib.bib59)); Jehiel et al. ([2007](#bib.bib33))), and
    proposes optimizations to enhance empirical performance (Curry et al. ([2022](#bib.bib15));
    Duan et al. ([2024a](#bib.bib17), [b](#bib.bib18))). Compared to these works,
    we are the first to discuss the property of payment equivalence and the revenue-maximizing
    solution in the scenario of fine-tuning LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Game Theory and LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Other works also explored the intersection of game theory and large language
    models. Some research has proposed algorithms for training LLMs inspired by concepts
    in game theory, such as Nash learning from human feedback (Munos et al. ([2023](#bib.bib42))),
    consensus game (Jacob et al. ([2023](#bib.bib31))), and direct Nash optimization (Rosset
    et al. ([2024](#bib.bib54))), and Gemp et al. ([2024](#bib.bib25)). Furthermore,
    various studies assess LLMs from a game-theoretical perspective, examining aspects
    such as rationality (Chen et al. ([2023](#bib.bib8)); Fan et al. ([2023](#bib.bib22))),
    behavior in matrix games (Akata et al. ([2023](#bib.bib1)); Gandhi et al. ([2023](#bib.bib24));
    Lorè and Heydari ([2023](#bib.bib39))), and performance in strategic games like
    auctions (Guo et al. ([2023](#bib.bib27), [2024](#bib.bib28))), Werewolf (Xu et al.
    ([2023a](#bib.bib66), [b](#bib.bib67))), and Avalon (Wang et al. ([2023a](#bib.bib63))).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Efficient Practical Implementation of $p^{AFF}$.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the RLHF Game with $n$s. This can result in inefficiency due to the costly
    training. To address this problem, we propose two modifications to $p^{AFF}$ when
    calculating payments:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate an approximate $\widehat{{\psi}}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=\arg\max_{\theta\in\{\theta_{1},\cdots,\theta_{K}\}}ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adopt less iterations in the training process for calculating ${\psi}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})$
    that is not optimal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The first method needs only one training process (for ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})$’s
    report. In comparison, the second approach incurs higher training costs but guarantees
    strict DSIC.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and Future Work.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This paper investigates incentive design in fine-tuning large language models
    using multiple reward models. We formalize this scenario as the RLHF Game, where
    a service provider proposes training and payment rules, and agents strategically
    report their preferences. We demonstrate the necessity of payment schemes for
    incentivizing truthful reporting in general training rules and provide a comprehensive
    characterization of payment schemes that implement SW-Maximizing training rules
    in dominant strategies. These findings enhance the theoretical understanding of
    mechanism design in LLM fine-tuning and offer guidelines for implementing effective
    RLHF-based systems in various contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Future research in this field presents several promising directions. Firstly,
    investigating mechanisms integrating efficiency and incentive compatibility within
    the RLHF Game could significantly enhance its applicability in real-world scenarios.
    Secondly, modeling and examining more complex training rules, such as dynamic
    training rules, could deepen the understanding of this framework. Thirdly, designing
    mechanisms for more general cases that aggregate preferences into multiple models
    based on diversity considerations is crucial. Additionally, applying mechanism
    design theory to other scenarios related to large language models, such as API
    charge schemes, retrieval-augmented generation (RAG), and prompt engineering,
    offers valuable opportunities for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Akata et al. [2023] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge, and Eric Schulz. Playing repeated games with large language models.
    *arXiv preprint arXiv:2305.16867*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashlagi et al. [2010] Itai Ashlagi, Mark Braverman, Avinatan Hassidim, and Dov
    Monderer. Monotonicity and implementability. *Econometrica*, 78(5):1749–1772,
    2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergemann and Välimäki [2010] Dirk Bergemann and Juuso Välimäki. The dynamic
    pivot mechanism. *Econometrica*, 78(2):771–789, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bikhchandani et al. [2006] Sushil Bikhchandani, Shurojit Chatterji, Ron Lavi,
    Ahuva Mu’alem, Noam Nisan, and Arunava Sen. Weak monotonicity characterizes deterministic
    dominant-strategy implementation. *Econometrica*, 74(4):1109–1132, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boyd and Vandenberghe [2004] Stephen P Boyd and Lieven Vandenberghe. *Convex
    optimization*. Cambridge university press, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Briest et al. [2010] Patrick Briest, Shuchi Chawla, Robert Kleinberg, and S Matthew
    Weinberg. Pricing randomized allocations. In *Proceedings of the twenty-first
    annual ACM-SIAM symposium on Discrete Algorithms*, pages 585–597\. SIAM, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023] Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong.
    The emergence of economic rationality of gpt. *Proceedings of the National Academy
    of Sciences*, 120(51):e2316205120, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiesa et al. [2012] Alessandro Chiesa, Silvio Micali, and Zeyuan Allen Zhu.
    Mechanism design with approximate valuations. In *Proceedings of the 3rd Innovations
    in Theoretical Computer Science conference*, pages 34–38, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clarke [1971] Edward H Clarke. Multipart pricing of public goods. *Public choice*,
    pages 17–33, 1971.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conitzer and Sandholm [2004] Vincent Conitzer and Tuomas Sandholm. Self-interested
    automated mechanism design and implications for optimal combinatorial auctions.
    In *Proceedings of the 5th ACM Conference on Electronic Commerce*, pages 132–141,
    2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conitzer et al. [2024] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H
    Holliday, Bob M Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell,
    Hailey Schoelkopf, et al. Social choice for ai alignment: Dealing with diverse
    human feedback. *arXiv preprint arXiv:2404.10271*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coste et al. [2023] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
    Reward model ensembles help mitigate overoptimization. *arXiv preprint arXiv:2310.02743*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curry et al. [2022] Michael Curry, Tuomas Sandholm, and John Dickerson. Differentiable
    economics for randomized affine maximizer auctions. *arXiv preprint arXiv:2202.02872*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2024a] Zhijian Duan, Haoran Sun, Yurong Chen, and Xiaotie Deng.
    A scalable neural network for dsic affine maximizer auction design. *Advances
    in Neural Information Processing Systems*, 36, 2024a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2024b] Zhijian Duan, Haoran Sun, Yichong Xia, Siqiang Wang, Zhilin
    Zhang, Chuan Yu, Jian Xu, Bo Zheng, and Xiaotie Deng. Scalable virtual valuations
    combinatorial auction design by combining zeroth-order and first-order optimization
    method. *arXiv preprint arXiv:2402.11904*, 2024b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al. [2024] Kumar Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta,
    and Di Wang. Auctions with llm summaries. *arXiv preprint arXiv:2404.08126*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duetting et al. [2023] Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Haifeng
    Xu, and Song Zuo. Mechanism design for large language models. *arXiv preprint
    arXiv:2310.10826*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eisenstein et al. [2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad
    Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl,
    Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate
    but do not eliminate reward hacking. *arXiv preprint arXiv:2312.09244*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. [2023] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large
    language models serve as rational players in game theory? a systematic analysis.
    *arXiv preprint arXiv:2312.05488*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feizi et al. [2023] Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei,
    and Suho Shin. Online advertisements with llms: Opportunities and challenges.
    *arXiv preprint arXiv:2311.07601*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gandhi et al. [2023] Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic
    reasoning with language models. *arXiv preprint arXiv:2305.19165*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemp et al. [2024] Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari
    Dasagi, Luke Marris, Georgios Piliouras, and Karl Tuyls. States as strings as
    strategies: Steering language models with game-theoretic solvers. *arXiv preprint
    arXiv:2402.01704*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Groves [1973] Theodore Groves. Incentives in teams. *Econometrica: Journal
    of the Econometric Society*, pages 617–631, 1973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2023] Shangmin Guo, Haochuan Wang, Haoran Bu, Yi Ren, Dianbo Sui,
    Yu-Ming Shang, and Siting Lu. Large language models as rational players in competitive
    economics games. *arXiv preprint arXiv:2308.10032*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2024] Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui,
    Yuming Shang, and Siting Lu. Economics arena for large language models. *arXiv
    preprint arXiv:2401.01735*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heydenreich et al. [2009] Birgit Heydenreich, Rudolf Müller, Marc Uetz, and
    Rakesh V Vohra. Characterization of revenue equivalence. *Econometrica*, 77(1):307–316,
    2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivanova-Stenzel and Salmon [2008] Radosveta Ivanova-Stenzel and Timothy C Salmon.
    Revenue equivalence revisited. *Games and Economic Behavior*, 64(1):171–192, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jacob et al. [2023] Athul Paul Jacob, Yikang Shen, Gabriele Farina, and Jacob
    Andreas. The consensus game: Language model generation via equilibrium search.
    *arXiv preprint arXiv:2310.09139*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jang et al. [2023] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang,
    Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj
    Ammanabrolu. Personalized soups: Personalized large language model alignment via
    post-hoc parameter merging. *arXiv preprint arXiv:2310.11564*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jehiel et al. [2007] Philippe Jehiel, Moritz Meyer-Ter-Vehn, and Benny Moldovanu.
    Mixed bundling auctions. *Journal of Economic Theory*, 134(1):494–512, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. [2023] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao
    Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai
    alignment: A comprehensive survey. *arXiv preprint arXiv:2310.19852*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Köpf et al. [2024] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver
    Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large
    language model alignment. *Advances in Neural Information Processing Systems*,
    36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laufer et al. [2023] Benjamin Laufer, Jon Kleinberg, and Hoda Heidari. Fine-tuning
    games: Bargaining and adaptation for general-purpose models. *arXiv preprint arXiv:2308.04399*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likhodedov and Sandholm [2004] Anton Likhodedov and Tuomas Sandholm. Methods
    for boosting revenue in combinatorial auctions. In *AAAI*, pages 232–237, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lorè and Heydari [2023] Nunzio Lorè and Babak Heydari. Strategic behavior of
    large language models: Game structure vs. contextual framing. *arXiv preprint
    arXiv:2309.05898*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luenberger et al. [1984] David G Luenberger, Yinyu Ye, et al. *Linear and nonlinear
    programming*, volume 2. Springer, 1984.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyake [1998] Mitsunobu Miyake. On the incentive properties of multi-item auctions.
    *International Journal of Game Theory*, 27:1–19, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munos et al. [2023] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi
    Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    Andrea Michi, et al. Nash learning from human feedback. *arXiv preprint arXiv:2312.00886*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Myerson [1981] Roger B Myerson. Optimal auction design. *Mathematics of operations
    research*, 6(1):58–73, 1981.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nisan et al. [2007] Noam Nisan et al. Introduction to mechanism design (for
    computer scientists). *Algorithmic game theory*, 9:209–242, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nocedal and Wright [1999] Jorge Nocedal and Stephen J Wright. *Numerical optimization*.
    Springer, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. [2024] Chanwoo Park, Mingyang Liu, Kaiqing Zhang, and Asuman Ozdaglar.
    Principled rlhf from heterogeneous feedback via personalization and preference
    aggregation. *arXiv preprint arXiv:2405.00254*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pavan et al. [2014] Alessandro Pavan, Ilya Segal, and Juuso Toikka. Dynamic
    mechanism design: A myersonian approach. *Econometrica*, 82(2):601–653, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your
    language model is secretly a reward model. *Advances in Neural Information Processing
    Systems*, 36, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramé et al. [2024] Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert
    Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits
    of weight averaged reward models. *arXiv preprint arXiv:2401.12187*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roberts [1979] Kevin Roberts. The characterization of implementable choice rules.
    *Aggregation and revelation of preferences*, 12(2):321–348, 1979.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rochet [1987] Jean-Charles Rochet. A necessary and sufficient condition for
    rationalizability in a quasi-linear context. *Journal of mathematical Economics*,
    16(2):191–200, 1987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosset et al. [2024] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce,
    Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language
    models to self-improve with general preferences. *arXiv preprint arXiv:2404.03715*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saks and Yu [2005] Michael Saks and Lan Yu. Weak monotonicity suffices for truthfulness
    on convex domains. In *Proceedings of the 6th ACM conference on Electronic commerce*,
    pages 286–293, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sandholm and Likhodedov [2015] Tuomas Sandholm and Anton Likhodedov. Automated
    design of revenue-maximizing combinatorial auctions. *Operations Research*, 63(5):1000–1025,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. [2023] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong
    Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment:
    A survey. *arXiv preprint arXiv:2309.15025*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soumalias et al. [2024] Ermis Soumalias, Michael J Curry, and Sven Seuken. Truthful
    aggregation of llms with an application to online advertising. *arXiv preprint
    arXiv:2405.05905*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang and Sandholm [2012] Pingzhong Tang and Tuomas Sandholm. Mixed-bundling
    auctions with reserve prices. In *AAMAS*, pages 729–736, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vickrey [1961] William Vickrey. Counterspeculation, auctions, and competitive
    sealed tenders. *The Journal of finance*, 16(1):8–37, 1961.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2024] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang
    Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in
    large language models part ii: Reward modeling. *arXiv preprint arXiv:2401.06080*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s
    game of thoughts: Battle against deception through recursive contemplation. *arXiv
    preprint arXiv:2310.01320*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023b] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language
    models with human: A survey. *arXiv preprint arXiv:2307.12966*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2024] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
    Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained
    human feedback gives better rewards for language model training. *Advances in
    Neural Information Processing Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2023a] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. Exploring large language models for communication games:
    An empirical study on werewolf. *arXiv preprint arXiv:2309.04658*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2023b] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language
    agents with reinforcement learning for strategic play in the werewolf game. *arXiv
    preprint arXiv:2310.18940*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2024] Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing
    Sun, and Chuang Gan. Improving reinforcement learning from human feedback with
    efficient reward model ensemble. *arXiv preprint arXiv:2401.16635*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Omitted proof in Section [3](#S3 "3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See [3.3](#S3.Thmtheorem3 "Theorem 3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3
    Incentives for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For the case that $\vec{w}=1$ can be written as a programming problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}})\coloneqq\arg\max_{\theta\in\Theta}\quad
    f(v_{1}(\theta;\text{rm}_{1}),\cdots$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{LLM}_{\theta}({\bm{x}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: Because of the (3) of [3.2](#S3.Thmtheorem2 "Assumption 3.2\. ‣ 3.1 Necessity
    of Payment Rule ‣ 3 Incentives for General Training Rules ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), we can infer that the condition
    $\text{LLM}_{\theta}({\bm{x}})\geq 0$ (Luenberger et al. [[1984](#bib.bib40)]),
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"), $\frac{\partial
    v_{i}}{\partial\text{LLM}_{\theta}({\bm{x}})}=\text{rm}_{i}({\bm{x}})$, so we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'When the real reward model for agent $i$. We denote $S$. Then we take a small
    $\epsilon<\text{rm}_{i}({\bm{x}}_{2})$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\text{rm}^{\prime}_{i}({\bm{x}})=\begin{cases}\text{rm}_{i}({\bm{x}})+\epsilon,&amp;{\bm{x}}={\bm{x}}_{1},\\
    \text{rm}_{i}({\bm{x}})-\epsilon,&amp;{\bm{x}}={\bm{x}}_{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \text{rm}_{i}({\bm{x}}),&amp;{\bm{x}}\neq{\bm{x}}_{1},{\bm{x}}\neq{\bm{x}}_{2}.\end{cases}$$
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, $\text{rm}^{\prime}_{i}$ and $\theta^{\prime}={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}})$
    and $\mu^{\prime}$ and we can derive the following results.
  prefs: []
  type: TYPE_NORMAL
- en: (a) $$\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})></math>, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})}\leq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{1})}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: With $\text{rm}^{\prime}_{i}({\bm{x}}_{1})></math>. However, since for all <math
    id=$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial f}{\partial v_{i}}\text{rm}_{i}({\bm{x}})\leq\frac{\partial
    f}{\partial v_{i}}\text{rm}^{\prime}_{i}({\bm{x}}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: to satisfy the optimal condition in (1), there must be for all ${\bm{x}}\neq{\bm{x}}_{1}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}})}<\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}})}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Which is equivalent to $\text{LLM}_{\theta^{\prime}}({\bm{x}})<\text{LLM}_{\theta}({\bm{x}})$,
    can be proved by totally same method.
  prefs: []
  type: TYPE_NORMAL
- en: (b) The order of $\text{LLM}_{\theta}({\bm{x}})$ such that $\text{LLM}_{\theta^{\prime}}({\bm{x}}_{3})\geq\text{LLM}_{\theta}({\bm{x}}_{3})$.
    Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{3})}\geq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{3})}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since $\partial f/\partial D<0$, to satisfy (1), there must be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}})}\geq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}})}$ |  |'
  prefs: []
  type: TYPE_TB
- en: which is equivalent to $\text{LLM}_{\theta^{\prime}}({\bm{x}})\geq\text{LLM}_{\theta}({\bm{x}})$,
    there is $\text{LLM}_{\theta^{\prime}}({\bm{x}})\leq\text{LLM}_{\theta}({\bm{x}})$.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, with the results in (a) and (b), when $\text{LLM}_{\theta^{\prime}}({\bm{x}})\leq\text{LLM}_{\theta}({\bm{x}})$,
    there is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle v_{i}({\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}});\text{rm}_{i})-v_{i}({\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}});\text{rm}_{i})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: When $\text{LLM}_{\theta^{\prime}}({\bm{x}})\geq\text{LLM}_{\theta}({\bm{x}})$,
    there is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle v_{i}({\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}});\text{rm}_{i})-v_{i}({\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}});\text{rm}_{i})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: Note that both (2) and (3) are because of $\text{rm}_{i}({\bm{x}}_{1})\geq\text{rm}_{i}({\bm{x}}_{2})$,
    the “<math id=$$”s are hold. ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics
    of Payment Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design
    for LLM Fine-tuning with Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We first prove the necessity: if ${\psi}$ satisfies DSIC. Then for any $\text{rm}_{i},\text{rm}^{\prime}_{i}\in\mathcal{R}_{i}$,
    $k\geq 0$. By the property of DSIC, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle v_{i}({\psi}((\text{rm}_{i}^{j+1},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By definition of the function $l$, this is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Sum over all $j$, we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This means that ${\psi}$ satisfies cycle monotonicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we prove the sufficiency: if ${\psi}$ for any sequence from $\text{rm}_{i}$.
    In formal,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By cycle monotonicity, we have that for any finite and distinct sequence $[\text{rm}_{i},\text{rm}_{i}^{1},\text{rm}_{i}^{2},\cdots,\text{rm}_{i}^{k},\text{rm}^{\prime}_{i}]$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By the arbitrariness of the sequence, we can infer that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Since $l(\text{rm}^{\prime}_{i},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    such that for any agent $i$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\text{rm}^{*}$ is defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{rm}^{*}({\bm{x}})=1/&#124;T^{\ast}&#124;\quad\forall{\bm{x}}\in
    T^{\ast}.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Then, for any $\text{rm}_{i}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that (2) comes from the definition of $V$ that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle V(\text{rm}^{*},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+l(\text{rm}^{\prime}_{i},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This means that mechanism $({\psi},p)$ is implementable. ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [3.7](#S3.Thmtheorem7 "Proposition 3.7\. ‣ 3.2 Characteristics of Payment
    Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We follow the result Theorem 1.37 in Nisan et al. [[2007](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: Lemma A.1  (Theorem 1.37 in Nisan et al. [[2007](#bib.bib44)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume that the $\mathcal{R}_{1},\mathcal{R}_{2},\cdots,\mathcal{R}_{n}$ satisfy
    payment equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: Since in our paper, we assume that for all $i\in[n]$-dim vectors, this is a
    connected set in the usual metric in the Euclidean space. Therefore, the theorem
    holds. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Omitted proof in Section [4](#S4 "4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See [4.2](#S4.Thmtheorem2 "Theorem 4.2\. ‣ 4.1 Affine Maximizer Payment ‣ 4
    Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We assume that for group $i$ and the initial model is $\theta_{\text{init}}$.
  prefs: []
  type: TYPE_NORMAL
- en: (1) $({\psi},p^{AFF})$ satisfies DSIC.
  prefs: []
  type: TYPE_NORMAL
- en: We compare the utility between reporting $(\text{rm}_{i},w_{i})$. For convenience,
    we first simplify the notations by letting
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\theta^{\prime}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The valuation of group $i$ is the valuation for each agent multiply the real
    agent number:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle v_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle v_{i}^{\prime}$ |  |'
  prefs: []
  type: TYPE_TB
- en: According to the payment rule $p^{AFF}$ for $(\text{rm}^{\prime}_{i},w^{\prime}_{i})$
    is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p_{i}^{\prime}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, we can calculate the change in the utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle u_{i}^{\prime}-u_{i}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: The last inequality holds by the definition of $\theta$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta={\psi}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i})),\theta_{\text{init}})=\arg\max_{\hat{\theta}\in\Theta}ASW((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i})),\hat{\theta};\theta_{\text{init}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we can conclude that, for all $\overrightarrow{\text{rm}}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle u_{i}((\overrightarrow{\text{rm}},\vec{w});{\psi},p^{AFF},\text{rm}_{i},w_{i})\geq
    u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i}));{\psi},p^{AFF},\text{rm}_{i},w_{i}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: (2) $({\psi},p^{AFF})$ satisfies IR.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reuse the notations above and denote $\theta_{-i}$ truthfully report its
    reward model $\text{rm}_{i}$, the utility can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle u_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=w_{i}v_{i}(\theta_{-i};\text{rm}_{i})\geq 0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we can conclude that, for all $\overrightarrow{\text{rm}}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle u_{i}((\overrightarrow{\text{rm}},\vec{w});{\psi},p^{AFF},\text{rm}_{i},w_{i})\geq
    0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [4.4](#S4.Thmtheorem4 "Proposition 4.4\. ‣ 4.1 Affine Maximizer Payment
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '(1) For $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}p({\bm{x}})\log p({\bm{x}})/q({\bm{x}})$
    as an optimization problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{{\bm{x}}\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}({\bm{x}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{LLM}_{\theta}({\bm{x}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since we have assumed that the optimal point is unique, and the optimal model
    $\text{LLM}_{\theta}$ is that there exists $\mu\in\mathbb{R}$, such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, for the input $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$
    satisfies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For convenience, we define $\Delta({\bm{x}})=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}({\bm{x}})-\sum_{i=1}^{n}w_{i}\text{rm}_{i}({\bm{x}})$
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{LLM}_{\theta^{\prime}}({\bm{x}})=\text{LLM}_{\theta}({\bm{x}})e^{\frac{1}{\lambda}(\Delta({\bm{x}})+\mu-\mu^{\prime})}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that we also have the condition
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Since $1$2, we can infer that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle 1$ |  |'
  prefs: []
  type: TYPE_TB
- en: This is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{{\bm{x}}\in T^{\ast}}\Delta({\bm{x}})\leq\mu^{\prime}-\mu\leq\max_{{\bm{x}}\in
    T^{\ast}}\Delta({\bm{x}}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, the difference for $\text{LLM}_{\theta}({\bm{x}})$ can be bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;\text{LLM}_{\theta^{\prime}}({\bm{x}})-\text{LLM}_{\theta}({\bm{x}})&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\left&#124;1-e^{\frac{1}{\lambda}(\Delta({\bm{x}})+\mu-\mu^{\prime})}\right&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For any $$\delta></math>, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '(2) For $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}(p({\bm{x}})-q({\bm{x}}))^{2}$
    as an optimization problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{x\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{LLM}_{\theta}(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since we have assumed that the optimal point is unique, and the optimal model
    $\text{LLM}_{\theta}$ is that there exists $\mu\in\mathbb{R}$, such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, for the input $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$
    satisfies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For convenience, we define $\Delta(x)=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)-\sum_{i=1}^{n}w_{i}\text{rm}_{i}(x)$
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{LLM}_{\theta^{\prime}}(x)=\text{LLM}_{\theta}(x)+\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that we also have the condition
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Since $\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)=1$, we can infer that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{x\in T^{\ast}}\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime})=0.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mu^{\prime}-\mu=\frac{1}{&#124;T^{\ast}&#124;}\sum_{x\in
    T^{\ast}}\Delta(x).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, the difference for $\text{LLM}_{\theta}(x)$ can be bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For any <math id=$$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;\text{LLM}_{\theta^{\prime}}(x)-\text{LLM}_{\theta}(x)&#124;\leq\frac{1}{\lambda}\max_{x\in
    T^{\ast}}&#124;\Delta(x)&#124;\leq\delta.$ |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [4.5](#S4.Thmtheorem5 "Theorem 4.5\. ‣ 4.1 Affine Maximizer Payment ‣ 4
    Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We prove the equivalent version of payment equivalence: For any group $i$,
    $p^{\prime}$ for any $\text{rm}_{i}$ and will omit these notations.*'
  prefs: []
  type: TYPE_NORMAL
- en: We first redefine the functions $l(\cdot,\cdot)$ are defined on the types space
    of the group. For the simplified case in [Section 3](#S3 "3 Incentives for General
    Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"),
    the type is exactly the reward model $\text{rm}_{i}$ to represent the combination
    $(\text{rm}_{i},w_{i})$ is used to represented for the $\text{rm}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the simplified version discussed in [Section 3](#S3 "3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), we let $l(t^{\prime}_{i},t_{i})$. In formal,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: And $V(t^{\prime}_{i},t_{i})$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: We first prove the following lemma, which is a special case in Heydenreich et al.
    [[2009](#bib.bib29)],
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An implemented training rule ${\psi}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume there is a mechanism $({\psi},p)$, let $t_{i}^{0}=t^{\prime}_{i}$, we
    have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Sum over $j$, we get the following inequality
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{j=0}^{k}l(t_{i}^{j},t_{i}^{j+1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Since this holds for arbitrary finite and distinct sequences, we can infer that
    $V(t^{\prime}_{i},t_{i})\geq p(t_{i})-p(t^{\prime}_{i})$, there is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which means that $p(t^{\prime}_{i})-p(t_{i})=V(t_{i},t^{\prime}_{i})$, the payment
    $p(t_{i})$ both implement ${\psi}$ as $p^{*}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle p_{i}(t_{i})-p^{\prime}_{i}(t_{i})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: Note that $p^{*}$, $\vec{w}_{-i}$ on $(\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: Then we show that under [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine
    Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), SW-Maximizing training rule satisfies
    the condition stated in [Lemma B.1](#A2.Thmtheorem1 "Lemma B.1\. ‣ Proof. ‣ Appendix
    B Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"). Firstly, we show that for any $t_{i},t^{\prime}_{i}$ and $V(t^{\prime}_{i},t_{i})$
    to $t_{i}$. Since SW-Maximizing training rule is implementable, by [Theorem 3.5](#S3.Thmtheorem5
    "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), we know that the weight for any cycle is non-negative. Therefore,
    $V(t_{i},t^{\prime}_{i})+V(t^{\prime}_{i},t_{i})\geq 0$ must be satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: Then we show that for any $t_{i},t^{\prime}_{i}$ such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: This is suffice for $V(t_{i},t^{\prime}_{i})+V(t^{\prime}_{i},t_{i})\leq\epsilon$
    and $\sum_{j=0}^{k}l(t_{i}^{j+1},t_{i}^{j})$ respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we rewrite the LHS of [Equation 2](#A2.E2 "In Proof. ‣ Appendix B
    Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models") by the definition of the function $l(\cdot,\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: In the above equations, $\theta^{j}={\psi}(t_{i}^{j})$.
  prefs: []
  type: TYPE_NORMAL
- en: By [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣
    4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models"), when $\overrightarrow{\text{rm}}_{-i}$ such that if
    $\max_{x\in T^{\ast}}|w_{i}\text{rm}_{i}(x)-w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)|\leq\delta$.
  prefs: []
  type: TYPE_NORMAL
- en: We construct the sequence $P$. For each $0\leq j\leq n$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{i}^{j}=w_{i}^{0}=w_{i},\quad\text{rm}_{i}^{j}=\text{rm}_{i}^{j-1}+j(\frac{\text{rm}^{*}-\text{rm}}{n}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: And for each $n+1\leq j\leq 2n+1$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: In this construction, any $\text{rm}_{i}^{j}$. This ensures that the all reward
    models in the sequence is valid (normalized and non-negative). We can then divide
    the above equation into three parts, making the $w_{i}$ the same in the first
    and the last parts.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  | (a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+$ |  | (b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+$ |  | (c) |'
  prefs: []
  type: TYPE_TB
- en: We first show that (b) equals to $0$ and the uniqueness of the optimal point,
    we have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $$\displaystyle></math> |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that $\text{rm}^{*}(x)=\frac{1}{|T^{\ast}|}$. Thus, the above equation
    can rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: This contradicted the optimality of $\theta^{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we turn to (a). By the construction, for any $x\in T^{\ast}$ holds for
    all $x$. Then we can derive that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The case is similar to (c). By the construction, for any $x\in T^{\ast}$ holds
    for all $x$. Then we can derive that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: Combining the results from (a), (b), and (c), we have that under this construction,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By the arbitrariness of <math id="A2.19.p10.11.m1.1" class="ltx_Math" alttext="\epsilon></math>.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is proven that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V(t_{i},t^{\prime}_{i})+V(t_{i},t^{\prime}_{i})=0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: which means that $V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i})$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [4.6](#S4.Thmtheorem6 "Corollary 4.6\. ‣ 4.1 Affine Maximizer Payment ‣
    4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given the payment equivalence of ${\psi}$ here.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{h_{i}}\quad$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The solution of this programming can be trivially given by,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle h_{i}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=:$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, the revenue-maximizing payment is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+$ |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $\text{rm},\text{rm}^{\prime}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})&#124;\leq\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can derive that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\epsilon=\epsilon.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [4.8](#S4.Thmtheorem8 "Lemma 4.8\. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\hat{\theta}={\psi}(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta_{\text{init}})$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\hat{\theta};\theta_{\text{init}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(1)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\hat{\theta};\widehat{\text{rm}}_{i})-\epsilon\right)-\lambda
    D(\hat{\theta}&#124;&#124;\theta_{\text{init}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\hat{\theta};\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(2)}{\geq}ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{i=1}^{n}w_{i}v_{i}(\theta;\widehat{\text{rm}}_{i})-\lambda
    D(\theta&#124;&#124;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{(3)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\theta;\text{rm}_{i})-\epsilon\right)-\lambda
    D(\theta&#124;&#124;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-2\sum_{i=1}^{n}w_{i}\epsilon.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: (1) and (3) can be directly induced by [Lemma B.2](#A2.Thmtheorem2 "Lemma B.2\.
    ‣ Appendix B Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), and (2) holds by the definition of $\hat{\theta}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [4.9](#S4.Thmtheorem9 "Theorem 4.9\. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recall that the calculation of payment in $p^{AFF}$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Let $\vec{w}=(w_{i},\vec{w}_{-i})$, the utility function can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w};{\psi},p,\text{rm}_{i},w_{i})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta_{-i};\theta_{\text{init}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where we define $\theta={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w},\theta_{\text{init}})$
    or $w_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can derive that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(1)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(4)}{=}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\overset{(5)}{\geq}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: All the $\hat{\theta}$ and $\widehat{\text{rm}}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
