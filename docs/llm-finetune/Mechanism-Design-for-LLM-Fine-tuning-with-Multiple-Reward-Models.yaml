- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:36:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:36:32'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Mechanism Design for LLM Fine-tuning with Multiple Reward Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多奖励模型的LLM微调机制设计
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16276](https://ar5iv.labs.arxiv.org/html/2405.16276)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16276](https://ar5iv.labs.arxiv.org/html/2405.16276)
- en: Haoran Sun
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 孙浩然
- en: Peking University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: sunhaoran0301@stu.pku.edu.cn    Yurong Chen
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: sunhaoran0301@stu.pku.edu.cn    陈宇荣
- en: Peking University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: chenyurong@pku.edu.cn    Siwei Wang
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: chenyurong@pku.edu.cn    王思伟
- en: Microsoft Research Asia
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微软亚洲研究院
- en: siweiwang@microsoft.com    Wei Chen
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: siweiwang@microsoft.com    陈伟
- en: Microsoft Research Asia
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 微软亚洲研究院
- en: weic@microsoft.com    Xiaotie Deng
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: weic@microsoft.com    邓小铁
- en: Peking University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: xiaotie@pku.edu.cn
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: xiaotie@pku.edu.cn
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent research on fine-tuning large language models (LLMs) through the aggregation
    of multiple preferences has attracted considerable attention. However, the existing
    literature predominantly focuses on the empirical performance of aggregation algorithms,
    while neglecting the underlying motivation for agents to misreport their preferences.
    In this paper, we formalize this as a multi-parameter mechanism design problem,
    where an LLM provider designs both training and payment rules to achieve specific
    objectives and promote the truthful reporting of preferences. Firstly, we claim
    the necessity of a payment scheme by demonstrating that without payments, truth-telling
    is a strictly dominated strategy under a wide range of training rules. Then, we
    introduce the affine maximizer payment scheme for the social welfare maximizing
    training rules that are widely used in practice, which ensures both dominant-strategy
    incentive compatibility (DSIC) and individual rationality (IR). Furthermore, we
    prove that under mild conditions, any other payment rule that also implements
    these training rules in DSIC can be converted to the affine maximizer payment
    by adding a factor irrelevant to the agents’ own reports. We also show that this
    mechanism satisfies approximate DSIC when the input of the mechanism is a biased
    version of the reported preferences, showcasing its robustness in real-world applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于通过聚合多个偏好来微调大语言模型（LLMs）的研究引起了广泛关注。然而，现有文献主要集中在聚合算法的实际性能上，而忽略了代理人虚报其偏好的根本动机。在本文中，我们将其形式化为一个多参数机制设计问题，其中LLM提供者设计训练和支付规则，以实现特定目标并促进真实偏好的报告。首先，我们通过展示在广泛的训练规则下，没有支付的情况下真实报告是严格被支配的策略，来说明支付方案的必要性。然后，我们介绍了用于社会福利最大化训练规则的仿射最大化支付方案，这些规则在实践中广泛使用，确保了主导策略激励相容性（DSIC）和个人理性（IR）。此外，我们证明在温和条件下，任何其他也在DSIC中实现这些训练规则的支付规则，都可以通过添加一个与代理人自身报告无关的因子转化为仿射最大化支付方案。我们还展示了当机制的输入是报告偏好的有偏版本时，该机制满足近似DSIC，展示了其在现实世界应用中的鲁棒性。
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The pre-training and fine-tuning paradigm is fundamental in developing language
    models (Devlin et al. ([2018](#bib.bib16)); Radford et al. ([2018](#bib.bib49));
    Liu et al. ([2019](#bib.bib38)); Touvron et al. ([2023](#bib.bib60))). During
    pre-training, the model is fed with vast amounts of data to acquire a general
    capability to understand and generate language through self-supervised learning.
    The subsequent fine-tuning phase customizes these pre-trained models for specific
    downstream tasks using smaller, task-oriented datasets, ensuring that the model
    outputs are more closely aligned with particular requirements. As LLMs gain increasing
    popularity, there is a growing demand for fine-tuning basic LLMs, as basic models
    often fail to meet users’ demands, especially in catering to individual preferences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和微调范式是开发语言模型的基础（Devlin et al. ([2018](#bib.bib16)); Radford et al. ([2018](#bib.bib49));
    Liu et al. ([2019](#bib.bib38)); Touvron et al. ([2023](#bib.bib60)））。在预训练阶段，模型通过自监督学习接收大量数据，从而获得理解和生成语言的通用能力。随后，微调阶段使用较小的任务导向数据集来定制这些预训练模型，以确保模型的输出更符合特定要求。随着大语言模型（LLMs）的日益流行，对基础LLMs的微调需求也在增长，因为基础模型往往无法满足用户的需求，特别是在迎合个人偏好方面。
- en: The process of fine-tuning an LLM to align with certain human preferences is
    challenging to achieve through supervision (Ji et al. ([2023](#bib.bib34)); Köpf
    et al. ([2024](#bib.bib35)); Wang et al. ([2023b](#bib.bib64)); Shen et al. ([2023](#bib.bib57))),
    primarily due to the difficulty in constructing datasets with a substantial number
    of valid question-answer pairs for supervised training. Reinforcement learning
    from human feedback (RLHF) (Ouyang et al. ([2022](#bib.bib46)); Christiano et al.
    ([2017](#bib.bib10))) offers a promising solution to this problem. In RLHF, a
    reward model is first trained to be used as a proxy for human judgment. This model
    then provides reward signals for the standard reinforcement learning process.
    This technique of fine-tuning with a reward model has proven effective in encoding
    human preferences into models and has become a fundamental component of the training
    process for most advanced LLMs. With the advancement of RLHF, numerous studies
    have investigated efficient methods for aggregating multiple preferences into
    a single fine-tuned model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监督来微调LLM以符合某些人类偏好是一个具有挑战性的任务（Ji et al. ([2023](#bib.bib34)); Köpf et al. ([2024](#bib.bib35));
    Wang et al. ([2023b](#bib.bib64)); Shen et al. ([2023](#bib.bib57)))，主要由于构建具有大量有效问答对的数据集以进行监督训练的难度。来自人类反馈的强化学习（RLHF）（Ouyang
    et al. ([2022](#bib.bib46)); Christiano et al. ([2017](#bib.bib10)))提供了一个有前途的解决方案。在RLHF中，首先训练一个奖励模型作为人类判断的代理。该模型随后为标准的强化学习过程提供奖励信号。这种使用奖励模型进行微调的技术已被证明在将人类偏好编码到模型中是有效的，并已成为大多数先进LLMs训练过程中的一个基本组成部分。随着RLHF的进展，许多研究已经探讨了将多种偏好聚合到一个微调模型中的高效方法。
- en: However, most of these studies focus primarily on improving empirical performance
    across various metrics (Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Coste et al. ([2023](#bib.bib14)); Zhang et al.
    ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62)); Eisenstein et al. ([2023](#bib.bib21))).
    They often implicitly assume that we are accessible to real preferences, neglecting
    the possibility of agents’ misreporting their preferences. This problem becomes
    more crucial when we consider a real-world scenario, where different agents provide
    their preferences for the aggregation. In such cases, agents may engage in strategic
    misreporting to increase their utility. An intuitive example is that if an agent
    knows beforehand that the fine-tuning process aims to neutralize all preferences,
    it might pretend to have a more polarized preference as a beneficial strategy.
    These strategic behaviors can distort the final training results, even if the
    trained algorithm is highly effective. Nevertheless, this issue has not attracted
    sufficient attention in the existing literature, particularly concerning the fine-tuning
    process of LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数这些研究主要集中在提高各种指标的经验性能上（Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Coste et al. ([2023](#bib.bib14)); Zhang et
    al. ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62)); Eisenstein et al.
    ([2023](#bib.bib21)))。它们通常隐含地假设我们可以获得真实的偏好，忽视了代理人可能会误报其偏好的可能性。当我们考虑到现实世界场景时，这个问题变得更加重要，因为不同的代理人提供其偏好进行汇总。在这种情况下，代理人可能会进行战略性误报以增加其效用。一个直观的例子是，如果代理人事先知道微调过程旨在中和所有偏好，它可能会伪装成具有更极端的偏好作为一种有利的策略。这些战略行为可能会扭曲最终的训练结果，即使训练出的算法非常有效。然而，这个问题在现有文献中尚未受到足够的关注，特别是在LLMs的微调过程中。
- en: Our Contribution.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的贡献。
- en: In this paper, we mainly study the incentive design in such scenarios. First,
    we formalize this as a multi-parameter mechanism design problem between a fine-tuning
    service provider and groups of agents seeking fine-tuning services. The provider
    proposes a mechanism that includes a *training rule* for integrating different
    groups’ preferences into a fine-tuned model and a *payment rule* to charge the
    groups. After observing the mechanism, each group strategically reports its preference
    to maximize its utility. We consider that the subsequent fine-tuning process is
    implemented using RLHF, a standard method for aligning a model with human preference.
    Therefore, we abstract the preference of each group to be reward models, and term
    the whole scenario the *RLHF Game*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们主要研究这类场景中的激励设计。首先，我们将其形式化为一个多参数机制设计问题，该问题涉及一个微调服务提供商和寻求微调服务的多个代理群体。提供商提出了一种机制，其中包括一个*训练规则*，用于将不同群体的偏好整合到微调后的模型中，以及一个*支付规则*，用于向这些群体收费。在观察到该机制后，每个群体会策略性地报告其偏好以最大化自身效用。我们考虑到后续的微调过程是使用RLHF这一标准方法来使模型与人类偏好对齐。因此，我们将每个群体的偏好抽象为奖励模型，并将整个场景称为*RLHF游戏*。
- en: Secondly, we demonstrate the profitability of misreporting a polarized preference
    under a wide range of mechanisms that include only a training rule ([Theorem 3.3](#S3.Thmtheorem3
    "Theorem 3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). This
    underscores the necessity of a payment rule to address incentive issues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们展示了在仅包含训练规则的各种机制下，虚报极化偏好的获利情况 ([定理 3.3](#S3.Thmtheorem3 "定理 3.3. ‣ 3.1
    支付规则的必要性 ‣ 3 一般训练规则的激励 ‣ 具有多个奖励模型的LLM微调的机制设计"))。这突显了支付规则在解决激励问题中的必要性。
- en: Thirdly, we focus on a representative set of training rules, termed the SW-Maximizing
    training rules, in which the provider aims to maximize social welfare while incorporating
    different regularization measures. For SW-Maximizing training rules, we propose
    the affine maximizer payment scheme, a weighted version of the Vickrey-Clarke-Groves
    (VCG) payment (Vickrey ([1961](#bib.bib61)); Clarke ([1971](#bib.bib11)); Groves
    ([1973](#bib.bib26))). We prove that agents truthfully reporting their preferences
    constitutes a dominant strategy in such mechanisms ([Theorem 4.2](#S4.Thmtheorem2
    "Theorem 4.2\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). Utilizing
    the notion of payment equivalence, we prove that under a mild condition, any other
    payment rule that also implements these training rules in dominant-strategy incentive
    compatibility (DSIC) can be converted to the affine maximizer payment by adding
    a factor irrelevant to agents’ own reports ([Theorem 4.5](#S4.Thmtheorem5 "Theorem
    4.5\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣
    Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). We validate
    this condition for many commonly used regularization terms like KL-divergence ([Proposition 4.4](#S4.Thmtheorem4
    "Proposition 4.4\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
    Consequently, we derive the revenue-maximizing payment rule that implements SW-Maximizing
    training rules in both DSIC and individual rationality (IR) ([Corollary 4.6](#S4.Thmtheorem6
    "Corollary 4.6\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
    Furthermore, we show that this mechanism remains approximately DSIC when the input
    of the mechanism is a biased version of the reported preferences, which is an
    abstraction modeling for the inevitable errors that occur in practice. This showcases
    the robustness of the proposed mechanisms in real-world applications ([Theorem 4.9](#S4.Thmtheorem9
    "Theorem 4.9\. ‣ 4.2 Approximate Valuation ‣ 4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们关注一组具有代表性的训练规则，称为SW-最大化训练规则，其中提供者旨在最大化社会福利，同时结合不同的正则化措施。对于SW-最大化训练规则，我们提出了仿射最大化支付方案，这是Vickrey-Clarke-Groves
    (VCG)支付的加权版本（Vickrey ([1961](#bib.bib61))；Clarke ([1971](#bib.bib11))；Groves ([1973](#bib.bib26))）。我们证明了在这种机制中，代理人真实报告其偏好是一种主导策略（[定理
    4.2](#S4.Thmtheorem2 "定理 4.2. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。利用支付等价的概念，我们证明了在一个温和的条件下，任何其他也实现这些训练规则的支付规则在主导策略激励兼容性（DSIC）下都可以通过添加一个与代理人自身报告无关的因素转化为仿射最大化支付（[定理
    4.5](#S4.Thmtheorem5 "定理 4.5. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。我们验证了这一条件适用于许多常用的正则化项，如KL散度（[命题
    4.4](#S4.Thmtheorem4 "命题 4.4. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。因此，我们推导出了在DSIC和个人理性（IR）下实施SW-最大化训练规则的收益最大化支付规则（[推论
    4.6](#S4.Thmtheorem6 "推论 4.6. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。此外，我们还展示了当机制的输入是报告偏好的偏置版本时，这一机制在近似DSIC下仍然有效，这是一种对实际中不可避免的错误的抽象建模。这展示了所提出机制在现实世界应用中的鲁棒性（[定理
    4.9](#S4.Thmtheorem9 "定理 4.9. ‣ 4.2 近似估值 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。
- en: Primary Related Work.
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要相关工作。
- en: Several studies have investigated similar scenarios. Among them, Duetting et al.
    ([2023](#bib.bib20)) and Soumalias et al. ([2024](#bib.bib58)) are most related
    to ours. Duetting et al. ([2023](#bib.bib20)) examines the problem of designing
    a mechanism to aggregate multiple agents’ preferences based on each agent’s bids
    and determine their payments. However, they exclude the case where preferences
    can be misreported, which is the primary concern in our study. The concurrent
    work by Soumalias et al. ([2024](#bib.bib58)) also considers the mechanism design
    for aggregating multiple preferences. Their focus is mainly on the practical implementation
    of SW-Maximizing training rule with KL-divergence and the payment scheme that
    obtains both DSIC and interpretability. However, in this scenario, we are more
    concerned with the theoretical properties of more general mechanisms, including
    the implementability and the property of payment equivalence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究调查了类似的场景。其中，Duetting 等人 ([2023](#bib.bib20)) 和 Soumalias 等人 ([2024](#bib.bib58))
    与我们的研究最相关。Duetting 等人 ([2023](#bib.bib20)) 研究了设计一个机制以汇总多个代理的偏好，基于每个代理的竞标并确定他们的支付。然而，他们排除了偏好可能被错误报告的情况，这正是我们研究中的主要关注点。Soumalias
    等人 ([2024](#bib.bib58)) 的同期工作也考虑了汇总多个偏好的机制设计。他们的重点主要是实现 SW-Maximizing 训练规则与 KL-divergence
    的实际应用以及获取 DSIC 和解释性的支付方案。然而，在这种情况下，我们更关注的是更一般机制的理论性质，包括可实现性和支付等效性。
- en: Additionally, there are works studying other scenarios related to LLMs from
    the perspective of algorithmic game theory. Laufer et al. ([2023](#bib.bib36))
    abstracts the fine-tuning process as a bargaining game and characterizes the perfect
    sub-game equilibria. Dubey et al. ([2024](#bib.bib19)) proposes an auction where
    bidders compete to place their content within a summary generated by an LLM. Conitzer
    et al. ([2024](#bib.bib13)) considers incorporating social choice theory in LLM
    alignment. Feizi et al. ([2023](#bib.bib23)) explores the potential for leveraging
    LLMs in online advertising systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些研究从算法博弈论的角度研究与LLMs相关的其他场景。Laufer 等人 ([2023](#bib.bib36)) 将微调过程抽象为一种讨价还价游戏，并描述了完美子博弈均衡。Dubey
    等人 ([2024](#bib.bib19)) 提出了一个拍卖，其中竞标者竞争将他们的内容放入由LLM生成的摘要中。Conitzer 等人 ([2024](#bib.bib13))
    考虑在LLM对齐中引入社会选择理论。Feizi 等人 ([2023](#bib.bib23)) 探讨了在在线广告系统中利用LLMs的潜力。
- en: Paper Organization.
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 论文组织。
- en: In [Section 2](#S2 "2 Preliminaries and Model ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), we provide the preliminaries and the formal description
    of the RLHF Game. In [Section 3](#S3 "3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"), we study
    the incentive design for general training rules in the RLHF Game. We demonstrate
    the properties of mechanisms that consist of SW-Maximizing training rules and
    payment rules in [Section 4](#S4 "4 Social Welfare Maximizing Mechanism ‣ Mechanism
    Design for LLM Fine-tuning with Multiple Reward Models"). Further related work
    is provided in [Section 5](#S5 "5 Further Related Work ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), and we conclude in [Section 6](#S6
    "6 Discussion and Conclusion ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第2节](#S2 "2 Preliminaries and Model ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")，我们提供了预备知识和RLHF游戏的正式描述。在 [第3节](#S3 "3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")，我们研究了RLHF游戏中通用训练规则的激励设计。我们在 [第4节](#S4 "4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
    中展示了由 SW-Maximizing 训练规则和支付规则组成的机制的性质。进一步相关工作在 [第5节](#S5 "5 Further Related Work
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") 中提供，我们在 [第6节](#S6
    "6 Discussion and Conclusion ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models") 中总结。
- en: 2 Preliminaries and Model
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识与模型
- en: 2.1 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 预备知识
- en: Large Language Models.
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型。
- en: Large language models (LLMs) function as mappings from a sequence of tokens
    to a probability distribution over the next token. The input sequence is usually
    constrained by a maximum length $K$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）作为从一系列标记到下一个标记的概率分布的映射进行工作。输入序列通常受到最大长度 $K$ 的限制。
- en: 'An LLM parameterized by $\theta\in\Theta$. For practical purposes, the output
    sequence is also required to be of finite length. We assume the maximum output
    length is also $K$ generated by $g_{\theta}$ is given by:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由 $\theta\in\Theta$ 参数化的LLM。为了实际目的，输出序列也要求是有限长度的。我们假设由 $g_{\theta}$ 生成的最大输出长度也是
    $K$，公式如下：
- en: '|  | $\text{LLM}_{\theta}({\bm{x}})=\prod_{t=1}^{&#124;\bm{x}&#124;}g_{\theta}(x_{t}\mid{\bm{x}}_{<t}),$
    |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{LLM}_{\theta}({\bm{x}})=\prod_{t=1}^{&#124;\bm{x}&#124;}g_{\theta}(x_{t}\mid{\bm{x}}_{<t}),$
    |  |'
- en: where ${\bm{x}}_{<t}$. $\text{LLM}_{\theta}$ the probability of ${\bm{x}}$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bm{x}}_{<t}$。$\text{LLM}_{\theta}$ 的 ${\bm{x}}$ 概率。
- en: Reward Modeling.
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励建模。
- en: Reward modeling is instrumental for aligning LLMs with human preferences, particularly
    within the context of RLHF. In this process, a reward model $\text{rm}:T^{\ast}\to\mathbb{R}$
    over $T^{\ast}$. Unless otherwise stated, we use $\mathcal{R}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励建模在将 LLM 与人类偏好对齐中起着重要作用，特别是在 RLHF 的背景下。在此过程中，奖励模型 $\text{rm}:T^{\ast}\to\mathbb{R}$
    定义在 $T^{\ast}$ 上。除非另有说明，否则我们使用 $\mathcal{R}$。
- en: 2.2 Formulation of the RLHF Game
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 RLHF 游戏的公式化
- en: In this part, we present the formal description of the RLHF Game. There is one
    LLM *provider* and $n$ for all ${\bm{x}}\in T^{\ast}$. Let $\mathcal{R}$. The
    exact reward model $\text{rm}_{i}$, the valuation when it receives a model $\text{LLM}_{\theta}$
    is known by both the provider and the agents.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了 RLHF 游戏的正式描述。这里有一个 LLM *提供者* 和 $n$ 代表所有 ${\bm{x}}\in T^{\ast}$。设
    $\mathcal{R}$。确切的奖励模型 $\text{rm}_{i}$，当它接收到一个模型 $\text{LLM}_{\theta}$ 时，其估值为提供者和代理均所知。
- en: The provider first announces the mechanism, including a training rule ${\psi}$,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提供者首先公布机制，包括一个训练规则 ${\psi}$，
- en: '|  | $\displaystyle{\psi}:\mathcal{R}^{n}\times\mathcal{W}^{n}\times\Theta\to\Theta,$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\psi}:\mathcal{R}^{n}\times\mathcal{W}^{n}\times\Theta\to\Theta,$
    |  |'
- en: Both rules take $n$. In this case, the model coincides with most previous work,
    where agents’ incentives are not considered (Ramé et al. ([2024](#bib.bib51));
    Wu et al. ([2024](#bib.bib65)); Jang et al. ([2023](#bib.bib32)); Coste et al.
    ([2023](#bib.bib14)); Zhang et al. ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62));
    Eisenstein et al. ([2023](#bib.bib21))). Specifically, the training rule seeks
    to find the model that maximizes a certain objective function $f$. That is,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两种规则都涉及 $n$。在这种情况下，模型与大多数先前的工作一致，其中代理的激励未被考虑（Ramé 等人 ([2024](#bib.bib51))；吴等人
    ([2024](#bib.bib65))；张等人 ([2023](#bib.bib32))；Coste 等人 ([2023](#bib.bib14))；张等人
    ([2024](#bib.bib68))；王等人 ([2024](#bib.bib62))；Eisenstein 等人 ([2023](#bib.bib21))）。具体来说，训练规则旨在找到最大化某一目标函数
    $f$ 的模型。即，
- en: '|  | ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})\in\arg\max_{\theta\in\Theta}f(v_{1}(\theta;\text{rm}_{1}),\cdots,v_{n}(\theta;\text{rm}_{n}),\vec{w},D(\text{LLM}_{\theta}&#124;&#124;\text{LLM}_{\theta_{\text{init}}})),$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})\in\arg\max_{\theta\in\Theta}f(v_{1}(\theta;\text{rm}_{1}),\cdots,v_{n}(\theta;\text{rm}_{n}),\vec{w},D(\text{LLM}_{\theta}&#124;&#124;\text{LLM}_{\theta_{\text{init}}})),$
    |  |'
- en: where $D$ has a unique global optimal point for any possible inputs. Hence,
    in the rest of the paper, the “$\in$ is substituted by “=”.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 对任何可能的输入都有一个唯一的全局最优点。因此，在论文的其余部分，“$\in$ 被替换为 “=”。
- en: After observing the announced mechanism (${\psi}$, and its group size $\tilde{w}_{i}$
    is optimal, i.e., the final parameter satisfies $\theta_{\text{final}}={\psi}(\overrightarrow{\widetilde{\text{rm}}},\vec{\tilde{w}},\theta_{\text{init}})$,
    so the valuation for group $i$’s utility is
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察到公布的机制（${\psi}$，以及其组大小 $\tilde{w}_{i}$ 是最优的，即最终参数满足 $\theta_{\text{final}}={\psi}(\overrightarrow{\widetilde{\text{rm}}},\vec{\tilde{w}},\theta_{\text{init}})$，所以对组
    $i$ 的效用估值为
- en: '|  | $1$2 |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The groups may strategically report, thus $\overrightarrow{\widetilde{\text{rm}}}$.
    The goal of the LLM provider is to achieve its training objective based on the
    group’s true preferences, taking into account that the misreporting may distort
    the training outcome. To this end, it is crucial to incentivize all groups to
    report their information truthfully so that the provider is accessible to the
    groups’ private information. We formally define these desiderata of a mechanism
    as follows.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 各组可能会采取策略性报告，因此 $\overrightarrow{\widetilde{\text{rm}}}$。LLM 提供者的目标是基于组的真实偏好实现其训练目标，同时考虑到误报可能会扭曲训练结果。为此，至关重要的是激励所有组真实报告其信息，以便提供者能够访问各组的私人信息。我们正式定义机制的这些期望如下。
- en: Definition 2.1.
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1。
- en: A mechanism $({\psi},p)$, $\text{rm}^{\prime}_{i}$, $\theta_{\text{init}}$,
    we have
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机制 $({\psi},p)$，$\text{rm}^{\prime}_{i}$，$\theta_{\text{init}}$，我们有
- en: '|  | $1$2 |  | (DSIC) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (DSIC) |'
- en: Definition 2.2.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.2。
- en: A mechanism $({\psi},p)$, $\overrightarrow{\text{rm}}_{-i}$, we have
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机制 $({\psi},p)$，$\overrightarrow{\text{rm}}_{-i}$，我们有
- en: '|  | $u_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i})\geq
    0.$ |  | (IR) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $u_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i})\geq
    0.$ |  | (IR) |'
- en: DSIC means that for any group, truthfully reporting the reward model and the
    group size yields the highest utility, regardless of other groups’ reports. IR
    means that truthfully reporting always yields non-negative utilities. Only when
    both DSIC and IR are satisfied, all groups are incentivized to participate in
    this game and report truthfully. When a mechanism $({\psi},p)$ in DSIC, IR or
    both DSIC and IR. Especially, when we say the implementability of a training rule,
    we refer to the property of DSIC.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: DSIC 意味着对于任何群体，真实报告奖励模型和群体大小都会带来最高的效用，而不考虑其他群体的报告。IR 意味着真实报告总是带来非负效用。只有当同时满足
    DSIC 和 IR 时，所有群体才会被激励参与此游戏并真实报告。当一个机制$({\psi},p)$在 DSIC、IR 或同时在 DSIC 和 IR 下实施时，尤其是当我们讨论训练规则的可实现性时，我们指的是
    DSIC 的属性。
- en: 3 Incentives for General Training Rules
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 一般训练规则的激励
- en: In this section, we discuss the incentive design within the RLHF Game framework.
    As a warm-up, we consider a simplified scenario where all group sizes are equal
    to $1$. Unless stated otherwise, the results directly apply to the more general
    case where $\vec{w}$ is also private information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论 RLHF 游戏框架中的激励设计。作为热身，我们考虑一个简化的情境，其中所有群体的大小都等于 $1$。除非另有说明，否则结果直接适用于更一般的情况，其中
    $\vec{w}$ 也是私有信息。
- en: For the valuation function in this section, we consider a reasonable form $v(\cdot;\cdot)$
    defined as follows.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的估值函数中，我们考虑如下定义的合理形式$v(\cdot;\cdot)$。
- en: Assumption 3.1.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 3.1。
- en: 'For any agent with preference represented by reward model rm, its valuation
    on model $\text{LLM}_{\theta}$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何以奖励模型 rm 表示的代理，其对模型 $\text{LLM}_{\theta}$ 的估值：
- en: '|  | $v(\theta;\text{rm})=\mathbb{E}_{{\bm{x}}\sim\text{LLM}_{\theta}}\text{rm}({\bm{x}})=\sum_{{\bm{x}}\in
    T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\text{rm}({\bm{x}}).$ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $v(\theta;\text{rm})=\mathbb{E}_{{\bm{x}}\sim\text{LLM}_{\theta}}\text{rm}({\bm{x}})=\sum_{{\bm{x}}\in
    T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\text{rm}({\bm{x}}).$ |  |'
- en: In practice, this can be obtained by averaging the reward of the sequences sampled
    from an LLM. We discuss the influence of possible errors in this process in [Section 4](#S4
    "4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models").
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，这可以通过对从LLM中采样的序列的奖励进行平均来获得。我们在[第4节](#S4 "4 社会福利最大化机制 ‣ LLM微调的机制设计与多重奖励模型")讨论了这一过程中可能出现的错误的影响。
- en: 3.1 Necessity of Payment Rule
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 支付规则的必要性
- en: We begin by demonstrating the necessity of payment rules to ensure incentive
    compatibility for training rules under the following assumptions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示支付规则对于确保在以下假设下训练规则的激励相容性是必要的。
- en: Assumption 3.2.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 3.2。
- en: (1) For all $i\in[n]$ exists and $\partial f/\partial D<0$ exists and is positive.
    (3) For all $\overrightarrow{\text{rm}}$ for all ${\bm{x}}\in T^{\ast}$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 对于所有 $i\in[n]$ 存在且 $\partial f/\partial D<0$ 存在且为正。(3) 对于所有 $\overrightarrow{\text{rm}}$
    对于所有 ${\bm{x}}\in T^{\ast}$。
- en: 'The rationale of these assumptions is as follows: (1) is that we assume the
    training process aims to find a model $\text{LLM}_{\theta}$. And (3) is to exclude
    some extreme training rules that the training outcome remains the same for most
    input and changes drastically. In practice, (1) is satisfied for most training
    functions $f$ and the strength of regularization. At least, they are satisfied
    by the commonly used KL-divergence.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些假设的理由如下：(1) 是我们假设训练过程旨在找到一个模型 $\text{LLM}_{\theta}$。而 (3) 是排除一些极端的训练规则，即训练结果对于大多数输入保持不变，并且变化剧烈。在实践中，(1)
    对于大多数训练函数 $f$ 和正则化强度是满足的。至少，它们被常用的 KL 散度所满足。
- en: Combining these three conditions, we show that when the preference for some
    ${\bm{x}}$ for the ${\bm{x}}$ at all.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这三个条件，我们展示了当对某些 ${\bm{x}}$ 的偏好时，对于所有 ${\bm{x}}$。
- en: Theorem 3.3.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.3。
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [3.2](#S3.Thmtheorem2 "Assumption 3.2\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), when the payment rule $p\equiv 0$, such that $\text{rm}_{i}({\bm{x}})=1/|S|$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[3.1](#S3.Thmtheorem1 "假设 3.1\. ‣ 3 一般训练规则的激励 ‣ LLM微调的机制设计与多重奖励模型")和[3.2](#S3.Thmtheorem2
    "假设 3.2\. ‣ 3.1 支付规则的必要性 ‣ 3 一般训练规则的激励 ‣ LLM微调的机制设计与多重奖励模型")，当支付规则 $p\equiv 0$
    时，$\text{rm}_{i}({\bm{x}})=1/|S|$。
- en: Here, we call a strategy strongly dominated when another strategy yields strictly
    higher utility regardless of others’ reports. [Theorem 3.3](#S3.Thmtheorem3 "Theorem
    3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") tells us
    that truthful reporting is strongly dominated with only training rules, and thus
    will not be adopted by rational agents.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们称一个策略为强主导策略，当另一个策略无论其他人的报告如何都能产生严格更高的效用时。[定理 3.3](#S3.Thmtheorem3 "定理
    3.3。 ‣ 3.1 支付规则的必要性 ‣ 3 一般训练规则的激励 ‣ 多奖励模型下的 LLM 微调的机制设计") 告诉我们，真实报告在只有训练规则的情况下是强主导的，因此理性代理将不会采用它。
- en: 3.2 Characteristics of Payment Rules
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 支付规则的特点
- en: 'Having established the necessity of payment rules in this scenario, we mainly
    address two questions in the remainder of this section: First, *given a training
    rule ${\psi}$. Second, *for an implementable training rule ${\psi}$.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了支付规则在此情境下的必要性之后，我们主要解决本节剩余部分的两个问题：首先，*给定一个训练规则 ${\psi}$。其次，*对于一个可实施的训练规则
    ${\psi}$。*
- en: 'We resolve the first question primarily by utilizing the notion of *cycle monotonicity*,
    first proposed by Rochet ([1987](#bib.bib53)). Cycle monotonicity generalizes
    monotonicity defined in a single-parameter scenario ((Myerson, [1981](#bib.bib43))).
    In the RLHF Game, we define a function as $l(\text{rm}^{\prime},\text{rm};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})\coloneqq
    v_{i}({\psi}((\text{rm},\overrightarrow{\text{rm}}_{-i}),\theta_{\text{init}});\text{rm})-v_{i}({\psi}((\text{rm}^{\prime},\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}}));\text{rm})$)
    under $\overrightarrow{\text{rm}}_{-i}$. The cycle monotonicity is defined based
    on this function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用 *周期单调性* 的概念来解决第一个问题，这一概念最早由 Rochet 提出（[1987](#bib.bib53)）。周期单调性是对在单参数场景下定义的单调性的推广（（Myerson，[1981](#bib.bib43)））。在
    RLHF 游戏中，我们定义一个函数为 $l(\text{rm}^{\prime},\text{rm};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})\coloneqq
    v_{i}({\psi}((\text{rm},\overrightarrow{\text{rm}}_{-i}),\theta_{\text{init}});\text{rm})-v_{i}({\psi}((\text{rm}^{\prime},\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}}));\text{rm})$)
    在 $\overrightarrow{\text{rm}}_{-i}$ 下。周期单调性是基于这个函数定义的：
- en: Definition 3.4  (Cycle Monotonicity).
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.4  （周期单调性）。
- en: The training rule ${\psi}$ ($k\geq 0$ we have
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练规则 ${\psi}$（$k\geq 0$ 我们有
- en: '|  | $1$2 |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For general training rules, cycle monotonicity is a sufficient and necessary
    condition for implementability.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的训练规则，周期单调性是可实施性的充分必要条件。
- en: Theorem 3.5  (Rochet ([1987](#bib.bib53))).
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.5  （Rochet（[1987](#bib.bib53)））。
- en: A training rule ${\psi}$ is implementable if and only if it satisfies cycle
    monotonicity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练规则 ${\psi}$ 当且仅当它满足周期单调性时才是可实施的。
- en: In fact, the proof of [Theorem 3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)).
    ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") is constructive.
    However, for general implementable training rules, the calculation of the payment
    rules is too complex to be practical.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[定理 3.5](#S3.Thmtheorem5 "定理 3.5（Rochet（1987））。 ‣ 3.2 支付规则的特点 ‣ 3 一般训练规则的激励
    ‣ 多奖励模型下的 LLM 微调的机制设计") 的证明是构造性的。然而，对于一般的可实施训练规则，支付规则的计算过于复杂，不具备实用性。
- en: The second question is more general, so we primarily consider the concept of
    *payment equivalence* ((Ashlagi et al., [2010](#bib.bib2))) for an implementable
    training rule.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题更为一般化，因此我们主要考虑 *支付等价性*（（Ashlagi 等，[2010](#bib.bib2)））对于一个可实施的训练规则。
- en: Definition 3.6  (Payment Equivalence).
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.6  （支付等价性）。
- en: An implementable training rule ${\psi}$ such that
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可实施的训练规则 ${\psi}$，满足
- en: '|  | $1$2 |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Or equivalently, when fixing $\overrightarrow{\text{rm}}_{-i}$ for all $\text{rm}_{i}\in\mathcal{R}_{i}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 或者等效地，当固定 $\overrightarrow{\text{rm}}_{-i}$ 对于所有 $\text{rm}_{i}\in\mathcal{R}_{i}$。
- en: Payment equivalence indicates that the only way to modify a DSIC mechanism $({\psi},p)$’s
    payment function $p_{i}$ satisfies payment equivalence and we can figure out one
    mechanism $({\psi},p)$ among all these payment rules that implement ${\psi}$ in
    both DSIC and IR.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 支付等价性表明，修改一个 DSIC 机制 $({\psi},p)$ 的支付函数 $p_{i}$ 的唯一方法是满足支付等价性，我们可以在所有这些支付规则中找出一个机制
    $({\psi},p)$，使其在 DSIC 和 IR 下实现 ${\psi}$。
- en: 'Payment equivalence is influenced by the domain of the types: reward models
    and group sizes in the RLHF Game. When $\vec{w}\equiv 1$, which is a connected
    set in the Euclidean space. Thus, we can directly apply the result in Nisan et al.
    ([2007](#bib.bib44)) and get the following theorem.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 支付等价性受到类型领域的影响：奖励模型和RLHF游戏中的群体规模。当$\vec{w}\equiv 1$时，这是欧几里得空间中的一个连通集合。因此，我们可以直接应用Nisan等人的结果（[2007](#bib.bib44)），得到以下定理。
- en: Proposition 3.7.
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 3.7。
- en: When $\vec{w}\equiv 1$ is public information and the agents only report the
    reward models, all implementable training rules satisfy payment equivalence.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当$\vec{w}\equiv 1$是公开信息且代理仅报告奖励模型时，所有可实施的训练规则都满足支付等价性。
- en: However, when the group sizes $\vec{w}$. Thus, payment equivalence may not be
    satisfied for general training rules, and we will study this for a representative
    set of training rules in the following section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当群体规模$\vec{w}$时。因而，一般训练规则可能不满足支付等价性，我们将在下一节中研究这一点，针对一组代表性的训练规则。
- en: 4 Social Welfare Maximizing Mechanism
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 社会福利最大化机制
- en: In this section, we consider the scenario where group $i$ to truthfully report
    both $\text{rm}_{i}$, though it is possible to adopt the method used in the constructive
    proof for [Theorem 3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics
    of Payment Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design
    for LLM Fine-tuning with Multiple Reward Models") to derive the payment rule,
    the resulting payment rule can be complex and impractical.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们考虑群体$i$真实报告$\text{rm}_{i}$的情景，尽管可以采用构造性证明中的方法来推导支付规则，[定理 3.5](#S3.Thmtheorem5
    "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")的结果可能使支付规则变得复杂且不切实际。
- en: Therefore, in this section, our primary focus is on a subset of training rules
    designed to maximize social welfare under regularization constraints, which is
    commonly used in practice to aggregate various preferences (Boyd and Vandenberghe
    ([2004](#bib.bib5)); Nocedal and Wright ([1999](#bib.bib45))), balancing efficiency
    and fairness.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本节的主要关注点是设计用于在正则化约束下最大化社会福利的训练规则子集，这在实践中常用于聚合各种偏好（Boyd和Vandenberghe（[2004](#bib.bib5)）；Nocedal和Wright（[1999](#bib.bib45)）），以平衡效率和公平。
- en: Definition 4.1  (SW-Maximizing Training Rules).
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.1 （SW-最大化训练规则）。
- en: 'Given the reports $\overrightarrow{\text{rm}}$. Formally, it is represented
    as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 给定报告$\overrightarrow{\text{rm}}$。形式上，它表示为：
- en: '|  | ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{i=1}^{n}w_{i}v_{i}(\theta;\text{rm}_{i})-\lambda
    D(\text{LLM}_{\theta}&#124;&#124;\text{LLM}_{\theta_{\text{init}}}).$ |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{i=1}^{n}w_{i}v_{i}(\theta;\text{rm}_{i})-\lambda
    D(\text{LLM}_{\theta}&#124;&#124;\text{LLM}_{\theta_{\text{init}}}).$ |  |'
- en: Here, $\lambda$ is a hyperparameter that adjusts regularization strength.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\lambda$是一个调整正则化强度的超参数。
- en: Note that SW-Maximizing training rules constitute a set of training rules. We
    use ${\psi}\in\Psi^{SW}$ and $\theta_{\text{init}}$. One simple way to achieve
    it is to set a large $\lambda$.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，SW-最大化训练规则构成了一组训练规则。我们使用${\psi}\in\Psi^{SW}$和$\theta_{\text{init}}$。一种简单的方法是设置一个大的$\lambda$。
- en: 4.1 Affine Maximizer Payment
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 仿射最大化支付
- en: 'We introduce the affine maximizer payment rule (Roberts ([1979](#bib.bib52)))
    $p^{AFF}$, a weighted version of VCG payment (Vickrey ([1961](#bib.bib61)); Clarke
    ([1971](#bib.bib11)); Groves ([1973](#bib.bib26))):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了仿射最大化支付规则（Roberts（[1979](#bib.bib52)））$p^{AFF}$，这是VCG支付（Vickrey（[1961](#bib.bib61)）；Clarke（[1971](#bib.bib11)）；Groves（[1973](#bib.bib26)））的加权版本：
- en: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
- en: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
- en: The notations $ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$,
    the reported number of agents are $\vec{w}$. The affine social welfare consists
    of both the groups’ valuations and the regularization term. Formally,
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 符号$ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$中，报告的代理数量是$\vec{w}$。仿射社会福利包括了各个群体的估值和正则化项。形式上，
- en: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$
    |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$
    |  |'
- en: '|  | $\displaystyle ASW_{-j}(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle ASW_{-j}(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$
    |  |'
- en: We show that $p^{AFF}$ implements SW-Maximizing training rules in both DSIC
    and IR, which implies that truthfully reporting both reward models and group sizes
    constitutes a dominant Nash Equilibrium in this mechanism.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了$p^{AFF}$在DSIC和IR下实现SW-最大化培训规则，这意味着真实报告两种奖励模型和组大小构成了该机制中的一个主导纳什均衡。
- en: Theorem 4.2.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 4.2。
- en: For any ${\psi}\in\Psi^{SW}$ satisfies DSIC and IR.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何${\psi}\in\Psi^{SW}$满足DSIC和IR。
- en: Regarding payment equivalence, as we have mentioned in the previous section,
    the domain $\mathcal{W}\times\mathcal{R}$, the results in Nisan et al. ([2007](#bib.bib44))
    can not be directly applied. However, we show that under the following assumption,
    SW-Maximizing training rules satisfy payment equivalence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 关于支付等价性，如我们在前一节中提到的，领域$\mathcal{W}\times\mathcal{R}$中，Nisan等人（[2007](#bib.bib44)）的结果不能直接应用。然而，我们展示了在以下假设下，SW-最大化培训规则满足支付等价性。
- en: Assumption 4.3.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 4.3。
- en: For any $\epsilon></math>, <math id=$, then $\max_{{\bm{x}}\in T^{\ast}}|\text{LLM}_{\theta}({\bm{x}})-\text{LLM}_{\theta^{\prime}}({\bm{x}})|\leq\epsilon$.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何$\epsilon$，则$\max_{{\bm{x}}\in T^{\ast}}|\text{LLM}_{\theta}({\bm{x}})-\text{LLM}_{\theta^{\prime}}({\bm{x}})|\leq\epsilon$。
- en: This assumption is reasonable for most measures $D$ are sufficiently close,
    the training outcomes $\theta$ should also be close. Specifically, we validate
    this assumption for some widely used distance measures.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这一假设对大多数测量$D$是合理的，因为它们足够接近，培训结果$\theta$也应该接近。具体而言，我们验证了这一假设对一些广泛使用的距离测量方法。
- en: Proposition 4.4.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.4。
- en: '[4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4
    Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models") holds for SW-Maximizing training rules with regularizations
    KL-divergence, $D_{\mathrm{KL}}(p||q)=\sum_{{\bm{x}}\in T^{\ast}}p({\bm{x}})\log
    p({\bm{x}})/q({\bm{x}})$.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.3](#S4.Thmtheorem3 "假设 4.3\. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 多奖励模型的LLM微调机制设计")适用于具有正则化KL散度的SW-最大化培训规则，$D_{\mathrm{KL}}(p||q)=\sum_{{\bm{x}}\in
    T^{\ast}}p({\bm{x}})\log p({\bm{x}})/q({\bm{x}})$。'
- en: 'Under this assumption, we derive the following result:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在此假设下，我们得出以下结果：
- en: Theorem 4.5.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 4.5。
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), each training rule ${\psi}\in\Psi^{SW}$ satisfies payment equivalence.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.1](#S3.Thmtheorem1 "假设 3.1\. ‣ 3 一般培训规则的激励 ‣ 多奖励模型的LLM微调机制设计")和[4.3](#S4.Thmtheorem3
    "假设 4.3\. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 多奖励模型的LLM微调机制设计")下，每个培训规则${\psi}\in\Psi^{SW}$满足支付等价性。
- en: With the property of payment equivalence, we further investigate the revenue-maximizing
    payment rule that implements SW-Maximizing training rules in both DSIC and IR.
    Finding the revenue-maximizing multi-parameter mechanism is a challenging problem
    in classic mechanism design theory. However, since we have proved the payment
    equivalence for SW-Maximizing training rules, we can utilize the necessary condition
    defined in [Definition 3.6](#S3.Thmtheorem6 "Definition 3.6 (Payment Equivalence).
    ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") to formulate
    it as a optimization problem. Solving this problem provides the optimal payment
    rule under the same conditions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过支付等价性的属性，我们进一步研究了在DSIC和IR下实现SW-最大化培训规则的收益最大化支付规则。找到收益最大化的多参数机制是经典机制设计理论中的一个挑战性问题。然而，由于我们已经证明了SW-最大化培训规则的支付等价性，我们可以利用[定义
    3.6](#S3.Thmtheorem6 "定义 3.6 (支付等价性). ‣ 3.2 支付规则的特征 ‣ 3 一般培训规则的激励 ‣ 多奖励模型的LLM微调机制设计")中定义的必要条件将其制定为优化问题。解决这个问题提供了在相同条件下的最优支付规则。
- en: Corollary 4.6.
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 推论 4.6。
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), for each training rule ${\psi}\in\Psi^{SW}$ in both DSIC and
    IR is given by
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") 和 [4.3](#S4.Thmtheorem3
    "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
    的假设下，对于每个训练规则 ${\psi}\in\Psi^{SW}$，在 DSIC 和 IR 下给出：
- en: '|  | $\displaystyle p^{*}_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})$
    |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{*}_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: The relationship between the domains $\mathcal{R}\times\mathcal{W}$ includes
    all normalized reward models. Second, based on payment equivalence, finding the
    revenue-maximizing mechanism satisfying IR also needs information on the exact
    domains.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 域 $\mathcal{R}\times\mathcal{W}$ 之间的关系包括所有标准化的奖励模型。其次，根据支付等价性，找到满足 IR 的最大化收入机制也需要确切的域信息。
- en: 4.2 Approximate Valuation
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 近似估值
- en: In this part, we discuss the influence of error generated in practice on the
    incentive property in the RLHF Game. We abstract it as an approximate valuation
    problem (Chiesa et al. ([2012](#bib.bib9))). Formally, when group $i$ with a conditional
    distribution $F_{i}(\cdot|\text{rm}_{i})$ can be considered as the optimal for
    the deviated reward models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论实践中产生的误差对RLHF游戏中激励性质的影响。我们将其抽象为一个近似估值问题（Chiesa 等人 ([2012](#bib.bib9)））。形式上，当具有条件分布
    $F_{i}(\cdot|\text{rm}_{i})$ 的组 $i$ 可以被视为对于偏离的奖励模型是最优的时。
- en: We assume that agent groups are aware of the noise when feeding preferences
    into the mechanism. Therefore, their utilities will take it into account and have
    a different form. We use the capital letter $U_{i}$ and group size $w_{i}$ is
    given by
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设代理组在将偏好输入机制时，意识到噪声的存在。因此，他们的效用将考虑到这一点，并具有不同的形式。我们使用大写字母 $U_{i}$ 和组大小 $w_{i}$
    如下给出：
- en: '|  | $U_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i})=\mathbb{E}_{\widehat{\text{rm}}_{i}\sim
    F_{i}(\cdot&#124;\text{rm}^{\prime}_{i})}u_{i}((\widehat{\text{rm}}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i}).$
    |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i})=\mathbb{E}_{\widehat{\text{rm}}_{i}\sim
    F_{i}(\cdot&#124;\text{rm}^{\prime}_{i})}u_{i}((\widehat{\text{rm}}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i});{\psi},p,\text{rm}_{i},w_{i}).$
    |  |'
- en: Note that in defining $U_{i}$.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在定义 $U_{i}$ 时。
- en: 'We only consider the case when the noised input to the mechanism and the reported
    reward models are close:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅考虑机制的有噪声输入和报告的奖励模型接近的情况：
- en: Assumption 4.7  (Bounded Error).
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 4.7 （有界误差）。
- en: For any profile of reported reward models $\overrightarrow{\text{rm}}$s with
    non-zero probability satisfies
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何具有非零概率的报告奖励模型配置 $\overrightarrow{\text{rm}}$ 满足：
- en: '|  | $\max_{{\bm{x}}\in T^{\ast}}&#124;\widehat{\text{rm}}_{i}({\bm{x}})-\text{rm}_{i}({\bm{x}})&#124;\leq\epsilon\quad\forall
    i\in[n].$ |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{{\bm{x}}\in T^{\ast}}&#124;\widehat{\text{rm}}_{i}({\bm{x}})-\text{rm}_{i}({\bm{x}})&#124;\leq\epsilon\quad\forall
    i\in[n].$ |  |'
- en: We first show that by directly applying results in [Section 4.1](#S4.SS1 "4.1
    Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design
    for LLM Fine-tuning with Multiple Reward Models") to the noised input, the loss
    in the social welfare is upper-bounded by $2\epsilon\sum_{i=1}^{n}w_{i}$.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先表明，通过直接将 [Section 4.1](#S4.SS1 "4.1 Affine Maximizer Payment ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models") 的结果应用于有噪声的输入，社会福利的损失由 $2\epsilon\sum_{i=1}^{n}w_{i}$ 上界。
- en: Lemma 4.8.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 4.8。
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.7](#S4.Thmtheorem7 "Assumption 4.7 (Bounded Error). ‣ 4.2 Approximate Valuation
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), when the training rule ${\psi}\in\Psi^{SW}$, the
    loss in social welfare is bounded by
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") 和 [4.7](#S4.Thmtheorem7
    "Assumption 4.7 (Bounded Error). ‣ 4.2 Approximate Valuation ‣ 4 Social Welfare
    Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward
    Models") 的假设下，当训练规则 ${\psi}\in\Psi^{SW}$ 时，社会福利的损失由以下公式界定：
- en: '|  | $1$2 |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For training rule ${\psi}\in\Psi^{SW}$. Therefore, we can derive the following
    theorem based on [Lemma 4.8](#S4.Thmtheorem8 "Lemma 4.8\. ‣ 4.2 Approximate Valuation
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models").
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练规则${\psi}\in\Psi^{SW}$。因此，我们可以根据[引理 4.8](#S4.Thmtheorem8 "Lemma 4.8\. ‣
    4.2 Approximate Valuation ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism
    Design for LLM Fine-tuning with Multiple Reward Models")推导出以下定理。
- en: Theorem 4.9.
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 4.9。
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") and
    [4.7](#S4.Thmtheorem7 "Assumption 4.7 (Bounded Error). ‣ 4.2 Approximate Valuation
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), when the training rule ${\psi}\in\Psi^{SW}$, $\overrightarrow{\text{rm}}_{-i}$,
    we have
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")和[4.7](#S4.Thmtheorem7
    "Assumption 4.7 (Bounded Error). ‣ 4.2 Approximate Valuation ‣ 4 Social Welfare
    Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward
    Models")下，当训练规则${\psi}\in\Psi^{SW}$，$\overrightarrow{\text{rm}}_{-i}$时，我们有
- en: '|  | $1$2 |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: In other words, when $\vec{w}$-DSIC mechanism.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当$\vec{w}$-DSIC机制。
- en: This means that for any group $i$.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对于任何组$i$。
- en: 5 Further Related Work
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 进一步相关工作
- en: RLHF with Multiple Reward Models.
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多重奖励模型的RLHF。
- en: Research involving multiple reward models primarily focuses on developing algorithms
    to enhance practical performance. Some studies design methods to simultaneously
    satisfy multiple preferences (Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Park et al. ([2024](#bib.bib47))). Additionally,
    there is a body of work that trains multiple models for a single preference and
    then ensembles them to improve the robustness of RLHF (Coste et al. ([2023](#bib.bib14));
    Zhang et al. ([2024](#bib.bib68))), mitigate the influence of incorrect and ambiguous
    preferences in the dataset (Wang et al. ([2024](#bib.bib62))), and reduce reward
    hacking (Eisenstein et al. ([2023](#bib.bib21))). Unlike these approaches, our
    work considers how to collect misaligned preferences truthfully from different
    agents.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及多个奖励模型的研究主要集中在开发算法以提高实际性能。一些研究设计方法以同时满足多个偏好（Ramé等人（[2024](#bib.bib51)）；Wu等人（[2024](#bib.bib65)）；Jang等人（[2023](#bib.bib32)）；Park等人（[2024](#bib.bib47)））。此外，还有一类工作训练多个模型以满足单一偏好，然后将它们集成以提高RLHF的鲁棒性（Coste等人（[2023](#bib.bib14)）；Zhang等人（[2024](#bib.bib68)）），减轻数据集中不正确和模糊偏好的影响（Wang等人（[2024](#bib.bib62)）），并减少奖励作弊（Eisenstein等人（[2023](#bib.bib21)））。与这些方法不同，我们的工作考虑了如何真实地从不同代理处收集不一致的偏好。
- en: Multi-parameter Auctions.
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多参数拍卖。
- en: Several studies have explored the properties relevant to our paper in various
    multi-parameter auction scenarios, such as implementability (Rochet ([1987](#bib.bib53));
    Miyake ([1998](#bib.bib41)); Conitzer and Sandholm ([2004](#bib.bib12)); Saks
    and Yu ([2005](#bib.bib55)); Bikhchandani et al. ([2006](#bib.bib4)); Ashlagi
    et al. ([2010](#bib.bib2))) and payment equivalence (Ivanova-Stenzel and Salmon
    ([2008](#bib.bib30)); Heydenreich et al. ([2009](#bib.bib29)); Bergemann and Välimäki
    ([2010](#bib.bib3)); Pavan et al. ([2014](#bib.bib48))). Another central topic
    in auction theory is to design mechanisms that satisfy DSIC and IR while maximizing
    the expected revenue for the auctioneer. Although the single-parameter scenario
    has been resolved by Myerson ([1981](#bib.bib43)), the optimal auction design
    for multi-parameter settings remains an open question. Therefore, there is a stream
    of research focusing on a specific subset, affine maximizer auctions, which inherently
    satisfy DSIC and IR (Sandholm and Likhodedov ([2015](#bib.bib56)); Roberts ([1979](#bib.bib52));
    Likhodedov and Sandholm ([2004](#bib.bib37)); Briest et al. ([2010](#bib.bib7));
    Tang and Sandholm ([2012](#bib.bib59)); Jehiel et al. ([2007](#bib.bib33))), and
    proposes optimizations to enhance empirical performance (Curry et al. ([2022](#bib.bib15));
    Duan et al. ([2024a](#bib.bib17), [b](#bib.bib18))). Compared to these works,
    we are the first to discuss the property of payment equivalence and the revenue-maximizing
    solution in the scenario of fine-tuning LLMs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究在各种多参数拍卖场景中探讨了与我们论文相关的性质，如可实现性（Rochet ([1987](#bib.bib53)); Miyake ([1998](#bib.bib41));
    Conitzer 和 Sandholm ([2004](#bib.bib12)); Saks 和 Yu ([2005](#bib.bib55)); Bikhchandani
    等 ([2006](#bib.bib4)); Ashlagi 等 ([2010](#bib.bib2))) 和支付等价性（Ivanova-Stenzel 和
    Salmon ([2008](#bib.bib30)); Heydenreich 等 ([2009](#bib.bib29)); Bergemann 和 Välimäki
    ([2010](#bib.bib3)); Pavan 等 ([2014](#bib.bib48)))。拍卖理论中的另一个核心话题是设计满足DSIC和IR的机制，同时最大化拍卖人的预期收入。尽管单参数场景已由Myerson
    ([1981](#bib.bib43))解决，但多参数设置的最优拍卖设计仍然是一个悬而未决的问题。因此，存在一个关注特定子集的研究方向，仿射最大化拍卖，它本质上满足DSIC和IR（Sandholm
    和 Likhodedov ([2015](#bib.bib56)); Roberts ([1979](#bib.bib52)); Likhodedov 和
    Sandholm ([2004](#bib.bib37)); Briest 等 ([2010](#bib.bib7)); Tang 和 Sandholm ([2012](#bib.bib59));
    Jehiel 等 ([2007](#bib.bib33)))，并提出了优化方法以提高实证性能（Curry 等 ([2022](#bib.bib15)); Duan
    等 ([2024a](#bib.bib17), [b](#bib.bib18)))。与这些工作相比，我们首次探讨了支付等价性和在微调LLMs场景中的收入最大化解决方案的属性。
- en: Game Theory and LLMs.
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 博弈理论与大型语言模型（LLMs）。
- en: Other works also explored the intersection of game theory and large language
    models. Some research has proposed algorithms for training LLMs inspired by concepts
    in game theory, such as Nash learning from human feedback (Munos et al. ([2023](#bib.bib42))),
    consensus game (Jacob et al. ([2023](#bib.bib31))), and direct Nash optimization (Rosset
    et al. ([2024](#bib.bib54))), and Gemp et al. ([2024](#bib.bib25)). Furthermore,
    various studies assess LLMs from a game-theoretical perspective, examining aspects
    such as rationality (Chen et al. ([2023](#bib.bib8)); Fan et al. ([2023](#bib.bib22))),
    behavior in matrix games (Akata et al. ([2023](#bib.bib1)); Gandhi et al. ([2023](#bib.bib24));
    Lorè and Heydari ([2023](#bib.bib39))), and performance in strategic games like
    auctions (Guo et al. ([2023](#bib.bib27), [2024](#bib.bib28))), Werewolf (Xu et al.
    ([2023a](#bib.bib66), [b](#bib.bib67))), and Avalon (Wang et al. ([2023a](#bib.bib63))).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其他工作也探讨了博弈理论与大型语言模型的交集。一些研究提出了受博弈理论概念启发的LLMs训练算法，如来自人类反馈的纳什学习（Munos 等 ([2023](#bib.bib42)))、共识博弈（Jacob
    等 ([2023](#bib.bib31)))、直接纳什优化（Rosset 等 ([2024](#bib.bib54)))，以及Gemp 等 ([2024](#bib.bib25))。此外，各种研究从博弈理论的角度评估了LLMs，考察了如理性（Chen
    等 ([2023](#bib.bib8)); Fan 等 ([2023](#bib.bib22)))、在矩阵博弈中的行为（Akata 等 ([2023](#bib.bib1));
    Gandhi 等 ([2023](#bib.bib24)); Lorè 和 Heydari ([2023](#bib.bib39)))，以及在拍卖（Guo
    等 ([2023](#bib.bib27), [2024](#bib.bib28)))、狼人（Xu 等 ([2023a](#bib.bib66), [b](#bib.bib67)))
    和阿瓦隆（Wang 等 ([2023a](#bib.bib63)))等战略博弈中的表现。
- en: 6 Discussion and Conclusion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与结论
- en: Efficient Practical Implementation of $p^{AFF}$.
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $p^{AFF}$的高效实际实现。
- en: 'In the RLHF Game with $n$s. This can result in inefficiency due to the costly
    training. To address this problem, we propose two modifications to $p^{AFF}$ when
    calculating payments:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有$n$s的RLHF博弈中。这可能由于训练成本高而导致低效率。为解决此问题，我们提出了两种对$p^{AFF}$进行修改的方法来计算支付：
- en: '1.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Calculate an approximate $\widehat{{\psi}}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=\arg\max_{\theta\in\{\theta_{1},\cdots,\theta_{K}\}}ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算一个近似值 $\widehat{{\psi}}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=\arg\max_{\theta\in\{\theta_{1},\cdots,\theta_{K}\}}ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})$。
- en: '2.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Adopt less iterations in the training process for calculating ${\psi}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})$
    that is not optimal.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在计算 ${\psi}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})$
    的训练过程中采用更少的迭代，尽管这不是最优的。
- en: The first method needs only one training process (for ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})$’s
    report. In comparison, the second approach incurs higher training costs but guarantees
    strict DSIC.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法只需要一个训练过程（用于 ${\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})$
    的报告）。相比之下，第二种方法的训练成本更高，但保证严格的DSIC。
- en: Conclusion and Future Work.
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论和未来工作。
- en: This paper investigates incentive design in fine-tuning large language models
    using multiple reward models. We formalize this scenario as the RLHF Game, where
    a service provider proposes training and payment rules, and agents strategically
    report their preferences. We demonstrate the necessity of payment schemes for
    incentivizing truthful reporting in general training rules and provide a comprehensive
    characterization of payment schemes that implement SW-Maximizing training rules
    in dominant strategies. These findings enhance the theoretical understanding of
    mechanism design in LLM fine-tuning and offer guidelines for implementing effective
    RLHF-based systems in various contexts.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了使用多个奖励模型对大型语言模型进行微调的激励设计。我们将这种情况形式化为RLHF游戏，其中服务提供者提出培训和支付规则，代理人则战略性地报告他们的偏好。我们证明了在一般训练规则中激励真实报告的支付方案的必要性，并提供了在主导策略中实现SW-最大化训练规则的支付方案的全面特征描述。这些发现增强了对LLM微调中机制设计的理论理解，并为在各种背景下实施有效的基于RLHF的系统提供了指导。
- en: Future research in this field presents several promising directions. Firstly,
    investigating mechanisms integrating efficiency and incentive compatibility within
    the RLHF Game could significantly enhance its applicability in real-world scenarios.
    Secondly, modeling and examining more complex training rules, such as dynamic
    training rules, could deepen the understanding of this framework. Thirdly, designing
    mechanisms for more general cases that aggregate preferences into multiple models
    based on diversity considerations is crucial. Additionally, applying mechanism
    design theory to other scenarios related to large language models, such as API
    charge schemes, retrieval-augmented generation (RAG), and prompt engineering,
    offers valuable opportunities for further exploration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的未来研究展示了几个有前景的方向。首先，研究在RLHF游戏中整合效率与激励兼容性的机制可能显著提高其在实际场景中的适用性。其次，建模和研究更复杂的训练规则，例如动态训练规则，可能加深对这一框架的理解。第三，设计适用于更多一般情况的机制，这些机制基于多样性考虑将偏好汇聚到多个模型中，是至关重要的。此外，将机制设计理论应用于与大型语言模型相关的其他场景，如API收费方案、检索增强生成（RAG）和提示工程，也为进一步探索提供了宝贵的机会。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Akata et al. [2023] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge, and Eric Schulz. Playing repeated games with large language models.
    *arXiv preprint arXiv:2305.16867*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata 等 [2023] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias
    Bethge, 和 Eric Schulz. 与大型语言模型进行重复博弈。*arXiv 预印本 arXiv:2305.16867*，2023年。
- en: Ashlagi et al. [2010] Itai Ashlagi, Mark Braverman, Avinatan Hassidim, and Dov
    Monderer. Monotonicity and implementability. *Econometrica*, 78(5):1749–1772,
    2010.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashlagi 等 [2010] Itai Ashlagi, Mark Braverman, Avinatan Hassidim, 和 Dov Monderer.
    单调性和可实施性。*经济学期刊*，78(5):1749–1772，2010年。
- en: Bergemann and Välimäki [2010] Dirk Bergemann and Juuso Välimäki. The dynamic
    pivot mechanism. *Econometrica*, 78(2):771–789, 2010.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergemann 和 Välimäki [2010] Dirk Bergemann 和 Juuso Välimäki. 动态枢纽机制。*经济学期刊*，78(2):771–789，2010年。
- en: Bikhchandani et al. [2006] Sushil Bikhchandani, Shurojit Chatterji, Ron Lavi,
    Ahuva Mu’alem, Noam Nisan, and Arunava Sen. Weak monotonicity characterizes deterministic
    dominant-strategy implementation. *Econometrica*, 74(4):1109–1132, 2006.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bikhchandani 等 [2006] Sushil Bikhchandani, Shurojit Chatterji, Ron Lavi, Ahuva
    Mu’alem, Noam Nisan, 和 Arunava Sen. 弱单调性特征化确定性主导策略实施。*经济学期刊*，74(4):1109–1132，2006年。
- en: Boyd and Vandenberghe [2004] Stephen P Boyd and Lieven Vandenberghe. *Convex
    optimization*. Cambridge university press, 2004.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd和Vandenberghe [2004] Stephen P Boyd 和 Lieven Vandenberghe. *凸优化*. 剑桥大学出版社，2004年。
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley and Terry [1952] Ralph Allan Bradley 和 Milton E Terry。不完全区组设计的排序分析：I.
    配对比较法。*生物统计学*，39(3/4):324–345，1952年。
- en: Briest et al. [2010] Patrick Briest, Shuchi Chawla, Robert Kleinberg, and S Matthew
    Weinberg. Pricing randomized allocations. In *Proceedings of the twenty-first
    annual ACM-SIAM symposium on Discrete Algorithms*, pages 585–597\. SIAM, 2010.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Briest et al. [2010] Patrick Briest, Shuchi Chawla, Robert Kleinberg, 和 S Matthew
    Weinberg。随机分配的定价。在 *第21届年度 ACM-SIAM 离散算法研讨会论文集*，页585–597。SIAM，2010年。
- en: Chen et al. [2023] Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong.
    The emergence of economic rationality of gpt. *Proceedings of the National Academy
    of Sciences*, 120(51):e2316205120, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023] Yiting Chen, Tracy Xiao Liu, You Shan, 和 Songfa Zhong。GPT
    的经济理性出现。*国家科学院学报*，120(51):e2316205120，2023年。
- en: Chiesa et al. [2012] Alessandro Chiesa, Silvio Micali, and Zeyuan Allen Zhu.
    Mechanism design with approximate valuations. In *Proceedings of the 3rd Innovations
    in Theoretical Computer Science conference*, pages 34–38, 2012.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiesa et al. [2012] Alessandro Chiesa, Silvio Micali, 和 Zeyuan Allen Zhu。具有近似评估的机制设计。在
    *第3届理论计算机科学创新会议论文集*，页34–38，2012年。
- en: Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30, 2017.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, 和 Dario Amodei。基于人类偏好的深度强化学习。*神经信息处理系统进展*，30，2017年。
- en: Clarke [1971] Edward H Clarke. Multipart pricing of public goods. *Public choice*,
    pages 17–33, 1971.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clarke [1971] Edward H Clarke。公共物品的多部分定价。*公共选择*，页17–33，1971年。
- en: Conitzer and Sandholm [2004] Vincent Conitzer and Tuomas Sandholm. Self-interested
    automated mechanism design and implications for optimal combinatorial auctions.
    In *Proceedings of the 5th ACM Conference on Electronic Commerce*, pages 132–141,
    2004.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conitzer and Sandholm [2004] Vincent Conitzer 和 Tuomas Sandholm。以自我利益为导向的自动机制设计及其对最佳组合拍卖的影响。在
    *第5届 ACM 电子商务会议论文集*，页132–141，2004年。
- en: 'Conitzer et al. [2024] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H
    Holliday, Bob M Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell,
    Hailey Schoelkopf, et al. Social choice for ai alignment: Dealing with diverse
    human feedback. *arXiv preprint arXiv:2404.10271*, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conitzer et al. [2024] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley
    H Holliday, Bob M Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell,
    Hailey Schoelkopf, 等。用于 AI 对齐的社会选择：应对多样化的人类反馈。*arXiv 预印本 arXiv:2404.10271*，2024年。
- en: Coste et al. [2023] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
    Reward model ensembles help mitigate overoptimization. *arXiv preprint arXiv:2310.02743*,
    2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coste et al. [2023] Thomas Coste, Usman Anwar, Robert Kirk, 和 David Krueger。奖励模型集成有助于缓解过度优化。*arXiv
    预印本 arXiv:2310.02743*，2023年。
- en: Curry et al. [2022] Michael Curry, Tuomas Sandholm, and John Dickerson. Differentiable
    economics for randomized affine maximizer auctions. *arXiv preprint arXiv:2202.02872*,
    2022.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Curry et al. [2022] Michael Curry, Tuomas Sandholm, 和 John Dickerson。用于随机仿射最大化拍卖的可微经济学。*arXiv
    预印本 arXiv:2202.02872*，2022年。
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器的预训练。*arXiv
    预印本 arXiv:1810.04805*，2018年。
- en: Duan et al. [2024a] Zhijian Duan, Haoran Sun, Yurong Chen, and Xiaotie Deng.
    A scalable neural network for dsic affine maximizer auction design. *Advances
    in Neural Information Processing Systems*, 36, 2024a.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. [2024a] Zhijian Duan, Haoran Sun, Yurong Chen, 和 Xiaotie Deng。用于
    DSIC 仿射最大化拍卖设计的可扩展神经网络。*神经信息处理系统进展*，36，2024a年。
- en: Duan et al. [2024b] Zhijian Duan, Haoran Sun, Yichong Xia, Siqiang Wang, Zhilin
    Zhang, Chuan Yu, Jian Xu, Bo Zheng, and Xiaotie Deng. Scalable virtual valuations
    combinatorial auction design by combining zeroth-order and first-order optimization
    method. *arXiv preprint arXiv:2402.11904*, 2024b.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. [2024b] Zhijian Duan, Haoran Sun, Yichong Xia, Siqiang Wang, Zhilin
    Zhang, Chuan Yu, Jian Xu, Bo Zheng, 和 Xiaotie Deng。通过结合零阶和一阶优化方法的可扩展虚拟评估组合拍卖设计。*arXiv
    预印本 arXiv:2402.11904*，2024b年。
- en: Dubey et al. [2024] Kumar Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta,
    and Di Wang. Auctions with llm summaries. *arXiv preprint arXiv:2404.08126*, 2024.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey et al. [2024] Kumar Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta,
    和 Di Wang。带有 LLM 摘要的拍卖。*arXiv 预印本 arXiv:2404.08126*，2024年。
- en: Duetting et al. [2023] Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Haifeng
    Xu, and Song Zuo. Mechanism design for large language models. *arXiv preprint
    arXiv:2310.10826*, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duetting 等人 [2023] Paul Duetting, Vahab Mirrokni, Renato Paes Leme, Haifeng
    Xu 和 Song Zuo。大型语言模型的机制设计。*arXiv 预印本 arXiv:2310.10826*，2023年。
- en: Eisenstein et al. [2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad
    Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl,
    Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate
    but do not eliminate reward hacking. *arXiv preprint arXiv:2312.09244*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eisenstein 等人 [2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami,
    Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak
    Ramachandran 等人。帮助还是驯化？奖励模型集成减轻但不能消除奖励破解。*arXiv 预印本 arXiv:2312.09244*，2023年。
- en: Fan et al. [2023] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large
    language models serve as rational players in game theory? a systematic analysis.
    *arXiv preprint arXiv:2312.05488*, 2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 [2023] Caoyun Fan, Jindou Chen, Yaohui Jin 和 Hao He。大型语言模型能否作为博弈论中的理性玩家？系统分析。*arXiv
    预印本 arXiv:2312.05488*，2023年。
- en: 'Feizi et al. [2023] Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei,
    and Suho Shin. Online advertisements with llms: Opportunities and challenges.
    *arXiv preprint arXiv:2311.07601*, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feizi 等人 [2023] Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei 和 Suho
    Shin。在线广告与大型语言模型：机遇与挑战。*arXiv 预印本 arXiv:2311.07601*，2023年。
- en: Gandhi et al. [2023] Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic
    reasoning with language models. *arXiv preprint arXiv:2305.19165*, 2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gandhi 等人 [2023] Kanishk Gandhi, Dorsa Sadigh 和 Noah D Goodman。语言模型的战略推理。*arXiv
    预印本 arXiv:2305.19165*，2023年。
- en: 'Gemp et al. [2024] Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari
    Dasagi, Luke Marris, Georgios Piliouras, and Karl Tuyls. States as strings as
    strategies: Steering language models with game-theoretic solvers. *arXiv preprint
    arXiv:2402.01704*, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemp 等人 [2024] Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari
    Dasagi, Luke Marris, Georgios Piliouras 和 Karl Tuyls。状态作为字符串作为策略：使用博弈论解算器引导语言模型。*arXiv
    预印本 arXiv:2402.01704*，2024年。
- en: 'Groves [1973] Theodore Groves. Incentives in teams. *Econometrica: Journal
    of the Econometric Society*, pages 617–631, 1973.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Groves [1973] Theodore Groves。团队中的激励。*计量经济学：计量经济学会期刊*，页617–631，1973年。
- en: Guo et al. [2023] Shangmin Guo, Haochuan Wang, Haoran Bu, Yi Ren, Dianbo Sui,
    Yu-Ming Shang, and Siting Lu. Large language models as rational players in competitive
    economics games. *arXiv preprint arXiv:2308.10032*, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2023] Shangmin Guo, Haochuan Wang, Haoran Bu, Yi Ren, Dianbo Sui, Yu-Ming
    Shang 和 Siting Lu。大型语言模型作为竞争经济游戏中的理性玩家。*arXiv 预印本 arXiv:2308.10032*，2023年。
- en: Guo et al. [2024] Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui,
    Yuming Shang, and Siting Lu. Economics arena for large language models. *arXiv
    preprint arXiv:2401.01735*, 2024.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2024] Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui, Yuming
    Shang 和 Siting Lu。大型语言模型的经济学领域。*arXiv 预印本 arXiv:2401.01735*，2024年。
- en: Heydenreich et al. [2009] Birgit Heydenreich, Rudolf Müller, Marc Uetz, and
    Rakesh V Vohra. Characterization of revenue equivalence. *Econometrica*, 77(1):307–316,
    2009.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heydenreich 等人 [2009] Birgit Heydenreich, Rudolf Müller, Marc Uetz 和 Rakesh
    V Vohra。收入等价性的特征描述。*计量经济学*，77(1):307–316，2009年。
- en: Ivanova-Stenzel and Salmon [2008] Radosveta Ivanova-Stenzel and Timothy C Salmon.
    Revenue equivalence revisited. *Games and Economic Behavior*, 64(1):171–192, 2008.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivanova-Stenzel 和 Salmon [2008] Radosveta Ivanova-Stenzel 和 Timothy C Salmon。收入等价性重访。*游戏与经济行为*，64(1):171–192，2008年。
- en: 'Jacob et al. [2023] Athul Paul Jacob, Yikang Shen, Gabriele Farina, and Jacob
    Andreas. The consensus game: Language model generation via equilibrium search.
    *arXiv preprint arXiv:2310.09139*, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等人 [2023] Athul Paul Jacob, Yikang Shen, Gabriele Farina 和 Jacob Andreas。共识游戏：通过平衡搜索生成语言模型。*arXiv
    预印本 arXiv:2310.09139*，2023年。
- en: 'Jang et al. [2023] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang,
    Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj
    Ammanabrolu. Personalized soups: Personalized large language model alignment via
    post-hoc parameter merging. *arXiv preprint arXiv:2310.11564*, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等人 [2023] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack
    Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi 和 Prithviraj Ammanabrolu。个性化汤：通过事后参数合并实现个性化大型语言模型对齐。*arXiv
    预印本 arXiv:2310.11564*，2023年。
- en: Jehiel et al. [2007] Philippe Jehiel, Moritz Meyer-Ter-Vehn, and Benny Moldovanu.
    Mixed bundling auctions. *Journal of Economic Theory*, 134(1):494–512, 2007.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jehiel 等人 [2007] Philippe Jehiel, Moritz Meyer-Ter-Vehn 和 Benny Moldovanu。混合捆绑拍卖。*经济理论期刊*，134(1):494–512，2007年。
- en: 'Ji et al. [2023] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao
    Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai
    alignment: A comprehensive survey. *arXiv preprint arXiv:2310.19852*, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等 [2023] Jiaming Ji、Tianyi Qiu、Boyuan Chen、Borong Zhang、Hantao Lou、Kaile
    Wang、Yawen Duan、Zhonghao He、Jiayi Zhou、Zhaowei Zhang 等。AI 对齐：全面调查。*arXiv 预印本 arXiv:2310.19852*，2023
    年。
- en: Köpf et al. [2024] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver
    Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large
    language model alignment. *Advances in Neural Information Processing Systems*,
    36, 2024.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf 等 [2024] Andreas Köpf、Yannic Kilcher、Dimitri von Rütte、Sotiris Anagnostidis、Zhi
    Rui Tam、Keith Stevens、Abdullah Barhoum、Duc Nguyen、Oliver Stanley、Richárd Nagyfi
    等。Openassistant 对话——民主化大型语言模型对齐。*神经信息处理系统进展*，36，2024 年。
- en: 'Laufer et al. [2023] Benjamin Laufer, Jon Kleinberg, and Hoda Heidari. Fine-tuning
    games: Bargaining and adaptation for general-purpose models. *arXiv preprint arXiv:2308.04399*,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laufer 等 [2023] Benjamin Laufer、Jon Kleinberg 和 Hoda Heidari。微调游戏：一般模型的讨价还价与适应。*arXiv
    预印本 arXiv:2308.04399*，2023 年。
- en: Likhodedov and Sandholm [2004] Anton Likhodedov and Tuomas Sandholm. Methods
    for boosting revenue in combinatorial auctions. In *AAAI*, pages 232–237, 2004.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Likhodedov 和 Sandholm [2004] Anton Likhodedov 和 Tuomas Sandholm. 组合拍卖中提升收入的方法。在
    *AAAI* 上，页码 232–237，2004 年。
- en: 'Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2019] Yinhan Liu、Myle Ott、Naman Goyal、Jingfei Du、Mandar Joshi、Danqi Chen、Omer
    Levy、Mike Lewis、Luke Zettlemoyer 和 Veselin Stoyanov。Roberta：一种鲁棒优化的 BERT 预训练方法。*arXiv
    预印本 arXiv:1907.11692*，2019 年。
- en: 'Lorè and Heydari [2023] Nunzio Lorè and Babak Heydari. Strategic behavior of
    large language models: Game structure vs. contextual framing. *arXiv preprint
    arXiv:2309.05898*, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorè 和 Heydari [2023] Nunzio Lorè 和 Babak Heydari。大型语言模型的战略行为：游戏结构与情境框架。*arXiv
    预印本 arXiv:2309.05898*，2023 年。
- en: Luenberger et al. [1984] David G Luenberger, Yinyu Ye, et al. *Linear and nonlinear
    programming*, volume 2. Springer, 1984.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luenberger 等 [1984] David G Luenberger、Yinyu Ye 等。*线性与非线性编程*，第 2 卷。Springer，1984
    年。
- en: Miyake [1998] Mitsunobu Miyake. On the incentive properties of multi-item auctions.
    *International Journal of Game Theory*, 27:1–19, 1998.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyake [1998] Mitsunobu Miyake。多项拍卖的激励性质。*国际博弈论杂志*，27:1–19，1998 年。
- en: Munos et al. [2023] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi
    Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    Andrea Michi, et al. Nash learning from human feedback. *arXiv preprint arXiv:2312.00886*,
    2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munos 等 [2023] Rémi Munos、Michal Valko、Daniele Calandriello、Mohammad Gheshlaghi
    Azar、Mark Rowland、Zhaohan Daniel Guo、Yunhao Tang、Matthieu Geist、Thomas Mesnard、Andrea
    Michi 等。Nash 从人类反馈中学习。*arXiv 预印本 arXiv:2312.00886*，2023 年。
- en: Myerson [1981] Roger B Myerson. Optimal auction design. *Mathematics of operations
    research*, 6(1):58–73, 1981.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Myerson [1981] Roger B Myerson。最佳拍卖设计。*运筹学数学*，6(1):58–73，1981 年。
- en: Nisan et al. [2007] Noam Nisan et al. Introduction to mechanism design (for
    computer scientists). *Algorithmic game theory*, 9:209–242, 2007.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nisan 等 [2007] Noam Nisan 等。机制设计导论（面向计算机科学家）。*算法博弈论*，9:209–242，2007 年。
- en: Nocedal and Wright [1999] Jorge Nocedal and Stephen J Wright. *Numerical optimization*.
    Springer, 1999.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nocedal 和 Wright [1999] Jorge Nocedal 和 Stephen J Wright。*数值优化*。Springer，1999
    年。
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744, 2022.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 [2022] Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*，35:27730–27744，2022
    年。
- en: Park et al. [2024] Chanwoo Park, Mingyang Liu, Kaiqing Zhang, and Asuman Ozdaglar.
    Principled rlhf from heterogeneous feedback via personalization and preference
    aggregation. *arXiv preprint arXiv:2405.00254*, 2024.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 [2024] Chanwoo Park、Mingyang Liu、Kaiqing Zhang 和 Asuman Ozdaglar。通过个性化和偏好聚合从异质反馈中获得的原则性强化学习。*arXiv
    预印本 arXiv:2405.00254*，2024 年。
- en: 'Pavan et al. [2014] Alessandro Pavan, Ilya Segal, and Juuso Toikka. Dynamic
    mechanism design: A myersonian approach. *Econometrica*, 82(2):601–653, 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavan 等 [2014] Alessandro Pavan、Ilya Segal 和 Juuso Toikka。动态机制设计：一种 Myersonian
    方法。*经济学期刊*，82(2):601–653，2014 年。
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 [2018] Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever
    等。通过生成预训练提升语言理解。2018年。
- en: 'Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your
    language model is secretly a reward model. *Advances in Neural Information Processing
    Systems*, 36, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等 [2023] Rafael Rafailov、Archit Sharma、Eric Mitchell、Christopher D
    Manning、Stefano Ermon 和 Chelsea Finn。直接偏好优化：你的语言模型实际上是一个奖励模型。*Advances in Neural
    Information Processing Systems*，36，2023年。
- en: 'Ramé et al. [2024] Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert
    Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits
    of weight averaged reward models. *arXiv preprint arXiv:2401.12187*, 2024.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramé 等 [2024] Alexandre Ramé、Nino Vieillard、Léonard Hussenot、Robert Dadashi、Geoffrey
    Cideron、Olivier Bachem 和 Johan Ferret。Warm：加权平均奖励模型的好处。*arXiv preprint arXiv:2401.12187*，2024年。
- en: Roberts [1979] Kevin Roberts. The characterization of implementable choice rules.
    *Aggregation and revelation of preferences*, 12(2):321–348, 1979.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts [1979] Kevin Roberts。可实现选择规则的表征。*Aggregation and revelation of preferences*，12(2):321–348，1979年。
- en: Rochet [1987] Jean-Charles Rochet. A necessary and sufficient condition for
    rationalizability in a quasi-linear context. *Journal of mathematical Economics*,
    16(2):191–200, 1987.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rochet [1987] Jean-Charles Rochet。在准线性背景下理性化的必要和充分条件。*Journal of Mathematical
    Economics*，16(2):191–200，1987年。
- en: 'Rosset et al. [2024] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce,
    Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language
    models to self-improve with general preferences. *arXiv preprint arXiv:2404.03715*,
    2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosset 等 [2024] Corby Rosset、Ching-An Cheng、Arindam Mitra、Michael Santacroce、Ahmed
    Awadallah 和 Tengyang Xie。直接纳什优化：教语言模型用一般偏好自我提升。*arXiv preprint arXiv:2404.03715*，2024年。
- en: Saks and Yu [2005] Michael Saks and Lan Yu. Weak monotonicity suffices for truthfulness
    on convex domains. In *Proceedings of the 6th ACM conference on Electronic commerce*,
    pages 286–293, 2005.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saks 和 Yu [2005] Michael Saks 和 Lan Yu。在凸域上，弱单调性足以确保真实。在 *Proceedings of the
    6th ACM Conference on Electronic Commerce*，第286–293页，2005年。
- en: Sandholm and Likhodedov [2015] Tuomas Sandholm and Anton Likhodedov. Automated
    design of revenue-maximizing combinatorial auctions. *Operations Research*, 63(5):1000–1025,
    2015.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sandholm 和 Likhodedov [2015] Tuomas Sandholm 和 Anton Likhodedov。收入最大化组合拍卖的自动化设计。*Operations
    Research*，63(5):1000–1025，2015年。
- en: 'Shen et al. [2023] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong
    Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment:
    A survey. *arXiv preprint arXiv:2309.15025*, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 [2023] Tianhao Shen、Renren Jin、Yufei Huang、Chuang Liu、Weilong Dong、Zishan
    Guo、Xinwei Wu、Yan Liu 和 Deyi Xiong。大语言模型对齐：综述。*arXiv preprint arXiv:2309.15025*，2023年。
- en: Soumalias et al. [2024] Ermis Soumalias, Michael J Curry, and Sven Seuken. Truthful
    aggregation of llms with an application to online advertising. *arXiv preprint
    arXiv:2405.05905*, 2024.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soumalias 等 [2024] Ermis Soumalias、Michael J Curry 和 Sven Seuken。真实的LLMs聚合及其在在线广告中的应用。*arXiv
    preprint arXiv:2405.05905*，2024年。
- en: Tang and Sandholm [2012] Pingzhong Tang and Tuomas Sandholm. Mixed-bundling
    auctions with reserve prices. In *AAMAS*, pages 729–736, 2012.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 和 Sandholm [2012] Pingzhong Tang 和 Tuomas Sandholm。带有保留价格的混合捆绑拍卖。见于 *AAMAS*，第729–736页，2012年。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 [2023] Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放和高效的基础语言模型。*arXiv preprint arXiv:2302.13971*，2023年。
- en: Vickrey [1961] William Vickrey. Counterspeculation, auctions, and competitive
    sealed tenders. *The Journal of finance*, 16(1):8–37, 1961.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vickrey [1961] William Vickrey。反投机、拍卖和竞争性封闭招标。*The Journal of Finance*，16(1):8–37，1961年。
- en: 'Wang et al. [2024] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang
    Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in
    large language models part ii: Reward modeling. *arXiv preprint arXiv:2401.06080*,
    2024.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2024] Binghai Wang、Rui Zheng、Lu Chen、Yan Liu、Shihan Dou、Caishuang Huang、Wei
    Shen、Senjie Jin、Enyu Zhou、Chenyu Shi 等。大型语言模型中的RLHF秘密第二部分：奖励建模。*arXiv preprint
    arXiv:2401.06080*，2024年。
- en: 'Wang et al. [2023a] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s
    game of thoughts: Battle against deception through recursive contemplation. *arXiv
    preprint arXiv:2310.01320*, 2023a.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023a] 申智·王，常·刘，子龙·郑，思源·齐，硕·陈，启森·杨，安德鲁·赵，超飞·王，世基·宋，和高·黄。《阿瓦隆的思想游戏：通过递归反思对抗欺骗》。*arXiv
    预印本 arXiv:2310.01320*，2023a。
- en: 'Wang et al. [2023b] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language
    models with human: A survey. *arXiv preprint arXiv:2307.12966*, 2023b.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023b] 余飞·王，万军·钟，良友·李，飞·米，兴善·曾，文勇·黄，丽锋·尚，辛·姜，和群·刘。《使大型语言模型与人类对齐：综述》。*arXiv
    预印本 arXiv:2307.12966*，2023b。
- en: Wu et al. [2024] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
    Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained
    human feedback gives better rewards for language model training. *Advances in
    Neural Information Processing Systems*, 36, 2024.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2024]  Zeqiu Wu，Yushi Hu，Weijia Shi，Nouha Dziri，Alane Suhr，Prithviraj
    Ammanabrolu，Noah A Smith，Mari Ostendorf，和Hannaneh Hajishirzi。《细粒度人类反馈为语言模型训练提供更好的奖励》。*神经信息处理系统进展*，36，2024。
- en: 'Xu et al. [2023a] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. Exploring large language models for communication games:
    An empirical study on werewolf. *arXiv preprint arXiv:2309.04658*, 2023a.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023a] 余庄·徐，硕·王，鹏·李，福文·罗，晓龙·王，卫东·刘，和杨·刘。《探索大型语言模型在沟通游戏中的应用：狼人游戏的实证研究》。*arXiv
    预印本 arXiv:2309.04658*，2023a。
- en: Xu et al. [2023b] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language
    agents with reinforcement learning for strategic play in the werewolf game. *arXiv
    preprint arXiv:2310.18940*, 2023b.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023b] 泽莱·徐，超·余，飞·方，余·王，和艺·吴。《语言代理通过强化学习进行战略性狼人游戏》。*arXiv 预印本 arXiv:2310.18940*，2023b。
- en: Zhang et al. [2024] Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing
    Sun, and Chuang Gan. Improving reinforcement learning from human feedback with
    efficient reward model ensemble. *arXiv preprint arXiv:2401.16635*, 2024.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024] 申·张，镇芳·陈，孙丽·陈，艺康·沈，志清·孙，和创·甘。《通过高效的奖励模型集成改进人类反馈的强化学习》。*arXiv
    预印本 arXiv:2401.16635*，2024。
- en: Appendix A Omitted proof in Section [3](#S3 "3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 省略的证明在第[3](#S3 "3 一般训练规则的激励 ‣ 多奖励模型的LLM微调机制")节
- en: See [3.3](#S3.Thmtheorem3 "Theorem 3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3
    Incentives for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 见[3.3](#S3.Thmtheorem3 "定理 3.3。 ‣ 3.1 支付规则的必要性 ‣ 3 一般训练规则的激励 ‣ 多奖励模型的LLM微调机制")
- en: Proof.
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'For the case that $\vec{w}=1$ can be written as a programming problem:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$\vec{w}=1$的情况，可以写成一个编程问题：
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}})\coloneqq\arg\max_{\theta\in\Theta}\quad
    f(v_{1}(\theta;\text{rm}_{1}),\cdots$ |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}})\coloneqq\arg\max_{\theta\in\Theta}\quad
    f(v_{1}(\theta;\text{rm}_{1}),\cdots$ |  |'
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{s.t.}\quad\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})$
    |  |'
- en: '|  | $\displaystyle\text{LLM}_{\theta}({\bm{x}})$ |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{LLM}_{\theta}({\bm{x}})$ |  |'
- en: Because of the (3) of [3.2](#S3.Thmtheorem2 "Assumption 3.2\. ‣ 3.1 Necessity
    of Payment Rule ‣ 3 Incentives for General Training Rules ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), we can infer that the condition
    $\text{LLM}_{\theta}({\bm{x}})\geq 0$ (Luenberger et al. [[1984](#bib.bib40)]),
    such that
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[3.2](#S3.Thmtheorem2 "假设 3.2。 ‣ 3.1 支付规则的必要性 ‣ 3 一般训练规则的激励 ‣ 多奖励模型的LLM微调机制")中的(3)，我们可以推断条件为$\text{LLM}_{\theta}({\bm{x}})\geq
    0$（Luenberger et al. [[1984](#bib.bib40)]），使得
- en: '|  | $1$2 |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Under [3.1](#S3.Thmtheorem1 "Assumption 3.1\. ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"), $\frac{\partial
    v_{i}}{\partial\text{LLM}_{\theta}({\bm{x}})}=\text{rm}_{i}({\bm{x}})$, so we
    have
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.1](#S3.Thmtheorem1 "假设 3.1。 ‣ 3 一般训练规则的激励 ‣ 多奖励模型的LLM微调机制")下，$\frac{\partial
    v_{i}}{\partial\text{LLM}_{\theta}({\bm{x}})}=\text{rm}_{i}({\bm{x}})$，所以我们有
- en: '|  | $1$2 |  | (1) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'When the real reward model for agent $i$. We denote $S$. Then we take a small
    $\epsilon<\text{rm}_{i}({\bm{x}}_{2})$ as:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理$i$的真实奖励模型为$S$时。然后我们取一个小的$\epsilon<\text{rm}_{i}({\bm{x}}_{2})$，如：
- en: '|  | $$\displaystyle\text{rm}^{\prime}_{i}({\bm{x}})=\begin{cases}\text{rm}_{i}({\bm{x}})+\epsilon,&amp;{\bm{x}}={\bm{x}}_{1},\\
    \text{rm}_{i}({\bm{x}})-\epsilon,&amp;{\bm{x}}={\bm{x}}_{2}\\'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\text{rm}^{\prime}_{i}({\bm{x}})=\begin{cases}\text{rm}_{i}({\bm{x}})+\epsilon,&amp;{\bm{x}}={\bm{x}}_{1},\\
    \text{rm}_{i}({\bm{x}})-\epsilon,&amp;{\bm{x}}={\bm{x}}_{2}\\'
- en: \text{rm}_{i}({\bm{x}}),&amp;{\bm{x}}\neq{\bm{x}}_{1},{\bm{x}}\neq{\bm{x}}_{2}.\end{cases}$$
    |  |
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: \text{rm}_{i}({\bm{x}})，&amp;{\bm{x}}\neq{\bm{x}}_{1},{\bm{x}}\neq{\bm{x}}_{2}.\end{cases}$$
    |  |
- en: Intuitively, $\text{rm}^{\prime}_{i}$ and $\theta^{\prime}={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}})$
    and $\mu^{\prime}$ and we can derive the following results.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，$\text{rm}^{\prime}_{i}$ 和 $\theta^{\prime}={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}})$
    和 $\mu^{\prime}$，我们可以得出以下结果。
- en: (a) $$\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})></math>, we have
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $$\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})></math>，我们有
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})}\leq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{1})}.$ |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})}\leq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{1})}.$ |  |'
- en: With $\text{rm}^{\prime}_{i}({\bm{x}}_{1})></math>. However, since for all <math
    id=$, we have
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 $\text{rm}^{\prime}_{i}({\bm{x}}_{1})></math>$。然而，由于对于所有 <math id=$，我们有
- en: '|  | $\frac{\partial f}{\partial v_{i}}\text{rm}_{i}({\bm{x}})\leq\frac{\partial
    f}{\partial v_{i}}\text{rm}^{\prime}_{i}({\bm{x}}),$ |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial f}{\partial v_{i}}\text{rm}_{i}({\bm{x}})\leq\frac{\partial
    f}{\partial v_{i}}\text{rm}^{\prime}_{i}({\bm{x}}),$ |  |'
- en: to satisfy the optimal condition in (1), there must be for all ${\bm{x}}\neq{\bm{x}}_{1}$,
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足(1)中的最优条件，必须对所有 ${\bm{x}}\neq{\bm{x}}_{1}$，
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}})}<\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}})}.$ |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}})}<\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}})}.$ |  |'
- en: Which is equivalent to $\text{LLM}_{\theta^{\prime}}({\bm{x}})<\text{LLM}_{\theta}({\bm{x}})$,
    can be proved by totally same method.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于 $\text{LLM}_{\theta^{\prime}}({\bm{x}})<\text{LLM}_{\theta}({\bm{x}})$，可以通过完全相同的方法证明。
- en: (b) The order of $\text{LLM}_{\theta}({\bm{x}})$ such that $\text{LLM}_{\theta^{\prime}}({\bm{x}}_{3})\geq\text{LLM}_{\theta}({\bm{x}}_{3})$.
    Then we have
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $\text{LLM}_{\theta}({\bm{x}})$ 的顺序，使得 $\text{LLM}_{\theta^{\prime}}({\bm{x}}_{3})\geq\text{LLM}_{\theta}({\bm{x}}_{3})$。然后我们有
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{3})}\geq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{3})}.$ |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{3})}\geq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{3})}.$ |  |'
- en: Since $\partial f/\partial D<0$, to satisfy (1), there must be
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\partial f/\partial D<0$，为了满足(1)，必须有
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}})}\geq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}})}$ |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}})}\geq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}})}$ |  |'
- en: which is equivalent to $\text{LLM}_{\theta^{\prime}}({\bm{x}})\geq\text{LLM}_{\theta}({\bm{x}})$,
    there is $\text{LLM}_{\theta^{\prime}}({\bm{x}})\leq\text{LLM}_{\theta}({\bm{x}})$.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于 $\text{LLM}_{\theta^{\prime}}({\bm{x}})\geq\text{LLM}_{\theta}({\bm{x}})$，则
    $\text{LLM}_{\theta^{\prime}}({\bm{x}})\leq\text{LLM}_{\theta}({\bm{x}})$。
- en: Finally, with the results in (a) and (b), when $\text{LLM}_{\theta^{\prime}}({\bm{x}})\leq\text{LLM}_{\theta}({\bm{x}})$,
    there is
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，结合(a)和(b)的结果，当 $\text{LLM}_{\theta^{\prime}}({\bm{x}})\leq\text{LLM}_{\theta}({\bm{x}})$
    时，有
- en: '|  |  | $\displaystyle v_{i}({\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}});\text{rm}_{i})-v_{i}({\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}});\text{rm}_{i})$
    |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle v_{i}({\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}});\text{rm}_{i})-v_{i}({\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}});\text{rm}_{i})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: When $\text{LLM}_{\theta^{\prime}}({\bm{x}})\geq\text{LLM}_{\theta}({\bm{x}})$,
    there is
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $\text{LLM}_{\theta^{\prime}}({\bm{x}})\geq\text{LLM}_{\theta}({\bm{x}})$
    时，有
- en: '|  |  | $\displaystyle v_{i}({\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}});\text{rm}_{i})-v_{i}({\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}});\text{rm}_{i})$
    |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle v_{i}({\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i})),\theta_{\text{init}});\text{rm}_{i})-v_{i}({\psi}(\overrightarrow{\text{rm}},\theta_{\text{init}});\text{rm}_{i})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: Note that both (2) and (3) are because of $\text{rm}_{i}({\bm{x}}_{1})\geq\text{rm}_{i}({\bm{x}}_{2})$,
    the “<math id=$$”s are hold. ∎
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意（2）和（3）都是因为$\text{rm}_{i}({\bm{x}}_{1})\geq\text{rm}_{i}({\bm{x}}_{2})$，满足“<math
    id=$$”的条件。∎
- en: See [3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics
    of Payment Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design
    for LLM Fine-tuning with Multiple Reward Models")
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 见[3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics of
    Payment Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models")
- en: Proof.
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We first prove the necessity: if ${\psi}$ satisfies DSIC. Then for any $\text{rm}_{i},\text{rm}^{\prime}_{i}\in\mathcal{R}_{i}$,
    $k\geq 0$. By the property of DSIC, we have'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先证明必要性：如果${\psi}$满足DSIC。那么对于任何$\text{rm}_{i},\text{rm}^{\prime}_{i}\in\mathcal{R}_{i}$，$k\geq
    0$。根据DSIC的性质，我们有
- en: '|  | $\displaystyle v_{i}({\psi}((\text{rm}_{i}^{j+1},$ |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{i}({\psi}((\text{rm}_{i}^{j+1},$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: By definition of the function $l$, this is equivalent to
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 根据函数$l$的定义，这等同于
- en: '|  | $1$2 |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Sum over all $j$, we get
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有$j$求和，我们得到
- en: '|  | $1$2 |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This means that ${\psi}$ satisfies cycle monotonicity.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着${\psi}$满足循环单调性。
- en: 'Then we prove the sufficiency: if ${\psi}$ for any sequence from $\text{rm}_{i}$.
    In formal,'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们证明充分性：如果${\psi}$对来自$\text{rm}_{i}$的任何序列都成立。形式上，
- en: '|  | $1$2 |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: By cycle monotonicity, we have that for any finite and distinct sequence $[\text{rm}_{i},\text{rm}_{i}^{1},\text{rm}_{i}^{2},\cdots,\text{rm}_{i}^{k},\text{rm}^{\prime}_{i}]$,
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通过循环单调性，我们有，对于任何有限且不同的序列$[\text{rm}_{i},\text{rm}_{i}^{1},\text{rm}_{i}^{2},\cdots,\text{rm}_{i}^{k},\text{rm}^{\prime}_{i}]$，
- en: '|  | $1$2 |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: By the arbitrariness of the sequence, we can infer that
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序列的任意性，我们可以推断出
- en: '|  | $1$2 |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Since $l(\text{rm}^{\prime}_{i},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    such that for any agent $i$,
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$l(\text{rm}^{\prime}_{i},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$对于任何代理$i$，
- en: '|  | $1$2 |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\text{rm}^{*}$ is defined as follows.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\text{rm}^{*}$定义如下。
- en: '|  | $\text{rm}^{*}({\bm{x}})=1/&#124;T^{\ast}&#124;\quad\forall{\bm{x}}\in
    T^{\ast}.$ |  | (1) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{rm}^{*}({\bm{x}})=1/&#124;T^{\ast}&#124;\quad\forall{\bm{x}}\in
    T^{\ast}.$ |  | (1) |'
- en: Then, for any $\text{rm}_{i}$, we have
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于任何$\text{rm}_{i}$，我们有
- en: '|  |  | $1$2 |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'Note that (2) comes from the definition of $V$ that:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意（2）来源于$V$的定义：
- en: '|  | $\displaystyle V(\text{rm}^{*},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V(\text{rm}^{*},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle+l(\text{rm}^{\prime}_{i},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+l(\text{rm}^{\prime}_{i},\text{rm}_{i};\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: This means that mechanism $({\psi},p)$ is implementable. ∎
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着机制$({\psi},p)$是可实现的。∎
- en: See [3.7](#S3.Thmtheorem7 "Proposition 3.7\. ‣ 3.2 Characteristics of Payment
    Rules ‣ 3 Incentives for General Training Rules ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 见[3.7](#S3.Thmtheorem7 "Proposition 3.7\. ‣ 3.2 Characteristics of Payment Rules
    ‣ 3 Incentives for General Training Rules ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")
- en: Proof.
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We follow the result Theorem 1.37 in Nisan et al. [[2007](#bib.bib44)].
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们参考了Nisan等人的结果定理1.37[[2007](#bib.bib44)]。
- en: Lemma A.1  (Theorem 1.37 in Nisan et al. [[2007](#bib.bib44)]).
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理A.1（Nisan等人定理1.37[[2007](#bib.bib44)]）。
- en: Assume that the $\mathcal{R}_{1},\mathcal{R}_{2},\cdots,\mathcal{R}_{n}$ satisfy
    payment equivalence.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 假设$\mathcal{R}_{1},\mathcal{R}_{2},\cdots,\mathcal{R}_{n}$满足支付等价性。
- en: Since in our paper, we assume that for all $i\in[n]$-dim vectors, this is a
    connected set in the usual metric in the Euclidean space. Therefore, the theorem
    holds. ∎
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在我们的论文中，我们假设所有$i\in[n]$-dim向量，这在欧几里得空间的通常度量中是一个连通集。因此，定理成立。∎
- en: Appendix B Omitted proof in Section [4](#S4 "4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 第[4](#S4 "4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM
    Fine-tuning with Multiple Reward Models")节中省略的证明
- en: See [4.2](#S4.Thmtheorem2 "Theorem 4.2\. ‣ 4.1 Affine Maximizer Payment ‣ 4
    Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 见[4.2](#S4.Thmtheorem2 "Theorem 4.2\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
- en: Proof.
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We assume that for group $i$ and the initial model is $\theta_{\text{init}}$.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设对于组 $i$ 和初始模型为 $\theta_{\text{init}}$。
- en: (1) $({\psi},p^{AFF})$ satisfies DSIC.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: (1) $({\psi},p^{AFF})$ 满足 DSIC。
- en: We compare the utility between reporting $(\text{rm}_{i},w_{i})$. For convenience,
    we first simplify the notations by letting
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较报告 $(\text{rm}_{i},w_{i})$ 之间的效用。为了方便起见，我们首先通过设定来简化符号
- en: '|  | $\displaystyle\theta$ |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta$ |  |'
- en: '|  | $\displaystyle\theta^{\prime}$ |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{\prime}$ |  |'
- en: 'The valuation of group $i$ is the valuation for each agent multiply the real
    agent number:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 组 $i$ 的估值是每个代理的估值乘以实际的代理数量：
- en: '|  | $\displaystyle v_{i}$ |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{i}$ |  |'
- en: '|  | $\displaystyle v_{i}^{\prime}$ |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{i}^{\prime}$ |  |'
- en: According to the payment rule $p^{AFF}$ for $(\text{rm}^{\prime}_{i},w^{\prime}_{i})$
    is
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 根据支付规则 $p^{AFF}$，对于 $(\text{rm}^{\prime}_{i},w^{\prime}_{i})$ 是
- en: '|  | $\displaystyle p_{i}$ |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}$ |  |'
- en: '|  | $\displaystyle p_{i}^{\prime}$ |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}^{\prime}$ |  |'
- en: 'Therefore, we can calculate the change in the utility:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以计算效用的变化：
- en: '|  | $\displaystyle u_{i}^{\prime}-u_{i}=$ |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{i}^{\prime}-u_{i}=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle-$ |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle-$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: The last inequality holds by the definition of $\theta$
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一不等式由 $\theta$ 的定义保持
- en: '|  | $\theta={\psi}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i})),\theta_{\text{init}})=\arg\max_{\hat{\theta}\in\Theta}ASW((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i})),\hat{\theta};\theta_{\text{init}}).$
    |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta={\psi}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i})),\theta_{\text{init}})=\arg\max_{\hat{\theta}\in\Theta}ASW((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i})),\hat{\theta};\theta_{\text{init}}).$
    |  |'
- en: Therefore, we can conclude that, for all $\overrightarrow{\text{rm}}$, we have
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论，对于所有 $\overrightarrow{\text{rm}}$，我们有
- en: '|  | $\displaystyle u_{i}((\overrightarrow{\text{rm}},\vec{w});{\psi},p^{AFF},\text{rm}_{i},w_{i})\geq
    u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i}));{\psi},p^{AFF},\text{rm}_{i},w_{i}).$
    |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{i}((\overrightarrow{\text{rm}},\vec{w});{\psi},p^{AFF},\text{rm}_{i},w_{i})\geq
    u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),(w^{\prime}_{i},\vec{w}_{-i}));{\psi},p^{AFF},\text{rm}_{i},w_{i}).$
    |  |'
- en: (2) $({\psi},p^{AFF})$ satisfies IR.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: (2) $({\psi},p^{AFF})$ 满足 IR。
- en: 'We reuse the notations above and denote $\theta_{-i}$ truthfully report its
    reward model $\text{rm}_{i}$, the utility can be written as:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重用上述符号，并且表示 $\theta_{-i}$ 真实报告其奖励模型 $\text{rm}_{i}$，效用可以写作：
- en: '|  | $\displaystyle u_{i}$ |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{i}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=w_{i}v_{i}(\theta_{-i};\text{rm}_{i})\geq 0.$ |  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=w_{i}v_{i}(\theta_{-i};\text{rm}_{i})\geq 0.$ |  |'
- en: Therefore, we can conclude that, for all $\overrightarrow{\text{rm}}$, we have
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论，对于所有 $\overrightarrow{\text{rm}}$，我们有
- en: '|  | $\displaystyle u_{i}((\overrightarrow{\text{rm}},\vec{w});{\psi},p^{AFF},\text{rm}_{i},w_{i})\geq
    0.$ |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{i}((\overrightarrow{\text{rm}},\vec{w});{\psi},p^{AFF},\text{rm}_{i},w_{i})\geq
    0.$ |  |'
- en: ∎
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: See [4.4](#S4.Thmtheorem4 "Proposition 4.4\. ‣ 4.1 Affine Maximizer Payment
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4.4](#S4.Thmtheorem4 "命题 4.4. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 多奖励模型下的 LLM
    微调机制设计")
- en: Proof.
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: '(1) For $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}p({\bm{x}})\log p({\bm{x}})/q({\bm{x}})$
    as an optimization problem as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 对于 $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}p({\bm{x}})\log p({\bm{x}})/q({\bm{x}})$
    作为优化问题如下：
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{{\bm{x}}\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}({\bm{x}})$ |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{{\bm{x}}\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}({\bm{x}})$ |  |'
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})$
    |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{s.t.}\quad\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})$
    |  |'
- en: '|  | $\displaystyle\text{LLM}_{\theta}({\bm{x}})$ |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{LLM}_{\theta}({\bm{x}})$ |  |'
- en: Since we have assumed that the optimal point is unique, and the optimal model
    $\text{LLM}_{\theta}$ is that there exists $\mu\in\mathbb{R}$, such that
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设最优点是唯一的，并且最优模型 $\text{LLM}_{\theta}$ 存在 $\mu\in\mathbb{R}$，使得
- en: '|  | $1$2 |  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Similarly, for the input $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$
    satisfies
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于输入 $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$ 满足
- en: '|  | $1$2 |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For convenience, we define $\Delta({\bm{x}})=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}({\bm{x}})-\sum_{i=1}^{n}w_{i}\text{rm}_{i}({\bm{x}})$
    is given by
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们定义 $\Delta({\bm{x}})=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}({\bm{x}})-\sum_{i=1}^{n}w_{i}\text{rm}_{i}({\bm{x}})$
    由
- en: '|  | $\text{LLM}_{\theta^{\prime}}({\bm{x}})=\text{LLM}_{\theta}({\bm{x}})e^{\frac{1}{\lambda}(\Delta({\bm{x}})+\mu-\mu^{\prime})}.$
    |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{LLM}_{\theta^{\prime}}({\bm{x}})=\text{LLM}_{\theta}({\bm{x}})e^{\frac{1}{\lambda}(\Delta({\bm{x}})+\mu-\mu^{\prime})}.$
    |  |'
- en: Note that we also have the condition
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还有条件
- en: '|  | $1$2 |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Since $1$2, we can infer that
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $1$2，我们可以推断出
- en: '|  | $\displaystyle 1$ |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 1$ |  |'
- en: '|  | $\displaystyle 1$ |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 1$ |  |'
- en: This is equivalent to
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于
- en: '|  | $\displaystyle\min_{{\bm{x}}\in T^{\ast}}\Delta({\bm{x}})\leq\mu^{\prime}-\mu\leq\max_{{\bm{x}}\in
    T^{\ast}}\Delta({\bm{x}}).$ |  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{x}}\in T^{\ast}}\Delta({\bm{x}})\leq\mu^{\prime}-\mu\leq\max_{{\bm{x}}\in
    T^{\ast}}\Delta({\bm{x}}).$ |  |'
- en: Thus, the difference for $\text{LLM}_{\theta}({\bm{x}})$ can be bounded by
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\text{LLM}_{\theta}({\bm{x}})$ 的差异可以被界定为
- en: '|  | $\displaystyle&#124;\text{LLM}_{\theta^{\prime}}({\bm{x}})-\text{LLM}_{\theta}({\bm{x}})&#124;$
    |  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\text{LLM}_{\theta^{\prime}}({\bm{x}})-\text{LLM}_{\theta}({\bm{x}})&#124;$
    |  |'
- en: '|  |  | $\displaystyle\leq\left&#124;1-e^{\frac{1}{\lambda}(\Delta({\bm{x}})+\mu-\mu^{\prime})}\right&#124;$
    |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\left&#124;1-e^{\frac{1}{\lambda}(\Delta({\bm{x}})+\mu-\mu^{\prime})}\right&#124;$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: For any $$\delta></math>, we have
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $$\delta></math>，我们有
- en: '|  | $1$2 |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '(2) For $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}(p({\bm{x}})-q({\bm{x}}))^{2}$
    as an optimization problem as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对于 $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}(p({\bm{x}})-q({\bm{x}}))^{2}$ 作为以下优化问题：
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{x\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}(x)$ |  |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{x\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}(x)$ |  |'
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)$
    |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{s.t.}\quad\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)$
    |  |'
- en: '|  | $\displaystyle\text{LLM}_{\theta}(x)$ |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{LLM}_{\theta}(x)$ |  |'
- en: Since we have assumed that the optimal point is unique, and the optimal model
    $\text{LLM}_{\theta}$ is that there exists $\mu\in\mathbb{R}$, such that
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设最优点是唯一的，并且最优模型 $\text{LLM}_{\theta}$ 存在 $\mu\in\mathbb{R}$，使得
- en: '|  | $1$2 |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Similarly, for the input $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$
    satisfies
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于输入 $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$ 满足
- en: '|  | $1$2 |  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For convenience, we define $\Delta(x)=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)-\sum_{i=1}^{n}w_{i}\text{rm}_{i}(x)$
    is given by
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们定义 $\Delta(x)=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)-\sum_{i=1}^{n}w_{i}\text{rm}_{i}(x)$
    由
- en: '|  | $\text{LLM}_{\theta^{\prime}}(x)=\text{LLM}_{\theta}(x)+\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime}).$
    |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{LLM}_{\theta^{\prime}}(x)=\text{LLM}_{\theta}(x)+\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime}).$
    |  |'
- en: Note that we also have the condition
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还有条件
- en: '|  | $1$2 |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Since $\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)=1$, we can infer that
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)=1$，我们可以推断出
- en: '|  | $\displaystyle\sum_{x\in T^{\ast}}\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime})=0.$
    |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{x\in T^{\ast}}\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime})=0.$
    |  |'
- en: This is equivalent to
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于
- en: '|  | $\displaystyle\mu^{\prime}-\mu=\frac{1}{&#124;T^{\ast}&#124;}\sum_{x\in
    T^{\ast}}\Delta(x).$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu^{\prime}-\mu=\frac{1}{&#124;T^{\ast}&#124;}\sum_{x\in
    T^{\ast}}\Delta(x).$ |  |'
- en: Thus, the difference for $\text{LLM}_{\theta}(x)$ can be bounded by
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\text{LLM}_{\theta}(x)$ 的差异可以被界定为
- en: '|  | $1$2 |  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For any <math id=$$, we have
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 <math id=$$, 我们有
- en: '|  | $&#124;\text{LLM}_{\theta^{\prime}}(x)-\text{LLM}_{\theta}(x)&#124;\leq\frac{1}{\lambda}\max_{x\in
    T^{\ast}}&#124;\Delta(x)&#124;\leq\delta.$ |  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;\text{LLM}_{\theta^{\prime}}(x)-\text{LLM}_{\theta}(x)&#124;\leq\frac{1}{\lambda}\max_{x\in
    T^{\ast}}&#124;\Delta(x)&#124;\leq\delta.$ |  |'
- en: ∎
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: See [4.5](#S4.Thmtheorem5 "Theorem 4.5\. ‣ 4.1 Affine Maximizer Payment ‣ 4
    Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4.5](#S4.Thmtheorem5 "定理 4.5\. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ LLM 微调的机制设计与多个奖励模型")
- en: Proof.
  id: totrans-400
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We prove the equivalent version of payment equivalence: For any group $i$,
    $p^{\prime}$ for any $\text{rm}_{i}$ and will omit these notations.*'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们证明了支付等价的等效版本：对于任何组 $i$，$p^{\prime}$ 对于任何 $\text{rm}_{i}$ 并且将省略这些符号。
- en: We first redefine the functions $l(\cdot,\cdot)$ are defined on the types space
    of the group. For the simplified case in [Section 3](#S3 "3 Incentives for General
    Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"),
    the type is exactly the reward model $\text{rm}_{i}$ to represent the combination
    $(\text{rm}_{i},w_{i})$ is used to represented for the $\text{rm}_{i}$.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先重新定义函数 $l(\cdot,\cdot)$，这些函数定义在该组的类型空间上。对于[第3节](#S3 "3 Incentives for General
    Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")中的简化情况，类型正是奖励模型
    $\text{rm}_{i}$，用于表示组合 $(\text{rm}_{i},w_{i})$ 被用来表示 $\text{rm}_{i}$。
- en: Similar to the simplified version discussed in [Section 3](#S3 "3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), we let $l(t^{\prime}_{i},t_{i})$. In formal,
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第3节](#S3 "3 Incentives for General Training Rules ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models")中讨论的简化版本，我们令 $l(t^{\prime}_{i},t_{i})$。正式地，
- en: '|  | $1$2 |  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: And $V(t^{\prime}_{i},t_{i})$
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 和 $V(t^{\prime}_{i},t_{i})$
- en: '|  | $1$2 |  |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: We first prove the following lemma, which is a special case in Heydenreich et al.
    [[2009](#bib.bib29)],
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先证明以下引理，这是Heydenreich等人的一个特例[[2009](#bib.bib29)]，
- en: Lemma B.1.
  id: totrans-408
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.1。
- en: An implemented training rule ${\psi}$, we have
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的训练规则 ${\psi}$，我们有
- en: '|  | $V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i}).$ |  |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i}).$ |  |'
- en: Proof.
  id: totrans-411
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Assume there is a mechanism $({\psi},p)$, let $t_{i}^{0}=t^{\prime}_{i}$, we
    have that
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 假设存在一个机制 $({\psi},p)$，令 $t_{i}^{0}=t^{\prime}_{i}$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This can be rewritten as
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重写为
- en: '|  | $1$2 |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Sum over $j$, we get the following inequality
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 对 $j$ 求和，我们得到以下不等式
- en: '|  | $\displaystyle\sum_{j=0}^{k}l(t_{i}^{j},t_{i}^{j+1})$ |  |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{j=0}^{k}l(t_{i}^{j},t_{i}^{j+1})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Since this holds for arbitrary finite and distinct sequences, we can infer that
    $V(t^{\prime}_{i},t_{i})\geq p(t_{i})-p(t^{\prime}_{i})$, there is
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这一点对任意有限且不同的序列都成立，我们可以推断 $V(t^{\prime}_{i},t_{i})\geq p(t_{i})-p(t^{\prime}_{i})$，存在
- en: '|  | $1$2 |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: which means that $p(t^{\prime}_{i})-p(t_{i})=V(t_{i},t^{\prime}_{i})$, the payment
    $p(t_{i})$ both implement ${\psi}$ as $p^{*}$
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 $p(t^{\prime}_{i})-p(t_{i})=V(t_{i},t^{\prime}_{i})$，支付 $p(t_{i})$ 都实现了
    ${\psi}$ 作为 $p^{*}$。
- en: '|  |  | $\displaystyle p_{i}(t_{i})-p^{\prime}_{i}(t_{i})$ |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle p_{i}(t_{i})-p^{\prime}_{i}(t_{i})$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: Note that $p^{*}$, $\vec{w}_{-i}$ on $(\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$.
    ∎
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 $p^{*}$，$\vec{w}_{-i}$ 在 $(\overrightarrow{\text{rm}}_{-i},\theta_{\text{init}})$。∎
- en: Then we show that under [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine
    Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), SW-Maximizing training rule satisfies
    the condition stated in [Lemma B.1](#A2.Thmtheorem1 "Lemma B.1\. ‣ Proof. ‣ Appendix
    B Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"). Firstly, we show that for any $t_{i},t^{\prime}_{i}$ and $V(t^{\prime}_{i},t_{i})$
    to $t_{i}$. Since SW-Maximizing training rule is implementable, by [Theorem 3.5](#S3.Thmtheorem5
    "Theorem 3.5 (Rochet (1987)). ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models"), we know that the weight for any cycle is non-negative. Therefore,
    $V(t_{i},t^{\prime}_{i})+V(t^{\prime}_{i},t_{i})\geq 0$ must be satisfied.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们展示在[4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment
    ‣ 4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")下，SW-最大化训练规则满足[引理 B.1](#A2.Thmtheorem1 "Lemma B.1\.
    ‣ Proof. ‣ Appendix B Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")中陈述的条件。首先，我们展示对于任何 $t_{i},t^{\prime}_{i}$ 和 $V(t^{\prime}_{i},t_{i})$
    到 $t_{i}$。由于SW-最大化训练规则是可实现的，根据[定理 3.5](#S3.Thmtheorem5 "Theorem 3.5 (Rochet (1987)).
    ‣ 3.2 Characteristics of Payment Rules ‣ 3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")，我们知道任何周期的权重都是非负的。因此，必须满足
    $V(t_{i},t^{\prime}_{i})+V(t^{\prime}_{i},t_{i})\geq 0$。
- en: Then we show that for any $t_{i},t^{\prime}_{i}$ such that
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们证明对于任何 $t_{i},t^{\prime}_{i}$，使得
- en: '|  | $1$2 |  | (2) |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: This is suffice for $V(t_{i},t^{\prime}_{i})+V(t^{\prime}_{i},t_{i})\leq\epsilon$
    and $\sum_{j=0}^{k}l(t_{i}^{j+1},t_{i}^{j})$ respectively.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于 $V(t_{i},t^{\prime}_{i})+V(t^{\prime}_{i},t_{i})\leq\epsilon$ 和 $\sum_{j=0}^{k}l(t_{i}^{j+1},t_{i}^{j})$
    足够。
- en: Initially, we rewrite the LHS of [Equation 2](#A2.E2 "In Proof. ‣ Appendix B
    Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models") by the definition of the function $l(\cdot,\cdot)$.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们通过函数$l(\cdot,\cdot)$的定义重写[方程 2](#A2.E2 "证明。 ‣ 附录 B 省略的证明在第 4 节 ‣ 多奖励模型的LLM微调机制设计")的左侧。
- en: '|  |  | $1$2 |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: In the above equations, $\theta^{j}={\psi}(t_{i}^{j})$.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，$\theta^{j}={\psi}(t_{i}^{j})$。
- en: By [4.3](#S4.Thmtheorem3 "Assumption 4.3\. ‣ 4.1 Affine Maximizer Payment ‣
    4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models"), when $\overrightarrow{\text{rm}}_{-i}$ such that if
    $\max_{x\in T^{\ast}}|w_{i}\text{rm}_{i}(x)-w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)|\leq\delta$.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[4.3](#S4.Thmtheorem3 "假设 4.3\. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 多奖励模型的LLM微调机制设计")，当$\overrightarrow{\text{rm}}_{-i}$使得$\max_{x\in
    T^{\ast}}|w_{i}\text{rm}_{i}(x)-w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)|\leq\delta$。
- en: We construct the sequence $P$. For each $0\leq j\leq n$,
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构造了序列$P$。对于每个$0\leq j\leq n$，
- en: '|  | $w_{i}^{j}=w_{i}^{0}=w_{i},\quad\text{rm}_{i}^{j}=\text{rm}_{i}^{j-1}+j(\frac{\text{rm}^{*}-\text{rm}}{n}).$
    |  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{i}^{j}=w_{i}^{0}=w_{i},\quad\text{rm}_{i}^{j}=\text{rm}_{i}^{j-1}+j(\frac{\text{rm}^{*}-\text{rm}}{n}).$
    |  |'
- en: And for each $n+1\leq j\leq 2n+1$,
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个$n+1\leq j\leq 2n+1$，
- en: '|  | $1$2 |  |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: In this construction, any $\text{rm}_{i}^{j}$. This ensures that the all reward
    models in the sequence is valid (normalized and non-negative). We can then divide
    the above equation into three parts, making the $w_{i}$ the same in the first
    and the last parts.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个构造中，任何$\text{rm}_{i}^{j}$。这确保了序列中的所有奖励模型都是有效的（标准化且非负的）。然后我们可以将上述方程分为三部分，使得第一个和最后一部分中的$w_{i}$相同。
- en: '|  |  | $1$2 |  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  | (a) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  | (a) |'
- en: '|  | $\displaystyle+$ |  | (b) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ |  | (b) |'
- en: '|  | $\displaystyle+$ |  | (c) |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ |  | (c) |'
- en: We first show that (b) equals to $0$ and the uniqueness of the optimal point,
    we have that
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示(b)等于$0$和最优点的唯一性，我们有：
- en: '|  |  | $1$2 |  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $$\displaystyle></math> |  |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle></math> |  |'
- en: 'Note that $\text{rm}^{*}(x)=\frac{1}{|T^{\ast}|}$. Thus, the above equation
    can rewritten as:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到$\text{rm}^{*}(x)=\frac{1}{|T^{\ast}|}$。因此，上述方程可以重新写成：
- en: '|  |  | $1$2 |  |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | <math id=$$ |  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id=$$ |  |'
- en: This contradicted the optimality of $\theta^{n}$.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这与$\theta^{n}$的最优性相矛盾。
- en: 'Then we turn to (a). By the construction, for any $x\in T^{\ast}$ holds for
    all $x$. Then we can derive that:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们转到(a)。根据构造，对于任何$x\in T^{\ast}$都成立。然后我们可以推导出：
- en: '|  |  | $1$2 |  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'The case is similar to (c). By the construction, for any $x\in T^{\ast}$ holds
    for all $x$. Then we can derive that:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 情况类似于(c)。根据构造，对于任何$x\in T^{\ast}$都成立。然后我们可以推导出：
- en: '|  |  | $1$2 |  |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: Combining the results from (a), (b), and (c), we have that under this construction,
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 结合(a)、(b)和(c)中的结果，我们有，在这种构造下，
- en: '|  | $1$2 |  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: By the arbitrariness of <math id="A2.19.p10.11.m1.1" class="ltx_Math" alttext="\epsilon></math>.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 根据<math id="A2.19.p10.11.m1.1" class="ltx_Math" alttext="\epsilon></math>的任意性。
- en: Therefore, it is proven that
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已证明
- en: '|  | $V(t_{i},t^{\prime}_{i})+V(t_{i},t^{\prime}_{i})=0.$ |  |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(t_{i},t^{\prime}_{i})+V(t_{i},t^{\prime}_{i})=0.$ |  |'
- en: which means that $V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i})$. ∎
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着$V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i})$。∎
- en: See [4.6](#S4.Thmtheorem6 "Corollary 4.6\. ‣ 4.1 Affine Maximizer Payment ‣
    4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 见[4.6](#S4.Thmtheorem6 "推论 4.6\. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 多奖励模型的LLM微调机制设计")
- en: Proof.
  id: totrans-476
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Given the payment equivalence of ${\psi}$ here.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这里的${\psi}$的支付等价性。
- en: '|  | $\displaystyle\max_{h_{i}}\quad$ |  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{h_{i}}\quad$ |  |'
- en: '|  | s.t. | $1$2 |  |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $1$2 |  |'
- en: The solution of this programming can be trivially given by,
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这个编程的解可以很简单地给出，
- en: '|  | $\displaystyle h_{i}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=$
    |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{i}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=$
    |  |'
- en: '|  | $\displaystyle=:$ |  |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=:$ |  |'
- en: Therefore, the revenue-maximizing payment is
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，收入最大化支付是
- en: '|  | $\displaystyle p_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})=$
    |  |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})=$
    |  |'
- en: '|  | $\displaystyle+$ |  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ |  |'
- en: ∎
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Lemma B.2.
  id: totrans-487
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.2。
- en: For any $\text{rm},\text{rm}^{\prime}$, we have
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $\text{rm},\text{rm}^{\prime}$，我们有
- en: '|  | $&#124;v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})&#124;\leq\epsilon$
    |  |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left|v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})\right|\leq\epsilon$
    |  |'
- en: Proof.
  id: totrans-490
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We can derive that
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推导出
- en: '|  | $\displaystyle&#124;v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})&#124;$
    |  |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left|v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})\right|$
    |  |'
- en: '|  |  | $\displaystyle\leq\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\epsilon=\epsilon.$
    |  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\epsilon=\epsilon.$
    |  |'
- en: ∎
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: See [4.8](#S4.Thmtheorem8 "Lemma 4.8\. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4.8](#S4.Thmtheorem8 "Lemma 4.8. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
- en: Proof.
  id: totrans-496
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Let $\hat{\theta}={\psi}(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta_{\text{init}})$.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\hat{\theta}={\psi}(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta_{\text{init}})$。
- en: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\hat{\theta};\theta_{\text{init}})$
    |  |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\hat{\theta};\theta_{\text{init}})$
    |  |'
- en: '|  |  | $\displaystyle\overset{(1)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\hat{\theta};\widehat{\text{rm}}_{i})-\epsilon\right)-\lambda
    D(\hat{\theta}&#124;&#124;\theta_{\text{init}})$ |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(1)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\hat{\theta};\widehat{\text{rm}}_{i})-\epsilon\right)-\lambda
    D(\hat{\theta}||\theta_{\text{init}})$ |  |'
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\hat{\theta};\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\hat{\theta};\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
- en: '|  |  | $\displaystyle\overset{(2)}{\geq}ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(2)}{\geq}ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{i=1}^{n}w_{i}v_{i}(\theta;\widehat{\text{rm}}_{i})-\lambda
    D(\theta&#124;&#124;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{i=1}^{n}w_{i}v_{i}(\theta;\widehat{\text{rm}}_{i})-\lambda
    D(\theta||\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
- en: '|  |  | $\displaystyle\overset{(3)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\theta;\text{rm}_{i})-\epsilon\right)-\lambda
    D(\theta&#124;&#124;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(3)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\theta;\text{rm}_{i})-\epsilon\right)-\lambda
    D(\theta||\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-2\sum_{i=1}^{n}w_{i}\epsilon.$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-2\sum_{i=1}^{n}w_{i}\epsilon.$
    |  |'
- en: (1) and (3) can be directly induced by [Lemma B.2](#A2.Thmtheorem2 "Lemma B.2\.
    ‣ Appendix B Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), and (2) holds by the definition of $\hat{\theta}$.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 和 (3) 可以直接由 [引理 B.2](#A2.Thmtheorem2 "Lemma B.2. ‣ Appendix B Omitted proof
    in Section 4 ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
    推导，(2) 由 $\hat{\theta}$ 的定义得出。
- en: '|  | $1$2 |  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: See [4.9](#S4.Thmtheorem9 "Theorem 4.9\. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4.9](#S4.Thmtheorem9 "Theorem 4.9. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
- en: Proof.
  id: totrans-509
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Recall that the calculation of payment in $p^{AFF}$ is
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下 $p^{AFF}$ 中支付的计算
- en: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
- en: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
- en: 'Let $\vec{w}=(w_{i},\vec{w}_{-i})$, the utility function can be written as:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\vec{w}=(w_{i},\vec{w}_{-i})$，效用函数可以写成：
- en: '|  | $\displaystyle u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w};{\psi},p,\text{rm}_{i},w_{i})$
    |  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w};{\psi},p,\text{rm}_{i},w_{i})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta_{-i};\theta_{\text{init}}),$
    |  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta_{-i};\theta_{\text{init}}),$
    |  |'
- en: where we define $\theta={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w},\theta_{\text{init}})$
    or $w_{i}$.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 $\theta={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w},\theta_{\text{init}})$
    或 $w_{i}$。
- en: 'Therefore, we can derive that:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以推导出：
- en: '|  |  | $1$2 |  |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\overset{(1)}{\geq}$ |  |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(1)}{\geq}$ |  |'
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
- en: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
- en: '|  | $\displaystyle\overset{(4)}{=}$ |  |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(4)}{=}$ |  |'
- en: '|  | $\displaystyle\overset{(5)}{\geq}$ |  |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(5)}{\geq}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: All the $\hat{\theta}$ and $\widehat{\text{rm}}_{i}$.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的 $\hat{\theta}$ 和 $\widehat{\text{rm}}_{i}$。
- en: Therefore, we get
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到
- en: '|  | $1$2 |  |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
