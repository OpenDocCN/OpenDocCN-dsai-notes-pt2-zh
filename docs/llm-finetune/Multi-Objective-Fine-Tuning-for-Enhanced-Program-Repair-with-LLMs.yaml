- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:37:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.12636](https://ar5iv.labs.arxiv.org/html/2404.12636)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Boyang Yang^(1,2)¹, Haoye Tian^(3,4)¹, Jiadong Ren¹, Hongyu Zhang⁵, Jacques
    Klein⁴, Tegawendé F. Bissyandé⁴,
  prefs: []
  type: TYPE_NORMAL
- en: Claire Le Goues³, Shunfu Jin¹² 1Co-first authors who contributed equally to
    this work.2Corresponding author. ¹School of Information Science and Engineering,
    Yanshan University,
  prefs: []
  type: TYPE_NORMAL
- en: ²Jisuan Institute of Technology, Beijing JudaoYouda Network Technology Co. Ltd.,
  prefs: []
  type: TYPE_NORMAL
- en: ³School of Computer Science, Carnegie Mellon University,
  prefs: []
  type: TYPE_NORMAL
- en: ⁴SnT, University of Luxembourg,
  prefs: []
  type: TYPE_NORMAL
- en: ⁵School of Big Data and Software Engineering, Chongqing University yangboyang@jisuanke.com,
    tianhaoyemail@gmail.com, jdren@ysu.edu.cn, hongyujohn@gmail.com,
  prefs: []
  type: TYPE_NORMAL
- en: jacques.klein@uni.lu, tegawende.bissyande@uni.lu, clegoues@cs.cmu.edu, jsf@ysu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have demonstrated remarkable capabilities on a
    broad spectrum of downstream tasks. Within the realm of software engineering,
    specialized tasks on code, such as program repair, present unique challenges,
    necessitating fine-tuning to unlock state-of-the-art performance. Fine-tuning
    approaches proposed in the literature for LLMs on program repair tasks are however
    generally overlooking the need to reason about the logic behind code changes,
    beyond syntactic patterns in the data. High-performing fine-tuning experiments
    also usually come at very high computational costs. With MORepair, we propose
    a novel perspective on the learning focus of LLM fine-tuning for program repair:
    we not only adapt the LLM parameters to the syntactic nuances of the task of code
    transformation (objective ❶), but we also specifically fine-tune the LLM with
    respect to the logical reason behind the code change in the training data (objective
    ❷). Such a multi-objective fine-tuning will instruct LLMs to generate high-quality
    patches.'
  prefs: []
  type: TYPE_NORMAL
- en: We apply MORepair to fine-tune four open-source LLMs with different sizes and
    architectures. Experimental results on C++ and Java repair benchmarks show that
    the implemented fine-tuning effectively boosts LLM repair performance by 7.6%
    to 10% in Top-10 repair suggestions. We further show that our fine-tuning strategy
    yields superior performance compared to the incumbent state-of-the-art in fine-tuned
    models for program repair, Fine-tune-CoT and RepairLLaMA.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models have achieved promising performance on a variety of tasks
    in different domains. In software engineering, among many code-related tasks,
    automated program repair (APR) has greatly benefited from the general knowledge
    encoded in prominent models such as GPT-4 [[1](#bib.bib1)]. Recent studies [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)] have indeed shown that LLMs can even outperform
    traditional APR tools. Researchers have realized these achievements through two
    main strategies: prompt engineering and fine-tuning. Indeed, to steer LLMs towards
    adapting to the specific format of repair, few-shot learning techniques have been
    employed [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]
    where a small set of example patches are provided in the prompt along with the
    buggy code to repair. While few-shot-based approaches have shown better performance
    than initial zero-short-based attempts [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)],
    prompting is inherently limited by the pre-trained model capabilities. Prompting
    further often fails to produce high-quality patches within the constraints of
    developers’ attempt limits [[13](#bib.bib13)]. In contrast, fine-tuning-based
    approaches [[4](#bib.bib4), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)] strive to refine the fundamental capabilities of LLMs and have
    therefore demonstrated substantially greater potential in achieving reliable program
    repair. In practice, fine-tuning consists in adapting a pre-trained LLM on a very
    specific dataset, such as patches, or task, such as program repair, enabling the
    model to refine its knowledge and improve performance in targeted areas [[18](#bib.bib18)].
    Unfortunately, the existing literature proposes approaches that still face two
    major limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: ①
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Need for Reasoning on Repair Logic: The program repair task is complex: it
    demands some deep comprehension of control and data flow, of the developer intentions
    in the design of the buggy code, and finally of the intrinsic repair logic. Yet,
    most of the standard fine-tuning approaches for LLM-based program repair focus
    on optimizing the training dataset [[4](#bib.bib4), [15](#bib.bib15), [19](#bib.bib19)].
    Thus, while with such approaches the LLMs can be refined to notice some repair
    patterns, the actual logic reasoning behind the repair operation (”the why”) is
    not explicitly learned.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ②
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'High Cost: Fine-tuning for program repair generally requires large datasets
    to achieve state of the art performance. In recent works, Lajkó *et al.* [[15](#bib.bib15)]
    used 16k samples to fine-tune GPT-2, while RepairLLaMA [[17](#bib.bib17)] was
    fine-tuned with about 30-50k code pairs. Such large datasets further suggest the
    expenditure of significant computational resource. Creating and expanding these
    datasets takes substantial effort and time, emphasizing the resource-intensive
    nature of fine-tuning in program repair. With limited public datasets available,
    manual construction of training data further increases labor costs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This paper. In this work, we propose a new fine-tuning objective using natural
    language explanations of code changes to capture the logic underlying a given
    repair operation. This objective, which seeks conversational guidance, is considered
    in addition to the classical objective of learning code transformations. MORepair
    is thus designed as a novel, effective multi-objective fine-tuning framework for
    LLM-based program repair.
  prefs: []
  type: TYPE_NORMAL
- en: 'By focusing on conversational guidance, i.e., natural language, MORepair ensures
    that the learning is programming language-independent, making it suitable for
    multilingual repair scenarios. Furthermore, by conducting multi-objective learning,
    we indirectly scale up the learning dataset: more pattern combinations can be
    explored in a small dataset. We also observe that conversational guidance presents
    the benefit of providing various potential fix strategies that extend beyond the
    confines of a specific buggy code. As such, our approach does not depend on large-scale
    datasets for fine-tuning that are required by prior works. Experimentally, we
    show that with an order of magnitude smaller dataset, we achieve higher fine-tuning
    performance than prior works. Finally, to account for insufficient/missing patch
    descriptions, we rely on LLMs to generate high-quality patch guidance. The successful
    application of such automatically generated guidance is essential as it relieves
    APR from this expensive human input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply MORepair to fine-tune four open-source LLMS, namely CodeLlama-13B-instruct [[20](#bib.bib20)],
    CodeLlama-7B-instruct [[20](#bib.bib20)], StarChat-alpha [[21](#bib.bib21)], and
    Mistral-Instruct-7B-v0.1 [[22](#bib.bib22)], which are chosen to represent a variety
    of model sizes and architectures. These are assessed against two new repair benchmarks,
    EvalRepair-C++ and EvalRepair-Java, which we produced based on HumanEval [[23](#bib.bib23)]
    by including augmented test cases to avoid patch over-fitting [[24](#bib.bib24)].
    The experiments demonstrate that the proposed fine-tuning technique effectively
    improves the LLM performance on the program repair task: CodeLlama-13B-instruct
    performance is improved by 11% and 8% on the EvalRepair-C++ and EvalRepair-Java
    benchmarks, respectively. Similar performance improvements have been observed
    across all LLMs. We also show that MORepair is indeed superior to the fine-tuning
    approaches used for state of the art models such as Fine-tune-CoT [[25](#bib.bib25)]
    and RepairLLaMA [[17](#bib.bib17)]. Finally, we show that MORepair has the ability
    to narrow the performance gap between small open-source models and larger closed-source
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of our work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approach. We propose MORepair, a novel multi-objective fine-tuning framework
    designed specifically for LLM-based program repair. MORepair steers LLMs towards
    a precise understanding the reasoning logic behind the repair process, thereby
    enabling them to generate high-quality patches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benchmarks. We provide two new repair benchmarks, EvalRepair-C++ and EvalRepair-Java,
    consisting of 164 and 163 patches (pairs of code samples), respectively. EvalRepair-C++
    was created by manually introducing bugs into the ground truth C++ code from HumanEval-X [[26](#bib.bib26)],
    while EvalRepair-Java is derived from HumanEval-Java [[4](#bib.bib4)]. To mitigate
    patch overfitting impact on the reported performance metrics, we augment the original
    test suites of both benchmarks: we indeed observe a decline of up to $\sim$9%
    in terms of top-10 repair predictions from CodeLlama-13B-instruct when we apply
    it to the new EvalRepair-C++ benchmark.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments. We conduct a comprehensive evaluation of MORepair’s effectiveness
    for improving the performance of open source LLMs, as well as its generalizability
    across various LLMs and different programming languages. The assessment also considers
    baseline models and baseline fine-tuning approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insights. Through an ablation study, we assess the impact of LLM-generated guidance
    within the MORepair framework. Additionally, we compare the repair performance
    of MORepair with state-of-the-art fine-tuning methods, including Fine-tune-CoT
    and RepairLLaMA. This study highlights the value of LLM-generated guidance in
    MORepair and underscores MORepair’s superior effectiveness over current fine-tuning
    approaches in program repair tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Motivating Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1---  0_58980_RE_SEGV.cpp2+++  0_58980_AC.cpp3@@  -35,8  +35,8  @@4  num.push(cal(tmp1,tmp2,opr));5  }6  op.pop();7  }else  if  (s[i]  ==  ’+’  ||  s[i]  ==  ’-’)  {8-  while  (!op.empty())  {9+  while  (!op.empty()&&op.top()!=’(’)  {10  int  tmp1  =  num.top();11  num.pop();12  int  tmp2  =  num.top();'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: An example patch from TutorLLMCode.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [1](#S2.F1 "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") provides an example patch for repairing
    a C++ program in the TutorLLMCode dataset. The bug is related to the handling
    of precedence in the operator of arithmetic expressions. When operation expressions
    are mixed or rather interleaved, the buggy program could yield inaccurate evaluation
    of the arithmetic expressions. In TutorLLMCode, this repair case is an example
    of a patch that was necessary to fix the solution provided by a student to an
    Expression Evaluation problem. The solution to this engineering problem is to
    introduce parentheses in basic arithmetic operations (addition, subtraction, multiplication,
    division). Writing this code without taking into account the presence of parentheses
    may lead to bugs.
  prefs: []
  type: TYPE_NORMAL
- en: '1Line  39,  also  ensure  that  the  top  element  of  the  stack  is  not  a  left  parenthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 1: Human-generated guidance for yielding the patch in Figure [1](#S2.F1
    "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning for Enhanced Program
    Repair with LLMs")'
  prefs: []
  type: TYPE_NORMAL
- en: The human-generated guidance in Listing [1](#LST1 "Listing 1 ‣ II Motivating
    Example ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs")
    accurately points to the buggy location, and also conceptually describes the necessary
    code checks that are missing. We postulate that such information is precious as
    it informs about the logical reasoning behind the requested repair and abstracts
    away from the particular syntax of the associated program, enabling a fine-tuning
    process with this information to potentially generalize to a broader set of programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing [2](#LST2 "Listing 2 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") depicts the LLM-generated guidance for
    this repair case. The LLM was prompted with the buggy along the fixed code and
    instructed to provide step-by-step descriptions of the repair logic. Interestingly,
    it goes into more depth and offers a more comprehensive and contextual analysis
    of the buggy code. The LLM identifies the primary issue: the correct handling
    of ’+’ and ’-’ operators when not preceded by a ’(’, and highlights four potential
    problems. Concretely, the LLM-guidance outlines in details some corrections by
    refining the logic for operator precedence, by ensuring accurate parenthesis handling,
    by mentioning parameter order adjustments in the cal function for subtraction
    and division, and by introducing error handling for enhanced robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: '1Certainly!  Let’s  go  through  the  code  step  by  step  and  identify  the  issues  that  need  to  be  fixed:21.  **Operator  Precedence**:  In  the  original  code,  when  encountering  a  ’+’  or  ’-’,  it  immediately  evaluates  all  operations  in  the  stack  without  considering  operator  precedence.  This  is  incorrect  because  ’*’  and  ’/’  should  have  higher  precedence  and  should  be  evaluated  first.3  **Fix**:  When  encountering  a  ’+’  or  ’-’,  only  evaluate  the  top  of  the  stack  if  it  is  not  a  ’(’  and  if  the  operator  at  the  top  of  the  stack  is  ’+’  or  ’-’.  Do  not  evaluate  ’*’  or  ’/’  immediately;  they  should  be  evaluated  when  the  next  ’+’  or  ’-’  is  encountered  or  at  the  end  of  the  expression.42.  **Parentheses  Handling**:  The  original  code  correctly  handles  the  opening  parenthesis  ’(’,  but  when  it  encounters  a  closing  parenthesis  ’)’,  it  evaluates  all  operations  until  it  finds  the  corresponding  ’(’.  This  is  correct,  but  it  does  not  consider  operator  precedence  within  the  parentheses.5  **Fix**:  When  encountering  a  ’)’,  evaluate  the  operations  in  the  stack  until  you  reach  the  corresponding  ’(’.  Make  sure  to  evaluate  ’*’  and  ’/’  before  ’+’  and  ’-’.63.  **End  of  Expression**:  At  the  end  of  the  expression,  the  original  code  evaluates  all  remaining  operations  in  the  stack.  This  is  correct,  but  again,  it  does  not  consider  operator  precedence.7  **Fix**:  When  reaching  the  end  of  the  expression,  evaluate  the  remaining  operations  in  the  stack  while  considering  operator  precedence.  Evaluate  ’*’  and  ’/’  before  ’+’  and  ’-’.84.  **Function  ‘cal‘**:  The  ‘cal‘  function  has  the  parameters  in  the  wrong  order  for  subtraction  and  division.  This  will  lead  to  incorrect  results  when  these  operations  are  performed.9  **Fix**:  Swap  the  order  of  the  parameters  ‘x‘  and  ‘y‘  in  the  subtraction  and  division  cases  within  the  ‘cal‘  function  to  ensure  the  correct  order  of  operands.105.  **Error  Handling**:  The  original  code  does  not  handle  errors  or  invalid  input.  While  the  problem  statement  guarantees  valid  input,  it’s  good  practice  to  include  error  handling.11  **Fix**:  Add  error  handling  to  check  for  invalid  characters,  mismatched  parentheses,  or  other  potential  input  errors.  This  is  not  explicitly  required  by  the  problem  statement  but  is  a  good  practice.12By  addressing  these  issues,  the  code  should  correctly  evaluate  expressions  with  proper  operator  precedence  and  parentheses  handling.  Remember  to  test  the  code  with  various  expressions  to  ensure  that  all  edge  cases  are  handled  correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 2: LLM-generated guidance for the patch in Figure [1](#S2.F1 "Figure
    1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs")'
  prefs: []
  type: TYPE_NORMAL
- en: Our main intuition in this work is that the conversational guidance generated
    from repair examples will greatly benefit a model fine-tuning procedure for program
    repair.
  prefs: []
  type: TYPE_NORMAL
- en: III Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide an overview of our proposed approach, followed by
    a detailed description of the methodology, which is divided into specific steps
    across several subsections.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b15e35e421fa4adce13e33dc8f6af37d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of MORepair: The process unfolds in three phases—preparation,
    fine-tuning, and inference for code repair.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Overview]: we introduce MORepair, a novel multi-objective fine-tuning framework
    that empowers open-source LLMs to grasp repair logic and produce high-quality
    patches effectively. Figure [2](#S3.F2 "Figure 2 ‣ III Approach ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") illustrates our approach,
    which unfolds in three phases: training preparation, multi-objective fine-tuning,
    and repair inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the Training Preparation phase, we construct a dataset TutorLLMCode,
    consisting of 1,600 pairs of buggy and repaired code. This preparation includes
    LLM-generated guidance generated by GPT-4 to elucidate the nature of code bugs
    and their fixes (as detailed in Section [III-A](#S3.SS1 "III-A Training Preparation
    ‣ III Approach ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with
    LLMs")). The Multi-objective Fine-tuning phase applies the principles of multi-objective
    learning, targeting two specific learning objectives: (1) generating repaired
    code and (2) producing repaired code with guidance that explains the repair logic.
    Leveraging QLoRA allows for the fine-tuning of a low-rank adapter while freezing
    the original LLM parameters, cutting down the trainable parameters to only 1.84%
    and thus minimizing computational costs. In the Repair Inference phase, the ensemble
    of the pre-trained LLM and the fine-tuned repair adapter generates candidate patches
    for the provided buggy code, whose correctness is validated through the test cases
    from benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Training Preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The initial step in our approach involves preparing the training dataset for
    fine-tuning. We utilize a dataset TutorLLMCode, provided by a company, which includes
    1,600 pairs of buggy codes and repaired codes across 45 distinct programming tasks,
    each accompanied by descriptions of the respective programming tasks. Recognizing
    the effectiveness of few-shot learning [[27](#bib.bib27)] for straightforward
    tasks, researchers explored the innovative use of rationale as intermediate steps,
    known as chain-of-thought (CoT) [[28](#bib.bib28)], to enhance LLMs’ reasoning
    abilities. By employing GPT-4-1106-preview, we generate guidance that clarifies
    the nature of the bug and the logic behind the patch in natural language. The
    prompts used for generating this guidance with GPT-4 are depicted in Figure [3](#S3.F3
    "Figure 3 ‣ III-A Training Preparation ‣ III Approach ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs"). Aimed at benefiting the wider community
    while mitigating future data leakage risks, TutorLLMCode will be publicly available
    through authorized API. Listing [2](#LST2 "Listing 2 ‣ II Motivating Example ‣
    Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") illustrates
    an example of guidance generated by GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '1  This  is  a  programming  task  description  along  with  a  buggy  code:2  {{description}}3  {{buggy  code}}4  This  is  a  repaired  code:5  {{repaired  code}}6  Please  think  step  by  step  and  tell  me  how  to  fix  the  buggy  code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The prompt to generate guidance utilizing GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Multi-objective Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second step of our approach involves fine-tuning LLMs through multi-objective
    learning. Multi-objective learning, a paradigm in machine learning, aims to leverage
    relevant information across multiple tasks simultaneously to enhance the performance
    of each task [[29](#bib.bib29)]. In this context, we propose our framework MORepair,
    which applies multi-objective learning to teach open-source LLMs the intricacies
    of the program repair process during fine-tuning. This approach enables the LLMs
    to generate high-quality patches for buggy code. Specifically, the LLMs are fine-tuned
    with two objectives: (1) generating repaired code and (2) producing repaired code
    accompanied by guidance that clarifies the nature of the bugs and their logic.
    To optimize for these objectives, we calculate separate losses for each, denoted
    as $Loss_{1}$ for producing both the repaired code and its explanatory guidance.
    These losses are then combined using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Loss=Loss_{1}+\lambda Loss_{2}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\lambda$, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'For $Loss_{2}$, which assesses the LLM’s capability to generate relevant explanatory
    guidance alongside repaired code, the loss calculation extends to the entire sequence
    of both code and guidance tokens, using a similar cross-entropy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $n$. The effectiveness of these adjustments is encapsulated in the following
    formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(y_{i}&#124;\mathbf{x},y_{1},...,y_{i-1})^{\prime}=\sigma((X+\Delta
    X)(W+\Delta W))$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\sigma$ signifies the adjustments made via QLoRA. These adjustments are
    strategically implemented to minimize the specified loss functions, directly linking
    QLoRA’s parameter optimization to the overarching objective of enhancing the LLM’s
    performance in generating both code and explanatory guidance. By leveraging QLoRA,
    we fine-tune a mere 1.84% of pre-trained parameters in CodeLlama-13B-instruct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we incorporate NEFTune [[32](#bib.bib32)] to further enhance
    fine-tuned LLMs’ generalization. Noisy Embedding Fine-Tuning (NEFTune), presents
    a novel and effective augmentation technique that aims to prevent over-fitting
    during fine-tuning LLMs. NEFTune introduces random noise to the embedding vectors
    of training data during the forward pass of fine-tuning. Formally, for an embedding
    matrix $X_{emb}\in\mathbb{R}^{B\times L\times d}$ is the embedding dimension,
    NEFTune modifies the embeddings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X^{\prime}_{emb}=X_{emb}+(\frac{\alpha}{\sqrt{Ld}})\epsilon$ |  | (5)
    |'
  prefs: []
  type: TYPE_TB
- en: In this equation, $\epsilon\sim\text{Uniform}(-1,1)$ is a tunable hyper-parameter
    that scales the noise.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Repair Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the final step of our approach, we combine quantized LLM with QLoRA adapters
    to generate repaired codes during inference. The buggy code, represented by instruction
    $x$-th token. This vector $\mathbf{x}$ is fed into fine-tuned LLMs equipped with
    quantization and QLoRA adapters to facilitate efficient and precise program repair
    generation. The computation within each linear layer of the quantized LLM is performed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Y=X\cdot\text{doubleDequant}(c_{1},c_{2},W)+X\cdot L_{1}\cdot L_{2}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\text{doubleDequant}(\cdot)$ are the QLoRA adapter matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Through dequantization, we ensure that computations achieve the necessary precision
    for high-quality output, with each layer’s output feeding into the subsequent
    layer until a final probability distribution over the vocabulary is achieved.
    This distribution, $p(y_{i}|\mathbf{x},y_{1},...,y_{i-1})$. To generate diverse
    and coherent program repairs, we employ a blend of sampling techniques and hyper-parameters,
    setting the temperature and $top_{p}$. This step is crucial because the LLM’s
    output may encompass candidate repaired code and supplementary natural language
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: IV Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we outline the LLMs utilized in this study. Next, we detail the benchmark
    used for evaluation in our experiments. Following this, we explain the metrics
    employed to evaluate the repair capabilities of the fine-tuned LLMs. Lastly, we
    list the research questions we aim to address through this study.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Model Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the generalizability of our approach, it is crucial to experiment
    with LLMs of varying architectures and sizes. Given the significant computational
    resources required for training and deploying large-scale LLMs, as highlighted
    by Chen [[33](#bib.bib33)], we focused on code-targeted LLMs with parameters range
    of 7B to 16B. Table [I](#S4.T1 "TABLE I ‣ IV-A Model Selection ‣ IV Experimental
    Setup ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") presents
    our selected models, chosen based on their popularity (as indicated by downloads
    from HuggingFace) and the diversity of their underlying architectures. These LLMs
    include CodeLlama-13B-instruct [[20](#bib.bib20)], CodeLlama-7B-instruct [[20](#bib.bib20)],
    StarChat-alpha [[21](#bib.bib21)], and Mistral-Instruct-7B [[22](#bib.bib22)],
    allowing us to comprehensively assess our approach’s efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Selected Models'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Base Model | # Params | Downloads^* |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-instruct | CodeLlama | 13B |  46.4k |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B-instruct | CodeLlama |  7B |  59.5k |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | StarCoderBase | 16B |  24.9k |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-Instruct-7B-v0.1 | Mistral-7B-v0.1 |  7B | 773.6k |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Downloads” count reflects the number of times LLMs were downloaded from HuggingFace
    before Feb. 2024.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The selected models showcase a range of innovative features tailored to programming
    tasks. CodeLlama-13B-instruct and CodeLlama-7B-instruct, building on the Llama2
    architecture [[34](#bib.bib34)], offer infilling capabilities and optimized large-batch
    inference, demonstrating the adaptability of the CodeLlama [[20](#bib.bib20)]
    foundation. StarChat-alpha, based on StarCoder [[35](#bib.bib35)], introduces
    advanced pre-training techniques and benefits from expansive code datasets such
    as The Stack [[36](#bib.bib36)], illustrating a novel approach to leveraging data
    diversity for performance gains. Meanwhile, Mistral-Instruct-7B-v0.1, based on
    Mistral [[22](#bib.bib22)], emphasizes advancements in attention mechanisms, highlighting
    the potential for auto-regressive models in processing long sequences efficiently.
    In the following paragraphs, we denote CodeLlama-13B-instruct as CodeLlama-13B,
    CodeLlama-7B-instruct as CodeLlama-7B, and Mistral-Instruct-7B-v0.1 as Mistral-7B.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Evaluation Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To rigorously assess our code generation and translation framework, we leveraged
    HumanEval-X [[26](#bib.bib26)], a multilingual extension of the HumanEval benchmark
    specifically designed for programming languages such as C++, Java, JavaScript,
    Go, and Python. Each of the original 164 Python problems in HumanEval [[23](#bib.bib23)]
    is expanded in HumanEval-X to include equivalent problems in the other four languages,
    culminating in 820 distinct problem-solution pairs. This setup enables comprehensive
    evaluation across code generation and translation tasks in five popular programming
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: To establish a robust C++ program repair benchmark, we adapted the methodology
    from Jiang *et al.* [[4](#bib.bib4)] used for HumanEval-Java, injecting various
    types of bugs into the C++ section of HumanEval-X to form the EvalRepair-C++ benchmark.
    This benchmark encompasses 115 single-line bugs, 143 single-hunk bugs, 21 multi-hunk
    bugs, and 2 multi-function bugs, offering a broad spectrum of defect scenarios
    for evaluation. Recognizing the potential for overfitting due to a limited number
    of test cases, we enriched EvalRepair-C++ with additional test cases from the
    EvalPlus [[37](#bib.bib37)]. This expansion revealed that some original solutions
    in HumanEval-X’s C++ component failed to pass the new, more rigorous test cases.
    We corrected these issues, ensuring the augmented benchmark’s correctness. Consequently,
    the average number of test cases per problem in EvalRepair-C++ surged to 590,
    a significant increase to provide a more accurate assessment of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: HumanEval-Java serves as a critical benchmark for Java program repair, distinct
    in its exclusion from the pre-training datasets of existing LLMs to avoid data
    leakage [[4](#bib.bib4)]. By introducing EvalRepair-Java, which expands HumanEval-Java
    with additional EvalPlus [[37](#bib.bib37)] test cases, the average number of
    test cases per problem has been increased to 583\. This significant augmentation
    of test cases actively mitigates patch overfitting issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Mitigation of Patch Overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | EvalRepair-C++ | EvalRepair-Java |'
  prefs: []
  type: TYPE_TB
- en: '| ① # Original Test Cases | 7 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| ② # Augmented Test Cases | 590 | 583 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B TOP-10 with ① | 67.7 | 73.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B TOP-10 with ② | 58.5 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: As illustrated in Table [II](#S4.T2 "TABLE II ‣ IV-B Evaluation Benchmark ‣
    IV Experimental Setup ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs"), augmenting the test cases leads to a noticeable decline in the TOP-10
    of LLMs such as CodeLlama-13B, which experienced a reduction of 9.2% in EvalRepair-C++
    and 3.7% in EvalRepair-Java. The introduction of a more comprehensive set of test
    cases not only highlights the importance of rigorous evaluation in the development
    of LLMs but also sets a new standard for assessing their performance in program
    repair tasks. These benchmarks, EvalRepair-C++ and EvalRepair-Java, will be made
    publicly accessible via an API , ensuring that the research community can benefit
    from these resources for future explorations and improvements in the field without
    data leakage problem.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To accurately evaluate the effectiveness of LLMs in program repair, this study
    employs two primary metrics: TOP-5 and TOP-10\. The “TOP-k” metric is the scenario
    where, among the top k candidate patches produced by the LLMs, the code is considered
    successfully repaired if any candidates pass all test cases in the benchmark.
    This metrics selection is grounded in the observation by Kochhar *et al.* [[38](#bib.bib38)]
    that most developers tend to abandon automated debugging tools if they fail to
    identify the actual bugs within the first five attempts. Furthermore, Noller *et al.* [[13](#bib.bib13)]
    found that developers are unlikely to consider more than the top-10 ranked patches
    when seeking solutions. Reflecting on these insights and aligning with the findings
    from prior program repair studies [[39](#bib.bib39), [12](#bib.bib12), [13](#bib.bib13),
    [40](#bib.bib40)], our selection of the TOP-5 and TOP-10 metrics is not only justified
    but also crucial for ensuring our evaluation mirrors real-world developers scenarios
    and expectations.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Research Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RQ-1: How effective is fine-tuning with two objectives for program repair?
    We investigate the performance of MORepair’s multi-objective fine-tuning in contrast
    to standard, single-objective fine-tuning on the CodeLlama-13B. This comparative
    analysis is conducted using the EvalRepair-C++ and EvalRepair-Java benchmarks
    to assess not only the effectiveness of MORepair in improving program repair but
    also its ability to generalize across different programming languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ-2: How does model size or type impact repair performance of MORepair? We
    examine MORepair’s performance on LLMs with distinct sizes and architectures,
    including CodeLlama-13B, CodeLlama-7B, StarChat-alpha-16B, and Mistral-7B, on
    EvalRepair-C++ and EvalRepair-Java benchmarks. This study aims to validate MORepair’s
    generalization capability by comparing its fine-tuning effects against standard
    approaches and baseline performance across varying LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ-3: How does MORepair compare against MORepair with human guidance and state-of-the-art
    fine-tuning methods? Through an ablation study, we explore the influence of the
    source of guidance (LLM-generated vs. human-generated) on MORepair’s effectiveness.
    Additionally, we compare MORepair ’s performance to two advanced fine-tuning methodologies,
    Fine-tune-CoT [[25](#bib.bib25)] and RepairLLaMA [[17](#bib.bib17)], across various
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: V Experiments & Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-A Effectiveness of Multi-objective Fine-tuning for Program Repair
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Objective:] This study assesses MORepair’s impact on fine-tuning LLMs for
    program repair, comparing its multi-objective approach against standard single-objective
    fine-tuning and baseline LLMs without fine-tuning. Our investigation centers around
    two sub-questions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ-1.1 How does fine-tuning LLMs with MORepair compare to both standard fine-tuning
    and the baseline LLM in terms of repair performance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ-1.2 Does MORepair exhibit cross-language generalization in program repair
    tasks compared to standard fine-tuning and baseline LLM?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Experimental Design for RQ-1.1]: We fine-tune CodeLlama-13B using both MORepair
    and the standard fine-tuning approach. Here, the baseline represents CodeLlama-13B
    without any fine-tuning, serving as our control for evaluating the impact of fine-tuning.
    Standard fine-tuning refers to fine-tuning CodeLlama-13B to generate repaired
    code without other information, denoted as StdFT. In contrast, MORepair involves
    multi-objective fine-tuning, aiming to enhance LLM’s repair capabilities through
    additional natural language guidance. The comparative analysis is based on TOP-5
    and TOP-10 metrics on the benchmark EvalRepair-C++, detailed in Section [IV-B](#S4.SS2
    "IV-B Evaluation Benchmark ‣ IV Experimental Setup ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Results for RQ-1.1]: Table [III](#S5.T3 "TABLE III ‣ V-A Effectiveness
    of Multi-objective Fine-tuning for Program Repair ‣ V Experiments & Results ‣
    Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") shows MORepair’s
    significant repair performance enhancement on EvalRepair-C++ over both the baseline,
    and StdFT. Against the baseline, MORepair elevates TOP-5 by 20.7%, and TOP-10
    by 11.0%. Compared to StdFT, MORepair maintains its superiority with increments
    of 12.2% in TOP-5, and 5.5% in TOP-10\. These substantial improvements, particularly
    in TOP-5 and TOP-10, more than double the gains of StdFT, showcasing MORepair’s
    profound impact. The success of the MORepair approach underscores the advantage
    of multi-objective fine-tuning in fostering a more nuanced understanding and application
    of repair logic than what is achieved through standard fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Design for RQ-1.2]: To probe MORepair’s and StdFT’s capacity
    for cross-language generalization in program repair, we fine-tuned CodeLlama-13B
    with each method using the TutorLLMCode. Their performances were then evaluated
    on the Java repair benchmark EvalRepair-Java across TOP-5 and TOP-10 metrics,
    offering insights into how these approaches adapt to a language different from
    the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Results for RQ-1.2]: The repair performance presented in Table
    [IV](#S5.T4 "TABLE IV ‣ V-A Effectiveness of Multi-objective Fine-tuning for Program
    Repair ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program
    Repair with LLMs") for the EvalRepair-Java benchmark detail how both StdFT and
    MORepair extend their capabilities into a cross-language scenario. StdFT enhances
    the TOP-10 by 6.8% over the baseline (CodeLlama-13B), while MORepair further improves
    upon this, exhibiting an additional 1.2% increase in TOP-10 over StdFT. These
    enhancements validate the cross-language generalization capability of both fine-tuning
    approaches, with MORepair showcasing superior performance in adapting to Java,
    which is a shift from the training dataset’s programming language. Notably, MORepair
    achieves a 77.9% TOP-10, marking an 8.0% increase over the baseline. This significant
    improvement underscores MORepair’s effectiveness in cross-language repair scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Fine-tune CodeLlama-13B with StdFT and MORepair vs GPT-4 on EvalRepair-C++.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 97.6 | 98.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | 40.9 | 58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-StdFT | 49.4 (+ 8.5) | 64.0 (+ 5.5) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Fine-tune CodeLlama-13B with StdFT and MORepair vs GPT-4 on EvalRepair-Java.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 85.9 | 89.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | 54.0 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-StdFT | 62.0 (+ 8.0) | 76.7 (+ 6.8) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  prefs: []
  type: TYPE_TB
- en: Reflecting on the examples illustrated in Section [II](#S2 "II Motivating Example
    ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs"), the evaluation
    results of buggy codes “separate_paren_groups.cpp” in EvalRepair-C++ and “SEPARATE_PAREN_GROUPS.java”
    in EvalRepair-Java demonstrates the distinct effectiveness of MORepair and StdFT.
    MORepair successfully repairs these buggy codes within Top-10 attempts, while
    StdFT fails to accomplish these repairs. This result supports that cross-language
    generalization enhancement of MORepair is attributed to natural language guidance
    from GPT-4, enabling MORepair to learn language-independent repair logic.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we include GPT-4, state-of-the-art closed-source LLM, as a benchmark
    for upper limits of repair performance, as illustrated in Tables [III](#S5.T3
    "TABLE III ‣ V-A Effectiveness of Multi-objective Fine-tuning for Program Repair
    ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs") and [IV](#S5.T4 "TABLE IV ‣ V-A Effectiveness of Multi-objective Fine-tuning
    for Program Repair ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs"). The result shows that MORepair narrows the
    performance gap between CodeLlama-13B and GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S5.SS1.p9.pic1" class="ltx_picture" height="125.33" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,125.33) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 9.26 6.7)"><foreignobject width="582.07" height="111.93"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[RQ-1]
    Findings: (1) Fine-tuning with MORepair outperforms CodeLlama-13B baseline significantly
    in repair performance. The improvements in TOP-10 for EvalRepair-C++ and EvalRepair-Java
    are 11.0% and 8.0%, respectively, showcasing superior repair capabilities. (2)
    Against StdFT, MORepair shows repair performance gains with increases in TOP-5
    of 12.2% for EvalRepair-C++ and 7.3% for EvalRepair-Java, indicating generalization
    across programming languages. Insights: Our approach MORepair highlights multi-objective
    learning’s impact on automated program repair, proving its ability to enhance
    repair tasks.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Impact of Size or Type for Fine-tuning LLMs on Code Repair Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Objective]: To investigate RQ-2, we assess the impact of fine-tuning with
    MORepair on LLMs of varying sizes and architectures in terms of their code repair
    capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Design]: To examine the generalization of the MORepair approach
    across LLMs with different sizes and architectures, we selected CodeLlama-7B,
    StarChat-alpha (which has 16B parameters), and Mistral-7B as our base LLMs. These
    LLMs represent a diverse range of architectures, and CodeLlama-7B differs in size
    from the CodeLlama-13B assessed in RQ-1\. We fine-tune these LLMs using either
    standard fine-tuning (StdFT) or MORepair, then evaluate their performance on two
    benchmarks: EvalRepair-C++ and EvalRepair-Java.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Impact of model sizes or architectures in the effectiveness of fine-tuning
    on EvalRepair-C++.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | 40.9 | 58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-StdFT | 49.4 (+ 8.5) | 64.0 (+ 5.5) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B | 46.3 | 59.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B-StdFT | 50.0 (+ 3.7) | 61.6 (+ 2.5) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B-MORepair | 56.7 (+10.4) | 62.8 (+ 3.7) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | 50.0 | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-StdFT | 43.3 (- 6.7) | 58.5 (- 3.7) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-MORepair | 52.4 (+ 2.4) | 65.9 (+ 3.7) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | 32.3 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-StdFT | 39.0 (+ 6.7) | 46.3 (- 0.7) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-MORepair | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
  prefs: []
  type: TYPE_TB
- en: †
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Experimental Results]: Table [V](#S5.T5 "TABLE V ‣ V-B Impact of Size or Type
    for Fine-tuning LLMs on Code Repair Performance ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") outlines the TOP-5 and TOP-10
    repair performance metrics for baseline, StdFT, and MORepair across four LLMs
    on EvalRepair-C++. Notably, StdFT doesn’t consistently improve repair metrics,
    failing to surpass the repair performance of baseline on several base LLMs, such
    as StarChat-alpha. Conversely, MORepair consistently enhances performance across
    all metrics and LLMs, with a maximum 11.0% TOP-10 improvement over baseline and
    a maximum 7.4% TOP-10 over StdFT evaluated on EvalRepair-C++. This suggests superior
    generalizability of multi-objective learning across different LLMs for code repair.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Impact of model sizes or architectures on the effectiveness of fine-tuning
    on EvalRepair-Java.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | 54.0 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-StdFT | 62.0 (+ 8.0) | 76.7 (+ 6.8) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B-MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B | 49.7 | 62.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B-StdFT | 49.1 (- 0.6) | 60.7 (- 1.3) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B-MORepair | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | 43.6 | 60.7 |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-StdFT | 47.9 (+ 4.3) | 56.4 (- 4.3) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-MORepair | 56.4 (+12.8) | 66.3 (+ 5.6) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | 33.7 | 52.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-StdFT | 42.3 (+ 8.6) | 54.6 (+ 2.5) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-MORepair | 45.4 (+11.7) | 58.3 (+ 6.2) |'
  prefs: []
  type: TYPE_TB
- en: †
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table [VI](#S5.T6 "TABLE VI ‣ V-B Impact of Size or Type for Fine-tuning LLMs
    on Code Repair Performance ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") presents the TOP-5 and TOP-10 metrics
    for baseline, StdFT, and MORepair on the EvalRepair-Java benchmark across four
    LLMs. Unlike the results from EvalRepair-C++ in Table [V](#S5.T5 "TABLE V ‣ V-B
    Impact of Size or Type for Fine-tuning LLMs on Code Repair Performance ‣ V Experiments
    & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs"),
    CodeLlama-7B-StdFT under-performs on EvalRepair-Java, revealing StdFT’s inconsistent
    cross-language generalization. Similarly, StarChat-StdFT’s decline mirrors its
    performance on EvalRepair-C++, indicating StdFT’s limited adaptability across
    LLMs of different architectures. Conversely, MORepair demonstrates robust improvements
    over baseline and StdFT, with an increment of 5.5%-8.0% TOP-10 over baseline and
    1.2%-9.9% TOP-10 over StdFT evaluated on EvalRepair-Java. Despite StdFT showcasing
    a decrease in repair performance compared to the baseline of four LLMs, MORepair
    consistently improves over baseline in cross-language scenarios. This underscores
    the effectiveness of MORepair leveraging multi-objective learning and LLM-generated
    natural language guidance in enhancing repair capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S5.SS2.p5.pic1" class="ltx_picture" height="141.94" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,141.94) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 9.26 6.7)"><foreignobject width="582.07" height="128.53"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[RQ-2]
    Findings: MORepair consistently elevates repair performance across LLMs with varied
    sizes and architectures. Notably, it achieves a maximum 11.0% improvement in TOP-10
    scores over the baseline and a maximum 7.4% improvement over StdFT on EvalRepair-C++.
    On EvalRepair-Java, MORepair showcases 8.0% TOP-10 improvement over the baseline
    and 9.9% TOP-10 enhancement over StdFT, further highlighting its superior generalization.
    Insights: These findings underscore the versatility of LLMs in understanding and
    applying language-independent programming logic through strategies such as LLM-generated
    guidance and multi-objective learning, paving the way for advancements in program
    repair.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against
    State-of-the-Art Fine-tuning Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Objective]: This section is dedicated to examining the influence of source
    of guidance on MORepair’s repair capabilities and assessing MORepair’s comparative
    performance against advanced fine-tuning techniques. Specifically, we address
    the following sub-questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ-3.1: How does the code repair performance of MORepair differ when fine-tuned
    with LLM-generated guidance compared to human-generated guidance?'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ-3.2: How does the performance improvement of fine-tuning with MORepair against
    that achieved with existing methodologies, such as Fine-tune-CoT and RepairLLaMA?'
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Design for RQ-3.1]: To evaluate the impact of the source of guidance
    on MORepair’s code repair capabilities, we expanded our training dataset TutorLLMCode
    with human-generated instructions for each pair of buggy and corrected code, as
    illustrated in Listing [1](#LST1 "Listing 1 ‣ II Motivating Example ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs"). Human-generated guidance
    provides explicit repair strategies, contrasting with the LLM-generated advice,
    and serves as a new training dataset for MORepair. We then evaluate their code
    repair performance employing the EvalRepair-C++ and EvalRepair-Java benchmarks.
    Finally, we compare the LLMs fine-tuned with human-generated guidance against
    those fine-tuned with LLM-generated guidance. This comparison aims to identify
    which source of guidance (human-generated versus LLM-generated) more effectively
    enhances the fine-tuning process and results in superior code repair performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Impact of different source of guidance in the effectiveness of MORepair
    on EvalRepair-C++.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Guidance | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | Human | 52.4 (+11.5) | 66.5 (+ 8.0) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 61.6 (+20.7) | 69.5 (+11.0) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B | Human | 40.9 (- 5.4) | 54.9 (- 4.2) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 56.7 (+10.4) | 62.8 (+ 3.7) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | Human | 48.2 (- 1.8) | 59.8 (- 2.4) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 52.4 (+ 2.4) | 65.9 (+ 3.7) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | Human | 35.4 (+ 3.1) | 45.7 (- 1.3) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
  prefs: []
  type: TYPE_TB
- en: †
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE VIII: Impact of different source of guidance in the effectiveness of
    MORepair on EvalRepair-Java.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Guidance | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | Human | 63.2 (+ 9.2) | 76.1 (+ 6.2) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B | Human | 51.5 (+ 1.8) | 62.0 (+ 0.0) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | Human | 51.5 (+ 7.9) | 63.2 (+ 2.5) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 56.4 (+12.8) | 66.3 (+ 5.6) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | Human | 44.2 (+10.5) | 53.4 (+ 1.3) |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | 45.4 (+11.7) | 58.3 (+ 6.2) |'
  prefs: []
  type: TYPE_TB
- en: †
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Experimental Results for RQ-3.1]: The impact of different sources of guidance
    on the code repair capabilities of MORepair is quantitatively analyzed in this
    experiment, and results are presented in Tables [VII](#S5.T7 "TABLE VII ‣ V-C
    Evaluating the Impact of Guidance Sources and Comparing MORepair against State-of-the-Art
    Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs") for EvalRepair-C++, and [VIII](#S5.T8 "TABLE
    VIII ‣ V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against
    State-of-the-Art Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") for EvalRepair-Java. These
    tables illustrate that LLM-generated guidance significantly surpasses human-generated
    guidance in enhancing code repair performance. Employing LLM-generated guidance
    resulted in TOP-10 improvements over their human-generated counterparts of 3.0%
    to 7.9% for EvalRepair-C++ and 1.2% to 5.5% for EvalRepair-Java. Furthermore,
    Listing [2](#LST2 "Listing 2 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") and [1](#LST1 "Listing 1 ‣ II Motivating
    Example ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs")
    provide illustrative examples of the guidance produced by LLMs and humans, respectively.
    These examples demonstrate how LLM-generated guidance tends to be more structured
    and insightful, which likely contributes to the observed improvements in code
    repair tasks over human-generated guidance.'
  prefs: []
  type: TYPE_NORMAL
- en: A detailed analysis highlights significant variance in the effectiveness of
    human-generated guidance across different model sizes. For example, by leveraging
    human-generated guidance, CodeLlama-13B achieves an 8.0% and 6.2% TOP-10 increment
    compared to the baseline on EvalRepair-C++ and EvalRepair-Java, respectively.
    In contrast, CodeLlama with another size 7B exhibits a 4.2% decrease of TOP-10
    on EvalRepair-C++. This variation emphasizes the superior text comprehension and
    reasoning capabilities of larger LLMs, such as Llama2-13B, over smaller models
    like Llama2-7B[[34](#bib.bib34)], underscoring the significance of model size
    in effectively utilizing human-generated guidance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Design for RQ-3.2]: To evaluate the effectiveness of MORepair,
    we compare it with two advanced fine-tuning approaches for code repair tasks:
    RepairLLaMA [[17](#bib.bib17)] and Fine-tune-CoT [[25](#bib.bib25)]. RepairLLaMA
    fine-tunes LLMs using code representation and fault localization information to
    repair buggy codes. This approach requires manually annotated perfect fault location
    information before repairing the buggy code, contrasting with our MORepair, which
    directly repairs buggy code without additional manual costs. Since Silva *et al.*
    only released the code and the checkpoint of fine-tuned CodeLlama-7B, and they
    have not released the training dataset, thus we can only reproduce their results
    based on CodeLlama-7B. To provide the necessary input information for the inference
    of RepairLLaMA, we manually annotated the fault localization information of EvalRepair-C++
    and EvalRepair-Java. Fine-tune-CoT is a method that utilizes the chain-of-thought
    data generated by large language models to fine-tune small models, thereby transferring
    complex reasoning capabilities from teacher models to student models. We implement
    the Fine-tune-CoT by concatenating the repaired code with guidance information
    to serve as the target objective for fine-tuning. We fine-tune all four selected
    LLMs with Fine-tune-CoT, evaluating their repair performance on EvalRepair-C++
    and EvalRepair-Java, compared to MORepair.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Experimental Results for RQ-3.2]: The results, as detailed in Tables [IX](#S5.T9
    "TABLE IX ‣ V-C Evaluating the Impact of Guidance Sources and Comparing MORepair
    against State-of-the-Art Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") and [X](#S5.T10 "TABLE X ‣
    V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against State-of-the-Art
    Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs"), clearly demonstrate that MORepair surpasses
    both Fine-tune-CoT and RepairLLaMA across TOP-5 and TOP-10 metrics on EvalRepair-C++
    and EvalRepair-Java benchmarks. This establishes the robustness of MORepair in
    enhancing code repair tasks. It is noteworthy that, when evaluating the repair
    performance of RepairLLaMA, benchmarks comprising manually annotated bug localization
    information, represent more information than what MORepair received. Despite this,
    MORepair demonstrates a more substantial improvement in repair performance than
    RepairLLaMA, which failed to achieve a TOP-10 enhancement in both benchmarks.
    This indicates that LLM-based program repair can achieve better repair performance
    without first conducting bug localization and then proceeding to patch generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, Fine-tune-CoT demonstrates mixed results. It increased TOP-10 by
    9.8% on EvalRepair-C++ for CodeLlama-13B, outperforming the StdFT by 3.0% of TOP-10\.
    However, Fine-tune-CoT gains only a 1.3% improvement in TOP-10 evaluated on EvalRepair-Java
    and did not enhance TOP-10 over the other baselines, indicating the lack of cross-language
    generalization. These findings underscore the effectiveness of MORepair in code
    repair compared to state-of-the-art fine-tuning approaches like RepairLLaMA and
    Fine-tune-CoT.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S5.SS3.p10.pic1" class="ltx_picture" height="158.54" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,158.54) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 9.26 6.7)"><foreignobject width="582.07" height="145.13"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">[RQ-3]
    Findings: (1) LLM-generated guidance is more effective in enhancing the reasoning
    capabilities during fine-tuning LLMs than human-generated guidance, highlighting
    its key role in MORepair. (2) MORepair outperforms Fine-tune-CoT and RepairLLaMA
    on EvalRepair-C++ and EvalRepair-Java, even when RepairLLaMA is provided with
    perfect fault location information. Insights: (1) LLM-generated guidance signifies
    that the previously manual task of annotating datasets with rationale can now
    be automatically generated by LLMs, leading to liberation from labor constraints.
    (2) The outperforming results of the end-to-end fine-tuning approach MORepair
    confirm that LLM-based program repair can perform well without the need to identify
    fault location before generating patches.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Performance of LLMs fine-tuned with Fine-tune-CoT, RepairLLaMA, and
    MORepair on EvalRepair-C++.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Approach | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | Fine-tune-CoT | 56.7 (+15.8) | 68.3 (+ 9.8) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B | Fine-tune-CoT | 42.7 (- 3.6) | 55.5 (- 3.6) |'
  prefs: []
  type: TYPE_TB
- en: '| RepairLLaMA^* | 52.4 (+ 6.0) | 55.5 (- 3.6) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 56.7 (+10.4) | 62.8 (+ 3.7) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | Fine-tune-CoT | 37.8 (-12.2) | 43.9 (-18.3) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 52.4 (+ 2.4) | 65.9 (+ 3.9) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | Fine-tune-CoT | 33.5 (+ 1.2) | 37.8 (-14.3) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RepairLLaMA only has the version of CodeLlama-7B.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: †
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE X: Performance of LLMs fine-tuned with Fine-tune-CoT, RepairLLaMA, and
    MORepair on EvalRepair-Java.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Approach | TOP-5 | TOP-10 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-13B | Fine-tune-CoT | 59.5 (+ 5.5) | 71.2 (+ 1.3) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  prefs: []
  type: TYPE_TB
- en: '| CodeLlama-7B | Fine-tune-CoT | 45.4 (- 4.3) | 57.7 (- 4.3) |'
  prefs: []
  type: TYPE_TB
- en: '| RepairLLaMA^* | 52.1 (+ 2.4) | 60.1 (- 1.9) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
  prefs: []
  type: TYPE_TB
- en: '| StarChat-alpha | Fine-tune-CoT | 41.7 (- 1.9) | 54.6 (- 6.1) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 56.4 (+12.8) | 66.3 (+ 5.6) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | Fine-tune-CoT | 36.8 (+ 3.1) | 46.0 (- 6.1) |'
  prefs: []
  type: TYPE_TB
- en: '| MORepair | 45.4 (+11.7) | 58.3 (+ 6.2) |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RepairLLaMA only has the version of CodeLlama-7B.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: †
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VI Threats to Validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Threats to Internal Validity. The choice of base LLMs may impact the experimental
    conclusions. To minimize potential bias, we have conducted experiments using four
    LLMs varying different sizes and architectures, including CodeLlama-13B-instruct,
    CodeLlama-7B-instruct, StarChat-alpha, and Mistral-Instruct-7B. By diversifying
    the LLMs selection, we aim to ensure that our findings are not limited to a specific
    LLM type or scale.
  prefs: []
  type: TYPE_NORMAL
- en: Threats to External Validity. Insufficient test cases in the evaluation benchmarks
    may lead to patch over-fitting problems, where LLMs successfully pass the limited
    test cases without genuinely understanding or correcting the underlying logical
    errors. To address this issue, we have integrated test cases from EvalPlus [[37](#bib.bib37)]
    to enhance the diversity of the test cases in our benchmark EvalRepair-C++ and
    EvalRepair-Java, detailed in Section [IV-B](#S4.SS2 "IV-B Evaluation Benchmark
    ‣ IV Experimental Setup ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs"). This helps us to assess the LLMs’ repair performance in a more realistic
    setting and improves the external validity of our conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Threats to Construct Validity. The inherent randomness in generating outputs
    by LLMs could undermine the validity of experimental conclusions. To address this
    issue, we utilize LLMs to produce outputs ten times, subsequently calculating
    the TOP-5 and TOP-10 metrics. By considering multiple rounds of generated outputs,
    we aim to minimize the impact of randomness on our findings and ensure that the
    conclusions are based on a more stable and representative set of results.
  prefs: []
  type: TYPE_NORMAL
- en: VII Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, code LLMs [[41](#bib.bib41), [42](#bib.bib42), [35](#bib.bib35),
    [22](#bib.bib22), [20](#bib.bib20)] have made significant strides in advancing
    the field of code-related tasks, especially in program repair. Among the various
    methodologies employed, fine-tuning has become a crucial technique for adapting
    LLMs to specific domain applications, demonstrating significant improvements in
    program repair tasks [[15](#bib.bib15), [16](#bib.bib16), [4](#bib.bib4), [18](#bib.bib18),
    [43](#bib.bib43), [14](#bib.bib14)]. TFix, proposed by Berabi *et al.* [[14](#bib.bib14)],
    leveraging T5 [[19](#bib.bib19)] fine-tuned with GitHub commits to surpass existing
    learning-based repair approaches for JavaScript programs. Lajkó *et al.* [[15](#bib.bib15)]
    fine-tuned GPT-2 [[44](#bib.bib44)] with 16k samples of JavaScript codes, evaluated
    both the pre-trained (baseline) and the fine-tuned GPT-2 model on a dataset of
    18,736 created from GitHub commits, with 16,863 samples used for fine-tuning and
    1,559 samples for testing, achieving a 15.5% improvement in TOP-10 accuracy on
    a JavaScript benchmark. The results showed that while the pre-trained model could
    generate syntactically and semantically correct source code, fine-tuning increased
    the number of correctly repaired programs from 27 to 269, significantly boosting
    its performance. Jiang *et al.* [[4](#bib.bib4)] studied the impact of LLMs on
    automated program repair (APR) and evaluated fine-tuning LLMs on four APR benchmarks,
    including a new benchmark HumanEval-Java to avoid the data leakage issue. Experiments
    showed that the best LLMs fixed 72% more bugs in total on the four benchmarks
    than the best deep learning-based APR technique, and fine-tuning further improved
    LLMs’ fixing capabilities, enabling them to fix 46% to 164% more bugs than the
    best deep learning APR technique. Huang *et al.* [[16](#bib.bib16)] found that
    UniXcoder [[45](#bib.bib45)], an LLM smaller than CodeT5 [[46](#bib.bib46)], could
    achieve superior repair performance through fine-tuning, challenging the notion
    that larger models always perform better. RepairLLaMA [[17](#bib.bib17)] presented
    a novel fine-tuning approach to automated program repair by combining specialized
    code representations with efficient fine-tuning techniques. This approach allowed
    RepairLLaMA to effectively adapt LLMs for the program repair task, significantly
    surpassing CodeLlama-13B [[20](#bib.bib20)] baseline on multiple Java benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Diverging from prior fine-tuning practices that utilize LLMs for program repair,
    which predominantly concentrated on enriching the training datasets with standard
    single-objective fine-tuning approaches [[15](#bib.bib15), [4](#bib.bib4), [16](#bib.bib16),
    [14](#bib.bib14)], our approach MORepair leverage multi-objective learning and
    LLM-generated guidance during fine-tuning, consistently achieving superior repair
    performance compared to standard single-objective fine-tuning approach, and state-of-the-art
    fine-tuning approaches RepairLLaMA and Fine-tune-CoT.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper introduces a novel multi-objective fine-tuning framework MORepair
    that empowers open-source LLMs to effectively learn repair logic and generate
    high-quality patches for program repair tasks. Our approach employs a multi-objective
    learning strategy, simultaneously optimizing for generating repaired code and
    producing corresponding explanatory guidance during fine-tuning. By employing
    multi-objective learning and explanatory guidance on four LLMs with different
    architectures and sizes, MORepair outperforms StdFT and baseline models, achieving
    up to 11.0% and 8.0% improvements over baseline in TOP-10 on EvalRepair-C++ and
    EvalRepair-Java benchmarks, respectively. These findings highlight MORepair’s
    robustness and adaptability across different programming languages and various
    LLM architectures and sizes. Furthermore, MORepair surpasses existing state-of-the-art
    fine-tuning methods such as Fine-tune-CoT and RepairLLaMA across four LLMs in
    both benchmarks. Our ablation study emphasizes the significant impact of multi-objective
    learning and the distinct advantages of LLM-generated guidance over human-generated
    guidance in enhancing program repair performance. Our work highlights the significance
    of employing a multi-objective learning strategy and LLM-generated natural language
    guidance in advancing code repair tasks, paving the way for more intelligent and
    efficient automated program repair paradigms in the future.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing 162 out of
    337 bugs for $0.42 each using chatgpt,” *arXiv preprint arXiv:2304.00385*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Z. Fan, X. Gao, A. Roychoudhury, and S. H. Tan, “Improving automatically
    generated code from codex via automated program repair,” *arXiv preprint arXiv:2205.10583*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of code language models
    on automated program repair,” in *Proceedings of the 45th International Conference
    on Software Engineering*, ser. ICSE ’23.   IEEE Press, 2023, p. 1430–1442\. [Online].
    Available: https://doi.org/10.1109/ICSE48619.2023.00125'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Cao, M. Li, M. Wen, and S.-c. Cheung, “A study on prompt design, advantages
    and limitations of chatgpt for deep learning program repair,” *arXiv preprint
    arXiv:2304.08191*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Phung, J. Cambronero, S. Gulwani, T. Kohn, R. Majumdar, A. Singla, and
    G. Soares, “Generating high-precision feedback for programming syntax errors using
    large language models,” *arXiv preprint arXiv:2302.04662*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Peng, S. Gao, C. Gao, Y. Huo, and M. Lyu, “Domain knowledge matters:
    Improving prompts with fix templates for repairing python type errors,” in *Proceedings
    of the 46th IEEE/ACM International Conference on Software Engineering*, 2024,
    pp. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] H. Joshi, J. C. Sanchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radiček,
    “Repair is nearly generation: Multilingual program repair with llms,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 37, no. 4, 2023, pp.
    5131–5140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Feng and C. Chen, “Prompting is all you need: Automated android bug
    replay with large language models,” in *Proceedings of the 46th IEEE/ACM International
    Conference on Software Engineering*, 2024, pp. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. H. Tan, “Automated
    repair of programs from large language models,” in *2023 IEEE/ACM 45th International
    Conference on Software Engineering (ICSE)*.   IEEE, 2023, pp. 1469–1481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. A. Prenner, H. Babii, and R. Robbes, “Can openai’s codex fix bugs?
    an evaluation on quixbugs,” in *Proceedings of the Third International Workshop
    on Automated Program Repair*, 2022, pp. 69–75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Fu, C. Tantithamthavorn, T. Le, V. Nguyen, and D. Phung, “Vulrepair:
    a t5-based automated software vulnerability repair,” in *Proceedings of the 30th
    ACM Joint European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering*, 2022, pp. 935–947.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Noller, R. Shariffdeen, X. Gao, and A. Roychoudhury, “Trust enhancement
    issues in program repair,” in *Proceedings of the 44th International Conference
    on Software Engineering*, 2022, pp. 2228–2240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] B. Berabi, J. He, V. Raychev, and M. Vechev, “Tfix: Learning to fix coding
    errors with a text-to-text transformer,” in *International Conference on Machine
    Learning*.   PMLR, 2021, pp. 780–791.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Lajkó, D. Horváth, V. Csuvik, and L. Vidács, “Fine-tuning gpt-2 to
    patch programs, is it worth it?” in *International Conference on Computational
    Science and Its Applications*.   Springer, 2022, pp. 79–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li, and Y. Zhang, “An
    empirical study on fine-tuning large language models of code for automated program
    repair,” in *2023 38th IEEE/ACM International Conference on Automated Software
    Engineering (ASE)*.   IEEE, 2023, pp. 1162–1174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Silva, S. Fang, and M. Monperrus, “Repairllama: Efficient representations
    and fine-tuned adapters for program repair,” *arXiv preprint arXiv:2312.15698*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Jiang, T. Lutellier, and L. Tan, “Cure: Code-aware neural machine translation
    for automatic program repair,” in *2021 IEEE/ACM 43rd International Conference
    on Software Engineering (ICSE)*.   IEEE, 2021, pp. 1161–1173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified
    text-to-text transformer,” *Journal of Machine Learning Research*, vol. 21, no.
    140, pp. 1–67, 2020\. [Online]. Available: http://jmlr.org/papers/v21/20-074.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,
    J. Liu, T. Remez, J. Rapin *et al.*, “Code llama: Open foundation models for code,”
    *arXiv preprint arXiv:2308.12950*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] L. Tunstall, N. Lambert, N. Rajani, E. Beeching, T. Le Scao, L. von Werra,
    S. Han, P. Schmid, and A. Rush, “Creating a coding assistant with starcoder,”
    *Hugging Face Blog*, 2023, https://huggingface.co/blog/starchat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux,
    P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, “Mistral
    7b,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Yang, A. Zhikhartsev, Y. Liu, and L. Tan, “Better test cases for better
    automated program repair,” in *Proceedings of the 2017 11th joint meeting on foundations
    of software engineering*, 2017, pp. 831–841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] N. Ho, L. Schmid, and S.-Y. Yun, “Large language models are reasoning
    teachers,” *arXiv preprint arXiv:2212.10071*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen,
    A. Wang, Y. Li, T. Su, Z. Yang, and J. Tang, “Codegeex: A pre-trained model for
    code generation with multilingual evaluations on humaneval-x,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Z. Shen, Z. Liu, J. Qin, M. Savvides, and K.-T. Cheng, “Partial is better
    than all: Revisiting fine-tuning strategy for few-shot learning,” in *Proceedings
    of the AAAI conference on artificial intelligence*, vol. 35, no. 11, 2021, pp.
    9594–9602.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Zhang and Q. Yang, “An overview of multi-task learning,” *National
    Science Review*, vol. 5, no. 1, pp. 30–43, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” *arXiv preprint
    arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient
    finetuning of quantized llms,” *Advances in Neural Information Processing Systems*,
    vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] N. Jain, P.-y. Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli,
    B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha *et al.*, “Neftune:
    Noisy embeddings improve instruction finetuning,” *arXiv preprint arXiv:2310.05914*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Y. Chen, D. Hazarika, M. Namazifar, Y. Liu, D. Jin, and D. Hakkani-Tur,
    “Empowering parameter-efficient transfer learning by recognizing the kernel structure
    in self-attention,” *arXiv preprint arXiv:2205.03720*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,
    C. Akiki, J. Li, J. Chim *et al.*, “Starcoder: may the source be with you!” *arXiv
    preprint arXiv:2305.06161*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, and M. Brundage, “A hazard
    analysis framework for code synthesis large language models,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, “Is your code generated by chatgpt
    really correct? rigorous evaluation of large language models for code generation,”
    *arXiv preprint arXiv:2305.01210*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] P. S. Kochhar, X. Xia, D. Lo, and S. Li, “Practitioners’ expectations
    on automated fault localization,” in *Proceedings of the 25th international symposium
    on software testing and analysis*, 2016, pp. 165–176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] F. Huq, M. Hasan, M. M. A. Haque, S. Mahbub, A. Iqbal, and T. Ahmed, “Review4repair:
    Code review aided automatic program repairing,” *Information and Software Technology*,
    vol. 143, p. 106765, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] C.-P. Wong, P. Santiesteban, C. Kästner, and C. Le Goues, “Varfix: balancing
    edit expressiveness and search effectiveness in automated program repair,” in
    *Proceedings of the 29th ACM joint meeting on European software engineering conference
    and symposium on the foundations of software engineering*, 2021, pp. 354–366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    and C. Xiong, “Codegen: An open large language model for code with multi-turn
    program synthesis,” *arXiv preprint arXiv:2203.13474*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
    W.-t. Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A generative model for code
    infilling and synthesis,” *arXiv preprint arXiv:2204.05999*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy,
    “Inferfix: End-to-end program repair with llms,” *arXiv preprint arXiv:2303.07263*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, “Unixcoder: Unified
    cross-modal pre-training for code representation,” *arXiv preprint arXiv:2203.03850*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware unified
    pre-trained encoder-decoder models for code understanding and generation,” *arXiv
    preprint arXiv:2109.00859*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
