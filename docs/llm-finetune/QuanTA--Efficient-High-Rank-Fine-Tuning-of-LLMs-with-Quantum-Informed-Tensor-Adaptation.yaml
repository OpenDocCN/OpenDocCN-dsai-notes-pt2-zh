- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:36:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00132](https://ar5iv.labs.arxiv.org/html/2406.00132)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  prefs: []
  type: TYPE_NORMAL
- en: Zhuo Chen^(12)  Rumen Dangovski^(13)  Charlotte Loh^(13)
  prefs: []
  type: TYPE_NORMAL
- en: Owen Dugan²  Di Luo^(124)^∗  Marin Soljačić^(12)
  prefs: []
  type: TYPE_NORMAL
- en: ¹NSF AI Institute for Artificial Intelligence and Fundamental Interactions
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Physics, Massachusetts Institute of Technology
  prefs: []
  type: TYPE_NORMAL
- en: ³Department of Department of EECS, Massachusetts Institute of Technology
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Department of Physics, Harvard University
  prefs: []
  type: TYPE_NORMAL
- en: '{chenzhuo,rumenrd,cloh,odugan,diluo,soljacic}@mit.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We propose Quantum-informed Tensor Adaptation (QuanTA), a novel, easy-to-implement,
    fine-tuning method with no inference overhead for large-scale pre-trained language
    models. By leveraging quantum-inspired methods derived from quantum circuit structures,
    QuanTA enables efficient high-rank fine-tuning, surpassing the limitations of
    Low-Rank Adaptation (LoRA)—low-rank approximation may fail for complicated downstream
    tasks. Our approach is theoretically supported by the universality theorem and
    the rank representation theorem to achieve efficient high-rank adaptations. Experiments
    demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic
    reasoning, and scalability compared to traditional methods. Furthermore, QuanTA
    shows superior performance with fewer trainable parameters compared to other approaches
    and can be designed to integrate with existing fine-tuning algorithms for further
    improvement, providing a scalable and efficient solution for fine-tuning large
    language models and advancing state-of-the-art in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-trainied large language models (LLMs) have revolutionized natural language
    processing (NLP) by achieving state-of-the-art performance across various tasks
    [[1](#bib.bib1), [2](#bib.bib2)]. Traditionally, these models are adapted to specific
    downstream applications via full fine-tuning, where all model parameters are retrained.
    However, as model sizes increase, the computational cost and memory requirements
    for full fine-tuning become prohibitive, especially with models like GPT-3 [[3](#bib.bib3)]
    with 175 billion parameters, Mixtral [[4](#bib.bib4)] with $8\times 22$ billion
    parameters, and more recently the LLaMA series [[5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7)], containing soon up to 400 billion parameters [[8](#bib.bib8)].
    These constraints have spurred the development of parameter-efficient fine-tuning
    (PEFT) methods, which aim to adapt LLMs by updating only a small subset of parameters,
    thereby reducing resource demands [[9](#bib.bib9), [10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: Among PEFT methods, Low-Rank Adaptation (LoRA) [[10](#bib.bib10)] has gained
    prominence due to its simplicity and effectiveness. LoRA fine-tunes LLMs by introducing
    low-rank matrices into the pre-trained model’s weight updates, pragmatically reducing
    the number of trainable parameters while maintaining performance close to full
    fine-tuning in many tasks. However, LoRA’s reliance on low-rank approximations
    can sometimes lead to a performance gap compared to full fine-tuning, particularly
    for complex tasks, as it may not capture all necessary task-specific adaptations
    [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: Recently, there have been many attempts to generalize LoRA using tensor-based
    methods [[12](#bib.bib12), [13](#bib.bib13)]. However, these approaches primarily
    focus on reducing the number of trainable parameters within the low-rank framework
    yet they continue to face the same limitations of restricted representation. In
    Quantum mechanics, quantum circuit provides a natural realization of unitary matrix
    which is full rank, motivating us to develop new schemes for high-rank fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/305dde22abc6325da8b0861076435b00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes
    the weight matrix update as a outer product of two low-rank matrices, limiting
    its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate
    on specific axes of the (reshaped) input, enabling high-rank parameterization.
    Supported by the universality theorem and rank representation theorem, QuanTA
    can represent arbitrary matrices effectively, allowing it to achieve performance
    comparable to or sometimes even better than full fine-tuning, with only a fraction
    of the parameters. Note: the performance graph is a conceptual illustration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by these advancements, we propose Quantum-informed Tensor Adaptation
    (QuanTA) ¹¹1[https://github.com/quanta-fine-tuning/quanta](https://github.com/quanta-fine-tuning/quanta)
    a novel, easy-to-implement, fine-tuning method with no inference overhead inspired
    by quantum circuits (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QuanTA: Efficient
    High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")). QuanTA
    enables efficient high-rank adaptations by utilizing tensor operations analogous
    to those in quantum circuits, addressing the limitations inherent in low-rank
    methods like LoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We introduce QuanTA, a novel, easy-to-implement, PEFT method with no inference
    overhead inspired by quantum circuits, enabling efficient high-rank fine-tuning
    without additional inference latency and offering the potential for integration
    with other existing PEFT methods for further enhancement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present the universality theorem and the rank representation theorem, theoretically
    proving that QuanTA can efficiently parameterize high-rank matrices, overcoming
    the limitations of low-rank methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We validate QuanTA’s performance through extensive experiments, demonstrating
    significant improvements in various reasoning tasks and efficiency compared to
    traditional methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Parameter-Efficient Fine-Tuning (PEFT) methods aim to address the computational
    burdens associated with fine-tuning large-scale models by adjusting a relatively
    small fraction of the total parameters to fit a specific downstream task. Roughly
    speaking, there are three existing categories of PEFT methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adapter-based methods. These methods introduce additional trainable modules
    into the structure of a pre-trained, otherwise frozen, model. These modules can
    be integrated in various ways: series adapters are interposed between existing
    layers like attention or MLP components [[9](#bib.bib9), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)], while parallel adapters coexist alongside these components
    [[17](#bib.bib17)]. In general, these methods tend to increase the inference load
    due to the extra components that are not readily integrated into the original
    model weights.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prompt/Prefix-based methods. These methods employ additional prompts or soft
    tokens at the beginning of the input sequence, focusing fine-tuning efforts on
    these newly introduced vector embeddings while maintaining the original model
    weights static [[18](#bib.bib18), [19](#bib.bib19)]. However, this approach can
    suffer from suboptimal performance and increased inference times. In addition,
    the soft tokens take up space of real tokens and therefore reduce the effective
    context size available for the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reparameterization-based methods. These methods modify the existing weights
    with some parameter-efficient parameterization during the fine-tuning phase. Among
    these methods, Low-Rank Adaptation (LoRA) [[10](#bib.bib10)] and its variants,
    such as DoRA [[20](#bib.bib20)] and VeRA [[21](#bib.bib21)], are particularly
    noteworthy for their widespread adoption and robust performance across various
    tasks. In addition to LoRA, many other PEFT methods also belong to this category,
    including more sophisticated approaches such as Hadamard [[22](#bib.bib22)], Kronecker
    product [[23](#bib.bib23)] reparameterizations as well as many other methods [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]. Crucially, methods in this
    category do not impose additional inference burdens after fine-tuning as the modified
    weights can be merged into the pre-trained model weights prior to deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides these three categories, there are additional PEFT methods such as LoTA
    [[12](#bib.bib12)], where tensor decompositions are performed across multiple
    weights, LoRETTA [[13](#bib.bib13)], which uses tensor train decomposition for
    each weight matrix and has both adapter-based and reparameterization-based variants,
    MPO-based fine-tuning [[28](#bib.bib28)], and very recently LISA [[29](#bib.bib29)],
    ReFT [[30](#bib.bib30)] and MoRA [[31](#bib.bib31)].
  prefs: []
  type: TYPE_NORMAL
- en: Physics-inspired machine learning In parallel, there have been various attempts
    to integrate physics-based priors into machine learning for many years. Symmetries
    and physics structure have been incorporated into the neural networks architecture
    and training in various applications to achieve notable performance [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]. Various classical and quantum physics processes
    have been utilized to design new neural networks [[40](#bib.bib40), [41](#bib.bib41)]
    and generative models [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)].
  prefs: []
  type: TYPE_NORMAL
- en: '3 Motivation: Low Rank is not Always Sufficient'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/187795ce528e94b5b9778c76afcd9093.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Subspace similarities between two LoRA experiments of different ranks
    (64 and 128) for two datasets. Each point $(i,j)$ right singular vectors of the
    $r=128$ are plotted. DROP dataset has a significantly high “intrinsic rank” than
    RTE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Accuracy/$F_{1}$ LoRA[r=64] 86.0 55.2 $\text{LLaMA2}_{\text{7B}}$ LoRA[r=128]
    85.8 56.2
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Performance of base and LoRA fine-tuned LLaMA2-7B on RTE [[49](#bib.bib49)]
    and DROP [[50](#bib.bib50)] datasets. We use accuracy and $F_{1}$-score as the
    metrics for them respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA operates under the hypothesis that parameter updates during fine-tuning
    exhibit a low “intrinsic rank.” For a pretrained weight matrix $W_{0}\in\mathbb{R}^{d\times
    k}$ are low-rank matrices. In this configuration, only $A$ is limited to $r$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the original LoRA paper shows empirical evidence to support the low-rank
    hypothesis, recently it has been found that this hypothesis may still fail for
    more complex tasks, especially for those that significantly differ from the pre-training
    dataset, leading to suboptimal performance [[11](#bib.bib11), [31](#bib.bib31)].
    To assess the general applicability of the low-rank hypothesis, we examine two
    datasets of varying difficulties: the RTE dataset [[49](#bib.bib49)], a classification
    task where the model is tasked to verify the correctness of statements, and the
    DROP dataset [[50](#bib.bib50)], a generation task where the model performs discrete
    reasoning over paragraphs. We posit that the RTE dataset is simpler, thus more
    likely to conform to the low-rank hypothesis, whereas the DROP dataset presents
    a greater challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [1](#S3.T1 "Table 1 ‣ Figure 2 ‣ 3 Motivation: Low Rank is
    not Always Sufficient ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation"), the LLaMA2-7B model [[6](#bib.bib6)] in general can achieve
    a better score on the RTE dataset than the DROP dataset. In addition, as we increase
    the rank from 64 to 128, LoRA’s performance on the RTE dataset remains the same,
    consistent with the low-rank hypothesis, while the performance on the DROP dataset
    improves, suggesting the DROP dataset may require a higher “intrinsic rank.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further measure the “intrinsic rank” of weight updates for these datasets,
    we follow the methodology outlined in [[10](#bib.bib10)] and compare the subspace
    spanned by the right singular vectors of the resulting weight updates between
    the $r=64$ singular vectors of the $r=64$ values, and quickly decays to 0 for
    larger $i$, indicating that fine-tuning on the RTE dataset has a low “intrinsic
    rank.” Conversely, for the DROP dataset, subspace similarity remains large across
    all 64 singular vectors, demonstrating a high “intrinsic rank.” Additional details
    of subspace similarity and addition data are provide in Appendix [A](#A1 "Appendix
    A Additional Details on Subspace Similarity ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation")'
  prefs: []
  type: TYPE_NORMAL
- en: 'These findings demonstrate the necessity of high-rank fine-tuning in complex
    tasks, challenging the effectiveness of LoRA. This naturally prompts the following
    question: *How can we design efficient methods to facilitate high-rank updates
    during fine-tuning?*'
  prefs: []
  type: TYPE_NORMAL
- en: '4 Preliminary: Quantum Circuit'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The behavior of quantum mechanical systems, especially those involving particles
    with discrete degrees of freedom, is well described by matrix theory. Quantum
    circuits naturally realize unitary matrices whose sizes grow exponentially with
    the number of particles, providing a potent framework for high-rank representation.
    Here, we review some fundamental concepts of quantum states and quantum circuits
    to motivate our approach.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum state and vector representation. An $N$ the components and $\ket{i}$
    is the binary representation of $i$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2af8c2cc725bd032e56b17802fb99656.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Any unitary matrix can be decomposed into a quantum circuit using
    one- and two-qubit gates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantum circuit and matrix representation. A quantum circuit is a unitary matrix
    $\mathcal{U}\in\mathbb{U}(2^{N})\subset\mathbb{C}^{2^{N}\times 2^{N}}$²²2Typically,
    quantum circuits and quantum gates are considered within the group $\mathbb{SU}(2^{N})$
    factor, which does not affect the results presented in this paper.. These gates
    are applied to specific qubits as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: for a one-qubit gate applied to qubit $n$, and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: for a two-qubit gate applied to qubits $m$ do not need to be consecutive qubits.)
  prefs: []
  type: TYPE_NORMAL
- en: 'A quantum circuit comprises a series of these one- and two-qubit gates $\{U^{(\alpha)}\}$
    applied sequentially to the quantum state:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{U}\ket{\psi}=\prod_{\alpha}U^{(\alpha)}\ket{\psi}.$ |  | (3)
    |'
  prefs: []
  type: TYPE_TB
- en: Since quantum circuits are unitary, they inherently represent full-rank matrices
    in finite-dimensional systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Universality of quantum circuit. Similar to the universal approximation theorem
    for neural networks, it has been established that any quantum circuit on $N$ qubits
    can be decomposed into a quantum circuit using only one- and two-qubit gates [[51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53)], as shown in Figure [3](#S4.F3 "Figure 3 ‣
    4 Preliminary: Quantum Circuit ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs
    with Quantum-Informed Tensor Adaptation"). This is particularly relevant for reparameterization-based
    fine-tuning methods, where we aim to parameterize a matrix matching the shape
    of the base model’s weight matrix using a small number of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Quantum-informed Tensor Adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since quantum circuits offer an elegant parameterization for large unitary
    matrices of shape $2^{N}\times 2^{N}$, by relaxing the unitarity constraint and
    allowing for arbitrary local dimensions, we can develop an effective tool for
    high-rank, parameter-efficient fine-tuning. Inspired by this, we propose QuanTA:
    Quantum-informed Tensor Adaptation, which parameterizes the parameter updates
    in a way analogous to a quantum circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: Construction. To illustrate the construction of QuanTA, we focus on the case
    of square weight matrices $W\in\mathbb{R}^{d\times d}$ does not need to be prime
    and the decomposition does not need to be unique. This condition is often satisfied
    for large language models. By reshaping $x\in\mathbb{R}^{d}$th axis corresponding
    to a qudit with local dimension $d_{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a quantum circuit, QuanTA consists of “gates” (or tensors) that apply
    to only specific axes. Since single-axis gates are subsets of two-axis gates,
    it suffices to consider parameterizations using only two-axis gates. Let $T^{(\alpha)}$th
    axes with corresponding dimensions $d_{m^{(\alpha)}}$. Analogous to applying a
    two-qubit gate to a quantum state, applying this tensor to the hidden vector is
    defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where the $\alpha$th axes created as batch dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'QuanTA is then constructed by sequentially applying a collection of such tensors
    $\{T^{(\alpha)}\}$ in the same manner as a quantum circuit:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{T}x:=\prod_{\alpha}T^{(\alpha)}x.$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Although it is difficult to write the full Eq. ([5](#S5.E5 "In 5 Quantum-informed
    Tensor Adaptation ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation")) in index notation for an arbitrary set of tensors, we demonstrate
    in Appendix [F](#A6 "Appendix F Systematical Way to Generate einsum Expressions
    ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation") that the einsum expression for this operation can be systematically
    generated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concrete example of translating Eq. ([5](#S5.E5 "In 5 Quantum-informed
    Tensor Adaptation ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation")) to index notations and einsum, consider the case of $N=3$
    consists of three tensors, each applied to two axes (as depicted in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with
    Quantum-Informed Tensor Adaptation")). In this case, it is easy to express in
    index notation the application of the QuanTA operator to the hidden vector;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: as well as the calculation of the full QuanTA matrix;
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Although Eq. [6](#S5.E6 "In 5 Quantum-informed Tensor Adaptation ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")
    and [7](#S5.E7 "In 5 Quantum-informed Tensor Adaptation ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation") may look complex
    in their formulation, in practice they can be easily implemented respectively
    using einsum as'
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,CnRvcmNoLmVpbnN1bSgiLi4uYWJjLGVmYmMsZGlhZixnaGRlLT4uLi5naGkiLCB4LCBUXzMsIFRfMiwgVF8xKQo=)torch.einsum("…abc,efbc,diaf,ghde->…ghi",  x,  T_3,  T_2,  T_1)'
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,CnRvcmNoLmVpbnN1bSgiZWZiYyxkaWFmLGdoZGUtPmdoaWFiYyIsIFRfMywgVF8yLCBUXzEpCg==)torch.einsum("efbc,diaf,ghde->ghiabc",  T_3,  T_2,  T_1)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization method. At initialization, the adapted model should be the same
    as the base model and all the weight updates should be 0\. However, enforcing
    $\mathcal{T}x=0$, impeding gradient propagation through the tensors and negatively
    impacting training performance.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, we use another set of tensors $\{S^{(\alpha)}\}$ but
    remain frozen throughout fine-tuning. We then define the adapted layer as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=W_{\theta}x:=W_{0}x+\mathcal{T}_{\theta}x-\mathcal{S}x,$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where we use the subscript $\theta$ exactly cancel out, ensuring the adapted
    layer reduces to the base model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this initialization method does not introduce additional
    costs. After initialization, the full $\mathcal{S}$ and simplify the adapted layer
    to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=W_{\theta}x=W_{0}^{\prime}x+\mathcal{T}_{\theta}x.$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 6 Theoretical Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we list a few important theorem and provide the proofs in Appendix [C](#A3
    "Appendix C Additional Theoretical Results and Proofs ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 6.1  (Universality of QuanTA).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $W$ is a power of 2 and $\prod_{n}d_{n}=2^{M}$ and $d_{n^{(\alpha)}}$.
  prefs: []
  type: TYPE_NORMAL
- en: We note that the fine-tuning method KronA [[23](#bib.bib23)] can be incorporated
    into our framework and considered as a special case of QuanTA.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 6.2  (Rank representation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $R=r(\mathcal{T})$, $d^{(\alpha)}=d_{m^{(\alpha)}}d_{n^{(\alpha)}}$ be the
    total number of tensors. The following inequality always holds
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{\alpha}\frac{dR^{(\alpha)}}{d^{(\alpha)}}-d(N_{T}-1)\leq R\leq\min_{\alpha}{\frac{dR^{(\alpha)}}{d^{(\alpha)}}}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: In the special case when all the tensors are full rank ($R^{{\alpha}}=d^{(\alpha)}$).
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 6.3  (Composition openness).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There exists a set of matrices $\mathbb{S}=\{\mathcal{M}_{k}\}$.
  prefs: []
  type: TYPE_NORMAL
- en: We note that the composition openness is not satisfied by LoRA. This is because,
    for any two matrices with LoRA structure, the composition of the matrices still
    falls into the set of matrices with LoRA structure, which limits the expressivity
    of LoRA. In contrast, the composition openness of QuanTA indicates that the expressivity
    of QuanTA can increase as the depth of the neural network increases, even if the
    neural network is close to linear.
  prefs: []
  type: TYPE_NORMAL
- en: No inference overhead. As reparameterization-based methods, QuanTA does not
    impose any inference latency, since the trained $\mathcal{T}$ operator can be
    explicitly constructed as a matrix and merged into the base model weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and computational complexity during fine-tuning. In the forward pass,
    only a hidden vector of size $d$th axes, so the total computational complexity
    for a QuanTA layer is $d\cdot\sum_{\alpha}d_{m^{(\alpha)}}d_{n^{(\alpha)}}$ for
    all $m$, QuanTA reduces to full fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To benchmark QuanTA against other fine-tuning methods, we performed experiments
    on a wide range of datasets (see Appendix [D](#A4 "Appendix D Datasets ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")
    for details). For all experiments, we avoid optimizing the hyperparameters on
    the test set. Instead, we create a validation set from the train set and optimize
    the hyperparameters on the validation set. All the results reported in this section
    are averaged over multiple experiments with varying random seeds, and the term
    “parameters” and “# params” in this section always refer to the trainable parameters.
    Details on the experiments and hyperparameters are shown in Appendix [E](#A5 "Appendix
    E Hyperparameters and Experimental Details ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model PEFT Method # Params (%) $F_{1}$ (Ours) 0.041% 59.5 QuanTA${}_{16\textrm{-}16\textrm{-}16}$
    LoRA[r=8] 0.024% 74.3 QuanTA${}_{16\textrm{-}8\textrm{-}8\textrm{-}8}$ (Ours)
    0.014% 79.4'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Benchmark of various fine-tuning methods on the DROP dataset using
    LLaMA2 7-70 billion parameter models as the base model. In each case, we report
    the average of $F_{1}$ score over 2-4 experiments with different random seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/967540d8a8b2bba194f3f51325084b84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Benchmark of different fine-tuning methods on the DROP dataset as
    a function of training parameters using LLaMA2 7 billion parameter model as the
    base model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DROP Dataset. We begin our benchmark with the DROP dataset [[50](#bib.bib50)],
    chosen as a representative example that requires high-rank fine-tuning. In Table [2](#S7.T2
    "Table 2 ‣ Figure 4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation"), we compare our QuanTA method
    with LoRA of different ranks, as well as series and parallel adapters, by fine-tuning
    LLaMA2 [[6](#bib.bib6)] with up to 70 billion parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [2](#S7.T2 "Table 2 ‣ Figure 4 ‣ 7 Experiments ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation"),
    LoRA consistently underperforms compared to other fine-tuning methods. While increasing
    the rank improves performance, LoRA still falls short, suggesting the necessity
    of high-rank fine-tuning for this task. In addition, QuanTA achieves performance
    on par with, or better than, full fine-tuning using only a a small fraction of
    the parameters, demonstrating the effectiveness of QuanTA’s high-rank fine-tuning
    capability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To investigate how these methods scale with the number of trainable parameters,
    we conducted experiments varying the number of trainable parameters on LLaMA2-7B
    model. The results are shown in Fig. [4](#S7.F4 "Figure 4 ‣ 7 Experiments ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation").
    Each point in the figure represents an average of four experiments with different
    random seeds, and the standard deviation across these experiments is shown as
    error bars ⁴⁴4We vary both the random seed for model initialization and the sampled
    train, dev, and test datasets, which could be the reason of large standard deviations..'
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in the figure, QuanTA achieves performance comparable to or better
    than full fine-tuning using a small fraction of trainable parameters. Conversely,
    LoRA only achieves subpar performance with a small number of trainable parameters,
    though its performance improves with an increase in parameters. Other PEFT methods,
    such as series and parallel adapters, achieve results close to full fine-tuning
    but use significantly more parameters than QuanTA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model PEFT Method # Params (%) Accuracy ($\uparrow$ FT 100% 71.3 82.1 78.6
    90.2 79.0 82.9 67.2 76.8 78.5 Prefix* 0.11% 64.3 76.8 73.9 42.1 72.1 72.9 54.0
    60.6 64.6 Series* 0.99% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel*
    3.54% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3 LoRA* 0.83% 68.9 80.7 77.4
    78.1 78.8 77.8 61.3 74.8 74.7 DoRA^† 0.43% 70.0 82.6 79.7 83.2 80.6 80.6 65.4
    77.6 77.5 DoRA^† 0.84% 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 QuanTA (Ours)
    0.041% 71.6 83.0 79.7 91.8 81.8 84.0 68.3 82.1 80.3 $\text{LLaMA}_{\text{13B}}$
    LoRA^† 0.70% 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA^† 0.35% 74.5 88.8
    80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA^† 0.71% 74.6 89.3 79.9 95.5 85.6 90.5
    80.4 85.8 85.2 QuanTA (Ours) 0.035% 74.3 88.1 81.8 95.1 87.3 91.1 81.7 87.2 85.8'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Benchmark on various commonsense reasoning tasks. All results of models
    and PEFT methods labeled with “*” are from [[54](#bib.bib54)], and results with
    “^†” are from [[20](#bib.bib20)].'
  prefs: []
  type: TYPE_NORMAL
- en: Commonsense Reasoning. We continue to evaluate our method on a collection of
    commonsense reasoning datasets. Following the methodology in [[54](#bib.bib54)],
    we first fine-tune the model on the Commonsense170K dataset [[54](#bib.bib54)],
    a comprehensive collection of commonsense reasoning questions, and subsequently
    evaluate it on eight different downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [3](#S7.T3 "Table 3 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation"), we benchmark our
    QuanTA method against other fine-tuning techniques using 7- and 13-billion-parameter
    LLaMA and LLaMA2 models, as well as the 8-billion-parameter LLaMA3 model. Alongside
    prefix tuning, adapter methods, and LoRA, we also compare our approach to the
    recently proposed LoRA variant, the DoRA method [[20](#bib.bib20)]. The results
    clearly indicate that our QuanTA method outperforms LoRA in all cases and surpasses
    the DoRA method in most benchmarks, using less than one-tenth of the parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Arithmetic Reasoning. We further test our method on arithmetic reasoning tasks
    by fine-tuning the model on Math10K dataset [[54](#bib.bib54)] and assessing its
    performance on four tasks. We note that while [[54](#bib.bib54)] includes additional
    downstream tasks in the arithmetic reasoning benchmark, some test data was later
    found to have leaked into the training dataset. In this study, we only benchmark
    the four downstream tasks unaffected by this data leakage. Additionally, our evaluation
    procedure differs slightly from that in [[54](#bib.bib54)] (see Appendix [E](#A5
    "Appendix E Hyperparameters and Experimental Details ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation") for details).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S7.T4 "Table 4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation") presents the evaluation results
    on the four downstream tasks. Notably, all questions in the AQuA dataset are multiple-choice
    with mostly five options, and all models except GPT-3.5 failed to achieve accuracy
    higher than 20%. Therefore, we conclude that all models perform equally poorly
    on this task and exclude it from the average accuracy computation. This phenomenon
    is also consistent with previous findings [[54](#bib.bib54), [20](#bib.bib20)].
    The results show that QuanTA significantly outperforms LoRA and even surpasses
    full fine-tuning with a small number of parameters. It is surprising that QuanTA
    exceeds full fine-tuning in these tasks, which may be due to overfitting or the
    challenges of optimizing hyperparameters for full fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations. QuanTA currently requires applying the tensors sequentially to
    the hidden vectors, which may result in underutilizing the GPU when the tensors
    are too small. It will be helpful to develop a more efficient implementation to
    fully utilize GPU resources. The hyperparameters in QuanTA, such as the number
    of tensors applying on the same axes, have not been optimized. Choosing an optimal
    set of tensors could further enhance the performance of QuanTA. In the current
    experiments, we only consider LLaMA model series and a thorough study on different
    models will be beneficial if more computational resources are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model PEFT Method # Params (%) Accuracy ($\uparrow$ LoRA 0.67% 16.7 72.3 90.8
    84.3 82.5 QuanTA (Ours) 0.13% 18.9 72.4 94.5 84.8 83.9'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Benchmark on various arithmetic reasoning tasks. GPT-3.5 (labeled
    with “*”) results are taken from [[54](#bib.bib54)].'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduced QuanTA, a novel, easy-to-implement, PEFT method
    with no inference overhead for large language models. QuanTA leverages quantum-inspired
    techniques to achieve high-rank adaptations, addressing the limitations of existing
    low-rank methods. QuanTA introduces high-rank fine-tuning through the universality
    theorem and rank representation theorem. Our extensive experiments demonstrate
    the efficacy of QuanTA across various tasks, including commonsense reasoning,
    arithmetic reasoning, and scalability. QuanTA consistently outperforms traditional
    fine-tuning methods and other PEFT approaches, achieving superior performance
    with a significantly smaller number of trainable parameters. This highlights the
    potential of quantum-informed techniques in enhancing the adaptability and efficiency
    of large language models.
  prefs: []
  type: TYPE_NORMAL
- en: QuanTA offers a scalable and efficient solution for fine-tuning large language
    models, advancing the state-of-the-art in natural language processing. There are
    several promising directions for future research and development of QuanTA. Expanding
    its application to a wider range of tasks and specialized domains could demonstrate
    its versatility and robustness. Combining QuanTA with other PEFT methods or incorporating
    it into ensemble models might further enhance performance, particularly for complex
    tasks. The parameter efficiency of QuanTA may also imply a lower chance of overfitting.
    Additionally, exploring advanced optimization techniques tailored specifically
    for QuanTA could improve convergence rates and overall efficiency. Further design
    based on principles from quantum computing, such as entanglement and superposition,
    may lead to even more efficient fine-tuning methods. Our work paves the way for
    further exploration of quantum-informed methods or even future quantum technologies
    for machine learning, making it a valuable approach for both research and practical
    applications with broader impacts.
  prefs: []
  type: TYPE_NORMAL
- en: Broader Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of QuanTA represents an important advancement in the fine-tuning
    of LLMs, with profound societal implications. By leveraging quantum-informed methods,
    QuanTA reduces computational and memory demands, making advanced NLP capabilities
    more accessible and cost-effective. This democratization of AI technology can
    facilitate its adoption in resource-constrained environments, bridging technological
    disparities. Additionally, the integration of quantum techniques could spark interdisciplinary
    innovations, enhancing healthcare diagnostics, financial risk assessment, and
    personalized education. Furthermore, QuanTA’s efficiency aligns with global sustainability
    efforts by reducing the energy consumption associated with AI training, contributing
    to the reduction of AI’s carbon footprint. Thus, QuanTA not only advances NLP
    but also promotes inclusive, sustainable, and impactful AI technologies across
    various sectors. However, the deployment of such powerful AI models raises concerns
    about data privacy, security, and the potential misuse of AI technologies. Addressing
    these ethical and societal challenges is crucial to ensure that the benefits of
    QuanTA are realized responsibly and equitably.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors acknowledge support from the National Science Foundation under Cooperative
    Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental
    Interactions, [http://iaifi.org/](http://iaifi.org/)). This material is based
    upon work supported by the U.S. Department of Energy, Office of Science, National
    Quantum Information Science Research Centers, Co-design Center for Quantum Advantage
    (C2QA) under contract number DE-SC0012704\. The research was sponsored by the
    United States Air Force Research Laboratory and the Department of the Air Force
    Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement
    Number FA8750-19-2-1000\. The computations in this paper were run on the FASRC
    cluster supported by the FAS Division of Science Research Computing Group at Harvard
    University.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    Conference of the North American Chapter of the Association for Computational
    Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever.
    Language models are unsupervised multitask learners, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
    models are few-shot learners. In Advances in Neural Information Processing Systems,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard
    Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
    Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril,
    Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] AI@Meta. Llama 3 model card. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] AI@Meta. Introducing Meta Llama 3: The most capable openly available LLM
    to date — ai.meta.com. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    2024. [Accessed 22-05-2024].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient
    transfer learning for NLP. In International Conference on Machine Learning, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
    models. In International Conference on Learning Representations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip
    Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan
    Frankle, Cody Blakeney, and John P. Cunningham. Lora learns less and forgets less,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Aleksandr Mikhalev,
    and Ivan Oseledets. Lotr: Low tensor rank weight adaptation, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Yifan Yang, Jiajun Zhou, Ngai Wong, and Zheng Zhang. Loretta: Low-rank
    economic tensor-train adaptation for ultra-low-parameter fine-tuning of large
    language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić,
    Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. AdapterHub: A framework for
    adapting transformers. In Conference on Empirical Methods in Natural Language
    Processing, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao,
    Ahmed Hassan Awadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient
    model tuning. In Conference on Empirical Methods in Natural Language Processing,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter:
    An easy approach for improving the parameter-efficiency of adapters. In Findings
    of the Association for Computational Linguistics: EMNLP 2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
    Neubig. Towards a unified view of parameter-efficient transfer learning. In International
    Conference on Learning Representations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for
    parameter-efficient prompt tuning. In Conference on Empirical Methods in Natural
    Language Processing, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation. In The 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank
    Wang, Kwang-Ting Cheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation.
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based
    random matrix adaptation. In The Twelfth International Conference on Learning
    Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara: Low-rank hadamard
    product for communication-efficient federated learning. In International Conference
    on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J.
    Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker
    adapter, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Yiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Multilora: Democratizing
    lora for better multi-task learning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu,
    and Maosong Sun. Sparse low-rank adaptation of pre-trained language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis,
    Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation
    for parameter-efficient fine-tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation
    of large models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan Xie, Zhong-Yi Lu, and
    Ji-Rong Wen. Enabling lightweight fine-tuning for pre-trained language model compression
    based on matrix product operators. In the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and
    Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient large language
    model fine-tuning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky,
    Christopher D. Manning, and Christopher Potts. Reft: Representation finetuning
    for language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu
    Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. Mora: High-rank
    updating for parameter-efficient fine-tuning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem
    with artificial neural networks. Science, 355(6325):602–606, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Di Luo, Zhuo Chen, Kaiwen Hu, Zhizhen Zhao, Vera Mikyoung Hur, and Bryan K.
    Clark. Gauge-invariant and anyonic-symmetric autoregressive neural network for
    quantum lattice models. Phys. Rev. Res., 5:013216, Mar 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Zhuo Chen, Di Luo, Kaiwen Hu, and Bryan K. Clark. Simulating 2+1d lattice
    quantum electrodynamics at finite density with neural flow wavefunctions, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Zhuo Chen, Laker Newhouse, Eddie Chen, Di Luo, and Marin Soljacic. ANTN:
    Bridging autoregressive neural networks and tensor networks for quantum many-body
    simulation. In Advances in Neural Information Processing Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Di Luo and Bryan K Clark. Backflow transformations via neural networks
    for quantum many-body wave functions. Physical review letters, 122(22):226401,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Di Luo, Giuseppe Carleo, Bryan K Clark, and James Stokes. Gauge equivariant
    neural networks for quantum lattice gauge theories. Physical review letters, 127(27):276402,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai
    Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant
    neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed
    neural networks: A deep learning framework for solving forward and inverse problems
    involving nonlinear partial differential equations. Journal of Computational physics,
    378:686–707, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural
    networks. Advances in neural information processing systems, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel,
    and Shirley Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
    Deep unsupervised learning using nonequilibrium thermodynamics. In International
    Conference on Machine Learning, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models. In Advances in Neural Information Processing Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Yang Song and Stefano Ermon. Generative modeling by estimating gradients
    of the data distribution. In Advances in Neural Information Processing Systems,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, and Ben Poole. Score-based generative modeling through stochastic differential
    equations. In International Conference on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Jin-Guo Liu and Lei Wang. Differentiable learning of quantum circuit born
    machines. Physical Review A, 98(6):062324, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Ziming Liu, Di Luo, Yilun Xu, Tommi Jaakkola, and Max Tegmark. Genphys:
    From physical processes to generative models. arXiv preprint arXiv:2304.02637,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Edwin Stoudenmire and David J Schwab. Supervised learning with tensor
    networks. Advances in neural information processing systems, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian
    Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark
    for general-purpose language understanding systems. In Advances in Neural Information
    Processing Systems, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh,
    and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning
    over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers), 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A Yu Kitaev. Quantum computations: algorithms and error correction. Russian
    Mathematical Surveys, 52(6):1191, dec 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] A. Yu. Kitaev, A. H. Shen, and M. N. Vyalyi. Classical and Quantum Computation.
    American Mathematical Society, USA, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum
    Information: 10th Anniversary Edition. Cambridge University Press, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing,
    Xing Xu, Soujanya Poria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient
    fine-tuning of large language models. In Proceedings of the 2023 Conference on
    Empirical Methods in Natural Language Processing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] David P. DiVincenzo. Two-bit gates are universal for quantum computation.
    Physical Review A, 51(2):1015–1022, February 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Jean-Luc Brylinski and Ranee Brylinski. Universal quantum gates, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
    Socialiqa: Commonsense reasoning about social interactions, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence?, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a
    suit of armor conduct electricity? a new dataset for open book question answering,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction
    by rationale generation: Learning to solve and explain algebraic word problems.
    ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, and John Schulman. Training verifiers to solve math word problems,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh
    Hajishirzi. Mawps: A math word problem repository. In North American Chapter of
    the Association for Computational Linguistics, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really
    able to solve simple math word problems?, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee,
    Danqi Chen, and Sanjeev Arora. Fine-tuning large language models with just forward
    passes. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Additional Details on Subspace Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the main paper, we use the subspace similarity to measure the “intrinsic
    rank” of fine-tuning on a specific dataset. In this section, we provide more details
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: While it is tempting to compute the rank by performing singular value decomposition
    (SVD) on the weight matrices of fully fine-tuned models, such measurement generally
    overestimates the intrinsic rank due to random parameter updates during fine-tuning
    that are irrelevant to the performance on the downstream tasks. The authors of
     [[10](#bib.bib10)] proposes a better way to measure the intrinsic rank, which
    we describe as follows.
  prefs: []
  type: TYPE_NORMAL
- en: First, we run LoRA fine-tuning for two different ranks $r_{1}$. Then, we perform
    singular value decompositions on the weights to obtain $\Delta W^{(r)}=U^{(r)}S^{(r)}{V^{(r)}}^{\top}$
    columns of $V^{(r_{1})}$ (first $j$. The subspace similarity between these two
    subspace is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (A.1) |'
  prefs: []
  type: TYPE_TB
- en: This function equals 1 if any subspace can be contained in the other, equals
    0 if the two subspaces are orthogonal, and in general measures the overlap between
    0 and 1 between the two subspaces. FOr fine-tuning that has a low “intrinsic rank”;
    only the subspace spanned by the first few singular vectors (that correspond to
    the largest few singular vectors) should be similar, with the rest nearly perpendicular
    originating from random noise during fine-tuning. Thus, the subspace similarity
    should be close to 1 only when either $i$. On the other hand, when the “intrinsic
    rank” is high, all the singular vectors in one subspace can be important and therefore
    would appear in the other. In this case, the subspace similarity can remain high
    for all values of $i$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the main paper, we choose $r_{1}=64$, and measure the subspace similarity
    for both the RTE dataset [[49](#bib.bib49)] and the DROP dataset [[50](#bib.bib50)],
    and reported the values corresponding to the query weight matrix of the 16th layer.
    In this section, we include results corresponding to additional weight matrices.
    In Fig. [A.1](#A1.F1 "Figure A.1 ‣ Appendix A Additional Details on Subspace Similarity
    ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation") and [A.2](#A1.F2 "Figure A.2 ‣ Appendix A Additional Details on Subspace
    Similarity ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation"), we show the subspace similarities for the value weight matrix
    at layer 16 and 23\. We observe that the similar behaviors appear for these two
    weight matrices as in the main paper, where the RTE dataset exhibits a low “intrinsic-rank”,
    while the DROP dataset has a high “intrinsic-rank”.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb0445df7fc4a80dc735592a830f5639.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.1: Subspace similarities between two LoRA experiments of different
    ranks (64 and 128) for two datasets for the value weight matrix at layer 16.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae843ba5c14b6eb2ca0e0f99018ed536.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.2: Subspace similarities between two LoRA experiments of different
    ranks (64 and 128) for two datasets for the value weight matrix at layer 23.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Constructing General QuanTA Operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the main paper, we discussed QuanTA operation when the layer weight is square
    $W_{0}\in\mathbb{R}^{d\times d}$. Here, we will consider more general cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first consider general rectangular matrices $W_{0}\in\mathbb{R}^{d\times
    k}$ and $k=k_{1}\times k_{2}\times\cdots\times k_{N}$ and $k$. (For example, LLaMA2-70B
    model contains many $1024\times 8192$ for all $n></math>th axes. After applying
    this tensor, the hidden vector changes shape from <math id=$, making the hidden
    vector into the correct size. Then, one just needs to make sure that all the tensors
    subsequent to this tensor needs to have the correct shape when applying to the
    first axis. A pictorial representation is shown in Fig. [B.3](#A2.F3 "Figure B.3
    ‣ Appendix B Constructing General QuanTA Operators ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51bf4e3a4b449bd7d0e5da5c870bd397.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure B.3: Illustration of how to parameterize rectangular matrix with QuanTA'
  prefs: []
  type: TYPE_NORMAL
- en: In the special case where the “rectangular” tensor is the last tensor applied
    to the hidden vector, this operation can be equivalently written as applying a
    “square” tensor of shape $\mathbb{R}^{d_{1}d_{n}\times d_{1}d_{n}}$, this operation
    needs to be reversed, and it can be expressed as first padding the hidden vector
    to appropriate size, before applying the tensor circuit.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, it is possible to choose any set of $\{d_{n}\}_{n=1}^{N}$),
    as well as any of tensors that transforms $\mathbb{R}^{d_{1}\times d_{2}\times\cdots\times
    d_{N}}\rightarrow\mathbb{R}^{k_{1}\times k_{2}\times\cdots\times k_{N}}$ to $k$
    and $k$ to be merged into the original weight matrix, by padding and truncating
    the $\mathcal{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Additional Theoretical Results and Proofs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the main paper, we have listed a few important theorems. In this section,
    we provide the proof for these theorems. We will first need the following lemma,
    which is fundamental to modern quantum computation. The proof of this lemma can
    be found in any modern quantum computation textbook such as Ref. [[53](#bib.bib53)]
    or in Ref. [[55](#bib.bib55), [56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: Lemma C.1  (Universality of two-qubit gates).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Any $2^{M}\times 2^{M}$.
  prefs: []
  type: TYPE_NORMAL
- en: This immediately gives us the following corollary.
  prefs: []
  type: TYPE_NORMAL
- en: Corollary C.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Any $\prod_{n}d_{n}\times\prod_{n}d_{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is possible to reshape the matrix into $2^{M}\times 2^{M}$ and use Lemma [C.1](#A3.Thmlemma1
    "Lemma C.1 (Universality of two-qubit gates). ‣ Appendix C Additional Theoretical
    Results and Proofs ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation") to obtain a two-qubit gates representation. Since two-qubit
    gates are a subset of two-qudit gates, this already concludes the proof. However,
    one can group the two-qubit gates that apply to the same qudit to reduce reduce
    the gate count. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: We also need another lemma from quantum computation [[53](#bib.bib53)].
  prefs: []
  type: TYPE_NORMAL
- en: Lemma C.2  (Phase Rotation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any diagonal unitary matrix of size $2^{M}\times 2^{M}$ and each two-qubit
    gate is an analytic function of $\{\theta_{k}\}$, that can exactly represent the
    diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Corollary [C.1](#A3.Thmcorollary1 "Corollary C.1\. ‣ Appendix C
    Additional Theoretical Results and Proofs ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation"), Lemma [C.2](#A3.Thmlemma2 "Lemma
    C.2 (Phase Rotation). ‣ Appendix C Additional Theoretical Results and Proofs ‣
    QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")
    can also be extended to'
  prefs: []
  type: TYPE_NORMAL
- en: Corollary C.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any diagonal unitary matrix of size $\prod_{n}d_{n}\times\prod_{n}d_{n}$
    and each two-qudit gate is an analytic function of $\{\theta_{k}\}$, that can
    exactly represent the diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can analytically continue Corollary [C.2](#A3.Thmcorollary2 "Corollary C.2\.
    ‣ Appendix C Additional Theoretical Results and Proofs ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation") to nonunitary diagonal
    matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: Corollary C.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any diagonal matrix of size $\prod_{n}d_{n}\times\prod_{n}d_{n}$, where
    the structure of the sequence is fixed for all possible set of $\{\theta_{k}\}$,
    that can exactly represent the diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider Corollary [C.2](#A3.Thmcorollary2 "Corollary C.2\. ‣ Appendix C Additional
    Theoretical Results and Proofs ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs
    with Quantum-Informed Tensor Adaptation"), since both the full unitary matrix
    and the sequence of two-qudit gates are finite, and are analytic functions of
    $\{\theta_{k}\}$’s to be imaginary numbers concludes the proof. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are finally ready to prove the universality theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem C.1  (Universality of QuanTA).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $W$ is a power of 2 and $\prod_{n}d_{n}=2^{M}$ and $d_{n^{(\alpha)}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $U$. Since $U$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Theorem C.2  (Rank representation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $R=r(\mathcal{T})$, $d^{(\alpha)}=d_{m^{(\alpha)}}d_{n^{(\alpha)}}$ be the
    total number of tensors. The following inequality always holds
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{\alpha}\frac{dR^{(\alpha)}}{d^{(\alpha)}}-d(N_{T}-1)\leq R\leq\min_{\alpha}{\frac{dR^{(\alpha)}}{d^{(\alpha)}}}.$
    |  | (C.2) |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rank of the product of two matrices $A$. In QuanTA, each tensor can be viewed
    as a large matrix, where $T^{(\alpha)}$ times the rank of the product of the identity
    matrices, which equals $\frac{dR^{(\alpha)}}{d^{(\alpha)}}$. Then, using the above
    inequality multiple times concludes our proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Theorem C.3  (Composition openness).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There exists a set of matrices $\mathbb{S}=\{\mathcal{M}_{k}\}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We consider a set of matrices $\mathbb{S}$ has two layers of CNOT gates which
    can generate more entanglement than any element $\mathcal{M}_{3}\in\mathbb{S}$
    that only contains one layer of CNOT gates. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dataset Task # Train # Val # Test Metric Answer DROP [[50](#bib.bib50)] Reading
    comprehension with discrete reasoning 2000 800 1200 $F_{1}$-Score Phrase Commonsense170K
    [[54](#bib.bib54)] Commonsense reasoning (Train) 170020 400 – – – BoolQ [[57](#bib.bib57)]
    Commonsense reasoning (Test) – – 3270 Accuracy Yes/No PIQA [[58](#bib.bib58)]
    Commonsense reasoning (Test) – – 1838 Accuracy Option SIQA [[59](#bib.bib59)]
    Commonsense reasoning (Test) – – 508 Accuracy Option HellaSwag [[60](#bib.bib60)]
    Commonsense reasoning (Test) – – 10042 Accuracy Option WinoGrande [[61](#bib.bib61)]
    Commonsense reasoning (Test) – – 1267 Accuracy Option ARC-Easy [[62](#bib.bib62)]
    Commonsense reasoning (Test) – – 2376 Accuracy Option ARC-Challenge [[62](#bib.bib62)]
    Commonsense reasoning (Test) – – 1172 Accuracy Option OBQA [[63](#bib.bib63)]
    Commonsense reasoning (Test) – – 500 Accuracy Option Math10K [[54](#bib.bib54)]
    Arithmetic reasoning (Train) 9519 400 – – – AQuA [[64](#bib.bib64)] Arithmetic
    reasoning (Test) – – 254 Accuracy Option GSM8K [[65](#bib.bib65)] Arithmetic reasoning
    (Test) – – 1319 Accuracy Number MAWPS [[66](#bib.bib66)] Arithmetic reasoning
    (Test) – – 238 Accuracy Number SVAMP [[67](#bib.bib67)] Arithmetic reasoning (Test)
    – – 1000 Accuracy Number'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table D.1: List of datasets used in this work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we describe the datasets used in this paper. In Table [D.1](#A4.T1
    "Table D.1 ‣ Appendix D Datasets ‣ QuanTA: Efficient High-Rank Fine-Tuning of
    LLMs with Quantum-Informed Tensor Adaptation"), the list of datasets used in this
    paper is listed.'
  prefs: []
  type: TYPE_NORMAL
- en: For the DROP dataset [[50](#bib.bib50)], we subsample 2000 samples from the
    original train set as our train set, 800 samples from the train set as our validation
    set, and 1200 samples from the original validation set as our test set, since
    the original dataset does not contain a test set on Hugging Face. In addition,
    the $F_{1}$-score is used to measure the closeness of the models’ output compared
    to the ground truth since it is in general a phrase.
  prefs: []
  type: TYPE_NORMAL
- en: For all commonsense reasoning tasks, we first fine-tune a single model on the
    Commonsense170K dataset collected by [[54](#bib.bib54)], and evaluate the same
    model on eight different commonsense reasoning tasks [[57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)],
    which we use the version provided by [[54](#bib.bib54)]. The Commonsense170K dataset
    is split into a train set of 170020 samples, and a validation set of 400 samples.
    All of the commonsense reasoning tasks are either Yes/No questions or multiple
    choice questions. In these tasks, the model is asked to choose the best answer
    from all the options, and accuracy is used as the evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: For all arithmetic reasoning tasks, we fine-tune a single model on the Math10K
    dataset [[54](#bib.bib54)] and evaluate the same model on four different tasks
    [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)]. We split
    the Math10K dataset into a train set of 9519 samples, and a validation set of
    400 samples. Similar to the commonsense reasoning tasks, we use the version of
    the datasets provided by [[54](#bib.bib54)]. In addition, in [[54](#bib.bib54)],
    there was found some data leak issues in some of the arithmetic datasets. Here,
    we only consider the datasets that are unaffected. In the arithmetic reasoning
    tasks, although the model is asked to generate the step-by-step solution for the
    final answer, only the final answer is parsed to measure the accuracy. For AQuA,
    we parse the output text to find the last character such that it is one of the
    options. For the other three tasks which require numerical answers, we simply
    parse the last number from the output text, and consider the answer to be correct
    if it is the same as the ground truth for up to 4 decimal places.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Hyperparameters and Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the hyperparameter choices and the experimental
    details. All the experiments are conducted on NVIDIA A100 GPUs with 80 GB memory.
    GPU count used in each experiment will be explained later. The code used to produce
    the experiments is released on GitHub at [https://github.com/quanta-fine-tuning/quanta](https://github.com/quanta-fine-tuning/quanta).
    Our code is implemented using [[54](#bib.bib54)] and  [[68](#bib.bib68)] as references.
  prefs: []
  type: TYPE_NORMAL
- en: E.1 QuanTA parameterization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although QuanTA supports decomposing the hidden dimension into an arbitrary
    number of axes $N$ and $5$ tensors when $N=5$ and $N=5$, we show the construction
    in Fig. [E.4](#A5.F4 "Figure E.4 ‣ E.1 QuanTA parameterization ‣ Appendix E Hyperparameters
    and Experimental Details ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with
    Quantum-Informed Tensor Adaptation")'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/96bedd8709cf574ab433ead76867f645.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure E.4: QuanTA architecture used in this work for $N=4$.'
  prefs: []
  type: TYPE_NORMAL
- en: E.2 Experiments on DROP dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [E.2](#A5.T2 "Table E.2 ‣ E.2 Experiments on DROP dataset ‣ Appendix
    E Hyperparameters and Experimental Details ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation"), we show the hyperparameters
    used for the DROP experiments. Only LoRA and QuanTA are applied to the 13- and
    70-billion-parameter LLaMA2 models. For the 13-billion-parameter model or smaller,
    only a single A100 GPU is used for fine-tuning. And for the 70-billion-parameter
    model, four A100 GPUs are used. For all experiments, the hyperparameters are only
    optimized on the 7-billion-parameter LLaMA2 model, and applied directly on larger
    models. In addition, all the hyperparameters are optimized on the validation set,
    before evaluating the model on the test set. We further note that we choose the
    best checkpoint obtained during fine-tuning, in terms of the $F_{1}$-score on
    the validation set, as the final model to apply on the test set. Because of this,
    the number-of-epoch parameter does not introduce a significant effect to the final
    result, as long as the training converges. Therefore, this hyperparameter is chosen
    rather arbitrarily between 3 and 6\. We further note that the batch sizes reported
    here are the effective batch sizes, including the gradient accumulation steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Hyperparameters | Values |'
  prefs: []
  type: TYPE_TB
- en: '| FT | Batch Size | {2, 4, 8} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | {5e-6, 1e-5} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Series Adapters | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Bottleneck | [64, 128] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | Default |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel Adapters | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Bottleneck | [64 ,128] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | Default |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | {1e-4, 2e-4, 5e-4, 6e-4} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $r$ | [4, 6, 8, 16, 24, 32, 48, 64, 96, 128] |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\alpha$ | 16 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | (q_proj v_proj) (Same as Default) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | [1, 4] |'
  prefs: []
  type: TYPE_TB
- en: '| QuanTA | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $N$ | [3, 4, 5] |'
  prefs: []
  type: TYPE_TB
- en: '|  | $d_{1}$ | [8-8-4-4-4, 8-8-8-8, 16-8-8-4, 16-16-4-4, |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 16-16-16, 16-8-8-5, 16-8-8-8] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | (q_proj v_proj) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | [1, 4] |'
  prefs: []
  type: TYPE_TB
- en: 'Table E.2: Hyperparameters used for DROP dataset for various fine-tuning methods.
    Curly brackets include the hyperparameter values tested during hyperparameter
    optimization, with the actual hyperparameter(s) underscored. Square brackets include
    hyperparameter values for different experiments conducted in the main paper.'
  prefs: []
  type: TYPE_NORMAL
- en: E.3 Experiments on commonsense datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We explain the details of the experiments on commonsense datasets. As mentioned
    before, in this experiment, we first fine-tune the model on the joint Commonsense170K
    dataset and evaluate the same fine-tuned model on all downstream tasks. Similar
    to the drop dataset, we optimize the hyperparameters on the validation set that
    we created from the Commonsense170K dataset and choose the best checkpoint in
    terms of the validation accuracy to evaluate on the benchmarks. The hyperparameters
    are listed in Table [E.3](#A5.T3 "Table E.3 ‣ E.3 Experiments on commonsense datasets
    ‣ Appendix E Hyperparameters and Experimental Details ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Hyperparameters | Values |'
  prefs: []
  type: TYPE_TB
- en: '| FT | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | 1e-5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| QuanTA | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | {5e-5, 1e-4} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $N$ | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $d_{1}$ | [16-8-8-4, 16-8-8-5] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | (q_proj v_proj) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table E.3: Hyperparameters used for commonsense experiments. Curly brackets
    include the hyperparameter values tested during hyperparameter optimization, with
    the actual hyperparameter(s) underscored. Square brackets include hyperparameter
    values for different experiments conducted in the main paper.'
  prefs: []
  type: TYPE_NORMAL
- en: E.4 Experiments on arithmetic datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We further explain the details of the experiments on arithmetic datasets. Similar
    to previous, we first fine-tune the model on the joint Math10K dataset and evaluate
    the same fine-tuned model on all downstream tasks and we optimize the hyperparameters
    on the validation set that we created from the Math10K dataset and choose the
    best checkpoint in terms of the validation accuracy to evaluate on the benchmarks.
    The hyperparameters are listed in Table [E.4](#A5.T4 "Table E.4 ‣ E.4 Experiments
    on arithmetic datasets ‣ Appendix E Hyperparameters and Experimental Details ‣
    QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation").
    Notice that we choose a different set of module for LoRA to match the experimental
    setup of [[54](#bib.bib54), [20](#bib.bib20)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Hyperparameters | Values |'
  prefs: []
  type: TYPE_TB
- en: '| FT | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | 1e-5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | {1e-4, 3e-4} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $r$ | 32 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\alpha$ | 16 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | (q_proj k_proj v_proj up_proj down_proj) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| QuanTA | Batch Size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '|  | Scheduler | Linear Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '|  | Learning Rate | {1e-4, 3e-4} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Weight Decay | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dropout | 0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $N$ | 4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $d_{1}$ | {16-8-8-4, 16-16-4-4, 16-8-8-5, 16-16-5-4} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Modules | {(q_proj v_proj), |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | (q_proj k_proj v_proj up_proj down_proj)} |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of GPUs | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table E.4: Hyperparameters used for arithmetic experiments. Curly brackets
    include the hyperparameter values tested during hyperparameter optimization, with
    the actual hyperparameter(s) underscored. Square brackets include hyperparameter
    values for different experiments conducted in the main paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Systematical Way to Generate einsum Expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the main paper, we show an example of how to implement QuanTA operation easily
    using einsum. Here, we show how to systematically generate the einsum expression
    more generally. For illustrative purposes, we focus on the case where there is
    exactly one tensor for every combination of two axes.
  prefs: []
  type: TYPE_NORMAL
- en: First, we show how to generate the einsum expression for applying the QuanTA
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,CmltcG9ydCBpdGVydG9vbHMKaW1wb3J0IG9wdF9laW5zdW0gYXMgb2UKXHBhcmRlZiBxdWFudGFfYXBwbHlfZWluc3VtX2V4cHIoTik6CmN1cnJlbnRfc3ltYm9sc19pbmRzID0gbGlzdChyYW5nZShOKSkKXHBhcmV4cHIgPSAiLi4uIgpmb3IgaSBpbiBjdXJyZW50X3N5bWJvbHNfaW5kczoKZXhwciArPSBvZS5nZXRfc3ltYm9sKGkpClxwYXJmb3IgKGRpbTEsIGRpbTIpIGluIGl0ZXJ0b29scy5jb21iaW5hdGlvbnMocmFuZ2UoLTEsIC1OLTEsIC0xKSwgMik6CnN5bWJvbF9pbmQxID0gY3VycmVudF9zeW1ib2xzX2luZHNbZGltMV0Kc3ltYm9sX2luZDIgPSBjdXJyZW50X3N5bWJvbHNfaW5kc1tkaW0yXQpzeW1ib2xfaW5kMyA9IHN5bWJvbF9pbmQxICsgTgpzeW1ib2xfaW5kNCA9IHN5bWJvbF9pbmQyICsgTgpleHByICs9ICIsIiArIFwNb2UuZ2V0X3N5bWJvbChzeW1ib2xfaW5kNCkgKyBcDW9lLmdldF9zeW1ib2woc3ltYm9sX2luZDMpICsgXA1vZS5nZXRfc3ltYm9sKHN5bWJvbF9pbmQyKSArIFwNb2UuZ2V0X3N5bWJvbChzeW1ib2xfaW5kMSkKY3VycmVudF9zeW1ib2xzX2luZHNbZGltMV0gPSBzeW1ib2xfaW5kMwpjdXJyZW50X3N5bWJvbHNfaW5kc1tkaW0yXSA9IHN5bWJvbF9pbmQ0ClxwYXJleHByICs9ICItPi4uLiIKZm9yIGkgaW4gY3VycmVudF9zeW1ib2xzX2luZHM6CmV4cHIgKz0gb2UuZ2V0X3N5bWJvbChpKQpccGFycmV0dXJuIGV4cHIK)import  itertoolsimport  opt_einsum  as  oe\pardef  quanta_apply_einsum_expr(N):current_symbols_inds  =  list(range(N))\parexpr  =  "…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parfor  (dim1,  dim2)  in  itertools.combinations(range(-1,  -N-1,  -1),  2):symbol_ind1  =  current_symbols_inds[dim1]symbol_ind2  =  current_symbols_inds[dim2]symbol_ind3  =  symbol_ind1  +  Nsymbol_ind4  =  symbol_ind2  +  Nexpr  +=  ","  +  \  oe.get_symbol(symbol_ind4)  +  \  oe.get_symbol(symbol_ind3)  +  \  oe.get_symbol(symbol_ind2)  +  \  oe.get_symbol(symbol_ind1)current_symbols_inds[dim1]  =  symbol_ind3current_symbols_inds[dim2]  =  symbol_ind4\parexpr  +=  "->…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parreturn  expr'
  prefs: []
  type: TYPE_NORMAL
- en: Then, applying the QuanTA operator to the hidden vector is as simple as
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,CnkgPSB0b3JjaC5laW5zdW0ocXVhbnRhX2FwcGx5X2V4cHIsIHgsICpUKQo=)y  =  torch.einsum(quanta_apply_expr,  x,  *T)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, it is easy to generate the einsum expression for obtaining the full
    QuanTA operator as
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,CmltcG9ydCBpdGVydG9vbHMKaW1wb3J0IG9wdF9laW5zdW0gYXMgb2UKXHBhcmRlZiBxdWFudGFfb3BfZWluc3VtX2V4cHIoTik6CmN1cnJlbnRfc3ltYm9sc19pbmRzID0gbGlzdChyYW5nZShOKSkKXHBhcmV4cHIgPSAiLi4uIgpmb3IgaSBpbiBjdXJyZW50X3N5bWJvbHNfaW5kczoKZXhwciArPSBvZS5nZXRfc3ltYm9sKGkpClxwYXJmb3IgKGRpbTEsIGRpbTIpIGluIGl0ZXJ0b29scy5jb21iaW5hdGlvbnMocmFuZ2UoLTEsIC1OLTEsIC0xKSwgMik6CnN5bWJvbF9pbmQxID0gY3VycmVudF9zeW1ib2xzX2luZHNbZGltMV0Kc3ltYm9sX2luZDIgPSBjdXJyZW50X3N5bWJvbHNfaW5kc1tkaW0yXQpzeW1ib2xfaW5kMyA9IHN5bWJvbF9pbmQxICsgTgpzeW1ib2xfaW5kNCA9IHN5bWJvbF9pbmQyICsgTgpleHByICs9ICIsIiArIFwNb2UuZ2V0X3N5bWJvbChzeW1ib2xfaW5kNCkgKyBcDW9lLmdldF9zeW1ib2woc3ltYm9sX2luZDMpICsgXA1vZS5nZXRfc3ltYm9sKHN5bWJvbF9pbmQyKSArIFwNb2UuZ2V0X3N5bWJvbChzeW1ib2xfaW5kMSkKY3VycmVudF9zeW1ib2xzX2luZHNbZGltMV0gPSBzeW1ib2xfaW5kMwpjdXJyZW50X3N5bWJvbHNfaW5kc1tkaW0yXSA9IHN5bWJvbF9pbmQ0ClxwYXJleHByICs9ICItPi4uLiIKZm9yIGkgaW4gY3VycmVudF9zeW1ib2xzX2luZHM6CmV4cHIgKz0gb2UuZ2V0X3N5bWJvbChpKQpccGFycmV0dXJuIGV4cHJbMTpdCg==)import  itertoolsimport  opt_einsum  as  oe\pardef  quanta_op_einsum_expr(N):current_symbols_inds  =  list(range(N))\parexpr  =  "…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parfor  (dim1,  dim2)  in  itertools.combinations(range(-1,  -N-1,  -1),  2):symbol_ind1  =  current_symbols_inds[dim1]symbol_ind2  =  current_symbols_inds[dim2]symbol_ind3  =  symbol_ind1  +  Nsymbol_ind4  =  symbol_ind2  +  Nexpr  +=  ","  +  \  oe.get_symbol(symbol_ind4)  +  \  oe.get_symbol(symbol_ind3)  +  \  oe.get_symbol(symbol_ind2)  +  \  oe.get_symbol(symbol_ind1)current_symbols_inds[dim1]  =  symbol_ind3current_symbols_inds[dim2]  =  symbol_ind4\parexpr  +=  "->…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parreturn  expr[1:]'
  prefs: []
  type: TYPE_NORMAL
- en: and obtaining the full QuanTA operator is
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,CmZ1bGxfVCA9IHRvcmNoLmVpbnN1bShxdWFudGFfb3BfZXhwciwgKlQpCg==)full_T  =  torch.einsum(quanta_op_expr,  *T)'
  prefs: []
  type: TYPE_NORMAL
- en: We note that the padding and truncation operators are omitted when the QuanTA
    operator has a different size than the original weight matrix. In addition, in
    our actual implementation, we use opt_einsum library to optimize the contraction
    order, reducing the contraction cost.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Example Model Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide some example output of QuanTA fine-tuned LLaMA model.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Task | Model Output |'
  prefs: []
  type: TYPE_TB
- en: '| DROP | Prompt: Passage:  Hoping to rebound from their embarrassing home loss to the Lions, the Raiders flew to Invesco Field at Mile High for an AFC West duel with the Denver Broncos.  In the first quarter, Oakland trailed early as Broncos QB Jay Cutler completed a 9-yard TD pass to WR Brandon Stokley for the only score of the period.  In the second quarter, the Raiders got on the board with kicker Sebastian Janikowski getting a 38-yard field goal.  However, Denver continued to pound away as RB Cecil Sapp got a 4-yard TD run, while kicker Jason Elam got a 23-yard field goal. In the third quarter, Oakland began to come back as QB Josh McCown (who was a gametime decision heading into the game) completed a 46-yard TD pass to WR Jerry Porter for the only score of the period.  In the fourth quarter, the Raiders took the lead as DT Gerard Warren sacked Cutler in the end zone for a safety, while LB Thomas Howard returned an interception 44 yards for a touchdown (followed by a successful two-point conversion pass from McCown to WR Ronald Curry).  However, the Broncos tied the game up with Elam’s 20-yard field goal.  In overtime, Oakland managed to make Denver go three-and-out on their first possession.  A 33-yard run by RB LaMont Jordan helped set up Janikowski for a game-winning 52-yard field goal. Broncos head coach Mike Shanahan called timeout before the kick could begin.  Janikowski’s second try hit off the very tip of the left goal post and was no good, giving Denver a chance to win the game.  The Broncos won with Elam getting a 23-yard field goal. With the loss, not only did the Raiders fall to 0-2, but they had lost 11-straight games (currently the NFL’s longest losing streak) dating back to Week 9 of the 2006 season.
    Question: How many field goals did each kicker score in the first half? Answer:
    Output: 1 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table G.5: Examples of QuanTA trained LLaMA2-7B Outputs for the DROP dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Task | Model Output |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | Prompt: Please answer the following question with true or false, question: is ford escape a 4 wheel drive vehicle? Answer format: true/false
    Highest probability choice: Answer: the correct answer is true. |'
  prefs: []
  type: TYPE_TB
- en: '| SIQA | Prompt: Please choose the correct answer to the question: Carson took Lee’s risk by going skydiving with him off of the plane. What will Lee want to do after? Answer1: hug Carson Answer2: buy a ticket Answer3: kick Carson. Answer format: answer1/answer2/answer3
    Highest probability choice: Answer: the correct answer is answer1. |'
  prefs: []
  type: TYPE_TB
- en: '| SIQA | Prompt: Please choose the correct ending to complete the given sentence: Personal Care and Style: [header] How to make ice balls [title] Buy a package of water balloons. [step] This method is cheap, quick, and easy-perfect if you don’t want to spend money on specialty molds for making ice balls. All you’ll need is a few round water balloons (and, of course, water and a freezer. Ending1: ) [substeps] Uninflated balloons: this method requires 2 balls, 1 ice cream stick and 2 water balloons in a large bag (1 at a time). Open the sides of your volcano and shake the tupperware from side to side a few times. Ending2: ) [substeps] Of course, there is no telling if the water balloons will be successful; you just know that they will eventually explode on the first try. Plus, they’re basically too small to correctly hang from your roof. Ending3: ) [substeps] Remember that, if you’re using your ice balls to cool your drinks, they’ll need to be able to fit inside your glasses. Thus, when it comes to water balloons, generally, the smaller, the better. Ending4: ) [substeps] Use unopened water balloons, because they tend to be too full. Plastic ones are dry and will be harder to put in the freezer. Answer format: ending1/ending2/ending3/ending4
    Highest probability choice: Answer: the correct answer is ending3. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table G.6: Examples of QuanTA trained LLaMA3-8B Outputs for selected commonsense
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Task | Model Output |'
  prefs: []
  type: TYPE_TB
- en: '| MAWPS | Prompt: Bob will rent a car for the weekend . He can choose one of 2 payment plans . The first plan costs 57.98 for 2 days plus 13 cents per mile . The second plan costs 45.98 for 2 days plus 18 cents per mile . How many miles does Bob need to drive for the 2 plans to cost the same ?
    Answer: Output: Let’s assume that Bob drives x miles.  For the first plan, the cost would be: 57.98 + 0.13x  For the second plan, the cost would be: 45.98 + 0.18x  We want to find the value of x such that the two costs are equal.  So, we can set up an equation:  57.98 + 0.13x = 45.98 + 0.18x  Simplifying this equation, we get:  0.05x = 12.00  Dividing both sides by 0.05, we get:  x = 240  Therefore, Bob needs to drive 240 miles for the two plans to cost the same.
    |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table G.7: Examples of QuanTA trained LLaMA2-7B Outputs for a selected arithmetic
    dataset.'
  prefs: []
  type: TYPE_NORMAL
