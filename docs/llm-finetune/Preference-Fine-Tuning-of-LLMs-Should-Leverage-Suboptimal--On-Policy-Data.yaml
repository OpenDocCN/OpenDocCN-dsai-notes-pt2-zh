- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:37:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14367](https://ar5iv.labs.arxiv.org/html/2404.14367)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \reportnumber\correspondingauthor
  prefs: []
  type: TYPE_NORMAL
- en: 'anikait@stanford.edu, ftajwar@cs.cmu.edu. Project Website: [https://understanding-rlhf.github.io/](https://understanding-rlhf.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Fahim Tajwar Anikait Singh Archit Sharma Stanford University Rafael Rafailov
    Stanford University Jeff Schneider Carnegie Mellon University Tengyang Xie UW-Madison
    Stefano Ermon Stanford University Chelsea Finn Stanford University Aviral Kumar
    Google DeepMind
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Learning from preference labels plays a crucial role in fine-tuning large language
    models. There are several distinct approaches for preference fine-tuning, including
    supervised learning, on-policy reinforcement learning (RL), and contrastive learning.
    Different methods come with different implementation tradeoffs and performance
    differences, and existing empirical findings present different conclusions, for
    instance, some results show that online RL is quite important to attain good fine-tuning
    results, while others find (offline) contrastive or even purely supervised methods
    sufficient. This raises a natural question: *what kind of approaches are important
    for fine-tuning with preference data and why?* In this paper, we answer this question
    by performing a rigorous analysis of a number of fine-tuning techniques on didactic
    and full-scale LLM problems. Our main finding is that, in general, approaches
    that use on-policy sampling or attempt to push down the likelihood on certain
    responses (i.e., employ a “negative gradient”) outperform offline and maximum
    likelihood objectives. We conceptualize our insights and unify methods that use
    on-policy sampling or negative gradient under a notion of mode-seeking objectives
    for categorical distributions. Mode-seeking objectives are able to alter probability
    mass on specific bins of a categorical distribution at a fast rate compared to
    maximum likelihood, allowing them to relocate masses across bins more effectively.
    Our analysis prescribes actionable insights for preference fine-tuning of LLMs
    and informs how data should be collected for maximal improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-training endows a large language model (LLM) with knowledge about the world.
    Yet, it does not provide a lever to control responses from these models, especially
    when we want these solutions to optimize some task-dependent success criteria
    (e.g., align with human preferences, optimize correctness or compactness). To
    align LLMs with downstream success criteria, they are then fine-tuned with downstream
    objectives after pre-training. In this paper, we focus on fine-tuning problems
    that aim to optimize for binary preferences (from humans or other AI models).
    A plethora of methods have been proposed for this sort of fine-tuning, including
    supervised learning on filtered responses (Gulcehre et al., [2023](#bib.bib19)),
    contrastive training (Rafailov et al., [2023](#bib.bib38)), and on-policy reinforcement
    learning (RL) (Ouyang et al., [2022](#bib.bib35)) on a reward function extracted
    from human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, while all of these methods aim to discover identical optimal policies,
    achieving this in practice would require full data coverage and infinite computation.
    These requirements are not met in practice, and hence, the choice of the loss
    function and the optimization procedure affects performance. However, a lack of
    a clear understanding of different approaches, coupled with different tradeoffs
    in implementation, has resulted in substantial confusion: practitioners are unsure
    as to: (1) whether RL (Ouyang et al., [2022](#bib.bib35)) is required at all,
    or contrastive approaches (Rafailov et al., [2023](#bib.bib38); Gheshlaghi Azar
    et al., [2023](#bib.bib18)), supervised fine-tuning are good enough; and (2) whether
    preference data should be collected with models in the loop (i.e., in an “on-policy”
    fashion) or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to provide clarity on these questions by performing a rigorous study
    to understand the behavior of existing methods when optimizing for preferences.
    Our study operates under assumptions typical in preference fine-tuning, including
    the existence of an underlying ground truth reward function that explains the
    preference data. We study methods that train an LLM policy to optimize a surrogate
    loss given by the expected reward under a model of the reward function (learned
    from preference data) penalized by the KL-divergence between the policy and a
    reference policy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77d72652c94fa18502fb8c08ed35c861.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Left: an illustration of various fine-tuning techniques. On-policy
    sampling gradually shifts policy mass from $\pi_{\mathrm{ref}}$ compared to policies
    that only maximize some sort of likelihood, $\pi_{\text{sup}}$ already lies in
    the high likelihood regions of $\pi_{\mathrm{ref}}$, offline supervised methods
    can work well. No on-policy sampling or negative gradients may be needed.'
  prefs: []
  type: TYPE_NORMAL
- en: To answer the above questions, we develop an analysis framework consisting of
    didactic bandit problems, synthetic LLM problems, and full-scale LLM problems,
    constructed out of AlpacaFarm (Dubois et al., [2024](#bib.bib14)) and UltraFeedback (Cui
    et al., [2023](#bib.bib11)). We then study behaviors of different methods given
    coverage conditions and geometric relationships in the problem. *Our main observation*
    is that algorithms that use on-policy RL in a reward model or attempt to push-down
    likelihood on certain responses, i.e., utilize a negative gradient term as in
    contrastive objectives tend to outperform other offline supervised objectives
    with no on-policy sampling or negative gradient. This is surprising because both
    on-policy and offline methods still utilize the same data for learning. We also
    find that using on-policy sampling and negative gradients are especially important
    when high-reward responses appear in less-likely regions of the reference policy
    distribution, and provide benefits complementary to each other. In particular,
    we find that supervised objectives such as Pref-FT and Binary Feed-ME (Dubois
    et al., [2024](#bib.bib14)) are not able to effectively move probability mass
    from low reward responses to high-reward responses. Sampling on-policy responses
    for training, contrastive learning, or employing both on-policy sampling and contrastive
    training can accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: We theoretically show that approaches that use on-policy RL or certain variants
    of contrastive training exhibit “mode-seeking” behavior, resulting in faster accumulation
    of probability mass on a subset of high-reward responses during learning. This
    behavior is in contrast to “mode-covering” supervised objectives that attempt
    to increase likelihood on all high-reward responses, and as a result, are unable
    to efficiently increase probability mass enough on one subset of high-reward responses.
    We then compare the behavior of a representative mode-seeking objective, the reverse
    KL-divergence, with the mode-covering forward KL-divergence to formalize this
    behavior for categorical distributions. Conceptually, this ability to commit to
    a certain subset of high-reward responses enables algorithms with on-policy sampling
    (and optionally, a negative gradient) to perform better than likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Our work presents several actionable takeaways for practitioners. First, we
    tie the performance of various methods to geometric conditions on the problem,
    which can inform practitioners which approach to use. Second, we observe a tradeoff
    between drawing more on-policy samples and performing more gradient steps with
    a different policy training objective. Understanding this tradeoff is useful for
    practitioners since on-policy sampling and training present different computational
    tradeoffs. Finally, since the performance of fine-tuning is tied to the data composition,
    we study the effect of conditions on the coverage of the preference data, which
    could inform data collection.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A dominant recipe for fine-tuning LLMs is to run supervised next token prediction
    (“supervised fine-tuning”) on a dataset of high-quality responses to obtain a
    good policy initialization. This is followed by fine-tuning on a dataset of human
    preferences (Casper et al., [2023](#bib.bib6); Ouyang et al., [2022](#bib.bib35)).
    This fine-tuning can use on-policy RL methods such as REINFORCE (Sutton et al.,
    [1999](#bib.bib45)) or PPO (Schulman et al., [2017](#bib.bib41)) to maximize the
    predictions of a reward model obtained from the preference data, regularized with
    a KL constraint. Another approach (Dubois et al., [2024](#bib.bib14)) performs
    supervised fine-tuning on the filtered set of preferred completions in the preference
    dataset. A different family of methods runs supervised learning on preferred responses
    iteratively such as ReST (Gulcehre et al., [2023](#bib.bib19)), RWR (Hu et al.,
    [2023](#bib.bib21)), and SuperHF (Mukobi et al., [2023](#bib.bib32)). Alternatively,
    methods such as DPO (Rafailov et al., [2023](#bib.bib38)), IPO (Gheshlaghi Azar
    et al., [2023](#bib.bib18)), SLiC-HF (Zhao et al., [2023](#bib.bib61)), and KTO (ContextualAI,
    [2024](#bib.bib9)) learn directly from human preferences, with no explicit reward
    model. Concurrent work also runs DPO iteratively (Yuan et al., [2024](#bib.bib60);
    Chen et al., [2024](#bib.bib8)). These methods come with different tradeoffs necessitating
    a study to understand their behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior analysis work. To understand the effect of preference fine-tuning, prior
    work attempts to uncover its effect on network parameters for a certain set of
    tasks (Jain et al., [2023](#bib.bib22); Lee et al., [2024](#bib.bib29)). Our analysis
    is complementary in that it studies conditions when different algorithms perform
    well, and is applicable to any downstream task. Kirk et al. ([2023](#bib.bib27))
    study the contribution of RL fine-tuning on generalization to out-of-distribution
    prompts but this is complementary to our approach. Gao et al. ([2022](#bib.bib17));
    Coste et al. ([2023](#bib.bib10)); Eisenstein et al. ([2023](#bib.bib15)) study
    reward over-optimization to better build reward models, which is complementary
    to the behavior of the policy optimization approach. Agarwal et al. ([2023](#bib.bib2))
    develop a recipe that uses the mode-seeking KL divergence for knowledge distillation:
    this prior work is largely centered in the problem setting of distillation and
    does not study the optimization behavior of RL, contrastive, or supervised objectives.
    Perhaps closely related to our work is Singhal et al. ([2023](#bib.bib44)), which
    investigates the interplay between PPO and the composition of preference data,
    but this analysis is largely concentrated on studying the length bias of RL fine-tuning
    rather than developing insights into the behavior of fine-tuning algorithms. We
    do design didactic examples that use rewards dependent on length, but this is
    solely for analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently, Ahmadian et al. ([2024](#bib.bib3)) show that REINFORCE may simply
    be enough for preference fine-tuning of LLMs and complex policy optimization methods
    such as PPO may not be needed. Our conclusions are mostly complementary, though
    we do observe that PPO is more robust to sample reuse than REINFORCE. Concurrently,
    Sharma et al. ([2024](#bib.bib43)) compares contrastive and supervised fine-tuning
    on LLM-generated data, but this work does not study the role of coverage or geometric
    conditions. Nevertheless their conclusions that various approaches perform similarly
    when the peak in the reward function (i.e., oracle AI preferences) aligns with
    the likely regions in the data (i.e., responses generated from the same AI model),
    thus providing evidence to support our findings.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Characterizing And Unifying Preference Fine-Tuning Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typical preference fine-tuning methods use a variety of objectives including
    RL, maximum likelihood, and contrastive learning. While the huge number of fine-tuning
    methods inhibits us from empirically analyzing each of them, in this section we
    characterize several existing methods into different families and subsequently
    study a representative member from each family.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Preliminaries and Notation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Typically, before training on preference data, a pre-trained model is fine-tuned
    on high-quality data from the task of interest via supervised fine-tuning (SFT),
    to obtain a “reference” model $\pi_{\mathrm{ref}}$ denotes a prompt and $\mathbf{y}_{w}^{(i)},\mathbf{y}_{l}^{(i)}$.
    One popular framework for this is the Bradley-Terry (BT) model (Bradley and Terry,
    [1952](#bib.bib4)), assuming that human preferences can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3.1) |'
  prefs: []
  type: TYPE_TB
- en: 'Given this reward function $r^{*}$. While the ultimate goal of preference fine-tuning
    is to find the *unconstrained* optimum of the reward function, in practice, we
    often replace the reward function with a reward model. Since the reward model
    is erroneous, we apply KL-constraint to prevent exploitation in the reward model.
    To align our results with typical preference fine-tuning procedures, we will consider
    such a KL-constrained reward optimization as our fine-tuning goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\!\!\!\!\max_{\pi_{\theta}}\leavevmode\nobreak\ $ |  | (3.2)
    |'
  prefs: []
  type: TYPE_TB
- en: The regularizer, weighted by $\beta$ under the reverse KL divergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reward model training. In order to fine-tune an LLM policy $\pi_{\theta}(\mathbf{y}|\mathbf{x})$).
    Explicit reward models are trained using the following classification objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3.3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r_{\theta}(\mathbf{x},\mathbf{y})=\beta\left[\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})\right].$
    |  | (3.4) |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Characterizing Fine-Tuning Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With a reward model $r_{\phi}(\mathbf{x},\mathbf{y})$. Since we cannot empirically
    investigate all of these methods, we group them into different categories (summary
    shown in [Table 1](#S3.T1 "In 3.2 Characterizing Fine-Tuning Methods ‣ 3 Characterizing
    And Unifying Preference Fine-Tuning Methods ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")). In particular, we are interested in whether
    these methods employ:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*on-policy sampling*: an explicit sampling of new responses from the policy
    (e.g., PPO, REINFORCE) or purely learning from offline data (e.g., RWR, DPO, IPO)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*on-policy sample reuse*: for only those approaches that perform on-policy
    sampling, whether the approach makes more than one gradient update on a given
    prompt-response $(\mathbf{x},\mathbf{y})$ for PPO, online RWR)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*negative gradient*: whether the approach explicitly minimizes a loss that
    attempts to “push-down” likelihood on certain responses by multiplying the gradient
    of their likelihood with a negative coefficient (e.g., contrastive methods such
    as DPO; RL methods REINFORCE, PPO)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On-policy RL approaches such as PPO (Schulman et al., [2017](#bib.bib42)) and
    REINFORCE (Williams, [1992](#bib.bib51)) explicitly sample new responses from
    the current snapshot of the learned policy, $\mathbf{y}_{i}\sim\pi_{\theta}(\cdot|\mathbf{x}_{i})$,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{\prime}\leftarrow\theta-\eta\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}},\mathbf{y}\sim\pi_{\theta}(\cdot&#124;\mathbf{x})}\left[\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})\cdot\bar{r}_{\phi}(\mathbf{x},\mathbf{y})\right]\leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \text{(REINFORCE)},$ |  | (3.5) |'
  prefs: []
  type: TYPE_TB
- en: is the gradient update employed by REINFORCE, where $\bar{r}_{\phi}(\mathbf{x},\mathbf{y})$
    corresponds to a normalized estimate of the reward model’s predictions over a
    batch of samples drawn from the policy. As we discuss in more detail in [Section D.1](#A4.SS1
    "D.1 Score/Reward Standardization ‣ Appendix D Additional Algorithmic Details
    ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data")), using a normalized reward estimate instead of directly the raw reward
    value helps reduce the variance of the policy gradient estimate. High variance
    gradients slow down convergence and even sometimes lead to sub-optimal solutions
    in deep RL (Mei et al., [2022](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the use of normalized reward estimates, policy gradient approaches behave
    distinctly from maximum likelihood supervised learning: a policy gradient update
    also updates the parameters $\theta$. This means that on-policy RL also has a
    form of the “negative gradient”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO differs from REINFORCE because it employs *sample reuse* in addition to
    on-policy sampling: unlike REINFORCE which only performs a single gradient update
    on a response sampled from the current policy, PPO can utilize a response for
    several policy updates. To prevent making updates on overly off-policy responses,
    there is a mechanism in place to filter responses by the magnitude of the importance
    ratio between the current policy $\pi_{\theta}(\mathbf{y}|\mathbf{x})$ and the
    data collection policy.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also remark that while on-policy methods do generate new rollouts
    from the policy, these responses are still scored by a reward model (and not the
    ground truth reward function, i.e., humans). Since reward labels come from a reward
    model, on-policy preference fine-tuning approaches are instances of offline model-based
    RL (Yu et al., [2021](#bib.bib58), [2020](#bib.bib57); Kidambi et al., [2020](#bib.bib25))
    methods that run on-policy rollouts against a learned dynamics and reward model
    (due to the single step nature of preference fine-tuning, there is no dynamics
    model).
  prefs: []
  type: TYPE_NORMAL
- en: '| Fine-Tuning Approach | On-Policy Sampling | Sample Reuse | Negative Gradient
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PPO | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| REINFORCE | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| DPO, IPO, and variants | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pref-FT, Binary FeedMe | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| offline RWR, offline Best-of-N | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReST, RWR, online Best-of-N | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Grouping various fine-tuning methods along the axes on-policy sampling,
    sample reuse, and negative gradient. Since offline methods do not collect on-policy
    data, the question of discarding or reusing on-policy samples is not applicable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On-policy supervised approaches such as RAFT (Dong et al., [2023](#bib.bib13)),
    ReST (Gulcehre et al., [2023](#bib.bib19)), and SuperHF (Mukobi et al., [2023](#bib.bib32))
    iteratively minimize a weighted maximum likelihood loss inspired by Peters and
    Schaal ([2007](#bib.bib36)); Korbak et al. ([2022](#bib.bib28)). For a given prompt
    $\mathbf{x}_{i}$ as in the case of reward-weighted regression (RWR) or obtain
    the subset of $K$ to a scalar value conditioned on other responses $\mathbf{y}_{i}^{k}$,
    these methods maximize:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'These algorithms employ sample reuse because they operate in a “batched” online
    fashion: instead of performing *exactly one* gradient step on a given model sample;
    RWR, ReST, and SuperHF run more gradient updates, after which new samples are
    drawn. However, since these methods only maximize likelihood (i.e., only positive
    multipliers), there is no negative gradient effect.'
  prefs: []
  type: TYPE_NORMAL
- en: Fully offline methods like DPO and IPO run contrastive training on the preference
    dataset $\mathcal{D}_{\text{pref}}$. Despite no on-policy sampling, contrastive
    loss between winning and losing responses explicitly attempts to reduce log-likelihood
    ratio $\log\left(\frac{\pi_{\theta}(\mathbf{y}|\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})}\right)$.
    Another offline method is Pref-FT (Dubois et al., [2024](#bib.bib14)) which runs
    supervised fine-tuning on preferred responses. These methods in general are akin
    to offline model-free methods, in that no reward model is utilized by these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Research Questions and Analysis Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our goal is to understand the behaviors of various procedures for fine-tuning
    language models. As discussed above, typically these methods differ along the
    use of on-policy sampling (with additional differences pertaining to sample reuse)
    and the presence of a negative gradient. We build a setup to understand these
    differences empirically by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 1: When does on-policy sampling improve over offline fine-tuning,
    even though on-policy samples are annotated by a reward model, which itself is
    learned from offline data? Is sample reuse useful or harmful for on-policy methods?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 2: When does an explicit negative gradient help the discovery of effective
    policies compared to maximum likelihood approaches such as distilling the Best-of-N
    policy?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question 3: Does on-policy sampling offer complementary benefits to negative
    gradient, resulting in better performance with effective contrastive approaches
    (e.g., DPO)?'
  prefs: []
  type: TYPE_NORMAL
- en: To gain practically useful and actionable insights, we must answer these questions
    in the context of coverage and geometric relations between the training data,
    reference policy, and the reward function. These relations affect the shape of
    the optimally fine-tuned policy and dictate the dynamics of various objectives
    under consideration. We consider specific conditions and relations that we discuss
    next.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Coverage Conditions and Geometric Relationships
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dynamics of the KL-constrained surrogate optimization problem ([Equation 3.2](#S3.E2
    "In 3.1 Preliminaries and Notation ‣ 3 Characterizing And Unifying Preference
    Fine-Tuning Methods ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data")) depends on the geometric alignment between the ground-truth
    reward function $r^{*}$ also dictates the correctness of reward estimates and
    hence controls the efficacy of the surrogate fine-tuning optimization. Likewise,
    the performance of purely offline methods (e.g., offline best-of-N or contrastive
    methods such as offline DPO) that do not use a reward model also depends on the
    relative geometric alignment between $r^{*}$) and also on the relative coverage
    of preference data (i.e., the lower the coverage, the harder it is to discover
    high-reward responses). To understand the efficacy of various methods, we consider
    multiple scenarios that differ along these two factors:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[C1]: the geometric alignment between the ground-truth reward function $r^{*}$.
    This concept is analogous to that of a “concentrability coefficient” (Munos and
    Szepesvári, [2008](#bib.bib33)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[C2]: the coverage of the preference data used to train the surrogate reward
    model $r_{\phi}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Understanding the behavior of various approaches as a function of these factors
    will allow us to better understand the performance of various approaches on downstream
    fine-tuning in terms of problem geometry [C1] and statistical learning considerations [C2].
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Tasks and Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We construct a variety of didactic and LLM tasks that allow us to gain intuition
    for performance of different methods under various scenarios grouped along relationships
    [C1] and [C2].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d4ecd0db8df19edc67464f81e2fcede.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The didactic bandit problem which we use for our analysis in this
    paper. Reference policy initialization and reward slice for each token (the total
    reward is a mean of token-level rewards). The optima of reward functions $\mathbf{R}_{1}$
    occur in low-density and high-density regions respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Didactic $N$ of size $100$ is a sequence of $N=10$ is roughly aligned with the
    mode of the reference policy. We hypothesize that on-policy sampling will be crucial
    to optimize reward function $\mathbf{R}_{1}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b1929eefa9b138fd53761aea7680f59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Word length distribution. Above, we show the word length distribution
    for the preferred and dispreferred completions of the Left: min and Right: skew
    synthetic LLM datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthetic LLM fine-tuning problems. Next, we will generalize our intuitions
    from bandit problems to the LLM setting. Instead of directly experimenting with
    human preferences, we first study two synthetic problems that utilize hand-crafted
    reward functions, which can be approximated via reward models. Access to functional
    forms of these hand-crafted reward functions will enable us to track the ground-truth
    objective throughout training to see if our insights about various approaches
    under condition [C1] will hold even when learning against a reward model. Subsequently,
    we run this experiment with an altered skewed preference data distribution (see
    Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Tasks and Datasets ‣ 4 Research Questions and
    Analysis Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data")) to understand the effect of coverage conditions [C2]. We consider two
    reward functions: (1) one that minimizes the response length (“Min Length”), analogous
    to $\mathbf{R}_{1}$. The Skew Length scenario skews the preference data in the
    Min Length problem scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: Full-scale LLM fine-tuning. Finally, we scale up our study to full-scale LLMs,
    with real preference data. Recent work (Singhal et al., [2023](#bib.bib44)) shows
    that preference labels are usually biased towards much longer responses, indicating
    that preference fine-tuning usually admits a geometric relationship where the
    mode of the reward function is distinct from the mode of human data (and hence,
    any reference policy). For the majority of our experiments, we use preference
    datasets from the AlpacaFarm benchmark (Dubois et al., [2024](#bib.bib14)). We
    also scale up our experiments to UltraChat (Ding et al., [2023](#bib.bib12)),
    a $\sim 10$ times larger dataset with responses from many strong LLMs such as
    GPT 4 and GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 A Generic Fine-Tuning Algorithm Encapsulating All Axes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To systematically analyze the behavior of fine-tuning methods that differ along
    the axes discussed in [Section 3.2](#S3.SS2 "3.2 Characterizing Fine-Tuning Methods
    ‣ 3 Characterizing And Unifying Preference Fine-Tuning Methods ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data"), in this section, we introduce
    a generic algorithm with different hyperparameters associated with each axes.
    With a generic algorithm of this sort, we will be able to answer our research
    questions by varying each hyperparameter. Our unified practical algorithm is shown
    [Algorithm 1](#alg1 "In 4.3 A Generic Fine-Tuning Algorithm Encapsulating All
    Axes ‣ 4 Research Questions and Analysis Setup ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data"). While on-policy algorithms perform
    steps 1 and 2 of on-policy data collection with a reward model, purely offline
    methods (e.g., DPO and RWR) utilize preference data directly.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 A Unified Fine-Tuning Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: for training iterations do     (1) Sample $B/C$ responses for $\frac{B}{C}$,
    with rewards drawn           from the learned reward model $\widehat{r}_{\phi}(\mathbf{y}|\mathbf{x})$,
    each with $M$ prescribed by the fine-tuning method.         end for     end forend for
  prefs: []
  type: TYPE_NORMAL
- en: 'To study the impact of on-policy sampling, we vary the extent to which updates
    are made on data from the current policy. We can control this by two means in
    [Algorithm 1](#alg1 "In 4.3 A Generic Fine-Tuning Algorithm Encapsulating All
    Axes ‣ 4 Research Questions and Analysis Setup ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data"): (1) by varying the total number
    of samples $|\mathcal{D}|=\frac{B}{C}\times C=B$ of on-policy samples (i.e., a
    larger $T$ is larger. While both approaches enable us to control how on-policy
    an algorithm is, approach (1) does not reuse samples (since $\mathcal{D}$ in the
    dataset to understand the role of the negative gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Empirical Analysis Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will present the results of our empirical study to answer
    our research questions. To answer each question, we will begin by studying the
    didactic bandit problem with the ground-truth reward function, followed by synthetic
    and then full-scale LLM fine-tuning problems.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1 Question 1: The Role of On-Policy Sampling'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand the role of on-policy sampling, we will investigate if on-policy
    sampling can improve performance for several approaches followed by making conclusions
    regarding sample reuse.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We first study on-policy sampling as a function of the geometric relationship
    [C1] in our bandit setting (see [Figure 2](#S4.F2 "In 4.2 Tasks and Datasets ‣
    4 Research Questions and Analysis Setup ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")), with no sampling error. Then, we will
    extend our conclusions to the LLM setting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc1e440ec2de6b93bb8e0c9fd83493bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: On-policy sampling on bandit problems. Performance of on-policy best-of-N
    as a function of the data sampled in each iteration. Larger batch sizes result
    in more off-policy updates. Left: (i) reward vs update step for $\mathbf{R}_{1}$,
    but less severe degradation for $\mathbf{R}_{2}$, where peaks in the reference
    policy and reward function are more aligned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Didactic bandit problems.  [Figure 4](#S5.F4 "In 5.1.1 Takeaway 1: On-Policy
    Sampling in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of
    On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data") shows that given a fixed amount
    of total data budget, *sampling data more frequently from more recent policies*,
    but in smaller batches, results in better performance with both $\mathbf{R}_{1}$,
    $\mathbb{D}_{\text{KL}}(\pi_{\theta}||\pi_{\text{gen}})$ results in higher peak
    values of this divergence during training indicating further deviation from the
    data at intermediate times during training. This means that being more on-policy
    corresponds to better performance and faster convergence for best-of-N.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, we also note in [Figure 4](#S5.F4 "In 5.1.1 Takeaway 1: On-Policy
    Sampling in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of
    On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data") that the performance degradation
    with more off-policy updates is substantially milder for $\mathbf{R}_{2}$, indicating
    that when the peak in the reward function lies in the high likely regions of the
    reference policy, a higher degree of off-policy updates is tolerable.'
  prefs: []
  type: TYPE_NORMAL
- en: '| [C1] $\downarrow$ and $\pi_{\text{ref}}$ overlap |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| peaks of $r^{*}$ |'
  prefs: []
  type: TYPE_TB
- en: '| peaks of $r^{*}$ Skew Length |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Coverage conditions and geometric relations that we study with synthetic
    LLM fine-tuning data. The three settings we study differ in terms of overlap between
    $\pi_{\text{ref}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthetic LLM problems. In this problem setting, we optimize the policy against
    a reward model, which is learned from preference data. Per [Section 4.2](#S4.SS2
    "4.2 Tasks and Datasets ‣ 4 Research Questions and Analysis Setup ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"), we construct
    three scenarios that differ along geometric ([C1]) and coverage ([C2]) conditions
    as depicted in [Table 2](#S5.T2 "In 5.1.1 Takeaway 1: On-Policy Sampling in the
    Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling
    ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"). The peak of the reward in the Min Length scenario
    appears in the less likely regions of $\pi_{\text{ref}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9952493d5b740fd5fc70ded1604622d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: On-policy sampling for PPO in the Min Length scenario. This plot
    keeps the minibatch size $M$). Left: average completion length (lower the better),
    and Right: proxy reward vs gradient steps. Being more on-policy results in better
    performance. The mini-batch size $M$ used for gradient updates is kept fixed to
    avoid confounders arising from the use of stochastic optimization procedures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We present our results for one algorithm in detail (in this case, PPO) ([Figures 5](#S5.F5
    "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance
    ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"),
    [6](#S5.F6 "Figure 6 ‣ 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model
    Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical
    Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data") and [7](#S5.F7 "Figure 7 ‣ 5.1.1 Takeaway 1: On-Policy Sampling
    in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")) and then present a summary bar chart showing
    that our conclusions also transfer to other algorithms (such as REINFORCE and
    RWR) ([Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward
    Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣
    5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data")). Extending insights from the bandit problem, in
    the Min Length scenario, we find that being more on-policy (i.e., a smaller $B$
    and $B=256$. This indicates that with a significant overlap between the preference
    data and the reference policy, on-policy sampling still leads to better performance
    with fewer updates. We also find similar trends across on-policy variants of RWR
    and REINFORCE, where modulo training instabilities, being more on-policy results
    in better performance ([Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy Sampling
    in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data"); Min Length).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1cae1c97dd8996680f44c1ed1685fa6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: On-policy sampling for PPO in the Mode Length scenario. In this case,
    since the peak in the reward function and the highly likely regions of the reference
    policy are close, we find that the degree of on-policyness does not significantly
    affect performance. Left: distance to mode i.e., |completion length - average
    length in the dataset| (lower the better), Right: proxy reward vs gradient steps.
    As optimal policy $\pi^{*}$ used for the gradient update is kept fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Mode Length scenario, where the preferred response for each preference
    pair are those that are closest to the average length in the dataset (203), varying
    the degree of on-policy sampling by adjusting the sampling frequency largely does
    not affect either the proxy or gold reward for PPO ([Figure 6](#S5.F6 "In 5.1.1
    Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance ‣ 5.1
    Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")). We make similar
    observations for other algorithms: [Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy
    Sampling in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of
    On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"); Mode Length: different degrees
    of on-policyness perform similarly, except the more on-policy runs sometimes exhibit
    instability. This is in agreement with the results from the bandit setting above:
    when the peak in the reward function lies in highly likely regions under the reference
    policy, on-policy sampling has minor effect and more off-policy configurations
    of the algorithm can perform similarly too.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e45086ce03467fcb1d29dc627b95f314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: On-policy sampling for PPO on the Skew Length scenario. Being more
    on-policy results in faster convergence and better performance. Left: average
    completion length (lower the better), and Right: proxy reward vs gradient steps.
    Being more on-policy results in better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to evaluate the robustness of these findings under more challenging
    coverage conditions, we deliberately skew the length distribution in the preference
    dataset to make it distinct from the reference policy (called Skew Length). Concretely,
    with a 95% probability, we truncate the length of the response by sampling a length
    from an exponential distribution, which naturally leads to a shorter completion
    length. The remaining 5% of samples are drawn from the standard SFT policy to
    simulate the broader coverage for the preference data. Overall, the resulting
    data admits a significantly skewed distribution over response lengths, as visualized
    in [Figure 3](#S4.F3 "In 4.2 Tasks and Datasets ‣ 4 Research Questions and Analysis
    Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
    Not only does the peak in the reward function now appear in less likely regions
    of the reference policy, but to succeed, an optimization algorithm must now do
    the required heavy lifting to shift the probability mass to the low-density regions
    of the response space that maximize reward.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/160778de12b6c18b293005f46921c218.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Summary: effect of on-policy sampling on synthetic LLM problems.
    Average gold reward over the course of training for RWR, and REINFORCE with different
    $B$-axis is small), with performance differences largely due to instability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our detailed results of running PPO in this setting are shown in [Figure 7](#S5.F7
    "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance
    ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
    In this setting, we still find that more on-policy updates lead to a higher gold
    reward with PPO. In addition, we also observe much larger gaps in proxy reward
    values attained at any given gradient step compared to the Min Length scenario,
    in favor of on-policy sampling. For other algorithms, we also observe strong and
    clear trends supporting that on-policy sampling with a smaller but frequently
    sampled batch results in better performance as shown in the summary plot (see
    [Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model
    Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical
    Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"); Skew Length).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ec6c49be2b18da1badaeee35a03e1e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Effect of on-policy sampling on AlpacaFarm with a fixed mini-batch,
    but varying batch size $B$ makes updates more off-policy and this results in lower
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Full-scale LLM problems. Finally, we evaluate if our insights transfer to the
    full-scale AlpacaFarm setup. We use a Pythia-1.4B model as our reference policy
    and generate two responses per prompt. We label the preferred and dispreferred
    responses with a gold reward model of human preferences from AlpacaFarm to construct
    a preference dataset. [Figure 9](#S5.F9 "In 5.1.1 Takeaway 1: On-Policy Sampling
    in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data") shows that our intuitions from the simple
    bandit and synthetic LLM experiments transfer to this real preference learning
    task, as making updates on only on-policy samples leads to higher gold reward
    for both on-policy RWR and REINFORCE.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy Data'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the previous section, exactly one gradient step was taken on a given sample
    and we found that making updates on stale data was not helpful due to off-policy
    updates. Is there any scenario under which we can still attain good policy performance
    despite employing off-policy updates? In this section, we will answer this question,
    and show that it might be possible to learn with off-policy updates for some algorithms
    if we are allowed to make more than one update on a given sample. Of course, a
    substantial amount of sample reuse is detrimental since it would lead to more
    off-policy updates, thus leading to statistical or even propensity overfiting (Swaminathan
    and Joachims, [2015](#bib.bib46)) for some methods, but it is reasonable to surmise
    that some amount of sample reuse can help. To study sample reuse, we compare methods
    when $$T></math> gradient steps can be made on a given sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f53e4ea644f2f6902b20cef07b88e02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Effect of on-policy sample reuse on bandit problems. Reward vs gradient
    steps for a different number of inner iteration steps, $T$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We study sample reuse for on-policy RWR in the bandit setting in [Figure 10](#S5.F10
    "In 5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy
    Data ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
    While increasing $T$; $T=10$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthetic LLM problems. We also evaluate the effect of sample reuse on synthetic
    LLM problems. In this case, we study two algorithms PPO and on-policy best-of-N
    to be able to understand the effect of sample reuse on multiple algorithms. In
    contrast to the performance degradation with off-policy updates induced due to
    stale samples in PPO, we find that off-policy updates induced due to sample reuse
    do not hurt performance ([Figure 11](#S5.F11 "In 5.1.2 Takeaway 2: On-Policy Sample
    Reuse Can Enable Leveraging Off-Policy Data ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data"); PPO), with even $T=8$ to $2$, i.e., performing
    two gradient updates on each sample improves the golden reward for best-of-N ([Figure 11](#S5.F11
    "In 5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy
    Data ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data");
    Best-of-N) within a given data sampling budget.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31f9c4fb0f84f4f16a20af4c6770d37d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Effect of on-policy sample reuse in the Min Length scenario. Average
    completion length (i.e., the lower the better) vs gradient steps for a different
    number of inner iteration steps, $T$ implies that the algorithm is more off-policy.
    Observe that some sample reuse can improve sample efficiency (T = 2 outperforms
    T = 1), but excessive sample reuse can hurt performance. Also note that algorithms
    with mechanisms to control off-policy updates such as PPO are suited to perform
    better in the off-policy sample reuse setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Why do PPO and best-of-N respond differently to sample reuse? We believe that
    this is because PPO employs an off-policy correction, and hence, significantly
    off-policy samples do not contribute to the gradient, addressing the well-known
    challenge of propensity overfitting (Swaminathan and Joachims, [2015](#bib.bib46)).
    This is not the case with on-policy best-of-N, where excessive sample reuse can
    hurt exploration, because training on old samples with a log-likelihood loss push
    the current policy to be close to the stale data-generating policy. That said,
    more than one gradient step can still be useful when presented with a fixed data
    budget, unless it bottlenecks exploration of high reward regions.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SS1.SSS2.p5.pic1" class="ltx_picture" height="119.27" overflow="visible"
    version="1.1" width="656.13"><g transform="translate(0,119.27) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 15 97.13)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Takeaways
    for on-policy sampling</text></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 12.82)"><foreignobject width="612.83" height="78.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">On-policy sampling generally
    improves performance and efficiency, especially in cases when the peak of reward
    appears farther from the reference policy, even when the reward model is learned
    from the same preference dataset that methods without on-policy learning also
    use. In some cases, sample reuse can reduce the dependency on on-policy sampling
    of data, but it presents a tradeoff by reducing the exploration of the response
    space.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: '5.2 Question 2: The Role of Negative Gradient'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand the role of negative gradient, we will compare contrastive algorithms
    such as DPO and IPO with maximum likelihood methods such as RWR (or Pref-FT, which
    attempts to increase the likelihood of the preferred response only) and best-of-N
    in a fully offline setting, where no new on-policy samples are used. We will also
    aim to understand the mechanisms behind these methods.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.1 Takeaway 1: Negative Gradient Enables Faster Convergence Amongst Offline
    Methods'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e149bc4cf2a558e8b93cab24a4622b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Negative gradients on the didactic bandit problems. Average reward
    during training and the KL-reward trade-off for four algorithms in the fully offline
    setting: best-of-N (no negative gradient), RWR (no negative gradient), best-of-N
    + an explicit negative gradient on dispreferred actions, and IPO (with negative
    gradient). Negative gradient helps find a better policy by aggressively pushing
    down the likelihood of bad actions, and this leads to larger KL values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by comparing a representative set of offline algorithms on the didactic
    bandit problem. These methods include those that do not use a contrastive update
    on the didactic bandit problem, namely offline supervised approaches, Best-of-N
    and offline RWR, and offline IPO (Gheshlaghi Azar et al., [2023](#bib.bib18)),
    a representative offline fine-tuning method which uses a contrastive negative
    gradient term. We also consider a variant of best-of-N where we explicitly add
    a term to the loss function that attempts to minimize the likelihood of the dispreferred
    response akin to unlikelihood (Welleck et al., [2020](#bib.bib50)) (see [Section G.2](#A7.SS2
    "G.2 Algorithmic Details ‣ Appendix G More on Didactic Bandit Problems ‣ Appendices
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    for more details). In [Figure 12](#S5.F12 "In 5.2.1 Takeaway 1: Negative Gradient
    Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2: The Role
    of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"), we find that IPO and best-of-N
    + negative gradient learn a better policy from an offline dataset collected from
    sub-optimal $\pi_{\mathrm{ref}}$ are far away from each other). While best-of-N
    attains a higher reward when the reward function is given by $\mathbf{R}_{2}$,
    it still underperforms IPO. We suspect that this is because maximizing likelihood
    on some responses alone is not enough to steer the learned policy away meaningfully
    away from $\pi_{\mathrm{ref}}$ and $r^{*}$. This is possibly due to the much smaller
    space of possible tokens and responses, where maximum likelihood methods perform
    well enough.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6fd1d65766684761d02c524c2368478f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Negative gradients in synthetic LLM problems. Completion length
    (inverse of the true reward) for three offline algorithms. DPO outperforms Pref-FT
    and offline RWR in Min Length and the Skew Length settings, where the peak in
    $r^{*}$ are misaligned. For the Mode Length setting, all of the algorithms perform
    similarly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthetic LLM problems. Our experiments in the synthetic LLM setting corroborate
    this finding. Here we compare Pref-FT with DPO (with negative gradients). In the
    Min Length setting, we find in [Figure 13](#S5.F13 "In 5.2.1 Takeaway 1: Negative
    Gradient Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2:
    The Role of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") that DPO significantly outperforms
    Pref-FT. On the other hand, when the peak in the ground-truth reward appears in
    high-likely regions of the reference policy and the preference data $\mathcal{D}_{\text{pref}}$
    is covered by the preference dataset $\mathcal{D}_{\text{pref}}$, we also find
    that DPO is much more effective in driving the policy further from the reference
    initialization and outperforms Pref-FT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/602d77636f569e9dc05989ccff47aab1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Negative gradients in AlpacaFarm (left) and UltraFeedback (right)
    for offline methods. We plot the increase in average gold reward compared to the
    reference model for different offline approaches. Algorithms with a negative gradient
    such as DPO outperform approaches such as Pref-FT not utilizing any negative gradient
    term.'
  prefs: []
  type: TYPE_NORMAL
- en: Full-scale LLM fine-tuning. Finally, we compare supervised Pref-FT and contrastive
    DPO when fine-tuning on actual preference data. In addition to AlpacaFarm, we
    also run experiments using the Ultra-Feedback (Ding et al., [2023](#bib.bib12))
    dataset. For the Ultra-Feedback dataset, we use different models (GPT-3.5, GPT-4)
    to generate responses to various prompts. The resulting dataset has a broader
    preference dataset distribution than $\pi_{\mathrm{ref}}$compared to methods that
    do not utilize a negative gradient (e.g., Pref-FT).
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.2 Takeaway 2: Mechanisms Explaining the Behavior of the Negative Gradient'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Having seen that using a negative gradient leads to much better performance,
    we next attempt to understand the mechanism behind this better performance. To
    do so, we visualize the evolution of the log-likelihoods of the preferred response
    and the dispreferred response in a held-out dataset as multiple gradient steps
    are taken on an offline preference optimization loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrastive training increases the gap between the likelihoods of preferred
    and dispreferred responses. Perhaps as expected, we find that DPO-style contrastive
    training is more effective at increasing the gap between the likelihoods of preferred
    and dispreferred responses compared to offline Pref-FT in several LLM settings:
    the synthetic LLM settings with Min Length and Skew Length, and full-scale AlpacaFarm
    and UltraFeedback settings ([Figure 15](#S5.F15 "In 5.2.2 Takeaway 2: Mechanisms
    Explaining the Behavior of the Negative Gradient ‣ 5.2 Question 2: The Role of
    Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data")). More concretely, note that the
    margin for Pref-FT largely converges to 0, whereas offline DPO can enable a larger
    margin.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/317735ad905e3047a71e6ab0a88b358c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Difference in likelihoods of preferred and dispreferred responses.
    DPO increases the log probability margin $\log\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})-\log\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})$
    more compared to non-contrastive methods such as Pref-FT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e63af5bd1ce6c3cd9a08cb3c150872b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: DPO implicit reward during training. We observe that with fewer
    prompts, contrastive methods can increase the implicit reward, $r_{\theta}(\mathbf{x},\mathbf{y})=\log\left(\pi_{\theta}(\mathbf{y}|\mathbf{x})\right)-\log\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})$,
    of the preferred response while reducing this quantity for the dispreferred response,
    however as the number of data points grows, this may not be possible and the likelihood
    of both positives and negatives might reduce.'
  prefs: []
  type: TYPE_NORMAL
- en: Changes in log likelihoods depend on model capacity, reference initialization,
    data size, and composition. The natural next question is if DPO-like objectives
    use the probability mass recovered by increasing the reward margin between $\mathbf{y}_{w}$
    in the dataset, then induced rewards will always decrease. This does not contradict
    our findings because this condition is not satisfied in typical fine-tuning pipelines
    where *both* $\mathbf{y}_{w}$ is obtained by first running supervised Pref-FT
    only on $\mathbf{y}_{w}$ and $\mathbf{y}_{l}$ on the bandit problem while varying
    the size of the preference dataset. Following standard protocols, both $\mathbf{y}_{l}$
    while reducing the likelihood of $\mathbf{y}_{l}$, dataset size, and composition,
    contrastive objectives such as DPO extrapolate, and this extrapolation might produce
    good or bad responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also observe a similar trend in full-scale LLM experiments in [Figure 17](#S5.F17
    "In 5.2.2 Takeaway 2: Mechanisms Explaining the Behavior of the Negative Gradient
    ‣ 5.2 Question 2: The Role of Negative Gradient ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"):
    we observe a decrease in the log-likelihoods of both the preferred and dispreferred
    responses throughout training on AlpacaFarm with small 1.4B Pythia policies. However,
    using a Mistral7B model to train a policy on the UltraFeedback dataset results
    in an increasing value of log-likelihood of $\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})$.
    In contrast, perhaps as expected, running Pref-FT increases the likelihoods of
    both $\mathbf{y}_{w}$ ([Figure 17](#S5.F17 "In 5.2.2 Takeaway 2: Mechanisms Explaining
    the Behavior of the Negative Gradient ‣ 5.2 Question 2: The Role of Negative Gradient
    ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f36dca2af44e9882a9cad842c4cc23ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: DPO reward estimates for Pref-FT and DPO on AlpacaFarm and UltraFeedback.
    For a Pythia-1.4B model trained on AlpacaFarm, DPO decreases the implicit reward,
    $r_{\theta}(\mathbf{x},\mathbf{y})=\beta\left[\log\pi_{\theta}(\mathbf{y}|\mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})\right]$
    and decrease the reward for $\mathbf{y}_{l}$, whereas Pref-FT increases both.
    In both cases, DPO leads to a higher margin than Pref-FT.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SS2.SSS2.p5.pic1" class="ltx_picture" height="119.27" overflow="visible"
    version="1.1" width="656.13"><g transform="translate(0,119.27) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 15 97.13)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Takeaways
    for negative gradients</text></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 12.82)"><foreignobject width="612.83" height="78.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">A negative gradient improves
    over offline supervised methods when the peak in the reward appears in less likely
    regions of $\pi_{\mathrm{ref}}$, model capacity is large, and $\pi_{\mathrm{ref}}$.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: '5.3 Question 3: On-Policy Sampling and Negative Gradients are Complementary'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on our findings that both on-policy sampling and negative gradients are
    independently effective, we now study if combining them would provide any additional
    benefits. To understand this, we empirically study a straightforward on-policy
    variant of DPO/IPO: instead of utilizing the PPO or Best-of-N objective on on-policy
    samples, for each prompt $\mathbf{x}$, and construct preference pairs by taking
    the higher reward completion as the preferred one and lower reward completion
    as the dispreferred one. This recipe is similar to concurrent works such as Rosset
    et al. ([2024](#bib.bib40)). Then we calculate the DPO/IPO loss on this preference
    dataset and update our model accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a50055e0669386e2cfc016f8e0996da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: On-policy sampling + negative gradients in bandit setup. Complimentary
    benefit of on-policy sampling and negative gradients. Online IPO (using both on-policy
    sampling and negative gradients) performs better than offline IPO (negative gradients
    but no on-policy sampling) and RWR (on-policy sampling but no negative gradients).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance on bandit and synthetic LLM problems. [Figure 18](#S5.F18 "In 5.3
    Question 3: On-Policy Sampling and Negative Gradients are Complementary ‣ 5 Empirical
    Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data") shows that the on-policy version of IPO achieves both faster
    convergence and better performance compared to the offline version, for both $\mathbf{R}_{1}$
    in the didactic bandit problem. We also ran on-policy DPO in synthetic LLM problems
    we studied and found it to converge significantly faster and to a better solution
    than offline DPO, on-policy RL, and on-policy variants of supervised learning
    approaches as shown in [Figure 19](#S5.F19 "In 5.3 Question 3: On-Policy Sampling
    and Negative Gradients are Complementary ‣ 5 Empirical Analysis Results ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"). We also find
    that on-policy versions of contrastive approaches exhibit favorable computational
    vs wall-clock time tradeoffs compared to purely on-policy RL methods and even
    offline contrastive methods that may not find as good solutions as their on-policy
    counterparts (see [Appendix B](#A2 "Appendix B Computational vs Wall-Clock Time
    Tradeoff for Various Methods ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b6f119930c63ad1d9f2a91af4972db1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: On-policy sampling + negative gradients in LLM length experiments.
    Complimentary benefit of on-policy sampling and negative gradients on the synthetic
    LLM length experiments. On-policy DPO performs the best where optimal policy and
    reference policy lies far from each other (min length and skew length), and all
    algorithms perform similarly when these two policies are close (mode length).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Why can on-policy versions of contrastive methods perform better than on-policy
    RL? We saw in [Section 5.2.1](#S5.SS2.SSS1 "5.2.1 Takeaway 1: Negative Gradient
    Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2: The Role
    of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data") that offline contrastive training
    with a negative gradient was effective at quickly reorganizing probability mass
    to high-reward responses covered by the preference data. When combined with on-policy
    sampling, this behavior results in faster convergence: for any given batch of
    on-policy data, contrastive training with a negative gradient can quickly reconfigure
    the policy distribution within the support of the on-policy data obtained thus
    far (i.e., it provides a stronger, low-variance learning signal). Similarly to
    how best-of-N + negative gradient outperforms vanilla best-of-N but underperforms
    DPO in [Figure 12](#S5.F12 "In 5.2.1 Takeaway 1: Negative Gradient Enables Faster
    Convergence Amongst Offline Methods ‣ 5.2 Question 2: The Role of Negative Gradient
    ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"), PPO also improves over RWR without a negative gradient
    term (in the bandit setting this corresponds to a better reward-KL tradeoff in
    [Figure 18](#S5.F18 "In 5.3 Question 3: On-Policy Sampling and Negative Gradients
    are Complementary ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data") and in the synthetic LLM setting
    this appears in final performance), but it is still unable to match on-policy
    DPO in [Figure 19](#S5.F19 "In 5.3 Question 3: On-Policy Sampling and Negative
    Gradients are Complementary ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data"). Note that this does not
    mean that on-policy DPO would always outperform PPO, but that it might be a good
    choice for users to experiment with on-policy versions of contrastive methods.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.SS3.p4.pic1" class="ltx_picture" height="119.27" overflow="visible"
    version="1.1" width="656.13"><g transform="translate(0,119.27) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 15 97.13)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Takeaways
    for on-policy sampling + negative gradient</text></g></g></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 12.82)"><foreignobject width="612.83"
    height="78.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">On-policy
    sampling and offline negative gradients present complementary benefits, in that
    the best offline loss function with negative gradients can be used to train on
    on-policy data, improving over on-policy RL or supervised learning. Conceptually,
    while sampling responses on policy provides coverage of the response space, an
    effective negative gradient loss provides a stronger learning signal given a set
    of samples. It can also result in computational benefits.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conceptual Unification and Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With empirical results showing the benefits of on-policy sampling and negative
    gradient for preference fine-tuning of LLMs, in this section, we attempt to conceptually
    understand the benefits by building a mental model. In this section, we will first
    unify these seemingly distinct notions of on-policy sampling and negative gradient
    into a unified notion of mode-seeking objectives, and contrast them against mode-covering
    maximum likelihood objectives. Then, we will contrast the learning dynamics of
    the reverse KL-divergence, a representative mode-seeking objective against the
    mode-seeking forward KL-divergence (i.e., the supervised learning loss) to intuitively
    explain some of our findings.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Seeking Modes Unifies On-Policy Sampling and Negative Gradients
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we will show that the notion of mode-seeking divergences unifies
    on-policy sampling and negative gradients for the various objectives we investigated
    in the paper. Specifically, we show below that several on-policy RL methods that
    we studied optimize the reverse KL-divergence, and are hence mode-seeking, offline
    contrastive methods that employ a negative gradient are also mode-seeking, and
    finally, supervised weighted maximum likelihood approaches (e.g., offline Best-of-N,
    Pref-FT, Binary FeedMe) are mode-covering. First, we show that on-policy sampling
    leads to mode-seeking behavior. To do this, we prove that RL and supervised objectives
    combined on-policy sampling optimize the reverse KL divergence, which is known
    to be mode-seeking.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 6.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On-policy RL and on-policy weighted-likelihood methods optimize a regularized
    version of a reverse KL-divergence with respect to the optimal policy and are
    hence mode seeking.
  prefs: []
  type: TYPE_NORMAL
- en: A proof for Lemma [6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is shown in [Section C.1.1](#A3.SS1.SSS1 "C.1.1 On-policy Methods Are Mode-Seeking
    ‣ C.1 Unifying On-Policy Sampling and Negative Gradients via Mode-Seeking Divergences
    ‣ Appendix C More Details on Conceptual Unification and Theoretical Analysis ‣
    Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). Next, we show that offline contrastive methods that employ a negative
    gradient are also mode-seeking. While these approaches do not optimize the reverse
    KL-divergence, we can still show that the probability mass obtained by minimizing
    density on negative responses $\mathbf{y}_{l}$) compared to other categories.
    When the offline dataset consists of multiple high-reward categories, this preference
    to put more probability mass on the mode of the current policy results in mode-seeking
    behavior, compared to increasing probability mass on all high-reward categories.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 6.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\theta_{t}$. Consider contrastive approaches that induce a negative gradient
    under a functional form shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6.1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $c_{1}$. In contrast, weighted maximum likelihood without the negative
    gradient sets $c_{2}=0$, there always exists an appropriate dataset of positive
    and negative samples $\mathcal{D}$, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6.2) |'
  prefs: []
  type: TYPE_TB
- en: 'In addition, if the model class $\pi_{\theta}$ that satisfies this condition):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'then, we find that the likelihood of positives is larger (and similarly likelihood
    of negatives is smaller) when <math id=$$, i.e., when a negative gradient term
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\mathbb{E}_{\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l}\sim\mathcal{D}}\left[\log\pi_{\theta}(\mathbf{y}_{w}&#124;\mathbf{x})\right]\Big{&#124;}_{c_{2}></math>
    |  | (6.3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | <math id=$$ |  | (6.4) |'
  prefs: []
  type: TYPE_TB
- en: 'A proof for [Lemma 6.2](#S6.Thmtheorem2 "Lemma 6.2\. ‣ 6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is provided in [Section C.1.2](#A3.SS1.SSS2 "C.1.2 Contrastive Approaches
    (e.g., DPO/IPO) are Mode-Seeking ‣ C.1 Unifying On-Policy Sampling and Negative
    Gradients via Mode-Seeking Divergences ‣ Appendix C More Details on Conceptual
    Unification and Theoretical Analysis ‣ Appendices ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"). This result indicates that
    for appropriate negative responses, a contrastive update accelerates the rate
    of increase of probability mass on $\mathbf{y}_{w}$, which offline weighted maximum
    likelihood. This corresponds to mode-seeking behavior. The update induced by DPO
    admits a similar form (see the discussion after Equation 7 in Rafailov et al.
    ([2023](#bib.bib38))). This theoretical result also corroborates our findings
    in the experiments in Section [5.2.1](#S5.SS2.SSS1 "5.2.1 Takeaway 1: Negative
    Gradient Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2:
    The Role of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") regarding the negative gradient
    term. The gradient of IPO also admits a similar form ([Section C.1.2](#A3.SS1.SSS2
    "C.1.2 Contrastive Approaches (e.g., DPO/IPO) are Mode-Seeking ‣ C.1 Unifying
    On-Policy Sampling and Negative Gradients via Mode-Seeking Divergences ‣ Appendix
    C More Details on Conceptual Unification and Theoretical Analysis ‣ Appendices
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we note that purely offline versions of supervised methods such as RWR,
    ReST, and BoN, that only maximize weighted likelihood are mode-covering because
    these objectives can be shown to maximize the forward KL-divergence against the
    optimal policy (proof in [Section C.1.3](#A3.SS1.SSS3 "C.1.3 Supervised Offline
    Algorithms are Mode-Covering ‣ C.1 Unifying On-Policy Sampling and Negative Gradients
    via Mode-Seeking Divergences ‣ Appendix C More Details on Conceptual Unification
    and Theoretical Analysis ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")).
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 6.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider offline supervised methods that maximize weighted log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})=-\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}}}\left[\mathbb{E}_{\mathbf{y}\sim\pi_{\mathrm{ref}}(.&#124;\mathbf{x})}[\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{y})]\right]$ |  | (6.5) |'
  prefs: []
  type: TYPE_TB
- en: where $F(\mathbf{x},\mathbf{y})\geq 0$, there exists a response $\mathbf{y}$).
    Then these methods optimize a forward KL-divergence.
  prefs: []
  type: TYPE_NORMAL
- en: '6.2 Case Study: Mode-Seeking Reverse KL vs. Mode-Covering Forward KL'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Having seen that mode-seeking and mode-covering divergences can unify on-policy
    sampling and negative gradients, in this section, we perform a theoretical analysis
    to quantify the behavior of the two representative mode-seeking and mode-covering
    objectives: reverse KL (mode-seeking) and forward KL (mode-covering) objectives
    on categorical distributions, parameterized via independent logits. Our goal is
    to formalize the intuition that a mode-seeking objective can sharpen the probability
    mass on only certain high-reward regions, thereby leading to aggressive reorganization
    of probability mass. This helps corroborate our experiments that on-policy sampling
    in a reward model and offline negative sampling is still useful to quickly align
    the policy with the target distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Notation and setup. For this result, we will study training a categorical distribution
    $p(\mathbf{x})$ is an independent parameter. We train $p(\mathbf{x})$ of this
    gradient descent as $p_{t}$, induced by forward and reverse KL.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 6.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For any given distribution $p_{t}$ are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Forward KL: | $\displaystyle\log\frac{p^{f}_{t+1}(\mathbf{x})}{p_{t}(\mathbf{x})}=\eta\left(q(\mathbf{x})-p_{t}(\mathbf{x})\right)+\mathbb{Z}.$
    |  | (6.6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reverse KL: | $\displaystyle\log\frac{p^{r}_{t+1}(\mathbf{x})}{p_{t}(\mathbf{x})}=\eta\left(p_{t}(\mathbf{x})\left[\log\frac{q(\mathbf{x})}{p_{t}(\mathbf{x})}+\mathbb{D}_{\text{KL}}(p_{t}(\cdot)&#124;&#124;q(\cdot))\right]\right)+\mathbb{Z}^{\prime},$
    |  | (6.7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{Z}$ denote constant normalization factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a proof of [Lemma 6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2 Case Study: Mode-Seeking
    Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"), see [Section C.2](#A3.SS2 "C.2 Characterization of Gradients of Forward
    and Reverse KL ‣ Appendix C More Details on Conceptual Unification and Theoretical
    Analysis ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"). In principle, upon convergence, both the reverse and forward
    KL-divergences should find the optimally fine-tuned distribution, $q(\mathbf{x})$:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 6.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $p^{f}_{t+1}(\mathbf{x})$. Define $\Delta^{f}_{t}$, obtained from the forward
    and reverse divergences respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6.8) |'
  prefs: []
  type: TYPE_TB
- en: 'and $\Delta_{t}^{r}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reverse KL modifies probability mass more aggressively than the forward KL.
    If $\mathbf{x}_{1}$, $$\delta_{2}></math>.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reverse KL increases probability mass only on a subset of categories that equal
    target likelihoods. If $\mathbf{x}_{1}$, where $\mathbf{c}_{0}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reverse KL aggressively reduces probability mass on less-likely categories in
    the target distribution. If $\mathbf{x}_{1}$, where $\mathbf{c}_{1}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A proof of Theorem [6.5](#S6.Thmtheorem5 "Theorem 6.5\. ‣ 6.2 Case Study: Mode-Seeking
    Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is shown in [Section C.3](#A3.SS3 "C.3 Quantifying the Differences Between
    Forward and Reverse KL ‣ Appendix C More Details on Conceptual Unification and
    Theoretical Analysis ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"). Essentially, this theorem enlists several cases
    where the forward KL modifies probability mass in different amounts across various
    categories, but the reverse KL acts disproportionately. In particular, case 1
    says that the reverse KL exhibits more disproportionate probability mass changes
    on categories with equal likelihood $p_{t}(\mathbf{x})$ under certain conditions.
    Finally, case 3 shows that when the likelihood of a category is significantly
    larger than the target $q(\mathbf{x})$. In this case, while the forward KL will
    increase log probability ratios for both $\mathbf{x}_{1}$ value. These results
    highlight some scenarios under which the reverse KL can more efficiently re-organize
    probability mass across categories.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S6.SS2.p5.pic1" class="ltx_picture" height="119.27" overflow="visible"
    version="1.1" width="656.13"><g transform="translate(0,119.27) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 15 97.13)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Mode-seeking
    vs. mode-covering objectives for categorical distributions</text></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 12.82)"><foreignobject
    width="612.83" height="78.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Typically the benefits of mode-seeking behavior are more apparent
    when the model $p(\mathbf{x})$, reverse KL can quickly re-distribute probability
    mass to only a subset of the required categories likely in target distribution,
    within a few gradient steps.</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion, Conclusion, and Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We attempted to understand which components are particularly important for fine-tuning
    language models with preference data. Through extensive experiments on different
    fine-tuning problems in both didactic and LLM settings, we established that on-policy
    sampling is crucial for good performance especially when the peak in the ground-truth
    reward lies in less-likely regions of the reference policy initialization. That
    said, in practice, doing so requires preference datasets with broader coverage
    than the reference policy. We also showed that negative gradients can enable faster
    convergence and that objectives that induce a negative gradient are complementary
    to using on-policy sampling. Finally, we show that the notion of mode-seeking
    divergences unifies the notion of on-policy sampling and negative gradient. Our
    case study comparing forward and reverse KL divergences demonstrates the superiority
    of the reverse KL divergence in re-distributing probability mass efficiently,
    supporting our empirical findings pertaining to on-policy sampling and negative
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: While we conceptualize our observations, a limitation is that we don’t derive
    rigorous statistical guarantees in this work. As an example, we note that while
    the notion of concentrability coefficients (and associated guarantees) can potentially
    provide guarantees on on-policy sampling, to the best of our knowledge the notion
    of the negative gradient is not fully studied in the literature. We conjecture
    that negative gradient can perhaps be formalized statistically from the lens of
    providing a lower variance learning signal; it would be interesting for future
    work to formalize this. It would also be interesting to study more recent approaches
    based on minimax formulations (e.g., Munos et al. ([2023](#bib.bib34)); Yuan et al.
    ([2024](#bib.bib60)); Swamy et al. ([2024](#bib.bib47)); Chen et al. ([2024](#bib.bib8)))
    in our empirical and conceptual framework. Next, while we consider the coverage
    of preference data relative to that of the reference policy in our study, this
    is a simplification that does not account for the coverage of the pre-training
    distribution which future work can incorporate. Finally, we remark that our study
    does not explore the effect of reward model quality, which tends to also play
    a central role in LLM fine-tuning. It would be interesting to extend our analysis
    to incorporate the role of reward model quality and parameterization.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We would like to thank Yi Su, Rishabh Agarwal, Zhang-Wei Hong, Young Geng, Abitha
    Thankaraj, Yuxiao Qu, So Yeon Min, Yutong He, Kevin Li, Sukjun Hwang, Khurram
    Yamin, Charlie Snell, Amrith Setlur, Kaylee Burns, Eric Mitchell, and others in
    CMU Russ Lab, CMU Auton Lab, Stanford IRIS Lab, and Stanford Ermon Group for discussions
    and feedback. AK thanks Aleksandra Faust, George Tucker, and Sergey Levine for
    informative discussions. This research is supported by computational resources
    from Google TPU Research Cloud (TRC) and the National Science Foundation. FT thanks
    Ruslan Salakhutdinov for insightful suggestions during this project. AS gratefully
    acknowledges the support of the NSF Graduate Research Fellowship Program.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adolphs et al. (2022) Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar
    Sukhbaatar, and Jason Weston. The CRINGE Loss: Learning what language not to model.
    *arXiv e-prints*, art. arXiv:2211.05826, November 2022. [10.48550/arXiv.2211.05826](https:/doi.org/10.48550/arXiv.2211.05826).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahmadian et al. (2024) Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh
    Fadaee, Julia Kreutzer, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting
    reinforce style optimization for learning from human feedback in llms. *arXiv
    preprint arXiv:2402.14740*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bradley and Terry (1952) Ralph Allan Bradley and Milton E. Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952. ISSN 00063444. URL [http://www.jstor.org/stable/2334029](http://www.jstor.org/stable/2334029).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl
    Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
    Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie,
    Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum,
    Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii
    Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan,
    David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental
    limitations of reinforcement learning from human feedback. *Transactions on Machine
    Learning Research*, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=bx24KpJ4Eb](https://openreview.net/forum?id=bx24KpJ4Eb).
    Survey Certification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2024) Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kianté Brantley,
    Dipendra Misra, Jason D. Lee, and Wen Sun. Dataset Reset Policy Optimization for
    RLHF. *arXiv e-prints*, art. arXiv:2404.08495, April 2024. [10.48550/arXiv.2404.08495](https:/doi.org/10.48550/arXiv.2404.08495).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2024) Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan
    Gu. Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.
    *arXiv e-prints*, art. arXiv:2401.01335, January 2024. [10.48550/arXiv.2401.01335](https:/doi.org/10.48550/arXiv.2401.01335).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ContextualAI (2024) ContextualAI. Human-centered loss functions (halos), 2024.
    URL [https://github.com/ContextualAI/HALOs](https://github.com/ContextualAI/HALOs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coste et al. (2023) Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
    Reward Model Ensembles Help Mitigate Overoptimization. *arXiv e-prints*, art.
    arXiv:2310.02743, October 2023. [10.48550/arXiv.2310.02743](https:/doi.org/10.48550/arXiv.2310.02743).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu,
    Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting Language
    Models with High-quality Feedback. *arXiv e-prints*, art. arXiv:2310.01377, October
    2023. [10.48550/arXiv.2310.01377](https:/doi.org/10.48550/arXiv.2310.01377).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by
    scaling high-quality instructional conversations. *arXiv preprint arXiv:2305.14233*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie
    Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward
    ranked finetuning for generative foundation model alignment, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dubois et al. (2024) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan
    Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
    Alpacafarm: A simulation framework for methods that learn from human feedback,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eisenstein et al. (2023) Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad
    Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl,
    Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or Herding? Reward
    Model Ensembles Mitigate but do not Eliminate Reward Hacking. *arXiv e-prints*,
    art. arXiv:2312.09244, December 2023. [10.48550/arXiv.2312.09244](https:/doi.org/10.48550/arXiv.2312.09244).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethayarajh et al. (2023) Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe
    Kiela. Human-aware loss functions (halos). Technical report, Contextual AI, 2023.
    https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2022) Leo Gao, John Schulman, and Jacob Hilton. Scaling Laws for
    Reward Model Overoptimization. *arXiv e-prints*, art. arXiv:2210.10760, October
    2022. [10.48550/arXiv.2210.10760](https:/doi.org/10.48550/arXiv.2210.10760).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gheshlaghi Azar et al. (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal
    Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A General
    Theoretical Paradigm to Understand Learning from Human Preferences. *arXiv e-prints*,
    art. arXiv:2310.12036, October 2023. [10.48550/arXiv.2310.12036](https:/doi.org/10.48550/arXiv.2310.12036).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language
    modeling. *arXiv preprint arXiv:2308.08998*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret,
    and Mathieu Blondel. Direct Language Model Alignment from Online AI Feedback.
    *arXiv e-prints*, art. arXiv:2402.04792, February 2024. [10.48550/arXiv.2402.04792](https:/doi.org/10.48550/arXiv.2402.04792).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2023) Jian Hu, Li Tao, June Yang, and Chandler Zhou. Aligning Language
    Models with Offline Learning from Human Feedback. *arXiv e-prints*, art. arXiv:2308.12050,
    August 2023. [10.48550/arXiv.2308.12050](https:/doi.org/10.48550/arXiv.2308.12050).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2023) Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P.
    Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktäschel, and David Scott Krueger.
    Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks.
    *arXiv e-prints*, art. arXiv:2311.12786, November 2023. [10.48550/arXiv.2311.12786](https:/doi.org/10.48550/arXiv.2311.12786).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (23) Andrej Karpathy. minGPT. URL [https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khaki et al. (2024) Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, and Prathap Ramachandra.
    RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method
    for Alignment of Large Language Models. *arXiv e-prints*, art. arXiv:2402.10038,
    February 2024. [10.48550/arXiv.2402.10038](https:/doi.org/10.48550/arXiv.2402.10038).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kidambi et al. (2020) Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli,
    and Thorsten Joachims. Morel: Model-based offline reinforcement learning. *arXiv
    preprint arXiv:2005.05951*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirk et al. (2023) Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
    Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding
    the Effects of RLHF on LLM Generalisation and Diversity. *arXiv e-prints*, art.
    arXiv:2310.06452, October 2023. [10.48550/arXiv.2310.06452](https:/doi.org/10.48550/arXiv.2310.06452).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korbak et al. (2022) Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc
    Dymetman. On reinforcement learning and distribution matching for fine-tuning
    language models with no catastrophic forgetting. *Advances in Neural Information
    Processing Systems*, 35:16203–16220, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2024) Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg,
    Jonathan K. Kummerfeld, and Rada Mihalcea. A Mechanistic Understanding of Alignment
    Algorithms: A Case Study on DPO and Toxicity. *arXiv e-prints*, art. arXiv:2401.01967,
    January 2024. [10.48550/arXiv.2401.01967](https:/doi.org/10.48550/arXiv.2401.01967).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022) Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui
    Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text
    generation with reinforced unlearning. *Advances in neural information processing
    systems*, 35:27591–27609, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mei et al. (2022) Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba
    Szepesvari, and Dale Schuurmans. The role of baselines in policy gradient optimization.
    *Advances in Neural Information Processing Systems*, 35:17818–17830, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukobi et al. (2023) Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim,
    Gitta Kutyniok, Kush Bhatia, and Silas Alberti. SuperHF: Supervised Iterative
    Learning from Human Feedback. *arXiv e-prints*, art. arXiv:2310.16763, October
    2023. [10.48550/arXiv.2310.16763](https:/doi.org/10.48550/arXiv.2310.16763).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munos and Szepesvári (2008) Rémi Munos and Csaba Szepesvári. Finite-time bounds
    for fitted value iteration. *Journal of Machine Learning Research*, 9(5), 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munos et al. (2023) Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad
    Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist,
    Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier
    Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash Learning from
    Human Feedback. *arXiv e-prints*, art. arXiv:2312.00886, December 2023. [10.48550/arXiv.2312.00886](https:/doi.org/10.48550/arXiv.2312.00886).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters and Schaal (2007) Jan Peters and Stefan Schaal. Reinforcement learning
    by reward-weighted regression for operational space control. In *Proceedings of
    the 24th International Conference on Machine Learning*, pages 745–750\. ACM, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2024) Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea
    Finn. From r to $q^{*}$: Your language model is secretly a q-function. *arXiv
    preprint arXiv:2404.12358*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosset et al. (2024) Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce,
    Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language
    models to self-improve with general preferences. *arXiv preprint arXiv:2404.03715*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. *arXiv e-prints*,
    art. arXiv:1707.06347, July 2017. [10.48550/arXiv.1707.06347](https:/doi.org/10.48550/arXiv.1707.06347).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2024) Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn,
    Kushal Arora, and Thomas Kollar. A critical evaluation of ai feedback for aligning
    large language models. *arXiv preprint arXiv:2402.12366*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singhal et al. (2023) Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett.
    A long way to go: Investigating length correlations in rlhf. *arXiv preprint arXiv:2310.03716*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton et al. (1999) Richard S Sutton, David McAllester, Satinder Singh, and
    Yishay Mansour. Policy gradient methods for reinforcement learning with function
    approximation. In S. Solla, T. Leen, and K. Müller, editors, *Advances in Neural
    Information Processing Systems*, volume 12\. MIT Press, 1999. URL [https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swaminathan and Joachims (2015) Adith Swaminathan and Thorsten Joachims. The
    self-normalized estimator for counterfactual learning. In *advances in neural
    information processing systems*, pages 3231–3239, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swamy et al. (2024) Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven
    Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from
    human feedback. *arXiv preprint arXiv:2401.04056*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'von Werra et al. (2020) Leandro von Werra, Younes Belkada, Lewis Tunstall,
    Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer
    reinforcement learning. [https://github.com/huggingface/trl](https://github.com/huggingface/trl),
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2020) Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan,
    Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training.
    In *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SJeYe0NtvH](https://openreview.net/forum?id=SJeYe0NtvH).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) R. J. Williams. Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. *Machine Learning*, 8(3-4):229–256,
    May 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022) Annie Xie, Fahim Tajwar, Archit Sharma, and Chelsea Finn.
    When to ask for help: Proactive interventions in autonomous reinforcement learning.
    In *Advances in Neural Information Processing Systems*, volume 35, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2023) Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong,
    Heng Ji, Nan Jiang, and Tong Zhang. Iterative Preference Learning from Human Feedback:
    Bridging Theory and Practice for RLHF under KL-Constraint. *arXiv e-prints*, art.
    arXiv:2312.11456, December 2023. [10.48550/arXiv.2312.11456](https:/doi.org/10.48550/arXiv.2312.11456).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2024) Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu
    Mei, Guangju Wang, Chao Yu, and Yi Wu. Is DPO Superior to PPO for LLM Alignment?
    A Comprehensive Study. *arXiv e-prints*, art. arXiv:2404.10719, April 2024. [10.48550/arXiv.2404.10719](https:/doi.org/10.48550/arXiv.2404.10719).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yarats et al. (2021a) Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel
    Pinto. Mastering visual continuous control: Improved data-augmented reinforcement
    learning. *arXiv preprint arXiv:2107.09645*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yarats et al. (2021b) Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation
    is all you need: Regularizing deep reinforcement learning from pixels. In *International
    Conference on Learning Representations*, 2021b. URL [https://openreview.net/forum?id=GY6-6sTvGaf](https://openreview.net/forum?id=GY6-6sTvGaf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James
    Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy
    optimization. *arXiv preprint arXiv:2005.13239*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021) Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran,
    Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy
    optimization. *Advances in neural information processing systems*, 34:28954–28967,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar
    Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. *arXiv
    preprint arXiv:2401.10020*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li,
    Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-Rewarding Language Models.
    *arXiv e-prints*, art. arXiv:2401.10020, January 2024. [10.48550/arXiv.2401.10020](https:/doi.org/10.48550/arXiv.2401.10020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad
    Saleh, and Peter J. Liu. SLiC-HF: Sequence Likelihood Calibration with Human Feedback.
    *arXiv e-prints*, art. arXiv:2305.10425, May 2023. [10.48550/arXiv.2305.10425](https:/doi.org/10.48550/arXiv.2305.10425).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al. (2020) Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
    Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Connections to Existing Fine-Tuning Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our proposed framework also allows us to explain experiments and evaluations
    in several existing LLM fine-tuning results, and as a result, implies several
    practical guidelines for LLM practitioners. On the AlpacaFarm benchmark (Dubois
    et al., [2024](#bib.bib14)), our results corroborate the gap between conditional
    supervised fine-tuning objectives such as binary FeedME and reward conditioning,
    and RL or contrastive training methods such as PPO and DPO: these results are
    perhaps even more extreme in that these conditional and weighted supervised fine-tuning
    objectives are not even able to outperform regular SFT. Methods that utilize on-policy
    sampling such as ReST (Gulcehre et al., [2023](#bib.bib19)) and Quark (Lu et al.,
    [2022](#bib.bib30)) do outperform SFT but still underperform on-policy RL or on-policy
    contrastive training. The top-performing methods on the benchmark are offline
    DPO, which uses a negative gradient, and PPO, which leverages on-policy sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, methods such as self-rewarding language models (Yuan et al., [2024](#bib.bib59)),
    OAIF (Guo et al., [2024](#bib.bib20)), DR-PO (Chang et al., [2024](#bib.bib7)),
    Hybrid-DPO (Xiong et al., [2023](#bib.bib53)), and RS-DPO (Khaki et al., [2024](#bib.bib24))
    couple on-policy sampling or rejection sampling with contrastive training objectives.
    These works corroborate our observation regarding the efficacy of on-policy sampling
    and negative gradients and how they are complementary. Approaches such as CRINGE (Adolphs
    et al., [2022](#bib.bib1)) combine maximum likelihood with a token level contrastive
    loss term and show gains over solely utilizing supervised likelihood, corroborating
    our insights about negative gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrently to us, Xu et al. ([2024](#bib.bib54)) show that on many practical
    LLM fine-tuning problems offline DPO underperforms on-policy PPO. While we do
    not study the same LLM fine-tuning problems, the insights from this work corroborate
    our findings, which in turn extend insights from this work. For instance, this
    work observes that DPO can learn to find out-of-distribution responses, which
    is consistent with our analysis in Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Takeaway
    2: Mechanisms Explaining the Behavior of the Negative Gradient ‣ 5.2 Question
    2: The Role of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") that offline DPO training
    might increase probability mass on the highly likely regions of $\pi_{\theta}$.
    To avoid this issue, this work prescribes an iterated DPO recipe where the reference
    policy (i.e., the SFT policy in their setting) is used to iteratively collect
    new samples for DPO training. Section [5.3](#S5.SS3 "5.3 Question 3: On-Policy
    Sampling and Negative Gradients are Complementary ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    arrives at a similar conclusion that using on-policy samples for policy optimization,
    though we recommend collecting samples from the current policy and not the reference
    policy, which might fail to cover important regions of the space when the peak
    in the reward function appears farther away from the high-likely regions of the
    reference policy.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Computational vs Wall-Clock Time Tradeoff for Various Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Bandit (R1) | Min Length | Skew Length |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reward ($\uparrow$) | Time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Offline DPO / IPO | 0.82 (0.04) | 1.7 hours | 1.0 (0.0) | 1.3 hours | 11.8
    (14.0) | 0.12 hours |'
  prefs: []
  type: TYPE_TB
- en: '| On-policy PPO | 0.92 (0.01) | 0.93 hours | 20.5 (25.4) | 4.84 hours | 15.8
    (11.1) | 7.26 hours |'
  prefs: []
  type: TYPE_TB
- en: '| On-policy RWR | 0.88 (0.01) | 0.12 hours | 65.5 (36.7) | 15.5 hours | 15.8
    (9.3) | 15.5 hours |'
  prefs: []
  type: TYPE_TB
- en: '| On-policy DPO / IPO | 0.92 (0.01) | 0.12 hours | 1.0 (0.0) | 0.4 hours |
    0.0 (0.0) | 0.4 hours |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Wall-clock time comparisons. Comparison between on-policy and offline
    variants of contrastive objectives (DPO/IPO) in terms of reward and wall-clock
    time required till convergence of the run. Generally, on-policy contrastive approaches
    achieve both superior reward and wall-clock time as opposed to offline contrastive
    approaches (offline DPO/IPO) and on-policy RL (PPO, RWR). Synthetic LLM experiments
    use a single A40 GPU. Bandit experiments use a Intel(R) Xeon(R) CPU E5-2698 v4
    @ 2.20GHz CPU, with 4 threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural takeaway extending the empirical results from Section [5.3](#S5.SS3
    "5.3 Question 3: On-Policy Sampling and Negative Gradients are Complementary ‣
    5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data") is that on-policy variants of contrastive approaches
    might provide for an better tradeoff between computation and wall-clock time.
    We perform a comparison of wall-clock time needed to run our experiments in [Table 3](#A2.T3
    "In Appendix B Computational vs Wall-Clock Time Tradeoff for Various Methods ‣
    Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). in particular, we found that on-policy DPO only requires 0.4 hours to
    converge, while offline DPO requires a wall-clock time of 1.3 hours to converge
    to the same solution in the Min Length scenario. In the Skew Length scenario,
    where the learned policy must deviate from the initial reference policy substantially,
    we find that while offline DPO can converge a bit quickly (0.12 hours), it flatlines
    at a sub-optimal solution (completion length of 11.8) as compared to on-policy
    DPO which takes merely 0.4 hours to reach a more optimal solution. This is far
    more time-efficient compared to other on-policy methods such as PPO and RWR that
    present a sampling bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C More Details on Conceptual Unification and Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.1 Unifying On-Policy Sampling and Negative Gradients via Mode-Seeking Divergences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we provide proofs for the claims in [6.1](#S6.SS1 "6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). We will show that on-policy methods and offline constrastive methods,
    both are mode-seeking as opposed to supervised maximum likelihood approaches,
    which are mode-covering. This conceptually explains the differences in their behaviors
    that we observe in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: C.1.1 On-policy Methods Are Mode-Seeking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, we prove [Lemma 6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6.1 Seeking Modes
    Unifies On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and
    Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"), i.e., we want to show that on-policy RL methods and on-policy
    versions of weighted supervised learning methods optimize regularized version
    of a reverse KL-divergence.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Both on-policy RL algorithms and on-policy versions of weighted supervised
    learning, optimize the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{RL}}(\mathcal{D}_{\text{pref}},\pi_{\theta})=-\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}}}[\mathbb{E}_{\mathbf{y}\sim\pi_{\theta}(.&#124;\mathbf{x})}[r(\mathbf{x},\mathbf{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_{\theta}(.&#124;\mathbf{x})&#124;&#124;\pi_{\mathrm{ref}}(.&#124;\mathbf{x})]]$
    |  | (C.1) |'
  prefs: []
  type: TYPE_TB
- en: 'Following Appendix A.1 of Rafailov et al. ([2023](#bib.bib38)), there exists
    some policy $\pi^{*}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r(\mathbf{x},\mathbf{y})=\beta\log Z(\mathbf{x})+\beta\log\left(\frac{\pi^{*}(\mathbf{y}&#124;\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $Z(\mathbf{x})=\sum_{\mathbf{y}}\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})\exp\left(\frac{r(\mathbf{x},\mathbf{y})}{\beta}\right)$
    is the partition function. Combining these two, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{RL}}(\mathcal{D}_{\text{pref}},\pi_{\theta})={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Note that $Z(\mathbf{x})$ is equivalent to optimizing the reverse KL-divergence.
    Since optimizing the reverse KL-divergence is mode-seeking, we see that on-policy
    RL algorithms have mode-seeking behavior. ∎
  prefs: []
  type: TYPE_NORMAL
- en: C.1.2 Contrastive Approaches (e.g., DPO/IPO) are Mode-Seeking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we show that this is also the case for contrastive approaches as we prove
    [Lemma 6.2](#S6.Thmtheorem2 "Lemma 6.2\. ‣ 6.1 Seeking Modes Unifies On-Policy
    Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical Analysis
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'First consider an input $\mathbf{x}$. Consider the gradient update (with a
    small enough learning rate):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'We shall prove that for all possible models $\theta$, such that after taking
    the gradient update, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The core idea behind this proof is the normalization of the probability simplex.
    We proceed with a combination of mathematical inducation and contradiction: assume
    that $\omega_{t}\Big{|}_{{c_{2}></math> and then study under what conditions is
    it possible that for any pairing of positives and negatives, <math id=$ is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\omega^{\prime}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\omega+\eta\left[c_{1}\left&#124;\left&#124;\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}_{w}&#124;\mathbf{x})\right&#124;\right&#124;^{2}+c_{2}\left&#124;\left&#124;\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}_{l}&#124;\mathbf{x})\right&#124;\right&#124;^{2}-(c_{1}+c_{2})\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}_{l}&#124;\mathbf{x})^{\top}\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}_{w}&#124;\mathbf{x})\right].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Now, define: $1$2, then we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(t+1;\mathbf{y}_{w},\mathbf{y}_{l},\mathbf{x})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Suppose that for all negatives $\mathbf{y}_{l}$, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\forall\mathbf{y}_{l}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies$ |  |'
  prefs: []
  type: TYPE_TB
- en: which is a contradiction since $c_{1}></math>. This means that if <math id=$
    and $c_{2}=0$.
  prefs: []
  type: TYPE_NORMAL
- en: For the second part of this result, we note that when the gradient dot products
    are negative and $$c_{2}></math> for the negative response. This proves the second
    part of this statement. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradients for both DPO and IPO exhibit the form in [Lemma 6.2](#S6.Thmtheorem2
    "Lemma 6.2\. ‣ 6.1 Seeking Modes Unifies On-Policy Sampling and Negative Gradients
    ‣ 6 Conceptual Unification and Theoretical Analysis ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"). We now show that the gradient
    of both DPO and IPO takes the form shown in [Equation 6.1](#S6.E1 "In Lemma 6.2\.
    ‣ 6.1 Seeking Modes Unifies On-Policy Sampling and Negative Gradients ‣ 6 Conceptual
    Unification and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"). From Rafailov et al. ([2023](#bib.bib38)), the gradient
    of the DPO loss is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $c^{\text{DPO}}(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})=\sigma\left(\beta\log\frac{\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})}{\pi_{\mathrm{ref}}\mathbf{y}_{l}|\mathbf{x})}-\beta\log\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})}{\pi_{\mathrm{ref}}\mathbf{y}_{w}|\mathbf{x})}\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: Now we derive the gradient of the IPO loss. Define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'The gradient of the IPO loss is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\theta}\mathcal{L}_{\text{IPO}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: C.1.3 Supervised Offline Algorithms are Mode-Covering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now we prove [Lemma 6.3](#S6.Thmtheorem3 "Lemma 6.3\. ‣ 6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"), which shows that supervised offline methods that optimize a maximum likelihood
    loss exhibit mode-covering behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Offline supervised methods optimize the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Define a new distribution
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tilde{\pi}(\mathbf{y}&#124;\mathbf{x})=\frac{\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{y})}{Z(\mathbf{x})}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here $Z(\mathbf{x})=\sum_{\mathbf{z}}\pi_{\mathrm{ref}}(\mathbf{z}|\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{z})$ is the normalization constant. It is easy to check that
    this a valid conditional distribution. This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Hence offline supervised methods minimize the re-weighted forward KL-divergence.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Characterization of Gradients of Forward and Reverse KL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For simplicity, let $\mathcal{X}$ be our network that outputs $V$ over discrete
    tokens $1,\ldots,V$ using the softmax function, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}(x)=\frac{\exp(f_{i}(x))}{\sum_{k=1}^{V}\exp(f_{k}(x))}$
    |  | (C.2) |'
  prefs: []
  type: TYPE_TB
- en: for any $x\in\mathcal{X}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now assume that for some given input $x$ via SGD:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{0}(x)\leftarrow p_{\text{ref}}(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $p_{t+1}(x)\leftarrow p_{t}(x)-\eta\nabla_{f(x)}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))\big{&#124;}_{p=p_{t}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $p_{\text{ref}}$ is the true distribution of interest. In contrast, $\mathbb{D}_{\text{KL}}(p||q)$
    in a similar fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'We shall now prove [Lemma 6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2 Case Study:
    Mode-Seeking Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification
    and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"). We break this lemma in two parts. First, let us investigate
    how the gradients $\nabla_{f(x)}\mathbb{D}_{\text{KL}}(q(x)||p(x))\big{|}_{p=p_{t}}$
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma C.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The gradients of forward and reverse KL are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))=p_{j}(x)-q_{j}(x)$
    |  | (C.3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))=p_{j}(x)\left[\log\frac{p_{j}(x)}{q_{j}(x)}-\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))\right]$
    |  | (C.4) |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We start with the definition of KL-divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'This proves [Equation C.3](#A3.E3 "In Lemma C.1\. ‣ C.2 Characterization of
    Gradients of Forward and Reverse KL ‣ Appendix C More Details on Conceptual Unification
    and Theoretical Analysis ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data"). Similarly, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Now we calculate the partial derivative with respect to $f_{j}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\frac{\sum_{i}f_{i}(x)e^{f_{i}(x)}}{\sum_{k}e^{f_{k}(x)}}={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\frac{\partial}{\partial f_{j}}\log\left(\sum_{k}e^{f_{k}(x)}\right)=\frac{e^{f_{j}(x)}}{\sum_{k}e^{f_{k}(x)}}=p_{j}(x)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: And for the third term,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\frac{\sum_{i}\log q_{i}(x)e^{f_{i}(x)}}{\sum_{k}e^{f_{k}(x)}}={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Putting it all together, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{f_{j}}\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))={}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: completing our proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof for Lemma [6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2 Case Study: Mode-Seeking
    Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). Now, if the logits $f_{t}$ is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{t+1}_{j}(x)={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle={}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s consider what the characterization of $p^{t+1}$ for the forward kl:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p^{t+1}_{j}(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Noticing that the denominator is just a normalization constant, we can write
    this as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{p^{t+1}_{j}(x)}{p^{t}_{j}(x)}\propto\exp\left(-\eta\left(p_{j}^{t}(x)-q_{j}(x)\right)\right)$
    |  | (C.5) |'
  prefs: []
  type: TYPE_TB
- en: 'Similarly the characterization of $p^{t+1}$ for the reverse KL looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{p^{t+1}_{j}(x)}{p^{t}_{j}(x)}$ |  | (C.6) |'
  prefs: []
  type: TYPE_TB
- en: 'This completes the proof of [Lemma 6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2
    Case Study: Mode-Seeking Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual
    Unification and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data").'
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Quantifying the Differences Between Forward and Reverse KL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section we will prove [Theorem 6.5](#S6.Thmtheorem5 "Theorem 6.5\.
    ‣ 6.2 Case Study: Mode-Seeking Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual
    Unification and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"): specifically, we will study certain special cases
    to explain the differences between approaches that optimize the forward and reverse
    KL divergences. We drop the subscript $t$ from all terms to prevent notational
    clutter.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We prove these statements case by case. First we prove the result for Case
    1\. In this scenario, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The gap between $\Delta^{f}$ is now given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})-\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Now, we note by mean-value theorem, that there exists a $c_{0}\in[q(\mathbf{x}_{2}),q(\mathbf{x}_{1})]$
    such that,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Since <math id=$$, we have that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This quantity is positive when $$p(\mathbf{x}_{1})></math>. This shows the result
    for Case 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we prove Case 2. In this setting we are given $q(\mathbf{x}_{1})=q(\mathbf{x}_{2})\geq
    p(\mathbf{x}_{1})\geq p(\mathbf{x}_{2})+\beta$ are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})=-\eta\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)\leq-\eta\beta.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'On the other hand, the expression for $\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ +\eta\mathrm{D}_{\text{KL}}(p,q)\underbrace{\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)}_{\geq
    0}.$ |  | (C.7) |'
  prefs: []
  type: TYPE_TB
- en: 'Now we analyze each sub-term independently. First, we note the following expression
    for term (b):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle(b)\coloneqq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Combining $(a)$, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle(a)+(b)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (C.8) |'
  prefs: []
  type: TYPE_TB
- en: where $c^{\prime}$. Hence, if $p(\mathbf{x}_{2})$, although $\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})<0$.
    This concludes the proof.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we prove Case 3. Similar to the previous case, here $\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})=-\eta(p(\mathbf{x}_{1})-p(\mathbf{x}_{2}))\leq-\eta\beta<0$,
    we need to prove that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)\cdot\log
    q(\mathbf{x}_{1})\leavevmode\nobreak\ $ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha_{0}$. By applying mean value theorem, on the RHS of this equation,
    we note that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Then, to attain the desired inequality, we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left[p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right]\cdot\left[\log
    q(\mathbf{x}_{1})-1-\log c^{\prime\prime}\right]\leq\alpha_{0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that since $c^{\prime\prime}\geq p(\mathbf{x}_{2})$, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(\mathbf{x}_{1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\implies\log q(\mathbf{x}_{1})\leq\log\mathbf{c}_{1}+\log
    c^{\prime\prime},$ |  |'
  prefs: []
  type: TYPE_TB
- en: the LHS of this equation will be smaller than the RHS $\alpha_{0}$. This proves
    the result for this case. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Additional Algorithmic Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: D.1 Score/Reward Standardization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Online methods such as PPO or RWR that uses a learned reward model can suffer
    from gradient variance issues due to the differences in the reward score. In particular,
    adding or subtracting a baseline $b$ sampled from policy $\pi_{\theta}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bar{r}_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})=\frac{r_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})-\hat{\mu}}{\hat{\sigma}}$
    |  | (D.1) |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{\mu}=\frac{1}{\mathcal{B}}\sum_{i=1}^{\mathcal{B}}r_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})$.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 IPO
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'IPO (Gheshlaghi Azar et al., [2023](#bib.bib18)) is a contrastive algorithm
    similar to DPO. The key difference between them is their loss function: DPO optimizes
    the negative log-sigmoid loss whereas IPO optimizes an MSE-type objective. Formally,
    the IPO objective is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (D.2) |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Method Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We did an extensive sweep over hyperparameters for individual offline and online
    algorithms for the language model experiments. We built our algorithm implementations
    off of the Huggingface TRL implementation (von Werra et al., [2020](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: E.1 Standardized Parameters (Consistent for all Methods)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 4: Algorithm Agnostic Hyperparamters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $B$ | 64 | Batch Size |'
  prefs: []
  type: TYPE_TB
- en: '| $B_{mini}$ | 8 | Mini-Batch Size |'
  prefs: []
  type: TYPE_TB
- en: '| $G$ | 8 | Gradient Accumulation Steps |'
  prefs: []
  type: TYPE_TB
- en: '| $\hat{\pi}_{\theta}$ | Pythia1.4B, Mistral-7b | Policy Architecture |'
  prefs: []
  type: TYPE_TB
- en: '| $\hat{R}_{\theta}$ | Pythia410M, Mistral-7B | Reward Model Architecture |'
  prefs: []
  type: TYPE_TB
- en: '| optimizer | Adam | Gradient Optimizer |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Sampling Hyperparamters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| top_k | 0.0 | Disables top-k sampling |'
  prefs: []
  type: TYPE_TB
- en: '| top_p | 1.0 | Disables nucleus sampling |'
  prefs: []
  type: TYPE_TB
- en: '| do_sample | True | Enables sampling |'
  prefs: []
  type: TYPE_TB
- en: '| max_new_tokens | 256 | Maximum number of new tokens to generate |'
  prefs: []
  type: TYPE_TB
- en: '| temperature | 1.0 | Sets sampling temperature (1.0 for default) |'
  prefs: []
  type: TYPE_TB
- en: '| use_cache | True | Uses past key/values attentions if supported by the model
    |'
  prefs: []
  type: TYPE_TB
- en: E.2 DPO (Rafailov et al., [2023](#bib.bib38))
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 6: DPO Hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| lr | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | learning rate |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta$, $0.5$ | KL weight |'
  prefs: []
  type: TYPE_TB
- en: E.3 Pref-FT (Dubois et al., [2024](#bib.bib14))
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 7: Pref-FT/Binary FeedMe Hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6 | learning rate |'
  prefs: []
  type: TYPE_TB
- en: E.4 PPO (Schulman et al., [2017](#bib.bib41))
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 8: PPO Hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | Learning rate. |'
  prefs: []
  type: TYPE_TB
- en: '| vf_coef | 0.1 | Coefficient for the value function loss. |'
  prefs: []
  type: TYPE_TB
- en: '| adap_kl_ctrl | True | Enables adaptive KL penalty control. |'
  prefs: []
  type: TYPE_TB
- en: '| init_kl_coef | 0.2 | Initial coefficient for KL penalty. |'
  prefs: []
  type: TYPE_TB
- en: '| target_kl | 0.1 | Target KL divergence for policy updates. |'
  prefs: []
  type: TYPE_TB
- en: '| $N$ | 1 | actions per prompt |'
  prefs: []
  type: TYPE_TB
- en: E.5 RWR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 9: RWR Hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | learning rate |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta$, $20$ | temperature |'
  prefs: []
  type: TYPE_TB
- en: '| $N$ | 1 | actions per prompt |'
  prefs: []
  type: TYPE_TB
- en: E.6 Iterated Best-of-N (Mukobi et al., [2023](#bib.bib32))
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 10: Iterated BofN Hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Values | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | learning rate |'
  prefs: []
  type: TYPE_TB
- en: '| $N$ | actions per prompt |'
  prefs: []
  type: TYPE_TB
- en: Appendix F Code For Running Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have made the code for this project public in this [repository](https://github.com/Asap7772/understanding-rlhf).
    The additional datasets used in our experiments are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Min Length](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data_minlength)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mode Length](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data_modelength)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Skew Length](https://huggingface.co/datasets/Asap7772/alpaca_skewexp_minlength_merged)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Relabelled AlpacaFarm](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We gratefully acknowledge the following codebases: [TRL](https://github.com/huggingface/trl) (von
    Werra et al., [2020](#bib.bib49)), [HALOs](https://github.com/ContextualAI/HALOs) (Ethayarajh
    et al., [2023](#bib.bib16)), [minGPT](https://github.com/karpathy/minGPT) ([Karpathy,](#bib.bib23)
    ), [DrQ-v2](https://github.com/facebookresearch/drqv2) (Yarats et al., [2021a](#bib.bib55),
    [b](#bib.bib56)) and [PAINT](https://github.com/tajwarfahim/proactive_interventions) Xie
    et al. ([2022](#bib.bib52)).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G More on Didactic Bandit Problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: G.1 Problem Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we present details of our didactic bandit problem. The reference policy
    shown in [Figure 2](#S4.F2 "In 4.2 Tasks and Datasets ‣ 4 Research Questions and
    Analysis Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is obtained by collecting 10000 samples from a Cauchy distribution with
    location $x_{0}=-0.7$, we label these bins $0,\ldots,99$ sequentially, and calculate
    the frequency of samples that fell into each bin. Finally, we define,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{\mathrm{ref}}(a_{i})=\frac{\text{Freq}(bin_{i})}{10000}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The reward functions $\mathbf{R}_{1}$ are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{R}_{1}(a)=\exp\left(-\left(\frac{a-70}{10}\right)^{2}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{R}_{2}(a)=\exp\left(-\left(\frac{a-20}{10}\right)^{2}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: G.2 Algorithmic Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the bandit setting, we consider five algorithms: (1) Best-of-N, (2) IPO,
    (3) REINFORCE, (4) PPO and (5) RWR.'
  prefs: []
  type: TYPE_NORMAL
- en: G.2.1 Best-of-N
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Best-of-N is similar to SuperHF (Mukobi et al., [2023](#bib.bib32))/ReST (Gulcehre
    et al., [2023](#bib.bib19)) and in some way their simplification for the bandit
    setting. Best-of-N collects $N$, and based on these rewards, choose the best action
    $\mathbf{y}_{\text{best}}=\operatorname*{arg\,max}_{\mathbf{y}_{i}}\mathbf{R}(\mathbf{x},\mathbf{y}_{i})$.
    Finally, the loss function is the negative log-likelihood of this best action.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{bofn}}(\pi_{\theta};\mathbf{x},\mathbf{y}_{1},\ldots,\mathbf{y}_{N})=-\log\pi_{\theta}(\mathbf{y}_{\text{best}}&#124;\mathbf{x})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In both the online and offline setting, we have a fixed set of prompts $\mathcal{D}_{\text{prompts}}$,
    we can form a training set as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D}_{\text{train}}(\mathcal{D}_{\text{prompts}},\pi)=\{(\mathbf{x},\mathbf{y}):\mathbf{x}\in\mathcal{D}_{\text{prompts}},\mathbf{y}=\operatorname*{arg\,max}_{\mathbf{y}_{i}}\mathbf{R}(\mathbf{x},\mathbf{y}_{i})\text{
    where }\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N}\sim\pi_{\mathrm{ref}}(.&#124;\mathbf{x})\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In the offline setting, we collect a fixed training dataset where actions are
    sampled from $\pi_{\mathrm{ref}}$, after every $T$ gradient steps, and discard
    the previous dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show the efficacy of negative gradient, we can also directly add a term
    to this loss function minimizing log probability on dispreferred actions. Explicitly,
    we consider the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$, in practice we only minimize the probability of dispreferred
    actions if it is above a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: G.2.2 IPO
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In contrast, IPO uses the loss function defined in [Equation D.2](#A4.E2 "In
    D.2 IPO ‣ Appendix D Additional Algorithmic Details ‣ Appendices ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"). While regular
    IPO is an offline algorithm that uses a fixed preference dataset $\mathcal{D}_{\text{pref}}$,
    we can generate completions $\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N}\sim\pi(.|\mathbf{x})$
    and $\mathbf{y}_{j}$ tuples.
  prefs: []
  type: TYPE_NORMAL
- en: In the offline setting, the preference dataset is collected by generating samples
    from the reference policy $\pi_{\mathrm{ref}}$ gradient steps, and discard the
    previous dataset.
  prefs: []
  type: TYPE_NORMAL
- en: G.2.3 REINFORCE
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For REINFORCE, we sample $\mathbf{y}\sim\pi_{\theta}(.|\mathbf{x})$, and use
    the following loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: G.2.4 PPO
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For PPO, let $\pi_{\text{gen}}$. Then we use the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where <math id=$$ is a hyperparameter that controls how much we clip off-policy
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: G.2.5 RWR
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For RWR, we use the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$ in our experiments unless otherwise noted.
  prefs: []
  type: TYPE_NORMAL
- en: G.3 Experiment Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For all experiments, we use $N=10$ randomly sampled prompts from tokens $\{0,\ldots,99\}$
    times for all experiments. We set $\tau=0.05$ and $\pi_{\mathrm{ref}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For all experiments, we use a small GPT (Radford et al., [2018](#bib.bib37);
    Brown et al., [2020](#bib.bib5))-like transformer architecture (named ‘GPT-Nano’)
    with 0.9M parameters. We took the implementation from this public repository:
    [minGPT](https://github.com/karpathy/minGPT) ([Karpathy,](#bib.bib23) ).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Additional Experiments on Synthetic LLM Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: H.1 Performance of Various Algorithms on the Mode Length Setting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 20](#A8.F20 "In H.1 Performance of Various Algorithms on the Mode Length
    Setting ‣ Appendix H Additional Experiments on Synthetic LLM Setup ‣ Appendices
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    shows the performance of various algorithms in the mode length setup. We see that
    all algorithms perform similarly here.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/330c456c96329e99bf580c55817e9cb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Performance of various algorithms on mode length setup. Distance
    to mode of the completion lengths from $\pi_{\text{ref}}$, 203, for different
    algorithms. All algorithms perform similarly, and varying degrees of on-policyness
    does not generally degrade performance.'
  prefs: []
  type: TYPE_NORMAL
- en: H.2 Effect of On-policy Samples vs Samples from an Older Policy in Synthetic
    Length Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figures 21](#A8.F21 "In H.2 Effect of On-policy Samples vs Samples from an
    Older Policy in Synthetic Length Settings ‣ Appendix H Additional Experiments
    on Synthetic LLM Setup ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data") and [22](#A8.F22 "Figure 22 ‣ H.2 Effect of On-policy
    Samples vs Samples from an Older Policy in Synthetic Length Settings ‣ Appendix
    H Additional Experiments on Synthetic LLM Setup ‣ Appendices ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") shows the effect of using
    on-policy samples vs samples from an older policy for RWR in the synthetic length
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86661445b67b80b2399d6077e4783dea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: On-policy sampling on Min Length (RWR). Effect of using on-policy
    samples vs samples from an older policy for RWR and the min length setup. In all
    experiments, the mini-batch size to calculate the gradient is fixed at 64, and
    we sample batch size $B$ thus makes the algorithm make updates on samples from
    an older policy. Left: average completion length (lower the better), and Right:
    proxy reward vs gradient steps. Being more on-policy results in better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/acf2581cf29ec73e828c44f55c287501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: On-policy sampling on Skew Length (RWR). Effect of using on-policy
    samples vs samples from an older policy for RWR and the skew length setup. Left:
    average completion length (lower the better), and Right: proxy reward vs gradient
    steps. Being more on-policy results in better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: H.3 Sample Reuse in Synthetic LLM Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 23](#A8.F23 "In H.3 Sample Reuse in Synthetic LLM Settings ‣ Appendix
    H Additional Experiments on Synthetic LLM Setup ‣ Appendices ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") shows the effect of sample
    reuse in the Skew Length setting: similar to Min Length ( [Figure 11](#S5.F11
    "In 5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy
    Data ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")),
    some sample reuse can improve sample efficiency. but excessive sample reuse can
    also hurt performance. Also, we see PPO with importance clipping is much better
    at sample reuse than Best-of-N.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5619f5f03a7cd6a0a720bfdf7599d0c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Effect of on-policy sample reuse in the Skew Length scenario. Average
    completion length (i.e., the lower the better) vs gradient steps for different
    numbers of inner iteration steps, $T$ implies that the algorithm is more off-policy.
    Observe that some sample reuse can improve sample efficiency (T = 2 and T = 4
    outperform T = 1), but excessive sample reuse can hurt performance (T = 8 becomes
    unstable for PPO). Also note that algorithms with mechanisms to control off-policy
    updates such as PPO with importance-weight clipping are suited to perform better
    in the off-policy sample reuse setting.'
  prefs: []
  type: TYPE_NORMAL
