- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:34:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16673](https://ar5iv.labs.arxiv.org/html/2408.16673)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \pdfcolInitStack
  prefs: []
  type: TYPE_NORMAL
- en: tcb@breakable
  prefs: []
  type: TYPE_NORMAL
- en: Ziniu Li The Chinese University of Hong Kong, Shenzhen Shenzhen Research Institute
    of Big Data Congliang Chen The Chinese University of Hong Kong, Shenzhen Shenzhen
    Research Institute of Big Data Tian Xu Nanjing University Zeyu Qin Hong Kong University
    of Science and Technology Jiancong Xiao University of Pennsylvania Ruoyu Sun Corresponding
    author. The Chinese University of Hong Kong, Shenzhen Shenzhen Research Institute
    of Big Data Zhi-Quan Luo The Chinese University of Hong Kong, Shenzhen Shenzhen
    Research Institute of Big Data
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models rely on Supervised Fine-Tuning (SFT) to specialize in
    downstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it
    often leads to overfitting and limited output diversity due to its aggressive
    updates to the data distribution. This paper aim to address these issues by introducing
    the maximum entropy principle, which favors models with flatter distributions
    that still effectively capture the data. Specifically, we develop a new distribution
    matching method called GEM, which solves reverse Kullback-Leibler divergence minimization
    with an entropy regularizer.
  prefs: []
  type: TYPE_NORMAL
- en: For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects. First,
    when applied to the UltraFeedback dataset to develop general instruction-following
    abilities, GEM exhibits reduced overfitting, evidenced by lower perplexity and
    better performance on the IFEval benchmark. Furthermore, GEM enhances output diversity,
    leading to performance gains of up to 7 points on math reasoning and code generation
    tasks using best-of-n sampling, even without domain-specific data. Second, when
    fine-tuning with domain-specific datasets for math reasoning and code generation,
    GEM also shows less overfitting and improvements of up to 10 points compared with
    CE.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) [[38](#bib.bib38), [53](#bib.bib53), [52](#bib.bib52)]
    are powerful generative models excelling in specialized tasks across various fields.
    Typically, LLMs first go through a pre-training stage [[42](#bib.bib42), [7](#bib.bib7)],
    where they learn to predict the next token from a large corpus of texts, such
    as books, scientific papers, and code. Despite this extensive pre-training, LLMs
    often struggle to follow instructions and answer users’ queries effectively, because
    such scenarios are not commonly encountered during pre-training. To improve their
    performance in these tasks, instruction tuning [[44](#bib.bib44), [60](#bib.bib60),
    [10](#bib.bib10)], also known as Supervised Fine-Tuning (SFT) [[39](#bib.bib39),
    [3](#bib.bib3)], is employed. This process involves using high-quality labeled
    data (i.e., prompt-response pairs) and typically utilizes supervised learning
    with Cross Entropy (CE) loss to maximize the likelihood of the labeled data. Later
    on, these models may be further aligned with human preferences to ensure their
    outputs align with human values [[39](#bib.bib39), [3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: SFT elicits the knowledge acquired from pre-training to answer various downstream
    questions and further paves the way for future developments, making it crucial
    part of the post-training pipeline [[68](#bib.bib68), [54](#bib.bib54), [34](#bib.bib34),
    [65](#bib.bib65)]. We expect models to generalize well by providing accurate answers
    and hope these answers are diverse as well. While the importance of generalization
    is clear, we also stress the significance of generation diversity. In creative
    writing, diversity sparks new ideas [[12](#bib.bib12)], and in chit-chat dialogues,
    users often appreciate having multiple output options to suit their preferences
    [[30](#bib.bib30)]. Many modern AI interfaces, such as ChatGPT and Claude AI,
    recognize this need by incorporating features like regeneration buttons. Additionally,
    generation diversity is vital when advanced generation algorithms are applied
    in LLMs to tackle complex tasks [[50](#bib.bib50)]. For example, the best-of-n
    sampling, commonly used in math reasoning [[50](#bib.bib50)] and code generation
    [[8](#bib.bib8)], benefits from selecting the optimal response from a diverse
    set of generated options.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a6ee4572a69c65650039af1ec30c655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of the difference between the standard CE and the proposed
    method GEM for SFT of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, models trained using CE loss in SFT often struggle with overfitting
    (see, e.g., [[16](#bib.bib16), [17](#bib.bib17)]) and poor generation diversity
    (see, e.g., [[40](#bib.bib40), [37](#bib.bib37)]). In theory, optimizing CE loss
    corresponds to minimizing the *forward* Kullback–Leibler (KL) divergence between
    the data distribution and the generative distribution of the LLM.¹¹1It is called
    “forward” KL because the loss is defined across the data distribution. We will
    later discuss *reverse* KL approaches, where the loss is defined across the generative
    model’s distribution. This distribution matching process aggressively increases
    the likelihood of observed data, which often exhibit narrow coverage of real distributions
    that have diverse outcomes that we want the LLM to learn. However, the CE loss
    is unaware of this, and the optimized model biases toward low-entropy distributions,
    resulting in reduced output diversity. One particular example is shown in [Figure 1](#S1.F1
    "In 1 Introduction ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity") (details are provided in [Appendix E](#A5
    "Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity")). Prior research
    [[41](#bib.bib41), [14](#bib.bib14)] links low-entropy predictive distributions
    with poor generalization, indicating that these issues are interconnected. While
    practitioners often use weight decay regularization with CE loss [[39](#bib.bib39),
    [3](#bib.bib3)], it does not fully address these problems, necessitating more
    principled approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions. We formulate fine-tuning of LLMs as a distribution matching
    problem and propose two principles. The first principle advocates *generative*
    distribution matching methods to encourage the model learns from both ground truth
    supervision and its own generated mistakes. This contrasts with the passive imitation
    of supervision used in the CE loss. The second principle is that the model should
    assign higher probabilities to the observed data while preventing over-memorization²²2By
    over-memorization, we mean that the probabilities of the supervised data become
    overwhelmingly dominant in the distributions after learning., especially when
    dealing with the limited data. To implement these principles, we study the formulation
    of reverse KL divergence minimization with entropy regularization. However, this
    formulation is technically challenging and may require adversarial training techniques
    akin to those used in GANs [[18](#bib.bib18)]. Our main technical contribution
    is the development of a new training algorithm, referred to as GEM (Generative
    Entropy-regularized Matching of distributions), that addresses the above challenge
    and is tractable as the CE loss. By adhering to the proposed principles, GEM favors
    flatter distributions that effectively capture the data; see [Figure 1](#S1.F1
    "In 1 Introduction ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity") for an illustration.'
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrate the effectiveness of our model by fine-tuning the Llama-3-8B
    pre-trained model³³3[https://huggingface.co/meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
    with two types of datasets. First, we fine-tune the model using the UltraFeedback
    dataset [[13](#bib.bib13)] to develop general instruction-following abilities.
    Our results show that GEM achieves lower evaluation perplexity than CE and better
    performance on the IFEval benchmark [[69](#bib.bib69)], indicating reduced overfitting.
    We also assess output diversity by evaluating the model’s ability to generate
    varied content in creative tasks, such as poem and story writing [[36](#bib.bib36)].
    Our findings show that GEM significantly enhances output diversity. This improved
    diversity translates into performance gains in math reasoning (GSM8K [[11](#bib.bib11)])
    and code generation tasks (HumanEval [[8](#bib.bib8)] and MBPP [[2](#bib.bib2)])
    when utilizing advanced generation strategies like Majority Voting (MV) and Best-Of-N
    (BON). In these tasks, GEM achieves performance improvements of up to 7 points.
    In a second experiment, we fine-tune the model on domain-specific datasets for
    specialized abilities in math reasoning (MetaMathQA dataset [[66](#bib.bib66)])
    and code generation (MagicCoder-OSS-Instruct dataset [[62](#bib.bib62)]), seperately.
    In this setting, GEM can outperform CE by up to 10 points when using MV and BON,
    further demonstrating its effectiveness. Overall, these results suggest that GEM
    can mitigate overfitting and improve output diversity compared with CE.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised Fine-tuning. SFT is the first stage of the post-training pipeline
    and plays an important role in subsequent developments. As mentioned in the introduction,
    using CE loss during the SFT stage often leads to overfitting and reduced output
    diversity. To address this, there is a line of research in scaling up SFT data
    (see, e.g., [[66](#bib.bib66), [62](#bib.bib62), [67](#bib.bib67)]), which, while
    effective, increases computational burden. Our work aims to develop training methods
    that more effectively leverage supervised data to mitigate overfitting and to
    enhance output diversity. While recent studies such as [[9](#bib.bib9), [29](#bib.bib29)]
    attempt to improve CE-trained models through techniques like self-play, we aim
    to address the limitations of CE loss and design methods that can improve the
    pre-trained models directly.
  prefs: []
  type: TYPE_NORMAL
- en: We also emphasize the importance of output diversity based on previous research.
    SFT-trained models are often further refined through Reinforcement Learning from
    Human Feedback (RLHF) to better align with human values [[39](#bib.bib39), [3](#bib.bib3)].
    Xiao et al. [[64](#bib.bib64)] studied the impact of SFT models on preference
    learning in RLHF, showing that if an SFT model collapses (i.e., becomes biased
    toward certain outputs with near-certain probability), it may further lead to
    preference collapse in the alignment. Their findings highlight the need to address
    collapse during the SFT stage. Additionally, SFT-trained models are often used
    as synthetic data generators for self-improvement (see, e.g., [[1](#bib.bib1),
    [15](#bib.bib15)]). In this context, maintaining output diversity is crucial.
    It helps find better solutions and alleviate the mode collapse issue [[19](#bib.bib19),
    [49](#bib.bib49)]. For specialized applications, Wang et al. [[59](#bib.bib59)]
    highlighted the benefits of output diversity through majority voting in math reasoning.
    Recent research [[6](#bib.bib6), [50](#bib.bib50)] have explored enhancing search
    over multiple generated responses with a verifier, showing that scaling up test-time
    compute is effective. Unlike these studies focused on inference-time techniques,
    our paper improves SFT training methods to promote diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy Regularization. Dubey et al. [[14](#bib.bib14)] investigated the application
    of maximum entropy in visual fine-grained classification tasks, where the challenge
    lies in distinguishing between very similar categories of objects. Although their
    objective differs from ours, their insights remain valuable. They proposed that
    achieving zero CE loss is not essential for high accuracy. Instead, they suggested
    that a conditional probability distribution where the argmax corresponds to the
    correct class is sufficient for many applications. This concept motivates our
    use of entropy regularization, which allows for assigning probabilities to alternative
    options beyond the observed data. Prior to our work, Pereyra et al. [[41](#bib.bib41)]
    also explored entropy regularization in the context of neural network training.
    Their method closely resembles the CE with entropy regularization that we investigate
    in this paper, and they found that penalizing confident outputs improves generalization.
    It is important to note that Pereyra et al. [[41](#bib.bib41)] focused on image
    classification tasks, while our focus is on text generation where data is sequential
    in nature and is more challenging. In the context of LLMs, Hu et al. [[23](#bib.bib23)]
    also explored the maximum entropy regularization by using GFlowNet [[4](#bib.bib4)],
    but their methods require a reward function rather than supervised data.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs). LLMs have a large vocabulary, denoted as $[K]=\{1,2,\ldots,K\}$,
    where each token $x_{i}\in[K]$ represents the sequence length. Let $f$ specifies
    the categorical distribution over $[K]$. Typically, $f$. For the $i$, its prediction
    probability is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $z_{t}\in\mathbb{R}^{K}$, and $z_{t}[i]$-th element of $z_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are pre-trained to predict the next token in a sequence, thereby learning
    complex conditional probability distributions from vast amounts of data. In practical
    applications, LLMs are tasked with generating a response $y$. However, these question-answer
    scenarios often differ from the textbook-like pre-training data, causing pre-trained
    LLMs to struggle in generating responses that follow human instructions effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised Fine-Tuning. To address the above issue, Supervised Fine-Tuning
    (SFT) is introduced. This process involves using a supervised dataset with high-quality
    prompt-response pairs $\{(x^{i},y^{i})\}_{i=1}^{N}$. In theory, this corresponds
    to minimizing the *forward* KL divergence between the data distribution $p$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta}D_{\mathrm{KL}}\left(p,f_{\theta}\right)\Longleftrightarrow\max_{\theta}\mathbb{E}_{x\sim\rho(\cdot)}{\mathbb{E}_{y\sim{p}(\cdot&#124;x)}[\log
    f_{\theta}(y&#124;x)]},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\rho$ can be treated as a constant and we omit it when the context is
    clear. In practice, many questions can correspond to multiple valid answers, and
    it is nearly impossible to collect a comprehensive high-quality dataset that encompasses
    all possibilities. As a result, the empirical data tends to be limited in size
    and often exhibits a narrower distribution than desired. In such scenarios, the
    CE loss function aggressively maximizes the likelihood of the available empirical
    data, adjusting the generative distribution $f_{\theta}$ to closely align with
    it. However, this approach can lead to poor generation diversity and overfitting,
    as previously noted.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Entropic Distribution Matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we explore principled approaches to address limitations of existing
    SFT that uses the CE loss, especially when dealing with limited data. We present
    two core principles: the first focuses on the methodology of distribution matching,
    while the second offers guidance on learning from limited data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first principle advocates for a *generative* approach to distribution matching.
    This approach encourages the model to learn from its own generated data and mistakes,
    rather than merely imitating supervised demonstrations. Unlike the traditional
    CE loss, which leads the model to imitate training data labels passively, a generative
    approach involves active learning through self-generated feedback. This principle
    is grounded in cognitive science [[47](#bib.bib47), [20](#bib.bib20)], which demonstrates
    that children learn more effectively through exploration and experimentation,
    adjusting their understanding based on discrepancies between expectations and
    reality. Similarly, research on Generative Adversarial Networks (GANs) [[18](#bib.bib18),
    [24](#bib.bib24)] supports this notion by showing how models can learn to produce
    realistic data through iterative refinement. To summarize, we propose:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 1: The distribution matching approach should be “generative”, meaning
    the model learns from both ground truth supervision and its own generated mistakes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second principle addresses the challenge of overfitting. We draw inspiration
    from neuroscience, specifically the concept of avoiding over-memorization and
    achieving balanced learning. In neuroscience, synaptic plasticity, particularly
    homeostatic plasticity, underscores the importance of maintaining balance in learning
    processes [[56](#bib.bib56), [55](#bib.bib55)]. Overly strengthening certain neural
    connections can lead to rigid, maladaptive behaviors, analogous to how assigning
    excessively high probabilities to observed tokens can result in over-memorization
    in models, thereby limiting their ability to adapt and generalize. Based on these
    insights, especially for limited data, we propose:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle 2: The model should assign higher probabilities to the observed data
    while preventing over-memorization.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Proposed Formulation: Reserve KL with Entropy Regularization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To implement the two principles outlined above, we propose studying the formulation
    of *reverse* KL divergence minimization with maximum entropy regularization. The
    objective is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{f}\mathbb{E}_{x}\big{\{}\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}\left[\log p(y&#124;x)\right]-\mathbb{E}_{y\sim f(\cdot&#124;x)}[\log
    f(y&#124;x)]}_{=-D_{\mathrm{KL}}(f,p)}+\gamma\cdot\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}[\log f(y&#124;x)]}_{{\mathcal{H}}(f)}\big{\}}.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'The first term in this formulation corresponds to the *reverse* KL divergence
    between the target distribution $p$. This term supports Principle 1 by encouraging
    the model to learn from its generated data samples. The second term, entropy regularization,
    aligns with Principle 2 by preventing over-memorization, as it ensures that the
    probabilities for labeled data do not become excessively high. Furthermore, it
    brings another benefit: the output diversity can be improved. In this case, greedy
    sampling can reliably output the knowledge in the training data. We also note
    that adding entropy regularization to the CE loss supports Principle 2 but not
    Principle 1; its limitations are discussed in [Appendix C](#A3 "Appendix C Discussion
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"), and we will show that it is inferior to the proposed approach
    in [Section 5](#S5 "5 Experiments ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity").'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the objective defined in [Equation 1](#S4.E1 "In 4.1 Proposed Formulation:
    Reserve KL with Entropy Regularization ‣ 4 Entropic Distribution Matching ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity") appears promising, it presents significant challenges in practice.
    The main challenge is that we only have access to empirical data from the distribution
    $p$, not its full probability density function, making the reverse KL term impossible
    to compute directly. Additionally, calculating the expectation of the reverse
    KL across the model’s generative distribution is not easy. This paper contributes
    a new algorithm to address these challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Proposed Algorithm: GEM'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present a practical algorithm for solving the optimization
    problem of reverse KL with entropy regularization. As discussed earlier, the key
    is to obtain an estimate for the log probability density function $\log p$ is
    not sequential. We begin by outlining a conceptually simple but technically complicated
    solution. Building on this proposal, we then introduce a more tractable solution
    by new techniques.
  prefs: []
  type: TYPE_NORMAL
- en: An Initial Proposal.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Drawing inspiration from GANs [[18](#bib.bib18), [25](#bib.bib25)], one may
    propose the following formulation for estimating the distribution $p$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{q}\max_{r}\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot&#124;x)}\left[h\left(r(x,y^{\texttt{real}})-r(x,y^{\texttt{gene}})\right)\right].$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Here we use the subscript real to denote the supervised data and gene to denote
    the model-generated data for clarity. In addition, $h$ acts as a discriminator,
    designed to maximize the gap between samples drawn from the data distribution
    $p$. The goal of the opponent $q$ is a linear function, it simplifies to the reward
    maximization problem $\max_{q}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot|x)}[r(x,y^{\texttt{gene}})]$,
    a fact that shall be used later.
  prefs: []
  type: TYPE_NORMAL
- en: 'On its theoretical foundation, Jolicoeur-Martineau [[25](#bib.bib25)] proved
    that the inner maximization problem is to find a divergence function between the
    distributions $p$, and thus the outer minimization problem is to reduce this divergence.
    Therefore, the solved $q$. If we can solve problem ([2](#S4.E2 "Equation 2 ‣ An
    Initial Proposal. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity")), we can substitute $\log p=\log q$, $q$, which are hard
    to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The variables $q$ are introduced to be adversarially trained in [Equation 2](#S4.E2
    "In An Initial Proposal. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution
    Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity");'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution $f$ has been solved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is important to note that optimizing the reward function $r$ can be particularly
    challenging when dealing with sequential data, often requiring Reinforcement Learning
    (RL) algorithms (see, e.g., [[22](#bib.bib22)]). Our initial attempts to implement
    the above proposal were not successful. We introduce a new tractable solution
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 GEM
  prefs: []
  type: TYPE_NORMAL
- en: Dataset ${\mathcal{D}}=\{(x_{i},y_{i}^{\texttt{real}})\}$1:Set $q_{k}=\texttt{softmax}(1/\beta*\log
    f_{\theta_{k}})$2:Define the loss function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 3:Update $\theta_{k+1}=\theta_{k}+\eta\cdot\nabla_{\theta}{{\mathcal{L}}}_{q}(f_{\theta})\mid_{\theta=\theta_{k}}$\Require\For
  prefs: []
  type: TYPE_NORMAL
- en: Proposed Solution.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At a high level, our approach simplifies the process by solving a single-stage
    optimization problem, eliminating the need to first estimate the distribution
    $p$. Our approach involves two key techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reparameterization: We reparameterize the discriminator using the generative
    distribution $f$ as a real-valued function and parameterize it as $\log f(y|x)$,
    ensuring it remains real-valued.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Direct entropy regularization: We introduce entropy regularization for the
    distribution $q$. This technique offers two advantages. First, it establishes
    a connection between $q$, eliminating the need of solving problem ([1](#S4.E1
    "Equation 1 ‣ 4.1 Proposed Formulation: Reserve KL with Entropy Regularization
    ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity")) separately. Second,
    since $q$ has a closed-form solution, it does not require explicit training.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Specifically, our formulation is that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{f}\quad$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\mathbb{E}_{y\sim\pi(\cdot&#124;x)}\left[\log
    f(y&#124;x)\right]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x))=\texttt{softmax}(1/\beta*\log
    f).$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'In this formulation, we optimize $f$. Simultaneously, we optimize $q$, incorporating
    entropy regularization. Fortunately, this yields a closed-form solution, so we
    do not need to maintain or explicitly train $q$ in [Equation 3](#S4.E3 "In Proposed
    Solution. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity"). Note that although $q$ generally, we do not calculate the
    gradient through $q$. This is similar to the target network used in RL [[35](#bib.bib35),
    [32](#bib.bib32)]. We have the following theoretical justification for this formulation.'
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Assume that $h$, ${\mathcal{L}}_{q}(f)$) corresponds to the optimal solution
    of Problem ([1](#S4.E1 "Equation 1 ‣ 4.1 Proposed Formulation: Reserve KL with
    Entropy Regularization ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Proposition 1](#Thmprop1 "Proposition 1\. ‣ Proposed Solution. ‣ 4.2 Proposed
    Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity") implies
    that solving the proposed problem in [Equations 3](#S4.E3 "In Proposed Solution.
    ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")
    and [4](#S4.E4 "Equation 4 ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm: GEM
    ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity") provides the optimal
    solution of reverse KL with entropy regularization in [Equation 1](#S4.E1 "In
    4.1 Proposed Formulation: Reserve KL with Entropy Regularization ‣ 4 Entropic
    Distribution Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). In practice, we can parameterize
    $f$ is calculated because we assume $y^{\texttt{gene}}$, meaning that the proposed
    formulation cannot solve the pure reverse KL minimization problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Intuition and Example.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We provide an intuitive understanding of GEM by explaining its training mechanism
    on a simple model: for a fixed $x\in{\mathcal{X}}$ with $\theta_{x}\in\mathbb{R}^{K}$
    as the linear function described in [Proposition 1](#Thmprop1 "Proposition 1\.
    ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). For a paired sample $(y^{\texttt{real}},y^{\texttt{gene}})=(i,j)$,
    we have the gradient for this sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\theta}{\mathcal{L}}_{q}(f_{\theta})[i,j]=\left\{\begin{array}[]{cc}w_{ij}e_{ij}&amp;\text{
    if }\quad i\neq j\\ \mathbf{0}&amp;\text{ otherwise}\end{array}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here $w_{ij}=p(y^{\texttt{real}}|x)q(y^{\texttt{gene}}|x)$, and $e_{ij}$-th
    element being $1$-th element being $-1$ otherwise. Thus, the gradient of this
    paired data gives a direction for moving the logit $\theta_{x}$-th position to
    $i$.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a numerical example where $\theta_{x}=[2,1]$, resulting in $f=[0.73,0.27]$,
    we have $q=[0.81,0.19]$. Given the data distribution $p=[0.9,0.1]$, leading to
    a relative logit change of $0.2$, resulting in a relative logit change of $0.28$
    due to the induced entropy regularization.
  prefs: []
  type: TYPE_NORMAL
- en: In the above analysis, we see that the distribution $q$, a *narrowed* distribution
    $q$, prioritizes the high-probability regions in $f$ contributes less. This contrasts
    with CE, which would push probabilities of non-labeled tokens towards the labeled
    ones, potentially causing overfitting. We also observe that $h$ for a general
    function $h$ is always equal to $1$ is the log-sigmoid function $h(u)=\log\texttt{sigmoid}(u)=u-\log(1+\exp(u))$,
    it results in a large weight when $y^{\texttt{real}}$ has already become dominant.
    Later on, we will study this function in experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 GEM for Sequential Data
  prefs: []
  type: TYPE_NORMAL
- en: Dataset ${\mathcal{D}}=\{(x_{i},y_{1},\ldots,y_{T})\}$ \Forsample index $i$
    “Reset” data distribution \Fortimestep index $t=1,\ldots,T$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widetilde{x}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\widetilde{{\mathcal{D}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: \EndFor\EndFor2:$f_{\theta}\leftarrow$ \EnsureGenerative model $f_{\theta}$\Require
  prefs: []
  type: TYPE_NORMAL
- en: Extensions to Sequential Data.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the above part, we have derived the algorithm for the case $y$. We can extend
    the formulation in [Equations 3](#S4.E3 "In Proposed Solution. ‣ 4.2 Proposed
    Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity") and [4](#S4.E4
    "Equation 4 ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution
    Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity") to the following:⁴⁴4Generally speaking, the
    prompt $x$ would also be sequential, but this does not affect our discussion and
    formulation as it serves the input to the conditional distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{f}\quad$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\mathbb{E}_{y_{1:T}\sim\pi(\cdot&#124;x)}\left[\log
    f(y_{1:T}&#124;x)\right]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x))$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, we encounter a challenge: the joint distribution of $y_{1:T}$ cannot
    be easily calculated as before. While Monte Carlo estimation—drawing samples to
    approximate the gradient—might seem like a viable solution, we found it does not
    work in experiments. We believe the main reason is that the sample space is huge,⁵⁵5For
    the Llama-3-8B model, its vocabulary size is 128k. For a typical case with a sequence
    length of 2048, the sample size is $128000^{2048}$ is quite different from the
    data distribution $p$ that we aim to learn.⁶⁶6Specifically, pre-trained models
    cannot generate the EOS (end-of-sentence) token properly, resulting in repetitive
    and less informative sequences, even with infinite length. But the supervised
    data has an EOS token and finite length. To bypass this challenge, methods proposed
    in [[9](#bib.bib9), [29](#bib.bib29)] rely on models that has been SFT-trained
    with CE. As mentioned, their methods cannot operate with pre-trained models directly.
    As a result, when we use stochastic sampling to estimate the gradient, it does
    not provide effective feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with the above challenge, we propose decomposing the multi-stage sequential
    optimization problem into multiple single-stage optimization problems and solve
    each efficiently. Concretely, we restrict the distribution matching to the case
    that the prefix samples up to time step $t$ and solves the optimization problem
    at the $t$-th time step as before. Its mathematical formulation is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{f}{\mathcal{L}}_{q}^{\texttt{seq}}(f)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{where}\quad\Delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The main advantage of this formulation is that for each sub-problem, we still
    have access to the exact conditional distribution, so the gradient estimation
    is accurate. The same idea applies to the training of distribution $q$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\left\{\sum_{t=1}^{T}\mathbb{E}_{y^{\texttt{real}}_{1:t-1}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y_{t}\sim\pi(\cdot&#124;x,y_{1:t-1}^{\texttt{real}})}[\log
    f(y_{t}&#124;x,y_{1:t-1}^{\texttt{real}})]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x,y_{1:t-1}^{\texttt{real}}))\right\}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'That is, we still have the closed-form solution that $q(\cdot|x,y_{1:t-1}^{\texttt{real}})=\texttt{softmax}(1/\beta\cdot\log
    f(\cdot|x,y_{1:t-1}^{\texttt{real}}))$ when used in [Equation 5](#S4.E5 "In Extensions
    to Sequential Data. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). We outline the proposed procedure in [Algorithm 2](#alg2
    "In Intuition and Example. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution
    Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity") and its PyTorch code is provided in [Appendix A](#A1
    "Appendix A Implementation of GEM ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity") for reference. We
    acknowledge that our technique draws inspiration from the data distribution “reset”
    trick introduced by [[45](#bib.bib45)] in imitation learning, in which the teacher
    first shows few demonstration actions and then the student is asked to finish
    the other actions in a full trajectory.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present our experiment results for fine-tuning the Llama-3-8B
    model (specifically, its pre-trained version). A brief overview of our experiment
    setting is provided below, with further details available in [Appendix D](#A4
    "Appendix D Experiment Details ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 General-Purpose Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Set-up. In this section, we develop an LLM that is capable of following instructions
    for various prompts. To this end, we utilize the UltraFeedback dataset [[13](#bib.bib13)],
    specifically the version filtered by the HuggingfaceH4 team⁷⁷7[https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).
    This dataset contains prompts from instruction datasets like Evol-Instruct and
    UltraChat, and responses generated by models such as GPT-4 and Llama-2-7B/13B/70B-Chat.
    For more information, see [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Each data point comprises two responses: one selected as the preferred option
    and the other as the rejected option, with the selection made by GPT-4\. In our
    study, we use the preferred response for SFT, a practice commonly adopted in previous
    research [[39](#bib.bib39), [3](#bib.bib3)]. Following [[66](#bib.bib66), [34](#bib.bib34),
    [13](#bib.bib13)] we set the learning rate to $2\times 10^{-5}$, employing a cosine
    learning rate decay schedule, and use a macro batch size of 128\. The maximum
    sequence length, encompassing both the prompt and response, is set to 2,048 tokens.
    Models are trained for three epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We implement the proposed GEM method with $\beta=0.7$. Our primary baseline
    is the standard CE loss. Additionally, we explore a variant incorporating a weight
    decay of 0.1, which has been commonly used in previous studies [[39](#bib.bib39),
    [3](#bib.bib3)]. We refer to this approach as CE + WD. We also implement a method
    called CE + Entropy, which adds an entropy regularization term of 0.1 to the CE
    loss. This method aligns with the proposed Principle 2 but not Principle 1 (see
    [Appendix C](#A3 "Appendix C Discussion ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity") for more discussion).'
  prefs: []
  type: TYPE_NORMAL
- en: Instruction-Following.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first examine the model’s learned ability in terms of instruction-following.
    We follow the IFEval benchmark in [[69](#bib.bib69)], which includes 500 prompts
    from 25 types of verifiable instructions, such as writing more than 400 words.
    Depending on whether we use strict or loose judgment and whether we measure accuracy
    in the prompt space or instruction space, there are four evaluation criteria:
    prompt-level strict accuracy, instruction-level strict accuracy, prompt-level
    loose accuracy, and instruction-level loose accuracy. For all metrics, a higher
    value indicates better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Performance of instruction-following on the benchmark IFEval [[69](#bib.bib69)].
    For all metrics, a higher value means a better instruction following ability.
    The best results are shown in bold, with the second-best underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Instruction-Following |'
  prefs: []
  type: TYPE_TB
- en: '| Strict Accuracy | Strict Accuracy | Loose Accuracy | Loose Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| (Prompt Level) | (Instruction Level) | (Prompt Level) | (Instruction Level)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 36.23 | 46.76 | 40.85 | 50.96 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 37.89 | 47.48 | 42.88 | 52.52 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 36.78 | 47.60 | 40.66 | 51.08 |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 37.34 | 48.20 | 41.96 | 52.64 |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 37.52 | 47.60 | 42.14 | 52.04 |'
  prefs: []
  type: TYPE_TB
- en: 'We evaluate the trained models using *greedy decoding* and present the results
    in [Table 1](#S5.T1 "In Instruction-Following. ‣ 5.1 General-Purpose Fine-tuning
    ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning of
    LLMs: Less Overfitting and Better Diversity"). We observe that CE underperforms
    compared with regularization-based methods, such as weight decay and entropy regularization,
    suggesting that CE suffers from overfitting. On average across the four criteria,
    GEM-LS improves by 1.1 points (2.5% relative) and 1.4 points (3.2% relative) compared
    to CE. We also observe this overfitting in the evaluation perplexity: GEM-LS and
    GEM-Linear achieve lower perplexity (around 3.16) than CE (3.48); see [Appendix E](#A5
    "Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity"). It is important
    to note that this overfitting is not due to over-optimization, as performance
    continues to improve over three training epochs for CE (36.15 in epoch 1, 41.45
    in epoch 2, and 43.70 in epoch 3).'
  prefs: []
  type: TYPE_NORMAL
- en: Creative Writing.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We continue to assess models’ output diversity in two creative writing tasks:
    poem writing and story writing. For poems, we use prompts from the poetry⁸⁸8[https://huggingface.co/datasets/merve/poetry](https://huggingface.co/datasets/merve/poetry)
    dataset on the Huggingface website, which includes 573 poems on themes such as
    love, nature, and mythology. For stories, we design 500 prompts based on the ROC
    story dataset [[36](#bib.bib36)]. In both cases, we prompt the models to write
    a poem or story titled “[X]” with no more than 200 words, where [X] is a title
    from the respective dataset. Following [[26](#bib.bib26)], we use three criteria
    to evaluate diversity: 1) N-gram diversity: the proportion of distinct n-grams
    in a single response (intra-diversity); 2) Self-BLEU diversity: the Self-BLEU
    score, treating one response as a reference among multiple generated responses
    (inter-diversity); 3) Sentence-BERT diversity: the cosine dissimilarity between
    pairs of responses in the embedding space. All criteria range from 0 to 100 (with
    Sentence-BERT diversity scaled by multiplying by 100), where higher values indicate
    greater diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate these metrics, we ask the trained models to generate 16 samples
    using the decoding configuration temperature=1, top_k=50, and top_p=0.9. The evaluation
    results are presented in [Table 2](#S5.T2 "In Creative Writing. ‣ 5.1 General-Purpose
    Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). In this task, we note that weight
    decay does not improve generation diversity, although it has shown effectiveness
    in mitigating overfitting in previous examples. On the other hand, entropy regularization,
    implemented to support Principle 2, brings the benefit of output diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Evaluation of generation diversity in creative tasks of poem writing
    and story writing. For all criterion, a higher value indicates greater diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Poem Writing |  | Story Writing |'
  prefs: []
  type: TYPE_TB
- en: '| N-gram | Self-BLEU | Sentence-BERT |  | N-gram | Self-BLEU | Sentence-BERT
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 48.50 | 72.50 | 21.79 |  | 48.74 | 72.77 | 21.94 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 48.58 | 71.29 | 21.80 |  | 48.85 | 71.73 | 21.79 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 53.74 | 75.82 | 23.80 |  | 53.86 | 76.11 | 23.94 |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 56.50 | 76.73 | 24.73 |  | 56.69 | 76.83 | 24.82 |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 56.55 | 76.31 | 24.63 |  | 56.82 | 76.61 | 24.68 | ![Refer to caption](img/ddc834ad55d41109cd6ca19e2a667790.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Performance of using advanced generation strategies such as best-of-n
    and majority voting in chatting (left), math reasoning (middle) and code generation
    (right) tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Chatting, Math Reasoning, and Code Generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this part, we show that improved generation diversity offers benefits beyond
    creative writing tasks. Specifically, diverse generation, when quality is ensured,
    is advantageous when using advanced generation methods such as Best-Of-N (BON)
    or Majority-Voting (MV) [[59](#bib.bib59)] to find better solutions. This is inline
    with recent advances in scaling up test-time compute [[6](#bib.bib6), [50](#bib.bib50)]
    and self-distillation [[48](#bib.bib48)]. Specifically, we conduct three experiments
    in chatting, math reasoning and code generation below to validate the superiority
    of GEM through its improved generation diversity. In this part, we use the configuration
    temperature=0.6, top_k=50, and top_p=0.9. A lower temperature is chosen here to
    enhance response quality ($0.6$ is the default value in Llama models). The overall
    performance is displayed in [Figure 2](#S5.F2 "In Creative Writing. ‣ 5.1 General-Purpose
    Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity") with detailed results in [Appendix E](#A5
    "Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity"). Specific settings
    and analyses are provided below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For chatting, we assess the model’s ability to generate human-preferred responses.
    We prompt the trained models to answer 805 questions from the AlpacaEval dataset
    [[31](#bib.bib31)]. For each question, the model generates 32 responses and a
    reward model is then used to select the best responses. We employ the reward model
    FsfairX-LLaMA3-RM-v0.1⁹⁹9[https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1](https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1),
    which has demonstrated top performance on RewardBench [[27](#bib.bib27)], making
    it a reliable choice for this task. Since the reward value itself does not mean
    anything, we choose the win rate as a metric. In particular, we estimate the win
    rate over GPT-4’s generated response by the Bradley–Terry model. From [Figure 2](#S5.F2
    "In Creative Writing. ‣ 5.1 General-Purpose Fine-tuning ‣ 5 Experiments ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity"), we observe that GEM-LS can achieve about 3 points improvement
    in the win rate compared with CE.'
  prefs: []
  type: TYPE_NORMAL
- en: For math reasoning, we evaluate performance on the GSM8K [[11](#bib.bib11)]
    benchmark, which contains 1,319 test questions. We use chain-of-thought prompts
    [[61](#bib.bib61)] to guide LLMs to generate 32 responses for each question. We
    assess answer accuracy using both Majority-Voting (MV) [[59](#bib.bib59)] and
    Best-Of-N (BON) methods. Compared with CE, GEM-LS shows improvements of up to
    4.8 points (7.7% relative) with MV and 2.5 points (2.8% relative) with BON. The
    strong performance of BON@32 indicates that while the model might know how to
    solve these questions, it is uncertain about these solutions in generation. This
    aligns with previous research [[28](#bib.bib28)], which found that even 7B language
    models demonstrate strong math reasoning abilities through sampling multiple responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'For code generation, we consider two benchmarks: HumanEval [[8](#bib.bib8)]
    and MBPP [[2](#bib.bib2)]. In these scenarios, the trained models are asked to
    generate Python code, and the executor judges their correctness. The common evaluation
    metric is the pass rate over multiple samples. We ask the trained models to generate
    200 samples to estimate the pass@100\. The generation configuration is the same
    as for the chatting task. We find that weight decay does not show significant
    improvement over CE, while GEM-LS can achieve up to a 7.6-point (10.7% relative)
    improvement over CE on HumanEval and a 6.4-point (9.0% relative) improvement on
    MBPP for pass@100.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Domain-specific Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we conduct experiments with domain-specific datasets. For math
    reasoning, we use the dataset MetaMathQA [[66](#bib.bib66)]. For code generation,
    we use the dataset Magicoder-OSS-Instruct [[62](#bib.bib62)]. The experimental
    setup, including training details and hyperparameters, is the same as before,
    and the specifics are provided in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4196260f5b1a49daea8bad5c9fcc0b26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Performance on GSM8K (left) and MATH (right) when fine-tuning Llama-3-8B
    with the MetaMathQA dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Math Reasoning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present the final answer accuracy for two benchmarks: GSM8K, which has been
    studied previously, and MATH [[21](#bib.bib21)], which is more challenging^(10)^(10)10MATH
    is not studied in models fine-tuned with the UltraFeedback dataset because models
    have poor performance in this task., as shown in [Figure 3](#S5.F3 "In 5.2 Domain-specific
    Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). Following the previous set-up,
    we evaluate performance using Majority Voting over 32 samples (MV@32), and Best-Of-N
    over 32 samples (BON@32). The greedy decoding performance is also reported. Our
    results show significant improvements in both majority voting and greedy decoding
    across both GSM8K and MATH. We observe that the weight decay regularization performs
    well on GSM8K but shows no clear improvement on MATH. In contrast, GEM-LS outperforms
    CE on GSM8K by 1.2 points (1.7% relative), 2.9 points (3.8% relative), and 2.6
    points (2.9% relative) for greedy decoding, MV@32, and BON@32, respectively. On
    the MATH benchmark, GEM-LS shows improvements of 1.9 points (8.0% relative), 1.7
    points (5.8% relative), and 1.6 points (2.7% relative) for the same methods. These
    improvements in greedy decoding indicate that entropy regularization methods effectively
    mitigate overfitting, while the enhancements in MV and BON suggest increased generation
    diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: Code Generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following the previous set-up, we report the pass rate over {1, 10, 100} on
    two key benchmarks, HumanEval and MBPP, in [Figure 4](#S5.F4 "In Code Generation.
    ‣ 5.2 Domain-specific Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity"). We
    find that the Pass@100 performance on HumanEval is reduced compared with the previous
    one while the Pass@100 performance on MBPP improved for all methods. We observe
    that weight decay (WD) does not achive consistent improvement while entropy regularization
    does. Notably, GEM-LS significantly enhances performance over CE: on HumanEval,
    it improves by 4.6 points (11.7% relative) for Pass@1, 6.5 points (11.1% relative)
    for Pass@10, and 9.7 points (14.7% relative) for Pass@100\. On MBPP, GEM-LS achieves
    gains of 3.4 points (6.3% relative) for Pass@1, 6.8 points (10.2% relative) for
    Pass@10, and 8.0 points (11.1% relative) for Pass@100\. These results suggest
    similar conclusions regarding overfitting and generation diversity as before.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/145a0c3451f7789e4cf8380a4e4da763.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Performance on HumanEval (left) and MBPP (right) when fine-tuning
    Llama-3-8B with the MagiCoder-OSS-Instruct dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose an alternative method for the SFT of LLMs to tackle
    the challenges of overfitting and limited generation diversity, which are often
    caused by the aggressive updates of the CE loss and limited data. We demonstrate
    the effectiveness of combining generative distribution matching with entropy regularization.
    We note that the improved diversity also boosts performance in downstream tasks
    when advanced generation methods, such as the best-of-n sampling, are used. Overall,
    our results indicate that the proposed method is well-suited for generative models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We focus on the initial stage of post-training pipeline in this paper and recognize
    that the models trained with our proposed methods can be further refined in subsequent
    stages. Notably, the enhanced diversity achieved by our approach can be advantageous
    in several contexts: it supports scaling up test-time computation [[6](#bib.bib6),
    [50](#bib.bib50)], improves exploration in RL methods [[46](#bib.bib46), [33](#bib.bib33)],
    addresses the preference collapse issue [[64](#bib.bib64)], facilitates self-improvement
    through distillation with best-of-n techniques [[48](#bib.bib48)], and helps mitigate
    mode collapse in synthetic data generation [[49](#bib.bib49), [5](#bib.bib5),
    [63](#bib.bib63)]. We see significant potential for our method in these areas
    and plan to explore these topics in future work.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Yushun Zhang for reading the manuscript and providing useful feedback.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adler et al. [2024] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab
    Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan
    Cohen, et al. Nemotron-4 340b technical report. *arXiv preprint arXiv:2406.11704*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. [2021] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup,
    and Yoshua Bengio. Flow network based generative models for non-iterative diverse
    candidate generation. *Advances in Neural Information Processing Systems*, 34:27381–27394,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertrand et al. [2023] Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis,
    Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining
    of generative models on their own data. *arXiv preprint arXiv:2310.00429*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. [2024] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark,
    Quoc V Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling
    inference compute with repeated sampling. *arXiv preprint arXiv:2407.21787*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in Neural Information
    Processing Systems 33*, pages 1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan
    Gu. Self-play fine-tuning converts weak language models to strong language models.
    *arXiv preprint arXiv:2401.01335*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *Journal of Machine Learning Research*,
    25(70):1–53, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Colton and Wiggins [2012] Simon Colton and Geraint A Wiggins. Computational
    creativity: The final frontier? In *ECAI 2012*, pages 21–26\. IOS Press, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. [2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang
    He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback:
    Boosting language models with scaled ai feedback. In *Forty-first International
    Conference on Machine Learning*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al. [2018] Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, and Nikhil
    Naik. Maximum-entropy fine grained classification. *Advances in neural information
    processing systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al. [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,
    Angela Fan, et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2024] Tingchen Fu, Deng Cai, Lemao Liu, Shuming Shi, and Rui Yan.
    Disperse-then-merge: Pushing the limits of instruction tuning via alignment tax
    reduction. *arXiv preprint arXiv:2405.13432*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gekhman et al. [2024] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir
    Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge
    encourage hallucinations? *arXiv preprint arXiv:2405.05904*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in neural information processing systems*, 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2023] Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chloé
    Clavel. The curious decline of linguistic diversity: Training language models
    on synthetic text. *arXiv preprint arXiv:2311.09807*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gweon et al. [2014] Hyowon Gweon, Hannah Pelton, Jaclyn A Konopka, and Laura E
    Schulz. Sins of omission: Children selectively explore when teachers are under-informative.
    *Cognition*, 132(3):335–341, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho and Ermon [2016] Jonathan Ho and Stefano Ermon. Generative adversarial imitation
    learning. In *Advances in Neural Information Processing Systems 29*, pages 4565–4573,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2023] Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume
    Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in
    large language models. *arXiv preprint arXiv:2310.04363*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jolicoeur-Martineau [2018] Alexia Jolicoeur-Martineau. The relativistic discriminator:
    a key element missing from standard gan. *arXiv preprint arXiv:1807.00734*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jolicoeur-Martineau [2020] Alexia Jolicoeur-Martineau. On relativistic f-divergences.
    In *International Conference on Machine Learning*, pages 4931–4939\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirk et al. [2023] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
    Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding
    the effects of rlhf on llm generalisation and diversity. *arXiv preprint arXiv:2310.06452*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lambert et al. [2024] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda,
    Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
    et al. Rewardbench: Evaluating reward models for language modeling. *arXiv preprint
    arXiv:2403.13787*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2024a] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng,
    Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess
    strong math capabilities. *arXiv preprint arXiv:2403.04706*, 2024a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2024b] Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo
    Garcia, and Mingyi Hong. Getting more juice out of the sft data: Reward learning
    from human demonstration improves sft for llm alignment. *arXiv preprint arXiv:2405.17888*,
    2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2015] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and
    Bill Dolan. A diversity-promoting objective function for neural conversation models.
    *arXiv preprint arXiv:1510.03055*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023a] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2022] Ziniu Li, Tian Xu, and Yang Yu. A note on target q-learning
    for solving finite mdps with a generative oracle. *arXiv preprint arXiv:2203.11489*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023b] Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, and
    Zhi-Quan Luo. Remax: A simple, effective, and efficient method for aligning large
    language models. *arXiv preprint arXiv:2310.10505*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
    What makes good data for alignment? a comprehensive study of automatic data selection
    in instruction tuning. *arXiv preprint arXiv:2312.15685*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
    learning. *Nature*, 518(7540):529–533, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mostafazadeh et al. [2016] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
    He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen.
    A corpus and cloze evaluation for deeper understanding of commonsense stories.
    In *Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 839–849, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O’Mahony et al. [2024] Laura O’Mahony, Leo Grinsztajn, Hailey Schoelkopf, and
    Stella Biderman. Attributing mode collapse in the fine-tuning of large language
    models. In *ICLR 2024 Workshop on Mathematical and Empirical Understanding of
    Foundation Models*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems 35*, pages 27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padmakumar and He [2023] Vishakh Padmakumar and He He. Does writing with language
    models reduce content diversity? *arXiv preprint arXiv:2309.05196*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pereyra et al. [2017] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz
    Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident
    output distributions. *arXiv preprint arXiv:1701.06548*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross et al. [2011] Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction
    of imitation learning and structured prediction to no-regret online learning.
    In *Proceedings of the 14th International Conference on Artificial Intelligence
    and Statistics*, pages 627–635, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv*, 1707.06347,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schulz and Bonawitz [2007] Laura E Schulz and Elizabeth Baraff Bonawitz. Serious
    fun: preschoolers engage in more exploratory play when evidence is confounded.
    *Developmental psychology*, 43(4):1045, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sessa et al. [2024] Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot,
    Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe
    Friesen, Geoffrey Cideron, et al. Bond: Aligning llms with best-of-n distillation.
    *arXiv preprint arXiv:2407.14622*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shumailov et al. [2023] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin
    Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on
    generated data makes models forget. *arXiv preprint arXiv:2305.17493*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snell et al. [2024] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
    Scaling llm test-time compute optimally can be more effective than scaling model
    parameters. *arXiv preprint arXiv:2408.03314*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2020] Ruoyu Sun, Tiantian Fang, and Alexander Schwing. Towards a
    better global loss landscape of gans. *Advances in Neural Information Processing
    Systems*, 33:10186–10198, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. [2024] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tunstall et al. [2023] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. *arXiv
    preprint arXiv:2310.16944*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turrigiano [2012] Gina Turrigiano. Homeostatic synaptic plasticity: local and
    global mechanisms for stabilizing neuronal function. *Cold Spring Harbor perspectives
    in biology*, 4(1):a005736, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turrigiano [2008] Gina G Turrigiano. The self-tuning neuron: synaptic scaling
    of excitatory synapses. *Cell*, 135(3):422–435, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Advances in Neural Information Processing Systems 30*, pages
    5998–6008, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vieillard et al. [2020] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier
    Pietquin, Rémi Munos, and Matthieu Geist. Leverage the average: an analysis of
    kl regularization in reinforcement learning. In *Advances in Neural Information
    Processing Systems 33*, pages 12163–12174, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *Proceedings of the 11st International
    Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2024] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. Magicoder: Empowering code generation with oss-instruct. In *Forty-first
    International Conference on Machine Learning*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2024] Ting Wu, Xuefeng Li, and Pengfei Liu. Progress or regress?
    self-improvement reversal in post-training. *arXiv preprint arXiv:2407.05013*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2024] Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong
    Fang, Qi Long, and Weijie J Su. On the algorithmic bias of aligning large language
    models with rlhf: Preference collapse and matching regularization. *arXiv preprint
    arXiv:2405.16455*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Young et al. [2024] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,
    Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open
    foundation models by 01\. ai. *arXiv preprint arXiv:2403.04652*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2023] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath:
    Bootstrap your own mathematical questions for large language models. *arXiv preprint
    arXiv:2309.12284*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2024] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat.
    When scaling meets llm finetuning: The effect of data, model and finetuning method.
    *arXiv preprint arXiv:2402.17193*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023a] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
    Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment.
    *arXiv preprint arXiv:2305.11206*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2023b] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma,
    Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation
    for large language models. *arXiv preprint arXiv:2311.07911*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Implementation of GEM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1def  gem_loss(logits,  labels,  beta=0.7,  ignore_index=-100,  h="linear"):23  shift_logits  =  logits[...,  :-1,  :].contiguous()4  shift_labels  =  labels[...,  1:].contiguous()56  mask  =  shift_labels  !=  ignore_index7  shift_logits  =  shift_logits[mask]8  shift_labels  =  shift_labels[mask]910  with  torch.no_grad():11  logits_on_labels  =  torch.gather(12  shift_logits,  dim=-1,  index=shift_labels.unsqueeze(-1)13  ).squeeze(-1)1415  logits_diff  =  shift_logits  -  logits_on_labels.unsqueeze(-1)16  if  h  ==  "linear":17  weights  =  torch.ones_like(logits_diff)18  elif  h  ==  "log_sigmoid":19  weights  =  F.sigmoid(0.01  *  logits_diff)20  else:21  raise  ValueError(h)2223  gene_log_probs  =  F.log_softmax(shift_logits,  dim=-1)24  q_probs  =  torch.exp(25  F.log_softmax(shift_logits  /  beta,  dim=-1)26  ).detach()2728  real_log_probs  =  torch.gather(29  gene_log_probs,  dim=-1,  index=shift_labels.unsqueeze(-1)30  ).squeeze(-1)3132  loss  =  -torch.sum(33  q_probs  *  weights  *  (real_log_probs.unsqueeze(-1)  -  gene_log_probs),  dim=-134  ).mean()3536  return  loss'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 1: Pytorch Code of GEM'
  prefs: []
  type: TYPE_NORMAL
- en: We have two remarks regarding the implementation above. First, we use a coefficient
    of $0.01$ to scale the input in the log-sigmoid function. This ensures that the
    function behaves nearly linearly. Second, this implementation requires almost
    the same GPU memory and computation time as the CE loss.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Proof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Proof of [Proposition 1](#Thmprop1 "Proposition 1\. ‣ Proposed Solution. ‣
    4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity").'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When $h$ is a linear function, we have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathcal{L}}_{q}(f)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim
    q(\cdot&#124;x)}\left[\log f(y^{\texttt{real}}&#124;x)\right]-\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot&#124;x)}\left[\log f(y^{\texttt{gene}}&#124;x)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim p(\cdot&#124;x)}\left[\log
    f(y^{\texttt{real}}&#124;x)\right]-\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{gene}}\sim
    q(\cdot&#124;x)}\left[\log f(y^{\texttt{gene}}&#124;x)\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: For any $x\in{\mathcal{X}}$, we have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial{\mathcal{L}}}{\partial f}=\frac{p-q}{f}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'To calculate the stationary point of ${\mathcal{L}}$. Since $q=\texttt{softmax}(1/\beta\cdot\log
    f)$. As analyzed in [Proposition 2](#Thmprop2 "Proposition 2\. ‣ Appendix B Proof
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"), for $\beta=1/(\gamma+1)$, this corresponds to the the
    optimal solution of minimizing reverse KL with entropy regularization.'
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For the entropy-regularized KL minimization problem in [Equation 1](#S4.E1
    "In 4.1 Proposed Formulation: Reserve KL with Entropy Regularization ‣ 4 Entropic
    Distribution Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"), in the function space, we have
    the optimal solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f^{\star}(y&#124;x)=\frac{1}{Z_{x}}p(y&#124;x)^{1/(\gamma+1)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{x}$.
  prefs: []
  type: TYPE_NORMAL
- en: The proof is based on the optimality condition of constrained optimization.
    Its proof can be found in the previous literature (see, e.g., [[58](#bib.bib58),
    Appendix A]). We note that the above closed-form solution cannot be applied in
    practice because we do not have access to the density function of the data distribution
    $p$.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ca919f06839283b044ea4dfb5540f84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Distributions of next-token probabilities for trained models with
    the UltraFeedback dataset, presented from top to bottom: CE, CE+Entropy, GEM-LS.
    The prompt is “Give me a single-digit number”. The top 300 probabilities are shown
    with a subsampling rate of 20 for clear visualization. A red dotted line indicates
    the probability threshold of $10^{-4}$. The figure demonstrates that the CE+Entropy
    model has a longer tail with higher probabilities assigned to some nonsensical
    tokens, marked with crosses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We discuss the formulation of forward KL with entropy regularization in this
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{f}\mathbb{E}_{x}\big{\{}\underbrace{\mathbb{E}_{y\sim
    p(\cdot&#124;x)}[\log f(y&#124;x)]}_{=-D_{\mathrm{KL}}(p,f)+\text{constant}}+\gamma\cdot\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}[-\log f(y&#124;x)]}_{={\mathcal{H}}(f)}\big{\}}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'This formulation supports the proposed Principle 2 but not Principle 1\. We
    find that this formulation leads to an improper increase in tail probabilities
    when maximizing the entropy, as illustrated in [Figure 5](#A3.F5 "In Appendix
    C Discussion ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs:
    Less Overfitting and Better Diversity"). In the context of LLMs, this increase
    often translates into nonsensical tokens in the vocabulary, leading to undesirable
    generation outputs (if additional strategies like top-k and top-p sampling are
    not used). A concrete example is provided in [Table 3](#A3.T3 "In Appendix C Discussion
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). The core issue arises because the gradient of the entropy
    regularizer can dominate for tokens with low probabilities. Specifically, the
    gradient of the forward KL is computed as $-p/f$. Consequently, for tokens with
    low probabilities in both $f$, the gradient given by the forward KL is much smaller
    than that given by the entropy regularizer, thus disproportionately increasing
    the tail probabilities. In contrast, the proposed reverse KL formulation with
    entropy regularization does not have this issue. This is because the optimization
    is defined over the generative distribution $f$ in our formulation, ensuring balanced
    gradients even for tokens with low probabilities (refer to [Equation 6](#A2.E6
    "In Proof of Proposition 1\. ‣ Appendix B Proof ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Examples of generated sentences for models trained with the UltraFeedback
    dataset. The symbol “[…]” indicates good sentences that have been omitted to save
    space. The vanilla generation configuration temperature=1, top_k=None, and top_p=1.0
    is used. This example shows that CE with entropy regularization can generate nonsensical
    tokens (shown in red) due to the increase in tail probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Whats the largest star in our galaxy? |'
  prefs: []
  type: TYPE_TB
- en: '| CE | Hello! Atlantis is a legendary city that was said to have existed in
    ancient Greece. According to the story, it was a highly advanced and prosperous
    city that was located on an island in the ocean. […] |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | Hello! Atlantis Documentary is a 2019 American documentaryéhoflict
    film directed by Já oblík and produced by Werner Herzog. The film explores the
    history and legacy of Atlantis, $\Box{}$ an ancient Greek city-state that was
    said to have_calendar knowledge and advanced technology, through interviews with
    scholars and historians.ython |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | Hello there! As a helpful, respectful, and honest assistant, I’d
    be happy to help you explore the fascinating topic of Atlantis! Atlantis is an
    ancient Greek myth that tells the story of a legendary realm said to have existed
    in the Atlantic Ocean, west of the Pillars of Hercules. […] |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All experiments are conducted using A800-80GB GPUs with the DeepSpeed distributed
    training framework, utilizing ZeRO-2 and gradient checkpointing without offloading.
    We use flash-attention-2 with deterministic backward for reproducibility. The
    experiments are based on the pretrained Llama-3-8B model, using Adam as the optimizer
    with a global batch size of 128\. Following [[66](#bib.bib66), [34](#bib.bib34),
    [13](#bib.bib13)], the learning rate is set to 2e-5, with a warm-up ratio of 0.03
    and cosine learning rate decay. Training is performed over 3 epochs. All supervised
    datasets are formatted into the chat format using the Llama-3-8B-Instruct’s tokenizer.
    When generation of responses is required for evaluation, we use the vLLM to accelerate
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: D.1 UltraFeedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the dataset filtered by HuggingfaceH4 team, which is available at [https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).
    The dataset contains 61,135 training samples and 1,000 test samples. For training,
    we set the maximum sequence length to 2,048, dropping longer sequences and padding
    shorter ones. To achieve a global batch size of 128, we use a per-device batch
    size of 4, a gradient accumulation step of 4, and 4 GPUs. The training times takes
    about 24 GPU hours. For the CE method, we have tuned hyperparameters for weight
    decay and entropy regularization, selecting values from $\{0.1,0.01,0.001\}$ provided
    the best overall results.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics, including perplexity, and entropy, are based on these 1,000
    test samples. For entropy calculation, we compute the conditional entropy, whose
    expectation can be calculated exactly, and average over the sequence. For the
    instruction-following evaluation, we use the IFEval benchmark from [[69](#bib.bib69)].
    We apply greedy decoding with a maximum generation length of 1,024 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: For the diversity evaluation in poem writing, we use prompts derived from the
    poetry dataset on the Huggingface website, which includes 573 poems on themes
    like love, nature, and mythology by poets such as William Shakespeare. We prompt
    the trained models with questions like, “Write a poem titled ‘[X]’ with no more
    than 200 words,” where [X] is a title from the dataset. For story writing, we
    create 500 prompts based on the ROC Story dataset (2017 winter) [[36](#bib.bib36)],
    asking models to “Write a story titled ‘[X]’ with no more than 200 words,” where
    [X] is a title from the dataset. The maximum number of generation tokens is set
    to 512\. The evaluation script follows the methodology from previous work by [[26](#bib.bib26)],
    using the script available at [https://github.com/facebookresearch/rlfh-gen-div](https://github.com/facebookresearch/rlfh-gen-div).
    For each question, 16 samples with the generation configuration temperature=1.0,
    top_k=50, top_p=0.9 is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the chat evaluation, we use the 805 test questions from the AlpacaEval
    dataset and employ the reward model FsfairX-LLaMA3-RM-v0.1. The maximum generation
    sequence length is set to 2048\. For each question, 32 samples are generated with
    the configuration temperature=0.6, top_k=50, top_p=0.9. To calculate the win rate,
    we use the Bradley-Terry model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(y\succ y^{\prime}\mid x)=\frac{\exp(r(x,y))}{\exp(r(x,y))+\exp(r(x,y^{\prime}))}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We use GPT-4 generated responses as a baseline for calculating the win rate,
    specifically the gpt4_1106_preview^(11)^(11)11[https://github.com/tatsu-lab/alpaca_eval/blob/main/results/gpt4_1106_preview/model_outputs.json](https://github.com/tatsu-lab/alpaca_eval/blob/main/results/gpt4_1106_preview/model_outputs.json)
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the math reasoning task on GSM8K, we use the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A4.SS1.p6.pic1" class="ltx_picture" height="74.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,74.6) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="47.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Your task is to answer the
    question below. Give step-by-step reasoning before you answer, and when you’re
    ready to answer, please use the format ”The answer is: …”. Question: {question}</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Answer extraction from the generated responses follows the approach from previous
    work [[66](#bib.bib66)], using the script available at [https://github.com/meta-math/MetaMath/blob/main/eval_gsm8k.py](https://github.com/meta-math/MetaMath/blob/main/eval_gsm8k.py).
    For each question, 32 responses are generated with the configuration temperature=0.6,
    top_k=50, top_p=0.9. The reported accuracy is based on 1,319 test questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the code generation tasks on HumanEval and MBPP, there are 164 test questions
    for HumanEval and 378 test questions for MBPP. We use the prompt from [[62](#bib.bib62)]:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="A4.SS1.p9.pic1" class="ltx_picture" height="58" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,58) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="30.44" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an exceptionally intelligent
    coding assistant that consistently delivers accurate and reliable responses to
    user instructions. @@ Instruction {instruction}</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: For each question, 200 responses are generated with the configuration temperature=0.6,
    top_k=50, top_p=0.9 to estimate the pass rate. The evaluation scripts are from
    [https://github.com/ise-uiuc/magicoder/blob/main/experiments/text2code.py](https://github.com/ise-uiuc/magicoder/blob/main/experiments/text2code.py).
  prefs: []
  type: TYPE_NORMAL
- en: D.2 MagiCoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the MagiCoder-OSS-Instruct dataset [[62](#bib.bib62)], which contains
    74,197 training samples and 1,000 test samples (randomly selected from the original
    training set). The maximum sequence length for training is 1,024\. To achieve
    a global batch size of 128, we use a per-device batch size of 8, gradient accumulation
    steps of 2, and 8 GPUs. The training takes approximately 24 GPU hours. The evaluation
    method is the same as previously described.
  prefs: []
  type: TYPE_NORMAL
- en: D.3 MetaMathQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the MetaMathQA dataset [[66](#bib.bib66)]. To make the code generation
    task manageable, we select a subset of 79,000 samples for training and 1,000 samples
    for evaluation. The maximum sequence length for training is set to 1,024\. To
    achieve a global batch size of 128, we use a per-device batch size of 8, gradient
    accumulation steps of 2, and 8 GPUs. Training takes approximately 24 GPU hours.
    The evaluation method is as previously described. For the MATH task, the prompt
    is the same as for the GSM8K task.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Additional Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E.1 General Purpose Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next-Token Prediction Distributions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We demonstrate the distribution collapse issue associated with the CE method
    using three simple prompts for the trained LLMs: 1) “Complete this sequence with
    a single letter: A, B, C, ___”; 2) “Give me a single-digit number”; and 3) “Tell
    me a type of fruit”. All prompts are designed to have answers with 1 token for
    visualization.^(12)^(12)12For the first prompt, while “D” is the most likely answer,
    “A” could also be a valid response due to the pattern A, B, C, A, B, C, $\ldots$.
    The distributions are visualized in [Figure 6](#A5.F6 "In Next-Token Prediction
    Distributions. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional Results
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). We see GEM-trained models produce flatter distributions,
    indicating support for multiple possible answers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0fc8b18e3a29d0aa15dfbc3081672c00.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Prompt: Complete this sequence with a single letter: A, B, C, ___'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1edc0cbd6fd4a807d443e6a974b76673.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Prompt: Give me a single-digit number.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9875282de62b40b90f2c81c7ac081c2c.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) Prompt: Tell me a type of fruit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Distributions of next-token probabilities for trained models with
    the UltraFeedback dataset, presented from left to right: CE, CE+WD, CE+Entropy,
    and GEM-Linear, and GEM-LS. Only top-10 probabilities are visualized for clarity.
    These examples highlight the issue of limited generation diversity in CE.'
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity and Entropy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For trained models, we also examine two statistics: perplexity, and entropy
    of the output distribution on 1,000 evaluation samples from the Ultrafeedback
    dataset. Results are reported in [Figure 7](#A5.F7 "In Perplexity and Entropy.
    ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional Results ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity").
    Using CE as a baseline, we make several observations. First, weight decay does
    not significantly change the statistics. Second, directly incorporating entropy
    regularization increases both perplexity and entropy considerably. Notably, this
    increase is mainly due to relatively large tail probabilities. Third, GEM generally
    reduces perplexity while increasing entropy. As a side note, the reduced evaluation
    perplexity does not directly translate to better performance in the area of LLMs
    (see e.g., [[68](#bib.bib68)]), but it does imply that GEM-trained models tend
    to favor grounded answers with high probability, thus enhancing diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52853a04b05ddea849afbb1f938cfb87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Evaluation perplexity and entropy. Models are trained with the UltraFeedback
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Chatting, Math Reasoning, and Code Generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We provide the detailed results in [Tables 4](#A5.T4 "In Chatting, Math Reasoning,
    and Code Generation. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional
    Results ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity"), [5](#A5.T5 "Table 5 ‣ Chatting, Math Reasoning,
    and Code Generation. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional
    Results ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity") and [6](#A5.T6 "Table 6 ‣ Chatting, Math Reasoning,
    and Code Generation. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional
    Results ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity"). We observe that even with less generation
    samples, GEM also shows better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Evaluation of reward and win rate on AlpacaEval dataset. Models are
    trained with the UltraFeedback dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reward |  | Win Rate |'
  prefs: []
  type: TYPE_TB
- en: '| BON@4 | BON@8 | BON@16 | BON@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 1.06 | 1.43 | 1.86 | 2.39 |  | 26.59 | 31.35 | 37.43 | 46.61 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 1.09 | 1.47 | 1.85 | 2.41 |  | 27.17 | 32.00 | 37.59 | 46.98 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 1.11 | 1.48 | 1.89 | 2.46 |  | 26.86 | 31.83 | 37.84 | 47.69
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 1.12 | 1.52 | 1.94 | 2.51 |  | 27.27 | 32.36 | 38.76 | 48.50
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 1.11 | 1.52 | 1.96 | 2.56 |  | 26.98 | 32.53 | 39.18 | 49.46 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Evaluation of accuracy on the math reasoning task GSM8K. Models are
    trained with the UltraFeedback dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | GSM8K |  |'
  prefs: []
  type: TYPE_TB
- en: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 51.63 | 55.57 | 58.61 | 62.17 |  | 65.28 | 74.68 | 82.11 | 90.22 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 54.51 | 58.76 | 62.47 | 65.66 |  | 69.90 | 77.48 | 84.46 | 90.45
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 53.75 | 56.63 | 60.58 | 64.44 |  | 67.32 | 76.57 | 83.93 | 91.21
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 53.68 | 58.07 | 62.77 | 65.58 |  | 69.83 | 79.30 | 86.50 | 91.96
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 55.95 | 60.42 | 64.82 | 67.02 |  | 70.05 | 79.68 | 86.96 | 92.72
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Performance of pass rate on the code generation tasks HumanEval and
    MBPP. Models are trained with the UltraFeedback dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | HumanEval |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| Pass@10 | Pass@20 | Pass@50 | Pass@100 |  | Pass@10 | Pass@20 | Pass@50 |
    Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 58.06 | 62.51 | 67.50 | 70.88 |  | 62.71 | 65.73 | 69.13 | 71.18 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 56.18 | 61.53 | 67.85 | 71.91 |  | 63.13 | 66.35 | 69.40 | 71.35
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 58.85 | 64.02 | 70.29 | 74.44 |  | 65.50 | 68.75 | 71.77 | 73.48
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 60.34 | 66.12 | 73.12 | 77.97 |  | 64.54 | 68.57 | 72.30 | 74.33
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 60.94 | 66.95 | 73.83 | 78.47 |  | 67.28 | 71.50 | 75.50 | 77.64
    |'
  prefs: []
  type: TYPE_TB
- en: E.2 Domain-specific Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We provide the detailed results in [Tables 7](#A5.T7 "In E.2 Domain-specific
    Fine-tuning ‣ Appendix E Additional Results ‣ Entropic Distribution Matching in
    Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity"), [8](#A5.T8
    "Table 8 ‣ E.2 Domain-specific Fine-tuning ‣ Appendix E Additional Results ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity") and [9](#A5.T9 "Table 9 ‣ E.2 Domain-specific Fine-tuning ‣
    Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). The results indicate that GEM
    outperforms CE even with fewer generated samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Evaluation of accuracy on the math reasoning task GSM8K. Models are
    trained with the MetaMathQA dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | GSM8K |  |'
  prefs: []
  type: TYPE_TB
- en: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 73.46 | 73.77 | 75.13 | 76.57 |  | 76.50 | 80.74 | 85.14 | 90.67 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 73.84 | 75.06 | 76.50 | 78.24 |  | 77.94 | 81.05 | 86.05 | 90.67
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 75.06 | 76.04 | 77.71 | 79.68 |  | 79.61 | 83.70 | 88.70 | 92.95
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 74.83 | 75.82 | 78.09 | 78.77 |  | 81.43 | 85.60 | 89.69 | 93.56
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 75.21 | 76.35 | 77.33 | 79.53 |  | 80.82 | 85.06 | 89.31 | 93.33
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Evaluation of accuracy on the math reasoning task MATH. Models are
    trained with the MetaMathQA dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | MATH |  |'
  prefs: []
  type: TYPE_TB
- en: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 26.40 | 27.04 | 28.30 | 29.34 |  | 33.20 | 39.98 | 48.20 | 58.46 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 26.20 | 27.02 | 28.38 | 29.56 |  | 33.22 | 39.32 | 47.54 | 57.10
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 28.06 | 29.26 | 30.34 | 31.20 |  | 35.58 | 41.84 | 50.66 | 59.64
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 27.62 | 29.30 | 30.64 | 31.48 |  | 36.82 | 43.74 | 52.04 | 60.30
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 27.46 | 28.88 | 29.92 | 31.00 |  | 36.00 | 42.98 | 50.96 | 60.12
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Performance of pass rate on the code generation tasks HumanEval and
    MBPP. Models are trained with the MagiCoder-OSS-Instruct dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | HumanEval |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| Pass@10 | Pass@20 | Pass@50 | Pass@100 |  | Pass@10 | Pass@20 | Pass@50 |
    Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '| CE | 58.71 | 61.50 | 64.18 | 65.86 |  | 66.54 | 68.68 | 70.76 | 71.95 |'
  prefs: []
  type: TYPE_TB
- en: '| CE+WD | 58.33 | 61.06 | 63.77 | 65.89 |  | 65.96 | 68.38 | 70.67 | 71.89
    |'
  prefs: []
  type: TYPE_TB
- en: '| CE+Entropy | 58.66 | 62.66 | 66.79 | 69.17 |  | 69.47 | 71.76 | 73.79 | 75.02
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-Linear | 58.69 | 62.39 | 67.16 | 70.64 |  | 72.00 | 74.54 | 76.74 | 78.08
    |'
  prefs: []
  type: TYPE_TB
- en: '| GEM-LS | 65.15 | 68.73 | 72.64 | 75.58 |  | 73.30 | 75.90 | 78.42 | 79.97
    |'
  prefs: []
  type: TYPE_TB
