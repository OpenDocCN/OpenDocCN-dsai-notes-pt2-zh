- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:34:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.10642](https://ar5iv.labs.arxiv.org/html/2408.10642)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shiming Xie First author    Hong Chen    Fred Yu    Zeye Sun    Xiuyu Wu   
    {shiming.xsm, wuyi.chen, fred.yf, zeye.szy, wuxiuyu.wxy }@antgroup.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instruct LLM provide a paradigm used in large scale language model to align
    LLM to human preference. The paradigm contains supervised fine tuning and reinforce
    learning from human feedback. This paradigm is also used in downstream scenarios
    to adapt LLM to specific corpora and applications. Comparing to SFT, there are
    many efforts focused on RLHF and several algorithms being proposed, such as PPO,
    DPO, IPO, KTO, MinorDPO and etc. Meanwhile most efforts for SFT are focused on
    how to collect, filter and mix high quality data. In this article with insight
    from DPO and MinorDPO, we propose a training metric for SFT to measure the discrepancy
    between the optimized model and the original model, and a loss function MinorSFT
    that can increase the training effectiveness, and reduce the discrepancy between
    the optimized LLM and original LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM trained on very large corpora is extremely powerful language model for completion
    tasks. SFT and RLHF(Ouyang et al., ([2022](#bib.bib6)), Ziegler et al., ([2020](#bib.bib14)))
    are two techniques that used to expose the LLM capability and align LLM answer
    to human instructions. With the increasing reasoning abilities, LLM are widely
    used in industries, and SFT and RLHF are also used to inject domain knowledge
    into LLM by training on domain corpora.
  prefs: []
  type: TYPE_NORMAL
- en: In the past most works are focused on RLHF and several algorithms are proposed,
    such as PPOSchulman et al., ([2017](#bib.bib11)), DPO(Rafailov et al., ([2023](#bib.bib10))),
    IPO(Azar et al., ([2023](#bib.bib2))), KTO(Ethayarajh et al., ([2024](#bib.bib4))),
    MinorDPO(Xie et al., ([2024](#bib.bib12))) and etc. One important point of RLHF
    is to constraint the optimized model not to deviate from the original model too
    much during the training, and thus PPO use KL constraints, DPO use a sample level
    dynamic coefficient related to distance between the preference pair, and IPO use
    a targeted distance between the preference pair and etc. The purpose of this constraint
    is to avoid over-fit on the domain corpora and to maintain LLM generalities. It’s
    an important hypothesis that the base model is powerful enough and the training
    should not change the language distribution too much to maintain the generality
    and diversity.
  prefs: []
  type: TYPE_NORMAL
- en: While back to SFT, most works are focused on collect, filter and mix high quality
    data. High quality data is undoubtedly important to get a high qualified and usable
    LLM, while the aforementioned hypothesis that optimized model should not deviate
    far from the original model is still important.
  prefs: []
  type: TYPE_NORMAL
- en: Our main contribution is that we introduce a training metrics used in DPO and
    MinorDPO into SFT phase, and propose an improved loss function MinorSFT. MinorSFT
    use a sample level coefficient to control the learning strength. It constraints
    the discrepancy more compared to raw SFT and may provide better performance result,
    at the cost of an additional hyper parameter and more computation.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforce Learning from human feedback(Ouyang et al., ([2022](#bib.bib6)), Ziegler
    et al., ([2020](#bib.bib14))) is a popular technique to align LLM to human preference.
    It uses SFT to train a supervised LLM on data of sampled prompt and labeled answer,
    then trains a reward model on preference pairs from human feedback and finally
    uses RL algorithm like PPO Schulman et al., ([2017](#bib.bib11)) to train an optimized
    LLM. The RL part contains a KL-divergence constraint to prevent the optimized
    LLM deviating too much from the base model.
  prefs: []
  type: TYPE_NORMAL
- en: DPO(Rafailov et al., ([2023](#bib.bib10))) is a simplified RL algorithm that
    optimize LLM directly on the preference data using a cross-entropy classification
    loss. DPO objective is to increase the relative log probability of preferred answer
    to dis-preferred answer. It incorporates a dynamic, sample level importance weight
    scaled by hyper-parameter $\beta$ account for the strength of the KL-divergence
    constraint. DPO introduces an important concept that LLM model itself is an implicit
    reward model, which means the LLM model can somehow measure the corpora during
    training phase. Rafailov et al., ([2024](#bib.bib9)) derive that DPO is token-level
    MDP and works as a general inverse Q-learning algorithm in a theoretical way.
  prefs: []
  type: TYPE_NORMAL
- en: MinorDPO(Xie et al., ([2024](#bib.bib12))) is a DPO variant. It justifies hyper-parameter
    $\beta$ in DPO is a constraint relate to the relative log probability margin of
    the preference pair, instead of the KL-divergence constraint. It introduces MinorDPO
    loss to reduce penalty on the reject(dis-preferred) answer to prevent over penalty
    on the reject answer, which implicitly keep to the hypothesis that optimized model
    should not deviate too much from the base model.
  prefs: []
  type: TYPE_NORMAL
- en: IPO(Azar et al., ([2023](#bib.bib2))) proves DPO may be prone to over-fitting
    when preferred probability over dis-preferred probability that is close to 1\.
    In IPO objective it uses a target value relate to the hyper-parameter $\beta$
    for the relative log probability of preferred to dis-preferred. However, it is
    somehow same as DPO, that it focuses on the relative log probability margin, so
    it has same problem as DPO mentioned in MinorDPO.
  prefs: []
  type: TYPE_NORMAL
- en: KTO(Ethayarajh et al., ([2024](#bib.bib4))) proposes human aware loss function.
    It separates the preference pair loss into two losses so that it doesn’t purely
    rely on paired preference data. Inside each separated loss, it estimates the KL
    term by matching input x’ with unrelated outputs z in the same batch, but without
    back-propagate through the KL term, and thus it also introduces an implicit constraint
    on the gradient which in turn affect the learning strength and deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Llama 3 (Dubey et al., ([2024](#bib.bib3))) presents a detailed way to collect,
    filter and mix high quality data for SFT and RL. For the RL part it uses DPO with
    an additional negative log-likelihood loss, similar to Pang et al., ([2024](#bib.bib8))
    and mentioned in Pal et al., ([2024](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Many efforts focus on RL part and use explicit or implicit constraints to limit
    optimized LLM deviation to reduce model regression. Inspired by DPO and MinorDPO,
    we think it worth a try to take the hypothesis into SFT to reduce LLM deviation
    and maintain diversity, and maybe able to increase performance further.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Minor SFT derivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DPO(Rafailov et al., ([2023](#bib.bib10))) derives its objective from RL in
    a closed form.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: It introduces $\hat{r}_{\theta}(x,y)=\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$.
    DPO objective is to maximize rewards margin between the preference pair.
  prefs: []
  type: TYPE_NORMAL
- en: The MinorDPO objective adds an additional constraints to dis-preferred samples,
    by replacing the original penalty $log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: So when probability of optimized model on dis-preferred sample is less than
    probability of reference model on dis-preferred sample, which means $log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}<=0$,
    it will ignore the dis-preferred , and focus on handling the preferred sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under this situation, the formula can be rewritten as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: We simply name this method SFT using DPO. Eq. [3](#S3.E3 "In 3.1 Minor SFT derivation
    ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation") tries to maximize reward on the preferred sample.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s derive the gradient equation of Eq. [3](#S3.E3 "In 3.1 Minor SFT derivation
    ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation")
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Compared to raw SFT loss gradient equation.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: $m$ is length of the answer. Normally, SFT use average over the answer, while
    DPO use sum over the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Eq. [4](#S3.E4 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT
    loss for LLM fine-tune to increase performance and reduce model deviation") and
    Eq. [5](#S3.E5 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT loss for
    LLM fine-tune to increase performance and reduce model deviation"), we see Eq.
    [4](#S3.E4 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT loss for LLM
    fine-tune to increase performance and reduce model deviation") $\nabla_{\theta}L_{preferred}$.
  prefs: []
  type: TYPE_NORMAL
- en: While Eq. [5](#S3.E5 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT loss
    for LLM fine-tune to increase performance and reduce model deviation") $\nabla_{\theta}L_{raw\_sft}$.
  prefs: []
  type: TYPE_NORMAL
- en: Here we introduce the sample level dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    into raw sft loss, and we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Since at the start of the training, $\pi_{\theta}$, so we multiply 2 to make
    it closer to the raw sft and get final MinorSFT gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 3.2 LLM Deviation metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Back to the reward aforementioned $\hat{r}_{\theta}(x,y)=\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$,
    DPO objective is to maximize the rewards margin between the preference pairs.
    And we can also treat the reward as a metric that measure
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: complexity of the sample. As the reward is $\beta(log\pi_{\theta}(y|x)-log\pi_{ref}(y|x))$
    gives high log probability, which indicate the sample is low complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: deviation of the model. If we treat the corpora as identical distribution, then
    high rewards mean high relative log probability difference between the optimized
    LLM $\pi_{\theta}$, which indicate a high deviation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So the sample dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    in Minor SFT has a clear physic meaning, lower complexity samples have a smaller
    coefficient than higher complexity samples. In this way it dynamically adjusts
    the training data distribution and the whole training process will pay more attention
    on higher complexity samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, this metric measures how far the optimized model deviate from the
    original model during training. But it has two limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reward is related to the hyper-parameter $\beta$ is not able to do comparison.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This reward is related to answer length. so training of distribution with different
    answer length is not able to do comparison.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We need a normalized metric that can be compared not only with different $\beta$
    and answer length, and get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m_{\theta}(x,y)=\frac{1}{N}\Sigma\frac{1}{m}log\frac{\pi_{\theta}(y&#124;x)}{\pi_{ref}(y&#124;x)}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: N is batch size, and m is answer length. Thus $m_{\theta}(x,y)$, it can compare
    with MinorSFT and SFT using DPO( Eq. [3](#S3.E3 "In 3.1 Minor SFT derivation ‣
    3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation"))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31e58df55b70eba2446a08281327a69d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) raw sft lr = 1e-5
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9d38180716ba2fa4f11eec9170f9a892.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) SFT use DPO lr = 2e-5 $\beta$ = 0.04
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e4df3682d023af24a71b6bb678cd5b2.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Minor SFT lr = 2e-5 $\beta$ = 0.04
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Normalized rewards during training'
  prefs: []
  type: TYPE_NORMAL
- en: The metric $m_{\theta}(x,y)$ can be used in both DPO with preference pair and
    SFT with only preferred. Figure [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation metric
    ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation") shows metric trends for three methods. Since the optimized LLM
    deviate from the reference model during to the training, they can also be treated
    as LLM model deviation trend. The metric value and trends can be used in a qualitative
    analysis of LLM deviation. And from Figure [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation
    metric ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance
    and reduce model deviation") even with larger learning rate(2e-5) for SFT use
    DPO [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation metric ‣ 3 Approach ‣ Minor SFT loss
    for LLM fine-tune to increase performance and reduce model deviation") and MinorSFT
    [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation metric ‣ 3 Approach ‣ Minor SFT loss
    for LLM fine-tune to increase performance and reduce model deviation"), they have
    a lower deviation value compared to SFT (1e-5) [1](#S3.F1 "Figure 1 ‣ 3.2 LLM
    Deviation metric ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance
    and reduce model deviation") for each training step, due to they both have a sample
    level dynamic coefficient that decays fast when the reward of the sample grow
    up( or in other words, when the complexity of the sample reduce down).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For training settings, we use Qwen2-7B-Instruction(qwe, ([2024](#bib.bib1)))
    as the base model. It expresses high performance in many benchmarks ¹¹1We tried
    several open datasets to train the base model, but with little performance improvement
    on the benchmarks, so in this experiment we use a private domain corpus. And use
    down-sample of FinanceIQ²²2https://huggingface.co/datasets/Duxiaoman-DI/FinanceIQ,
    fineval³³3https://huggingface.co/datasets/djdropthebit/fineval, ceval-exam(Huang
    et al., ([2023](#bib.bib5))) as test datasets to do evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: We use LLaMa-Factory(Zheng et al., ([2024](#bib.bib13))) as the training and
    inference framework with some customized code to implement the MinorSFT and SFT
    use DPO algorithm. The experiments use batch size 64, warm-up ratio 0.1, linear
    decay learning rate, 1 epoch and run 400+ steps.
  prefs: []
  type: TYPE_NORMAL
- en: For FinanceIQ and fineval we use the prompt """Please answer the questions based
    on the context provided. Please ensure that the original information (such as
    numbers, time, entities, opinions, etc.) is accurately cited when answering. If
    the user’s question cannot be answered based on the given context, please briefly
    explain why. If the answer involves mathematical calculations, please give priority
    to calling tools; if it involves numerical comparison, please give the comparison
    process; if it involves analysis or reasoning, please give the reasoning and analysis
    process""".
  prefs: []
  type: TYPE_NORMAL
- en: 'For ceval-exam we use the prompt """You need to choose one of the four options
    A, B, C, and D as the most appropriate answer to the question. You can only output
    one character, and this character must be one of A, B, C, and D. The question
    content is: <question> The four options are: A. <A> B. <B> C. <C> D. <D> Your
    answer is:""".'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d2a2628b57efc3dd02aada074399eb3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Full comparison data
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70f2aa6d341c6118340ac1389f715b52.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Best model comparison
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Accuracy comparison'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune
    to increase performance and reduce model deviation") shows the experiment result.
    We searched a group setting for each method. Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation")
    contains full detail of the comparison. Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation")
    shows the comparison between the best result of each method.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune
    to increase performance and reduce model deviation") shows after learning, Minor
    SFT get its best result with lr=2e-5 and $\beta$=0.04.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune
    to increase performance and reduce model deviation") indicates that Minor SFT(lr=2e-5,
    $\beta$=0.04) are all better than the base model. Minor SFT perform best in all
    three datasets compared to raw SFT and SFT use DPO. SFT use DPO wins FinanceIQ
    and ceval-exam but lose fineval compared to raw SFT.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment result shows several points.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each method have a performance increase from low learning rate to high learning
    rate, and get a performance decrease if continue to increase the learning rate
    after a certain threshold. Raw SFT get its best at lr=1e-5, MinorSFT and SFT use
    DPO get its best at lr=2e-5 and $\beta$=0.04.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minor SFT perform best in all three datasets. We give credit to the sample-level
    dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$.
    This coefficient implicitly adjust the corpus distribution, so that the training
    pays more effort on those high complexity(or difficult) samples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minor SFT need higher learning rate to get its best performance compared to
    raw SFT, because the sample dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    decay during training when the reward grows up(or when the complexity of the sample
    reduce down). However, even with high learning rate Minor SFT has a lower deviation
    compared to raw SFT, which can be see through Figure [1](#S3.F1 "Figure 1 ‣ 3.2
    LLM Deviation metric ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase
    performance and reduce model deviation").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SFT use DPO perform worse than Minor SFT, we think the cause is due to it use
    the same hyper-parameter $\beta$ is sample dependent while $\beta$ is sample independent,
    this bias cause the performance regression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $\beta$ has same meaning as in DPO, however it still brings more complexity
    compared to raw SFT. It needs some tuning to achieve the best performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Conclusion & Future work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired from DPO and MinorDPO, in this article we propose a training metric
    $m_{\theta}(x,y)$. It’s kind of a tradeoff to get better performance.
  prefs: []
  type: TYPE_NORMAL
- en: As the conclusion in above Experiment section, MinorSFT needs some higher learning
    rate compared to raw SFT. We design the MinorSFT coefficient same as the coefficient
    in DPO to simplify its meaning and understanding. The hyper-parameter $\beta$
    in MinorSFT has same meaning as in DPO. With appropriate tuning we are able to
    get a best performance LLM as in above experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Although the training metric $m_{\theta}(x,y)$ and answer length, we don’t have
    a way to know whether the optimized model is over-fit or under-fit during the
    training. It needs more research effort to find those metrics that can guide model’s
    fitting level.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: qwe, (2024) (2024). Qwen2 technical report.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azar et al., (2023) Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,
    D., Valko, M., and Munos, R. (2023). A general theoretical paradigm to understand
    learning from human preferences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al., (2024) Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,
    A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn,
    A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A.,
    Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B.,
    Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell,
    C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius,
    D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D.,
    Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova,
    E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G.,
    Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,
    H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov,
    I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah,
    J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang,
    J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun,
    J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,
    K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Rantala-Yeary,
    L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L.,
    Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M.,
    Singh, M., Paluri, M., Kardas, M., Oldham, M., Rita, M., Pavlova, M., Kambadur,
    M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov,
    N., Bogoychev, N., Chatterji, N., Duchenne, O., Çelebi, O., Alrassy, P., Zhang,
    P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura,
    P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R.,
    Cabral, R. S., Stojnic, R., Raileanu, R., Girdhar, R., Patel, R., Sauvestre, R.,
    Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini,
    S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang,
    S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S.,
    Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S.,
    Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T.,
    Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez,
    V., Gonguet, V., Do, V., Vogeti, V., Petrovic, V., Chu, W., Xiong, W., Fu, W.,
    Meers, W., Martinet, X., Wang, X., Tan, X. E., Xie, X., Jia, X., Wang, X., Goldschlag,
    Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert,
    Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Grattafiori, A., Jain, A.,
    Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A.,
    Sharma, A., Boesenberg, A., Vaughan, A., Baevski, A., Feinstein, A., Kallet, A.,
    Sangani, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton,
    A., Ryan, A., Ramchandani, A., Franco, A., Saraf, A., Chowdhury, A., Gabriel,
    A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi,
    B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B.,
    Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker,
    C., Burton, C., Mejia, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai,
    C., Tindal, C., Feichtenhofer, C., Civin, D., Beaty, D., Kreymer, D., Li, D.,
    Wyatt, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich,
    D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery,
    E., Presani, E., Hahn, E., Wood, E., Brinkman, E., Arcaute, E., Dunbar, E., Smothers,
    E., Sun, F., Kreuk, F., Tian, F., Ozgenel, F., Caggioni, F., Guzmán, F., Kanayet,
    F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G.,
    Thattai, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Shojanazeri,
    H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H.,
    Goldman, H., Molybog, I., Tufanov, I., Veliche, I.-E., Gat, I., Weissman, J.,
    Geboski, J., Kohli, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J.,
    Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings,
    J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J.,
    Wu, K., U, K. H., Saxena, K., Prasad, K., Khandelwal, K., Zand, K., Matosich,
    K., Veeraraghavan, K., Michelena, K., Li, K., Huang, K., Chawla, K., Lakhotia,
    K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo,
    L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M.,
    Tsimpoukelli, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov,
    M., Lathi, M., Keneally, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M.,
    Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat,
    M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N.,
    Singhal, N., Egebo, N., Usunier, N., Laptev, N. P., Dong, N., Zhang, N., Cheng,
    N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P.,
    Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina,
    P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R.,
    Murthy, R., Nayani, R., Mitra, R., Li, R., Hogan, R., Battey, R., Wang, R., Maheswari,
    R., Howes, R., Rinott, R., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon,
    S., Sidorov, S., Pan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S.,
    Lindsay, S., Feng, S., Lin, S., Zha, S. C., Shankar, S., Zhang, S., Zhang, S.,
    Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe,
    S., Satterfield, S., Govindaprasad, S., Gupta, S., Cho, S., Virk, S., Subramanian,
    S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Kohler, T., Robinson,
    T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi,
    V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V.,
    Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable,
    W., Tang, X., Wang, X., Wu, X., Wang, X., Xia, X., Wu, X., Gao, X., Chen, Y.,
    Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang,
    Hao, Y., Qian, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang,
    Z., and Zhao, Z. (2024). The llama 3 herd of models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ethayarajh et al., (2024) Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky,
    D., and Kiela, D. (2024). Kto: Model alignment as prospect theoretic optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al., (2023) Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su,
    T., Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and He, J. (2023). C-eval:
    A multi-level multi-discipline chinese evaluation suite for foundation models.
    arXiv preprint arXiv:2305.08322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al., (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P., Leike, J., and Lowe, R. (2022). Training language models to follow instructions
    with human feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pal et al., (2024) Pal, A., Karkhanis, D., Dooley, S., Roberts, M., Naidu,
    S., and White, C. (2024). Smaug: Fixing failure modes of preference optimisation
    with dpo-positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al., (2024) Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S.,
    and Weston, J. (2024). Iterative reasoning preference optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al., (2024) Rafailov, R., Hejna, J., Park, R., and Finn, C. (2024).
    From $r$: Your language model is secretly a q-function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al., (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S.,
    Manning, C. D., and Finn, C. (2023). Direct preference optimization: Your language
    model is secretly a reward model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al., (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    and Klimov, O. (2017). Proximal policy optimization algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al., (2024) Xie, S., Chen, H., Yu, F., Sun, Z., Wu, X., and Hu, Y. (2024).
    Minor dpo reject penalty to increase training robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al., (2024) Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., and
    Ma, Y. (2024). Llamafactory: Unified efficient fine-tuning of 100+ language models.
    arXiv preprint arXiv:2403.13372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al., (2020) Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
    A., Amodei, D., Christiano, P., and Irving, G. (2020). Fine-tuning language models
    from human preferences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
