- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'AGILE: A Novel Framework of LLM Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14751](https://ar5iv.labs.arxiv.org/html/2405.14751)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peiyuan Feng^∗¹ Yichen He^∗¹  Guanhua Huang^∗^†²  Yuan Lin^∗¹
  prefs: []
  type: TYPE_NORMAL
- en: Hanchong Zhang^∗^†³ Yuchen Zhang^∗¹ Hang Li¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹ByteDance Research  ²University of Science and Technology of China
  prefs: []
  type: TYPE_NORMAL
- en: ³Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
  prefs: []
  type: TYPE_NORMAL
- en: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We introduce a novel framework of LLM agents named AGILE (AGent that Interacts
    and Learns from Environments) designed to perform complex conversational tasks
    with users, leveraging LLMs, memory, tools, and interactions with experts. The
    agent’s abilities include not only conversation but also reflection, utilization
    of tools, and consultation with experts. We formulate the construction of such
    an LLM agent as a reinforcement learning problem, in which the LLM serves as the
    policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm.
    We focus on question answering and release a dataset for agents called ProductQA,
    comprising challenging questions in online shopping. Our extensive experiments
    on ProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained
    with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability
    of memory, tools, consultation, reflection, and reinforcement learning in achieving
    the agent’s strong performance. Datasets and code are available at [https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE).
  prefs: []
  type: TYPE_NORMAL
- en: '^($*$)^($*$)footnotetext: Equal contribution. Alphabet order.^($\dagger$)^($\dagger$)footnotetext:
    Work done during ByteDance Research internship.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have exhibited remarkable capabilities such as
    instruction following, reasoning, and zero-shot learning [[2](#bib.bib2), [39](#bib.bib39),
    [15](#bib.bib15), [21](#bib.bib21)], which have greatly catalyzed the development
    of autonomous agents based on LLMs [[23](#bib.bib23), [25](#bib.bib25), [1](#bib.bib1)],
    also known as LLM agents. Recent works propose several essential components or
    workflows to enhance the abilities of LLM agents, such as planning [[39](#bib.bib39),
    [44](#bib.bib44), [33](#bib.bib33)], reflection [[16](#bib.bib16), [34](#bib.bib34)],
    tool-use [[24](#bib.bib24), [31](#bib.bib31), [41](#bib.bib41)] and life-long
    learning [[36](#bib.bib36)]. However, it remains unclear how to integrate all
    components into a unified framework and optimize them end-to-end.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0ab0e0cb5ecfb005f3eb9e85e88f019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (a) Architecture of our agent system, including LLM, memory, tools,
    and executor. (b) A running example of AGILE in a customer service QA environment.
    The tokens (actions) generated by the LLM are in orange color and the tokens appended
    by the executor are in blue color.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we introduce a novel framework for LLM agents to unify various
    components and streamline their learning and operation processes. As shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of LLM
    Agents")(a), the architecture of the agent system, named AGILE, comprises four
    modules: LLM, memory, tools, and executor. Furthermore, the agent can interact
    with both users and experts. The LLM, functioning as the predictor of all actions,
    generates instructions and processes responses. The executor, working as the controller
    of all actions, interprets the LLM instructions to activate the corresponding
    modules and collects their responses for the LLM. For example, the executor can
    fetch a text from the memory and append it to the context of LLM, or extract an
    excerpt from the context and append it to the memory. The executor can also follow
    instructions of the LLM to utilize a search tool. In addition to skills such as
    reasoning, planning, and reflection, we propose a new ability called *seeking
    advice*, which means that the agent proactively consults human experts when it
    encounters a problem unsolvable. The agent can reflect on the expert feedback
    and memorize it for future use. Furthermore, we propose a training method based
    on reinforcement learning (RL), which simultaneously trains the policy of invoking
    different modules and the reasoning, planning, reflection, and seeking advice
    abilities of the LLM agent in an end-to-end fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: While the proposed agent framework is general, in this paper, we evaluate it
    in complex question answering (QA). It is a task an LLM agent has the potential
    of outperforming existing solutions such as the use of an LLM alone. However,
    existing QA benchmarks [[9](#bib.bib9), [42](#bib.bib42), [8](#bib.bib8), [22](#bib.bib22)]
    are designed for specific subsets of capabilities (e.g., reflection, memory retrieve,
    etc.) which cannot simultaneously investigate the ability to combine all modules
    and capabilities of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, we have developed a new benchmark called ProductQA. ProductQA
    comprises 88,229 question-answer pairs in customer service divided into 26 QA
    tasks, each corresponding to a distinct Amazon product category. This benchmark
    is based on real Amazon user queries and includes fact-based questions, reasoning
    questions, and product recommendation queries. It comprehensively evaluates agents’
    abilities to handle historical information and accumulated knowledge, leverage
    tools, interact with humans, perform self-evaluation, and conduct reflection.
    Additionally, the training and testing tasks are made disjoint to assess the agent’s
    ability to adapt to new product categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate our agent framework on two tasks, ProductQA and MedMCQA [[22](#bib.bib22)].
    For ProductQA, we use a two-stage training method based on Vicuna-13b [[5](#bib.bib5)].
    In the first stage, imitation learning is employed to create agile-vic13b-sft.
    In the second stage, the policy gradient algorithm of PPO [[32](#bib.bib32)] produces
    agile-vic13b-ppo. Experimental results show that agile-vic13b-ppo improves the
    relative total performance score by 9.2% over GPT-4 and by 90.8% over GPT-3.5\.
    Ablation studies confirm that all modules in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Framework of LLM Agents") are indispensable. Specifically, removing
    tools or memory usage negatively impacts the agent’s performance, leading to a
    25.9% or 17.4% increase in seeking advice, respectively, or a 9.3% or 4.0% relative
    decrease in the total score, respectively. Disabling the seeking advice function
    results in a 10.7% decrease in accuracy. Finally, agile-vic13b-ppo achieves a
    2.3% relative increase in total score compared to agile-vic13b-sft, demonstrating
    the necessity of PPO training. On MedMCQA, we train an agile-mek7b-ppo agent,
    initialized from Meerkat-7b [[13](#bib.bib13)], following the same two-stage procedure.
    Our agent improves the base LLM’s accuracy from 53.4% to 85.2% by seeking advice
    on 31.6% instances. This accuracy surpasses the SOTA accuracy of 79.1% by GPT4-MedPrompt [[20](#bib.bib20)].
    When all agents are able to seek advice, our agent also outperforms the GPT-4
    agent in terms of the total score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We propose a novel framework of LLM agents, formulated in the context of reinforcement
    learning. It facilitates end-to-end learning of agents. Notably, this framework
    enables the agent to seek advice from human experts, providing two advantages:
    1) It ensures high-level accuracy when dealing with complex and challenging questions,
    and 2) it fosters learning from humans, thereby enhancing its abilities to adapt
    to new tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop a benchmark, ProductQA, to comprehensively evaluate the agent’s capabilities
    in complex question answering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We perform experiments on ProductQA and MedMCQA to verify our framework and
    show that AGILE agents based on 13B and 7B LLMs trained with PPO can surpass GPT-4
    agents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 RL formulation of agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our agent framework comprises four elements: LLM, memory, tools and executor,
    see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of
    LLM Agents")(a). The LLM possesses a *context*, defined as the sequence of tokens
    it utilizes to generate the next token. In RL terminology, the agent conducts
    a token-level Markov decision process (MDP). The action space $\mathcal{A}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us examine the state transition more closely. For each action, the executor’s
    first operation is to append the token to the context, preparing the LLM for generating
    the next token. Then, the executor checks a registered list of *functions*. Each
    function is designed to execute a set of operations, including memory I/O, tool
    usage, and interaction with the environment. If the action (i.e., the token) matches
    a function name, the executor will execute the associated function implementation,
    further mutating the agent state. For instance, if the token is [GetQuestion],
    the executor will prompt the user for a new question and append it to the context;
    if the token is [UpdateMemory], the executor will write a specific segment of
    the context into the memory; if the token is [ClearContext], the executor will
    reset the context to [BOS]. In summary, the LLM interacts with the memory and
    tools by predicting function names, relying on the executor to execute these functions.
    See Table [1](#S2.T1 "Table 1 ‣ 2.1 RL formulation of agent ‣ 2 Methods ‣ AGILE:
    A Novel Framework of LLM Agents") for a full list of functions defined for a QA
    agent and see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework
    of LLM Agents")(b) for a running example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Functions for an exemplary customer service QA agent. Among them,
    [Reflection] and [PredictAnswer] are trivial functions, as the executor passes
    control immediately back to the LLM to start generating result tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function name | Function implementation |'
  prefs: []
  type: TYPE_TB
- en: '|  [GetQuestion] | Prompt the user for a question and append it to the context.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  [RetrieveMemory] | Retrieve relevant entries from the memory and append
    them to the context. |'
  prefs: []
  type: TYPE_TB
- en: '|  [SeekAdvice] | Ask human experts for advice and append it to the context.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  [Reflection] | $\emptyset$ |'
  prefs: []
  type: TYPE_TB
- en: '|  [UpdateMemory] | Write a specific segment of the context into the memory.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  [SearchProduct] | Extract a search query from the context, then invoke the
    search tool and append results to the context. |'
  prefs: []
  type: TYPE_TB
- en: '|  [PredictAnswer] | $\emptyset$ |'
  prefs: []
  type: TYPE_TB
- en: '|  [SubmitAnswer] | Extract a predicted answer from the context and submit
    it to the user. |'
  prefs: []
  type: TYPE_TB
- en: '|  [ClearContext] | Reset the context to a single token [BOS]. |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Policy learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We frame the policy learning problem as a task of training a language model.
    Consider an agent trajectory $\tau=(s_{1},a_{1},...,s_{n},a_{n})$ because the
    executor can delete context tokens.
  prefs: []
  type: TYPE_NORMAL
- en: In Imitation Learning (IL), we generate trajectories by observing human experts
    or more proficient agents, then we derive the training sequences to fine-tune
    the LLM. It is important to point out that (1) the loss is calculated on the action
    tokens only, and (2) $c_{i}$ as the attention mask.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some situations, an agent may produce very long trajectories, potentially
    yielding training sequences that span millions of tokens and are impractical for
    training. We can leverage the structure of the trajectory to partition it into
    smaller segments. For instance, if the agent resets its LLM context at the beginning
    of every QA session, then we can partition by the session boundary. Nevertheless,
    these sessions are not entirely independent; actions taken in earlier sessions
    can influence memory, creating lasting effects on subsequent sessions. To tackle
    this challenge of long-range dependencies, we propose a training algorithm detailed
    in Appendix [A](#A1 "Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Interaction with human experts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our agent framework enables the agent to proactively seek advice from human
    experts. For example, the agent can invoke a [SeekAdvice] function to request
    expert advice. This approach helps in two ways. Firstly, the agent can request
    the correct answer when its confidence is low, ensuring sufficient accuracy for
    the application. Secondly, the agent can use [Reflection] to distill general knowledge
    from the expert advice before storing it in memory. This accumulation of knowledge
    allows the agent to adapt to new tasks that it has not encountered during training.
  prefs: []
  type: TYPE_NORMAL
- en: Seeking advice involves complex decision-making. The agent must estimate its
    own confidence in the current session, predict the potential value of the advice
    for future sessions, and consider the cost of human resources. The optimal trade-off
    is difficult to annotate manually but aligns well with our RL framework. Specifically,
    the present risk, future value, and cost of action can all be represented as RL
    rewards, allowing this skill to be trained as part of the policy model on an end-to-end
    basis.
  prefs: []
  type: TYPE_NORMAL
- en: 3 The ProductQA dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We believe that product question answering in a real online shopping environment
    offers a comprehensive challenge for evaluating LLM agents. First, it demands
    expert knowledge about millions of products, including their technical specifications,
    usage in particular scenarios, and compatibility with other products. Second,
    answering some questions requires the use of tools, such as a product search tool.
    Third, the continuous emergence of new products necessitates the adaptability
    of the agent. This has motivated the creation of the ProductQA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ProductQA dataset consists of 26 QA tasks, each representing a distinct
    group of products within a specific category. Each group encompasses 17-20 products.
    We collected 20 groups for training and 6 for testing, allowing for assessing
    the agent’s adaptability to new tasks. We collected an average of 3,393 question-answer
    pairs for each product group. The questions within the same group are correlated,
    as knowledge from one answer may aid in addressing other questions. The dataset
    statistics are presented in Table [9](#A3.T9 "Table 9 ‣ Appendix C Tables ‣ AGILE:
    A Novel Framework of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is annotated by 20 professional annotators, each with at least
    a college degree, employed by a commercial data annotation company. We pay the
    company at market rates for professional annotation. See annotation guidelines
    in Appendix [E.2](#A5.SS2 "E.2 Annotation guidelines ‣ Appendix E Development
    of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents"). In addition,
    we release the code for the data pre-processing before human annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Product collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We gather products from the Amazon Review Data [[18](#bib.bib18)], which includes
    product metadata as well as reviews. We initially filter the Amazon Review Data
    to retain only popular products with at least 100 reviews, then cluster them by
    category tags. From these clusters, we select 26 based on the size of the cluster,
    each defined as a *product group*. Subsequently, we sample products from each
    product group. See Appendix [E.1](#A5.SS1 "E.1 Product collection ‣ Appendix E
    Development of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents")
    for more details about product group and product selection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the products are collected, annotators compile an information table for
    each product group. An example of such a table is presented in Table [2](#S3.T2
    "Table 2 ‣ 3.1 Product collection ‣ 3 The ProductQA dataset ‣ AGILE: A Novel Framework
    of LLM Agents"). To enhance the efficiency of the annotation process, we employ
    GPT-4 to extract as many product features as possible from the reviews. These
    features, together with the product metadata, are provided to the annotators for
    table creation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: An example of an information table for the headphones group.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Product ID | Title | Price | Brand | Headphone | Cable Type | Audio | Audio
    | … |'
  prefs: []
  type: TYPE_TB
- en: '| Type | Transmission | Output Mode |'
  prefs: []
  type: TYPE_TB
- en: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | over-ear | bluetooth
    | kleer | stereo | … |'
  prefs: []
  type: TYPE_TB
- en: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | earbud | 3.5mm Jack | analog | bass
    boost | … |'
  prefs: []
  type: TYPE_TB
- en: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | earbud | bluetooth | bluetooth
    | stereo | … |'
  prefs: []
  type: TYPE_TB
- en: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | over-ear | 3.5mm Jack | analog |
    stereo | … |'
  prefs: []
  type: TYPE_TB
- en: '| … | … | … | … | … | … | … | … | … |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Question | Long Answer | Short Answer |'
  prefs: []
  type: TYPE_TB
- en: '| Fact-QA | What is the size of the neodymium driver used in the JVC HA-EB75
    headphones? | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | 13.5 mm |'
  prefs: []
  type: TYPE_TB
- en: '| Search-QA | I’m an audiophile always on the move, so I need my music non-stop.
    Tell me, what’s the headphone with the longest playtime you have, either on-ear
    or in-ear? | I found a product that matches your criteria. ‘ABCShopUSA Wireless
    Earbuds True’ with asin: B00LJT2EPK | B00LJT2EPK |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning-QA | Will these headphones deliver comparable sound quality to
    wired alternatives when I am editing music? | No, these headphones may not suit
    your needs for music editing since they are wireless and can introduce audio compression
    and slight latency. Such issues can impact the precise listening experience crucial
    for professional audio editing tasks. | no |'
  prefs: []
  type: TYPE_TB
- en: 3.2 QA collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We identify three predominant types of questions in online shopping contexts:
    1) Fact-QA: questions concerning specific product details; 2) Search-QA: searches
    for product recommendations tailored to user preferences; 3) Reasoning-QA: questions
    whose answers require domain-specific reasoning, such as the implications of a
    product feature. Accordingly, we annotate question-answer pairs for these types.
    Each question is annotated with both a detailed paragraph-long answer and a concise
    short answer. The long answer should resemble a response from human customer service,
    while the short answer consists of a few words. We train the model to predict
    both answer types. The accuracy of the long answers is evaluated using GPT-4 (see
    Appendix [F](#A6 "Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM
    Agents") for the prompt); the short answers are assessed by exact match and are
    used for defining rewards for RL training.'
  prefs: []
  type: TYPE_NORMAL
- en: Fact-QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fact-QAs are constructed from product reviews. For each product, we provide
    GPT-4 with a batch of 30 reviews, prompting it to generate 20 questions and their
    corresponding answers before moving on to the next batch. We encourage GPT-4 to
    create diverse questions. The results are then given to annotators to refine and
    finalize the question-answer pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Search-QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Starting with an information table for a given product group, we generate random
    SQL expressions using a set of predefined rules. These expressions are then translated
    into natural language questions by GPT-4\. The answers are obtained by executing
    the SQL queries. Subsequently, human annotators thoroughly revise the QA pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning-QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As the first step, we collect professional knowledge for each product group.
    To enhance efficiency, we utilize GPT-4 to generate candidate knowledge entries
    based on the technical specifications from the information table. These entries
    are then curated and refined by human annotators. Here is an example of a knowledge
    entry: *Motherboards with the ATX form factor are ideally suited for high-performance
    computing tasks and gaming, due to their ample expansion slots for graphics cards
    and other peripherals that boost computing capabilities.* Finally, annotators
    develop question-answer pairs from these knowledge entries.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate our agent on two complex QA tasks: ProductQA and MedMCQA. MedMCQA [[22](#bib.bib22)]
    is a dataset for multiple-choice QA. It consists of questions from medical school
    entrance examinations, with 182,822 / 4,183 / 6,150 instances in the train/dev/test
    splits. We report results on the dev set without checkpoint selection.'
  prefs: []
  type: TYPE_NORMAL
- en: Agent definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our agent can invoke functions defined in Table [1](#S2.T1 "Table 1 ‣ 2.1 RL
    formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Framework of LLM Agents"). In
    a typical workflow, the agent prompts the user for a new question at the session
    start, then retrieves memory to get relevant information. The memory can be initialized
    as empty (ProdcutQA) or with domain knowledge (QA pairs from MedMCQA training
    dataset). The agent can optionally use tools (e.g. product search in ProductQA)
    to get more information, then decide whether to predict an answer directly or
    seek human advice. If the agent seeks advice, it obtains a human answer (ground-truth
    answer in our setting). The agent then uses a reflection round to extract general
    knowledge from the human answer, writing both the human answer and the reflected
    knowledge to its memory. Finally, the agent submits an answer to the user. In
    our setting, submitting a correct answer incurs a $+1$.'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training consists of two stages. First, we construct trajectories from
    the training data and employ imitation learning to train the agent. Then we apply
    Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Session-level optimization algorithm
    ‣ AGILE: A Novel Framework of LLM Agents") for further optimization by reinforcement
    learning. See Appendix [B](#A2 "Appendix B Implementation details of AGILE ‣ AGILE:
    A Novel Framework of LLM Agents") for implementation details. The agent’s LLM
    is initialized from Vicuna-13b-1.5 for ProductQA and Meerkat-7b for MedMCQA. We
    fine-tune the model for 2 epochs with a learning rate of 1e-5 and a batch size
    of 64\. We implement PPO for 1 epoch with a learning rate of 1e-6 and a batch
    size of 64\. The training runs on NVIDIA-H800.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We report three metrics for the agent: (a) Advice rate: the rate of seeking
    human advice; (b) Accuracy: the rate of predicting the correct answer; (c) Total
    score: the average reward across all sessions, taking the advice rate and the
    accuracy both into account.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We compare our agent against two types of baselines: 1) Prompting GPT-3.5 (gpt-3.5-turbo-0301)
    and GPT-4 (gpt-4-0613) [[21](#bib.bib21)] to directly answer the question, without
    working in an agent manner, noted as gpt3.5-prompt and gpt4-prompt. 2) Prompting
    GPT-3.5 and GPT-4 within the AGILE framework, noted as agile-gpt3.5-prompt and
    agile-gpt4-prompt. We carefully designed prompts for all baselines and they are
    shown in Appendix [F](#A6 "Appendix F Prompt templates ‣ AGILE: A Novel Framework
    of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Results on ProductQA. Here, X-prompt represents directly prompting
    model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents
    prompting or PPO training. We report results on short and long answers, respectively.
    The seeking advice cost is $c=0.3$. Results are average over six test tasks. See
    Table [10](#A3.T10 "Table 10 ‣ Appendix C Tables ‣ AGILE: A Novel Framework of
    LLM Agents") for individual product group performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Advice Rate  $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Short | Long | Short | Long |'
  prefs: []
  type: TYPE_TB
- en: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| agile-vicuna-13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-vic7b-ppo (ours) | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Ablation studies for disabling reflection, memory, seeking advice,
    tool use, or RL training. Here, non-adapt-advice means that seeking advice is
    invoked for the first $K$ equals to the number of [SeekAdvice] performed by agile-vic13b-ppo.
    See Table [11](#A3.T11 "Table 11 ‣ Appendix C Tables ‣ AGILE: A Novel Framework
    of LLM Agents") for ablation results on individual product groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Advice Rate $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Reflection | 0.270 | 0.852 | 0.771(-1.7%) |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Memory | 0.407 | 0.876 | 0.754(-4.0%) |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Advice | 0.000 | 0.747 | 0.747(-5.0%) |'
  prefs: []
  type: TYPE_TB
- en: '| non-adapt-advice | 0.233 | 0.812 | 0.742(-5.7%) |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Tool-Use | 0.492 | 0.864 | 0.717(-9.3%) |'
  prefs: []
  type: TYPE_TB
- en: '| w/o RL | 0.256 | 0.843 | 0.766(-2.3%) |'
  prefs: []
  type: TYPE_TB
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.784 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Results on ProductQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As Table [4](#S4.T4 "Table 4 ‣ Evaluation and baselines ‣ 4.1 Experimental
    setting ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") shows, our
    AGILE agent outperforms all baselines on ProductQA. Notably, the average total
    score of agile-vic13b-ppo across six test groups shows a relative improvement
    of 9.2% in short answers and 5.0% in long answers to agile-gpt4-prompt where the
    seeking advice cost is added into the prompt. Concretely, agile-vic13b-ppo uses
    a comparable number of seeking advice to achieve 7.4% higher accuracy in short
    answers than agile-gpt4-prompt, and as Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Results
    on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") shows,
    this accuracy improvement is consistent across the whole trajectory. Our agile-vic7b-ppo
    agent also outperforms agile-gpt4-prompt in average total scores. Note that the
    GPT-4 agent knows the seeking advice cost from its prompt (see Figure [6](#A6.F6
    "Figure 6 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE:
    A Novel Framework of LLM Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We investigate the impact of varying the seeking advice cost. As shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Framework
    of LLM Agents"), when the cost decreases, both the advice rate and the accuracy
    increase, indicating greater utilization of human assistance. Specifically, with
    a high cost of 0.5, the advice rate is close to 0, and at a low cost of 0.1, the
    accuracy is close to 1\. This result demonstrates that by adjusting the cost and
    through RL training, we can effectively manage the trade-off between accuracy
    and human cost. For instance, the agent can achieve 94.1% accuracy on the Motherboards
    task with a seeking advice cost of $c=0.1$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11ebaea0130ede1f05fbfb7f80981544.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Accuracy and advice rate over the following 200 sessions on ProductQA
    ($c=0.3$).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8876aba295eb5c94a265c610e02c322.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Advice rate, accuracy along with seeking advice cost $c$ on ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present ablation studies in Table [5](#S4.T5 "Table 5 ‣ Evaluation and baselines
    ‣ 4.1 Experimental setting ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents")
    to assess the contributions of individual agent components and the effects of
    RL training. The table indicates that disabling the option to seek advice (w/o
    Advice) leads to a 10.7% drop in accuracy and a 5.0% relative reduction in total
    score. Forcing the agent to seek advice at the initial part of the trajectory
    (Non-adapt Advice) causes a 4.2% decrease in accuracy, underscoring the value
    of adaptive decision-making. Removing reflection and memory capabilities (w/o
    Memory and w/o Reflection) both increase the frequency of advice-seeking, as the
    agent struggles to accumulate or leverage valuable knowledge, consequently decreasing
    the total score. Furthermore, disabling tool use (w/o Tool-Use) causes a substantial
    25.9% increase in the advice-seeking rate because the agent’s capabilities are
    diminished, making it more reliant on external advice. Lastly, RL training improves
    the relative total score by 2.3%, lowers the advice-seeking rate, and boosts accuracy,
    demonstrating that RL training effectively optimizes the policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Appendix [D](#A4 "Appendix D Case study ‣ AGILE: A Novel Framework of LLM
    Agents"), we present detailed examples of agile-vic13b-ppo illustrating how memory,
    tools, seeking advice, and reflection enhance the agent workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/318201b08ea44dac1475a1aafffcb6ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Advice rate over the following 200 sessions on ProductQA ($c=0.3$).'
  prefs: []
  type: TYPE_NORMAL
- en: Trend of advice rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ Ablation study ‣ 4.2 Results on ProductQA ‣ 4
    Experiments ‣ AGILE: A Novel Framework of LLM Agents") demonstrates a consistent
    decrease in the advice rate of agile-vic13b-ppo as more sessions are added to
    the trajectory. This decline can be attributed to the agent progressively accumulating
    knowledge and becoming more independent. Additionally, the figure illustrates
    that disabling RL training or reflection leads to a significant increase in the
    advice rate, underscoring the importance of RL training and reflection in reducing
    human costs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Results on MedMCQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 6: Results on the MedMCQA dev dataset. X-prompt represents directly prompting
    the model X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting, ablation studies or standard PPO training. The seeking
    advice cost is $c=0.4$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Advice Rate $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Meerkat-7b-prompt | - | 0.534 | - |'
  prefs: []
  type: TYPE_TB
- en: '| gpt3.5-prompt[[19](#bib.bib19)] | - | 0.501 | - |'
  prefs: []
  type: TYPE_TB
- en: '| gpt4-prompt[[19](#bib.bib19)] | - | 0.695 | - |'
  prefs: []
  type: TYPE_TB
- en: '| gpt4-Medprompt[[20](#bib.bib20)] | - | 0.791 | - |'
  prefs: []
  type: TYPE_TB
- en: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-mek7b-w/o Reflection | 0.368 | 0.790 | 0.643 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-mek7b-w/o Memory | 0.506 | 0.741 | 0.539 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-mek7b-w/o Advice | 0.000 | 0.620 | 0.620 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-mek7b-w/o RL | 0.322 | 0.837 | 0.708 |'
  prefs: []
  type: TYPE_TB
- en: '| agile-mek7b-ppo (ours) | 0.316 | 0.852 | 0.726 |'
  prefs: []
  type: TYPE_TB
- en: 'Our agile-mek7b-ppo agent, based on the smaller Meerkat-7b [[13](#bib.bib13)]
    model, reaches an accuracy of 85.2% with an advice rate of 31.6%. As Table [6](#S4.T6
    "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of
    LLM Agents") shows, this represents a 31.8% accuracy increase over the base model
    Meerkat-7b-prompt and a 6.1% increase over the state-of-the-art gpt4-Medprompt
    [[20](#bib.bib20)]. Table [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents") also shows that the ability to seek
    advice alone contributes a 23.2% accuracy gain, meaning that each instance of
    seeking advice corrects an average of 0.73 prediction errors. This indicates that
    PPO training effectively helps the agent identify its mistakes. For a fair comparison,
    we also evaluate agile-gpt3.5-prompt and agile-gpt4-prompt, which incorporate
    GPT-3.5 and GPT-4 within our AGILE framework. These agents also leverage advice-seeking
    to enhance accuracy, but without RL training, their total scores are lower than
    agile-mek7b-ppo. Finally, through ablation studies, we confirmed the essential
    roles of memory, reflection, seeking advice, and RL training in achieving high
    performance. Removing these components leads to a significant drop in total scores,
    detailed in Table [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 7: Related work on LLM agents. AGILE stands out as the pioneering work
    that trains the entire agent using reinforcement learning, incorporating proactive
    human advice-seeking.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM Agent | LLM | SFT | RL | Memory | Tools | Reflection |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Proactive &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Human-agent &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Interaction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| WebGPT [[17](#bib.bib17)] | GPT-3 | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct [[44](#bib.bib44)] | PaLM-540b | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion [[34](#bib.bib34)] | GPT-3/3.5/4 | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ChatDev [[25](#bib.bib25)] | ChatGPT-turbo-16k | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| RAP [[11](#bib.bib11)] | LLaMA-33b | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AutoAct [[27](#bib.bib27)] | LLaMA2-70b | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TPTU [[30](#bib.bib30)] | ChatGPT/InternLM | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AGILE (Ours) | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: LLM agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have demonstrated substantial capabilities in
    following instructions, reasoning, and planning. Numerous research works, as shown
    in Table [7](#S5.T7 "Table 7 ‣ 5 Related work ‣ AGILE: A Novel Framework of LLM
    Agents"), utilizing prompt engineering, have constructed remarkable LLM agents
    capable of autonomously resolving complex tasks across various environments [[23](#bib.bib23),
    [38](#bib.bib38), [1](#bib.bib1), [25](#bib.bib25), [3](#bib.bib3)]. Furthermore,
    extensive works identify key components in the design of LLM agents, including
    planning [[17](#bib.bib17), [33](#bib.bib33), [7](#bib.bib7), [27](#bib.bib27),
    [44](#bib.bib44), [30](#bib.bib30)], tool-use [[24](#bib.bib24), [41](#bib.bib41),
    [31](#bib.bib31)], and reflection [[34](#bib.bib34), [16](#bib.bib16)]. In this
    work, we enable the agent to utilize memory, tools and proactively learn from
    the environment. We then formulate the entire process within an RL framework so
    that all agent skills can be jointly optimized end-to-end.'
  prefs: []
  type: TYPE_NORMAL
- en: Human-agent interaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although LLMs face practical challenges, such as hallucination [[46](#bib.bib46)]
    and a lack of long-tail knowledge [[12](#bib.bib12)], consulting human experts
    can help mitigate these issues. Several studies have incorporated human experts
    into agent workflows. For instance, [[45](#bib.bib45)] establishes a static pipeline
    in which an agent passively accepts advice from a superior LLM. [[4](#bib.bib4),
    [26](#bib.bib26)] train models to proactively ask questions via behavior cloning.
    However, these methods ignore the fact that the decision to seek advice must be
    based on the LLM’s own knowledge and capabilities [[47](#bib.bib47), [14](#bib.bib14),
    [10](#bib.bib10)]. [[29](#bib.bib29)] use a calibrated version of an LLM’s token
    probabilities as a confidence measure, yet token probabilities tend to be overconfident [[40](#bib.bib40)],
    and existing calibration methods don’t generalize well to our agent setting when
    the LLM makes multiple decisions in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: LLM agent benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several benchmarks have been designed to assess the capabilities of agents.
    For instance, the Webshop [[43](#bib.bib43)] and Mind2Web [[6](#bib.bib6)] datasets
    evaluate agents’ tool usage and planning abilities within a web environment. HotPotQA [[42](#bib.bib42)]
    and TriviaQA [[9](#bib.bib9)] focus on agents’ reasoning and tool usage for question
    answering. ALFWorld [[35](#bib.bib35)] examines planning and navigation skills,
    while ScienceWorld [[37](#bib.bib37)] provides an interactive text-based environment
    to evaluate agents’ scientific aptitude. As illustrated in Table [8](#S5.T8 "Table
    8 ‣ LLM agent benchmarks ‣ 5 Related work ‣ AGILE: A Novel Framework of LLM Agents"),
    despite these existing benchmarks, none comprehensively addresses all the core
    challenges of real-world agent applications, such as handling long-tail knowledge,
    human-agent interaction, long-term memory usage, tool usage, self-evaluation,
    and reflection. This motivated us to develop ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Benchmarks for evaluating LLM agents. ProductQA features long trajectories,
    tool use, long-term knowledge accumulation, and cross-task capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Type | Fields | Size |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Long &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trajectory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tool &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Usage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Long-term &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cross &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Task &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Webshop [[43](#bib.bib43)] | Simulator | Web | 12,087 | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Mind2Web [[6](#bib.bib6)] | Simulator | Web | 2,350 | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ALFWorld [[35](#bib.bib35)] | Simulator | Navigation | 3,827 | ✗ | ✗ | ✗
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ScienceWorld [[37](#bib.bib37)] | Simulator | Science | 7,207 | ✗ | ✗ | ✗
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| HotPotQA [[42](#bib.bib42)] | QA | Wikipedia | 112,779 | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TriviaQA [[9](#bib.bib9)] | QA | Web | 95,956 | ✗ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ProductQA (ours) | QA | E-commerce | 88,229 | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 6 Conclusion and future work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we introduce a novel framework of LLM agents, called AGILE. First,
    the whole system of AGILE is trained end-to-end by reinforcement learning. Second,
    AGILE has the ability of seeking advice from external human experts. In addition,
    we develop a challenging dataset of complex QA, ProductQA, for comprehensive evaluation
    of an agent’s capabilities. Extensive experiments demonstrate that within our
    framework, an agent based on a smaller model after RL training can outperform
    GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'AGILE is a general agent framework and we can certainly consider multiple extensions
    of it. An agent can be equipped with more tools, such as multimodal perception,
    manipulations in physical environments, logical reasoning, among others. We posit
    that AGILE’s activities can be categorized into two distinct types: utilizing
    its LLM alone, and integrating the LLM with other tools. These two approaches
    conceptually align with the human cognitive processes known as System 1 and System
    2\. Furthermore, AGILE’s memory serves as a repository for the accumulation of
    experiences and knowledge, which is crucial for self-improvement. Consequently,
    AGILE offers an architecture for an very powerful agent that has the potential
    to attain human-level intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: AGILE also includes interactions between the agent and external human experts.
    The framework can be extended to allow interactions with humans or machine agents
    in various roles such as students or teachers, and in different formats such as
    debates or coordination. Furthermore, AGILE can be employed in multi-agent systems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry
    tools. arXiv preprint arXiv:2304.05376, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation.
    arXiv preprint arXiv:2309.17288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking
    before action: Gather information in embodied decision making with language models.
    arXiv preprint arXiv:2305.15695, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances
    in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    and Zhiting Hu. Reasoning with language model is planning with world model. arXiv
    preprint arXiv:2305.14992, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
    Szolovits. What disease does this patient have? a large-scale open domain question
    answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa:
    A large scale distantly supervised challenge dataset for reading comprehension.
    arXiv preprint arXiv:1705.03551, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson,
    et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented
    planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.
    Large language models struggle to learn long-tail knowledge. In International
    Conference on Machine Learning, pages 15696–15707\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, and Jaewoo Kang. Small language models
    learn enhanced reasoning skills from medical textbooks. arXiv preprint arXiv:2404.00376,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty:
    Linguistic invariances for uncertainty estimation in natural language generation.
    In The Eleventh International Conference on Learning Representations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine:
    Iterative refinement with self-feedback. Advances in Neural Information Processing
    Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
    Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint
    arXiv:2112.09332, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations
    using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
    2019 conference on empirical methods in natural language processing and the 9th
    international joint conference on natural language processing (EMNLP-IJCNLP),
    pages 188–197, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric
    Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo
    Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer
    McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White,
    and Eric Horvitz. Can generalist foundation models outcompete special-purpose
    tuning? case study in medicine, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] OpenAI. Gpt-4 technical report. ArXiv, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa:
    A large-scale multi-subject multi-choice dataset for medical domain question answering.
    In Conference on health, inference, and learning, pages 248–260\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla:
    Large language model connected with massive apis. arXiv preprint arXiv:2305.15334,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
    Yankai Lin, Zhong Zhang, Zhiyuan Liu, and Maosong Sun. Tell me more! towards implicit
    user intention understanding of language model driven agents. arXiv preprint arXiv:2402.09205,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning
    from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using
    siamese bert-networks, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu,
    Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that
    ask for help: Uncertainty alignment for large language model planners. arXiv preprint
    arXiv:2307.01928, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and
    tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. Advances in Neural Information
    Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
    Advances in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and
    Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances
    in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler,
    and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
    interactive learning. In International Conference on Learning Representations,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu.
    ScienceWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 11279–11298, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao
    Liang. Describe, explain, plan and select: Interactive planning with large language
    models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and
    Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence
    elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting
    chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen,
    Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse,
    explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards
    scalable real-world web interaction with grounded language agents. Advances in
    Neural Information Processing Systems, 35:20744–20757, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv
    preprint arXiv:2210.03629, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    arXiv preprint arXiv:2305.17878, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
    Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:
    a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey
    area: Expressions of overconfidence and uncertainty in language models. arXiv
    e-prints, pages arXiv–2302, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs: []
  type: TYPE_NORMAL
- en: \startcontents
  prefs: []
  type: TYPE_NORMAL
- en: '[sections] \printcontents[sections]l1'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Session-level optimization algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume that the entire trajectory $\tau$ is the memory before the session starts.
    In this section, we will explain how to transform a trajectory-level RL optimization
    algorithm into a session-level RL optimization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $r(\tau)$. The optimization objective is to maximize the following expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: For an arbitrary session index $i$, respectively. Accordingly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R(\theta)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\mathcal{S}_{i}$, indicating the expected total reward the agent expects
    to receive in the future. Averaging over all session indices, Eq. ([2](#A1.E2
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Framework
    of LLM Agents")) gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'In Eq. ([3](#A1.E3 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents")), the parameter $\theta$ only appears in the
    session-level expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '$R(\theta|\theta_{k})$. If we employ an iterative optimization procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize $\theta_{0}$ from a reference policy (obtained through SFT).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For $k=0,1,2,\cdots$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then $\theta$ will converge to an (at least locally) optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to illustrate why the optimization of $R(\theta|\theta_{k})$
    can be solved at the session level. Notice that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R(\theta&#124;\theta_{k})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(\tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'On the right-hand side, the first term involves two sampling steps. The first
    step samples $\tau_{1:i-1}\sim\pi_{\theta_{k}}$. As a result, if we define a *proxy
    reward*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Then, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{constant}.$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'By Eq. ([6](#A1.E6 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents")), $R(\theta|\theta_{k})$.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Session-level optimization
  prefs: []
  type: TYPE_NORMAL
- en: '1:Initialize $\theta_{0}$ by maximizing Eq. ([6](#A1.E6 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")).12:end for'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we present the session-level optimization algorithm as Algorithm [1](#alg1
    "Algorithm 1 ‣ Appendix A Session-level optimization algorithm ‣ AGILE: A Novel
    Framework of LLM Agents"). In this algorithm, the state advantage function is
    the only component that concerns inter-session correlation. While the algorithm
    is iterative, we anticipate that in practice, the outer loop will require only
    a few iterations to converge.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Implementation details of AGILE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 ProductQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementation of [GetQuestion]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This function prompts the user for a new question and appends it to the LLM
    context. Every question is raised for a specific product, thus it has an associated
    product ID. Based on this ID, the function also appends the product information
    table’s schema and the product metadata to the context.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of [RetrieveMemory]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This function employs the provided question as a query to retrieve the most
    relevant historical QA pair and the most relevant knowledge entry from the agent’s
    memory. To safeguard sensitive data from sellers, the agent is restricted to accessing
    QA records exclusively for the queried product from historical interactions. However,
    it is permitted to retrieve general knowledge from the whole trajectory since
    this information is not seller-specific. We utilize an embedding-based retrieval
    method, specifically employing the all-MiniLM-L6-v2 model [[28](#bib.bib28)] as
    the embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of [SearchProdcut]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This function utilizes the LLM to predict a SQL query based on the context,
    and then invoke a MySQL execution engine. It appends the result to the LLM context.
    If there is an execution error, then the error is appended to the context too.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of [SeekAdvice]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This requests for human expert advice and append it to the LLM context. In our
    implementation, the human expert simply returns the ground truth long answer from
    the ProductQA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of [PredictAnswer]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This function passes control to the LLM to continue generating a long answer
    and a short answer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of [Reflection]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This function passes control to the LLM to continue generating a reflection
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Training Data Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We generate training data on a session-by-session basis, where each session
    consists of a QA pair. A session begins with an initial memory, consisting of
    historical QA pairs and knowledge entries accumulated from previous sessions.
    Recall that the [RetrieveMemory] function retrieves only the most relevant QA
    pair and knowledge entry per session. Thus, in constructing training memories,
    it suffices to put the retrieved QA pair and the retrieved knowledge entry into
    the memory. We select them in the following stochastic way: the retrieved QA pair
    can be the most relevant QA pair from the training set, or a random QA pair, or
    omitted entirely; similarly for the retrieved knowledge entry.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the initial memory, we generate trajectories by following the agent
    workflow detailed in Section [4.1](#S4.SS1 "4.1 Experimental setting ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents"). Each trajectory begins with [GetUserQuestion]
    and [RetrieveMemory]. For QAs classified as Search-QA, a [SearchProduct] function
    is appended, followed by the corresponding SQL query and its execution result.
    For other QA types, if an associated knowledge entry exists and is successfully
    retrieved, the trajectory will extend with a [PredictAnswer] call with the ground
    truth answer as its result. If the knowledge entry is not retrieved or is absent,
    we use GPT-4 to evaluate whether the question can be answered with the available
    context. If affirmative, a [PredictAnswer] with the ground truth answer is appended.
    Otherwise, the trajectory extends with a [SeekAdvice] call with the ground truth
    answer as the advice, and a [Reflection] call, where the reflection result is
    the knowledge entry if it exists, or "no information" if not. Then the reflection
    result is appended to the memory via [UpdateMemory]. Finally, the trajectory is
    concluded by [SubmitAnswer].'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we constructed 55,772 session-level trajectories in total, from
    6 training tasks in ProductQA. This data is used for imitation learning. In PPO
    training, we reuse the initial memory data, while the session-level trajectories
    are generated by the model itself.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 MedMCQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For MedMCQA, the memory is initialized with all QA pairs from the training
    set, simulating that the agent has processed the training set before reaching
    the test set. We also add a knowledge entry for each QA pair, obtained through
    GPT-4 reflection (see Figure [11](#A6.F11 "Figure 11 ‣ Prompt templates for MedMCQA
    ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents") for the
    prompt).'
  prefs: []
  type: TYPE_NORMAL
- en: Training data generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We sample a subset of training data from MedMCQA to construct session-level
    trajectories. Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].
    The [RetrieveMemory] function retrieves the five most relevant QA pairs and pieces
    of knowledge from the initial memory, using the same embedding similarity search
    method employed in ProductQA. Then, we prompt GPT-4 to predict an answer with
    chain-of-thought reasoning. If the GPT-4 answer is correct, we append a [PredictAnswer]
    call, the GPT-4 chain-of-thought, and the ground-truth answer to the trajectory.
    If the GPT-4 answer is wrong, which suggests that the question is hard, we append
    a [SeekAdvice] call with the ground-truth answer, followed by a [Reflection] call
    with the reflection result generated by GPT-4\. Then the reflection result is
    appended to the memory via [UpdateMemory]. Finally, the trajectory is concluded
    by [SubmitAnswer]. In this way, we obtain 23,015 session-level trajectories in
    total.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Defining proxy reward for RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the question-answering tasks, sessions are not independent. Actions taken
    in earlier sessions can influence memory, creating lasting effects on subsequent
    sessions. As illustrated in Equation ([5](#A1.E5 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")), the term $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})$.
    Hence, we use the following heuristic definition,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="A2.E7.m1.3" class="ltx_Math" alttext="A_{i}=\beta\frac{\mathbbm{I}(N_{i+1:n}(q_{i})></math>
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $q_{i}$ by default.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 9: Statistics of the ProductQA dataset. # Products indicates the number
    of products within each group. # Fact-QA, # Search-QA and # Reasoning-QA display
    the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Groups | # Products | # Fact-QA | # Search-QA | # Reasoning-QA | Total |'
  prefs: []
  type: TYPE_TB
- en: '| Train | Blades | 20 | 2,147 | 769 | 631 | 3,547 |'
  prefs: []
  type: TYPE_TB
- en: '| Headlight Bulbs | 20 | 1,767 | 644 | 463 | 2,874 |'
  prefs: []
  type: TYPE_TB
- en: '| Cell Phones | 20 | 1,636 | 761 | 374 | 2,771 |'
  prefs: []
  type: TYPE_TB
- en: '| Portable Power Banks | 20 | 3,344 | 673 | 500 | 4,517 |'
  prefs: []
  type: TYPE_TB
- en: '| Dresses | 20 | 2,287 | 738 | 263 | 3,288 |'
  prefs: []
  type: TYPE_TB
- en: '| Everyday Bras | 20 | 1,942 | 684 | 336 | 2,962 |'
  prefs: []
  type: TYPE_TB
- en: '| Wrist Watches | 20 | 2,169 | 757 | 389 | 3,315 |'
  prefs: []
  type: TYPE_TB
- en: '| Blu-ray Players | 20 | 1,630 | 688 | 572 | 2,890 |'
  prefs: []
  type: TYPE_TB
- en: '| Camera Lenses | 20 | 1,859 | 769 | 1,025 | 3,653 |'
  prefs: []
  type: TYPE_TB
- en: '| Headphones | 20 | 5,432 | 766 | 583 | 6,781 |'
  prefs: []
  type: TYPE_TB
- en: '| Mice | 20 | 5,653 | 490 | 294 | 6,437 |'
  prefs: []
  type: TYPE_TB
- en: '| Point & Shoot Digital Cameras | 20 | 1,696 | 722 | 565 | 2,983 |'
  prefs: []
  type: TYPE_TB
- en: '| Coffee Machines | 20 | 4,184 | 681 | 638 | 5,503 |'
  prefs: []
  type: TYPE_TB
- en: '| Digital Scales | 20 | 2,724 | 391 | 682 | 3,797 |'
  prefs: []
  type: TYPE_TB
- en: '| Space Heaters | 20 | 2,283 | 674 | 498 | 3,455 |'
  prefs: []
  type: TYPE_TB
- en: '| Printers | 20 | 1,431 | 760 | 489 | 2,680 |'
  prefs: []
  type: TYPE_TB
- en: '| Litter | 20 | 1,860 | 753 | 507 | 3,120 |'
  prefs: []
  type: TYPE_TB
- en: '| Grips | 20 | 1,771 | 713 | 413 | 2,897 |'
  prefs: []
  type: TYPE_TB
- en: '| Gun Holsters | 20 | 1,679 | 94 | 1,362 | 3,135 |'
  prefs: []
  type: TYPE_TB
- en: '| Handheld Flashlights | 20 | 2,009 | 768 | 482 | 3,259 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | Leggings | 20 | 969 | 743 | 527 | 2,239 |'
  prefs: []
  type: TYPE_TB
- en: '| Camera Cases | 20 | 975 | 706 | 898 | 2,579 |'
  prefs: []
  type: TYPE_TB
- en: '| Motherboards | 20 | 989 | 736 | 826 | 2,551 |'
  prefs: []
  type: TYPE_TB
- en: '| All Pans | 20 | 973 | 747 | 275 | 1,995 |'
  prefs: []
  type: TYPE_TB
- en: '| Rollerball Pens | 20 | 967 | 760 | 603 | 2,330 |'
  prefs: []
  type: TYPE_TB
- en: '| Rifle Scopes | 17 | 979 | 714 | 978 | 2,671 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Detail performance of our methods and other baselines on six test
    product groups of ProductQA. X-prompt represents directly prompting the model
    X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting or PPO training. The Short and Long stand for the
    results evaluated on short answers and long answers, respectively. The seeking
    advice cost is $c=0.3$. The best total scores are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
  prefs: []
  type: TYPE_TB
- en: '| prompt | prompt | 13b-prompt | prompt | prompt | ppo(ours) | ppo(ours) |'
  prefs: []
  type: TYPE_TB
- en: '| Short | Long | Short | Long | Short | Long | Short | Long | Short | Long
    | Short | Long | Short | Long |'
  prefs: []
  type: TYPE_TB
- en: '| Camera Cases | Advice Rate $\downarrow$ | - | - | - | - | 0.182 | 0.182 |
    0.313 | 0.313 | 0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537
    | 0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 |
    0.722 | 0.738 | 0.758 | 0.716 | 0.781 | 0.762 |'
  prefs: []
  type: TYPE_TB
- en: '| Leggings | Advice Rate $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359
    | 0.359 | 0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497
    | 0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 |
    0.706 | 0.730 | 0.777 | 0.774 | 0.801 | 0.810 |'
  prefs: []
  type: TYPE_TB
- en: '| All Pans | Advice Rate $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336
    | 0.336 | 0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506
    | 0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 |
    0.718 | 0.738 | 0.788 | 0.776 | 0.800 | 0.803 |'
  prefs: []
  type: TYPE_TB
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | - | - | - | - | 0.130 | 0.130
    | 0.333 | 0.333 | 0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482
    | 0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 |
    0.698 | 0.739 | 0.727 | 0.720 | 0.752 | 0.760 |'
  prefs: []
  type: TYPE_TB
- en: '| Mother- boards | Advice Rate $\downarrow$ | - | - | - | - | 0.214 | 0.214
    | 0.303 | 0.303 | 0.225 | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525
    | 0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 |
    0.747 | 0.788 | 0.786 | 0.782 | 0.806 | 0.812 |'
  prefs: []
  type: TYPE_TB
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | - | - | - | - | 0.197 | 0.197 |
    0.293 | 0.293 | 0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502
    | 0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 |
    0.711 | 0.747 | 0.752 | 0.710 | 0.763 | 0.757 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | Advice Rate $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323
    | 0.323 | 0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508
    | 0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 |
    0.718 | 0.747 | 0.764 | 0.746 | 0.784 | 0.784 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Ablation study on ProductQA test tasks. w/o Reflection represents
    removing the reflection function. w/o Memory represents prohibiting memory component.
    w/o Advice represents removing the seeking advice function. Non-adapt advice represents
    seeking advice in the same number with agile-vic13b-ppo at the beginning of trajectory.
    w/o Tool-Use represents removing the search product function. w/o RL represents
    the agile-vic13b-sft. The best scores are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | w/o | w/o | w/o | Non-adapt | w/o | w/o | agile-vic- |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection | Memory | Advice | Advice | Tool-Use | RL | 13b-ppo |'
  prefs: []
  type: TYPE_TB
- en: '| Camera Cases | Advice Rate $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 |
    0.452 | 0.295 | 0.263 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
  prefs: []
  type: TYPE_TB
- en: '| Leggings | Advice Rate $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529
    | 0.290 | 0.251 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
  prefs: []
  type: TYPE_TB
- en: '| All Pans | Advice Rate $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550
    | 0.225 | 0.220 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
  prefs: []
  type: TYPE_TB
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212
    | 0.501 | 0.220 | 0.212 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
  prefs: []
  type: TYPE_TB
- en: '| Mother- boards | Advice Rate $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235
    | 0.483 | 0.285 | 0.235 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
  prefs: []
  type: TYPE_TB
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 |
    0.440 | 0.221 | 0.216 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | Advice Rate $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492
    | 0.256 | 0.233 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Score $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Performance of the model (agile-vic13b-ppo) trained on different
    seeking advice cost settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Seeking Advice Cost |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Camera Cases | Advice Rate | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
  prefs: []
  type: TYPE_TB
- en: '| Leggings | Advice Rate | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
  prefs: []
  type: TYPE_TB
- en: '| All Pans | Advice Rate | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
  prefs: []
  type: TYPE_TB
- en: '| Rollerball Pens | Advice Rate | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
  prefs: []
  type: TYPE_TB
- en: '| Motherboards | Advice Rate | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
  prefs: []
  type: TYPE_TB
- en: '| Rifle Scopes | Advice Rate | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | Advice Rate | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Case study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Case #1, illustrated in Table [13](#A4.T13 "Table 13 ‣ Appendix D Case study
    ‣ AGILE: A Novel Framework of LLM Agents"), provides a specific example demonstrating
    how agile-vic13b-ppo proactively seeks advice from a human expert for questions
    it cannot answer. Furthermore, it leverages reflection to extract general knowledge
    from the expert’s responses, which can then be applied in future QA sessions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case #2, shown in Table [14](#A4.T14 "Table 14 ‣ Appendix D Case study ‣ AGILE:
    A Novel Framework of LLM Agents"), demonstrates how agile-vic13b-ppo utilizes
    tools to address product recommendation questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: Case #1 from agile-vic13b-ppo on the test task, camera cases group,
    in ProductQA. ✓represents correct responses. Our agent proactively seeks advice
    from human experts at session 313 and summarizes a piece of general knowledge
    by reflection. In the following sessions, the agent leverages this knowledge to
    successfully answer three questions for different products at sessions 721, 1265,
    and 1364, respectively. It demonstrates the effectiveness of seeking advice and
    reflection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Session | Context | Action | Reflection / Answer |'
  prefs: []
  type: TYPE_TB
- en: '| 313 | Answer users’ [Question] about product B00PV0IEDY based on the following
    three types of information: 1\. [Product Metadata] … ‘Carrying Options’: ‘backpack
    straps’ … 2\. [Memory] Existing knowledge in your memory about camera_cases. camera
    cases with no tripod mount are not suitable for scenarios where a stable camera
    | Seek Advice | Human Expert: Certainly, the backpack straps on the camera case
    are designed for comfort even when carrying heavier loads. These straps enable
    ergonomic weight distribution, making the case ideal for prolonged travel. ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| setup is crucial, such as long-exposure photography or macro work where camera
    shake can significantly impact image quality.  3.[History] … … Question: Is the
    camera case designed for prolonged travel and capable of withstanding heavy loads?
    | Reflection: Camera cases with backpack straps feature ergonomic designs that
    help distribute the weight of the camera equipment across the user’s back, reducing
    strain and increasing comfort during transport. |'
  prefs: []
  type: TYPE_TB
- en: '| 721 | Answer users’ [Question] about product B000WH86BQ … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Is the camera case suitable for extended journeys
    and potentially heavy loads? | Predict Answer | Answer: The case is designed to
    be adjustable and has padded straps which makes it suitable for long-term use
    with heavy equipment. ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 1265 | Answer users’ [Question] about product B005J09OXU … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Will this camera case enhance the comfort of transporting
    my gear on treks? | Predict Answer | Answer: Certainly, a camera case with backpack
    straps is engineered to evenly distribute the weight across your shoulders, improving
    ergonomics and comfort. This is particularly beneficial for reducing fatigue and
    strain during prolonged activities like treks. ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 1364 | Answer users’ [Question] about product B000YA33DC … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Does this camera case facilitate easier transportation
    when I frequently travel? | Predict Answer | Answer: Certainly, the backpack straps
    on this camera case not only simplify the process of carrying but also evenly
    distribute the weight, making it an ideal option for those who are frequently
    on the move and prefer a more comfortable and balanced carrying solution. ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Case #2 from agile-vic13b-ppo on the ProductQA test set. ✓represents
    correct responses. By leveraging SQL searching tools, agent successfully find
    the correct product to recommend to users.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Session | Context | Action | Output |'
  prefs: []
  type: TYPE_TB
- en: '| 918 | Answer users’ [Question] about product B00FUHWE5M based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B00FUHWE5M from the leggings database. The schema of the leggings database is
    outlined below (in the format field[unit](value1, value2, …)). price[dollar] brand(Jescakoo,
    QZUnique, …) warmth(hot,moderate,cool) … 2\. [Memory] … 3\. [History] … … Question:
    I’ve heard good things about QZUnique. Spit out the first leggings you have from
    them. … | Search Product | SQL: SELECT product_id FROM “leggings” WHERE brand
    = “QZUnique” LIMIT 1;'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: The product that meets your requirements is as follows. ‘QZUnique Women’s
    Classic Retro Printed Casual Pattern Ankle Length Elastic Tights Leggings’ with
    asin: B012IKGF62 ✓ |'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1298 | Answer users’ [Question] about product B012AQGKXC based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B012AQGKXC from the motherboards database. The schema of the motherboards database
    is outlined below (in the format field[unit](value1, value2, …)). price[dollar]
    brand(ASRock, Gigabyte, …) memory_support[GB] … 2\. [Memory] … 3\. [History] …
    … Question: I’m looking for an ASRock motherboard with at least 32 GB of memory
    support. Don’t keep me waiting, hustle up and find it for me. … | Search Product
    | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND memory_support
    >= 32 LIMIT 1;'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: The product that would suit your preferences is: ‘AS Rock LGA1155 DDR3
    SATA3 USB3.0 Quad CrossFireX and Quad SLI A E ATX Motherboard Z77 EXTREME4’ with
    asin: B007KTY4A6 ✓ |'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Development of the ProductQA dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E.1 Product collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The product groups and the corresponding products are collected by the following
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter the Amazon Review Data to retain only products with at least 100 reviews,
    then cluster them by category tags.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sort the clusters by size, from largest to smallest. Manually review each cluster
    in order: we keep product clusters that involve diverse technical details and
    long-tail domain knowledge, such as electronics, from which we can potentially
    construct a diverse set of user questions. The manual review ends when we have
    collected 26 clusters. Each cluster is referred to as a *product group*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each product group, we remove the top 10% of products with the highest number
    of reviews. We exclude these most popular products from the datasets to prevent
    data leakage, as information about them is likely included in the pre-training
    set of LLMs. From the remaining items, we randomly select up to 20 products to
    form the final product set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: E.2 Annotation guidelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two annotation tasks, product table creation and QA collection. We
    provide the annotation guidelines in this Section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Task 1: Product table creation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each product group, we provide a series of features and their corresponding
    values for each product in the group. This information is obtained by prompting
    GPT-4 to extract data from the reviews of each product. The task of annotators
    is to construct a product table containing only the metadata. Please follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select up to 15 common features relevant to the product group. These features
    must include product ID, product title, brand, and price. Choose additional features
    based on their commonality and necessity within the product group.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each product in the product group, verify the feature values for each selected
    feature.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, the product tables are reviewed and refined by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Task 2: QA collection'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Annotators are required to fill out a table as shown in Table [15](#A5.T15
    "Table 15 ‣ Task 2: QA collection ‣ E.2 Annotation guidelines ‣ Appendix E Development
    of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents"). Each row
    contains a triplet consisting of a *question*, a *long answer*, and a *short answer*,
    all generated by GPT-4\. Annotators should fill the following columns: *Is question
    reasonable*, *Is long answer correct*, *Refined long answer*, *Is short answer
    correct* and *Refined short answer*. Please follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate the question: Verify if the *question* resembles a typical query found
    in real-world product conversations in online shopping. Select ‘yes’ or ‘no’ in
    the *Is question reasonable* column. Any question containing harmful information
    is considered unreasonable and should be labeled as ‘no’. If ‘no’ is selected,
    the row will be dropped, and you do not need to proceed with the subsequent steps
    for that row.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assess the long answer: Check if the *long answer* correctly responds to the
    *question*. Select ‘yes’, ‘no’ or ‘I do not know’ in the *Is long answer correct*
    column. Consider the following special cases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the long answer is ambiguous (e.g., ‘The product is designed to be waterproof,
    while some users do not think so.’), mark it as incorrect.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For numerical questions, an answer is considered correct if it fits the real-world
    scenario and the conclusion is clear. Specific values or ranges (e.g., 5cm, 5cm-10cm,
    several months) are acceptable if they correspond to the real-world scenario.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the long answer contains a specific piece of knowledge, verify its accuracy.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the long answer is incorrect or does not address the question, and you do
    not know the correct answer (even after checking the product information table,
    looking up the product URL, and searching online), select ‘I do not know’.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Any long answer containing harmful information should be labeled as ‘I do not
    know’.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: If you select ‘I do not know’, the row will be dropped, and you do not need
    to perform the subsequent steps for that row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Refine the long answer: If you select ‘no’ in step 2, provide a correct long
    answer in the *Refined long answer* column.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assess the short answer: Determine whether the *short answer* is correct. A
    short answer must be ‘yes’, ‘no’, or an entity. Choose ‘yes’ or ‘no’ in the *Is
    short answer correct* column. Consider the following special cases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the question is a choice and the short answer is ‘yes’ or ‘no’, it is incorrect.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the question pertains to degrees (e.g. ‘How durable … ?’) and the short answer
    is ‘yes’ or ‘no’, it is incorrect.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the short answer does not align with the long answer, it is incorrect.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Refine the short answer: If you select ‘no’ in step 4, provide a correct short
    answer in the *Refined short answer* column.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The authors will review the annotation in batches. Specifically, 5% of each
    batch will be checked. If the accuracy rate of the checked annotation is below
    98%, the entire batch will be relabeled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 15: An example of the ProductQA annotation table.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Is question | Long answer | Is long answer | Refined | Short answer
    | Is short answer | Refined |'
  prefs: []
  type: TYPE_TB
- en: '| reasonable | correct | long answer | correct | short answer |'
  prefs: []
  type: TYPE_TB
- en: '| What is the size of the neodymium driver used in the JVC HA-EB75 headphones?
    | [To fill] | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | [To fill] |
    [To fill] | 13.5 mm | [To fill] | [To fill] |'
  prefs: []
  type: TYPE_TB
- en: Appendix F Prompt templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt templates for ProductQA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [5](#A6.F5 "Figure 5 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents") shows the prompt template
    for gpt3.5-prompt, gpt4-prompt. Figure [6](#A6.F6 "Figure 6 ‣ Prompt templates
    for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents")
    provides the prompt template for agile-vicuna-13b-prompt, agile-gpt3.5-prompt,
    and agile-gpt4-prompt. We leave the "{knowledge} and "{history}" empty when evaluate
    gpt3.5-prompt and gpt4-prompt. The prompt template for reflection is shown in
    Figure [7](#A6.F7 "Figure 7 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt template for long answer evaluation is shown in Figure [8](#A6.F8
    "Figure 8 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE:
    A Novel Framework of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt templates for MedMCQA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [9](#A6.F9 "Figure 9 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents") provides the prompt template
    for Meerkat-7b-promp. Figure [10](#A6.F10 "Figure 10 ‣ Prompt templates for MedMCQA
    ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents") illustrates
    the prompt template for agile-gpt3.5-prompt, agile-gpt4-prompt. We leave the "{related_question}
    and "{related_knowledge}" empty when evaluate gpt3.5-prompt and gpt4-prompt. The
    prompt template for reflection is shown in Figure [11](#A6.F11 "Figure 11 ‣ Prompt
    templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework
    of LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8f2d9e157ef973cc7edef697f5a3458.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The prompt for gpt3.5-prompt and gpt4-prompt on ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5dba15a3e003e34c49dd2fa514d1c440.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The prompt for agile-vicuna-13b-prompt, agile-gpt3.5-prompt, and
    agile-gpt4-prompt on ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb0af70333b2eedd94917368c390479e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The prompt for reflection on ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5645d4b1b9b6f42f853a91b29f896b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The prompt for long answer evaluation on ProductQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec4d4bc5fb9d10d7589fe193311162a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The prompt for Meerkat-7b-prompt on MedMCQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/40880a4f862b06e21276adbbce4f5867.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The prompt for agile-gpt3.5-prompt and agile-gpt4-prompt on MedMCQA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3eb3b0c3a58ed7a1cd6e3928dae2383d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The prompt for reflection on MedMCQA.'
  prefs: []
  type: TYPE_NORMAL
