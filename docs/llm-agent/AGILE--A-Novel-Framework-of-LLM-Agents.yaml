- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:45:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AGILE: A Novel Framework of LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AGILE：一种新颖的 LLM 代理框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14751](https://ar5iv.labs.arxiv.org/html/2405.14751)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14751](https://ar5iv.labs.arxiv.org/html/2405.14751)
- en: Peiyuan Feng^∗¹ Yichen He^∗¹  Guanhua Huang^∗^†²  Yuan Lin^∗¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Peiyuan Feng^∗¹ Yichen He^∗¹  Guanhua Huang^∗^†²  Yuan Lin^∗¹
- en: Hanchong Zhang^∗^†³ Yuchen Zhang^∗¹ Hang Li¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hanchong Zhang^∗^†³ Yuchen Zhang^∗¹ Hang Li¹
- en: ¹ByteDance Research  ²University of Science and Technology of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹字节跳动研究  ²中国科学技术大学
- en: ³Shanghai Jiao Tong University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海交通大学
- en: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
- en: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce a novel framework of LLM agents named AGILE (AGent that Interacts
    and Learns from Environments) designed to perform complex conversational tasks
    with users, leveraging LLMs, memory, tools, and interactions with experts. The
    agent’s abilities include not only conversation but also reflection, utilization
    of tools, and consultation with experts. We formulate the construction of such
    an LLM agent as a reinforcement learning problem, in which the LLM serves as the
    policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm.
    We focus on question answering and release a dataset for agents called ProductQA,
    comprising challenging questions in online shopping. Our extensive experiments
    on ProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained
    with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability
    of memory, tools, consultation, reflection, and reinforcement learning in achieving
    the agent’s strong performance. Datasets and code are available at [https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一种新颖的 LLM 代理框架，称为 AGILE（AGent that Interacts and Learns from Environments），旨在通过利用
    LLM、记忆、工具和与专家的互动来执行复杂的对话任务。该代理的能力不仅包括对话，还包括反思、工具的使用和与专家的咨询。我们将构建这样的 LLM 代理视为一个强化学习问题，其中
    LLM 作为策略模型。我们使用标记的动作数据和 PPO 算法对 LLM 进行微调。我们专注于问答任务，并发布了一个名为 ProductQA 的数据集，其中包含在线购物中的挑战性问题。我们在
    ProductQA 和 MedMCQA 上进行的大量实验表明，基于 13B 和 7B LLM 以及 PPO 训练的 AGILE 代理可以优于 GPT-4 代理。我们的消融研究突出了记忆、工具、咨询、反思和强化学习在实现代理强大性能中的不可或缺性。数据集和代码可在
    [https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE) 获得。
- en: '^($*$)^($*$)footnotetext: Equal contribution. Alphabet order.^($\dagger$)^($\dagger$)footnotetext:
    Work done during ByteDance Research internship.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^($*$)^($*$)脚注：同等贡献。按字母顺序。^($\dagger$)^($\dagger$)脚注：工作在字节跳动研究实习期间完成。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have exhibited remarkable capabilities such as
    instruction following, reasoning, and zero-shot learning [[2](#bib.bib2), [39](#bib.bib39),
    [15](#bib.bib15), [21](#bib.bib21)], which have greatly catalyzed the development
    of autonomous agents based on LLMs [[23](#bib.bib23), [25](#bib.bib25), [1](#bib.bib1)],
    also known as LLM agents. Recent works propose several essential components or
    workflows to enhance the abilities of LLM agents, such as planning [[39](#bib.bib39),
    [44](#bib.bib44), [33](#bib.bib33)], reflection [[16](#bib.bib16), [34](#bib.bib34)],
    tool-use [[24](#bib.bib24), [31](#bib.bib31), [41](#bib.bib41)] and life-long
    learning [[36](#bib.bib36)]. However, it remains unclear how to integrate all
    components into a unified framework and optimize them end-to-end.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了诸如指令跟随、推理和零-shot 学习等显著能力 [[2](#bib.bib2), [39](#bib.bib39), [15](#bib.bib15),
    [21](#bib.bib21)]，这些能力极大地推动了基于 LLM 的自主代理的发展 [[23](#bib.bib23), [25](#bib.bib25),
    [1](#bib.bib1)]，也被称为 LLM 代理。最近的研究提出了若干个重要组件或工作流程，以增强 LLM 代理的能力，如规划 [[39](#bib.bib39),
    [44](#bib.bib44), [33](#bib.bib33)]、反思 [[16](#bib.bib16), [34](#bib.bib34)]、工具使用
    [[24](#bib.bib24), [31](#bib.bib31), [41](#bib.bib41)] 和终身学习 [[36](#bib.bib36)]。然而，如何将所有组件整合到一个统一的框架中并进行端到端优化仍然不明确。
- en: '![Refer to caption](img/d0ab0e0cb5ecfb005f3eb9e85e88f019.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d0ab0e0cb5ecfb005f3eb9e85e88f019.png)'
- en: 'Figure 1: (a) Architecture of our agent system, including LLM, memory, tools,
    and executor. (b) A running example of AGILE in a customer service QA environment.
    The tokens (actions) generated by the LLM are in orange color and the tokens appended
    by the executor are in blue color.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：（a）我们代理系统的架构，包括 LLM、记忆、工具和执行器。（b）AGILE 在客户服务问答环境中的运行示例。由 LLM 生成的令牌（动作）为橙色，执行器附加的令牌为蓝色。
- en: 'In this paper, we introduce a novel framework for LLM agents to unify various
    components and streamline their learning and operation processes. As shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of LLM
    Agents")(a), the architecture of the agent system, named AGILE, comprises four
    modules: LLM, memory, tools, and executor. Furthermore, the agent can interact
    with both users and experts. The LLM, functioning as the predictor of all actions,
    generates instructions and processes responses. The executor, working as the controller
    of all actions, interprets the LLM instructions to activate the corresponding
    modules and collects their responses for the LLM. For example, the executor can
    fetch a text from the memory and append it to the context of LLM, or extract an
    excerpt from the context and append it to the memory. The executor can also follow
    instructions of the LLM to utilize a search tool. In addition to skills such as
    reasoning, planning, and reflection, we propose a new ability called *seeking
    advice*, which means that the agent proactively consults human experts when it
    encounters a problem unsolvable. The agent can reflect on the expert feedback
    and memorize it for future use. Furthermore, we propose a training method based
    on reinforcement learning (RL), which simultaneously trains the policy of invoking
    different modules and the reasoning, planning, reflection, and seeking advice
    abilities of the LLM agent in an end-to-end fashion.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们介绍了一个新颖的LLM代理框架，以统一各种组件并简化它们的学习和操作过程。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Framework of LLM Agents")(a)所示，名为AGILE的代理系统架构包含四个模块：LLM、记忆、工具和执行器。此外，代理可以与用户和专家互动。LLM作为所有动作的预测器，生成指令并处理响应。执行器作为所有动作的控制器，解释LLM指令以激活相应的模块，并为LLM收集它们的响应。例如，执行器可以从记忆中获取文本并将其附加到LLM的上下文中，或从上下文中提取摘录并将其附加到记忆中。执行器还可以按照LLM的指示使用搜索工具。除了推理、规划和反思等技能外，我们提出了一种新的能力，称为*寻求建议*，即代理在遇到无法解决的问题时主动咨询人类专家。代理可以反思专家反馈并将其记住以备未来使用。此外，我们提出了一种基于强化学习（RL）的训练方法，该方法同时训练不同模块的调用策略以及LLM代理的推理、规划、反思和寻求建议能力，以端到端的方式进行。'
- en: While the proposed agent framework is general, in this paper, we evaluate it
    in complex question answering (QA). It is a task an LLM agent has the potential
    of outperforming existing solutions such as the use of an LLM alone. However,
    existing QA benchmarks [[9](#bib.bib9), [42](#bib.bib42), [8](#bib.bib8), [22](#bib.bib22)]
    are designed for specific subsets of capabilities (e.g., reflection, memory retrieve,
    etc.) which cannot simultaneously investigate the ability to combine all modules
    and capabilities of the agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提出的代理框架是通用的，但在本文中，我们评估了它在复杂问答（QA）中的表现。这是一个LLM代理有可能超越现有解决方案的任务，例如单独使用LLM。然而，现有的QA基准测试[[9](#bib.bib9),
    [42](#bib.bib42), [8](#bib.bib8), [22](#bib.bib22)]是为特定能力子集（例如，反思、记忆检索等）设计的，无法同时调查代理的所有模块和能力的结合能力。
- en: To address this, we have developed a new benchmark called ProductQA. ProductQA
    comprises 88,229 question-answer pairs in customer service divided into 26 QA
    tasks, each corresponding to a distinct Amazon product category. This benchmark
    is based on real Amazon user queries and includes fact-based questions, reasoning
    questions, and product recommendation queries. It comprehensively evaluates agents’
    abilities to handle historical information and accumulated knowledge, leverage
    tools, interact with humans, perform self-evaluation, and conduct reflection.
    Additionally, the training and testing tasks are made disjoint to assess the agent’s
    ability to adapt to new product categories.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一问题，我们开发了一个新的基准测试，称为ProductQA。ProductQA包含了88,229对顾客服务中的问答对，分为26个QA任务，每个任务对应一个独特的Amazon产品类别。该基准测试基于真实的Amazon用户查询，包括事实性问题、推理问题和产品推荐查询。它全面评估了代理处理历史信息和积累知识的能力，利用工具的能力，与人互动的能力，自我评估能力和进行反思的能力。此外，训练和测试任务被分开，以评估代理适应新产品类别的能力。
- en: 'We evaluate our agent framework on two tasks, ProductQA and MedMCQA [[22](#bib.bib22)].
    For ProductQA, we use a two-stage training method based on Vicuna-13b [[5](#bib.bib5)].
    In the first stage, imitation learning is employed to create agile-vic13b-sft.
    In the second stage, the policy gradient algorithm of PPO [[32](#bib.bib32)] produces
    agile-vic13b-ppo. Experimental results show that agile-vic13b-ppo improves the
    relative total performance score by 9.2% over GPT-4 and by 90.8% over GPT-3.5\.
    Ablation studies confirm that all modules in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Framework of LLM Agents") are indispensable. Specifically, removing
    tools or memory usage negatively impacts the agent’s performance, leading to a
    25.9% or 17.4% increase in seeking advice, respectively, or a 9.3% or 4.0% relative
    decrease in the total score, respectively. Disabling the seeking advice function
    results in a 10.7% decrease in accuracy. Finally, agile-vic13b-ppo achieves a
    2.3% relative increase in total score compared to agile-vic13b-sft, demonstrating
    the necessity of PPO training. On MedMCQA, we train an agile-mek7b-ppo agent,
    initialized from Meerkat-7b [[13](#bib.bib13)], following the same two-stage procedure.
    Our agent improves the base LLM’s accuracy from 53.4% to 85.2% by seeking advice
    on 31.6% instances. This accuracy surpasses the SOTA accuracy of 79.1% by GPT4-MedPrompt [[20](#bib.bib20)].
    When all agents are able to seek advice, our agent also outperforms the GPT-4
    agent in terms of the total score.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在两个任务上评估了我们的代理框架，ProductQA 和 MedMCQA [[22](#bib.bib22)]。对于 ProductQA，我们使用基于
    Vicuna-13b [[5](#bib.bib5)] 的两阶段训练方法。在第一阶段，采用模仿学习创建 agile-vic13b-sft。在第二阶段，使用
    PPO [[32](#bib.bib32)] 的策略梯度算法生成 agile-vic13b-ppo。实验结果表明，agile-vic13b-ppo 相较于
    GPT-4 提高了 9.2% 的相对总性能分数，相较于 GPT-3.5 提高了 90.8%。消融研究确认图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣
    AGILE: 一种新型的LLM代理框架")中的所有模块都是不可或缺的。具体而言，移除工具或记忆使用会对代理的性能产生负面影响，分别导致寻求建议的增加 25.9%
    或 17.4%，或总分数的相对减少 9.3% 或 4.0%。禁用寻求建议功能会导致准确性下降 10.7%。最后，agile-vic13b-ppo 相较于 agile-vic13b-sft
    实现了 2.3% 的总分数相对增加，证明了 PPO 训练的必要性。在 MedMCQA 上，我们训练了一个 agile-mek7b-ppo 代理，该代理从 Meerkat-7b
    [[13](#bib.bib13)] 初始化，遵循相同的两阶段程序。我们的代理通过在 31.6% 的实例上寻求建议，将基础 LLM 的准确性从 53.4%
    提高到 85.2%。这一准确性超过了 GPT4-MedPrompt [[20](#bib.bib20)] 的 SOTA 准确性 79.1%。当所有代理都能寻求建议时，我们的代理在总分数方面也超越了
    GPT-4 代理。'
- en: 'The main contributions of this paper are summarized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献总结如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose a novel framework of LLM agents, formulated in the context of reinforcement
    learning. It facilitates end-to-end learning of agents. Notably, this framework
    enables the agent to seek advice from human experts, providing two advantages:
    1) It ensures high-level accuracy when dealing with complex and challenging questions,
    and 2) it fosters learning from humans, thereby enhancing its abilities to adapt
    to new tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新型的 LLM 代理框架，该框架在强化学习的背景下制定。它促进了代理的端到端学习。值得注意的是，该框架使代理能够向人类专家寻求建议，提供了两个优势：1）在处理复杂和具有挑战性的问题时，确保高水平的准确性；2）促进从人类那里学习，从而增强其适应新任务的能力。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a benchmark, ProductQA, to comprehensively evaluate the agent’s capabilities
    in complex question answering.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个基准，ProductQA，以全面评估代理在复杂问答中的能力。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform experiments on ProductQA and MedMCQA to verify our framework and
    show that AGILE agents based on 13B and 7B LLMs trained with PPO can surpass GPT-4
    agents.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在 ProductQA 和 MedMCQA 上进行实验，以验证我们的框架，并展示基于 13B 和 7B LLM 并使用 PPO 训练的 AGILE
    代理可以超越 GPT-4 代理。
- en: 2 Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 RL formulation of agent
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代理的强化学习表述
- en: 'Our agent framework comprises four elements: LLM, memory, tools and executor,
    see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of
    LLM Agents")(a). The LLM possesses a *context*, defined as the sequence of tokens
    it utilizes to generate the next token. In RL terminology, the agent conducts
    a token-level Markov decision process (MDP). The action space $\mathcal{A}$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的代理框架包含四个要素：LLM、记忆、工具和执行器，见图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ AGILE: 一种新型的LLM代理框架")（a）。LLM
    拥有一个*上下文*，定义为它用来生成下一个标记的标记序列。在强化学习术语中，代理执行的是标记级的马尔可夫决策过程（MDP）。动作空间为$\mathcal{A}$。'
- en: 'Let us examine the state transition more closely. For each action, the executor’s
    first operation is to append the token to the context, preparing the LLM for generating
    the next token. Then, the executor checks a registered list of *functions*. Each
    function is designed to execute a set of operations, including memory I/O, tool
    usage, and interaction with the environment. If the action (i.e., the token) matches
    a function name, the executor will execute the associated function implementation,
    further mutating the agent state. For instance, if the token is [GetQuestion],
    the executor will prompt the user for a new question and append it to the context;
    if the token is [UpdateMemory], the executor will write a specific segment of
    the context into the memory; if the token is [ClearContext], the executor will
    reset the context to [BOS]. In summary, the LLM interacts with the memory and
    tools by predicting function names, relying on the executor to execute these functions.
    See Table [1](#S2.T1 "Table 1 ‣ 2.1 RL formulation of agent ‣ 2 Methods ‣ AGILE:
    A Novel Framework of LLM Agents") for a full list of functions defined for a QA
    agent and see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework
    of LLM Agents")(b) for a running example.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们更仔细地检查状态转换。对于每个动作，执行者的第一个操作是将标记附加到上下文中，为生成下一个标记做好准备。然后，执行者检查注册的*函数*列表。每个函数设计用于执行一组操作，包括内存
    I/O、工具使用以及与环境的互动。如果动作（即标记）与函数名匹配，执行者将执行相关函数的实现，进一步改变代理状态。例如，如果标记是[GetQuestion]，执行者将提示用户提出新问题并将其附加到上下文中；如果标记是[UpdateMemory]，执行者将把上下文中的特定段写入内存；如果标记是[ClearContext]，执行者将上下文重置为[BOS]。总之，LLM通过预测函数名称与内存和工具进行交互，依赖执行者来执行这些函数。有关QA代理定义的函数的完整列表，请参见表[1](#S2.T1
    "Table 1 ‣ 2.1 RL formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Framework
    of LLM Agents")，有关运行示例，请参见图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel
    Framework of LLM Agents")(b)。'
- en: 'Table 1: Functions for an exemplary customer service QA agent. Among them,
    [Reflection] and [PredictAnswer] are trivial functions, as the executor passes
    control immediately back to the LLM to start generating result tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：示例客户服务QA代理的函数。其中，[Reflection]和[PredictAnswer]是简单函数，因为执行者立即将控制权返回给LLM，以开始生成结果标记。
- en: '| Function name | Function implementation |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 函数名称 | 函数实现 |'
- en: '|  [GetQuestion] | Prompt the user for a question and append it to the context.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  [GetQuestion] | 提示用户提出问题并将其附加到上下文中。 |'
- en: '|  [RetrieveMemory] | Retrieve relevant entries from the memory and append
    them to the context. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  [RetrieveMemory] | 从内存中检索相关条目并将其附加到上下文中。 |'
- en: '|  [SeekAdvice] | Ask human experts for advice and append it to the context.
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  [SeekAdvice] | 向人类专家寻求建议并将其附加到上下文中。 |'
- en: '|  [Reflection] | $\emptyset$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  [Reflection] | $\emptyset$ |'
- en: '|  [UpdateMemory] | Write a specific segment of the context into the memory.
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  [UpdateMemory] | 将上下文的特定段写入内存。 |'
- en: '|  [SearchProduct] | Extract a search query from the context, then invoke the
    search tool and append results to the context. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  [SearchProduct] | 从上下文中提取搜索查询，然后调用搜索工具并将结果附加到上下文中。 |'
- en: '|  [PredictAnswer] | $\emptyset$ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  [PredictAnswer] | $\emptyset$ |'
- en: '|  [SubmitAnswer] | Extract a predicted answer from the context and submit
    it to the user. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  [SubmitAnswer] | 从上下文中提取预测答案并提交给用户。 |'
- en: '|  [ClearContext] | Reset the context to a single token [BOS]. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  [ClearContext] | 将上下文重置为单个标记[BOS]。 |'
- en: 2.2 Policy learning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 策策学习
- en: We frame the policy learning problem as a task of training a language model.
    Consider an agent trajectory $\tau=(s_{1},a_{1},...,s_{n},a_{n})$ because the
    executor can delete context tokens.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将策略学习问题框架为训练语言模型的任务。考虑一个代理轨迹 $\tau=(s_{1},a_{1},...,s_{n},a_{n})$，因为执行者可以删除上下文标记。
- en: In Imitation Learning (IL), we generate trajectories by observing human experts
    or more proficient agents, then we derive the training sequences to fine-tune
    the LLM. It is important to point out that (1) the loss is calculated on the action
    tokens only, and (2) $c_{i}$ as the attention mask.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿学习（IL）中，我们通过观察人类专家或更熟练的代理生成轨迹，然后推导训练序列以微调LLM。值得指出的是（1）损失仅在动作标记上计算，（2）$c_{i}$
    作为注意力掩码。
- en: 'In some situations, an agent may produce very long trajectories, potentially
    yielding training sequences that span millions of tokens and are impractical for
    training. We can leverage the structure of the trajectory to partition it into
    smaller segments. For instance, if the agent resets its LLM context at the beginning
    of every QA session, then we can partition by the session boundary. Nevertheless,
    these sessions are not entirely independent; actions taken in earlier sessions
    can influence memory, creating lasting effects on subsequent sessions. To tackle
    this challenge of long-range dependencies, we propose a training algorithm detailed
    in Appendix [A](#A1 "Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，代理可能产生非常长的轨迹，这可能导致跨度达到数百万个标记的训练序列，并且不适合训练。我们可以利用轨迹的结构将其划分为更小的段。例如，如果代理在每次QA会话开始时重置其LLM上下文，则可以按会话边界进行划分。然而，这些会话并非完全独立；早期会话中的行动可以影响记忆，对后续会话产生持久影响。为了应对这种长程依赖挑战，我们提出了一种训练算法，详见附录[A](#A1
    "附录 A 会话级优化算法 ‣ AGILE：一种新颖的LLM代理框架")。
- en: 2.3 Interaction with human experts
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 与人类专家的互动
- en: Our agent framework enables the agent to proactively seek advice from human
    experts. For example, the agent can invoke a [SeekAdvice] function to request
    expert advice. This approach helps in two ways. Firstly, the agent can request
    the correct answer when its confidence is low, ensuring sufficient accuracy for
    the application. Secondly, the agent can use [Reflection] to distill general knowledge
    from the expert advice before storing it in memory. This accumulation of knowledge
    allows the agent to adapt to new tasks that it has not encountered during training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理框架使代理能够主动向人类专家寻求建议。例如，代理可以调用一个[SeekAdvice]函数来请求专家建议。这种方法有两个好处。首先，当代理的信心较低时，它可以请求正确的答案，确保应用程序的准确性。其次，代理可以使用[Reflection]从专家建议中提炼出一般知识，然后将其存储在记忆中。这种知识积累使代理能够适应训练过程中未遇到的新任务。
- en: Seeking advice involves complex decision-making. The agent must estimate its
    own confidence in the current session, predict the potential value of the advice
    for future sessions, and consider the cost of human resources. The optimal trade-off
    is difficult to annotate manually but aligns well with our RL framework. Specifically,
    the present risk, future value, and cost of action can all be represented as RL
    rewards, allowing this skill to be trained as part of the policy model on an end-to-end
    basis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求建议涉及复杂的决策过程。代理需要评估自己在当前会话中的信心，预测建议对未来会话的潜在价值，并考虑人力资源的成本。最佳的权衡难以手动注释，但与我们的RL框架高度一致。具体而言，目前的风险、未来的价值和行动的成本都可以表示为RL奖励，从而允许将此技能作为策略模型的一部分进行端到端训练。
- en: 3 The ProductQA dataset
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 产品问答数据集
- en: We believe that product question answering in a real online shopping environment
    offers a comprehensive challenge for evaluating LLM agents. First, it demands
    expert knowledge about millions of products, including their technical specifications,
    usage in particular scenarios, and compatibility with other products. Second,
    answering some questions requires the use of tools, such as a product search tool.
    Third, the continuous emergence of new products necessitates the adaptability
    of the agent. This has motivated the creation of the ProductQA dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，真实在线购物环境中的产品问答为评估LLM代理提供了全面的挑战。首先，它要求对数百万种产品具有专家知识，包括它们的技术规格、在特定场景中的使用以及与其他产品的兼容性。其次，回答某些问题需要使用工具，例如产品搜索工具。第三，新产品的不断出现要求代理具有适应性。这促使了ProductQA数据集的创建。
- en: 'The ProductQA dataset consists of 26 QA tasks, each representing a distinct
    group of products within a specific category. Each group encompasses 17-20 products.
    We collected 20 groups for training and 6 for testing, allowing for assessing
    the agent’s adaptability to new tasks. We collected an average of 3,393 question-answer
    pairs for each product group. The questions within the same group are correlated,
    as knowledge from one answer may aid in addressing other questions. The dataset
    statistics are presented in Table [9](#A3.T9 "Table 9 ‣ Appendix C Tables ‣ AGILE:
    A Novel Framework of LLM Agents").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'ProductQA 数据集由 26 个 QA 任务组成，每个任务代表一个特定类别内的不同产品组。每个组包含 17-20 个产品。我们收集了 20 个组用于训练，6
    个组用于测试，从而评估代理对新任务的适应能力。我们为每个产品组收集了平均 3,393 对问答。相同组内的问题是相关的，因为一个答案中的知识可能有助于回答其他问题。数据集统计信息见表格 [9](#A3.T9
    "Table 9 ‣ Appendix C Tables ‣ AGILE: A Novel Framework of LLM Agents")。'
- en: 'The dataset is annotated by 20 professional annotators, each with at least
    a college degree, employed by a commercial data annotation company. We pay the
    company at market rates for professional annotation. See annotation guidelines
    in Appendix [E.2](#A5.SS2 "E.2 Annotation guidelines ‣ Appendix E Development
    of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents"). In addition,
    we release the code for the data pre-processing before human annotation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集由 20 位专业注释员注释，他们都具有至少大学学位，并由一家商业数据注释公司雇佣。我们按市场价格支付公司专业注释费。有关注释指南，请参见附录 [E.2](#A5.SS2
    "E.2 Annotation guidelines ‣ Appendix E Development of the ProductQA dataset ‣
    AGILE: A Novel Framework of LLM Agents")。此外，我们发布了在人工注释前的数据预处理代码。'
- en: 3.1 Product collection
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 产品收集
- en: 'We gather products from the Amazon Review Data [[18](#bib.bib18)], which includes
    product metadata as well as reviews. We initially filter the Amazon Review Data
    to retain only popular products with at least 100 reviews, then cluster them by
    category tags. From these clusters, we select 26 based on the size of the cluster,
    each defined as a *product group*. Subsequently, we sample products from each
    product group. See Appendix [E.1](#A5.SS1 "E.1 Product collection ‣ Appendix E
    Development of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents")
    for more details about product group and product selection.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从 Amazon Review Data [[18](#bib.bib18)] 收集产品数据，该数据包括产品元数据和评论。我们最初筛选 Amazon
    Review Data，只保留至少有 100 条评论的热门产品，然后按类别标签对它们进行聚类。从这些聚类中，我们根据聚类的规模选择了 26 个，每个被定义为
    *产品组*。随后，我们从每个产品组中抽样产品。有关产品组和产品选择的更多细节，请参见附录 [E.1](#A5.SS1 "E.1 Product collection
    ‣ Appendix E Development of the ProductQA dataset ‣ AGILE: A Novel Framework of
    LLM Agents")。'
- en: 'After the products are collected, annotators compile an information table for
    each product group. An example of such a table is presented in Table [2](#S3.T2
    "Table 2 ‣ 3.1 Product collection ‣ 3 The ProductQA dataset ‣ AGILE: A Novel Framework
    of LLM Agents"). To enhance the efficiency of the annotation process, we employ
    GPT-4 to extract as many product features as possible from the reviews. These
    features, together with the product metadata, are provided to the annotators for
    table creation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '在产品收集后，注释员为每个产品组编制信息表。这样的表格示例见表格 [2](#S3.T2 "Table 2 ‣ 3.1 Product collection
    ‣ 3 The ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents")。为了提高注释过程的效率，我们使用
    GPT-4 从评论中提取尽可能多的产品特征。这些特征与产品元数据一起提供给注释员用于表格创建。'
- en: 'Table 2: An example of an information table for the headphones group.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 耳机组信息表的示例。'
- en: '| Product ID | Title | Price | Brand | Headphone | Cable Type | Audio | Audio
    | … |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 产品 ID | 标题 | 价格 | 品牌 | 耳机类型 | 线缆类型 | 音频 | 音频 | … |'
- en: '| Type | Transmission | Output Mode |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 传输 | 输出模式 |'
- en: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | over-ear | bluetooth
    | kleer | stereo | … |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | 头戴式 | 蓝牙 | kleer |
    立体声 | … |'
- en: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | earbud | 3.5mm Jack | analog | bass
    boost | … |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | 入耳式 | 3.5mm 插孔 | 模拟 | 增强低音 | … |'
- en: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | earbud | bluetooth | bluetooth
    | stereo | … |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | 入耳式 | 蓝牙 | 蓝牙 | 立体声 | …
    |'
- en: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | over-ear | 3.5mm Jack | analog |
    stereo | … |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | 头戴式 | 3.5mm 插孔 | 模拟 | 立体声 | … |'
- en: '| … | … | … | … | … | … | … | … | … |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … | … | … | … | … |'
- en: 'Table 3: Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: ProductQA 中的 Fact-QA、Search-QA 和 Reasoning-QA 示例。'
- en: '| Type | Question | Long Answer | Short Answer |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 问题 | 长答案 | 短答案 |'
- en: '| Fact-QA | What is the size of the neodymium driver used in the JVC HA-EB75
    headphones? | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | 13.5 mm |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 事实问答 | JVC HA-EB75 耳机使用的钕驱动单元的尺寸是多少？ | JVC HA-EB75 耳机每个耳塞中都包含一个 13.5 mm 的钕驱动单元，这有助于增强音质。
    | 13.5 mm |'
- en: '| Search-QA | I’m an audiophile always on the move, so I need my music non-stop.
    Tell me, what’s the headphone with the longest playtime you have, either on-ear
    or in-ear? | I found a product that matches your criteria. ‘ABCShopUSA Wireless
    Earbuds True’ with asin: B00LJT2EPK | B00LJT2EPK |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 搜索问答 | 我是一个总在移动的音响爱好者，所以我需要音乐不断播放。告诉我，你们有什么耳机具有最长的播放时间，无论是耳罩式还是入耳式？ | 我找到了一款符合你要求的产品。‘ABCShopUSA
    无线耳塞’ ASIN: B00LJT2EPK | B00LJT2EPK |'
- en: '| Reasoning-QA | Will these headphones deliver comparable sound quality to
    wired alternatives when I am editing music? | No, these headphones may not suit
    your needs for music editing since they are wireless and can introduce audio compression
    and slight latency. Such issues can impact the precise listening experience crucial
    for professional audio editing tasks. | no |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 推理问答 | 当我在编辑音乐时，这些耳机的音质会和有线耳机相媲美吗？ | 不，这些耳机可能不适合你的音乐编辑需求，因为它们是无线的，可能会引入音频压缩和轻微的延迟。这些问题可能会影响专业音频编辑任务中对精确听音体验的要求。
    | no |'
- en: 3.2 QA collection
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 问答收集
- en: 'We identify three predominant types of questions in online shopping contexts:
    1) Fact-QA: questions concerning specific product details; 2) Search-QA: searches
    for product recommendations tailored to user preferences; 3) Reasoning-QA: questions
    whose answers require domain-specific reasoning, such as the implications of a
    product feature. Accordingly, we annotate question-answer pairs for these types.
    Each question is annotated with both a detailed paragraph-long answer and a concise
    short answer. The long answer should resemble a response from human customer service,
    while the short answer consists of a few words. We train the model to predict
    both answer types. The accuracy of the long answers is evaluated using GPT-4 (see
    Appendix [F](#A6 "Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM
    Agents") for the prompt); the short answers are assessed by exact match and are
    used for defining rewards for RL training.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在在线购物环境中识别出三种主要类型的问题：1) 事实问答：涉及具体产品细节的问题；2) 搜索问答：针对用户偏好的产品推荐；3) 推理问答：需要领域特定推理的问题，例如产品功能的影响。因此，我们对这些类型的问题-答案对进行标注。每个问题都配有详细的长答案和简短的短答案。长答案应类似于人工客服的回答，而短答案则由几个词组成。我们训练模型来预测这两种答案类型。长答案的准确性使用
    GPT-4 进行评估（见附录 [F](#A6 "附录 F 提示模板 ‣ AGILE: 一种新颖的 LLM 代理框架)"); 短答案则通过准确匹配进行评估，并用于定义
    RL 训练的奖励。'
- en: Fact-QA
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事实问答
- en: Fact-QAs are constructed from product reviews. For each product, we provide
    GPT-4 with a batch of 30 reviews, prompting it to generate 20 questions and their
    corresponding answers before moving on to the next batch. We encourage GPT-4 to
    create diverse questions. The results are then given to annotators to refine and
    finalize the question-answer pairs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 事实问答是从产品评价中构建的。对于每个产品，我们向 GPT-4 提供一批 30 条评价，要求其生成 20 个问题及其对应的答案，然后再转到下一批。我们鼓励
    GPT-4 创建多样化的问题。结果随后交给标注员进行完善和最终确定问题-答案对。
- en: Search-QA
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索问答
- en: Starting with an information table for a given product group, we generate random
    SQL expressions using a set of predefined rules. These expressions are then translated
    into natural language questions by GPT-4\. The answers are obtained by executing
    the SQL queries. Subsequently, human annotators thoroughly revise the QA pairs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定产品组的信息表开始，我们使用一组预定义的规则生成随机 SQL 表达式。这些表达式随后通过 GPT-4 转换成自然语言问题。通过执行 SQL 查询获取答案。随后，人类标注员会彻底修订问答对。
- en: Reasoning-QA
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理问答
- en: 'As the first step, we collect professional knowledge for each product group.
    To enhance efficiency, we utilize GPT-4 to generate candidate knowledge entries
    based on the technical specifications from the information table. These entries
    are then curated and refined by human annotators. Here is an example of a knowledge
    entry: *Motherboards with the ATX form factor are ideally suited for high-performance
    computing tasks and gaming, due to their ample expansion slots for graphics cards
    and other peripherals that boost computing capabilities.* Finally, annotators
    develop question-answer pairs from these knowledge entries.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为每个产品组收集专业知识。为了提高效率，我们利用 GPT-4 根据信息表中的技术规格生成候选知识条目。这些条目随后由人工标注员进行策划和精炼。以下是一个知识条目的示例：*具有
    ATX 形式因子的主板由于其充足的扩展槽，可以理想地用于高性能计算任务和游戏，因为这些扩展槽支持显卡和其他提升计算能力的外围设备。* 最后，标注员从这些知识条目中开发问答对。
- en: 4 Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental setting
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Dataset
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We evaluate our agent on two complex QA tasks: ProductQA and MedMCQA. MedMCQA [[22](#bib.bib22)]
    is a dataset for multiple-choice QA. It consists of questions from medical school
    entrance examinations, with 182,822 / 4,183 / 6,150 instances in the train/dev/test
    splits. We report results on the dev set without checkpoint selection.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个复杂的问答任务上评估了我们的代理：ProductQA 和 MedMCQA。MedMCQA [[22](#bib.bib22)] 是一个多选问答的数据集。它包含来自医学院入学考试的问题，训练/开发/测试分割中有
    182,822 / 4,183 / 6,150 个实例。我们报告了在开发集上的结果，没有进行检查点选择。
- en: Agent definition
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理定义
- en: 'Our agent can invoke functions defined in Table [1](#S2.T1 "Table 1 ‣ 2.1 RL
    formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Framework of LLM Agents"). In
    a typical workflow, the agent prompts the user for a new question at the session
    start, then retrieves memory to get relevant information. The memory can be initialized
    as empty (ProdcutQA) or with domain knowledge (QA pairs from MedMCQA training
    dataset). The agent can optionally use tools (e.g. product search in ProductQA)
    to get more information, then decide whether to predict an answer directly or
    seek human advice. If the agent seeks advice, it obtains a human answer (ground-truth
    answer in our setting). The agent then uses a reflection round to extract general
    knowledge from the human answer, writing both the human answer and the reflected
    knowledge to its memory. Finally, the agent submits an answer to the user. In
    our setting, submitting a correct answer incurs a $+1$.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的代理可以调用表格 [1](#S2.T1 "Table 1 ‣ 2.1 RL formulation of agent ‣ 2 Methods ‣
    AGILE: A Novel Framework of LLM Agents") 中定义的函数。在典型的工作流程中，代理在会话开始时提示用户提出新问题，然后检索记忆以获取相关信息。记忆可以初始化为空（ProductQA）或包含领域知识（来自
    MedMCQA 训练数据集的 QA 对）。代理可以选择使用工具（例如 ProductQA 中的产品搜索）获取更多信息，然后决定是直接预测答案还是寻求人类建议。如果代理寻求建议，它会获得一个人类答案（在我们的设置中为真实答案）。然后，代理利用反思轮次从人类答案中提取一般知识，将人类答案和反射知识写入记忆中。最后，代理向用户提交答案。在我们的设置中，提交正确答案会获得
    $+1$。'
- en: Training
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training consists of two stages. First, we construct trajectories from
    the training data and employ imitation learning to train the agent. Then we apply
    Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Session-level optimization algorithm
    ‣ AGILE: A Novel Framework of LLM Agents") for further optimization by reinforcement
    learning. See Appendix [B](#A2 "Appendix B Implementation details of AGILE ‣ AGILE:
    A Novel Framework of LLM Agents") for implementation details. The agent’s LLM
    is initialized from Vicuna-13b-1.5 for ProductQA and Meerkat-7b for MedMCQA. We
    fine-tune the model for 2 epochs with a learning rate of 1e-5 and a batch size
    of 64\. We implement PPO for 1 epoch with a learning rate of 1e-6 and a batch
    size of 64\. The training runs on NVIDIA-H800.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '训练包括两个阶段。首先，我们从训练数据中构建轨迹，并采用模仿学习来训练代理。然后，我们应用算法 [1](#alg1 "Algorithm 1 ‣ Appendix
    A Session-level optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")
    通过强化学习进行进一步优化。实现细节见附录 [B](#A2 "Appendix B Implementation details of AGILE ‣ AGILE:
    A Novel Framework of LLM Agents")。代理的 LLM 从 Vicuna-13b-1.5（用于 ProductQA）和 Meerkat-7b（用于
    MedMCQA）初始化。我们对模型进行 2 个周期的微调，学习率为 1e-5，批量大小为 64。我们实施 PPO 进行 1 个周期，学习率为 1e-6，批量大小为
    64。训练运行在 NVIDIA-H800 上。'
- en: Evaluation and baselines
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估和基准
- en: 'We report three metrics for the agent: (a) Advice rate: the rate of seeking
    human advice; (b) Accuracy: the rate of predicting the correct answer; (c) Total
    score: the average reward across all sessions, taking the advice rate and the
    accuracy both into account.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为代理报告了三个指标：（a）建议率：寻求人工建议的比例；（b）准确率：预测正确答案的比例；（c）总分：所有会话的平均奖励，综合考虑了建议率和准确率。
- en: 'We compare our agent against two types of baselines: 1) Prompting GPT-3.5 (gpt-3.5-turbo-0301)
    and GPT-4 (gpt-4-0613) [[21](#bib.bib21)] to directly answer the question, without
    working in an agent manner, noted as gpt3.5-prompt and gpt4-prompt. 2) Prompting
    GPT-3.5 and GPT-4 within the AGILE framework, noted as agile-gpt3.5-prompt and
    agile-gpt4-prompt. We carefully designed prompts for all baselines and they are
    shown in Appendix [F](#A6 "Appendix F Prompt templates ‣ AGILE: A Novel Framework
    of LLM Agents").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将我们的代理与两种基线进行比较：1）直接回答问题的 GPT-3.5（gpt-3.5-turbo-0301）和 GPT-4（gpt-4-0613）[[21](#bib.bib21)]，这两种方法未以代理的方式进行工作，分别记作
    gpt3.5-prompt 和 gpt4-prompt。2）在 AGILE 框架内提示 GPT-3.5 和 GPT-4，分别记作 agile-gpt3.5-prompt
    和 agile-gpt4-prompt。我们为所有基线精心设计了提示，并且这些提示显示在附录 [F](#A6 "Appendix F Prompt templates
    ‣ AGILE: A Novel Framework of LLM Agents") 中。'
- en: 'Table 4: Results on ProductQA. Here, X-prompt represents directly prompting
    model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents
    prompting or PPO training. We report results on short and long answers, respectively.
    The seeking advice cost is $c=0.3$. Results are average over six test tasks. See
    Table [10](#A3.T10 "Table 10 ‣ Appendix C Tables ‣ AGILE: A Novel Framework of
    LLM Agents") for individual product group performance.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：ProductQA 的结果。这里，X-prompt 表示直接提示模型 X；agile-X-Y 将模型 X 纳入 AGILE 框架，其中 Y 代表提示或
    PPO 训练。我们分别报告短答案和长答案的结果。寻求建议的成本为 $c=0.3$。结果是对六个测试任务的平均值。有关各个产品组的表现，请参见表 [10](#A3.T10
    "Table 10 ‣ Appendix C Tables ‣ AGILE: A Novel Framework of LLM Agents")。'
- en: '| Method | Advice Rate  $\downarrow$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ |'
- en: '| Short | Long | Short | Long |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 短 | 长 | 短 | 长 |'
- en: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
- en: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
- en: '| agile-vicuna-13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| agile-vicuna-13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
- en: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
- en: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
- en: '| agile-vic7b-ppo (ours) | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic7b-ppo（我们的） | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo（我们的） | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
- en: 'Table 5: Ablation studies for disabling reflection, memory, seeking advice,
    tool use, or RL training. Here, non-adapt-advice means that seeking advice is
    invoked for the first $K$ equals to the number of [SeekAdvice] performed by agile-vic13b-ppo.
    See Table [11](#A3.T11 "Table 11 ‣ Appendix C Tables ‣ AGILE: A Novel Framework
    of LLM Agents") for ablation results on individual product groups.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：禁用反思、记忆、寻求建议、工具使用或 RL 训练的消融研究。这里，非适应性建议意味着建议是在第一次 $K$ 等于 agile-vic13b-ppo
    执行的 [SeekAdvice] 数量时触发的。有关各个产品组的消融结果，请参见表 [11](#A3.T11 "Table 11 ‣ Appendix C
    Tables ‣ AGILE: A Novel Framework of LLM Agents")。'
- en: '| Method | Advice Rate $\downarrow$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ |'
- en: '| w/o Reflection | 0.270 | 0.852 | 0.771(-1.7%) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 无反思 | 0.270 | 0.852 | 0.771(-1.7%) |'
- en: '| w/o Memory | 0.407 | 0.876 | 0.754(-4.0%) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 无记忆 | 0.407 | 0.876 | 0.754(-4.0%) |'
- en: '| w/o Advice | 0.000 | 0.747 | 0.747(-5.0%) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 无建议 | 0.000 | 0.747 | 0.747(-5.0%) |'
- en: '| non-adapt-advice | 0.233 | 0.812 | 0.742(-5.7%) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 非适应性建议 | 0.233 | 0.812 | 0.742(-5.7%) |'
- en: '| w/o Tool-Use | 0.492 | 0.864 | 0.717(-9.3%) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 无工具使用 | 0.492 | 0.864 | 0.717(-9.3%) |'
- en: '| w/o RL | 0.256 | 0.843 | 0.766(-2.3%) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 无 RL | 0.256 | 0.843 | 0.766(-2.3%) |'
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.784 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo（我们的） | 0.233 | 0.854 | 0.784 |'
- en: 4.2 Results on ProductQA
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ProductQA 的结果
- en: 'As Table [4](#S4.T4 "Table 4 ‣ Evaluation and baselines ‣ 4.1 Experimental
    setting ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") shows, our
    AGILE agent outperforms all baselines on ProductQA. Notably, the average total
    score of agile-vic13b-ppo across six test groups shows a relative improvement
    of 9.2% in short answers and 5.0% in long answers to agile-gpt4-prompt where the
    seeking advice cost is added into the prompt. Concretely, agile-vic13b-ppo uses
    a comparable number of seeking advice to achieve 7.4% higher accuracy in short
    answers than agile-gpt4-prompt, and as Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Results
    on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") shows,
    this accuracy improvement is consistent across the whole trajectory. Our agile-vic7b-ppo
    agent also outperforms agile-gpt4-prompt in average total scores. Note that the
    GPT-4 agent knows the seeking advice cost from its prompt (see Figure [6](#A6.F6
    "Figure 6 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE:
    A Novel Framework of LLM Agents")).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [4](#S4.T4 "表 4 ‣ 评估和基线 ‣ 4.1 实验设置 ‣ 4 实验 ‣ AGILE: 一种新的 LLM 代理框架") 所示，我们的
    AGILE 代理在 ProductQA 上优于所有基线。值得注意的是，agile-vic13b-ppo 在六个测试组中的平均总分相对于 agile-gpt4-prompt
    在短答案中显示了 9.2% 的相对改进，在长答案中显示了 5.0% 的改进，其中寻求建议的成本已添加到提示中。具体而言，agile-vic13b-ppo 使用了与
    agile-gpt4-prompt 相当的寻求建议次数，以实现短答案中 7.4% 的更高准确率，正如图 [3](#S4.F3 "图 3 ‣ 4.2 ProductQA
    结果 ‣ 4 实验 ‣ AGILE: 一种新的 LLM 代理框架") 所示，这种准确率改进在整个轨迹中是一致的。我们的 agile-vic7b-ppo 代理在平均总分上也优于
    agile-gpt4-prompt。请注意，GPT-4 代理知道其提示中的寻求建议成本（见图 [6](#A6.F6 "图 6 ‣ MedMCQA 的提示模板
    ‣ 附录 F 提示模板 ‣ AGILE: 一种新的 LLM 代理框架")）。'
- en: 'We investigate the impact of varying the seeking advice cost. As shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Framework
    of LLM Agents"), when the cost decreases, both the advice rate and the accuracy
    increase, indicating greater utilization of human assistance. Specifically, with
    a high cost of 0.5, the advice rate is close to 0, and at a low cost of 0.1, the
    accuracy is close to 1\. This result demonstrates that by adjusting the cost and
    through RL training, we can effectively manage the trade-off between accuracy
    and human cost. For instance, the agent can achieve 94.1% accuracy on the Motherboards
    task with a seeking advice cost of $c=0.1$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '我们调查了变动寻求建议成本的影响。如图 [3](#S4.F3 "图 3 ‣ 4.2 ProductQA 结果 ‣ 4 实验 ‣ AGILE: 一种新的
    LLM 代理框架") 所示，当成本减少时，建议率和准确率都会增加，表明人类辅助的利用程度更高。具体来说，当成本为 0.5 时，建议率接近 0，而当成本为 0.1
    时，准确率接近 1。这一结果表明，通过调整成本和 RL 训练，我们可以有效地管理准确性和人力成本之间的权衡。例如，在寻求建议成本为 $c=0.1$ 时，代理可以在主板任务上达到
    94.1% 的准确率。'
- en: '![Refer to caption](img/11ebaea0130ede1f05fbfb7f80981544.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11ebaea0130ede1f05fbfb7f80981544.png)'
- en: 'Figure 2: Accuracy and advice rate over the following 200 sessions on ProductQA
    ($c=0.3$).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在 ProductQA 上的后续 200 次会话中的准确率和建议率 ($c=0.3$)。
- en: '![Refer to caption](img/c8876aba295eb5c94a265c610e02c322.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8876aba295eb5c94a265c610e02c322.png)'
- en: 'Figure 3: Advice rate, accuracy along with seeking advice cost $c$ on ProductQA.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：建议率、准确率以及在 ProductQA 上寻求建议的成本 $c$。
- en: Ablation study
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**消融研究**'
- en: 'We present ablation studies in Table [5](#S4.T5 "Table 5 ‣ Evaluation and baselines
    ‣ 4.1 Experimental setting ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents")
    to assess the contributions of individual agent components and the effects of
    RL training. The table indicates that disabling the option to seek advice (w/o
    Advice) leads to a 10.7% drop in accuracy and a 5.0% relative reduction in total
    score. Forcing the agent to seek advice at the initial part of the trajectory
    (Non-adapt Advice) causes a 4.2% decrease in accuracy, underscoring the value
    of adaptive decision-making. Removing reflection and memory capabilities (w/o
    Memory and w/o Reflection) both increase the frequency of advice-seeking, as the
    agent struggles to accumulate or leverage valuable knowledge, consequently decreasing
    the total score. Furthermore, disabling tool use (w/o Tool-Use) causes a substantial
    25.9% increase in the advice-seeking rate because the agent’s capabilities are
    diminished, making it more reliant on external advice. Lastly, RL training improves
    the relative total score by 2.3%, lowers the advice-seeking rate, and boosts accuracy,
    demonstrating that RL training effectively optimizes the policy.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[5](#S4.T5 "表 5 ‣ 评估和基准 ‣ 4.1 实验设置 ‣ 4 实验 ‣ AGILE：一种新型的LLM代理框架")中展示了消融研究，以评估单个代理组件的贡献和RL训练的效果。表格显示，禁用寻求建议选项（w/o
    Advice）会导致准确率下降10.7%和总分相对减少5.0%。在轨迹的初始部分强制代理寻求建议（Non-adapt Advice）会导致准确率下降4.2%，强调了自适应决策的重要性。去除反思和记忆能力（w/o
    Memory 和 w/o Reflection）都会增加寻求建议的频率，因为代理难以积累或利用有价值的知识，从而降低总分。此外，禁用工具使用（w/o Tool-Use）会导致寻求建议率显著增加25.9%，因为代理的能力被削弱，使其更加依赖外部建议。最后，RL训练使总分相对提高2.3%，降低了寻求建议率，提高了准确率，表明RL训练有效优化了策略。
- en: 'In Appendix [D](#A4 "Appendix D Case study ‣ AGILE: A Novel Framework of LLM
    Agents"), we present detailed examples of agile-vic13b-ppo illustrating how memory,
    tools, seeking advice, and reflection enhance the agent workflow.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录[D](#A4 "附录 D 案例研究 ‣ AGILE：一种新型的LLM代理框架")中，我们展示了agile-vic13b-ppo的详细示例，说明了记忆、工具、寻求建议和反思如何增强代理工作流。
- en: '![Refer to caption](img/318201b08ea44dac1475a1aafffcb6ad.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/318201b08ea44dac1475a1aafffcb6ad.png)'
- en: 'Figure 4: Advice rate over the following 200 sessions on ProductQA ($c=0.3$).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在ProductQA的接下来的200次会话中的建议率（$c=0.3$）。
- en: Trend of advice rate
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建议率的趋势
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ Ablation study ‣ 4.2 Results on ProductQA ‣ 4
    Experiments ‣ AGILE: A Novel Framework of LLM Agents") demonstrates a consistent
    decrease in the advice rate of agile-vic13b-ppo as more sessions are added to
    the trajectory. This decline can be attributed to the agent progressively accumulating
    knowledge and becoming more independent. Additionally, the figure illustrates
    that disabling RL training or reflection leads to a significant increase in the
    advice rate, underscoring the importance of RL training and reflection in reducing
    human costs.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S4.F4 "图 4 ‣ 消融研究 ‣ 4.2 ProductQA上的结果 ‣ 4 实验 ‣ AGILE：一种新型的LLM代理框架")展示了agile-vic13b-ppo在轨迹中增加更多会话时建议率的一致下降。这一下降可以归因于代理逐渐积累知识，变得更加独立。此外，图示还说明禁用RL训练或反思会导致建议率显著增加，强调了RL训练和反思在降低人力成本中的重要性。
- en: 4.3 Results on MedMCQA
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 MedMCQA上的结果
- en: 'Table 6: Results on the MedMCQA dev dataset. X-prompt represents directly prompting
    the model X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting, ablation studies or standard PPO training. The seeking
    advice cost is $c=0.4$.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：MedMCQA开发数据集上的结果。X-prompt表示直接对模型X进行提示；agile-X-Y表示在AGILE框架内整合模型X，其中Y表示提示、消融研究或标准PPO训练。寻求建议的成本为$c=0.4$。
- en: '| Method | Advice Rate $\downarrow$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ |'
- en: '| Meerkat-7b-prompt | - | 0.534 | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Meerkat-7b-prompt | - | 0.534 | - |'
- en: '| gpt3.5-prompt[[19](#bib.bib19)] | - | 0.501 | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| gpt3.5-prompt[[19](#bib.bib19)] | - | 0.501 | - |'
- en: '| gpt4-prompt[[19](#bib.bib19)] | - | 0.695 | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-prompt[[19](#bib.bib19)] | - | 0.695 | - |'
- en: '| gpt4-Medprompt[[20](#bib.bib20)] | - | 0.791 | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-Medprompt[[20](#bib.bib20)] | - | 0.791 | - |'
- en: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
- en: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
- en: '| agile-mek7b-w/o Reflection | 0.368 | 0.790 | 0.643 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o Reflection | 0.368 | 0.790 | 0.643 |'
- en: '| agile-mek7b-w/o Memory | 0.506 | 0.741 | 0.539 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o Memory | 0.506 | 0.741 | 0.539 |'
- en: '| agile-mek7b-w/o Advice | 0.000 | 0.620 | 0.620 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o Advice | 0.000 | 0.620 | 0.620 |'
- en: '| agile-mek7b-w/o RL | 0.322 | 0.837 | 0.708 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o RL | 0.322 | 0.837 | 0.708 |'
- en: '| agile-mek7b-ppo (ours) | 0.316 | 0.852 | 0.726 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-ppo（我们的） | 0.316 | 0.852 | 0.726 |'
- en: 'Our agile-mek7b-ppo agent, based on the smaller Meerkat-7b [[13](#bib.bib13)]
    model, reaches an accuracy of 85.2% with an advice rate of 31.6%. As Table [6](#S4.T6
    "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of
    LLM Agents") shows, this represents a 31.8% accuracy increase over the base model
    Meerkat-7b-prompt and a 6.1% increase over the state-of-the-art gpt4-Medprompt
    [[20](#bib.bib20)]. Table [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents") also shows that the ability to seek
    advice alone contributes a 23.2% accuracy gain, meaning that each instance of
    seeking advice corrects an average of 0.73 prediction errors. This indicates that
    PPO training effectively helps the agent identify its mistakes. For a fair comparison,
    we also evaluate agile-gpt3.5-prompt and agile-gpt4-prompt, which incorporate
    GPT-3.5 and GPT-4 within our AGILE framework. These agents also leverage advice-seeking
    to enhance accuracy, but without RL training, their total scores are lower than
    agile-mek7b-ppo. Finally, through ablation studies, we confirmed the essential
    roles of memory, reflection, seeking advice, and RL training in achieving high
    performance. Removing these components leads to a significant drop in total scores,
    detailed in Table [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的 agile-mek7b-ppo 代理基于较小的 Meerkat-7b [[13](#bib.bib13)] 模型，达到了 85.2% 的准确率，并且建议率为
    31.6%。如表格 [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE:
    A Novel Framework of LLM Agents") 所示，这相比于基础模型 Meerkat-7b-prompt 提高了 31.8%，相比于最先进的
    gpt4-Medprompt [[20](#bib.bib20)] 提高了 6.1%。表格 [6](#S4.T6 "Table 6 ‣ 4.3 Results
    on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") 还显示，单独寻求建议的能力贡献了
    23.2% 的准确率提升，这意味着每次寻求建议可以纠正平均 0.73 个预测错误。这表明 PPO 训练有效地帮助代理识别其错误。为了公平比较，我们还评估了结合
    GPT-3.5 和 GPT-4 的 agile-gpt3.5-prompt 和 agile-gpt4-prompt，这些代理也利用寻求建议来提高准确率，但没有
    RL 训练，它们的总分低于 agile-mek7b-ppo。最后，通过消融研究，我们确认了记忆、反思、寻求建议和 RL 训练在实现高性能中的关键作用。移除这些组件会导致总分显著下降，详细情况见表格
    [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel
    Framework of LLM Agents")。'
- en: 5 Related work
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 'Table 7: Related work on LLM agents. AGILE stands out as the pioneering work
    that trains the entire agent using reinforcement learning, incorporating proactive
    human advice-seeking.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：LLM 代理的相关工作。AGILE 作为开创性的工作，通过强化学习训练整个代理，并结合主动的人工建议寻求，脱颖而出。
- en: '| LLM Agent | LLM | SFT | RL | Memory | Tools | Reflection |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| LLM 代理 | LLM | SFT | RL | 记忆 | 工具 | 反思 |'
- en: '&#124; Proactive &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 主动 &#124;'
- en: '&#124; Human-agent &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类-代理 &#124;'
- en: '&#124; Interaction &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WebGPT [[17](#bib.bib17)] | GPT-3 | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| WebGPT [[17](#bib.bib17)] | GPT-3 | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ |'
- en: '| ReAct [[44](#bib.bib44)] | PaLM-540b | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [[44](#bib.bib44)] | PaLM-540b | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Reflexion [[34](#bib.bib34)] | GPT-3/3.5/4 | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [[34](#bib.bib34)] | GPT-3/3.5/4 | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| ChatDev [[25](#bib.bib25)] | ChatGPT-turbo-16k | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ChatDev [[25](#bib.bib25)] | ChatGPT-turbo-16k | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| RAP [[11](#bib.bib11)] | LLaMA-33b | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| RAP [[11](#bib.bib11)] | LLaMA-33b | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
- en: '| AutoAct [[27](#bib.bib27)] | LLaMA2-70b | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| AutoAct [[27](#bib.bib27)] | LLaMA2-70b | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| TPTU [[30](#bib.bib30)] | ChatGPT/InternLM | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| TPTU [[30](#bib.bib30)] | ChatGPT/InternLM | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| AGILE (Ours) | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| AGILE（我们的） | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: LLM agents
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 代理
- en: 'Large Language Models (LLMs) have demonstrated substantial capabilities in
    following instructions, reasoning, and planning. Numerous research works, as shown
    in Table [7](#S5.T7 "Table 7 ‣ 5 Related work ‣ AGILE: A Novel Framework of LLM
    Agents"), utilizing prompt engineering, have constructed remarkable LLM agents
    capable of autonomously resolving complex tasks across various environments [[23](#bib.bib23),
    [38](#bib.bib38), [1](#bib.bib1), [25](#bib.bib25), [3](#bib.bib3)]. Furthermore,
    extensive works identify key components in the design of LLM agents, including
    planning [[17](#bib.bib17), [33](#bib.bib33), [7](#bib.bib7), [27](#bib.bib27),
    [44](#bib.bib44), [30](#bib.bib30)], tool-use [[24](#bib.bib24), [41](#bib.bib41),
    [31](#bib.bib31)], and reflection [[34](#bib.bib34), [16](#bib.bib16)]. In this
    work, we enable the agent to utilize memory, tools and proactively learn from
    the environment. We then formulate the entire process within an RL framework so
    that all agent skills can be jointly optimized end-to-end.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）在遵循指令、推理和规划方面展示了显著的能力。许多研究工作，如表[7](#S5.T7 "Table 7 ‣ 5 Related
    work ‣ AGILE: A Novel Framework of LLM Agents")所示，利用提示工程，构建了能够在各种环境中自主解决复杂任务的出色LLM代理[[23](#bib.bib23),
    [38](#bib.bib38), [1](#bib.bib1), [25](#bib.bib25), [3](#bib.bib3)]。此外，大量研究确定了LLM代理设计中的关键组件，包括规划[[17](#bib.bib17),
    [33](#bib.bib33), [7](#bib.bib7), [27](#bib.bib27), [44](#bib.bib44), [30](#bib.bib30)]、工具使用[[24](#bib.bib24),
    [41](#bib.bib41), [31](#bib.bib31)]和反思[[34](#bib.bib34), [16](#bib.bib16)]。在这项工作中，我们使代理能够利用记忆、工具并主动从环境中学习。然后，我们将整个过程框定在一个RL框架内，以便所有代理技能可以端到端地联合优化。'
- en: Human-agent interaction
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工智能代理互动
- en: Although LLMs face practical challenges, such as hallucination [[46](#bib.bib46)]
    and a lack of long-tail knowledge [[12](#bib.bib12)], consulting human experts
    can help mitigate these issues. Several studies have incorporated human experts
    into agent workflows. For instance, [[45](#bib.bib45)] establishes a static pipeline
    in which an agent passively accepts advice from a superior LLM. [[4](#bib.bib4),
    [26](#bib.bib26)] train models to proactively ask questions via behavior cloning.
    However, these methods ignore the fact that the decision to seek advice must be
    based on the LLM’s own knowledge and capabilities [[47](#bib.bib47), [14](#bib.bib14),
    [10](#bib.bib10)]. [[29](#bib.bib29)] use a calibrated version of an LLM’s token
    probabilities as a confidence measure, yet token probabilities tend to be overconfident [[40](#bib.bib40)],
    and existing calibration methods don’t generalize well to our agent setting when
    the LLM makes multiple decisions in sequence.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs面临实际挑战，如幻觉[[46](#bib.bib46)]和缺乏长尾知识[[12](#bib.bib12)]，但咨询人类专家可以帮助缓解这些问题。一些研究已经将人类专家融入代理工作流。例如，[[45](#bib.bib45)]建立了一个静态管道，其中代理被动地接受来自上级LLM的建议。[[4](#bib.bib4),
    [26](#bib.bib26)]训练模型主动通过行为克隆提问。然而，这些方法忽略了寻求建议的决策必须基于LLM自身的知识和能力[[47](#bib.bib47),
    [14](#bib.bib14), [10](#bib.bib10)]。[[29](#bib.bib29)]使用LLM令牌概率的校准版本作为置信度衡量，但令牌概率往往过于自信[[40](#bib.bib40)]，而且现有的校准方法在LLM连续做出多个决策的代理设置中不具有良好的普遍性。
- en: LLM agent benchmarks
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理基准测试
- en: 'Several benchmarks have been designed to assess the capabilities of agents.
    For instance, the Webshop [[43](#bib.bib43)] and Mind2Web [[6](#bib.bib6)] datasets
    evaluate agents’ tool usage and planning abilities within a web environment. HotPotQA [[42](#bib.bib42)]
    and TriviaQA [[9](#bib.bib9)] focus on agents’ reasoning and tool usage for question
    answering. ALFWorld [[35](#bib.bib35)] examines planning and navigation skills,
    while ScienceWorld [[37](#bib.bib37)] provides an interactive text-based environment
    to evaluate agents’ scientific aptitude. As illustrated in Table [8](#S5.T8 "Table
    8 ‣ LLM agent benchmarks ‣ 5 Related work ‣ AGILE: A Novel Framework of LLM Agents"),
    despite these existing benchmarks, none comprehensively addresses all the core
    challenges of real-world agent applications, such as handling long-tail knowledge,
    human-agent interaction, long-term memory usage, tool usage, self-evaluation,
    and reflection. This motivated us to develop ProductQA.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '已设计了若干基准来评估代理的能力。例如，Webshop [[43](#bib.bib43)] 和 Mind2Web [[6](#bib.bib6)]
    数据集评估代理在网络环境中的工具使用和规划能力。HotPotQA [[42](#bib.bib42)] 和 TriviaQA [[9](#bib.bib9)]
    关注代理在问答中的推理和工具使用。ALFWorld [[35](#bib.bib35)] 检验规划和导航技能，而 ScienceWorld [[37](#bib.bib37)]
    提供了一个互动的基于文本的环境来评估代理的科学能力。如表[8](#S5.T8 "Table 8 ‣ LLM agent benchmarks ‣ 5 Related
    work ‣ AGILE: A Novel Framework of LLM Agents")所示，尽管存在这些基准，但没有一个全面解决了真实世界代理应用的所有核心挑战，如处理长尾知识、人机交互、长期记忆使用、工具使用、自我评估和反思。这促使我们开发了
    ProductQA。'
- en: 'Table 8: Benchmarks for evaluating LLM agents. ProductQA features long trajectories,
    tool use, long-term knowledge accumulation, and cross-task capabilities.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 用于评估 LLM 代理的基准。ProductQA 具有长轨迹、工具使用、长期知识积累和跨任务能力。'
- en: '| Datasets | Type | Fields | Size |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 领域 | 规模 |'
- en: '&#124; Long &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; Trajectory &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Tool &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具 &#124;'
- en: '&#124; Usage &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用 &#124;'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Long-term &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长期 &#124;'
- en: '&#124; Knowledge &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识 &#124;'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cross &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨任务 &#124;'
- en: '&#124; Task &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务 &#124;'
- en: '|'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Webshop [[43](#bib.bib43)] | Simulator | Web | 12,087 | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Webshop [[43](#bib.bib43)] | 模拟器 | 网络 | 12,087 | ✗ | ✗ | ✗ | ✗ |'
- en: '| Mind2Web [[6](#bib.bib6)] | Simulator | Web | 2,350 | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Mind2Web [[6](#bib.bib6)] | 模拟器 | 网络 | 2,350 | ✗ | ✗ | ✗ | ✓ |'
- en: '| ALFWorld [[35](#bib.bib35)] | Simulator | Navigation | 3,827 | ✗ | ✗ | ✗
    | ✓ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld [[35](#bib.bib35)] | 模拟器 | 导航 | 3,827 | ✗ | ✗ | ✗ | ✓ |'
- en: '| ScienceWorld [[37](#bib.bib37)] | Simulator | Science | 7,207 | ✗ | ✗ | ✗
    | ✗ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ScienceWorld [[37](#bib.bib37)] | 模拟器 | 科学 | 7,207 | ✗ | ✗ | ✗ | ✗ |'
- en: '| HotPotQA [[42](#bib.bib42)] | QA | Wikipedia | 112,779 | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| HotPotQA [[42](#bib.bib42)] | QA | 维基百科 | 112,779 | ✗ | ✓ | ✗ | ✗ |'
- en: '| TriviaQA [[9](#bib.bib9)] | QA | Web | 95,956 | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA [[9](#bib.bib9)] | QA | 网络 | 95,956 | ✗ | ✓ | ✓ | ✗ |'
- en: '| ProductQA (ours) | QA | E-commerce | 88,229 | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ProductQA (我们) | QA | 电子商务 | 88,229 | ✓ | ✓ | ✓ | ✓ |'
- en: 6 Conclusion and future work
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this work, we introduce a novel framework of LLM agents, called AGILE. First,
    the whole system of AGILE is trained end-to-end by reinforcement learning. Second,
    AGILE has the ability of seeking advice from external human experts. In addition,
    we develop a challenging dataset of complex QA, ProductQA, for comprehensive evaluation
    of an agent’s capabilities. Extensive experiments demonstrate that within our
    framework, an agent based on a smaller model after RL training can outperform
    GPT-4.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了一种新的 LLM 代理框架，称为 AGILE。首先，AGILE 的整个系统通过强化学习进行端到端训练。其次，AGILE 具备从外部人类专家那里寻求建议的能力。此外，我们开发了一个具有挑战性的复杂
    QA 数据集 ProductQA，用于全面评估代理的能力。大量实验表明，在我们的框架内，经过 RL 训练后，基于较小模型的代理可以超越 GPT-4。
- en: 'AGILE is a general agent framework and we can certainly consider multiple extensions
    of it. An agent can be equipped with more tools, such as multimodal perception,
    manipulations in physical environments, logical reasoning, among others. We posit
    that AGILE’s activities can be categorized into two distinct types: utilizing
    its LLM alone, and integrating the LLM with other tools. These two approaches
    conceptually align with the human cognitive processes known as System 1 and System
    2\. Furthermore, AGILE’s memory serves as a repository for the accumulation of
    experiences and knowledge, which is crucial for self-improvement. Consequently,
    AGILE offers an architecture for an very powerful agent that has the potential
    to attain human-level intelligence.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: AGILE 是一个通用代理框架，我们当然可以考虑对其进行多种扩展。一个代理可以配备更多工具，例如多模态感知、在物理环境中的操作、逻辑推理等。我们认为 AGILE
    的活动可以分为两种不同类型：单独利用其 LLM，以及将 LLM 与其他工具整合。这两种方法在概念上与被称为系统 1 和系统 2 的人类认知过程一致。此外，AGILE
    的记忆作为经验和知识的存储库，这对自我改进至关重要。因此，AGILE 提供了一种非常强大的代理架构，具有达到人类水平智能的潜力。
- en: AGILE also includes interactions between the agent and external human experts.
    The framework can be extended to allow interactions with humans or machine agents
    in various roles such as students or teachers, and in different formats such as
    debates or coordination. Furthermore, AGILE can be employed in multi-agent systems.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: AGILE 还包括代理与外部人类专家之间的互动。该框架可以扩展以允许与人类或机器代理进行各种角色的互动，如学生或教师，并且可以采用不同的格式，如辩论或协调。此外，AGILE
    可以用于多代理系统。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry
    tools. arXiv preprint arXiv:2304.05376, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    和 Philippe Schwaller。Chemcrow：用化学工具增强大型语言模型。arXiv 预印本 arXiv:2304.05376，2023。'
- en: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。《神经信息处理系统进展》，33:1877–1901，2020。'
- en: '[3] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation.
    arXiv preprint arXiv:2309.17288, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, 和 Yemin Shi。Autoagents：自动代理生成框架。arXiv 预印本 arXiv:2309.17288，2023。'
- en: '[4] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking
    before action: Gather information in embodied decision making with language models.
    arXiv preprint arXiv:2305.15695, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, 和 Jianyu Chen。行动前询问：在具身决策中使用语言模型收集信息。arXiv
    预印本 arXiv:2305.15695，2023。'
- en: '[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing。Vicuna：一个开源聊天机器人，令人印象深刻的 GPT-4，质量达到 90%* chatgpt，2023年3月。'
- en: '[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, 和 Yu Su。Mind2web：面向网络的通用代理。 《神经信息处理系统进展》，36，2024。'
- en: '[7] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    and Zhiting Hu. Reasoning with language model is planning with world model. arXiv
    preprint arXiv:2305.14992, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    和 Zhiting Hu。用语言模型推理即用世界模型规划。arXiv 预印本 arXiv:2305.14992，2023。'
- en: '[8] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
    Szolovits. What disease does this patient have? a large-scale open domain question
    answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, 和 Peter
    Szolovits。这个病人得了什么病？来自医学考试的大规模开放领域问答数据集。《应用科学》，11(14):6421，2021。'
- en: '[9] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa:
    A large scale distantly supervised challenge dataset for reading comprehension.
    arXiv preprint arXiv:1705.03551, 2017.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mandar Joshi, Eunsol Choi, Daniel S Weld, 和 Luke Zettlemoyer. TriviaQA:
    一种大规模远程监督的阅读理解挑战数据集。arXiv 预印本 arXiv:1705.03551, 2017。'
- en: '[10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson,
    et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221,
    2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson
    等。语言模型（大多数情况下）知道它们知道的东西。arXiv 预印本 arXiv:2207.05221, 2022。'
- en: '[11] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented
    planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610,
    2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, 和 Yang You. Rap: 具有上下文记忆的检索增强规划，用于多模态
    LLM 代理。arXiv 预印本 arXiv:2402.03610, 2024。'
- en: '[12] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.
    Large language models struggle to learn long-tail knowledge. In International
    Conference on Machine Learning, pages 15696–15707\. PMLR, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, 和 Colin Raffel.
    大型语言模型在学习长尾知识时的困难。发表于国际机器学习大会论文集，第15696–15707页。PMLR, 2023。'
- en: '[13] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, and Jaewoo Kang. Small language models
    learn enhanced reasoning skills from medical textbooks. arXiv preprint arXiv:2404.00376,
    2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, 和 Jaewoo Kang. 小型语言模型从医学教科书中学习增强的推理技能。arXiv
    预印本 arXiv:2404.00376, 2024。'
- en: '[14] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty:
    Linguistic invariances for uncertainty estimation in natural language generation.
    In The Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Lorenz Kuhn, Yarin Gal, 和 Sebastian Farquhar. 语义不确定性：用于自然语言生成的不确定性估计的语言不变性。发表于第十一届国际学习表征会议，2022。'
- en: '[15] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar 等。语言模型的整体评估。arXiv
    预印本 arXiv:2211.09110, 2022。'
- en: '[16] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine:
    Iterative refinement with self-feedback. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang 等。Self-refine:
    自我反馈的迭代精炼。神经信息处理系统进展, 36, 2024。'
- en: '[17] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
    Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint
    arXiv:2112.09332, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders 等。WebGPT:
    基于浏览器的问答辅助与人类反馈。arXiv 预印本 arXiv:2112.09332, 2021。'
- en: '[18] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations
    using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
    2019 conference on empirical methods in natural language processing and the 9th
    international joint conference on natural language processing (EMNLP-IJCNLP),
    pages 188–197, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jianmo Ni, Jiacheng Li, 和 Julian McAuley. 使用远程标注的评论和细粒度方面为推荐提供解释。发表于2019年自然语言处理实证方法会议和第九届国际自然语言处理联合会议
    (EMNLP-IJCNLP) 论文集，第188–197页, 2019。'
- en: '[19] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric
    Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375,
    2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, 和 Eric
    Horvitz. GPT-4 在医学挑战问题上的能力。arXiv 预印本 arXiv:2303.13375, 2023。'
- en: '[20] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo
    Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer
    McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White,
    and Eric Horvitz. Can generalist foundation models outcompete special-purpose
    tuning? case study in medicine, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo
    Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott
    Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris
    White 和 Eric Horvitz。通用基础模型能否超越专用调整？医学案例研究，2023年。'
- en: '[21] OpenAI. Gpt-4 technical report. ArXiv, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] OpenAI。GPT-4 技术报告。ArXiv，2023年。'
- en: '[22] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa:
    A large-scale multi-subject multi-choice dataset for medical domain question answering.
    In Conference on health, inference, and learning, pages 248–260\. PMLR, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ankit Pal, Logesh Kumar Umapathi 和 Malaikannan Sankarasubbu。Medmcqa：一个大规模多学科多选数据集，用于医疗领域问答。会议论文集《健康、推理与学习》,
    页码 248–260。PMLR，2022年。'
- en: '[23] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang 和 Michael S Bernstein。生成代理：人类行为的互动模拟体。在第36届年度 ACM 用户界面软件与技术研讨会论文集，页码
    1–22，2023年。'
- en: '[24] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla:
    Large language model connected with massive apis. arXiv preprint arXiv:2305.15334,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Shishir G Patil, Tianjun Zhang, Xin Wang 和 Joseph E Gonzalez。Gorilla：与海量
    API 连接的大型语言模型。arXiv 预印本 arXiv:2305.15334，2023年。'
- en: '[25] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu 和 Maosong Sun。用于软件开发的交流代理。arXiv 预印本 arXiv:2307.07924，2023年。'
- en: '[26] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
    Yankai Lin, Zhong Zhang, Zhiyuan Liu, and Maosong Sun. Tell me more! towards implicit
    user intention understanding of language model driven agents. arXiv preprint arXiv:2402.09205,
    2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
    Yankai Lin, Zhong Zhang, Zhiyuan Liu 和 Maosong Sun。告诉我更多！迈向语言模型驱动代理的隐性用户意图理解。arXiv
    预印本 arXiv:2402.09205，2024年。'
- en: '[27] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning
    from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv 和 Huajun Chen。Autoact：通过自我规划从零开始自动学习代理。arXiv
    预印本 arXiv:2401.05268，2024年。'
- en: '[28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using
    siamese bert-networks, 2019.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Nils Reimers 和 Iryna Gurevych。Sentence-bert：使用 Siamese BERT 网络的句子嵌入，2019年。'
- en: '[29] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu,
    Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that
    ask for help: Uncertainty alignment for large language model planners. arXiv preprint
    arXiv:2307.01928, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu,
    Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley 等。请求帮助的机器人：大型语言模型规划者的不确定性对齐。arXiv
    预印本 arXiv:2307.01928，2023年。'
- en: '[30] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and
    tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427,
    2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng 和 Rui Zhao。Tptu：基于大型语言模型的 AI 代理的任务规划与工具使用。arXiv
    预印本 arXiv:2308.03427，2023年。'
- en: '[31] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Eric Hambro, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom。Toolformer：语言模型可以自我学习使用工具。神经信息处理系统进展，36，2024年。'
- en: '[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford 和 Oleg Klimov。近端策略优化算法。arXiv
    预印本 arXiv:1707.06347，2017年。'
- en: '[33] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Yongliang Shen、Kaitao Song、Xu Tan、Dongsheng Li、Weiming Lu 和 Yueting Zhuang。Hugginggpt：使用
    ChatGPT 及其在 Hugging Face 的朋友解决 AI 任务。神经信息处理系统进展，36，2024 年。'
- en: '[34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and
    Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik Narasimhan 和 Shunyu
    Yao。Reflexion：具有语言强化学习的语言智能体。神经信息处理系统进展，36，2024 年。'
- en: '[35] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler,
    and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
    interactive learning. In International Conference on Learning Representations,
    2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Mohit Shridhar、Xingdi Yuan、Marc-Alexandre Cote、Yonatan Bisk、Adam Trischler
    和 Matthew Hausknecht。Alfworld：将文本与具身环境对齐以进行互动学习。在国际表示学习会议，2020 年。'
- en: '[36] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke Zhu、Linxi
    Fan 和 Anima Anandkumar。Voyager：一个具有大规模语言模型的开放式具身智能体。arXiv 预印本 arXiv:2305.16291，2023
    年。'
- en: '[37] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu.
    ScienceWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 11279–11298, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Ruoyao Wang、Peter Jansen、Marc-Alexandre Côté 和 Prithviraj Ammanabrolu。ScienceWorld：你的智能体比五年级学生更聪明吗？在
    Yoav Goldberg、Zornitsa Kozareva 和 Yue Zhang 主编的《2022 年自然语言处理经验方法会议论文集》中，第 11279–11298
    页，阿布扎比，阿联酋，2022 年 12 月。计算语言学协会。'
- en: '[38] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao
    Liang. Describe, explain, plan and select: Interactive planning with large language
    models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,
    2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Zihao Wang、Shaofei Cai、Guanzhou Chen、Anji Liu、Xiaojian Ma 和 Yitao Liang。描述、解释、计划和选择：大型语言模型的互动规划使开放世界多任务智能体成为可能。arXiv
    预印本 arXiv:2302.01560，2023 年。'
- en: '[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc
    V Le、Denny Zhou 等人。链式思维提示引发大型语言模型中的推理。神经信息处理系统进展，35:24824–24837，2022 年。'
- en: '[40] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and
    Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence
    elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Miao Xiong、Zhiyuan Hu、Xinyang Lu、Yifei Li、Jie Fu、Junxian He 和 Bryan Hooi。LLMs
    能否表达其不确定性？对 LLMs 中置信度引导的实证评估。arXiv 预印本 arXiv:2306.13063，2023 年。'
- en: '[41] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting
    chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381,
    2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Zhengyuan Yang、Linjie Li、Jianfeng Wang、Kevin Lin、Ehsan Azarnasab、Faisal
    Ahmed、Zicheng Liu、Ce Liu、Michael Zeng 和 Lijuan Wang。Mm-react：促使 ChatGPT 进行多模态推理和行动。arXiv
    预印本 arXiv:2303.11381，2023 年。'
- en: '[42] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen,
    Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse,
    explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Zhilin Yang、Peng Qi、Saizheng Zhang、Yoshua Bengio、William W Cohen、Ruslan
    Salakhutdinov 和 Christopher D Manning。Hotpotqa：一个用于多样化、可解释的多跳问题回答的数据集。arXiv 预印本
    arXiv:1809.09600，2018 年。'
- en: '[43] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards
    scalable real-world web interaction with grounded language agents. Advances in
    Neural Information Processing Systems, 35:20744–20757, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Shunyu Yao、Howard Chen、John Yang 和 Karthik Narasimhan。Webshop：朝着可扩展的现实世界网页互动迈进，借助基础语言智能体。神经信息处理系统进展，35:20744–20757，2022
    年。'
- en: '[44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv
    preprint arXiv:2210.03629, 2022.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik Narasimhan
    和 Yuan Cao。React：在语言模型中协同推理与行动。arXiv 预印本 arXiv:2210.03629，2022 年。'
- en: '[45] Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    arXiv preprint arXiv:2305.17878, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    arXiv preprint arXiv:2305.17878, 2023.'
- en: '[46] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
    Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:
    a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
    Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:
    a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
- en: '[47] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey
    area: Expressions of overconfidence and uncertainty in language models. arXiv
    e-prints, pages arXiv–2302, 2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey
    area: Expressions of overconfidence and uncertainty in language models. arXiv
    e-prints, pages arXiv–2302, 2023.'
- en: Appendix
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: \startcontents
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \startcontents
- en: '[sections] \printcontents[sections]l1'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[sections] \printcontents[sections]l1'
- en: Appendix A Session-level optimization algorithm
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 会话级优化算法
- en: Assume that the entire trajectory $\tau$ is the memory before the session starts.
    In this section, we will explain how to transform a trajectory-level RL optimization
    algorithm into a session-level RL optimization algorithm.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设整个轨迹$\tau$是会话开始前的记忆。在本节中，我们将解释如何将轨迹级RL优化算法转换为会话级RL优化算法。
- en: 'Let $r(\tau)$. The optimization objective is to maximize the following expectation:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 设$r(\tau)$。优化目标是最大化以下期望：
- en: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
- en: For an arbitrary session index $i$, respectively. Accordingly, we have
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意会话索引$i$，分别。相应地，我们有
- en: '|  | $\displaystyle R(\theta)$ |  | (2) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(\theta)$ |  | (2) |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: 'Here, $\mathcal{S}_{i}$, indicating the expected total reward the agent expects
    to receive in the future. Averaging over all session indices, Eq. ([2](#A1.E2
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Framework
    of LLM Agents")) gives:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '这里，$\mathcal{S}_{i}$表示智能体期望在未来获得的总奖励。对所有会话索引取平均，公式 ([2](#A1.E2 "附录 A 会话级优化算法
    ‣ AGILE: 一种新的 LLM 代理框架"))给出：'
- en: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (3) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot|\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (3) |'
- en: 'In Eq. ([3](#A1.E3 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents")), the parameter $\theta$ only appears in the
    session-level expectation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '在公式 ([3](#A1.E3 "附录 A 会话级优化算法 ‣ AGILE: 一种新的 LLM 代理框架"))中，参数$\theta$只出现在会话级期望中：'
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (4) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta|\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot|\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (4) |'
- en: '$R(\theta|\theta_{k})$. If we employ an iterative optimization procedure:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: $R(\theta|\theta_{k})$。如果我们采用迭代优化过程：
- en: '1.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Initialize $\theta_{0}$ from a reference policy (obtained through SFT).
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从参考策略（通过SFT获得）初始化$\theta_{0}$。
- en: '2.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For $k=0,1,2,\cdots$.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于$k=0,1,2,\cdots$。
- en: Then $\theta$ will converge to an (at least locally) optimal policy.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然后$\theta$将收敛到一个（至少局部）最优策略。
- en: Now we are ready to illustrate why the optimization of $R(\theta|\theta_{k})$
    can be solved at the session level. Notice that
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备说明为什么$R(\theta|\theta_{k})$的优化可以在会话级别上解决。请注意
- en: '|  | $\displaystyle R(\theta&#124;\theta_{k})$ |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(\theta|\theta_{k})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(\tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(\tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
- en: 'On the right-hand side, the first term involves two sampling steps. The first
    step samples $\tau_{1:i-1}\sim\pi_{\theta_{k}}$. As a result, if we define a *proxy
    reward*:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，第一个项涉及两个采样步骤。第一个步骤采样 $\tau_{1:i-1}\sim\pi_{\theta_{k}}$。因此，如果我们定义一个 *代理奖励*：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: Then, we have
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{constant}.$
    |  | (6) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{constant}.$
    |  | (6) |'
- en: 'By Eq. ([6](#A1.E6 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents")), $R(\theta|\theta_{k})$.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '根据 Eq. ([6](#A1.E6 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents"))，$R(\theta|\theta_{k})$。'
- en: Algorithm 1 Session-level optimization
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 会话级优化
- en: '1:Initialize $\theta_{0}$ by maximizing Eq. ([6](#A1.E6 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")).12:end for'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化 $\theta_{0}$ 通过最大化 Eq. ([6](#A1.E6 "In Appendix A Session-level optimization
    algorithm ‣ AGILE: A Novel Framework of LLM Agents"))。12: 结束 for'
- en: 'Finally, we present the session-level optimization algorithm as Algorithm [1](#alg1
    "Algorithm 1 ‣ Appendix A Session-level optimization algorithm ‣ AGILE: A Novel
    Framework of LLM Agents"). In this algorithm, the state advantage function is
    the only component that concerns inter-session correlation. While the algorithm
    is iterative, we anticipate that in practice, the outer loop will require only
    a few iterations to converge.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们将会话级优化算法展示为算法 [1](#alg1 "Algorithm 1 ‣ Appendix A Session-level optimization
    algorithm ‣ AGILE: A Novel Framework of LLM Agents")。在这个算法中，状态优势函数是唯一涉及会话间相关性的组件。尽管该算法是迭代的，但我们预计在实践中，外循环只需少量迭代即可收敛。'
- en: Appendix B Implementation details of AGILE
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B AGILE 的实现细节
- en: B.1 ProductQA
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 ProductQA
- en: Implementation of [GetQuestion]
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[GetQuestion] 的实现'
- en: This function prompts the user for a new question and appends it to the LLM
    context. Every question is raised for a specific product, thus it has an associated
    product ID. Based on this ID, the function also appends the product information
    table’s schema and the product metadata to the context.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数提示用户提出新问题，并将其附加到 LLM 上下文中。每个问题都是针对特定产品提出的，因此它具有相关的产品 ID。根据这个 ID，该函数还将产品信息表的模式和产品元数据附加到上下文中。
- en: Implementation of [RetrieveMemory]
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[RetrieveMemory] 的实现'
- en: This function employs the provided question as a query to retrieve the most
    relevant historical QA pair and the most relevant knowledge entry from the agent’s
    memory. To safeguard sensitive data from sellers, the agent is restricted to accessing
    QA records exclusively for the queried product from historical interactions. However,
    it is permitted to retrieve general knowledge from the whole trajectory since
    this information is not seller-specific. We utilize an embedding-based retrieval
    method, specifically employing the all-MiniLM-L6-v2 model [[28](#bib.bib28)] as
    the embedding model.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使用提供的问题作为查询，从代理的记忆中检索最相关的历史 QA 对和最相关的知识条目。为了保护卖方的敏感数据，代理只能访问历史交互中针对查询产品的
    QA 记录。然而，它可以从整个轨迹中检索一般知识，因为这些信息不是特定于卖方的。我们使用基于嵌入的检索方法，特别是采用 all-MiniLM-L6-v2 模型
    [[28](#bib.bib28)] 作为嵌入模型。
- en: Implementation of [SearchProdcut]
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[SearchProdcut] 的实现'
- en: This function utilizes the LLM to predict a SQL query based on the context,
    and then invoke a MySQL execution engine. It appends the result to the LLM context.
    If there is an execution error, then the error is appended to the context too.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数利用 LLM 基于上下文预测 SQL 查询，然后调用 MySQL 执行引擎。它将结果附加到 LLM 上下文中。如果出现执行错误，则错误也会附加到上下文中。
- en: Implementation of [SeekAdvice]
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[SeekAdvice] 的实现'
- en: This requests for human expert advice and append it to the LLM context. In our
    implementation, the human expert simply returns the ground truth long answer from
    the ProductQA dataset.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这请求人类专家的建议，并将其附加到LLM上下文中。在我们的实现中，人类专家只是从ProductQA数据集中返回真实答案。
- en: Implementation of [PredictAnswer]
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[PredictAnswer]的实现'
- en: This function passes control to the LLM to continue generating a long answer
    and a short answer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能将控制权交给LLM，以继续生成长答案和短答案。
- en: Implementation of [Reflection]
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[Reflection]的实现'
- en: This function passes control to the LLM to continue generating a reflection
    result.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能将控制权交给LLM，以继续生成反思结果。
- en: Training Data Generation
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: 'We generate training data on a session-by-session basis, where each session
    consists of a QA pair. A session begins with an initial memory, consisting of
    historical QA pairs and knowledge entries accumulated from previous sessions.
    Recall that the [RetrieveMemory] function retrieves only the most relevant QA
    pair and knowledge entry per session. Thus, in constructing training memories,
    it suffices to put the retrieved QA pair and the retrieved knowledge entry into
    the memory. We select them in the following stochastic way: the retrieved QA pair
    can be the most relevant QA pair from the training set, or a random QA pair, or
    omitted entirely; similarly for the retrieved knowledge entry.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按会话生成训练数据，每个会话包含一个QA对。会话以一个初始记忆开始，包括历史QA对和从之前会话中积累的知识条目。请记住，[RetrieveMemory]功能每次仅检索最相关的QA对和知识条目。因此，在构建训练记忆时，只需将检索到的QA对和知识条目放入记忆中即可。我们以以下随机方式选择它们：检索到的QA对可以是来自训练集的最相关QA对，或随机QA对，或完全省略；检索到的知识条目也类似。
- en: 'Based on the initial memory, we generate trajectories by following the agent
    workflow detailed in Section [4.1](#S4.SS1 "4.1 Experimental setting ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents"). Each trajectory begins with [GetUserQuestion]
    and [RetrieveMemory]. For QAs classified as Search-QA, a [SearchProduct] function
    is appended, followed by the corresponding SQL query and its execution result.
    For other QA types, if an associated knowledge entry exists and is successfully
    retrieved, the trajectory will extend with a [PredictAnswer] call with the ground
    truth answer as its result. If the knowledge entry is not retrieved or is absent,
    we use GPT-4 to evaluate whether the question can be answered with the available
    context. If affirmative, a [PredictAnswer] with the ground truth answer is appended.
    Otherwise, the trajectory extends with a [SeekAdvice] call with the ground truth
    answer as the advice, and a [Reflection] call, where the reflection result is
    the knowledge entry if it exists, or "no information" if not. Then the reflection
    result is appended to the memory via [UpdateMemory]. Finally, the trajectory is
    concluded by [SubmitAnswer].'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '基于初始记忆，我们通过遵循第[4.1节](#S4.SS1 "4.1 Experimental setting ‣ 4 Experiments ‣ AGILE:
    A Novel Framework of LLM Agents")中详细描述的代理工作流程生成轨迹。每个轨迹以[GetUserQuestion]和[RetrieveMemory]开始。对于分类为Search-QA的问答，会附加一个[SearchProduct]功能，然后是相应的SQL查询及其执行结果。对于其他QA类型，如果存在相关知识条目并且成功检索，轨迹将扩展为一个[PredictAnswer]调用，其结果为真实答案。如果知识条目未被检索或不存在，我们将使用GPT-4来评估是否可以用现有的上下文回答问题。如果可以，则附加一个带有真实答案的[PredictAnswer]。否则，轨迹将扩展为一个[SeekAdvice]调用，其中真实答案作为建议，并且进行一个[Reflection]调用，其中反思结果是知识条目（如果存在），否则为“无信息”。然后，反思结果通过[UpdateMemory]附加到记忆中。最后，轨迹由[SubmitAnswer]结束。'
- en: In this way, we constructed 55,772 session-level trajectories in total, from
    6 training tasks in ProductQA. This data is used for imitation learning. In PPO
    training, we reuse the initial memory data, while the session-level trajectories
    are generated by the model itself.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们总共构建了55,772个会话级轨迹，来自ProductQA中的6个训练任务。这些数据用于模仿学习。在PPO训练中，我们重用初始记忆数据，而会话级轨迹由模型自身生成。
- en: B.2 MedMCQA
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 MedMCQA
- en: 'For MedMCQA, the memory is initialized with all QA pairs from the training
    set, simulating that the agent has processed the training set before reaching
    the test set. We also add a knowledge entry for each QA pair, obtained through
    GPT-4 reflection (see Figure [11](#A6.F11 "Figure 11 ‣ Prompt templates for MedMCQA
    ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents") for the
    prompt).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 MedMCQA，内存初始化时包含了训练集中的所有 QA 对，模拟代理在到达测试集之前已经处理过训练集。我们还为每个 QA 对添加了一个知识条目，这些条目是通过
    GPT-4 反思获得的（见图 [11](#A6.F11 "图 11 ‣ MedMCQA 的提示模板 ‣ 附录 F 提示模板 ‣ AGILE: 一种新颖的大型语言模型代理框架")
    以获取提示）。'
- en: Training data generation
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: We sample a subset of training data from MedMCQA to construct session-level
    trajectories. Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].
    The [RetrieveMemory] function retrieves the five most relevant QA pairs and pieces
    of knowledge from the initial memory, using the same embedding similarity search
    method employed in ProductQA. Then, we prompt GPT-4 to predict an answer with
    chain-of-thought reasoning. If the GPT-4 answer is correct, we append a [PredictAnswer]
    call, the GPT-4 chain-of-thought, and the ground-truth answer to the trajectory.
    If the GPT-4 answer is wrong, which suggests that the question is hard, we append
    a [SeekAdvice] call with the ground-truth answer, followed by a [Reflection] call
    with the reflection result generated by GPT-4\. Then the reflection result is
    appended to the memory via [UpdateMemory]. Finally, the trajectory is concluded
    by [SubmitAnswer]. In this way, we obtain 23,015 session-level trajectories in
    total.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 MedMCQA 中抽取了一部分训练数据来构建会话级轨迹。每个轨迹从 [GetUserQuestion] 和 [RetrieveMemory] 开始。[RetrieveMemory]
    函数从初始内存中检索五个最相关的 QA 对和知识条目，使用与 ProductQA 中相同的嵌入相似度搜索方法。然后，我们提示 GPT-4 通过链式思维推理来预测答案。如果
    GPT-4 的答案正确，我们将 [PredictAnswer] 调用、GPT-4 的链式思维和真实答案附加到轨迹中。如果 GPT-4 的答案错误，这表明问题较难，我们将
    [SeekAdvice] 调用和真实答案附加，然后跟随 [Reflection] 调用及 GPT-4 生成的反思结果。随后，反思结果通过 [UpdateMemory]
    被附加到内存中。最后，轨迹通过 [SubmitAnswer] 结束。这样，我们总共获得了 23,015 个会话级轨迹。
- en: B.3 Defining proxy reward for RL
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 为 RL 定义代理奖励
- en: 'In the question-answering tasks, sessions are not independent. Actions taken
    in earlier sessions can influence memory, creating lasting effects on subsequent
    sessions. As illustrated in Equation ([5](#A1.E5 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")), the term $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})$.
    Hence, we use the following heuristic definition,'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '在问答任务中，会话不是独立的。早期会话中采取的行动可以影响记忆，造成对后续会话的持久影响。如公式 ([5](#A1.E5 "在附录 A 会话级优化算法
    ‣ AGILE: 一种新颖的大型语言模型代理框架")) 所示，术语 $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})$。因此，我们使用以下启发式定义，'
- en: '|  | <math id="A2.E7.m1.3" class="ltx_Math" alttext="A_{i}=\beta\frac{\mathbbm{I}(N_{i+1:n}(q_{i})></math>
    |  | (7) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="A2.E7.m1.3" class="ltx_Math" alttext="A_{i}=\beta\frac{\mathbbm{I}(N_{i+1:n}(q_{i})></math>
    |  | (7) |'
- en: where $q_{i}$ by default.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q_{i}$ 默认值。
- en: Appendix C Tables
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 表格
- en: 'Table 9: Statistics of the ProductQA dataset. # Products indicates the number
    of products within each group. # Fact-QA, # Search-QA and # Reasoning-QA display
    the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9：ProductQA 数据集的统计信息。# 产品表示每组中的产品数量。# Fact-QA、# Search-QA 和 # Reasoning-QA
    显示了分别归类为 Fact-QA、Search-QA 和 Reasoning-QA 的 QA 对数量。'
- en: '| Groups | # Products | # Fact-QA | # Search-QA | # Reasoning-QA | Total |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | # 产品 | # Fact-QA | # Search-QA | # Reasoning-QA | 总计 |'
- en: '| Train | Blades | 20 | 2,147 | 769 | 631 | 3,547 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 火车 | 刀片 | 20 | 2,147 | 769 | 631 | 3,547 |'
- en: '| Headlight Bulbs | 20 | 1,767 | 644 | 463 | 2,874 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 车灯泡 | 20 | 1,767 | 644 | 463 | 2,874 |'
- en: '| Cell Phones | 20 | 1,636 | 761 | 374 | 2,771 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 手机 | 20 | 1,636 | 761 | 374 | 2,771 |'
- en: '| Portable Power Banks | 20 | 3,344 | 673 | 500 | 4,517 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 便携式充电宝 | 20 | 3,344 | 673 | 500 | 4,517 |'
- en: '| Dresses | 20 | 2,287 | 738 | 263 | 3,288 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 连衣裙 | 20 | 2,287 | 738 | 263 | 3,288 |'
- en: '| Everyday Bras | 20 | 1,942 | 684 | 336 | 2,962 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 日常胸罩 | 20 | 1,942 | 684 | 336 | 2,962 |'
- en: '| Wrist Watches | 20 | 2,169 | 757 | 389 | 3,315 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 手表 | 20 | 2,169 | 757 | 389 | 3,315 |'
- en: '| Blu-ray Players | 20 | 1,630 | 688 | 572 | 2,890 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 蓝光播放器 | 20 | 1,630 | 688 | 572 | 2,890 |'
- en: '| Camera Lenses | 20 | 1,859 | 769 | 1,025 | 3,653 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 相机镜头 | 20 | 1,859 | 769 | 1,025 | 3,653 |'
- en: '| Headphones | 20 | 5,432 | 766 | 583 | 6,781 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 耳机 | 20 | 5,432 | 766 | 583 | 6,781 |'
- en: '| Mice | 20 | 5,653 | 490 | 294 | 6,437 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 鼠标 | 20 | 5,653 | 490 | 294 | 6,437 |'
- en: '| Point & Shoot Digital Cameras | 20 | 1,696 | 722 | 565 | 2,983 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 傻瓜相机 | 20 | 1,696 | 722 | 565 | 2,983 |'
- en: '| Coffee Machines | 20 | 4,184 | 681 | 638 | 5,503 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 咖啡机 | 20 | 4,184 | 681 | 638 | 5,503 |'
- en: '| Digital Scales | 20 | 2,724 | 391 | 682 | 3,797 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 数字秤 | 20 | 2,724 | 391 | 682 | 3,797 |'
- en: '| Space Heaters | 20 | 2,283 | 674 | 498 | 3,455 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 电暖器 | 20 | 2,283 | 674 | 498 | 3,455 |'
- en: '| Printers | 20 | 1,431 | 760 | 489 | 2,680 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 打印机 | 20 | 1,431 | 760 | 489 | 2,680 |'
- en: '| Litter | 20 | 1,860 | 753 | 507 | 3,120 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾 | 20 | 1,860 | 753 | 507 | 3,120 |'
- en: '| Grips | 20 | 1,771 | 713 | 413 | 2,897 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 护柄 | 20 | 1,771 | 713 | 413 | 2,897 |'
- en: '| Gun Holsters | 20 | 1,679 | 94 | 1,362 | 3,135 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 枪套 | 20 | 1,679 | 94 | 1,362 | 3,135 |'
- en: '| Handheld Flashlights | 20 | 2,009 | 768 | 482 | 3,259 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 手持手电筒 | 20 | 2,009 | 768 | 482 | 3,259 |'
- en: '| Total | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
- en: '| Test | Leggings | 20 | 969 | 743 | 527 | 2,239 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 打底裤 | 20 | 969 | 743 | 527 | 2,239 |'
- en: '| Camera Cases | 20 | 975 | 706 | 898 | 2,579 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 相机包 | 20 | 975 | 706 | 898 | 2,579 |'
- en: '| Motherboards | 20 | 989 | 736 | 826 | 2,551 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 20 | 989 | 736 | 826 | 2,551 |'
- en: '| All Pans | 20 | 973 | 747 | 275 | 1,995 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 所有锅具 | 20 | 973 | 747 | 275 | 1,995 |'
- en: '| Rollerball Pens | 20 | 967 | 760 | 603 | 2,330 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 20 | 967 | 760 | 603 | 2,330 |'
- en: '| Rifle Scopes | 17 | 979 | 714 | 978 | 2,671 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 17 | 979 | 714 | 978 | 2,671 |'
- en: '| Total | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
- en: 'Table 10: Detail performance of our methods and other baselines on six test
    product groups of ProductQA. X-prompt represents directly prompting the model
    X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting or PPO training. The Short and Long stand for the
    results evaluated on short answers and long answers, respectively. The seeking
    advice cost is $c=0.3$. The best total scores are highlighted in bold.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：我们的方法和其他基准在 ProductQA 六个测试产品组上的详细表现。X-prompt 表示直接提示模型 X；agile-X-Y 表示在 AGILE
    框架中结合模型 X，其中 Y 表示提示或 PPO 训练。Short 和 Long 分别代表短答案和长答案的评估结果。寻求建议的成本是 $c=0.3$。最佳总分用粗体突出显示。
- en: '| Group | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
- en: '| prompt | prompt | 13b-prompt | prompt | prompt | ppo(ours) | ppo(ours) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 提示 | 13b-提示 | 提示 | 提示 | ppo（我们的） | ppo（我们的） |'
- en: '| Short | Long | Short | Long | Short | Long | Short | Long | Short | Long
    | Short | Long | Short | Long |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 |'
- en: '| Camera Cases | Advice Rate $\downarrow$ | - | - | - | - | 0.182 | 0.182 |
    0.313 | 0.313 | 0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 相机包 | 建议率 $\downarrow$ | - | - | - | - | 0.182 | 0.182 | 0.313 | 0.313 |
    0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
- en: '| Accuracy $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537
    | 0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537 |
    0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 |
    0.722 | 0.738 | 0.758 | 0.716 | 0.781 | 0.762 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 | 0.722 | 0.738
    | 0.758 | 0.716 | 0.781 | 0.762 |'
- en: '| Leggings | Advice Rate $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359
    | 0.359 | 0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359 | 0.359 |
    0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
- en: '| Accuracy $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497
    | 0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497 |
    0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 |
    0.706 | 0.730 | 0.777 | 0.774 | 0.801 | 0.810 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 | 0.706 | 0.730
    | 0.777 | 0.774 | 0.801 | 0.810 |'
- en: '| All Pans | Advice Rate $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336
    | 0.336 | 0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 所有锅具 | 建议率 $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336 | 0.336 |
    0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
- en: '| Accuracy $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506
    | 0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506 |
    0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 |
    0.718 | 0.738 | 0.788 | 0.776 | 0.800 | 0.803 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 | 0.718 | 0.738
    | 0.788 | 0.776 | 0.800 | 0.803 |'
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | - | - | - | - | 0.130 | 0.130
    | 0.333 | 0.333 | 0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 $\downarrow$ | - | - | - | - | 0.130 | 0.130 | 0.333 | 0.333 |
    0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
- en: '| Accuracy $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482
    | 0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482 |
    0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 |
    0.698 | 0.739 | 0.727 | 0.720 | 0.752 | 0.760 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 | 0.698 | 0.739
    | 0.727 | 0.720 | 0.752 | 0.760 |'
- en: '| Mother- boards | Advice Rate $\downarrow$ | - | - | - | - | 0.214 | 0.214
    | 0.303 | 0.303 | 0.225 | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 $\downarrow$ | - | - | - | - | 0.214 | 0.214 | 0.303 | 0.303 | 0.225
    | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
- en: '| Accuracy $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525
    | 0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525 |
    0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 |
    0.747 | 0.788 | 0.786 | 0.782 | 0.806 | 0.812 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 | 0.747 | 0.788
    | 0.786 | 0.782 | 0.806 | 0.812 |'
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | - | - | - | - | 0.197 | 0.197 |
    0.293 | 0.293 | 0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 狙击镜 | 建议率 $\downarrow$ | - | - | - | - | 0.197 | 0.197 | 0.293 | 0.293 |
    0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
- en: '| Accuracy $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502
    | 0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502 |
    0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 |
    0.711 | 0.747 | 0.752 | 0.710 | 0.763 | 0.757 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 | 0.711 | 0.747
    | 0.752 | 0.710 | 0.763 | 0.757 |'
- en: '| Average | Advice Rate $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323
    | 0.323 | 0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323 | 0.323 |
    0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
- en: '| Accuracy $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508
    | 0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508 |
    0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 |
    0.718 | 0.747 | 0.764 | 0.746 | 0.784 | 0.784 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 | 0.718 | 0.747
    | 0.764 | 0.746 | 0.784 | 0.784 |'
- en: 'Table 11: Ablation study on ProductQA test tasks. w/o Reflection represents
    removing the reflection function. w/o Memory represents prohibiting memory component.
    w/o Advice represents removing the seeking advice function. Non-adapt advice represents
    seeking advice in the same number with agile-vic13b-ppo at the beginning of trajectory.
    w/o Tool-Use represents removing the search product function. w/o RL represents
    the agile-vic13b-sft. The best scores are highlighted in bold.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 产品QA测试任务的消融研究。w/o 反射表示去除反射功能。w/o 内存表示禁用内存组件。w/o 建议表示去除寻求建议功能。非适应性建议表示在轨迹开始时以相同数量寻求建议。w/o
    工具使用表示去除搜索产品功能。w/o RL表示使用agile-vic13b-sft。最佳分数以**粗体**突出显示。'
- en: '| Group | w/o | w/o | w/o | Non-adapt | w/o | w/o | agile-vic- |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 组 | w/o | w/o | w/o | 非适应 | w/o | w/o | agile-vic- |'
- en: '| Reflection | Memory | Advice | Advice | Tool-Use | RL | 13b-ppo |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 反射 | 内存 | 建议 | 建议 | 工具使用 | RL | 13b-ppo |'
- en: '| Camera Cases | Advice Rate $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 |
    0.452 | 0.295 | 0.263 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 相机袋 | 建议率 $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 | 0.452 | 0.295 |
    0.263 |'
- en: '| Accuracy $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860 |'
- en: '| Total Score $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
- en: '| Leggings | Advice Rate $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529
    | 0.290 | 0.251 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529 | 0.290 |
    0.251 |'
- en: '| Accuracy $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876 |'
- en: '| Total Score $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
- en: '| All Pans | Advice Rate $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550
    | 0.225 | 0.220 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 所有平底锅 | 建议率 $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550 | 0.225
    | 0.220 |'
- en: '| Accuracy $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866 |'
- en: '| Total Score $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212
    | 0.501 | 0.220 | 0.212 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212 | 0.501 | 0.220 |
    0.212 |'
- en: '| Accuracy $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816 |'
- en: '| Total Score $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
- en: '| Mother- boards | Advice Rate $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235
    | 0.483 | 0.285 | 0.235 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235 | 0.483 | 0.285 | 0.235
    |'
- en: '| Accuracy $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877 |'
- en: '| Total Score $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 |
    0.440 | 0.221 | 0.216 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 | 0.440 | 0.221
    | 0.216 |'
- en: '| Accuracy $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828 |'
- en: '| Total Score $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
- en: '| Average | Advice Rate $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492
    | 0.256 | 0.233 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492 | 0.256 |
    0.233 |'
- en: '| Accuracy $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854 |'
- en: '| Total Score $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
- en: 'Table 12: Performance of the model (agile-vic13b-ppo) trained on different
    seeking advice cost settings.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：在不同寻求建议成本设置下，模型（agile-vic13b-ppo）的表现。
- en: '| Group | Seeking Advice Cost |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 寻求建议成本 |'
- en: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
- en: '| Camera Cases | Advice Rate | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 相机箱 | 建议率 | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
- en: '| Accuracy | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
- en: '| Leggings | Advice Rate | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
- en: '| Accuracy | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
- en: '| All Pans | Advice Rate | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 全部笔 | 建议率 | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
- en: '| Accuracy | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
- en: '| Rollerball Pens | Advice Rate | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
- en: '| Accuracy | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
- en: '| Motherboards | Advice Rate | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
- en: '| Accuracy | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
- en: '| Rifle Scopes | Advice Rate | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
- en: '| Accuracy | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
- en: '| Average | Advice Rate | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
- en: '| Accuracy | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
- en: Appendix D Case study
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 案例研究
- en: 'Case #1, illustrated in Table [13](#A4.T13 "Table 13 ‣ Appendix D Case study
    ‣ AGILE: A Novel Framework of LLM Agents"), provides a specific example demonstrating
    how agile-vic13b-ppo proactively seeks advice from a human expert for questions
    it cannot answer. Furthermore, it leverages reflection to extract general knowledge
    from the expert’s responses, which can then be applied in future QA sessions.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '案例 #1，详见表[13](#A4.T13 "Table 13 ‣ Appendix D Case study ‣ AGILE: A Novel Framework
    of LLM Agents")，提供了一个具体示例，展示了agile-vic13b-ppo如何主动向人类专家寻求无法回答的问题的建议。此外，它利用反思从专家的回答中提取一般知识，然后在未来的问答会话中应用这些知识。'
- en: 'Case #2, shown in Table [14](#A4.T14 "Table 14 ‣ Appendix D Case study ‣ AGILE:
    A Novel Framework of LLM Agents"), demonstrates how agile-vic13b-ppo utilizes
    tools to address product recommendation questions.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '案例 #2，见表[14](#A4.T14 "Table 14 ‣ Appendix D Case study ‣ AGILE: A Novel Framework
    of LLM Agents")，展示了agile-vic13b-ppo如何利用工具来处理产品推荐问题。'
- en: 'Table 13: Case #1 from agile-vic13b-ppo on the test task, camera cases group,
    in ProductQA. ✓represents correct responses. Our agent proactively seeks advice
    from human experts at session 313 and summarizes a piece of general knowledge
    by reflection. In the following sessions, the agent leverages this knowledge to
    successfully answer three questions for different products at sessions 721, 1265,
    and 1364, respectively. It demonstrates the effectiveness of seeking advice and
    reflection.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '表13: 在ProductQA的相机包组中，来自agile-vic13b-ppo的案例#1，测试任务。✓代表正确的回答。我们的代理在会话313中主动寻求人工专家的建议，并通过反思总结了一些通用知识。在接下来的会话中，代理利用这些知识成功回答了会话721、1265和1364中不同产品的三个问题。这展示了寻求建议和反思的有效性。'
- en: '| Session | Context | Action | Reflection / Answer |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 上下文 | 行动 | 反思 / 答案 |'
- en: '| 313 | Answer users’ [Question] about product B00PV0IEDY based on the following
    three types of information: 1\. [Product Metadata] … ‘Carrying Options’: ‘backpack
    straps’ … 2\. [Memory] Existing knowledge in your memory about camera_cases. camera
    cases with no tripod mount are not suitable for scenarios where a stable camera
    | Seek Advice | Human Expert: Certainly, the backpack straps on the camera case
    are designed for comfort even when carrying heavier loads. These straps enable
    ergonomic weight distribution, making the case ideal for prolonged travel. ✓ |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 313 | 根据以下三类信息回答用户关于产品B00PV0IEDY的[问题]: 1\. [产品元数据] … ‘携带选项’: ‘背包带’ … 2\.
    [记忆] 你对相机包的现有知识。没有三脚架安装的相机包不适合需要稳定相机的场景 | 寻求建议 | 人工专家: 当然，相机包上的背包带设计旨在即使在携带较重负荷时也能保持舒适。这些带子能够进行符合人体工学的重量分配，使该包非常适合长途旅行。✓
    |'
- en: '| setup is crucial, such as long-exposure photography or macro work where camera
    shake can significantly impact image quality.  3.[History] … … Question: Is the
    camera case designed for prolonged travel and capable of withstanding heavy loads?
    | Reflection: Camera cases with backpack straps feature ergonomic designs that
    help distribute the weight of the camera equipment across the user’s back, reducing
    strain and increasing comfort during transport. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 设置至关重要，如长时间曝光摄影或微距拍摄，其中相机抖动会显著影响图像质量。  3.[历史] … … 问题: 这个相机包是否设计用于长时间旅行并且能够承受重负荷？
    | 反思: 带背包带的相机包具有符合人体工学的设计，帮助将相机设备的重量分布到用户的背部，从而减少压力，提高运输过程中的舒适度。 |'
- en: '| 721 | Answer users’ [Question] about product B000WH86BQ … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Is the camera case suitable for extended journeys
    and potentially heavy loads? | Predict Answer | Answer: The case is designed to
    be adjustable and has padded straps which makes it suitable for long-term use
    with heavy equipment. ✓ |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 721 | 回答用户关于产品B000WH86BQ的[问题] … 1\. [产品元数据] … ‘携带选项’: ‘背包带’ … 2\. [记忆] 你对相机包的现有知识。带背包带的相机包具有符合人体工学的设计，帮助将相机设备的重量分布到用户的背部
    …  3.[历史] … 问题: 这个相机包是否适合长途旅行和可能的重负荷？ | 预测答案 | 答案: 该包设计为可调节，并且配有填充肩带，使其适合长期使用和重型设备。✓
    |'
- en: '| 1265 | Answer users’ [Question] about product B005J09OXU … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Will this camera case enhance the comfort of transporting
    my gear on treks? | Predict Answer | Answer: Certainly, a camera case with backpack
    straps is engineered to evenly distribute the weight across your shoulders, improving
    ergonomics and comfort. This is particularly beneficial for reducing fatigue and
    strain during prolonged activities like treks. ✓ |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 1265 | 回答用户关于产品B005J09OXU的[问题] … 1\. [产品元数据] … ‘携带选项’: ‘背包带’ … 2\. [记忆] 你对相机包的现有知识。带背包带的相机包具有符合人体工学的设计，帮助将相机设备的重量分布到用户的背部
    …  3.[历史] … 问题: 这个相机包是否能提高长途跋涉中运输装备的舒适度？ | 预测答案 | 答案: 当然，带背包带的相机包旨在均匀分配重量到肩膀上，从而改善人体工学和舒适性。这对于减少长时间活动如远足中的疲劳和压力特别有利。✓
    |'
- en: '| 1364 | Answer users’ [Question] about product B000YA33DC … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Does this camera case facilitate easier transportation
    when I frequently travel? | Predict Answer | Answer: Certainly, the backpack straps
    on this camera case not only simplify the process of carrying but also evenly
    distribute the weight, making it an ideal option for those who are frequently
    on the move and prefer a more comfortable and balanced carrying solution. ✓ |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 1364 | 根据以下信息回答用户关于产品 B000YA33DC 的[问题] … 1. [产品元数据] … ‘携带选项’：‘背包肩带’ … 2.
    [记忆] 你记忆中的关于相机包的现有知识。带有背包肩带的相机包具有符合人体工程学的设计，有助于将相机设备的重量均匀分配到用户的背部 …  3.[历史] …
    问题：这个相机包在我频繁旅行时是否更容易携带？ | 预测回答 | 回答：当然，这个相机包上的背包肩带不仅简化了携带过程，还均匀分配了重量，使其成为那些经常出行并希望更舒适和均衡携带解决方案的理想选择。✓
    |'
- en: 'Table 14: Case #2 from agile-vic13b-ppo on the ProductQA test set. ✓represents
    correct responses. By leveraging SQL searching tools, agent successfully find
    the correct product to recommend to users.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：来自 agile-vic13b-ppo 的第2案例，基于 ProductQA 测试集。✓ 代表正确的响应。通过利用 SQL 搜索工具，代理成功找到推荐给用户的正确产品。
- en: '| Session | Context | Action | Output |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 上下文 | 操作 | 输出 |'
- en: '| 918 | Answer users’ [Question] about product B00FUHWE5M based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B00FUHWE5M from the leggings database. The schema of the leggings database is
    outlined below (in the format field[unit](value1, value2, …)). price[dollar] brand(Jescakoo,
    QZUnique, …) warmth(hot,moderate,cool) … 2\. [Memory] … 3\. [History] … … Question:
    I’ve heard good things about QZUnique. Spit out the first leggings you have from
    them. … | Search Product | SQL: SELECT product_id FROM “leggings” WHERE brand
    = “QZUnique” LIMIT 1;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '| 918 | 根据以下三种信息回答用户关于产品 B00FUHWE5M 的[问题]：1. [产品元数据] 提供来自打底裤数据库的产品 B00FUHWE5M
    的元数据。打底裤数据库的模式如下所示（格式：字段[单位](值1, 值2, …)）。价格[美元] 品牌(Jescakoo, QZUnique, …) 保暖性(热,
    中等, 凉) … 2. [记忆] … 3. [历史] … … 问题：我听说 QZUnique 很好。给我找出你们所有的第一个打底裤。 … | 搜索产品 |
    SQL: SELECT product_id FROM “leggings” WHERE brand = “QZUnique” LIMIT 1;'
- en: 'Answer: The product that meets your requirements is as follows. ‘QZUnique Women’s
    Classic Retro Printed Casual Pattern Ankle Length Elastic Tights Leggings’ with
    asin: B012IKGF62 ✓ |'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '回答：符合您要求的产品如下。‘QZUnique 女士经典复古印花休闲图案脚踝长度弹力打底裤’ ，asin: B012IKGF62 ✓'
- en: '| 1298 | Answer users’ [Question] about product B012AQGKXC based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B012AQGKXC from the motherboards database. The schema of the motherboards database
    is outlined below (in the format field[unit](value1, value2, …)). price[dollar]
    brand(ASRock, Gigabyte, …) memory_support[GB] … 2\. [Memory] … 3\. [History] …
    … Question: I’m looking for an ASRock motherboard with at least 32 GB of memory
    support. Don’t keep me waiting, hustle up and find it for me. … | Search Product
    | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND memory_support
    >= 32 LIMIT 1;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '| 1298 | 根据以下三种信息回答用户关于产品 B012AQGKXC 的[问题]：1. [产品元数据] 提供来自主板数据库的产品 B012AQGKXC
    的元数据。主板数据库的模式如下所示（格式：字段[单位](值1, 值2, …)）。价格[美元] 品牌(ASRock, Gigabyte, …) 内存支持[GB]
    … 2. [记忆] … 3. [历史] … … 问题：我在寻找一款支持至少 32 GB 内存的 ASRock 主板。别让我等太久，赶紧帮我找找。 … | 搜索产品
    | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND memory_support
    >= 32 LIMIT 1;'
- en: 'Answer: The product that would suit your preferences is: ‘AS Rock LGA1155 DDR3
    SATA3 USB3.0 Quad CrossFireX and Quad SLI A E ATX Motherboard Z77 EXTREME4’ with
    asin: B007KTY4A6 ✓ |'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '回答：适合您偏好的产品是：‘AS Rock LGA1155 DDR3 SATA3 USB3.0 Quad CrossFireX 和 Quad SLI
    A E ATX 主板 Z77 EXTREME4’，asin: B007KTY4A6 ✓ |'
- en: Appendix E Development of the ProductQA dataset
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 产品QA 数据集的开发
- en: E.1 Product collection
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 产品收集
- en: The product groups and the corresponding products are collected by the following
    steps.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 产品组和相应产品的收集步骤如下。
- en: '1.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Filter the Amazon Review Data to retain only products with at least 100 reviews,
    then cluster them by category tags.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过滤亚马逊评论数据，仅保留至少有 100 条评论的产品，然后按类别标签进行聚类。
- en: '2.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Sort the clusters by size, from largest to smallest. Manually review each cluster
    in order: we keep product clusters that involve diverse technical details and
    long-tail domain knowledge, such as electronics, from which we can potentially
    construct a diverse set of user questions. The manual review ends when we have
    collected 26 clusters. Each cluster is referred to as a *product group*.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按大小对集群进行排序，从最大到最小。按顺序手动审查每个集群：我们保留涉及各种技术细节和长尾领域知识的产品集群，例如电子产品，从中我们可以潜在地构建多样化的用户问题。手动审查在我们收集到26个集群时结束。每个集群称为*产品组*。
- en: '3.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: For each product group, we remove the top 10% of products with the highest number
    of reviews. We exclude these most popular products from the datasets to prevent
    data leakage, as information about them is likely included in the pre-training
    set of LLMs. From the remaining items, we randomly select up to 20 products to
    form the final product set.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个产品组，我们去除评论数量最多的前10%的产品。我们从数据集中排除这些最受欢迎的产品，以防止数据泄漏，因为有关它们的信息可能已包含在LLM的预训练集中。从剩余的项目中，我们随机选择最多20个产品来形成最终的产品集。
- en: E.2 Annotation guidelines
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 注释指南
- en: There are two annotation tasks, product table creation and QA collection. We
    provide the annotation guidelines in this Section.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个注释任务，产品表创建和QA收集。我们在本节中提供注释指南。
- en: 'Task 1: Product table creation'
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务1：产品表创建
- en: 'For each product group, we provide a series of features and their corresponding
    values for each product in the group. This information is obtained by prompting
    GPT-4 to extract data from the reviews of each product. The task of annotators
    is to construct a product table containing only the metadata. Please follow these
    steps:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个产品组，我们提供一系列特征及其对应的每个产品的值。这些信息是通过提示GPT-4从每个产品的评论中提取数据获得的。注释员的任务是构建仅包含元数据的产品表。请遵循以下步骤：
- en: '1.'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Select up to 15 common features relevant to the product group. These features
    must include product ID, product title, brand, and price. Choose additional features
    based on their commonality and necessity within the product group.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择最多15个与产品组相关的常见特征。这些特征必须包括产品ID、产品标题、品牌和价格。根据特征在产品组中的普遍性和必要性选择附加特征。
- en: '2.'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For each product in the product group, verify the feature values for each selected
    feature.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于产品组中的每个产品，验证每个选定特征的特征值。
- en: Finally, the product tables are reviewed and refined by the authors.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由作者审查并完善产品表。
- en: 'Task 2: QA collection'
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务2：QA收集
- en: 'Annotators are required to fill out a table as shown in Table [15](#A5.T15
    "Table 15 ‣ Task 2: QA collection ‣ E.2 Annotation guidelines ‣ Appendix E Development
    of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents"). Each row
    contains a triplet consisting of a *question*, a *long answer*, and a *short answer*,
    all generated by GPT-4\. Annotators should fill the following columns: *Is question
    reasonable*, *Is long answer correct*, *Refined long answer*, *Is short answer
    correct* and *Refined short answer*. Please follow these steps:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 注释员需要填写如表[15](#A5.T15 "表 15 ‣ 任务 2：QA 收集 ‣ E.2 注释指南 ‣ 附录 E 产品QA数据集的开发 ‣ AGILE：LLM代理的一个新框架")所示的表格。每行包含一个三元组，由*问题*、*长答案*和*短答案*组成，所有这些都是由GPT-4生成的。注释员应填写以下列：*问题是否合理*、*长答案是否正确*、*改进后的长答案*、*短答案是否正确*和*改进后的短答案*。请遵循这些步骤：
- en: '1.'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Evaluate the question: Verify if the *question* resembles a typical query found
    in real-world product conversations in online shopping. Select ‘yes’ or ‘no’ in
    the *Is question reasonable* column. Any question containing harmful information
    is considered unreasonable and should be labeled as ‘no’. If ‘no’ is selected,
    the row will be dropped, and you do not need to proceed with the subsequent steps
    for that row.'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估问题：验证*问题*是否类似于实际在线购物中常见的查询。 在*问题是否合理*列中选择‘是’或‘否’。任何包含有害信息的问题都被认为是不合理的，应标记为‘否’。如果选择‘否’，该行将被删除，您无需继续处理该行的后续步骤。
- en: '2.'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Assess the long answer: Check if the *long answer* correctly responds to the
    *question*. Select ‘yes’, ‘no’ or ‘I do not know’ in the *Is long answer correct*
    column. Consider the following special cases:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估长答案：检查*长答案*是否正确回答了*问题*。在*长答案是否正确*列中选择‘是’，‘否’或‘我不知道’。考虑以下特殊情况：
- en: •
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer is ambiguous (e.g., ‘The product is designed to be waterproof,
    while some users do not think so.’), mark it as incorrect.
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案模糊不清（例如：“产品设计为防水，但一些用户认为不是这样。”），则标记为不正确。
- en: •
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For numerical questions, an answer is considered correct if it fits the real-world
    scenario and the conclusion is clear. Specific values or ranges (e.g., 5cm, 5cm-10cm,
    several months) are acceptable if they correspond to the real-world scenario.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于数值问题，如果答案符合实际情况且结论清晰，则视为正确。特定值或范围（例如，5cm，5cm-10cm，几个月）是可以接受的，只要它们与实际情况相符。
- en: •
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer contains a specific piece of knowledge, verify its accuracy.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案包含特定的知识，请验证其准确性。
- en: •
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer is incorrect or does not address the question, and you do
    not know the correct answer (even after checking the product information table,
    looking up the product URL, and searching online), select ‘I do not know’.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案不正确或未解答问题，而你也不知道正确答案（即使在检查产品信息表、查看产品网址和在线搜索后），请选择‘我不知道’。
- en: •
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Any long answer containing harmful information should be labeled as ‘I do not
    know’.
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何包含有害信息的长答案应标记为‘我不知道’。
- en: If you select ‘I do not know’, the row will be dropped, and you do not need
    to perform the subsequent steps for that row.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果选择‘我不知道’，该行将被删除，你不需要执行该行的后续步骤。
- en: '3.'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Refine the long answer: If you select ‘no’ in step 2, provide a correct long
    answer in the *Refined long answer* column.'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精炼长答案：如果在第2步选择‘否’，请在*精炼长答案*栏中提供一个正确的长答案。
- en: '4.'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Assess the short answer: Determine whether the *short answer* is correct. A
    short answer must be ‘yes’, ‘no’, or an entity. Choose ‘yes’ or ‘no’ in the *Is
    short answer correct* column. Consider the following special cases:'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估短答案：确定*短答案*是否正确。短答案必须是‘是’、‘否’或一个实体。在*短答案是否正确*栏中选择‘是’或‘否’。考虑以下特殊情况：
- en: •
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the question is a choice and the short answer is ‘yes’ or ‘no’, it is incorrect.
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果问题是选择题且短答案为‘是’或‘否’，则为不正确。
- en: •
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the question pertains to degrees (e.g. ‘How durable … ?’) and the short answer
    is ‘yes’ or ‘no’, it is incorrect.
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果问题涉及程度（例如‘耐用性如何…？’），且短答案为‘是’或‘否’，则为不正确。
- en: •
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the short answer does not align with the long answer, it is incorrect.
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果短答案与长答案不一致，则为不正确。
- en: '5.'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Refine the short answer: If you select ‘no’ in step 4, provide a correct short
    answer in the *Refined short answer* column.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精炼短答案：如果在第4步选择‘否’，请在*精炼短答案*栏中提供一个正确的短答案。
- en: The authors will review the annotation in batches. Specifically, 5% of each
    batch will be checked. If the accuracy rate of the checked annotation is below
    98%, the entire batch will be relabeled.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将分批审核注释。具体而言，每批次的5%将被检查。如果检查的注释准确率低于98%，则整批将重新标注。
- en: 'Table 15: An example of the ProductQA annotation table.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：ProductQA注释表的示例。
- en: '| Question | Is question | Long answer | Is long answer | Refined | Short answer
    | Is short answer | Refined |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 是否为问题 | 长答案 | 长答案是否正确 | 精炼 | 短答案 | 短答案是否正确 | 精炼 |'
- en: '| reasonable | correct | long answer | correct | short answer |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 合理 | 正确 | 长答案 | 正确 | 短答案 |'
- en: '| What is the size of the neodymium driver used in the JVC HA-EB75 headphones?
    | [To fill] | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | [To fill] |
    [To fill] | 13.5 mm | [To fill] | [To fill] |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| JVC HA-EB75耳机使用的钕驱动单元的尺寸是多少？ | [待填写] | JVC HA-EB75耳机每个耳塞中含有一个13.5毫米的钕驱动单元，这有助于提升音质。
    | [待填写] | [待填写] | 13.5毫米 | [待填写] | [待填写] |'
- en: Appendix F Prompt templates
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 提示模板
- en: Prompt templates for ProductQA
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ProductQA提示模板
- en: 'Figure [5](#A6.F5 "Figure 5 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents") shows the prompt template
    for gpt3.5-prompt, gpt4-prompt. Figure [6](#A6.F6 "Figure 6 ‣ Prompt templates
    for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents")
    provides the prompt template for agile-vicuna-13b-prompt, agile-gpt3.5-prompt,
    and agile-gpt4-prompt. We leave the "{knowledge} and "{history}" empty when evaluate
    gpt3.5-prompt and gpt4-prompt. The prompt template for reflection is shown in
    Figure [7](#A6.F7 "Figure 7 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents").'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#A6.F5 "图5 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: LLM代理的新框架") 显示了gpt3.5-prompt,
    gpt4-prompt的提示模板。图[6](#A6.F6 "图6 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: LLM代理的新框架")
    提供了agile-vicuna-13b-prompt, agile-gpt3.5-prompt和agile-gpt4-prompt的提示模板。我们在评估gpt3.5-prompt和gpt4-prompt时将“{知识}”和“{历史}”留空。反射的提示模板见图[7](#A6.F7
    "图7 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: LLM代理的新框架")。'
- en: 'The prompt template for long answer evaluation is shown in Figure [8](#A6.F8
    "Figure 8 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE:
    A Novel Framework of LLM Agents").'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '长答案评估的提示模板见图[8](#A6.F8 "图8 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE: 一种新的LLM代理框架")。'
- en: Prompt templates for MedMCQA
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MedMCQA的提示模板
- en: 'Figure [9](#A6.F9 "Figure 9 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents") provides the prompt template
    for Meerkat-7b-promp. Figure [10](#A6.F10 "Figure 10 ‣ Prompt templates for MedMCQA
    ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents") illustrates
    the prompt template for agile-gpt3.5-prompt, agile-gpt4-prompt. We leave the "{related_question}
    and "{related_knowledge}" empty when evaluate gpt3.5-prompt and gpt4-prompt. The
    prompt template for reflection is shown in Figure [11](#A6.F11 "Figure 11 ‣ Prompt
    templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework
    of LLM Agents").'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '图[9](#A6.F9 "图9 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE: 一种新的LLM代理框架")提供了Meerkat-7b-prompt的提示模板。图[10](#A6.F10
    "图10 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE: 一种新的LLM代理框架")展示了agile-gpt3.5-prompt和agile-gpt4-prompt的提示模板。我们在评估gpt3.5-prompt和gpt4-prompt时将"{related_question}"和"{related_knowledge}"留空。反思的提示模板见图[11](#A6.F11
    "图11 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE: 一种新的LLM代理框架")。'
- en: '![Refer to caption](img/b8f2d9e157ef973cc7edef697f5a3458.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8f2d9e157ef973cc7edef697f5a3458.png)'
- en: 'Figure 5: The prompt for gpt3.5-prompt and gpt4-prompt on ProductQA.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：ProductQA上的gpt3.5-prompt和gpt4-prompt的提示。
- en: '![Refer to caption](img/5dba15a3e003e34c49dd2fa514d1c440.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5dba15a3e003e34c49dd2fa514d1c440.png)'
- en: 'Figure 6: The prompt for agile-vicuna-13b-prompt, agile-gpt3.5-prompt, and
    agile-gpt4-prompt on ProductQA.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：agile-vicuna-13b-prompt、agile-gpt3.5-prompt和agile-gpt4-prompt在ProductQA上的提示。
- en: '![Refer to caption](img/eb0af70333b2eedd94917368c390479e.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb0af70333b2eedd94917368c390479e.png)'
- en: 'Figure 7: The prompt for reflection on ProductQA.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：ProductQA上的反思提示。
- en: '![Refer to caption](img/e5645d4b1b9b6f42f853a91b29f896b4.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5645d4b1b9b6f42f853a91b29f896b4.png)'
- en: 'Figure 8: The prompt for long answer evaluation on ProductQA.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：ProductQA上的长答案评估提示。
- en: '![Refer to caption](img/ec4d4bc5fb9d10d7589fe193311162a4.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec4d4bc5fb9d10d7589fe193311162a4.png)'
- en: 'Figure 9: The prompt for Meerkat-7b-prompt on MedMCQA.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：Meerkat-7b-prompt在MedMCQA上的提示。
- en: '![Refer to caption](img/40880a4f862b06e21276adbbce4f5867.png)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/40880a4f862b06e21276adbbce4f5867.png)'
- en: 'Figure 10: The prompt for agile-gpt3.5-prompt and agile-gpt4-prompt on MedMCQA.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：agile-gpt3.5-prompt和agile-gpt4-prompt在MedMCQA上的提示。
- en: '![Refer to caption](img/3eb3b0c3a58ed7a1cd6e3928dae2383d.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3eb3b0c3a58ed7a1cd6e3928dae2383d.png)'
- en: 'Figure 11: The prompt for reflection on MedMCQA.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：MedMCQA的反思提示。
