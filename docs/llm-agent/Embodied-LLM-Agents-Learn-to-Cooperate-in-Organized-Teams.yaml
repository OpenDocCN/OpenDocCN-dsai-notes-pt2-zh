- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Embodied LLM Agents Learn to Cooperate in Organized Teams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.12482](https://ar5iv.labs.arxiv.org/html/2403.12482)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xudong Guo¹  Kaixuan Huang²  Jiale Liu³  Wenhui Fan¹  Natalia Vélez²
  prefs: []
  type: TYPE_NORMAL
- en: Qingyun Wu³  Huazheng Wang⁴  Thomas L. Griffiths²  Mengdi Wang²
  prefs: []
  type: TYPE_NORMAL
- en: ¹Tsinghua University ²Princeton University
  prefs: []
  type: TYPE_NORMAL
- en: ³Penn State University  ⁴Oregon State University
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning,
    and decision-making, drawing upon their extensive world knowledge and proficiency
    in language-related tasks. LLMs thus hold tremendous potential for natural language
    interaction within multi-agent systems to foster cooperation. However, LLM agents
    tend to over-report and comply with any instruction, which may result in information
    redundancy and confusion in multi-agent cooperation. Inspired by human organizations,
    this paper introduces a framework that imposes prompt-based organization structures
    on LLM agents to mitigate these problems. Through a series of experiments with
    embodied LLM agents and human-agent collaboration, our results highlight the impact
    of designated leadership on team efficiency, shedding light on the leadership
    qualities displayed by LLM agents and their spontaneous cooperative behaviors.
    Further, we harness the potential of LLMs to propose enhanced organizational prompts,
    via a Criticize-Reflect process, resulting in novel organization structures that
    reduce communication costs and enhance team efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern intelligent systems, such as autonomous vehicle networks and swarms of
    drones, often involve complex decision-making processes where multiple agents
    or components must collaborate seamlessly to achieve specific objectives Wang
    et al. ([2020](#bib.bib53)); Vinyals et al. ([2019](#bib.bib51)); Zhang et al.
    ([2019a](#bib.bib63)); Wang et al. ([2021](#bib.bib54)). In these systems, communication
    among the various agents is pivotal, as it dictates the flow of information, coordination
    of tasks, and overall system performance Zhang et al. ([2019b](#bib.bib66)); Guo
    et al. ([2023](#bib.bib13)); Foerster et al. ([2016](#bib.bib9)); Das et al. ([2019](#bib.bib7)).
    Agents in traditional multi-agent systems often have to communicate in pre-specified
    ways, such as exchanging gradients, sharing data, state observations and actions,
    etc Kim et al. ([2020](#bib.bib18)); Lin et al. ([2021](#bib.bib24)); Foerster
    et al. ([2016](#bib.bib9)). The emergence of large language models (LLMs) makes
    it possible for AI agents to communicate and cooperate using natural language,
    bringing enormous flexibility and potential for more nuanced and human-understandable
    interactions Park et al. ([2023](#bib.bib33)); Hong et al. ([2023](#bib.bib16));
    Mandi et al. ([2023](#bib.bib29)); Chen et al. ([2023a](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the flexibility of LLMs, integrating them into practical multi-agent
    systems remains a challenge. While LLMs are trained and finetuned for text generation
    and instruction-following, they are not necessarily tailored to multi-agent cooperation.
    Modern LLMs are prone to over-reporting and obeying instructions, as a by-product
    of RLHF finetuning (Bai et al., [2022](#bib.bib2)), and they can ignore critical
    information (Liu et al., [2023a](#bib.bib25)) or be distracted by irrelevant information (Shi
    et al., [2023](#bib.bib43)), especially when the context is long (see Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams") for examples). While recent studies involving agent-based LLMs have demonstrated
    they are capable of solving problems through multi-agent collaboration Li et al.
    ([2023b](#bib.bib22)); Zhang et al. ([2023c](#bib.bib62)); Mandi et al. ([2023](#bib.bib29)),
    it is worth noting that such collaborations often follow predefined patterns designed
    using heuristics to channel the behavior of the models productively Li et al.
    ([2023b](#bib.bib22)). Creating systems that support free-flowing interaction
    between LLMs in a way that could potentially scale to include human collaborators
    is still an open problem.
  prefs: []
  type: TYPE_NORMAL
- en: This paper investigates the collaborative potential of LLM agents working in
    teams. Drawing on prior studies in human collaboration from cognitive and economic
    perspectives, there is potential for organizations to be redesigned to more effectively
    manage the limited attention span within teams, as suggested by Simon et al. ([1971](#bib.bib46)),
    and mitigate individual limitations and enhance overall team performance, as highlighted
    by Van Zandt ([1999](#bib.bib49)) and Vélez et al. ([2023](#bib.bib50)). Specifically,
    we study two research questions. First, *what role do organizational structures
    play in multi-LLM-agent systems?* Second, *how can we optimize these organizational
    structures to support efficient multi-agent coordination?* By leveraging AutoGen Wu
    et al. ([2023](#bib.bib56)), a generic multi-agent conversation framework, we
    develop a framework for studying how to best organize embodied LLM agents to communicate
    and collaborate in physical/simulated non-text environments Zhang et al. ([2023c](#bib.bib62)).
    Our framework offers the flexibility to prompt and organize LLM agents into various
    team structures, facilitating versatile inter-agent communication. It also serves
    as a testbed to empirically evaluate the traditional ideas proposed in the organization
    theory literature.
  prefs: []
  type: TYPE_NORMAL
- en: Our initial experiments in this setting reveal that uncoordinated LLM agents
    often send redundant and repetitive messages and interrupt others’ actions, leading
    to chaos (see Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams") and Appendix [D](#A4 "Appendix D Ineffective
    Communication ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")).
    To remedy these issues, we explore organizational structures that allow multiple
    LLM agents to collaborate and complete a common task efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The first organizational structure we explore is a hierarchy, a classic object
    of study in organizational theory March and Simon ([1958](#bib.bib30)); Radner
    ([1993](#bib.bib39)); Chisholm ([1992](#bib.bib6)); Bolton and Dewatripont ([1994](#bib.bib3));
    Garicano ([2000](#bib.bib11)); Dodds et al. ([2003](#bib.bib8)). With a designated
    leader, LLM agents work more efficiently and collaboratively. For the example
    of a three-agent team, imposing a leader improves efficiency by up to 30% with
    almost no extra communication cost (up to 3%), consistent with findings for human
    organizations Dodds et al. ([2003](#bib.bib8)). This also holds true in five-agent
    cases. Further, LLM agents demonstrated the potential to elect their own leader
    and adjust leadership dynamically via communication. With proper organizations,
    LLM agents exhibit a variety of cooperative behaviors that mimic humans. For example,
    agents can provide constructive suggestions and seek help from others; they can
    also execute appropriate interactions for a hierarchy such as reporting back on
    task progress; see Figures [6](#S4.F6 "Figure 6 ‣ 4.3 Emergence of Cooperative
    Behaviors ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams") and [7](#S4.F7 "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4
    Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") and
    Appendix [C](#A3 "Appendix C Emergent Cooperative Behaviors in an Organization
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams"). We also tested
    human-agent collaboration, and observe that, unsurprisingly, human leaders are
    much better at coordinating a team of agents when compared to AI agents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/811ed5569e1d26daa4dae7c811046bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example of disorganized communication and interruption, without a
    designated leader. In a team of three GPT-4 agents, two agents engaged in unnecessary
    communication and made disordered decisions, causing a delay due to the lack of
    a predefined organization. We identified many more examples including conflicting
    messages and repetitive communications, see Appendix [D](#A4 "Appendix D Ineffective
    Communication ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams").'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to testing existing organizational structures, we explore the use
    of LLMs to improve the organization prompt. To this end, we develop a Criticize-Reflect
    framework, adopting a dual LLM architecture, to reflect on the team performance
    and generate improved and novel organization prompts. Through this iterative process,
    our LLM agents spontaneously form novel, effective team structures, leading to
    reduced communication costs and improved efficiency; see Figures [8](#S4.F8 "Figure
    8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams") and [9](#S4.F9 "Figure 9 ‣ 4.4 Novel Organizational
    Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As powerful LLMs inherit abundant world knowledge and also general reasoning
    ability, there are increasing efforts to deploy LLMs as the reasoning core for
    decision-making to build human-like autonomous agents (Sun et al., [2023](#bib.bib47);
    Zhu et al., [2023](#bib.bib70); Hao et al., [2023a](#bib.bib14)). This requires
    observations of the RL environment to be translated into natural language in a
    way that is easier for language models to process. The reasoning of the language
    models also needs to be turned into a viable action for execution. Popular prompting
    techniques for doing so include ReAct (Yao et al., [2022](#bib.bib59)) and Reflexion (Shinn
    et al., [2023](#bib.bib45)). Other methods that involve fine-tuning the language
    models have also been explored (Hao et al., [2023b](#bib.bib15)). In addition,
    various techniques have been proposed to mitigate the biases and constraints of
    language models, including chain-of-thought reasoning (Wei et al., [2022](#bib.bib55)),
    external tools (Shen et al., [2023](#bib.bib42); Patil et al., [2023](#bib.bib34)),
    external documents (Wang et al., [2023](#bib.bib52)) and skill libraries (Zhu
    et al., [2023](#bib.bib70)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Multi-Agent Cooperation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-agent cooperation has been extensively studied for decades under various
    topics such as communication efficiency, planning, leadership, and team dynamics
    using different platforms (Lowe et al., [2017](#bib.bib27); Samvelyan et al.,
    [2019](#bib.bib41); Resnick et al., [2018](#bib.bib40); Puig et al., [2021](#bib.bib37))
    (see recent surveys for detail (Oroojlooy and Hajinezhad, [2023](#bib.bib31);
    Zhang et al., [2021](#bib.bib65); Gronauer and Diepold, [2022](#bib.bib12))).
    Previous works mainly focused on communication through continuous vectors (Das
    et al., [2019](#bib.bib7)) or discrete symbols (Lowe et al., [2017](#bib.bib27);
    Jaques et al., [2019](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: Recent work (Xu et al., [2023](#bib.bib57); Zhang et al., [2023d](#bib.bib64);
    Wu et al., [2023](#bib.bib56); Li et al., [2023a](#bib.bib21)) showed that multiple
    LLM agents or human-agent teams can improve upon single LLM in solving pure text-based
    tasks, such as creative writing, reasoning, and code generation. Liu et al. ([2023b](#bib.bib26)),
    Hong et al. ([2023](#bib.bib16)) and Zheng et al. ([2023](#bib.bib67)) further
    explored agent selection or role assignment to improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have also been applied to multi-agent cooperation for embodied tasks (Agashe
    et al., [2023](#bib.bib1); Mandi et al., [2023](#bib.bib29); Park et al., [2023](#bib.bib33);
    Chen et al., [2023a](#bib.bib4)). Besides, Zhang et al. ([2023b](#bib.bib61))
    proposed an intention inference framework to enhance the cooperation of LLM agents
    without explicit communication. Li et al. ([2023b](#bib.bib22)) investigated LLM-agents
    collaboration for Theory of Mind inferences tasks with a broadcast-only communication
    protocol and homogeneous policies. Zhang et al. ([2023c](#bib.bib62)) studied
    embodied multi-agent cooperation in the two-agent and the one-human-one-agent
    settings. Chen et al. ([2023b](#bib.bib5)) explored different fixed communication
    structures for multi-LLM-robots. These initial explorations are also limited to
    fixed team structures and are not optimized for communication efficiency. In contrast,
    our work explores the impact of deploying and optimizing organizational structures,
    allowing $\geq 3$ agents in a team, for efficient multi-agent communication and
    cooperation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Prompt Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Language models are sensitive to prompts. The format of the prompt can have
    a substantial influence on performance (Gao et al., [2020](#bib.bib10); Wei et al.,
    [2022](#bib.bib55); Zhou et al., [2022a](#bib.bib68); Shi et al., [2023](#bib.bib43);
    Zou et al., [2023](#bib.bib71); Qi et al., [2023](#bib.bib38)). Various research
    efforts have aimed at improving performance through prompt optimization. Typical
    approaches include heuristic search using language models’ knowledge (Gao et al.,
    [2020](#bib.bib10); Shin et al., [2020](#bib.bib44)), first-order methods like
    soft prompt tuning (Lester et al., [2021](#bib.bib20)), and prefix tuning (Li
    and Liang, [2021](#bib.bib23)). In this work, we focus on obtaining an interpretable
    prompt in the form of natural language, drawing on insights from Yang et al. ([2023](#bib.bib58)),
    Zhou et al. ([2022b](#bib.bib69)), and Pryzant et al. ([2023](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Architecture and Multi-Agent Communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We adopt the embodied LLM-agent architecture proposed by Zhang et al. ([2023c](#bib.bib62))
    and expand it to enable organized teams of $\geq 3$ agents to communicate, plan,
    and act in physical/simulated environments. Figure [2](#S3.F2 "Figure 2 ‣ 3.1
    Architecture and Multi-Agent Communication ‣ 3 Method ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams") illustrates our architecture. Borrow insights
    from Zhang et al. ([2023c](#bib.bib62)), we adopt four standard modules: Configurator,
    Perception Module, Memory Module, and Execution Module. They are responsible for
    configuring the agents, translating environmental observations into text, storing
    & retrieving historical information, and executing actions, respectively (Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1 Architecture and Multi-Agent Communication ‣ 3 Method ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(a)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0c035fffedab9133538b106aad43e7a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Multi-LLM-agent architecture. (a) The modules of an LLM agent and
    the composition of prompts. (b) There are two phases in one time step: Communication
    phase and Action phase. In the communication phase, the agents take turns communicating
    by broadcasting or selecting receivers to send distinct messages. The agents can
    also choose to keep silent. Comm is short for Communication; PO is short for Partial
    Observation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d388d043eeb70ebad8f33ea2366218f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Criticize-Reflect architecture for improving organizational structure.
    The red agent represents the leader in an organization. The Critic evaluates the
    trajectories and analyzes the agents’ performance. Together with the external
    costs from the environment, the Coordinator proposes a new organization prompt
    to improve the team efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous works focused on two-agent cooperation, in which case the communication
    can be simply treated as an extra action Mandi et al. ([2023](#bib.bib29)); Zhang
    et al. ([2023c](#bib.bib62)). In contrast, we aim to enable three or more agents
    to work in a team and cooperate through organized communication. Thus we design
    the architecture with several features that facilitate organized multi-agent communication
    (Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Architecture and Multi-Agent Communication
    ‣ 3 Method ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")(b)):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We impose an organizational structure for the agent team via prompting, i.e.,
    including a textual description as part of the prompt for each round of communication.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM agents keep alternating between two phases during their task: the communication
    phase and the action phase. The standalone communication phase supports richer
    team structures and flexible communication patterns.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During communication, agents take turns to communicate. An agent can choose
    to broadcast a message, select one recipient for a message, choose multiple recipients
    and send them distinct messages, or remain silent. Agents keep their own history
    of communication and can respond to messages from previous communications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Criticize-Reflect Method for Improving Organizational Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We leverage powerful LLMs to optimize the organization prompt, borrowing insights
    from (Yang et al., [2023](#bib.bib58)). To do so, we introduce a dual-LLM framework
    to allow the multi-LLM-agent system to ponder and improve the organizational structure.
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Architecture and Multi-Agent Communication ‣
    3 Method ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") illustrates
    the architecture of our framework. It consists of two LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM critic: Inspired by the Actor-Critic method of reinforcement learning Konda
    and Tsitsiklis ([1999](#bib.bib19)), we introduce an LLM critic to evaluate the
    team’s performance based on verbal feedback. The team critic takes as input the
    dialogue and action history of one episode. Then, the critic analyzes the input
    and reasons to extract and summarize the key steps that are believed to influence
    the performance in the episode. Also, the critic provides a textual evaluation
    of agents’ behaviors and the ranking of their leadership. Note that the critic
    in our method is different from Zhang et al. ([2023a](#bib.bib60)) where the critic
    is a centralized controller offering suggestions to each agent. See the prompts
    in Appendix [A](#A1 "Appendix A Prompt Templates ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM coordinator: The LLM coordinator takes as input the outputs of the LLM
    critic as well as cost metrics of previous episodes from the environment. It reflects
    on these data and generates thoughts based on the analysis of the past episodes.
    The coordinator then generates three distinct new prompts and chooses the best
    out of them. For more details please refer to Appendix [A](#A1 "Appendix A Prompt
    Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For each new organization prompt, we run for one episode following the new organizational
    structure and then return the dialogue and action history to the critic. By alternating
    between criticizing and reflecting on current structure, the framework discovers
    more effective, novel organizational structures with *self-improvement*.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Environment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We chose VirtualHome-Social Puig et al. ([2018](#bib.bib36), [2021](#bib.bib37))
    as the environment and extended it to support multi-LLM-agent communication and
    interaction. In this environment, agents are humanoid helpers in a virtual home
    doing housekeeping, where the tasks include Prepare afternoon tea, Wash dishes,
    Prepare a meal, Put groceries, Set up a dinner table, etc. For instance, in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams"), the agents cooperate to prepare afternoon tea by searching for and transporting
    task-specific items (chocolate, juice, wine, etc.) to a target location (the coffee
    table). The environment generates symbolic observations of the objects in the
    home and their relations. Each agent only observes the objects in the open containers
    located in her room and teammates in the same room, but she can walk to another
    room to explore. Any agent can communicate with any other agent, not subject to
    a range limit.
  prefs: []
  type: TYPE_NORMAL
- en: Each episode starts from an initial state where agents are randomly located
    in the environment and all containers are closed. The episode terminates when
    the task is fully completed. To evaluate the team’s efficiency we measure the
    number of time steps taken to task completion, and we report the average number
    of tokens communicated between agents per step. In our experiment, each run initializes
    with an independently randomized state to obtain the mean and a confidence interval.
    We adopt GPT-4, GPT-3.5-turbo (Ouyang et al., [2022](#bib.bib32)), and Llama2-70B (Touvron
    et al., [2023](#bib.bib48)) as LLMs in our agents. The temperature is set as 0.8,
    the maximum number of output tokens is 256, and the number of completion choices
    to generate is 1.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Main Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76e84804a48acdebbe6ad010391e61d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Organized teams with a designated leader achieve higher efficiency.
    (a,b) Comparison between the case of disorganized agents, the case where a leader
    is appointed, the case where agents choose their own leader dynamically, and the
    case where a human player replaces an agent to be the leader. Note that GPT-3.5-turbo
    doesn’t support leadership election. (c,d) Comparing leadership quality for GPT-3.5-turbo
    vs. GPT-4\. The confidence intervals of Human as the leader group are calculated
    over 3 seeds while others are over 20 seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ceb70a394c6f28d7e599c28c02fe10e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Examples of communication messages when there is a designated leader.
    Left: messages from lead agents; Right: messages from non-lead agents. GPT-4 (upper),
    GPT-3.5-turbo (center), and Llama2-70B (lower) demonstrated different communication
    styles.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 A Designated Leader Enhances Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first studied the effect of organizational structures and leadership on LLM
    agents. For benchmarking, we experimented with disorganized LLM agents without
    providing any organization prompt. In this case, agents still communicate with
    one another and work to complete the overall task. However, we discovered frequent
    occasions where agents send redundant, repetitive messages and interfere with
    one another. See Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams") for an illustration and see Appendix [D](#A4
    "Appendix D Ineffective Communication ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams") for more examples. Numeric metrics are reported in Appendix
    Table [1](#A2.T1 "Table 1 ‣ B.1 Complete list of basic experimental results ‣
    Appendix B Additional Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams").
  prefs: []
  type: TYPE_NORMAL
- en: When a leader is appointed via the organization prompt, we observe improved
    team performance – the teams completed the task in less time (Figure [4](#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(a)). After running the 3$\times$). Compared to the disorganized teams,
    teams with a designated leader only have a slightly increased or even less communication
    cost (Figure [4](#S4.F4 "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams")(b)). This is consistent with patterns seen in
    previous models of hierarchical organizations Dodds et al. ([2003](#bib.bib8)).
    For additional experiments on Llama2-70B, please see Appendix Table [1](#A2.T1
    "Table 1 ‣ B.1 Complete list of basic experimental results ‣ Appendix B Additional
    Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams"). The communication
    styles of leaders and non-leaders were clearly differentiated, as shown in Figure
    [5](#S4.F5 "Figure 5 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we asked the agents to elect their own leader. The leadership was reelected
    about every 9 time steps, based on information extracted from the latest 12 messages.
    We observe that agents are generally not power-seeking: they often vote for others
    to lead. In some occasions, agents favored candidates who exhibited higher knowledge
    levels, for example, one agent thought that “Given that Agent_2 has found a necessary
    item, it makes sense for him to be the leader in this round." However, on most
    occasions, we could not tell whether agents made their votes based on rational
    reasoning or just random thoughts (see Appendix [E](#A5 "Appendix E Examples of
    Election ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")). In the
    case of the 3$\times$; see Figure [4](#S4.F4 "Figure 4 ‣ 4 Main Results ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams")(a)). However, this improvement
    was accompanied by a substantial increase in communication cost (i.e., token usages),
    akin to real-world scenarios where relaxing hierarchical structure potentially
    increases communication costs Malone ([2004](#bib.bib28)).'
  prefs: []
  type: TYPE_NORMAL
- en: The proposed multi-LLM-agent architecture is also human-friendly to support
    *human-AI collaboration*. In the experiment, we ask a human player to replace
    the leader in the team of 3 GPT-4 agents. We recruit three human players to conduct
    the experiments. Figure [4](#S4.F4 "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")(a, b) demonstrates that human leadership
    achieved better task completion time and improved communication efficiency compared
    with GPT-4 as the leader. Please find more examples of dialogues between the human
    leader and LLM agents in Appendix [F](#A6 "Appendix F Examples of Human-AI Collaboration
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Leadership and Open Communication Matters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM agents have different levels of leadership. In the team with a mixture of
    GPT-4 and GPT-3.5-turbo agents, appointing GPT-4 as the leader increases the team
    efficiency higher than if GPT-3.5-turbo is the leader (Figure [4](#S4.F4 "Figure
    4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")(c,d)).
    We ran this experiment on teams of three agents and five agents, respectively.
    In both scenarios, the task completion time and communication cost are reduced
    when GPT-4 acts as the leader. This finding implies different levels of leadership
    between these LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: We also observed that encouraging constructive feedback to the leader agent
    helped performance. Motivated by successful human organizations, we tried to promote
    open communications among LLM agents by adding an additional prompt that "If the
    leader’s instructions are not right, you can correct the leader". Figure [4](#S4.F4
    "Figure 4 ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(c, d) illustrates the results. Interestingly, this modification improves
    the team’s overall efficiency and reduces the time to task completion when the
    team is made up of 3$\times$). In both experiments, the communication cost increases.
    We present more details about these behaviors in Appendix [G](#A7 "Appendix G
    Examples of Correction ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Emergence of Cooperative Behaviors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We delved into the behaviors of LLM agents in an organized team to investigate
    how organization prompts influence agents’ communication and decisions. Analysis
    of their dialogue history revealed that agents demonstrated a variety of cooperative
    behaviors, such as reporting, correction, task allocation, and asking for help
    (see Figure [6](#S4.F6 "Figure 6 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4
    Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") for
    an example dialogue).
  prefs: []
  type: TYPE_NORMAL
- en: 'One may argue that these types of behaviors could also emerge due to the nature
    of LLMs, even without a pre-specified team structure. Thus we performed a quantitative
    analysis to study the impact of an organization prompt on these behaviors. We
    followed a three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We defined three major categories of human cooperative behaviors: (i) Information
    sharing: agents influence others by offering new information, either actively
    or by being asked. Reporting to the leader, sharing new observations, and answering
    questions belong to this category. (ii) Leadership & assistance: agents, especially
    the leader if there is one, can influence others by changing their plans. The
    behaviors include task allocation, correction, and asking for help. (iii) Request
    for guidance: agents actively request new information or plans for their own decision
    making.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We developed a standalone prompt-based GPT-4- classifier to analyze each piece
    of dialogue. The classifier decides whether to label the dialogue with any subset
    of the aforementioned labels. The classifier has an accuracy of 91.67% when tested
    on 20 human-labeled dialogue samples with 60 labels (see Appendix [A](#A1 "Appendix
    A Prompt Templates ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")
    for the prompt and Appendix [H](#A8 "Appendix H Examples of Cooperative Behaviors
    Classification by Humans and GPT-4 ‣ Embodied LLM Agents Learn to Cooperate in
    Organized Teams") for the test samples).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the classifier to label messages generated by our teams of agents, and
    report the percentages of messages with cooperative behaviors. Note that a message
    may have multiple labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure [7](#S4.F7 "Figure 7 ‣ 4.3 Emergence of Cooperative Behaviors ‣ 4 Main
    Results ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams") reports
    the results and illustrates the behavior patterns for different LLM agents. The
    results support several observations. Even in a disorganized team, LLM agents
    love to tell others what to do. Leadership & assistance accounts for around <math
    id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="></math> of the dialogues.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, when the team has a hierarchical organization the lead LLM agent
    would presume a dominant role and give orders to others (amount to $></math> of
    their communication), while other members tend to follow and give fewer orders
    compared with the disorganized case. (Figure [7](#S4.F7 $GPT-4 agents). The agents
    emerge three types of cooperative behaviors: information sharing, leadership &
    assistance, and request for guidance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ee813bf273b85b4b24351647ad68f69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Emergent cooperative behaviors of LLM agents. We analyzed the communication
    log of the mixture team (1$\times$GPT-3.5-turbo) and asked another GPT-4 to annotate
    agent’s cooperative behaviors. (a) Behavior of disorganized agents. (b) Behavior
    of a team led by a GPT-4 agent. (c) Behavior of a team led by a GPT-3.5-turbo
    agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Novel Organizational Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having evaluated the merits of different kinds of structures, we let the LLM
    propose novel organizational structures and iteratively refine the organization
    prompts using the Criticize-Reflect method discussed in Section 3 (see also Figure
    [3](#S3.F3 "Figure 3 ‣ 3.1 Architecture and Multi-Agent Communication ‣ 3 Method
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")).
  prefs: []
  type: TYPE_NORMAL
- en: Figure [8](#S4.F8 "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")(a) visualizes the
    reflection process. The system was initialized with a basic organization prompt,
    i.e., “Agent_1 as the leader to coordinate the task". As the Reflection process
    moves forward, the Coordinator generates a sequence of evolving organization prompts,
    picking up key words like “hierarchical" and “dynamic" that imply more complex
    team structures.
  prefs: []
  type: TYPE_NORMAL
- en: We compared the team’s performance before and after the Criticize-Reflect steps.
    Figure [8](#S4.F8 "Figure 8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results
    ‣ Embodied LLM Agents Learn to Cooperate in Organized Teams")(b) illustrates the
    team’s efficiency. We observe that for 3$\times$GPT-3.5-turbo).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Critic analyzes the records of action and dialogue, and performance metrics
    from the most recent episode. It provides evaluation for the full team’s trajectory,
    feedback to individual agents and their rankings. See below for an example of
    the Critic output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/cd00c7d893a996136a40c658f1ce3b17.png)'
  prefs: []
  type: TYPE_IMG
- en: As an ablation study, we removed the Critic from our architecture and only performed
    the Reflection step. The results are shown in Figure [8](#S4.F8 "Figure 8 ‣ 4.4
    Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams")(b), indicating that Reflection without the Critic
    leads to performance decline ($t(38)=1.96,p<.05$). In this case, the Coordinator
    needs to digest all dialogue history and generate a new organization prompt. This
    did not work well and led to rather vague outcomes, for example, “Establish a
    flexible communication network with rotating leadership roles assigned based on
    agents’ task-specific expertise to facilitate swift decision-making and reduce
    unnecessary communication steps.” This comparison highlights the role of the Critic
    and the importance of having a dual Criticize-Reflect architecture. For more results/prompts
    generated by the reflection process, please refer to Appendix [I](#A9 "Appendix
    I Examples of New Prompts after Reflection ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, it is worth mentioning that LLMs are able to generate highly complex
    prompts that imply novel organizational structures that are rarely seen in human
    societies. We illustrate the communication patterns as team structures in Figure [9](#S4.F9
    "Figure 9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams") together with the three novel structures
    proposed by Criticize-Reflect: (c) chain, (d) dual-leader, and (e) dynamic structures,
    which are the best structures of the three settings in Figure [8](#S4.F8 "Figure
    8 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")(b, c) respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to test the generalizability of the novel organizational structures,
    we pick the best novel prompt, the one illustrated in Figure [9](#S4.F9 "Figure
    9 ‣ 4.4 Novel Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams")(e), proposed by the Criticize-Reflect
    architecture on the Prepare afternoon tea task. We test it on a set of six new
    tasks, comprising of three easy tasks and three hard tasks²²2The hard tasks have
    typical numbers of steps to accomplish the tasks <math id="footnote2.m1.1" class="ltx_Math"
    alttext="></math>).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2b48c3bbf776a27292b44bba91a6c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The reflection and improvement process for finding novel organizational
    structures. (a) The experiment was done using the 1$\times$GPT-3.5-turbo team.
    The organization prompt evolves during the iterations, and takes on additional
    keywords such as "central", "hierarchical", and "dynamic". (b) The confidence
    intervals are calculated over 20 seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0bfc553da8d0197dcde654730b61e15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Communication patterns and the corresponding organizational prompts
    for different team structures. (a) Team without organization prompts. (b) Team
    with a leader. (c) A team in the chain structure. (d) A dual-leader team. (e)
    A team with a dynamic leadership. (c, d, e) are proposed by LLM via Reflection.
    Red-robot nodes mark the lead agents, and other nodes are the followers. Edges
    mark the accumulated communication cost between the two nodes (darker edge means
    higher token costs).'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We developed a novel multi-LLM-agent architecture to facilitate communication
    and organize the embodied agent teams for enhanced cooperation. Moreover, we proposed
    the Criticize-Reflect architecture based on LLMs to generate more efficient organizational
    prompts. Extensive experiments with various group settings and organizational
    structures demonstrate that a hierarchically-organized team with a designated/elected
    leader has superior team efficiency, which can be further improved by Criticize-Reflect.
  prefs: []
  type: TYPE_NORMAL
- en: The current work is performed in a single environment and lacks human evaluation.
    Future work shall expand the evaluation to a broader set of environments, allowing
    human evaluation and more complex human-AI collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research studies the integration of prompt-based organizational structures
    to teams of LLM agents, contributing to more efficient and coherent multi-agent
    interactions. These findings have the potential to greatly influence the deployment
    of more effective and autonomous multi-agent systems in various fields, including
    robotics, virtual assistants, etc. For example, the study has potential applications
    in disaster response scenarios, where efficient multi-agent coordination is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agashe et al. (2023) Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent
    coordination abilities in large language models. *arXiv preprint arXiv:2310.03903*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolton and Dewatripont (1994) Patrick Bolton and Mathias Dewatripont. The firm
    as a communication network. *The Quarterly Journal of Economics*, 109(4):809–839,
    1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023a) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong
    Sun, and Jie Zhou. AgentVerse: Facilitating multi-agent collaboration and exploring
    emergent behaviors in agents. *arXiv preprint arXiv:2308.10848*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023b) Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and
    Chuchu Fan. Scalable multi-robot collaboration with large language models: Centralized
    or decentralized systems? *arXiv preprint arXiv:2309.15943*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chisholm (1992) Donald Chisholm. *Coordination without hierarchy: Informal
    structures in multiorganizational systems*. University of California Press, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Das et al. (2019) Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra,
    Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication.
    In *International Conference on Machine Learning*, pages 1538–1546\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dodds et al. (2003) Peter Sheridan Dodds, Duncan J Watts, and Charles F Sabel.
    Information exchange and the robustness of organizational networks. *Proceedings
    of the National Academy of Sciences*, 100(21):12516–12521, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foerster et al. (2016) Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas,
    and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement
    learning. In *Advances in Neural Information Processing Systems*, volume 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020) Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained
    language models better few-shot learners. *arXiv preprint arXiv:2012.15723*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garicano (2000) Luis Garicano. Hierarchies and the organization of knowledge
    in production. *Journal of Political Economy*, 108(5):874–904, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gronauer and Diepold (2022) Sven Gronauer and Klaus Diepold. Multi-agent deep
    reinforcement learning: a survey. *Artificial Intelligence Review*, pages 1–49,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2023) Xudong Guo, Daming Shi, and Wenhui Fan. Scalable communication
    for multi-agent reinforcement learning via transformer-based email mechanism.
    In *Proceedings of the Thirty-Second International Joint Conference on Artificial
    Intelligence, IJCAI-23*, pages 126–134, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. (2023a) Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with
    world model. *arXiv preprint arXiv:2305.14992*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2023b) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt:
    Augmenting frozen language models with massive tools via tool embeddings. *arXiv
    preprint arXiv:2305.11554*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
    Ran, Lingfeng Xiao, and Chenglin Wu. MetaGPT: Meta programming for multi-agent
    collaborative framework. *arXiv preprint arXiv:2308.00352*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaques et al. (2019) Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar
    Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social
    influence as intrinsic motivation for multi-agent deep reinforcement learning.
    In *International conference on machine learning*, pages 3040–3049\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020) Woojun Kim, Jongeui Park, and Youngchul Sung. Communication
    in multi-agent reinforcement learning: Intention sharing. In *International Conference
    on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konda and Tsitsiklis (1999) Vijay Konda and John Tsitsiklis. Actor-critic algorithms.
    In *Advances in Neural Information Processing Systems*, volume 12, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. CAMEL: Communicative agents for ”mind” exploration
    of large language model society. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell,
    Dana Hughes, Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration
    via large language models. *arXiv preprint arXiv:2310.10701*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli, editors, *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, pages 4582–4597, August 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, and
    Phillip Isola. Learning to ground multi-agent communication with autoencoders.
    In *Advances in Neural Information Processing Systems*, volume 34, pages 15230–15242,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang.
    Dynamic LLM-agent network: An LLM-agent collaboration framework with agent team
    optimization. *arXiv preprint arXiv:2310.02170*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe et al. (2017) Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and
    Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malone (2004) Thomas W Malone. *The future of work*. Harvard Business Review
    Press, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mandi et al. (2023) Zhao Mandi, Shreeya Jain, and Shuran Song. RoCo: Dialectic
    multi-robot collaboration with large language models. *arXiv preprint arXiv:2307.04738*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: March and Simon (1958) James G March and Herbert A Simon. *Organizations*. Wiley,
    1958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oroojlooy and Hajinezhad (2023) Afshin Oroojlooy and Davood Hajinezhad. A review
    of cooperative multi-agent deep reinforcement learning. *Applied Intelligence*,
    53(11):13677–13722, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In *Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology*, pages 1–22, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. Gorilla: Large language model connected with massive apis. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pryzant et al. (2023) Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang
    Zhu, and Michael Zeng. Automatic prompt optimization with "gradient descent" and
    beam search. *arXiv preprint arXiv:2305.03495*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puig et al. (2018) Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities
    via programs. In *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 8494–8502, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puig et al. (2021) Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong
    Liao, Joshua B. Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help:
    A challenge for social perception and human-AI collaboration. *arXiv preprint
    arXiv:2010.09890*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and
    Prateek Mittal. Visual adversarial examples jailbreak aligned large language models.
    In *The Second Workshop on New Frontiers in Adversarial Machine Learning*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radner (1993) Roy Radner. The organization of decentralized information processing.
    *Econometrica: Journal of the Econometric Society*, pages 1109–1146, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resnick et al. (2018) Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz,
    Jakob Foerster, Julian Togelius, Kyunghyun Cho, and Joan Bruna. Pommerman: A multi-agent
    playground. *arXiv preprint arXiv:1809.07124*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de
    Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS
    Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge.
    In *Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems*, pages 2186–2188, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
    in huggingface. *arXiv preprint arXiv:2303.17580*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can
    be easily distracted by irrelevant context. In *International Conference on Machine
    Learning*, pages 31210–31227\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. *arXiv preprint arXiv:2010.15980*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simon et al. (1971) Herbert A Simon et al. Designing organizations for an information-rich
    world. *Computers, communications, and the public interest*, 72:37, 1971.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. Adaplanner: Adaptive planning from feedback with language models. *arXiv
    preprint arXiv:2305.16653*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van Zandt (1999) Timothy Van Zandt. Decentralized information processing in
    the theory of organizations. In *Contemporary Economic Issues: Economic Behaviour
    and Design*, pages 125–160\. Springer, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vélez et al. (2023) Natalia Vélez, Brian Christian, Mathew Hardy, Bill D Thompson,
    and Thomas L Griffiths. How do humans overcome individual computational limitations
    by working together? *Cognitive Science*, 47(1):e13232, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka,
    Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S.
    Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury
    Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
    Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith,
    Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps,
    and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement
    learning. *Nature*, 575(7782):350–354, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Jiawei Wang, Tianyu Shi, Yuankai Wu, Luis Miranda-Moreno,
    and Lijun Sun. Multi-agent graph reinforcement learning for connected automated
    driving. In *Proceedings of the 37th International Conference on Machine Learning
    (ICML)*, pages 1–6, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Lu Wang, Lei Han, Xinru Chen, Chengchang Li, Junzhou Huang,
    Weinan Zhang, Wei Zhang, Xiaofeng He, and Dijun Luo. Hierarchical multiagent reinforcement
    learning for allocating guaranteed display ads. *IEEE Transactions on Neural Networks
    and Learning Systems*, pages 1–13, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2023) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents
    with reinforcement learning for strategic play in the werewolf game. *arXiv preprint
    arXiv:2310.18940*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. *arXiv preprint
    arXiv:2309.03409*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Bin Zhang, Hangyu Mao, Jingqing Ruan, Ying Wen, Yang Li,
    Shao Zhang, Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, et al. Controlling large
    language model-based agents for large-scale decision-making: An actor-critic approach.
    *arXiv preprint arXiv:2311.13884*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe
    Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang,
    Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. ProAgent: Building proactive
    cooperative AI with large language models. *arXiv preprint arXiv:2308.11339*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023c) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied
    agents modularly with large language models. *arXiv preprint arXiv:2307.02485*,
    2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019a) Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen
    Zhu, Zihan Zhou, Weinan Zhang, Yong Yu, Haiming Jin, and Zhenhui Li. CityFlow:
    A multi-agent reinforcement learning environment for large scale city traffic
    scenario. In *The World Wide Web Conference*, pages 3620–3624\. ACM, 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023d) Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration
    mechanisms for llm agents: A social psychology view. *arXiv preprint arXiv:2310.02124*,
    2023d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent
    reinforcement learning: A selective overview of theories and algorithms. *Handbook
    of reinforcement learning and control*, pages 321–384, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication
    in multi-agent reinforcement learning via variance based control. In *Advances
    in Neural Information Processing Systems*, volume 32, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Yi Zheng, Chongyang Ma, Kanle Shi, and Haibin Huang. Agents
    meet okr: An object and key results driven agent system with hierarchical self-collaboration
    and self-evaluation. *arXiv preprint arXiv:2311.16542*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022a) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022b) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level
    prompt engineers. *arXiv preprint arXiv:2211.01910*, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft:
    Generally capable agents for open-world enviroments via large language models
    with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Prompt Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We list the prompts of Actor, Communicator, Critic, and Coordinator as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Actor and the Communicator. ORGANIZATION_INSTRUCTION is the placeholder for
    the organization instruction prompt, either manually designed or automatically
    generated. The environment will provide text descriptions for the current GOAL,
    PROGRESS, and AVAILABLE_ACTIONS. We include the latest $12$ steps of actions as
    ACTION_HISTORY.
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/8ea4fa7ef42c8ea3c23608e17af42353.png)'
  prefs: []
  type: TYPE_IMG
- en: '![[Uncaptioned image]](img/bfa009e0abcccfdcadaf3c11a160ac8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Critic. We provide the full trajectory as the input to TRAJECTORIES. Additionally,
    ORGANIZATION_INSTRUCTION and GOAL of the current task and organization are also
    provided as an additional context.
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/c4daa3bf09d28016620353ff97a09ea3.png)'
  prefs: []
  type: TYPE_IMG
- en: Coordinator. In “Instruction examples”, we include the basic setting (goal,
    organization structure instruction), the communication cost, the number of steps
    taken, as well as the summarized information generated by the Critic (leadership
    ranking, problems, summary of the trajectory) for the Coordinator.
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/ee391781f1db561ab2af77dfcb5086c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Classifier. We feed the messages to the GPT-4 classifier and get the labels.
    The rubrics are manually written after investigating the communication logs.
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/77a4b37d04dd659b753e2de61b8107ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Appendix B Additional Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Complete list of basic experimental results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present the full results of various group settings and organization instructions
    in Appendix Table [1](#A2.T1 "Table 1 ‣ B.1 Complete list of basic experimental
    results ‣ Appendix B Additional Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams"). Here, we also include the results of 1$\times$Llama2-70B.
    Surprisingly, GPT-4 exhibits poorer leadership than Llama2-70B in this case. The
    communication costs for the teams containing Llama2-70B are much higher than those
    containing GPT-3.5-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Performance for different organization instructions. When there are
    two different kinds of LLMs in the group, Agent_1 is GPT-4, and Agent_2 is the
    other type of LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Group setting Organization instruction Time Communication cost 3$\times$2.51
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Across Task Generalizability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct experiments across the tasks to test the generalizability of the
    prompt “dynamic leadership” (Figure [9](#S4.F9 "Figure 9 ‣ 4.4 Novel Organizational
    Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams")(e)) found using Criticize-Reflect architecture on the Prepare_Afternoon_Tea
    task and report the performance in Figure [10](#A2.F10 "Figure 10 ‣ B.2 Across
    Task Generalizability ‣ Appendix B Additional Results ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams"); see Section [4.4](#S4.SS4 "4.4 Novel Organizational
    Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate in Organized
    Teams") for the complete setting and discussions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7306404984be22da0289e47d952a965e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The organized team structure with a designated leader and the novel
    structure proposed by Criticize-Reflect architecture generalized to different
    tasks. The prompt for dynamic leadership is proposed by Criticize-Reflect architecture
    on the Prepare_Afternoon_Tea task shown in Figure [8](#S4.F8 "Figure 8 ‣ 4.4 Novel
    Organizational Structures ‣ 4 Main Results ‣ Embodied LLM Agents Learn to Cooperate
    in Organized Teams")(a). The experiment was done using the 1$\times$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Emergent Cooperative Behaviors in an Organization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By investigating the messages between agents, we mainly observe the following
    cooperative behaviors, as summarized in Table [2](#A3.T2 "Table 2 ‣ Appendix C
    Emergent Cooperative Behaviors in an Organization ‣ Embodied LLM Agents Learn
    to Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Typical cooperative behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: Type Description Example Sharing information An agent shares her observations
    to others, reports her task-related progress to others, or responds to other agents’
    requests (Ex 1.)“I’m in the bathroom. There’s an unchecked $<$ (190).”
  prefs: []
  type: TYPE_NORMAL
- en: (Ex 2.) “I’ll check the cabinet in the bedroom ” Giving orders An agent gives
    orders to others, either by directly giving a command or by a polite request “I
    still need to find $<$ (371). Can you help me search the bedroom for the remaining
    item?” Asking for information An agent asks other agents about their location,
    task progress, or other information (Ex 1.) “Where are you now?”
  prefs: []
  type: TYPE_NORMAL
- en: (Ex 2.) “Any updates from the kitchen?”
  prefs: []
  type: TYPE_NORMAL
- en: '(Ex 3.) “Do we know the location of the coffeetable?” Exchanging information
    An agent shares one agent’s information to another agent Agent 3 $\to$ Agent 1:
    “ Found cupcake and juice in bedroom, plus a wine. ”;'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent 1 $\to$ Agent 2: “You don’t need to search the bedroom, the juice (380)
    is there. ”'
  prefs: []
  type: TYPE_NORMAL
- en: (Ex 2.) “There are no more wine or juice in the kitchen cabinets. We need to
    come up with another plan.”
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Ineffective Communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are also cases in which language model agents fail to communicate efficiently.
    From the messages between agents, we summarize the typical categories in Table [3](#A4.T3
    "Table 3 ‣ Appendix D Ineffective Communication ‣ Embodied LLM Agents Learn to
    Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Typical ineffective communication behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type Description Example Duplicated message The same message is said more than
    once Agent 1 $\to$ Agent 3: “Check kitchencabinet or dishwasher for wine, I’ll
    check the stove.”,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent 2 $\to$ Agent 1: “Explore the bathroom for the second wine.”'
  prefs: []
  type: TYPE_NORMAL
- en: Agent 2 $\to$ (190)”
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent 2 $\to$ Agent 2: “Continue checking the kitchen cabinets for remaining
    items.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent 2 $\to$ Agent 3: “I haven’t found any of the remaining items in the kitchen.
    Have you found any of the required items in the living room?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent 3 $\to$ Agent 2: “I haven’t explored the bathroom yet.”'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Examples of Election
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figure [11](#A5.F11 "Figure 11 ‣ Appendix E Examples of Election ‣ Embodied
    LLM Agents Learn to Cooperate in Organized Teams"), the agents vote to elect a
    new leader. We can observe behaviors such as nominations for themselves and other
    agents, voting, and consensus achievement. We find that the agents are not power-seeking
    and may give up leadership early. The agents prefer to vote for others instead
    of nominating themselves (5 times more during the whole task). The elected leader
    also does not plan to keep the position but to nominate others for the next round.
    Also, the agents’ standpoint can be easily influenced by others. The agents do
    not debate much to win the election but reach a consensus soon. For example, Agent_1
    gives up running for herself but votes for Agent_2 because of Agent_3’s support.
    Furthermore, sometimes nominations and votes are determined by hallucinations.
    For example, at step 2, Agent_2 nominates Agent_1 as he was the first one to propose
    a search strategy. However, based on the previous dialogues, Agent_1 has not proposed
    any strategy yet.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07afc797a172deb4fb9ebed1a772942d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Examples of the election of a new leader. It takes two steps to
    vote and negotiate to determine the new leader in this case. Note that Agent_3
    chooses not to send a message as the election is done and no more information
    to be shared for now. All the messages in the figure are broadcasts.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Examples of Human-AI Collaboration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conducted experiments involving a team consisting of one human player and
    two GPT-4 agents, with the human player acting as the leader. Figure [12](#A6.F12
    "Figure 12 ‣ Appendix F Examples of Human-AI Collaboration ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams") illustrates the remarkable collaboration
    between humans and AI.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/717e044952d5a44796035482d96f9c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Examples of human-AI collaboration when the human player leads two
    GPT-4 agents (Agent_2&3).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Examples of Correction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to hallucination and the limit of the dialogue history buffer, the leader
    may forget what has happened and give wrong orders. When the prompt encourages
    the agents to correct the leader when necessary by adding If the leader’s instructions
    are not right, you can correct the leader, some correction behaviors appear, as
    shown in Figure [13](#A7.F13 "Figure 13 ‣ Appendix G Examples of Correction ‣
    Embodied LLM Agents Learn to Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, the leader Agent_1 gives an unnecessary and repetitious
    instruction. Then Agent_2 corrects the leader to avoid time wasting. In the second
    example, the leader Agent_1 may have hallucinations and cannot remember what Agent_3
    is holding clearly (cupcake and wine in the message while juice and wine in the
    thoughts). Therefore, Agent_3 clarifies that she is not holding the cupcake and
    wine and shares her next plan with the leader.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3652b8f887f9732a7321148d27452ded.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Examples of correction dialogues and the corresponding thoughts.
    The prompt includes If the leader’s instructions are not right, you can correct
    the leader.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Examples of Cooperative Behaviors Classification by Humans and GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We ask a human evaluator and GPT-4 to label the dialogues into three different
    behavior categories shown in Table [4](#A8.T4 "Table 4 ‣ Appendix H Examples of
    Cooperative Behaviors Classification by Humans and GPT-4 ‣ Embodied LLM Agents
    Learn to Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Dialogues and behavior labels given by humans and GPT-4. Label 1:
    information sharing; Label 2: leadership & assistance; Label 3: Request for guidance.'
  prefs: []
  type: TYPE_NORMAL
- en: Dialogue Human GPT-4 Label 1* Label 2 Label 3 Label 1 Label 2 Label 3 Hey, where
    are you? Please let me know your location so that I can assign you a task. 0 1
    1 0 1 1 I’m currently in the bedroom where I found an unchecked cabinet. Please
    explore the livingroom and start checking for the required items. 1 1 0 1 1 0
    I’m currently in the bedroom where I found an unchecked cabinet. I haven’t found
    any of the required items yet. 1 0 0 1 0 0 Check the cabinet in the bedroom. I’ll
    check the one in the bathroom. 1 1 0 0 1 0 I haven’t found any of the required
    items yet. Did you find any in the kitchen? 1 0 1 1 0 1 I haven’t found any of
    the required items yet. Have you found any in the bathroom? 1 0 1 1 0 1 Agent
    3, I found a pudding in one of the kitchen cabinets. Please continue checking
    the other kitchen cabinets for the remaining items. 1 1 0 1 1 0 I haven’t found
    any of the required items yet. Did you find any other required items in the kitchen?
    1 0 1 1 0 1 I haven’t found any of the remaining items yet. 1 0 0 1 0 0 I’m in
    the living room searching for the remaining items. 1 0 0 1 0 0 Okay, I will keep
    checking the kitchen cabinets for the remaining items. 1 0 0 1 0 0 I’m currently
    in the living room searching for the remaining items. 1 0 0 1 0 0 I haven’t found
    any of the remaining items yet. 1 0 0 1 0 0 I’m still searching the living room.
    Have you found any of the required items? 1 0 1 1 0 1 Put items on the table once
    you find them. Keep searching your current areas. 0 1 0 0 1 0 I haven’t found
    any of the remaining items in the kitchen. Have you found any of the required
    items in the living room? 1 0 1 1 0 1 I haven’t explored the bathroom yet. Have
    you found any of the required items in the living room? 1 0 1 1 0 1 I have explored
    the bedroom and found wine, cupcake, and juice. I still need to find pudding.
    Can you help me search the bedroom for the remaining item? 1 1 1 1 1 0 I found
    the wine, cupcake, and juice in the bedroom. Agent 1 wants me to put them on the
    coffee table and then check the bathroom cabinet. 1 0 0 1 1 0 Agent 1 wants us
    to check if there’s another wine in the kitchen. 1 0 0 0 1 0
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Examples of New Prompts after Reflection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We list more prompts generated by the Criticize-Reflect architecture in Figure [14](#A9.F14
    "Figure 14 ‣ Appendix I Examples of New Prompts after Reflection ‣ Embodied LLM
    Agents Learn to Cooperate in Organized Teams").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/27f7dd8d32fc94d014704c6b6ea88f94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Examples of Prompts generated via Reflection. The first row is generated
    with the Critic, while the second row is without the Critic, where the new prompts
    are relatively vague. Note that there is no Agent Z in the team.'
  prefs: []
  type: TYPE_NORMAL
