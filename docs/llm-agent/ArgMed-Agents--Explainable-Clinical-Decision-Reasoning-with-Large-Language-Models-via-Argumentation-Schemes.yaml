- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language
    Models via Argumentation Schemes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.06294](https://ar5iv.labs.arxiv.org/html/2403.06294)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shengxin Hong¹ Liang Xiao²    Xin Zhang¹&Jianxia Chen²
  prefs: []
  type: TYPE_NORMAL
- en: ¹Detroit Green Technology Institute, Hubei University of Technology, China
  prefs: []
  type: TYPE_NORMAL
- en: ²School of Computer Science, Hubei University of Technology, China seikin.shengxinhong@gmail.com,
    lx@mail.hbut.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are two main barriers to using large language models (LLMs) in clinical
    reasoning. Firstly, while LLMs exhibit significant promise in Natural Language
    Processing (NLP) tasks, their performance in complex reasoning and planning falls
    short of expectations. Secondly, LLMs use uninterpretable methods to make clinical
    decisions that are fundamentally different from the clinician’s cognitive processes.
    This leads to user distrust. In this paper, we present a multi-agent framework
    called ArgMed-Agents, which aims to enable LLM-based agents to make explainable
    clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation
    iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism
    for modeling cognitive processes in clinical reasoning), and then constructs the
    argumentation process as a directed graph representing conflicting relationships.
    Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent
    arguments to support decision. ArgMed-Agents enables LLMs to mimic the process
    of clinical argumentative reasoning by generating explanations of reasoning in
    a self-directed manner. The setup experiments show that ArgMed-Agents not only
    improves accuracy in complex clinical decision reasoning problems compared to
    other prompt methods, but more importantly, it provides users with decision explanations
    that increase their confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) OpenAI ([2023](#bib.bib25)) have received a lot
    of attention for their human-like performance in a variety of domains. In the
    medical field especially, preliminary studies have shown that LLMs can be used
    as clinical assistants for tasks such as writing clinical texts Nayak et al. ([2023](#bib.bib22)),
    providing biomedical knowledge Singhal et al. ([2022](#bib.bib33)) and drafting
    responses to patients’ questions Ayers et al. ([2023](#bib.bib1)). However, the
    following barriers to building an LLM-based clinical decision-making system still
    exist: (i) LLMs still struggle to provide secure, stable answers when faced with
    highly complex clinical reasoning tasks Pal et al. ([2023](#bib.bib26)). (ii)
    There is a perception that LLMs use unexplainable methods to arrive at clinical
    decisions (known as black boxes), which may have led to user distrust Eigner and
    Händler ([2024](#bib.bib11)).'
  prefs: []
  type: TYPE_NORMAL
- en: To address these barriers, exploring the capabilities of LLMs in argumentative
    reasoning is a promising direction. Argumentation is a means of conveying a compelling
    point of view that can increase user acceptance of a position. Its considered
    a fundamental requirement for building Human-Centric AI Dietz et al. ([2022](#bib.bib9)).
    As computational argumentation has become a growing area of research in Natural
    Language Processing (NLP) Dietz et al. ([2021](#bib.bib8)), researchers have begun
    to apply argumentation to a wide range of clinical reasoning applications, including
    analysis of clinical discussions Qassas et al. ([2015](#bib.bib29)), clinical
    decision making Hong et al. ([2023](#bib.bib14)); Zeng et al. ([2020](#bib.bib43));
    Sassoon et al. ([2021](#bib.bib30)), address clinical conflicting Čyras et al.
    ([2018](#bib.bib6)). In the recent past, some work has assessed the ability of
    LLMs in argumentation reasoning Chen et al. ([2023](#bib.bib4)); Castagna et al.
    ([2024](#bib.bib3)) or non-monotonic reasoning Xiu et al. ([2022](#bib.bib42)).
    Although LLMs show some potential for computational argumentation, more results
    show that LLMs perform poorly in logical reasoning tasks Xie et al. ([2024](#bib.bib41)),
    and better ways to utilise LLMs for non-monotonic reasoning tasks need to be explored.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, LLM as agent studies have been surprisingly successful Zhang et al.
    ([2023](#bib.bib44)); Wang et al. ([2023](#bib.bib38)); Shi et al. ([2024](#bib.bib32)).
    These methods use LLMs as computational engines for autonomous agents, and optimise
    the reasoning, planning capabilities of LLMs through external tools (e.g. symbolic
    solvers, APIs, retrieval tools, etc.) Pan et al. ([2023](#bib.bib27)); Shi et
    al. ([2024](#bib.bib32)), multi-agent interactions Tang et al. ([2024](#bib.bib35))
    and novel algorithmic frameworks Gandhi et al. ([2023](#bib.bib13)). Through this
    design, LLMs agents can interact with the environment and generate action plans
    through intermediate reasoning steps that can be executed sequentially to obtain
    an effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by these concepts, we present ArgMed-Agents, a multi-agent framework
    designed for explainable clinical decision reasoning. We formalised the cognitive
    process of clinical reasoning using an argumentation scheme for clinical decision
    (ASCD) as a prompt strategy for interactive reasoning by LLM agents. There are
    three types of agents in ArgMed-Agents: the Generator, the Verifier, and the Reasoner.
    the Generator generates arguments to support clinical decisions based on the argumentation
    scheme; the Verifier checks the arguments for legitimacy based on the critical
    question, and if not legitimate, it asks the Generator to generate attack arguments;
    Reasoner is a symbolic solver that identifies reasonable, non-contradictory arguments
    in the resulting directed argumentation graph as decision support.'
  prefs: []
  type: TYPE_NORMAL
- en: In our method, we do not expect every proposed argument or detection of Generator
    or Verifier to be correct, instead we consider their generation as a assumption.
    The LLM agents are induced to recursively iterate in a self-argumentative manner
    through the prompt strategy , while the newly proposed assumptions always contradict
    the old ones, and eventually the Reasoner eliminates unreasonable assumptions
    and identifies coherent arguments, leading to consistent reasoning results. ArgMed-Agents
    enables LLMs to explain its own outputs in terms of self-cognitive profiling by
    modelling their own generation as a prompt for question recursion. The generative
    process derives the attack relations of the arguments are determined based on
    the ASCD prompt strategy. For example, when there exist two decisions as arguments
    $a$, they attack each other based on the decision exclusivity defined in ASCD.
  prefs: []
  type: TYPE_NORMAL
- en: Our experiment was divided into two parts, evaluating accuracy and explainability
    of ArgMed-Agents clinical reasoning, respectively. First, we conducted experiments
    on two datasets, including MedQA Jin et al. ([2020](#bib.bib17)) and PubMedQA
    Jin et al. ([2019](#bib.bib16)). To better align with real-world application scenarios,
    our study focused on a zero-shot setting. Second, we used predictability and traceability
    as measures of explainability by manually assessing whether the clinical reasoning
    process of ArgMed-Agents is meaningful in terms of explanation for users. The
    results show that ArgMed-Agents achieves better performance in both accuracy and
    explainability compared to direct generation and Chain of Thought (CoT).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Abstract Argumentation Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Abstract Argumentation (AA) frameworks Dung ([1995](#bib.bib10)) are pair $\langle\mathcal{A},\mathcal{R}\rangle$.
    On top of that, Dung defines some notions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\exists a\in\mathcal{A}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of arguments is conflict-free if there is no attack between its arguments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to decide whether a single parameter can be accepted or whether multiple
    parameters can be accepted at the same time, the argumentation system allows for
    the use of various semantics to compute the set of parameters (called extensions).
    For example, Given an AA framework $\langle\mathcal{A},\mathcal{R}\rangle$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Argumentation Scheme
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concept of Argumentation Scheme (AS) originated within the domain of informal
    logic, stemming from the seminal works of Walton et al. ([2008](#bib.bib36));
    Walton ([1996](#bib.bib37)). Argumentation Scheme serves as a semi-formalized
    framework for capturing and analyzing human reasoning patterns. Formally defined
    as $AS=\langle P,c,V\rangle$). A pivotal aspect of the argumentation scheme is
    the delineation of Critical Questions (CQs) pertinent to AS. Failure to address
    them prompts a challenge to both the premises and the conclusion posited by the
    scheme. Consequently, the role of CQs is to instigate argument generation; when
    an AS is contested, it engenders the formulation of a counter-argument in response
    to the initial AS. This iterative process culminates in the construction of an
    attack argument graph, facilitating a nuanced understanding of argumentative dynamics.
    Figure [1](#S3.F1 "Figure 1 ‣ 3 Argumentation Scheme for Clinial Decision ‣ ArgMed-Agents:
    Explainable Clinical Decision Reasoning with Large Language Models via Argumentation
    Schemes") illustrates three templates of argumentation schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Argumentation Scheme for Clinial Decision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the past, numerous studies Oliveira et al. ([2018](#bib.bib24)); Sassoon
    et al. ([2021](#bib.bib30)); Qassas et al. ([2015](#bib.bib29)) have explored
    the application of argumentation in the clinical domain. In this section, we provide
    a summary of these endeavors, focusing on the development of Argumentation Schemes
    for Clinical Decision (ASCD). ASCD encapsulate various argumentation scheme tailored
    for clinical decision-making processes and the reasoning process. Additionally,
    we propose to refine and adapt these schemes to enhance their suitability for
    LLM. ASCD consists of three argumentation schemes which are Argumentation Scheme
    for Decision (ASD), Argumentation Scheme for Side Effect (ASSE) and Argumentation
    Scheme for Better Decision (ASBD). Figure [1](#S3.F1 "Figure 1 ‣ 3 Argumentation
    Scheme for Clinial Decision ‣ ArgMed-Agents: Explainable Clinical Decision Reasoning
    with Large Language Models via Argumentation Schemes") includes the components
    of the three argumentation schemes and the derivations between them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bdec46be281a957e88faa4649cb70708.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The derivation rules between the argumentation schemes in ASCD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ASCD formalizes the decision-making process for clinicians. Clinical decision
    reasoning begins with ASD, where decisions are related to clinical goals. Formally,
    in Figure [1](#S3.F1 "Figure 1 ‣ 3 Argumentation Scheme for Clinial Decision ‣
    ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models
    via Argumentation Schemes") we model the derivation of CQs. When the CQs are rejected,
    new ASs are generated as arguments to refute the first proposed AS based on the
    derivation relation. It is worth noting that when a CQ without a derivation relationship
    (e.g., ASD.CQ1, ASSE.CQ1) is rejected, the AS will be refuted by itself (on the
    grounds of the CQ’s answer). We illustrate how to reason using ASCD with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joey is a 50 year old male patient who has suffered a stroke and has high blood
    pressure. For Joey, the goal is to control his blood pressure;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, we represent the initial ASD reasoning result as $ASD_{1}(joey,control\_blood\_pressure,ACEI)\rightarrow
    ACEI$. Notably, since we set exclusivity between ASD, in order for ASD.CQ1 and
    ASD.CQ3 to be valid, when these two CQs are rejected, that ASD first refutes itself
    (forming a closed loop) and then generates a new ASD.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f669a371d121b588b822a291d8c84bea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An example from the MedQA USMLE dataset, with the entire process
    of ArgMed-Agents reasoning about the clinical problem. Notably, the letters in
    the argumentation framework correspond to the serial numbers of the four generators
    on the right, representing the premises and conclusion generated by that generator.
    In the argumentation framework, the red nodes ($A$) represent arguments in support
    of the beliefs.'
  prefs: []
  type: TYPE_NORMAL
- en: '4 ArgMed-Agents: a Multi-LLM-Agents Framework'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [3](#S3 "3 Argumentation Scheme for Clinial Decision ‣ ArgMed-Agents:
    Explainable Clinical Decision Reasoning with Large Language Models via Argumentation
    Schemes"), we discussed Argumentation Schemes for clinical decision(ASCD). ASCD
    is a semi-formal reasoning template for clinical decision making by defining the
    logical structure and the reasoning mechanism in the clinical reasoning process.
    However, the clinical decision support systems based on argumentation scheme in
    the past Sassoon et al. ([2021](#bib.bib30)); Qassas et al. ([2015](#bib.bib29))
    have often been implemented through knowledge-based approaches, which rely on
    expert knowledge to build knowledge bases and rule. In this section, we propose
    a multi-agent framework called ArgMed-Agents, which supports the seamless integration
    of prompt strategy designed based on ASCD into agent interactions. Our approach
    enhances LLMs to be able to perform explainable clinical decision reasoning without
    the need for expert involvement in knowledge encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 ASCD-based Multi-agent Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ArgMed-Agents framework includes three distinct types of LLMs agent:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generator(s): Instantiate the AS according to the current situation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verifier: Whenever Generator generates an instantiation of an AS, Verifier
    checks the accuracy of the instantiation of that AS via CQ. When the CQ validation
    is rejected, return to the generator the reason why the CQ rejected, so that the
    generator generates a new AS based on the derivation rules of that CQ. The process
    iterates until no more new AS are generated.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reasoner: A symbolic solver for computational argumentation. At the end of
    the iteration, the arguments presented by the generator constitute a complete
    argumentation framework, and it identifies a subset of coherent and reasonable
    arguments within that argumentation framework.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We convert ASCD into a step-by-step reasoning interaction protocol as a prompt
    strategy and define the interaction behavior between LLM agents based on ASCD
    prompt strategy, which describes the specification of the interaction behavior
    between different types of agents under the ASCD reasoning mechanism. Figure [2](#S3.F2
    "Figure 2 ‣ 3 Argumentation Scheme for Clinial Decision ‣ ArgMed-Agents: Explainable
    Clinical Decision Reasoning with Large Language Models via Argumentation Schemes")
    depicts how ArgMed-Agents interacts. In this example, $Generator_{A}$ would attack
    itself. The Verifier facilitates an iterative process of mutual debate between
    Generators, which forms an argumentation framework that can be solved by the symbolic
    Reasonner. We provide a formal description of the interaction process in Appendix
    A.'
  prefs: []
  type: TYPE_NORMAL
- en: In ArgMed-Agents, we do not expect single generation or single verification
    to be correct. ArgMed-Agents uses generation as a assumption to recursively prompt
    the model with critical questions, identifying conflicting and erroneous arguments
    in an iterative process. Ultimately, such mutually attacking arguments are converted
    into a formal framework that solves for a subset of reasonably coherent arguments
    via Reasoner.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical motivation for our method stems from non-monotonic logic, logical
    intuition and cognitive clinical. Studies have shown that LLM performs reasonably
    well for simple reasoning, single-step reasoning problems, however, as the number
    of reasoning steps rises, the rate of correct reasoning decreases in a catastrophic
    manner Creswell et al. ([2022](#bib.bib5)). Thus, we consider that LLM is primed
    with rational intuition, but lacks true logical reasoning ability. In light of
    this, ASCD Prompt Stratygy guides LLM in a recursive form to generate a series
    of casual reasoning steps as a tentative conclusion, with any further evidence
    withdrawing their conclusion. The process ultimately leads to a directed graph
    representing the disputed relationships. In addition this, agents with different
    LLMs roles within ArgMed-Agents interact collaboratively with each other in a
    formalized process consistent with clinical reasoning (i.e., following the ASCD
    prompt strategy). This design not only facilitates LLMs to engage in clinical
    reasoning in a critical-thinking manner, which in turn maintains a cognitive processes
    consistent with clinicians to improve the explainability of decisions, but also
    allows for the effective highlighting of some implicit knowledge in LLMs that
    cannot be readily accessed through traditional prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Explainable Argumentative Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ArgMed-Agent obtains a complete argument graph by iterating through the ASCD
    Prompt Strategy. In this section, we describe how Reasoner reasons and explains
    decisions through these arguments. First, we map the reasoning results of ASCD
    to the AA framework.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An AA framework for ASCD is a pair $\langle\mathcal{A},\mathcal{R}\rangle$
    such that:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\exists arg=\langle P,c,V\rangle\in\mathcal{A}$ is constituted by the argumentation
    schemes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{A}=Args_{d}(\mathcal{A})\cup Args_{b}(\mathcal{A})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In Definition [1](#Thmdefinition1 "Definition 1\. ‣ 4.2 Explainable Argumentative
    Reasoning ‣ 4 ArgMed-Agents: a Multi-LLM-Agents Framework ‣ ArgMed-Agents: Explainable
    Clinical Decision Reasoning with Large Language Models via Argumentation Schemes"),
    arguments in support of decisions $Args_{d}(\mathcal{A})$ correspond to ASSE and
    ASBD in the argumentation scheme. These two types of arguments play different
    roles; arguments in support of decisions build on beliefs and goals and try to
    justify choices, while arguments in support of beliefs always try to undermine
    decision arguments. Therefore, based on the modeling of ASCD, we make the following
    setup for the attack relation between these two types of arguments: first, arguments
    in support of different decisions are in conflict with each other, which is consistent
    with our previous description of decisions being exclusive in ASCD. Second, arguments
    in support of decisions are not allowed to attack arguments in support of beliefs,
    which is dictated by the modeling of ASCD, as can be seen in Figure [1](#S3.F1
    "Figure 1 ‣ 3 Argumentation Scheme for Clinial Decision ‣ ArgMed-Agents: Explainable
    Clinical Decision Reasoning with Large Language Models via Argumentation Schemes"),
    where ASSE and ASBD are always trying to disprove ASD, while the converse does
    not work. Formalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Given an AA framework for ASCD $\langle\mathcal{A},\mathcal{R}\rangle$ such
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\forall arg_{1},arg_{2}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\nexists(arg_{1},arg_{2})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we use the preferred semantics in the AA framework to illustrate how the
    Reasoner Agent selects the set of possible admissible arguments to support clinical
    decision.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an AA framework for ASCD $\langle\mathcal{A},\mathcal{R}\rangle$.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An AA framework for ASCD is given below such that $Args_{d}(\mathcal{A})=\{A,B,C\}$
    are optional because they are supported by preferred extension.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS2.p4.pic1" class="ltx_picture" height="254.32" overflow="visible"
    version="1.1" width="357.03"><g transform="translate(0,254.32) matrix(1 0 0 -1
    0 0) translate(-152.35,0) translate(0,225.48)"><g stroke="#000000" fill="#000000"
    stroke-width="0.75pt"><g transform="matrix(1.0 0.0 0.0 1.0 205.75 -111.3)" fill="#000000"
    stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 87.54)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 92.38)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="20.56"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">A</foreignobject></g></g></g></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 156.96 -220.87)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 171.82)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 176.66)"><g class="ltx_tikzmatrix_col ltx_nopad_l
    ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="20.42" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g></g></g></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 252.62 -220.87)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 171.82)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 176.66)"><g class="ltx_tikzmatrix_col ltx_nopad_l
    ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="25.49" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">C</foreignobject></g></g></g></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 240.53 -36.67)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 30.13)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 34.97)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="17.75" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">E</foreignobject></g></g></g></g> <g transform="matrix(1.0
    0.0 0.0 1.0 172.98 -35.18)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 28.98)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 33.82)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="17.75" height="16.6" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">D</foreignobject></g></g></g></g> <g transform="matrix(1.0
    0.0 0.0 1.0 306.98 -197.58)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 165.08)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 169.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><foreignobject width="197.79" height="113.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Admissible Sets: $\displaystyle\{B,D,E\},$</foreignobject></g></g></g></g></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, we present a proposition to justify the Reasoner agent’s use
    of preferred semantics for decision making.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an AA framework for ASCD $\langle\mathcal{A},\mathcal{R}\rangle$ where,
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\exists a\in Args_{d}(\mathcal{A})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\exists a\in Args_{d}(\mathcal{E})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Proposition [1](#Thmproposition1 "Proposition 1\. ‣ 4.2 Explainable Argumentative
    Reasoning ‣ 4 ArgMed-Agents: a Multi-LLM-Agents Framework ‣ ArgMed-Agents: Explainable
    Clinical Decision Reasoning with Large Language Models via Argumentation Schemes")
    states that using preferred semantics on the AA framework for ASCD always selects
    extensions that contain the only acceptable decision, which is consistent with
    our expected reasoning. Meanwhile, arguments from $Args_{b}(\mathcal{E})$ Fan
    and Toni ([2015](#bib.bib12)). The proof of the proposition proceeds as follows:
    We first prove that there will be only one decision in the preferred extension.
    According to Definition [2](#Thmdefinition2 "Definition 2\. ‣ 4.2 Explainable
    Argumentative Reasoning ‣ 4 ArgMed-Agents: a Multi-LLM-Agents Framework ‣ ArgMed-Agents:
    Explainable Clinical Decision Reasoning with Large Language Models via Argumentation
    Schemes"), we know that decisions are exclusive, so different decisions do not
    appear in the same admissible set. Secondly, we prove that the preferred extension
    will always contain a argument in support of decision. According to the modeling
    of ASCD as well as Definition [2](#Thmdefinition2 "Definition 2\. ‣ 4.2 Explainable
    Argumentative Reasoning ‣ 4 ArgMed-Agents: a Multi-LLM-Agents Framework ‣ ArgMed-Agents:
    Explainable Clinical Decision Reasoning with Large Language Models via Argumentation
    Schemes"), it can be seen that arguments in support of decision (ASD) are built
    on top of arguments in support of beliefs (ASSE and ASBD), i.e., the inclusion
    of ASD arguments does not lead to the ASSE and ASBD arguments becoming unacceptable.
    Therefore, the set is maximized when the acceptable set includes pro-decision
    arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an AA framework for ASCD $\langle\mathcal{A},\mathcal{R}\rangle$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clinical reasoning errors are discussed in Tang et al. ([2024](#bib.bib35)).
    They consider that most clinical reasoning errors in LLMs are due to confusion
    about domain knowledge. On the other hand, research by Singhal et al. ([2022](#bib.bib33))
    points out that LLMs may produce compelling misinformation about medical treatment,
    so it is crucial to recognize this illusion, which is difficult for humans to
    detect. For this purpose, we define the relevant mechanisms for identifying clinical
    reasoning errors in Reasoner as follows: When any decision in the AA framework
    is not accepted, we consider there are errors in the reasoning by ArgMed-Agents.
    This mechanism assists ArgMed-Agents in identifying the capability boundaries
    of LLMs when their knowledge reserves are insufficient to address certain issues.This
    helps ArgMed-Agents avoid the risks associated with adopting erroneous decisions
    and achieving more robust and safe clinical reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we demonstrate the potential of ArgMed-Agents in clinical decision
    reasoning by evaluating the accuracy and explainability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We implemented Generator and Verifier in ArgMed-Agents using the APIs GPT-3.5-turbo
    and GPT-4 provided by OpenAI OpenAI ([2023](#bib.bib25)), and according to our
    setup, the LLM Agents are implemented with the same LLM and different few-shot
    prompts. Each agent is configured with specific parameters: the temperature is
    set to 0.0, as well as a dialogue limit of 4, indicating the maximum number of
    decisions allowed for ArgMed-Agents to generate, which is to prevent the agents
    from getting into loops with each other. On the other hand, we using python3 to
    implemente a symbolic solver of abstract argumentation framework as Reasoner.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following two datasets were used to assess the accuracy of ArgMed-Agents
    clinical reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MedQA Jin et al. ([2020](#bib.bib17)):Answering multiple-choice questions derived
    from the United States Medical License Exams (USMLE). This dataset is sourced
    from official medical board exams and encompasses questions in English, simplified
    Chinese, and traditional Chinese. The total question counts for each language
    are 12,723, 34,251, and 14,123, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PubMedQA Jin et al. ([2019](#bib.bib16)): A biomedical question and answer
    (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer
    yes/no/maybe research questions using the corresponding abstracts (e.g., Do preoperative
    statins reduce atrial fibrillation after coronary artery bypass graft surgery?).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We randomly selected 300 examples in each dataset for our experiments. We set
    the baseline in our experiments to compare with gpt direct generation and Chain
    of Thought(CoT) Wei et al. ([2023](#bib.bib39)). It is worth noting that we focussed
    on evaluating the performance of ArgMed-Agents on clinical decision reasoning,
    however, there were some biomedical general knowledge quiz-type questions in MedQA
    and PubMedQA, and so we intentionally excluded this part of the questioning in
    selecting the example.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | MedQA | PubMedQA |'
  prefs: []
  type: TYPE_TB
- en: '|  | Direct | 52.7 | 68.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | CoT | 48.0 | 71.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ArgMed-Agents | 62.1 | 78.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Direct | 67.8 | 72.9 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | CoT | 71.4 | 77.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ArgMed-Agents | 83.3 | 81.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results of the accuracy of various clinical decision reasoning methods
    on MedQA and PubMedQA datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5.1 Accuracy ‣ 5 Experiments ‣ ArgMed-Agents: Explainable
    Clinical Decision Reasoning with Large Language Models via Argumentation Schemes")
    shows the accuracy results of MedQA and PubMedQA. We compared ArgMed-Agents to
    several baselines in direct generation and CoT settings. Notably, both GPT-3.5-turbo
    and GPT4 models showed that our proposed ArgMed-Agents improved accuracy on a
    clinical decision reasoning task compared to a baseline such as CoT. This result
    demonstrates the effectiveness of ArgMed-Agents to enhance the clinical reasoning
    performance of LLMs. Interestingly, after we repeated the experiment several times,
    we found that the introduction of CoT sometimes leads to a surprising drop in
    performance. The reason for this may be that when LLMs experience hallucinatory
    phenomena, the use of CoT will amplify such hallucinations indefinitely. In contrast,
    our approach of using mutual argumentation iterations between multiple agents
    effectively mitigates this problem. We analysed the data from our experiments
    and found that each AS’s CQ1 (i.e., asking the Verifier whether there is evidence
    to support the argument made by the Generator) acts as a hallucination detector
    in the reasoning process. On the other hand, we analysed the examples in which
    both Direct Generation, CoT and ArgMed-Agents reasoned wrongly. We found that
    ArgMed-Agents can report 76% of errors in these examples according to Reasoner’s
    definition [4](#Thmdefinition4 "Definition 4\. ‣ 4.2 Explainable Argumentative
    Reasoning ‣ 4 ArgMed-Agents: a Multi-LLM-Agents Framework ‣ ArgMed-Agents: Explainable
    Clinical Decision Reasoning with Large Language Models via Argumentation Schemes").
    This provides initial evidence that ArgMed-Agents not only has higher accuracy
    compared to baseline, but also that it is safer for clinical decision reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Human Evaluation for Explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main focus of Explainable AI (XAI) is usually the reasoning behind decisions
    or predictions made by AI that become more understandable and transparent. In
    the context of artificial intelligence, Explainability is defined as follows ISO
    ([2020](#bib.bib15)):'
  prefs: []
  type: TYPE_NORMAL
- en: …level of understanding how the AI-based system came up with a given result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on this criterion, we define two measures of LLMs’ explanability:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'predictability(Pre.): Can the explanations given by LLMs help users predict
    LLM decisions?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'proof-based traceability(Tra.): Can the model reason about the correct decision
    based on the correct path of inference (i.e. does the path of inference as an
    explanation have relevance to the LLM’s decision)?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Method | Pre. | Tra. | $\%$ Arg. |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 0.63 | 2.8 | 59.5$\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| ArgMed-Agents | 0.91 | 4.2 | 87.5$\%$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Human evaluation result on 20 examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the evaluation of predictability, our experimental setup is as follows:
    we recorded the inputs and the reasoning process (e.g., the complete dialogue
    between Generator and Verifier in ArgMed-Agents and the corresponding argumentation
    framework). We invited 50 undergraduate and graduate students in medical-related
    disciplines to answer questions given only the inputs to the LLMs as well as the
    explanations (four questions were distributed to each individual), and counted
    to what extent they were able to predict the decisions of the LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our team has produced a fully functional knowledge-based clinical decision
    support system at an early stage that can be used to assist in treatment decisions
    for complex diseases such as cancer, neurological disorders, and infectious diseases.
    Figure [3](#S5.F3 "Figure 3 ‣ 5.2 Human Evaluation for Explainability ‣ 5 Experiments
    ‣ ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models
    via Argumentation Schemes") illustrates this knowledge-based CDSS, consisting
    of computer-interpretable guidelines in the form of Resource Description Framework
    (RDF) Klyne and Carroll ([2004](#bib.bib20)) and the corresponding inference engine.
    We use the knowledge-based CDSS’s process of performing reasoning on the 20 examples
    in MedQA as a logical criterion for measuring the proof-based traceability of
    LLMs. We manually evaluated the consistency of ArgMed-Agents’ reasoning paths
    with logical criteria and the relevance of explanations to decision making, scoring
    each example on a scale of one to five. CoT performance was also used as a comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0056aa6c5c303ca1364daad6e45389f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A screenshot of the knowledge-based CDSS for diagnosing depression,
    where prismatic nodes represent enquiry, circular nodes represent decision, and
    square nodes represent action.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [2](#S5.T2 "Table 2 ‣ 5.2 Human Evaluation for Explainability ‣ 5
    Experiments ‣ ArgMed-Agents: Explainable Clinical Decision Reasoning with Large
    Language Models via Argumentation Schemes"), it is shown that ArgMed-Agents outperforms
    CoT in both measures of explainability. Specifically, in the experiment evaluating
    predictability, 50 participants answered the question a total of 200 times, of
    which participants succeeded 91 out of 100 times in predicting ArgMed-Agents decisions,
    compared to 63/100 times in predicting the CoT. On the other hand, most of the
    reasoning nodes of the knowledge-based CDSS can be found to correspond in the
    reasoning process of ArgMed-Agents, whereas the reasoning process of the CoT method
    to arrive at an answer is often logically incomplete. Interestingly, we found
    that the reasoning of the knowledge-based CDSS on many examples can be regarded
    as a reasoning subgraph of ArgMed-Agents, probably because ArgMed-Agents traverses
    all the reasoning paths in a randomly generated manner, and thus the reasoning
    graph includes not only the reasoning paths of the correct decisions, but also
    the explanations of the incorrect decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 LLM-based Clinical Decision Support System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Extensive research highlights the potential for LLMs to be used in medicine
    Bao et al. ([2023](#bib.bib2)); Nori et al. ([2023](#bib.bib23)); Jin et al. ([2023](#bib.bib18)).
    However, LLMs still struggle to make safe and trustworthy decisions when they
    encounter clinical decisions that require complex medical expertise and good reasoning
    skills Singhal et al. ([2022](#bib.bib33)). Therefore, much work has begun to
    focus on ways to enhance clinical reasoning in LLMs. Tang et al. ([2024](#bib.bib35))
    proposes a Multi-disciplinary Collaboration (MC) framework that utilises LLM-based
    agents in a role-playing environment that engages in collaborative multi-round
    discussions until consensus is reached. Despite the results achieved, the method
    is unable to formalise the iterative results in such a way as to enhance the inference
    performance of the LLM using inference tools. Savage et al. ([2024](#bib.bib31))
    proposes a method that uses diagnostic reasoning prompts to improve clinical reasoning
    abilities and interpretability in LLM. However, their approach does not focus
    on critical reasoning in clinical decision-making, which allows LLM to generate
    explanations of why one decision is ”good” but not why others were not chosen.
    In addition to this, the work such as Singhal et al. ([2023](#bib.bib34)); Li
    et al. ([2023](#bib.bib21)) fine-tuned LLMs using extensive datasets collected
    from the medical and biomedical literature. Unlike our approach, ours focuses
    more on exploiting the latent medical knowledge inherent in LLMs to improve their
    reasoning ability in a training-free environment.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Logical Reasoning with LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An extensive body of research has been dedicated to utilizing symbolic systems
    to augment reasoning, which includes code environments, knowledge graphs, and
    formal theorem provers Pan et al. ([2023](#bib.bib27), [2024](#bib.bib28)); Wu
    et al. ([2023](#bib.bib40)). In the study by Jung et al. Jung et al. ([2022](#bib.bib19)),
    reasoning is constructed as a satisfiability problem of its logical relations
    through inverse causal reasoning, with consistent reasoning enhanced using SAT
    solvers to improve the reasoning abilities of Large Language Models (LLMs). Zhang
    et al.’s work Zhang et al. ([2023](#bib.bib44)) employs cumulative reasoning to
    decompose tasks into smaller components, thereby simplifying the problem-solving
    process and enhancing efficiency. Xiu et al. Xiu et al. ([2022](#bib.bib42)) delve
    into the non-monotonic reasoning capabilities of LLMs. However, despite the initial
    promise exhibited by LLMs, their performance falls short in terms of generalization
    and proof-based traceability, with a significant decline in performance observed
    with increasing depth of reasoning. In line with our study, some recent works
    have started to explore the potential for argumentative reasoning in LLMs Chen
    et al. ([2023](#bib.bib4)), aiming to enhance LLMs argumentative reasoning abilities
    de Wynter and Yuan ([2023](#bib.bib7)); Castagna et al. ([2024](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, We propose a novel medical multi-agent interaction framework called
    ArgMed-Agents, which inspires agents of different roles to iterate through argumentation
    and critical questioning, systematically generating an abstract argumentation
    framework. Then, using a Reasoner agent to identify a set of reasonable and coherent
    arguments in this framework as decision support. The experimental results indicate
    that, compared to different baselines, using ArgMed-Agent for clinical decision-making
    reasoning achieves greater accuracy and provides inherent explanations for its
    inferences.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the success of ArgMed-Agents, there are still some limitations. In particular,
    abstract arguments alone may struggle with clinical reasoning tasks that numerical
    calculations or probabilistic reasoning. Therefore, future research will focus
    on enhancing the capabilities of ArgMed-Agents to address these challenges. One
    promising avenue involves exploring the integration of value-based argumentation
    or probabilistic argumentation techniques. Our goal is to provide healthcare professionals
    with powerful tools to enhance their decision-making process and ultimately improve
    patient outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no ethical issues.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ayers et al. [2023] John W. Ayers, Adam Poliak, Mark Dredze, Eric C. Leas, Zechariah
    Zhu, Jessica B. Kelley, Dennis J. Faix, Aaron M. Goodman, Christopher A. Longhurst,
    Michael Hogarth, and Davey M. Smith. Comparing Physician and Artificial Intelligence
    Chatbot Responses to Patient Questions Posted to a Public Social Media Forum.
    JAMA Internal Medicine, 183(6):589–596, 06 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. [2023] Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu,
    Cheng Zhong, Jiajie Peng, Xuanjing Huang, and Zhongyu Wei. Disc-medllm: Bridging
    general large language models and real-world medical consultation, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Castagna et al. [2024] Federico Castagna, Nadin Kokciyan, Isabel Sassoon, Simon
    Parsons, and Elizabeth Sklar. Computational argumentation-based chatbots: a survey,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023] Guizhen Chen, Liying Cheng, Luu Anh Tuan, and Lidong Bing.
    Exploring the potential of large language models in computational argumentation,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creswell et al. [2022] Antonia Creswell, Murray Shanahan, and Irina Higgins.
    Selection-inference: Exploiting large language models for interpretable logical
    reasoning, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Čyras et al. [2018] Kristijonas Čyras, Brendan Delaney, Denys Prociuk, Francesca
    Toni, Martin Chapman, Jesús Domínguez, and Vasa Curcin. Argumentation for explainable
    reasoning with conflicting medical recommendations. CEUR Workshop Proceedings,
    2237, January 2018. 2018 Joint Reasoning with Ambiguous and Conflicting Evidence
    and Recommendations in Medicine and the 3rd International Workshop on Ontology
    Modularity, Contextuality, and Evolution, MedRACER + WOMoCoE 2018 ; Conference
    date: 29-10-2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de Wynter and Yuan [2023] Adrian de Wynter and Tommy Yuan. I wish to have an
    argument: Argumentative reasoning in large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dietz et al. [2021] Emmanuelle Dietz, Antonis Kakas, and Loizos Michael. Computational
    argumentation and cognition, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dietz et al. [2022] Emmanuelle Dietz, Antonis Kakas, and Loizos Michael. Argumentation:
    A calculus for human-centric ai. Frontiers in Artificial Intelligence, 5, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dung [1995] Phan Minh Dung. On the acceptability of arguments and its fundamental
    role in nonmonotonic reasoning, logic programming and n-person games. Artificial
    Intelligence, 77(2):321–357, 1995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigner and Händler [2024] Eva Eigner and Thorsten Händler. Determinants of llm-assisted
    decision-making, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan and Toni [2015] Xiuyi Fan and Francesca Toni. On computing explanations
    in argumentation. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial
    Intelligence, AAAI’15, page 1496–1492\. AAAI Press, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gandhi et al. [2023] Kanishk Gandhi, Dorsa Sadigh, and Noah D. Goodman. Strategic
    reasoning with language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. [2023] Shengxin Hong, Liang Xiao, and Jianxia Chen. An interaction
    model for merging multi-agent argumentation in shared clinical decision making.
    In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),
    pages 4304–4311, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ISO [2020] ISO, IEC: AWI TS 29119-11: Software and systems engineering - software
    testing - part 11: Testing of AI systems. Technical report, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2019] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen,
    and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2020] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi
    Fang, and Peter Szolovits. What disease does this patient have? a large-scale
    open domain question answering dataset from medical exams, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2023] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt:
    Augmenting large language models with domain tools for improved access to biomedical
    information, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jung et al. [2022] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra
    Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent
    reasoning with recursive explanations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klyne and Carroll [2004] Graham Klyne and Jeremy J. Carroll. Resource description
    framework (rdf): Concepts and abstract syntax. W3C Recommendation, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nayak et al. [2023] Anupama Nayak, Michael S Alkaitis, Karthik Nayak, Martin
    Nikolov, Kevin P Weinfurt, and Kevin Schulman. Comparison of history of present
    illness summaries generated by a chatbot and senior internal medicine residents.
    JAMA Intern Med, 183(9):1026–1027, Sep 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nori et al. [2023] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard
    Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu,
    Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin,
    Naoto Usuyama, Chris White, and Eric Horvitz. Can generalist foundation models
    outcompete special-purpose tuning? case study in medicine, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oliveira et al. [2018] Tiago Oliveira, Jérémie Dauphin, Ken Satoh, Shusaku Tsumoto,
    and Paulo Novais. Argumentation with goals for clinical decision support in multimorbidity.
    In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent
    Systems, 2018. 2031–2033.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pal et al. [2023] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.
    Med-halt: Medical domain hallucination test for large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2023] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang
    Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful
    logical reasoning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2024] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap.
    IEEE Transactions on Knowledge and Data Engineering, page 1–20, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qassas et al. [2015] Malik Al Qassas, Daniela Fogli, Massimiliano Giacomin,
    and Giovanni Guida. Analysis of clinical discussions based on argumentation schemes.
    Procedia Computer Science, 64:282–289, 2015. Conference on ENTERprise Information
    Systems/International Conference on Project MANagement/Conference on Health and
    Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015
    October 7-9, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sassoon et al. [2021] Isabel Sassoon, Nadin Kökciyan, Sanjay Modgil, and Simon
    Parsons. Argumentation schemes for clinical decision support. Argument and Computation,
    12(3):329–355, November 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Savage et al. [2024] Thomas Savage, Abhishek Nayak, Robert Gallo, and et al.
    Diagnostic reasoning prompts reveal the potential for large language model interpretability
    in medicine. npj Digital Medicine, 7:20, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2024] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang
    Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D. Wang. Ehragent: Code empowers
    large language models for few-shot complex tabular reasoning on electronic health
    records, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singhal et al. [2022] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi,
    Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen
    Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli,
    Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S.
    Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu,
    Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and
    Vivek Natarajan. Large language models encode clinical knowledge, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singhal et al. [2023] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery
    Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal,
    Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant
    Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev,
    Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale
    Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam,
    and Vivek Natarajan. Towards expert-level medical question answering with large
    language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2024] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun
    Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language
    models as collaborators for zero-shot medical reasoning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walton et al. [2008] Douglas Walton, Chris Reed, and Fabrizio Macagno. Argumentation
    Schemes. Cambridge University Press, New York, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walton [1996] Douglas Walton. Argumentation Schemes for Presumptive Reasoning.
    Routledge, 1st edition, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li,
    Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah,
    Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications
    via multi-agent conversation, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2024] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou,
    Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world
    planning with language agents, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiu et al. [2022] Yeliang Xiu, Zhanhao Xiao, and Yongmei Liu. LogicNMR: Probing
    the non-monotonic reasoning ability of pre-trained language models. In Yoav Goldberg,
    Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational
    Linguistics: EMNLP 2022, pages 3616–3626, Abu Dhabi, United Arab Emirates, December
    2022\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. [2020] Zhiwei Zeng, Zhiqi Shen, Benny Toh Hsiang Tan, Jing Jih Chin,
    Cyril Leung, Yu Wang, Ying Chi, and Chunyan Miao. Explainable and Argumentation-based
    Decision Making with Qualitative Preferences for Diagnostics and Prognostics of
    Alzheimer’s Disease. In Proceedings of the 17th International Conference on Principles
    of Knowledge Representation and Reasoning, pages 816–826, 9 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih
    Yao. Cumulative reasoning with large language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Formal Description of ArgMed-Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 ArgMed-Agents Interaction Reasoning
  prefs: []
  type: TYPE_NORMAL
- en: 1:Question $Q$)24:end function
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B ASCD Reasoning Mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Argumentation Scheme | Critical Questions | Reject Rule | Derive Role | Attack
    Rule |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Argumentation Scheme |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| for Decision(ASD) | ASD.CQ1 | NO (no evidence to support the argument) |
    none | self-attack |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASD.CQ2 | YES (there are side effects of decision-making) | ASSE | ASSE
    attack ASD |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASD.CQ3 | NO (incomplete decisions to fulfil goals) | none | self-attack
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASD.CQ4 | YES (existence of alternative decisions) | ASD | ASBD attack
    the worse decision |'
  prefs: []
  type: TYPE_TB
- en: '| in ASD_1 and ASD_2 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Argumentation Scheme |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| for Side Effect(ASSE) | ASSE.CQ1 | NO (no evidence to support the argument)
    | none | self-attack |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASSE.CQ2 | NO (the side-effect is acceptable) | none | self-attack |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASSE.CQ3 | YES (there are ways to ameliorate side effects) | ASD | none
    |'
  prefs: []
  type: TYPE_TB
- en: '| Argumentation Scheme |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| for Better Decision(ASBD) | ASBD.CQ1 | NO (no evidence to support the argument)
    | none | self-attack |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: List of specialized schemes, CQs, when CQs will be rejected and their
    derived roles and attack rules between derived roles and initial schemes. Notably,
    every ASD is attacking each other by default, so it is not shown in the attack
    rules.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C More Case Details in Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a82eff4588abaf8dd2da648b816fe126.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Example of correct reasoning by ArgMed-Agents. We show the flow of
    the multi-agent dialogue and the resulting formal AA framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8d0b933be92c11e95ff772dd88fde0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: (continued) Example of correct reasoning by ArgMed-Agents. We provide
    runnable demo files with available GPT-4 API Keys in the code that include these
    cases.'
  prefs: []
  type: TYPE_NORMAL
