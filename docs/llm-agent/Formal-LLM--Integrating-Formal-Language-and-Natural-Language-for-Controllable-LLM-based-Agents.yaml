- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.00798](https://ar5iv.labs.arxiv.org/html/2402.00798)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zelong Li    Wenyue Hua    Hao Wang    He Zhu    Yongfeng Zhang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically
    generate and execute multi-step plans to solve complex tasks. However, since LLM’s
    content generation process is hardly controllable, current LLM-based agents frequently
    generate invalid or non-executable plans, which jeopardizes the performance of
    the generated plans and corrupts users’ trust in LLM-based agents. In response,
    this paper proposes a novel “Formal-LLM” framework for LLM-based agents by integrating
    the expressiveness of natural language and the precision of formal language. Specifically,
    the framework allows human users to express their requirements or constraints
    for the planning process as an automaton. A stack-based LLM plan generation process
    is then conducted under the supervision of the automaton to ensure that the generated
    plan satisfies the constraints, making the planning process controllable. We conduct
    experiments on both benchmark tasks and practical real-life tasks, and our framework
    achieves over 50% overall performance increase, which validates the feasibility
    and effectiveness of employing Formal-LLM to guide the plan generation of agents,
    preventing the agents from generating invalid and unsuccessful plans. Further,
    more controllable LLM-based agents can facilitate the broader utilization of LLM
    in application scenarios where high validity of planning is essential. The work
    is open-sourced at [https://github.com/agiresearch/Formal-LLM](https://github.com/agiresearch/Formal-LLM).
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Model, Agents, Controllability
  prefs: []
  type: TYPE_NORMAL
- en: ¹Department of Computer Science, Rutgers University, New Brunswick, NJ 08854,
    USA
  prefs: []
  type: TYPE_NORMAL
- en: zelong.li, wenyue.hua, hw488, hz375, yongfeng.zhang@rutgers.edu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerous applications have emerged with the rapid development of Large Language
    Models (LLM). One notable application is the LLM-based agent, which is capable
    of automatically generating and executing multi-step plans to solve complex tasks.
    While the LLM-based agent exhibits creativity, there is a concern about the potential
    generation of unreasonable and invalid plans, undermining the effectiveness of
    agents. For example, generating a plan that attempts to process image data using
    a tool designed for text can lead to errors. Recent studies have pointed out the
    challenges of LLM-based agents in developing non-executable plans without sufficient
    human oversight (Ge et al., [2023a](#bib.bib13); Yuan et al., [2024](#bib.bib47)).
    Addressing these challenges is crucial for improving agent performance, increasing
    the generation of valid plans, and maintaining user trust. Various attempts have
    been made to control LLM text generation, such as incorporating hard constraints
    (Takase and Okazaki, [2019](#bib.bib39); Carlsson et al., [2022](#bib.bib5)),
    soft constraints (Gu et al., [2022](#bib.bib17); Lu et al., [2022](#bib.bib29)),
    or a combination of both (Chen et al., [2024](#bib.bib6)). However, the focus
    of controlling LLM-based agents emphasizes the validity of plans and tool-use
    over purely text generation. Some recent studies leverage LLM as a parser to extract
    a chain of tools from the generated texts based on pre-designed prompts (Ge et al.,
    [2023a](#bib.bib13); Yuan et al., [2024](#bib.bib47)), yet achieving a satisfactory
    rate of valid plans remains elusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle the problems of valid plan generation, we propose a framework named
    “Formal-LLM”, which integrates the expressiveness of natural language and the
    precision of formal language, as shown by the toy example in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Formal-LLM: Integrating Formal Language and Natural
    Language for Controllable LLM-based Agents"). Specifically, to control the LLM-based
    agent’s plan generation, human users construct a context-free grammar (CFG) as
    the formal language to represent the constraints for the agent. The CFG is then
    automatically translated into a pushdown automaton (PDA). When LLM conducts planning,
    it is prompted to follow the state transition defined by the automaton. This is
    realized by limiting the LLM-based agent’s choices at each step to the valid actions
    defined by the PDA at its current state, which helps to guarantee that the constraint
    is satisfied in the final generated plan. We choose PDA in this work because some
    tasks need to be solved by a tree-structured plan rather than a simple chain-structured
    plan, as shown by the examples in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Formal-LLM
    Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents"),
    and such tree-structured plans require a PDA to generate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/200b1cbf5b1bb78a9702a4cde58417e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The Formal-LLM workflow with a toy example. To control the LLM-based
    agent’s plan generation, human users construct a formal language (context-free
    grammar, CFG) to represent the natural language constraints. The formal language
    is then translated into a pushdown automaton (PDA). When LLM conducts planning,
    it needs to follow the state transition defined by the automaton, which helps
    to guarantee that the constraint is satisfied in the final generated plan. We
    choose PDA because some plans exhibit tree-structure (as shown by other examples
    in the paper), which requires a PDA to generate.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we introduce the backtracking mechanism in the Formal-LLM to increase
    the probability of finding valid plans for the agent, which enables the planning
    process to return to the previous step when a dead end is reached on the automaton.
    Besides, traditional LLM-based agent fine-tuning techniques such as Reinforcement
    Learning from Task Feedback (RLTF) (Ge et al., [2023a](#bib.bib13)) rely on the
    rewards from the agent’s plan execution to fine-tune the LLM parameters. However,
    since many invalid plans may be generated, many rewards are actually not informative
    for LLM-based agent fine-tuning. Our Formal-LLM approach guarantees that invalid
    plans are excluded during the agent’s plan generation. As a result, our approach
    helps to increase the amount of valid rewards for LLM fine-tuning, which improves
    the performance of fine-tuned LLM-based agents.
  prefs: []
  type: TYPE_NORMAL
- en: In our experiments, we implement the Formal-LLM framework across various LLMs,
    including both open-source and closed-source LLMs. We also test our framework
    on both benchmark tasks and real-life practical tasks. Specifically, the benchmark
    tasks involve the LLM-based agent utilizing different tools to solve complex problems
    through multiple steps. The real-life scenarios encompass daily routines, cooking
    instructions, and commercial risk management, each imposing specific domain-knowledge
    or common-sense constraints. Our findings demonstrate the Formal-LLM’s capability
    to generate reasonable plans. The framework substantially enhances the overall
    performance on the benchmark tasks by over 50% and can always generate executable
    plans. In real-life scenarios, we provide qualitative analyses into the improvements
    brought by the Formal-LLM framework, affirming its feasibility and effectiveness
    in rendering the agents more controllable.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM-based AI Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An AI agent is an autonomous entity capable of making decisions and executing
    actions within a specific environment to effectively tackle diverse complex tasks
    using AI techniques (Ge et al., [2023b](#bib.bib14); Wang et al., [2023](#bib.bib43);
    Xi et al., [2023](#bib.bib46)). The emergence of Large Language Models (LLMs),
    exemplified by the GPT series (Radford et al., [2019](#bib.bib34); Brown et al.,
    [2020](#bib.bib3); OpenAI, [2023](#bib.bib30)) and the LLaMA series (Touvron et al.,
    [2023a](#bib.bib40), [b](#bib.bib41)), has spurred the exploration of LLM-based
    agents (Ge et al., [2023a](#bib.bib13); Huang et al., [2022](#bib.bib23)). These
    agents utilize LLMs as their central cognitive component or controller, broadening
    their perceptual and action capabilities through approaches like multimodal perception
    and tool usage (Schick et al., [2023](#bib.bib38); Ge et al., [2023a](#bib.bib13);
    Qin et al., [2023](#bib.bib33)). In contrast to pre-LLM era AI agents, LLM-based
    agents showcase creativity, manifesting in the ability to generate innovative
    ideas without additional learning (Franceschelli and Musolesi, [2023](#bib.bib12)),
    indicating a degree of self-directed exploration and decision-making (Xi et al.,
    [2023](#bib.bib46)). Despite LLM-based agent’s application in diverse real-world
    scenarios such as software development (Li et al., [2023a](#bib.bib25); Qian et al.,
    [2023](#bib.bib31)), scientific research (Boiko et al., [2023](#bib.bib2)), and
    system management (Liu et al., [2023](#bib.bib28)), recent studies have highlighted
    issues of non-executable plans being generated without adequate human oversight
    (Ge et al., [2023a](#bib.bib13); Yuan et al., [2024](#bib.bib47)). If these plans
    lack executability, the utility of LLM-based agents in fields that require high
    validity is compromised, and their unreliability erodes user trust. To address
    this challenge, we propose integrating natural language and precise automaton
    during the agent’s planning process to enable the controllable generation of valid
    plans.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Controllable LLM Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, there are few works on controllable LLM-based
    agents, and discussions primarily revolve around controllable LLM text generation.
    Controlling LLM text generation typically falls into two categories: hard constraints
    and soft constraints (Qin et al., [2022](#bib.bib32)). Hard constraints involve
    constraints on the desired text length, designated keywords for the generated
    text (Takase and Okazaki, [2019](#bib.bib39); Carlsson et al., [2022](#bib.bib5)),
    and constraint vocabulary space in decoding time (Hemmer et al., [2023](#bib.bib18);
    Geng et al., [2024](#bib.bib15), [2023](#bib.bib16)), while soft constraints restrict
    the output based on specific semantics, such as sentiments or topics (Gu et al.,
    [2022](#bib.bib17); Lu et al., [2022](#bib.bib29); Li et al., [2020](#bib.bib26)).
    A recent work (Chen et al., [2024](#bib.bib6)) attempts to integrate both constraints
    into a unified approach. However, text generation control has disadvantages and
    problems. First, text generation control could hardly be directly applied to closed-source
    LLMs due to their need to access the decoding probability (Geng et al., [2024](#bib.bib15)).
    Second, controllable LLM-based agents prioritize the validity of plan and tool
    use over purely text generation. Recent studies on LLM-based agents have noted
    limited effectiveness when employing text constraint generation on LLM-based agents
    and have explored using LLMs as a parser to extract planning information from
    generated texts (Ge et al., [2023a](#bib.bib13); Yuan et al., [2024](#bib.bib47));
    but relying heavily on the parser LLM’s effectiveness may not yield a satisfactory
    plan valid rate, especially in complex tasks. We propose an automaton-guided LLM-based
    agent planning to ensure that the generated plans adhere 100% to the constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminary Knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Context-free Grammar
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many LLM-based agent operating tools that take multiple inputs and
    produce one output. For example, the tool “Visual Question Answering” receives
    an image and a text as the inputs and generates a text as the output. The planning
    involving such tools forms a tree structure, and such tree-structured plans require
    Context-Free Grammar (CFG) to express the rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CFG consists of four components: terminals (symbols unable to be replaced),
    nonterminals (symbols subject to replacement), start symbol (a unique nonterminal,
    usually denoted as $S$), and productions (rules governing symbol substitution).
    The CFG production follows the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A\rightarrow\alpha$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $A$ in any situations. Eq.([2](#S3.E2 "Equation 2 ‣ 3.1 Context-free
    Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and
    Natural Language for Controllable LLM-based Agents")) presents a CFG example with
    two productions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;S\rightarrow\varepsilon\\ &amp;S\rightarrow aSb\end{split}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $a,b$ in a certain amount of CFG production steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/161f65620140b063a92e4408ba78da7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A Pushdown Automaton (PDA) example. The edge represents the transition
    function. For example, the edge $(a,Z;SZ)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pushdown Automaton
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every CFG can be transformed into an equivalent nondeterministic Pushdown Automaton
    (PDA) with an algorithm (Hopcroft et al., [2001](#bib.bib20)). A PDA is a machine
    that moves through a series of states based on a given string input, a transition
    function, and the top element of a stack. The automaton reads the letters in the
    input sequentially, and if there exists a path leading the machine to an accept
    state right after consuming the entire string, the input string is considered
    an accepting word. The set of all accepting words is referred to the CFL accepted
    by the automaton. Since the PDA process is more intuitive and automated than CFG
    for guiding the prompting of LLMs, our framework is designed on top of PDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary
    Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") as an example to show how PDA works. This automaton includes
    a stack alphabet ($\{S,Z\}$ is the stack top now).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This PDA is equivalent to the CFG in Eq.([2](#S3.E2 "Equation 2 ‣ 3.1 Context-free
    Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and
    Natural Language for Controllable LLM-based Agents")), i.e., every accepting word
    of this PDA belongs to the CFL defined in Eq.([2](#S3.E2 "Equation 2 ‣ 3.1 Context-free
    Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and
    Natural Language for Controllable LLM-based Agents")). We still use $aabb$ as
    the stack top.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 The Formal-LLM Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Motivation and Challenge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Natural language is easily understandable by humans, but it may also lack precision
    in certain application scenarios. In contrast, formal language is defined in clear
    mathematical and machine readable forms, which could be hard to comprehend for
    human users, but it also possesses great precision. Thus, we aim to effectively
    integrate the benefits of both natural and formal languages through the pushdown
    automaton (PDA) for more controllable planning of LLM-based agents in scenarios
    requiring precise and valid plans.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing our Formal-LLM framework presents non-trivial challenges. The first
    challenge lies in the potential struggle of the LLM to directly comprehend and
    process formal language due to the limited CFG- or PDA-related corpus in pre-training
    data. Hence, natural language prompts are necessary to effectively describe the
    status of the automaton, bridging the gap between formal and natural language
    for LLM’s planning. Given that the automaton has the potential to generate an
    infinite number of plans, the second challenge arises because the automaton only
    ensures the validity and executability of the plans by adhering to defined constraints,
    but cannot guarantee the performance and optimality of the generated plan. Therefore,
    leveraging the solid natural language understanding ability of LLM and other methods,
    such as fine-tuning and backtracking mechanisms, becomes crucial for enhancing
    the planning performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Formulating Constraint to Automaton
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The overall flow of our framework is presented in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents"). We begin by illustrating the process of converting
    constraints into formal language and automata for benchmark tasks, using the OpenAGI
    benchmark (Ge et al., [2023a](#bib.bib13)) as an example, which incorporates a
    set of domain expert models as tools and lists a series of intricate problems
    that cannot be addressed with a single tool. These tools are categorized into
    six primary groups according to the input and output modalities, as indicated
    in Table [3](#A2.T3 "Table 3 ‣ B.1 Bechmark Tasks and Tools ‣ Appendix B Appendix
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: If the plans for the benchmark tasks generated by the LLM-based agent deviate
    from the expected data format, the execution performance degrades significantly
    or even results in errors. For instance, consider the Colorization tool, which
    operates on image data as input and output. Given that image data is represented
    as a 3-D tensor, providing a 1-D tensor text string as input to Colorization could
    lead to errors, specifically due to incorrect data dimensions. Consequently, the
    entire plan becomes invalid and cannot be executed. In such cases, natural language
    may struggle to articulate these constraints precisely and concisely, whereas
    formal language can express them straightforwardly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use symbols to represent different elements: $T$ for specific tools, in
    the order of Table [3](#A2.T3 "Table 3 ‣ B.1 Bechmark Tasks and Tools ‣ Appendix
    B Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for
    Controllable LLM-based Agents"). The context-free grammar (CFG) below outlines
    the constraints on data formats with Polish prefix notation.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}&amp;I\rightarrow AI&#124;CT\\ &amp;T\rightarrow BI&#124;DT&#124;EIT&#124;FTT\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;A\rightarrow a_{1}&#124;a_{2}&#124;a_{3}&#124;a_{4}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;B\rightarrow b_{1}&#124;b_{2}&#124;b_{3}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;C\rightarrow c_{1}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;D\rightarrow d_{1}&#124;d_{2}&#124;d_{3}&#124;d_{4}&#124;d_{5}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;E\rightarrow e_{1}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;F\rightarrow f_{1}\end{split}$$ |  | (3) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where $I\rightarrow AI|CT$), the result is still an image. Considering a specific
    task, such as “Given blurry gray-scale images, how to return the object names
    in English step by step?”, we know that the task’s inputs are images and the final
    outputs should be texts. To formalize this, we introduce two additional constraints
    to Eq.([3](#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The
    Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}&amp;S\rightarrow T\\ &amp;I\rightarrow i\end{split}$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: 'i.e., the CFG starts with the symbol $T$) with the text and output image to
    derive the final text.” As mentioned in Section [3.2](#S3.SS2 "3.2 Pushdown Automaton
    ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural
    Language for Controllable LLM-based Agents"), the CFG can be equivalently converted
    into a PDA, illustrated in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Formal-LLM Prompts
    and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating
    Formal Language and Natural Language for Controllable LLM-based Agents"). These
    steps allow us to transform the constraints into automata for subsequent utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In certain scenarios, creating an automaton can be more straightforward than
    formulating a formal language. Section [B.4.1](#A2.SS4.SSS1 "B.4.1 Daily Plan
    ‣ B.4 Prompt for Real-life Tasks ‣ Appendix B Appendix ‣ Formal-LLM: Integrating
    Formal Language and Natural Language for Controllable LLM-based Agents") describes
    a real-life task for daily work planning. The constraints in this case revolve
    around time, allowing us to represent time as distinct states, as depicted in
    Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Formal-LLM Prompts and Planning from Automaton
    ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural
    Language for Controllable LLM-based Agents"). The automaton initiates the design
    of an activity that concludes at 20:00, and a valid plan is generated when the
    automaton reaches 10:00 while being in a state where three meals have been consumed.
    This approach involves designing an effective PDA without the need for a CFG.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Formal-LLM Prompts and Planning from Automaton
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [4.2](#S4.SS2 "4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM
    Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents"), we convert the natural language planning constraints into
    a PDA, where any accepted word represents a valid and executable plan. The reason
    is that LLM could struggle to directly comprehend or process the CFG due to limited
    exposure to formal language during pre-training. Hence, we employ natural language
    prompts for LLM to comprehend the task and generate plans that can be easily read
    by humans; simultaneously, we use the PDA to guide the process of generating natural
    language plans.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The automaton initiates the plan generation process from the start state. In
    situations where multiple viable options are available for the automaton to proceed,
    a prompt is created to inquire the LLM, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents"). This prompt encompasses a comprehensive {task
    description} to mitigate potential forgetting issues of the LLM (Hua et al., [2023](#bib.bib22)),
    details about the {current progress}, and {feasible choices} determined by the
    PDA. For example, consider the PDA in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Formal-LLM
    Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents")
    when it is in state $q_{1}$, we directly use the tool name (e.g., Colorization)
    instead of the category name (e.g., “Image-in, Image-out tools”) in the prompt.
    This is because the tool name provides more concrete information for the LLM to
    comprehend the tool’s functionality, enabling the LLM to make informed decisions
    without influencing the plan’s executability. A prompt example is provided in
    Appendix [B.3](#A2.SS3 "B.3 Prompt for Benchmark Tasks ‣ Appendix B Appendix ‣
    Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of employing PDA for LLM-based agent planning is its ability
    to articulate plans that involve tools with many-input-single-output (e.g., Image-Text
    pair as input, Text as output), assuming the number of inputs of each tool is
    deterministic. To illustrate, we depict the derivation tree of the string $e_{1}a_{1}ib_{1}i$,
    because they are pushed into the stack at the same time. Consequently, we can
    readily identify the tools that will operate on this image/text data in the subsequent
    step and incorporate this information in the prompt, enabling the LLM to make
    more informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ba7477831c7ec0aef666cafe7e029c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An equivalent PDA to the CFG of the combination of Eq.([3](#S4.E3
    "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents")) and Eq.([4](#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint
    to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language
    and Natural Language for Controllable LLM-based Agents")). $\varepsilon$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2fdf517a18acfc98f02f0c36474c3353.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The PDA includes 13 states, $\{10,11,...,19,20,Start,Fin\}$ is enumerated
    from 20 to 13.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cb12150e295894828ef1b3622aadfe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The derivation tree for $e_{1}a_{1}ib_{1}i$ for the example benchmark
    task in Section [4.2](#S4.SS2 "4.2 Formulating Constraint to Automaton ‣ 4 The
    Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Backtracking Mechanism and Reinforcement Learning from Task Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An accepting word of an automaton can be infinitely long. However, we anticipate
    that the plan proposed by the LLM-based agent can be executed within a finite
    number of steps. Consider the PDA in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Context-free
    Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and
    Natural Language for Controllable LLM-based Agents"), any length of the string
    in the form of $a^{n}b^{n}$ is an accepting word. Similarly, in planning tasks,
    tools can be employed multiple times, such as applying the Text Summarization
    tool to the original text and infinitely repeating the same tool on the output.
    However, such planning behavior may be nonsensical and inefficient. Hence, for
    the tasks outlined in this paper, each tool is restricted to a single use for
    a given task, preventing infinite planning for both benchmark and real-life scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon imposing constraints on the tool usage, a new challenge emerges: the automaton
    may encounter a dead-end during plan generation. Consider the formal language
    in Eq.([3](#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The
    Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents")) and Eq.([4](#S4.E4 "Equation 4 ‣ 4.2 Formulating
    Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating
    Formal Language and Natural Language for Controllable LLM-based Agents")). Upon
    analyzing the language, we observe that the only means of transitioning from the
    symbol $T$. Consequently, a dead-end situation may arise following the imposition
    of tool usage limits.'
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, we propose to document the automaton’s details at
    each step, encompassing the current state and stack, and the part of the word
    that has been generated. When the automaton confronts a dead-end (indicating no
    viable next step), we initiate backtracking to the preceding steps until there
    is an unexplored branch. The backtracking involves retracing the automaton to
    the prior step, reinstating the automaton details based on the recorded stack,
    and eliminating the dead-end branch from the prior choice list. Consequently,
    the automaton is capable and guaranteed to generate a valid, executable plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'The design above only guarantees the validity of the generated plans. For example,
    the plan $e_{1}a_{1}ib_{1}i$ shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Formal-LLM
    Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents")
    is an executable plan for the task “Given blurry grayscale images, how to return
    the object names in English step by step?”. However, this may not be the optimal
    plan. Thus, as an additional enhancement in plan quality, particularly when employing
    open-source LLMs, we integrate Reinforcement Learning from Task Feedback (RLTF)
    (Ge et al., [2023a](#bib.bib13)) following the application of our Formal-LLM framework.
    After the generation of a plan by the LLM-based agent for a training task, the
    plan is executed on benchmark data to assess its performance. Then, the performance
    is used as a reward for Reinforcement Learning (RL) to update the parameters of
    the LLM. Our framework ensures that invalid plans are always excluded during the
    LLM-based agent’s plan generation. Therefore, the number of valid rewards for
    LLM fine-tuning increases, and our approach can improve the performance of fine-tuned
    LLM-based agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Backbone LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We test our Formal-LLM framework on both closed-source and open-source LLMs.
    Closed-source LLMs include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5-turbo (Brown et al., [2020](#bib.bib3)) is a generative pre-trained
    transformer of OpenAI with a strong ability of various language tasks based on
    instructions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claude-2 (Claude-2, [2023](#bib.bib9)) is a transformer LLM of Anthropic trained
    with unsupervised learning and Reinforcement Learning from Human Feedback (RLHF).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 (OpenAI, [2023](#bib.bib30)) is a follow-up version of GPT-3.5, which
    is more powerful than its predecessors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open-source LLMs include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flan-T5-Large (Chung et al., [2022](#bib.bib8)) is an encoder-decoder language
    model with 770 million parameters, pre-trained by instruction fine-tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vicuna-7B (Chiang et al., [2023](#bib.bib7)) is a decoder-only chatbot trained
    by fine-tuning the LLaMA model (Touvron et al., [2023a](#bib.bib40)) with user-shared
    conversations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA-2 (Touvron et al., [2023b](#bib.bib41)) is a successor to the original
    LLaMA model. It is a more powerful language model. In this work, we use the 13-billion
    size model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2 Learning Schema of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We adopt the following LLM learning schema:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-shot Learning (Zero) directly inputs the prompt to the LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot Learning (Few) presents a set of high-quality demonstrations in the
    prompt, each consisting of both input and desired output on the target task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning from Task Feedback (RLTF) executes a plan and takes its
    performance as the reward to optimize the LLM through an RL process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formal-LLM (F-LLM) is our proposed framework, leveraging automaton to control
    the LLM’s generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formal-LLM plus RLTF (F-LLM+RLTF) applies our Formal-LLM framework on top of
    RLTF, which excludes invalid plans and increases the amount of valid rewards for
    RLTF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Specifically, we utilize Zero, Few, and F-LLM frameworks for closed-source LLMs,
    as these frameworks do not need to modify the LLM’s parameters. Regarding open-source
    LLMs, we compare RLTF, F-LLM and F-LLM+RLTF, given that the RLTF method surpasses
    Zero and Few, as exhaustively discussed in recent work (Ge et al., [2023a](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Experimentation Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.3.1 Benchmark Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the benchmark experiment, we utilize the tools and multi-step tasks in
    the OpenAGI platform (Ge et al., [2023a](#bib.bib13)). The tools are detailed
    in Table [3](#A2.T3 "Table 3 ‣ B.1 Bechmark Tasks and Tools ‣ Appendix B Appendix
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") in the Appendix, and the tasks are categorized based on their
    output type and ground-truth label type (Task 1, 2, and 3). Then, based on different
    task types, different metrics are employed to gauge the performance: CLIP Score
    (Hessel et al., [2021](#bib.bib19)), assessing the similarity between text and
    image, is utilized for Text-to-Image tasks; BERT Score (Zhang et al., [2020](#bib.bib50)),
    evaluating text generation with BERT, is applied when both data labels and the
    expected outputs are texts; and ViT Score (Wu et al., [2020](#bib.bib45)) gauges
    the similarity between the image label and image output. Additionally, we construct
    Task X, which is a subset of “Task 1 $\cup$ Task 3” that require tree-structured
    plan rather than chain-structured plan due to the use of many-input-single-ouput
    tools, such as Visual Question Answering. Task X is used to test the complex planning
    ability of our Formal-LLM framework. The data samples in Task X are evaluated
    using their corresponding metric. Example tasks of each category are illustrated
    in Table [4](#A2.T4 "Table 4 ‣ B.3 Prompt for Benchmark Tasks ‣ Appendix B Appendix
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Real-life Practical Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We also experiment with real-life planning scenarios, encompassing daily plans,
    cooking recipes, and risk management, where validity and rationality are crucial.
    In these scenarios, the concept of tool is generalized to include various types
    of steps in a plan, such as events, actions, or activities, as they can take diverse
    forms supporting the execution of the plan. For example, a tool in the daily planning
    task may be an activity such as taking breakfast. We provide qualitative analyses
    between the Zero and F-LLM learning schema using GPT-4 backbone for practical
    tasks. We use GPT-4 for the experiment of practical tasks because other LLMs can
    hardly generate readable plans for these tasks, and we test under Zero and F-LLM
    due to the limited sample size and that these two learning frameworks do not require
    fine-tuning or accessing to the LLM parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics / Task | GPT-3.5-turbo | Claude-2 | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero | Few | F-LLM (Ours) | Zero | Few | F-LLM (Ours) | Zero | Few | F-LLM
    (Ours) |'
  prefs: []
  type: TYPE_TB
- en: '| % of Valid Plans | 29% | 71% | 100% | 29% | 47% | 100% | 53% | 76% | 100%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Task 1 (CLIP Score) | 0.0 | 0.0 | 0.3056 | 0.0 | 0.2543 | 0.3056 | 0.0 |
    0.3055 | 0.3056 |'
  prefs: []
  type: TYPE_TB
- en: '| Task 2 (BERT Score) | 0.1914 | 0.3820 | 0.6364 | 0.2111 | 0.5038 | 0.6275
    | 0.2076 | 0.6307 | 0.5102 |'
  prefs: []
  type: TYPE_TB
- en: '| Task 3 (ViT Score) | 0.2437 | 0.7497 | 0.6470 | 0.4082 | 0.5416 | 0.7137
    | 0.5058 | 0.6480 | 0.7689 |'
  prefs: []
  type: TYPE_TB
- en: '| Task X | 0.0 | 0.0 | 0.0658 | 0.0 | 0.0 | 0.2799 | 0.0 | 0.0 | 0.2876 |'
  prefs: []
  type: TYPE_TB
- en: '| Average over tasks | 0.1443 | 0.3345 | 0.4846 | 0.1838 | 0.3773 | 0.5420
    | 0.1992 | 0.4662 | 0.4914 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Benchmark task performances under different settings for three closed-source
    LLMs. Zero is for Zero-shot Learning, Few is for Few-shot Learning, and F-LLM
    is for Formal-LLM. The boldface numbers denote the highest score under each task
    type using the same LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics / Tasks | Flan-T5-Large | Vicuna-7B | LLaMA-2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| RLTF | F-LLM | F-LLM+RLTF | RLTF | F-LLM | F-LLM+RLTF | RLTF | F-LLM | F-LLM+RLTF
    |'
  prefs: []
  type: TYPE_TB
- en: '| % of Valid Plans | 24% | 100% | 100% | 29% | 100% | 100% | 47% | 100% | 100%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Task 1 (CLIP Score) | 0.0 | 0.3049 | 0.3049 | 0.0 | 0.3122 | 0.3139 | 0.0610
    | 0.1601 | 0.3060 |'
  prefs: []
  type: TYPE_TB
- en: '| Task 2 (BERT Score) | 0.3327 | 0.5164 | 0.5287 | 0.1475 | 0.4948 | 0.4673
    | 0.1611 | 0.4220 | 0.5565 |'
  prefs: []
  type: TYPE_TB
- en: '| Task 3 (ViT Score) | 0.6632 | 0.6264 | 0.7469 | 0.6958 | 0.5948 | 0.8618
    | 0.7106 | 0.7043 | 0.6808 |'
  prefs: []
  type: TYPE_TB
- en: '| Task X | 0.0 | 0.0728 | 0.4046 | 0.0 | 0.4127 | 0.4029 | 0.0 | 0.3846 | 0.4163
    |'
  prefs: []
  type: TYPE_TB
- en: '| Average over tasks | 0.3111 | 0.4451 | 0.5321 | 0.2009 | 0.4824 | 0.5162
    | 0.3101 | 0.4498 | 0.5390 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Benchmark task performances under different settings for three open-source
    LLMs. RLTF is for Reinforcement Learning from Task Feedback, F-LLM is for Formal-LLM,
    and F-LLM+RLTF is for using the generated plans by F-LLM to calculate the reward
    for RLTF. The boldface numbers denote the highest score under each task type using
    the same LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Experimental Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The experiment results on the benchmark tasks are shown in Table [1](#S5.T1
    "Table 1 ‣ 5.3.2 Real-life Practical Tasks ‣ 5.3 Experimentation Datasets ‣ 5
    Experiments ‣ Formal-LLM: Integrating Formal Language and Natural Language for
    Controllable LLM-based Agents") and Table [2](#S5.T2 "Table 2 ‣ 5.3.2 Real-life
    Practical Tasks ‣ 5.3 Experimentation Datasets ‣ 5 Experiments ‣ Formal-LLM: Integrating
    Formal Language and Natural Language for Controllable LLM-based Agents"), referring
    to the closed-source and open-source LLMs, respectively. Each row stands for a
    type of task, each column represents the learning schema of an LLM-based agent,
    and every three columns are the results of the same LLM. For closed-source LLMs,
    among the three learning schemata without revising the LLM parameter, almost all
    the best scores under each type of task belong to our F-LLM framework. For open-source
    LLMs, our F-LLM without fine-tuning is already better than RLTF (the best schema
    in the OpenAGI platform (Ge et al., [2023a](#bib.bib13))) in most cases, except
    for the ViT Score (Task 3), because the Type-3 tasks take the most significant
    portion of the RLTF’s fine-tuning data, and thus RLTF is adequately optimized
    on these tasks. The performance gain of our F-LLM framework comes from the 100%
    executable plans and supporting tree-structured planning for the difficult tasks
    in Task X. As a comparison, the best open-source LLM (LLaMA-2-13B with RLTF) can
    only generate 47% executable plans, and the best closed-source LLM (GPT-4 with
    few shot) can generate 76% executable plans. Additionally, none of the baselines
    can handle the Task X (Score = 0.0). Fortunately, due to the increased amounts
    of valid rewards from the 100% executable plans with our F-LLM framework, the
    F-LLM+RLTF approach enables boosted performance, as demonstrated by its better
    scores in Table [2](#S5.T2 "Table 2 ‣ 5.3.2 Real-life Practical Tasks ‣ 5.3 Experimentation
    Datasets ‣ 5 Experiments ‣ Formal-LLM: Integrating Formal Language and Natural
    Language for Controllable LLM-based Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the benchmark experiment results show that our F-LLM framework
    is an effective method to integrate the benefits of both natural and formal languages
    for more controllable and valid planning of LLM-based agents.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We also show the results of real-life examples by applying our Formal-LLM to
    the GPT models. Due to the page limit, we put the complete results in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 'From our observation, after applying our Formal-LLM, the generated plan from
    the GPT-based agent is more complete, reasonable and specific to the case. In
    the daily plan example, the agent fails to fit all the activities into the plan
    without the Formal-LLM, while it can achieve this goal by applying the Formal-LLM.
    Still take Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Formal-LLM Prompts and Planning from
    Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language
    and Natural Language for Controllable LLM-based Agents") as an example, our Formal-LLM
    framework can limit the planning to 10:00$\sim$20:00, while without Formal-LLM,
    the agent schedules activities after 20:00, even if we mention the constraint
    “Generate a plan for all the activities between 10:00 and 20:00” in natural language,
    which shows the advantages of formal language guided planning. For the cooking
    recipe example, without strict automaton constraint, GPT may generate steps such
    as “stir-fry the Chinese broccoli until it changes color” after “once the water
    boils, add the Chinese broccoli”. This is unreasonable because it’s impossible
    to stir-fry when broccoli is in boiling water. As for the risk management example,
    the generated plan without our framework is too general that it could be used
    for any other two companies. However, the plan after applying Formal-LLM is more
    specific and focuses on the potential antitrust risk between Microsoft and Blizzard.
    Thus, after applying the automaton which describes the constraints, the generated
    plan has better quality under the Formal-LLM framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we introduce the innovative Formal-LLM framework for LLM-based
    agents, combining LLM’s power of natural language comprehension with the precision
    of formal language. Our experiments, encompassing benchmark tasks and real-life
    practical scenarios, affirm the feasibility and effectiveness of employing automaton
    to control the agent’s generation of valid plans. More controllable LLM-based
    agents can augment the potential utilization of LLM in applications where high
    validity of planning holds significant importance.
  prefs: []
  type: TYPE_NORMAL
- en: Several potential extensions to our work merit consideration. First, automating
    the translation of natural language into formal language for agents could further
    improve the framework, as designing formal language and automata by domain experts
    may not be the most efficient approach. Additionally, this work focuses on LLM
    plan generation based on formal language; however, another important problem to
    explore in the future is LLM plan verification based on formal language. Furthermore,
    our framework uses one large automaton to control the whole planning process,
    while another potential solution is to use multiple small automata, each describing
    one constraint, and the agent only accesses the corresponding one when needed
    during plan generation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boiko et al. (2023) Daniil A. Boiko, Robert MacKnight, and Gabe Gomes. 2023.
    Emergent autonomous scientific research capabilities of large language models.
    arXiv:2304.05332 [physics.chem-ph]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
    Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection
    with transformers. In *European conference on computer vision*. Springer, 213–229.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlsson et al. (2022) Fredrik Carlsson, Joey Öhman, Fangyu Liu, Severine Verlinden,
    Joakim Nivre, and Magnus Sahlgren. 2022. Fine-grained controllable text generation
    using non-residual prompting. In *Proceedings of the 60th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers)*. 6837–6857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2024) Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, and Zhendong Mao.
    2024. Benchmarking Large Language Models on Controllable Generation under Diversified
    Instructions. arXiv:2401.00690 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    et al. 2022. Scaling instruction-finetuned language models. arXiv:2210.11416 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claude-2 (2023) Claude-2\. 2023. Model card and evaluations for claude models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conde et al. (2022) Marcos V Conde, Ui-Jin Choi, Maxime Burchi, and Radu Timofte.
    2022. Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration.
    In *European Conference on Computer Vision*. 669–687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ICLR*
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Franceschelli and Musolesi (2023) Giorgio Franceschelli and Mirco Musolesi.
    2023. On the Creativity of Large Language Models. arXiv:2304.00008 [cs.AI]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. (2023a) Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023a. OpenAGI: When LLM Meets Domain
    Experts. In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. (2023b) Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    and Yongfeng Zhang. 2023b. LLM as OS, Agents as Apps: Envisioning AIOS, Agents
    and the AIOS-Agent Ecosystem. *arXiv* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geng et al. (2024) Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski,
    and Robert West. 2024. Sketch-Guided Constrained Decoding for Boosting Blackbox
    Large Language Models without Logit Access. *arXiv preprint arXiv:2401.09967*
    (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geng et al. (2023) Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert
    West. 2023. Grammar-constrained decoding for structured NLP tasks without finetuning.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*. 10932–10952.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022) Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng
    Gong, and Bing Qin. 2022. A Distributional Lens for Multi-Aspect Controllable
    Text Generation. In *Proceedings of the 2022 Conference on Empirical Methods in
    Natural Language Processing*, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang
    (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,
    1023–1043. [https://doi.org/10.18653/v1/2022.emnlp-main.67](https://doi.org/10.18653/v1/2022.emnlp-main.67)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hemmer et al. (2023) Arthur Hemmer, Mickael Coustaty, Nicola Bartolo, Jerome
    Brachat, and Jean-Marc Ogier. 2023. Lazy-k Decoding: Constrained Decoding for
    Information Extraction. In *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*. 6727–6736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopcroft et al. (2001) John E Hopcroft, Rajeev Motwani, and Jeffrey D Ullman.
    2001. Introduction to automata theory, languages, and computation. *Acm Sigact
    News* 32, 1 (2001), 60–65.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation
    of Large Language Models. arXiv:2106.09685 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua et al. (2023) Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji,
    Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. 2023. War and peace (waragent):
    Large language model-based multi-agent simulation of world wars. *arXiv:2311.17227*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for
    Embodied Agents. In *Proceedings of the 39th International Conference on Machine
    Learning* *(Proceedings of Machine Learning Research, Vol. 162)*, Kamalika Chaudhuri,
    Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (Eds.).
    PMLR, 9118–9147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. BART:
    Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension. arXiv:1910.13461 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023a. CAMEL: Communicative Agents for "Mind"
    Exploration of Large Language Model Society. In *Thirty-seventh Conference on
    Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Lei Li, Li Chen, and Yongfeng Zhang. 2020. Towards controllable
    explanation generation for recommender systems via neural template. In *Companion
    proceedings of the web conference 2020*. 198–202.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu,
    Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. 2023b. Trocr: Transformer-based
    optical character recognition with pre-trained models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 37\. 13094–13102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs
    as Agents. arXiv:2308.03688 [cs.AI]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022) Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui
    Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable
    text generation with reinforced unlearning. *Advances in neural information processing
    systems* 35 (2022), 27591–27609.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) Josh et al OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. (2023) Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng
    Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    2023. Communicative Agents for Software Development. arXiv:2307.07924 [cs.SE]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2022) Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi.
    2022. Cold decoding: Energy-based constrained text generation with langevin dynamics.
    *Advances in Neural Information Processing Systems* 35 (2022), 9538–9551.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng
    Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,
    Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning
    Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,
    Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu,
    and Maosong Sun. 2023. Tool Learning with Foundation Models. arXiv:2304.08354 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research* 21, 1 (2020), 5485–5551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion
    models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*. 10684–10695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2020) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2020. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. arXiv:1910.01108 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
    Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takase and Okazaki (2019) Sho Takase and Naoaki Okazaki. 2019. Positional Encoding
    to Control Output Sequence Length. In *Proceedings of the 2019 Conference of the
    North*. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. arXiv:2302.13971 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    arXiv:2307.09288 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin
    Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022. GIT: A Generative Image-to-text
    Transformer for Vision and Language. arXiv:2205.14100 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Ji-Rong Wen. 2023. A Survey on Large Language Model based Autonomous
    Agents. arXiv:2308.11432 [cs.AI]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) Ronald J Williams. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. *Machine learning* 8 (1992),
    229–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao
    Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter
    Vajda. 2020. Visual Transformers: Token-based Image Representation and Processing
    for Computer Vision. arXiv:2006.03677 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. The
    Rise and Potential of Large Language Model Based Agents: A Survey. arXiv:2309.07864 [cs.AI]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2024) Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang
    Shen, Ren Kan, Dongsheng Li, and Deqing Yang. 2024. EASYTOOL: Enhancing LLM-based
    Agents with Concise Tool Instruction. *arXiv preprint arXiv:2401.06201* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zamir et al. (2022) Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat,
    Fahad Shahbaz Khan, and Ming-Hsuan Yang. 2022. Restormer: Efficient transformer
    for high-resolution image restoration. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 5728–5739.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng,
    Angela S Lin, Tianhe Yu, and Alexei A Efros. 2017. Real-time user-guided image
    colorization with learned deep priors. *ACM Transactions on Graphics (TOG)* 36,
    4 (2017), 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Impact Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '¹¹footnotetext: [https://github.com/richzhang/colorization](https://github.com/richzhang/colorization)²²footnotetext:
    [https://huggingface.co/caidas/swin2SR-classical-sr-x2-64](https://huggingface.co/caidas/swin2SR-classical-sr-x2-64)³³footnotetext:
    [https://github.com/swz30/Restormer](https://github.com/swz30/Restormer)⁴⁴footnotetext:
    [https://huggingface.co/google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)⁵⁵footnotetext:
    [https://huggingface.co/facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)⁶⁶footnotetext:
    [https://huggingface.co/nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)⁷⁷footnotetext:
    [https://huggingface.co/CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)⁸⁸footnotetext:
    [https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)⁹⁹footnotetext:
    [https://huggingface.co/facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)^(10)^(10)footnotetext:
    [https://huggingface.co/t5-base](https://huggingface.co/t5-base)^(11)^(11)footnotetext:
    [https://huggingface.co/distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)^(12)^(12)footnotetext:
    [https://huggingface.co/gpt2](https://huggingface.co/gpt2)^(13)^(13)footnotetext:
    [https://huggingface.co/microsoft/git-base-textvqa](https://huggingface.co/microsoft/git-base-textvqa)^(14)^(14)footnotetext:
    [https://huggingface.co/distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)'
  prefs: []
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none of which we feel
    must be specifically highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Bechmark Tasks and Tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The tool list integrated in the OpenAGI platform (Ge et al., [2023a](#bib.bib13))
    is shown in Table [3](#A2.T3 "Table 3 ‣ B.1 Bechmark Tasks and Tools ‣ Appendix
    B Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for
    Controllable LLM-based Agents") and the different types of tasks as well as example
    tasks are shown in Table [4](#A2.T4 "Table 4 ‣ B.3 Prompt for Benchmark Tasks
    ‣ Appendix B Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input Modality | Output Modality | Tool Description | Expert Model |'
  prefs: []
  type: TYPE_TB
- en: '| Image | Image ($A$) | Colorizer[A](#footnotex1 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Zhang et al., [2017](#bib.bib49)) |'
  prefs: []
  type: TYPE_TB
- en: '| Super-Resolution ($a_{2}$) | Swin2SR[A](#footnotex2 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Conde et al., [2022](#bib.bib10)) |'
  prefs: []
  type: TYPE_TB
- en: '| Image Denoising ($a_{3}$) | Restormer[A](#footnotex3 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Zamir et al., [2022](#bib.bib48)) |'
  prefs: []
  type: TYPE_TB
- en: '| Image Deblurring ($a_{4}$) | Restormer[A](#footnotex3 "Appendix A Impact
    Statements ‣ Formal-LLM: Integrating Formal Language and Natural Language for
    Controllable LLM-based Agents") (Zamir et al., [2022](#bib.bib48)) |'
  prefs: []
  type: TYPE_TB
- en: '| Text ($B$) | ViT[A](#footnotex4 "Appendix A Impact Statements ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents")
    (Dosovitskiy et al., [2021](#bib.bib11)) |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection ($b_{2}$) | DETR[A](#footnotex5 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Carion et al., [2020](#bib.bib4)) |'
  prefs: []
  type: TYPE_TB
- en: '| Image Captioning ($b_{3}$) | Vision Encoder Decoder[A](#footnotex6 "Appendix
    A Impact Statements ‣ Formal-LLM: Integrating Formal Language and Natural Language
    for Controllable LLM-based Agents") (Li et al., [2023b](#bib.bib27)) |'
  prefs: []
  type: TYPE_TB
- en: '| Text | Image ($C$) | Stable Diffusion[A](#footnotex7 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Rombach et al., [2022](#bib.bib36)) |'
  prefs: []
  type: TYPE_TB
- en: '| Text ($D$) | DistilBERT[A](#footnotex8 "Appendix A Impact Statements ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents")
    (Sanh et al., [2020](#bib.bib37)) |'
  prefs: []
  type: TYPE_TB
- en: '| Text Summarization ($d_{2}$) | BART[A](#footnotex9 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Lewis et al., [2019](#bib.bib24)) |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Translation ($d_{3}$) | T5[A](#footnotex10 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Raffel et al., [2020](#bib.bib35)) |'
  prefs: []
  type: TYPE_TB
- en: '| Fill Mask ($d_{4}$) | DistilBERT[A](#footnotex11 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Sanh et al., [2020](#bib.bib37)) |'
  prefs: []
  type: TYPE_TB
- en: '| Text Generation ($d_{5}$) | GPT-2[A](#footnotex12 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Radford et al., [2019](#bib.bib34)) |'
  prefs: []
  type: TYPE_TB
- en: '| Image-Text Pair | Text ($E$) | GIT[A](#footnotex13 "Appendix A Impact Statements
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents") (Wang et al., [2022](#bib.bib42)) |'
  prefs: []
  type: TYPE_TB
- en: '| Text-Text Pair | Text ($F$) | DistilBERT[A](#footnotex14 "Appendix A Impact
    Statements ‣ Formal-LLM: Integrating Formal Language and Natural Language for
    Controllable LLM-based Agents") (Sanh et al., [2020](#bib.bib37)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Tool list integrated in OpenAGI platform (Ge et al., [2023a](#bib.bib13))
    for benchmark tasks. The tools are categorized into six primary groups according
    to the input and output modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our framework and all baselines are implemented by PyTorch, an open-source library.
    We follow the implementation setting of the OpenAGI platform (Ge et al., [2023a](#bib.bib13))
    for baselines. For the F-LLM+RLTF framework, we use the REINFORCE (Williams, [1992](#bib.bib44))
    as the core reinforcement learning algorithm of RLTF. We use the original checkpoint
    of each backbone LLM without supervised fine-tuning. We set the maximum number
    of updating epochs at 30 and use Adam as the optimizer with the learning rate
    at 0.001 for RLTF. Also, we apply Low-Rank Adaptation (LoRA) (Hu et al., [2021](#bib.bib21))
    to the RLTF for efficient fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Prompt for Benchmark Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An example of the CFG and the corresponding PDA are shown in Eq.([3](#S4.E3
    "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents")) and Eq.([4](#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint
    to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language
    and Natural Language for Controllable LLM-based Agents")) as well as Figure [3](#S4.F3
    "Figure 3 ‣ 4.3 Formal-LLM Prompts and Planning from Automaton ‣ 4 The Formal-LLM
    Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents"). Different benchmark tasks share a common subset of constraints
    shown in Eq.([3](#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton
    ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural
    Language for Controllable LLM-based Agents")) corresponding to the input-output
    constraints of the tools. Besides, each task has it own contraint on the input-outpt
    of the task, resulting in different contraints in Eq.([4](#S4.E4 "Equation 4 ‣
    4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents")),
    and thus the final PDA (Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Formal-LLM Prompts and
    Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating
    Formal Language and Natural Language for Controllable LLM-based Agents")) for
    different tasks are different.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-shot Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem:  {task_description}.What  is  its  soltuion?  Use  ’Setp’  to  mark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Few-shot Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '{few  shot  examples  in  the  format  of:{Problem:  {task_description}.Solution:Step  1:  ...Step  2:  ......Step  k:  ...}}Problem:  {task_description}.Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RLTF Prompt (RLTF executes the solution and use the performance as reward to
    fine-tune the LLM):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem:  {task_description}.Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt (zero-shot):'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  will  help  me  generate  a  plan  for  the  problem:  {task_description}  by  answering  a  series  of  my  questions.{current_progress}To  get  the  {data_modality},  we  have  the  following  choices:{choice_list}Your  answer  should  be  only  an  integer,  referring  to  the  desired  choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM + RLTF (execute the solution of Formal-LLM and use the performance
    as reward to fine-tune the LLM):'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  will  help  me  generate  a  plan  for  the  problem:  {task_description}  by  answering  a  series  of  my  questions.{current_progress}To  get  the  {data_modality},  we  have  the  following  choices:{choice_list}Your  answer  should  be  only  an  integer,  referring  to  the  desired  choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  will  help  me  generate  a  plan  for  the  problem:  "Given  a  grayscale  image,  how  to  return  the  regular  image  step  by  step?"  by  answering  a  series  of  my  questions.Current  Progress:Step  n:  Use  Image  Super  Resolution;Step  (n-1):  ?To  get  the  input  image  of  "Image  Super  Resolution",  we  have  the  following  choices:1:  the  output  of  Colorization,2:  the  output  of  Image  Denoising,3:  the  output  of  Image  Deblurring,4:  the  output  of  Text  to  Image  Generation,5:  Input  Image.Your  answer  should  be  only  an  integer,  referring  to  the  desired  choice.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Metrics | Output | Label | Evaluation | Task Example |'
  prefs: []
  type: TYPE_TB
- en: '| Task 1 | CLIP Score | Image | Text | Text-to-Image similarity | Given clozed
    English text, how to |'
  prefs: []
  type: TYPE_TB
- en: '| generate an image step by step? |'
  prefs: []
  type: TYPE_TB
- en: '| Task 2 | BERT Score | Text | Text | Text-to-Text similarity | Given noisy
    grayscale image, how to return |'
  prefs: []
  type: TYPE_TB
- en: '| the caption in German step by step? |'
  prefs: []
  type: TYPE_TB
- en: '| Task 3 | ViT Score | Image | Image | Image-to-Image similarity | Given blurry
    grayscale image, how to return |'
  prefs: []
  type: TYPE_TB
- en: '| the regular image step by step? |'
  prefs: []
  type: TYPE_TB
- en: '| Task X | Corresponding Score | / | / | / | Given low-resolution noisy blurry
    grayscale |'
  prefs: []
  type: TYPE_TB
- en: '| image and English query, how to answer the |'
  prefs: []
  type: TYPE_TB
- en: '| question in German step by step? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Benchmark task examples under each category. Task X is a subset of
    “Task 1 $\cup$ Task 3”, which is a subset of tasks that require a tree-structured
    plan rather than chain-structured plan due to the use of many input-single-output
    tools. Task X is used to test the complex planning ability of our Formal-LLM framework.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Prompt for Real-life Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: B.4.1 Daily Plan
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The automaton is displayed in Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Formal-LLM
    Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM:
    Integrating Formal Language and Natural Language for Controllable LLM-based Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate  a  plan  for  activities  between  10:00  and  20:00.Breakfast,  lunch,  and  supper  need  1  hour  each.Outdoor  activities:  basketball  playing  13:00  -  15:00;  do  grocery  shopping  needs  1  hour.Indoor  activities:  house  cleaning  needs  1  hour;  homework  needs  three  hours;  turning  on  the  washer/laundry  machine  needs  0  minutes  but  needs  to  stay  home  for  one  hour.Other  constrain:Cannot  play  basketball  within  an  hour  after  a  meal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate  a  plan  for  activities  between  10:00  and  20:00.Breakfast,  lunch,  and  supper  need  1  hour  each.Outdoor  activities:  basketball  playing  13:00  -  15:00;  do  grocery  shopping  needs  1  hour.Indoor  activities:  house  cleaning  needs  1  hour;  homework  needs  three  hours;  turning  on  the  washer/laundry  machine  needs  0  minutes  but  needs  to  stay  home  for  one  hour.Other  constrain:Cannot  play  basketball  within  an  hour  after  a  meal.Let’s  start  planning  from  the  end.{current_progress}Decide  on  the  activity  ending  at  {current_hour}:00.Here  are  possible  options:{choice_list}Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate  a  plan  for  activities  between  10:00  and  20:00.Breakfast,  lunch,  and  supper  need  1  hour  each.Outdoor  activities:  basketball  playing  13:00  -  15:00;  do  grocery  shopping  needs  1  hour.Indoor  activities:  house  cleaning  needs  1  hour;  homework  needs  three  hours;  turning  on  the  washer/laundry  machine  needs  0  minutes  but  needs  to  stay  home  for  one  hour.Other  constrain:Cannot  play  basketball  within  an  hour  after  a  meal.Let’s  start  planning  from  the  end.Current  Progress:17:00  -  20:00  Doing  homework.Decide  on  the  activity  ending  at  17:00.Here  are  possible  options:1.  Eating  supper.2.  Grocery  shopping.3.  House  cleaning.4.  Turning  on  the  washer/laundry  machine.5.  Doing  nothing  for  one  hour.Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4.2 Cooking Recipe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The formal language for the cooking task is a CFG, shown as Eq.([5](#A2.E5
    "Equation 5 ‣ B.4.2 Cooking Recipe ‣ B.4 Prompt for Real-life Tasks ‣ Appendix
    B Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for
    Controllable LLM-based Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}&amp;S\rightarrow aAB\\ &amp;B\rightarrow aCD&#124;aCE\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;C\rightarrow aF\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;D\rightarrow aAI\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;E\rightarrow bH\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;F\rightarrow bAG\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;G\rightarrow cd\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;H\rightarrow ef&#124;eI\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;I\rightarrow cf\end{split}$$ |  | (5) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where nonterminals: $S$ is for broccoli.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate  a  broccoli  beef  cooking  plan.The  ingredients  include  raw  beef  slices,  carpaccio,  broccoli,  onions,  ginger,  garlic,  and  water.The  seasonings  include  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper,  sugar,  vinegar.Cooking  utensils  including  woks  and  cooking  pots.Tableware  including  chopsticks,  spoons,  wok  spoons,  and  several  bowls.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate  a  broccoli  beef  cooking  plan.The  ingredients  include  raw  beef  slices,  carpaccio,  broccoli,  onions,  ginger,  garlic,  and  water.The  seasonings  include  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper,  sugar,  vinegar.Cooking  utensils  including  woks  and  cooking  pots.Tableware  including  chopsticks,  spoons,  wok  spoons,  and  several  bowls.{current_progress}Decide  on  the  previous  step  before  current  progress.Here  are  possible  options  to  get  {target_item}  for  the  step:  {parent_step}.{choice_list}Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate  a  broccoli  beef  cooking  plan.The  ingredients  include  raw  beef  slices,  carpaccio,  broccoli,  onions,  ginger,  garlic,  and  water.The  seasonings  include  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper,  sugar,  vinegar.Cooking  utensils  including  woks  and  cooking  pots.Tableware  including  chopsticks,  spoons,  wok  spoons,  and  several  bowls.Current  Progress:Step  n:  Then,  we  get  the  cooked  broccoli  beef.Step  n-1:  Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok.Step  n-2:  Prepare  the  seasoning:  ginger,  garlic,  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper  for  the  step:  "Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok."Step  n-3:  ?Decide  on  the  previous  step  before  current  progress.Here  are  possible  options  to  get  the  mixture  of  beef  and  broccoli  for  the  step:  "Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok."1:  Combine  lightly  cooked  beef  and  lightly  cooked  broccoli  in  a  wok.2:  Combine  lightly  cooked  beef  and  blanched  and  drained  broccoli  in  a  wok.Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4.3 Risk Management
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b0e28323bc63f3a051ce6f0609f1e47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The flowchart for risk management task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Domain experts draw a flowchart of risk management as Figure [6](#A2.F6 "Figure
    6 ‣ B.4.3 Risk Management ‣ B.4 Prompt for Real-life Tasks ‣ Appendix B Appendix
    ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents"). The flowchart is essentially a Deterministic Finite Automaton
    (DFA), which is a special case of Pushdown Automaton (PDA). A question is generated
    to inquiry LLM when the automaton reaches a new state. The LLM can select a reasonable
    branch according to its knowledge of the specific seller, buyer, and company name.
    In the experiment, the seller is Shareholders of Blizzard Entertainment, the company
    is Blizzard Entertainment, and the buyer is Microsoft.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task:  You  are  a  plan  maker  to  design  a  risk  management  plan  for  deals  related  to  companies.Scenario:  {Seller  A}  wants  to  sell  the  {Company  B}  which  {Buyer  C}  wants  to  buy.  Design  a  risk  management  plan  for  {Buyer  C}  before  negotiating  a  detailed  contract.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task:  You  are  a  plan  maker  to  design  a  risk  management  plan  for  deals  related  to  companies.Scenario:  {Seller  A}  wants  to  sell  the  {Company  B}  which  {Buyer  C}  wants  to  buy.  Design  a  risk  management  plan  for  {Buyer  C}  before  negotiating  a  detailed  contract.{question  in  the  flowchart}{choice_list}Your  answer  should  be  only  one  number,  such  as  1,  referring  to  the  option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal-LLM Prompt Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task:  You  are  a  plan  maker  to  design  a  risk  management  plan  for  deals  related  to  companies.Scenario:  Shareholders  of  Blizzard  Entertainment  wants  to  sell  the  Blizzard  Entertainment  which  Microsoft  wants  to  buy.  Design  a  risk  management  plan  for  Microsoft  before  negotiating  a  detailed  contract.Question:  Does  Microsoft  buying  Blizzard  Entertainment  trigger  mandatory  antitrust  filing?1:  Yes.2:  No.3:  Insufficient  information  to  make  a  judgment.Your  answer  should  be  only  one  number,  such  as  1,  referring  to  the  option.'
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Complete Results of Real-life Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Daily plan result from GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '10:00  -  11:00  :  Breakfast11:00  -  12:00  :  House  Cleaning12:00  -  13:00  :  Free  Time13:00  -  15:00  :  Basketball  Playing15:00  -  16:00  :  Free  Time16:00  -  17:00  :  Lunch17:00  -  18:00  :  Grocery  Shopping18:00  -  19:00  :  Homework19:00  -  20:00  :  Laundry  and  Supper20:00  -  23:00  :  Continue  HomeworkIn  this  plan,  basketball  playing  is  scheduled  from  13.00  to  15.00  with  an  hour  after  breakfast  to  avoid  playing  right  after  eating  as  per  the  constraint.  Lunch  is  then  scheduled  after  basketball  and  Shopping  done  after  that.  Since  laundry  doesn’t  take  time,  it  has  been  added  with  supper  and  rest  of  the  time  is  allocated  to  homework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Daily plan result after applying Formal-LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '10:00  -  10:00  Turning  on  the  washer/laundry  machine.10:00  -  11:00  House  cleaning.11:00  -  12:00  Eating  breakfast.12:00  -  13:00  Grocery  shopping.13:00  -  15:00  Playing  basketball.15:00  -  16:00  Eating  lunch.16:00  -  19:00  Doing  homework.19:00  -  20:00  Eating  supper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cooking recipe result from GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingredients  Preparation1.  Sliced  Beef:  300  grams2.  Chinese  Broccoli  (Gai  Lan):  As  needed,  cut  into  segments3.  Scallions:  1  stalk,  cut  into  sections4.  Ginger:  As  needed,  sliced5.  Garlic:  As  needed,  minced6.  Water:  As  needed7.  Cooking  Oil:  As  needed8.  Salt:  As  needed9.  Light  Soy  Sauce:  2  tablespoons10.  Cooking  Wine:  1  tablespoon11.  White  Pepper  Powder:  As  needed12.  Sugar:  1  tablespoon13.  Vinegar:  1  tablespoonCooking  Utensils  and  Tableware  Preparation1.  Stir-fry  Pan:  12.  Cooking  Pot:  13.  Tableware:  Chopsticks,  Soup  Spoon,  Stir-fry  Pan  Spoon,  Several  BowlsCooking  StepsPreparatory  Steps1.  Marinate  the  beef  slices  with  salt  and  cooking  wine,  stirring  evenly,  and  let  it  marinate  for  10  minutes.2.  Heat  the  stir-fry  pan  and  add  an  appropriate  amount  of  cooking  oil.Sautéing  the  Beef1.  Once  the  oil  is  hot,  quickly  stir-fry  the  marinated  beef  slices  until  they  change  color.2.  Add  scallion  sections,  ginger  slices,  and  minced  garlic.  Continue  stirring  until  a  fragrant  aroma  is  released.Sautéing  the  Chinese  Broccoli1.  In  the  stir-fry  pan,  add  water.  Once  the  water  boils,  add  the  Chinese  broccoli.2.  Quickly  stir-fry  the  Chinese  broccoli  until  it  changes  color,  and  season  with  salt.Seasoning1.  Add  light  soy  sauce,  white  pepper  powder,  sugar,  and  vinegar.  Stir  well.2.  Continue  stirring  to  ensure  that  the  seasoning  is  evenly  coated  on  the  beef  and  Chinese  broccoli.Completion1.  Ensure  the  beef  is  cooked  through,  and  the  Chinese  broccoli  is  flavorful.  Remove  from  heat.Serving1.  Transfer  the  Chinese  Broccoli  Beef  to  bowls.2.  Serve  with  rice  or  noodles.Tips1.  Avoid  marinating  the  beef  slices  for  too  long  to  maintain  a  good  texture.2.  Control  the  sautéing  time  for  Chinese  broccoli  to  preserve  its  crisp  and  tender  texture.3.  Adjust  the  seasoning  according  to  personal  taste  preferences,  adding  or  reducing  the  amount  of  salt,  sugar,  and  vinegar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cooking recipe result after applying Formal-LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here  is  the  final  plan:Step  1:  Wash  raw  beef  slices  with  water.Step  2:  Prepare  the  seasoning:  ginger,  garlic,  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper  for  the  step:  "Marinate  clean  slices  of  beef  in  a  bowl  with  seasoning".Step  3:  Marinate  clean  slices  of  beef  in  a  bowl  with  seasoning.Step  4:  Stir-fry  the  marinaded  beef  in  a  wok.Step  5:  Wash  broccoli  with  water.Step  6:  Blanch  clean  broccoli  in  a  cooking  pot.Step  7:  Use  a  bowl  to  drain  the  blanched  broccoli.Step  8:  Combine  lightly  cooked  beef  and  blanched  and  drained  broccoli  in  a  wok.Step  9:  Prepare  the  seasoning:  ginger,  garlic,  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper  for  the  step:  "Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok".Step  10:  Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok.Step  11:  Then,  we  get  the  cooked  broccoli  beef.Carefully  cooking  at  each  step!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Risk management result from GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Risk  Management  Plan:Objective:  To  ensure  a  beneficial  acquisition  of  Blizzard  Entertainment  by  Microsoft  while  managing  and  mitigating  potential  risks.I.  Risk  Identification1.  Financial  Risks:  A  complete  and  thorough  financial  audit  of  Blizzard  must  be  performed  to  determine  potential  financial  risks,  such  bright  as  debts,  pending  lawsuit  settlements,  past  or  ongoing  tax  issues,  declining  revenue,  etc.2.  Operational  Risks:  Understand  Blizzard’s  operational  weaknesses  and  strengths,  and  the  impact  of  the  acquisition  on  Microsoft’s  operations.3.  Legal  Risks:  Review  Blizzard’s  legal  compliance  status  in  all  jurisdictions  it  operates  in.  Especially,  risks  related  to  intellectual  property  rights,  privacy  and  data  security  laws,  and  labor  laws  must  be  evaluated.4.  Regulatory  Risks:  Determine  any  potential  regulatory  approvals  Microsoft  might  need  for  the  acquisition.II.  Risk  Assessment:1.  Quantify  each  identified  risk  wherever  possible,  in  terms  of  potential  financial  impact,  time  delay,  brand  damage  etc.2.  Determine  the  likelihood  of  each  risk  occurrence.3.  Prioritize  the  risks  based  on  their  potential  impact  and  the  likelihood  of  occurrence.III.  Risk  Mitigation  Planning:1.  Financial  Risk:  This  could  be  mitigated  through  leverage  and  insurance,  renegotiation  of  terms,  or  such  other  financial  instruments.2.  Operational  Risk:  To  mitigate  this  risk,  Microsoft  can  plan  to  gradually  integrate  Blizzard,  keeping  key  management  personnel,  or  finding  synergies  with  Microsoft’s  current  operations.3.  Legal  Risk:  Microsoft  should  get  warranties  and  indemnities  from  Blizzard’s  owners  to  cover  potential  post-acquisition  legal  disputes.4.  Regulatory  Risk:  Early  engagement  with  regulators  and  professional  advice  can  help  smooth  the  acquisition  process.IV.  Risk  Monitoring:A  system  to  continuously  monitor  and  manage  these  risks  should  be  set  up.  This  could  involve  regular  reporting,  monitoring  industry  developments,  legal  changes  etc.V.  Contingency  Planning:Establish  contingency  plans  for  high-impact  risks.  For  instance,  a  backup  plan  if  the  acquisition  fails,  or  if  post-acquisition  integration  does  not  go  as  planned.VI.  Communication:Continual  communication  with  stakeholders  to  obtain  feedback  and  to  convey  risk  management  efforts.  This  will  ensure  that  all  parties  are  aware  of  the  risks  and  the  measures  being  taken  to  mitigate  them.VII.  Insurance  Cover:Consider  adequate  insurance  coverage  that  includes  Directors’  and  Officers’  Liability  and  Professional  Indemnity  Insurance  for  all  potential  issues  that  might  arise  in  connection  with  the  proposed  transaction.The  above  steps  will  form  the  roadmap  for  Microsoft  while  negotiating  terms  for  the  acquisition  of  Blizzard  Entertainment.  Understanding  risks  before  they  come  up  ensures  that  Microsoft  isn’t  surprised  during  negotiations,  hence  effectively  mitigating  them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Risk management result after applying Formal-LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here  is  the  final  plan  for  Microsoft:1:  We  need  more  information  to  ensure  Shareholders  of  Blizzard  Entertainment  is  entitled  to  sell  Blizzard  Entertainment.  But  we  assume  we  have  found  out  enough  information  to  proceed  the  risk  assessment  process.2:  Then,  based  on  current  information,  we  believe  Blizzard  Entertainment  is  in  good  standing  without  material  outstanding  liabilities.3:  Then,  based  on  current  information,  we  believe  neither  CFIUS  filings  nor  foreign  investment  filings  in  China  are  needed.4:  Then,  we  need  to  submit  the  regional  antitrust  filing  of  the  United  States  and/or  China.  But  we  assume  the  filings  will  be  approved  to  proceed  the  risk  assessment  process.5:  Pass  the  preliminary  risk  analysis.  Details  to  be  discussed  in  the  transaction  documents.Note:  This  is  a  risk  assessment  process  provided  based  on  current  information.  Please  ensure  the  accuracy  of  the  provided  information  and  possible  additional  supplemental  information.'
  prefs: []
  type: TYPE_NORMAL
