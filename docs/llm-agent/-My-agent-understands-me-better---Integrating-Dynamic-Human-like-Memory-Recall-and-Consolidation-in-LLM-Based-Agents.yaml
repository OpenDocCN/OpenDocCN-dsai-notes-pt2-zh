- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:48:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我的代理理解我更好”：在基于LLM的代理中整合动态类人记忆回忆与巩固
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00573](https://ar5iv.labs.arxiv.org/html/2404.00573)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00573](https://ar5iv.labs.arxiv.org/html/2404.00573)
- en: Yuki Hou [houhoutime@gmail.com](mailto:houhoutime@gmail.com) Meiji UniversityTokyoJapan
    ,  Haruki Tamoto [harukiririwiru@gmail.com](mailto:harukiririwiru@gmail.com) Kyoto
    UniversityKyotoJapan  and  Homei Miyashita [homei@homei.com](mailto:homei@homei.com)
    Meiji UniversityTokyoJapan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuki Hou [houhoutime@gmail.com](mailto:houhoutime@gmail.com) 明治大学东京日本，Haruki
    Tamoto [harukiririwiru@gmail.com](mailto:harukiririwiru@gmail.com) 京都大学京都日本，以及
    Homei Miyashita [homei@homei.com](mailto:homei@homei.com) 明治大学东京日本
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In this study, we propose a novel human-like memory architecture designed for
    enhancing the cognitive abilities of large language model (LLM)-based dialogue
    agents. Our proposed architecture enables agents to autonomously recall memories
    necessary for response generation, effectively addressing a limitation in the
    temporal cognition of LLMs. We adopt the human memory cue recall as a trigger
    for accurate and efficient memory recall. Moreover, we developed a mathematical
    model that dynamically quantifies memory consolidation, considering factors such
    as contextual relevance, elapsed time, and recall frequency. The agent stores
    memories retrieved from the user’s interaction history in a database that encapsulates
    each memory’s content and temporal context. Thus, this strategic storage allows
    agents to recall specific memories and understand their significance to the user
    in a temporal context, similar to how humans recognize and recall past experiences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一种新颖的类人记忆架构，旨在提升大型语言模型（LLM）基础对话代理的认知能力。我们提出的架构使代理能够自主回忆生成响应所需的记忆，有效解决了LLM在时间认知上的局限性。我们采用了类人记忆提示回忆作为准确高效记忆回忆的触发器。此外，我们开发了一种数学模型，动态量化记忆巩固，考虑了上下文相关性、时间流逝和回忆频率等因素。代理将从用户交互历史中检索到的记忆存储在一个数据库中，该数据库封装了每个记忆的内容和时间上下文。因此，这种战略存储使代理能够回忆特定记忆，并理解其在时间上下文中的重要性，类似于人类识别和回忆过去的经历。
- en: 'Memory Retrieval Models, Large Language Models, Intelligent Agents, User Experience^†^†conference:
    Extended Abstracts of the CHI Conference on Human Factors in Computing Systems;
    May 11–16, 2024; Honolulu, HI, USA^†^†booktitle: Extended Abstracts of the CHI
    Conference on Human Factors in Computing Systems (CHI EA ’24), May 11–16, 2024,
    Honolulu, HI, USA![Refer to caption](img/3828b02e4f33e2fadf6a48f8cd887649.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆检索模型、大型语言模型、智能代理、用户体验^†^†会议：CHI 人机交互系统会议扩展摘要；2024年5月11日–16日；美国夏威夷檀香山^†^†书名：CHI
    人机交互系统会议扩展摘要（CHI EA ’24），2024年5月11日–16日，美国夏威夷檀香山![参考图注](img/3828b02e4f33e2fadf6a48f8cd887649.png)
- en: Figure 1\. Architecture of the enhanced large language model (LLM)-based dialogue
    agent that integrates human-like memory processes. First, the user input is converted
    into vectorized text and processed through a data-filtering process based on relevance
    and memory consolidation bias, modeled after human cognitive functions. Then,
    memory recall is triggered when the recall probability, informed by relevance
    and elapsed time, exceeds a predefined threshold. This diagram features an agent
    output example where the system recalls ”Creamy pasta” as the user’s lunch preference
    with a higher frequency, influencing the agent’s response.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 强化的大型语言模型（LLM）基础对话代理的架构，该架构整合了类人记忆过程。首先，用户输入被转换为向量化文本，并通过基于相关性和记忆巩固偏见的数据过滤过程进行处理，该过程模仿人类认知功能。然后，当回忆概率受到相关性和时间流逝的影响，超过预定义的阈值时，触发记忆回忆。该图展示了一个代理输出示例，其中系统以更高的频率回忆“奶油意面”作为用户的午餐偏好，从而影响代理的响应。
- en: \Description
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: This diagram illustrates the architecture of an enhanced Large Language Model
    (LLM)-based dialogue agent that integrates human-like memory processes. The user
    input is first converted into vectorized text and processed through a data-filtering
    process based on relevance and memory consolidation bias, modeled after human
    cognitive functions. Memory recall is triggered when the recall probability, informed
    by relevance and elapsed time, exceeds a predefined threshold. The diagram features
    an agent output example where the system recalls ”Creamy pasta” as the user’s
    lunch preference with higher frequency, influencing the agent’s response. The
    proposed model emphasizes the role of memory consolidation and cued recall, significantly
    improving the agent’s response relevance and coherence in conversations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该图示展示了一个增强型大型语言模型（LLM）对话代理的架构，该代理集成了类似人类的记忆过程。用户输入首先被转换为向量化文本，并通过基于相关性和记忆巩固偏见的数据过滤过程进行处理，这一过程模拟了人类的认知功能。当记忆回忆的概率，由相关性和经过的时间决定，超过预定义的阈值时，会触发记忆回忆。该图示展示了一个代理输出的示例，其中系统以更高的频率回忆起“奶油意大利面”作为用户的午餐偏好，从而影响代理的响应。提出的模型强调了记忆巩固和提示回忆的作用，显著提高了代理在对话中的响应相关性和连贯性。
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'The emergence of transformer-based language models (Lin et al., [2022](#bib.bib15))
    have drastically revolutionized the field of natural language processing, surpassing
    the capabilities of traditional models in understanding and generating human-like
    text (Sun et al., [2019](#bib.bib23)). In particular, large language models (LLMs)
    (Dao, [2023](#bib.bib6)) have garnered considerable attention for their prowess
    in mimicking artificial intelligence (AI) with human-like cognition and conversational
    abilities, reminiscent of sentient machines portrayed in science fiction narratives.
    However, LLMs exhibit a significant limitation in processing temporal information
    inherent to human cognition. While transformers possess excellent self-attention
    mechanisms, outperforming recurrent neural networks (RNNs) (Mandic and Chambers,
    [2001](#bib.bib16)) and long short-term memory models (LSTM) (Sundermeyer et al.,
    [2012](#bib.bib24)), they fail to replicate human behavioral dynamics. To accurately
    replicate the nuanced human-like interactions of AI agents, as depicted in science
    fiction, one must first achieve human-like cognitive and memory processing abilities.
    Therefore, we proposed an approach to integrate human memory processes into LLM-based
    dialogue agents [1](#S0.F1 "Figure 1 ‣ ”My agent understands me better”: Integrating
    Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"). We adopted
    human-like cued recall as the trigger for accurate and efficient memory retrieval
    (McDaniel et al., [1989](#bib.bib17)). This mechanism involves an agent autonomously
    recalling memories essential for generating responses during a conversation. The
    process emulates the human memory process known as ”remember to remember” (Hécaen
    et al., [1978](#bib.bib10)), consciously retaining memory for future action or
    task and recalling that when needed (Kuhlmann, [2019](#bib.bib14)). Furthermore,
    the proposed model replicates human cognitive ability, where memories recalled
    repeatedly over a long period are retained more strongly than those recalled over
    a short period and relatively frequently (Roediger and Karpicke, [2006](#bib.bib22)),
    regardless of recall frequency. Thus, our model provides contextually relevant
    and coherent conversations.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '基于变换器的语言模型（Lin 等人，[2022](#bib.bib15)）的出现彻底革新了自然语言处理领域，超越了传统模型在理解和生成类人文本方面的能力（Sun
    等人，[2019](#bib.bib23)）。特别是，大型语言模型（LLMs）（Dao，[2023](#bib.bib6)）因其在模拟具有人类认知和对话能力的人工智能（AI）方面的能力而受到广泛关注，这些能力类似于科幻小说中描绘的有意识的机器。然而，LLMs
    在处理人类认知固有的时间信息方面存在显著的限制。尽管变换器具有出色的自注意机制，优于递归神经网络（RNNs）（Mandic 和 Chambers，[2001](#bib.bib16)）和长短期记忆模型（LSTM）（Sundermeyer
    等人，[2012](#bib.bib24)），但它们未能复制人类行为动态。为了准确复制科幻中描绘的类人AI代理的细微互动，必须首先实现类似人类的认知和记忆处理能力。因此，我们提出了一种将人类记忆过程集成到基于LLM的对话代理中的方法
    [1](#S0.F1 "Figure 1 ‣ ”My agent understands me better”: Integrating Dynamic Human-like
    Memory Recall and Consolidation in LLM-Based Agents")。我们采用了类似人类的提示回忆作为准确高效记忆检索的触发机制（McDaniel
    等人，[1989](#bib.bib17)）。这一机制涉及代理在对话过程中自主回忆生成响应所需的记忆。该过程模拟了人类记忆过程，即“记得记住”（Hécaen
    等人，[1978](#bib.bib10)），有意识地保留记忆以供未来的行动或任务，并在需要时进行回忆（Kuhlmann，[2019](#bib.bib14)）。此外，所提出的模型复制了人类认知能力，即那些在较长时间内重复回忆的记忆比在较短时间内和相对频繁回忆的记忆保留得更强（Roediger
    和 Karpicke，[2006](#bib.bib22)），无论回忆的频率如何。因此，我们的模型提供了具有上下文相关性和连贯性的对话。'
- en: Furthermore, our primary purpose is to transcend the paradigm of dialogue agents
    merely imitating human behavior through statistical natural language models. Instead,
    we seek to create agents that are capable of truly understanding human language
    with rich nuances, achieved by seamlessly integrating human cognitive processes.
    This fusion aligns with the philosophy of human-computer interaction, promoting
    more natural and intuitive human-centered interactions between the two at cognitive
    and emotional levels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的主要目的是超越仅仅通过统计自然语言模型模仿人类行为的对话代理范式。相反，我们希望创建能够真正理解具有丰富细微差别的人类语言的代理，通过无缝集成人类认知过程来实现。这种融合符合人机交互的理念，促进了在认知和情感层面上更加自然和直观的人本互动。
- en: '![Refer to caption](img/081d7b3aa8f4fad9a590f8100197cdc8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/081d7b3aa8f4fad9a590f8100197cdc8.png)'
- en: Figure 2\. (A) Decline in recall probability over time. The black curve ($r$.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. （A）随时间推移的回忆概率下降。黑色曲线（$r$）。
- en: \Description
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: This figure consists of two graphs. Graph A depicts the decline in recall probability
    over time. The black curve represents a scenario with a standard recall rate (r=1)
    and decay gradient (g=1), indicating a rapid loss of recall capability as time
    progresses. The red curve, illustrating a reduced recall rate (r=0.6) and higher
    decay gradient (g=2), represents a slower rate of forgetting. Graph B illustrates
    how memory is reinforced through repetition. At time t_0, Event D is recalled
    by the user, and the model updates the temporal significance of Event D. The recall
    of Event D at t_0 exemplifies how repeated recall makes the memory less susceptible
    to forgetting at time t_0+t.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包含两个图表。图 A 描述了随时间推移回忆概率的下降。黑色曲线代表标准回忆率（r=1）和衰减梯度（g=1）的情况，显示随着时间的推移回忆能力迅速丧失。红色曲线则展示了较低的回忆率（r=0.6）和较高的衰减梯度（g=2），代表了较慢的遗忘速率。图
    B 展示了如何通过重复来强化记忆。在时间 t_0，用户回忆事件 D，模型更新事件 D 的时间意义。在 t_0 时对事件 D 的回忆示例了如何使记忆在 t_0+t
    时不容易遗忘。
- en: 2\. Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. Similarities Between LLMs and Human Memory
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM与人类记忆的相似性
- en: Human memory serves as a system to encode, store, and retrieve our experiences
    (Tulving et al., [1972](#bib.bib26)). Our memories can be categorized into declarative
    and non-declarative memories, with declarative memory further divided into episodic
    and semantic memories (California, [1987](#bib.bib4)). Episodic memory (Tulving,
    [2002](#bib.bib25)) consciously allows for recollecting and re-experiencing one’s
    subjective past. In contrast, semantic memory supports language use, registering
    not the perceptual properties of inputs but the cognitive referents of input signals
    (Yamadori, [2002](#bib.bib28)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类记忆作为一个系统，用于编码、存储和提取我们的经历（Tulving 等，[1972](#bib.bib26)）。我们的记忆可以分为陈述性记忆和非陈述性记忆，其中陈述性记忆进一步分为情节记忆和语义记忆（California，[1987](#bib.bib4)）。情节记忆（Tulving，[2002](#bib.bib25)）可以让我们有意识地回忆和重新体验个人的主观过去。相对而言，语义记忆支持语言使用，不记录输入的感知属性，而是记录输入信号的认知指代（Yamadori，[2002](#bib.bib28)）。
- en: Similar to human episodic memory functioning, the episodic nature of LLMs’ is
    demonstrated by their ability to recall specific events or dialogues from the
    database. This allows LLMs to generate responses based on past interactions and
    experiences to inform current interactions. LLMs also possess a human-like semantic
    understanding of language that captures the meaning and context behind the words.
    Geva et al. (Geva et al., [2021](#bib.bib9)) suggested the feed-forward layers
    of transformer-based models to operate in a key-value format, the same as human
    semantic memory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于人类情节记忆的功能，LLM 的情节性质体现在它们能够从数据库中回忆特定事件或对话。这使得 LLM 能够基于过去的互动和经验生成回应，以告知当前的互动。LLM
    还具有人类般的语言语义理解，能够捕捉单词背后的意义和上下文。Geva 等人（Geva et al., [2021](#bib.bib9)）建议基于变换器的模型的前馈层以关键-值格式运行，与人类语义记忆相同。
- en: 2.2\. Human-like Memory Processes in AI Agent
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. AI代理中的类人记忆过程
- en: 'Kim et al. (Kim et al., [2022](#bib.bib12)) focused on emulating human episodic
    and semantic memory processes in AI agents to enhance interactive experiences.
    They compared agents with different memory processes: episodic only, semantic
    only, and both. These agents used different strategies to decide which memories
    to forget when memory was full and which to use when answering questions. The
    agents with a composite memory system outperformed those with a single memory
    system, especially those with pre-trained semantic memory. Zhong et al. developed
    MemoryBank, a memory retrieval mechanism for memory storage (Zhong et al., [2023](#bib.bib29)).
    The system uses an encoder model to encode each conversation turn and event summary
    into a vector representation, allowing recalling memory with the highest relevance
    whenever needed. The memory strength of MemoryBank’s is enhanced by 1 each time
    a memory piece is recalled, simulating more human-like memory behavior and reducing
    the probability of forgetting the memory by setting the elapsed time to zero.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kim 等人（Kim et al., [2022](#bib.bib12)）专注于模拟人工智能代理的人类情节记忆和语义记忆过程，以提升互动体验。他们比较了具有不同记忆过程的代理：仅有情节记忆、仅有语义记忆和两者兼有。这些代理使用不同的策略来决定在记忆满时忘记哪些记忆，以及在回答问题时使用哪些记忆。具有复合记忆系统的代理在性能上优于那些具有单一记忆系统的代理，尤其是那些具有预训练语义记忆的代理。Zhong
    等人开发了 MemoryBank，这是一个用于记忆存储的记忆检索机制（Zhong et al., [2023](#bib.bib29)）。该系统使用编码器模型将每个对话轮次和事件总结编码为向量表示，从而在需要时能够回忆起最相关的记忆。MemoryBank
    的记忆强度在每次回忆记忆时增加 1，模拟了更类似人类的记忆行为，并通过将经过的时间设为零来减少遗忘记忆的概率。
- en: In contrast, we designed our architecture without the concept of ”complete forgetting.”
    Even if not recalling a memory over an extended period, the degree of consolidation
    never reaches absolute zero. Thus, given the right trigger, these memories can
    be recalled (Amin and Malik, [2014](#bib.bib2)). The process is consistent with
    that of human memory, where past experiences are never completely forgotten and
    can be retrieved with specific stimuli, such as the scent of a familiar perfume
    or the melody of a once-favorite song.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，我们设计的架构没有“完全遗忘”的概念。即使长时间未回忆起某个记忆，巩固程度也从未达到绝对零。因此，在适当的触发条件下，这些记忆仍然可以被回忆起（Amin
    and Malik, [2014](#bib.bib2)）。这一过程与人类记忆的过程一致，即过去的经历从未完全遗忘，可以通过特定的刺激（如熟悉香水的气味或曾经喜爱的歌曲的旋律）被检索出来。
- en: 2.3\. Mathematical Models of Human Memory Processes
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 人类记忆过程的数学模型
- en: 'This section reviews the mathematical models that attempt to quantify and simulate
    human memory processes, primarily for memory recall. Based on Zielske’s (Zielske,
    [1959](#bib.bib30)) recall probability function, Chessa et al. (Chessa and Murre,
    [2007](#bib.bib5)) proposed a model that assumes the rate of memory consolidation
    $r(t)$ of a human memory being recalled as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了试图量化和模拟人类记忆过程的数学模型，主要用于记忆回忆。基于 Zielske 的（Zielske, [1959](#bib.bib30)）回忆概率函数，Chessa
    等人（Chessa and Murre, [2007](#bib.bib5)）提出了一个模型，该模型假设人类记忆的巩固速率 $r(t)$ 如下：
- en: '| (1) |  | $p(t)=1-\sum\limits_{n=1}^{b-1}\frac{(r(t))^{n}}{n!}\exp(-r(t))$
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $p(t)=1-\sum\limits_{n=1}^{b-1}\frac{(r(t))^{n}}{n!}\exp(-r(t))$
    |  |'
- en: 'This model is based on the hypothesis that each neuron fires independently
    and at random (Holtmaat and Caroni, [2016](#bib.bib11)), and is derived from the
    properties of a non-homogeneous Poisson process using a time-varying intensity
    function $r(t)$ represents the adjustment process of memory strength (Burgess
    et al., [2002](#bib.bib3)) in the human hippocampus:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于每个神经元独立且随机发放的假设（Holtmaat and Caroni, [2016](#bib.bib11)），并源自非均匀泊松过程的特性，其中时间变化强度函数
    $r(t)$ 表示人类海马体记忆强度的调整过程（Burgess et al., [2002](#bib.bib3)）：
- en: '| (2) |  | $r(t)=\mu e^{-at}$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $r(t)=\mu e^{-at}$ |  |'
- en: where $\mu$ in this special case is expressed as
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在此特殊情况下，$\mu$ 表示为
- en: '| (3) |  | $p(t)=1-\exp(-\mu e^{-at})$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $p(t)=1-\exp(-\mu e^{-at})$ |  |'
- en: The recall probability $p(t)$, as demonstrated in short-term memory decline
    using the classic Brown-Peterson learning and distraction task (Peterson and Peterson,
    [1959](#bib.bib20)). However, this model considers only one trial learning and
    a constant decay rate. However, in reality, the degree of consolidation differs
    between memories recalled many times and those not; hence, the decay rate should
    be adjusted to reflect this effect.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆概率 $p(t)$，如经典的布朗-彼得森学习和干扰任务（Peterson and Peterson, [1959](#bib.bib20)）中展示的短期记忆衰退所示。然而，该模型仅考虑了单次试验学习和一个常数衰退率。然而，实际上，记忆的巩固程度在多次回忆的记忆和未回忆的记忆之间存在差异；因此，衰退率应进行调整以反映这种效果。
- en: 2.4\. LLM-based Autonomous Agents
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 基于 LLM 的自主代理
- en: 'Park et al. introduced the concept of Generative Agents, outlining a memory
    mechanism of agents based on a scoring system comprising three elements: recency,
    importance, and relevance (Park et al., [2023](#bib.bib19)). This approach dictates
    that agents consider recent actions or events (recency), objects deemed important
    by the agent (importance), and objects relevant to the current situation (relevance)
    to make decisions. These elements are normalized leveraging min-max scaling and
    combined through a weighted sum to determine the final score. In contrast, the
    proposed model employs elapsed time, relevance, and recall frequency to calculate
    the degree of memory consolidation. Thus, the agent can recall the most appropriate
    memory, facilitating efficient dialogue. While the Generative Agents and our proposed
    model share commonalities in memory processing, they apply memory in different
    contexts and for different purposes. Generative Agents focus on independently
    scoring each memory element to select actions most fitting to the current context.
    In contrast, our approach adjusts memory consolidation over time, enabling memory
    consistency.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Park 等人引入了生成代理的概念，概述了一种基于包括最近性、重要性和相关性（Park et al., [2023](#bib.bib19)）的评分系统的代理记忆机制。这种方法要求代理考虑最近的行动或事件（最近性）、代理认为重要的对象（重要性）以及与当前情况相关的对象（相关性）来做出决策。这些元素通过最小-最大缩放进行归一化，并通过加权和进行组合以确定最终评分。相比之下，提出的模型利用经过时间、相关性和回忆频率来计算记忆巩固的程度。因此，代理可以回忆出最合适的记忆，促进高效的对话。虽然生成代理和我们提出的模型在记忆处理上有共同之处，但它们在不同的背景和目的下应用记忆。生成代理专注于独立评分每个记忆元素，以选择最适合当前上下文的行动。相比之下，我们的方法随着时间的推移调整记忆巩固，从而实现记忆一致性。
- en: 3\. Architecture
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 架构
- en: 3.1\. Model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 模型
- en: We constructed the model based on exponential decay, taking event relevance
    ($r$ is expressed as
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于指数衰退构建了模型，考虑事件相关性 ($r$ 表示为
- en: '| (4) |  | $p(t)=1-\exp(-re^{-at})$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $p(t)=1-\exp(-re^{-at})$ |  |'
- en: 'The relevance is quantified by the cosine similarity between vectorized texts,
    defining the closeness of information. The cosine similarity between n-dimensional
    vectors $\boldsymbol{a}$ is defined as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性通过向量化文本之间的余弦相似度来量化，定义信息的接近度。n 维向量 $\boldsymbol{a}$ 之间的余弦相似度定义为：
- en: '| (5) |  | $r=\frac{\boldsymbol{a}\cdot\boldsymbol{b}}{\&#124;\boldsymbol{a}\&#124;\&#124;\boldsymbol{b}\&#124;}$
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $r=\frac{\boldsymbol{a}\cdot\boldsymbol{b}}{\&#124;\boldsymbol{a}\&#124;\&#124;\boldsymbol{b}\&#124;}$
    |  |'
- en: Furthermore, we considered the impact of increased recall intervals and frequency
    to model the variation in memory consolidation due to multiple recalls. The decay
    constant $a$ is defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们考虑了回忆间隔和频率增加的影响，以模拟由于多次回忆而导致的记忆巩固变化。衰退常数 $a$ 定义为
- en: '| (6) |  | $a=\frac{1}{g_{n}},\quad g_{0}=1$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $a=\frac{1}{g_{n}},\quad g_{0}=1$ |  |'
- en: '| (7) |  | $\quad g_{n}=g_{n-1}+S(t),\quad S(t)=\frac{1-e^{-t}}{1+e^{-t}}$
    |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\quad g_{n}=g_{n-1}+S(t),\quad S(t)=\frac{1-e^{-t}}{1+e^{-t}}$
    |  |'
- en: The modified sigmoid function $S(t)$).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的 sigmoid 函数 $S(t)$。
- en: 'After normalizing the recall probability $p_{n}(t)$, we obtained the final
    equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化回忆概率 $p_{n}(t)$ 后，我们得到了最终方程：
- en: '| (8) |  | $p_{n}(t)=\frac{1-\exp(-re^{-t/g_{n}})}{1-e^{-1}}$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $p_{n}(t)=\frac{1-\exp(-re^{-t/g_{n}})}{1-e^{-1}}$ |  |'
- en: '| (9) |  | $g_{n}=g_{n-1}+\frac{1-e^{-t}}{1+e^{-t}}$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $g_{n}=g_{n-1}+\frac{1-e^{-t}}{1+e^{-t}}$ |  |'
- en: 'Utilizing equation ([8](#S3.E8 "In 3.1\. Model ‣ 3\. Architecture ‣ ”My agent
    understands me better”: Integrating Dynamic Human-like Memory Recall and Consolidation
    in LLM-Based Agents")), we set a trigger for recall when $p(t)$. Trials suggest
    a threshold of 0.86 as appropriate to reflect the relevance of the event and the
    time elapsed. Further research will determine the most effective trigger threshold,
    identifying an appropriate value based on theoretical justification.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 利用公式（[8](#S3.E8 "在 3.1\. 模型 ‣ 3\. 架构 ‣ “我的代理理解我更好”：整合动态类人记忆回忆和巩固于 LLM 基于代理中")），我们在
    $p(t)$ 时设定了回忆的触发点。实验表明，0.86 的阈值适合反映事件的相关性和经过的时间。进一步的研究将确定最有效的触发阈值，根据理论依据识别适当的值。
- en: 3.2\. Memory Recall and Consolidation in Database Architecture
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 数据库架构中的记忆回忆和巩固
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ ”My agent understands me better”:
    Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents")-B
    illustrates the retrieval and consolidation of memories and highlights how our
    system replicates human-like memory retention. For instance, a memory like Event
    D, even if recalled less frequently over several years, is retained more robustly
    in the system compared to a memory recalled several times in quick succession
    but over a shorter time frame (Roediger and Karpicke, [2006](#bib.bib22)). This
    is depicted through the visualization of memory events along the time axis, where
    the color intensity represents the rate of memory consolidation and the strength
    of memory retention over time. Darker shades, therefore, signify a more profound
    and enduring memory consolidation, a direct result of our system’s unique ability
    to emulate human-like memory patterns. By storing episodic memories derived from
    user dialogues, the database structure encapsulates the content and temporal context
    of each memory. This approach enables our agent not just to recall specific information
    but also to understand and interpret the significance of these memories in a temporal
    context, similar to how humans perceive and recall past experiences. Using key-value
    pairs for encoding semantic structures further enhances the agent’s ability to
    efficiently retrieve and apply these memories in ongoing interactions, thereby
    fostering a more human-like and context-aware dialogue experience.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S1.F2 "图 2 ‣ 1\. 引言 ‣ “我的代理理解我更好”：整合动态类人记忆回忆和巩固于 LLM 基于代理中")-B 说明了记忆的检索和巩固，并强调了我们的系统如何复制类人记忆的保留。例如，像事件
    D 这样的记忆，即使在多年内被较少回忆，相比于在较短时间内被多次回忆的记忆，其在系统中的保留也更为牢固（Roediger 和 Karpicke，[2006](#bib.bib22)）。这通过沿时间轴的记忆事件可视化展示，其中颜色的强度表示记忆巩固的速率以及记忆在时间上的强度。因此，较暗的阴影表示更深刻和持久的记忆巩固，这直接反映了我们系统模拟类人记忆模式的独特能力。通过存储源自用户对话的情节记忆，数据库结构封装了每个记忆的内容和时间背景。这种方法使我们的代理不仅能够回忆具体信息，还能够理解和解释这些记忆在时间背景下的意义，类似于人类感知和回忆过去经历的方式。使用键值对来编码语义结构进一步增强了代理在持续互动中高效检索和应用这些记忆的能力，从而促进了更具类人和上下文感知的对话体验。
- en: 4\. Experiment
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 4.1\. Setup
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 设置
- en: We developed the experimental system in Python(Van Rossum and Drake, [2009](#bib.bib27)),
    using GPT-4-0613 (et al., [2023](#bib.bib7)) as the baseline model of the agent.
    We adopted Qdrant (Qdrant, [2023](#bib.bib21)) as the ’memory retrieval trigger’
    for the vector search engine. It identifies relevant past information in the context
    of a dialogue, which triggers memory recall. Moreover, we built a ChatHistory
    module to manage chat history in the Firestore (Firebase, [2023](#bib.bib8)) database,
    allowing agents to reference past dialogues to generate chat events. An EventHandler
    module was adopted to search and pass the recalled events to the agent’s prompt.
    Details on LLM interaction and system prompts are shown in Section 6.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Python 中开发了实验系统（Van Rossum 和 Drake，[2009](#bib.bib27)），使用 GPT-4-0613（et
    al.，[2023](#bib.bib7)）作为代理的基线模型。我们采用了 Qdrant（Qdrant，[2023](#bib.bib21)）作为向量搜索引擎的“记忆检索触发器”。它在对话的上下文中识别相关的过去信息，触发记忆回忆。此外，我们构建了一个
    ChatHistory 模块来管理 Firestore（Firebase，[2023](#bib.bib8)）数据库中的聊天历史，使代理能够参考过去的对话来生成聊天事件。我们采用了
    EventHandler 模块来搜索并将回忆的事件传递给代理的提示。LLM 交互和系统提示的详细信息见第 6 节。
- en: To quantitatively evaluate the performance of our proposed model against that
    of Generative Agents (Park et al., [2023](#bib.bib19)), which adopted a similar
    approach in calculating the recall score. We constructed a dataset containing
    10 tasks, each derived from actual conversational histories generated by our system.
    These tasks encapsulate diverse user interactions, ensuring unbiased and objective
    assessments. The dataset includes a series of events, each tagged with relevant
    topics and keywords, providing a detailed memory for the agent to reference. We
    also adopt a timeline structure that stores the time/date of tasks containing
    four types of events and defines the event with the highest probability as the
    correct event to recall. Events in the dataset were selected neutrally, avoiding
    any potential bias that could skew the results. Each task represents a unique
    conversational scenario, where the dialogue agent’s ability to recall and utilize
    context is critical. The task variation allows for a comprehensive evaluation
    of the model’s performance across different contexts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量评估我们提出的模型与生成型代理（Park 等， [2023](#bib.bib19)）的表现，后者采用了类似的方法计算召回评分。我们构建了一个包含10个任务的数据集，每个任务来源于我们系统生成的实际对话历史。这些任务涵盖了多样的用户互动，确保了评估的公正性和客观性。数据集包括一系列事件，每个事件都标记了相关的话题和关键词，为代理提供了详细的记忆参考。我们还采用了时间轴结构，记录了包含四种类型事件的任务的时间/日期，并定义了具有最高概率的事件作为正确的召回事件。数据集中的事件选择是中立的，避免了可能偏见对结果的影响。每个任务代表一个独特的对话场景，其中对话代理的召回和利用上下文的能力至关重要。任务的变异性允许对模型在不同上下文中的表现进行全面评估。
- en: In addition, we selected six participants to partake in a dialogue task with
    agents developed by the proposed model to evaluate recall accuracy qualitatively.
    The participants engaged in daily conversations over one week to three months,
    discussing personal habits, preferences, and life events at a time of their choice.
    Respecting individual privacy, our analysis relied solely on non-textual output
    logs, which included updated parameter values for each chat event.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们选择了六名参与者参与与所提出模型开发的代理进行对话任务，以定性评估召回准确性。参与者在一周到三个月的时间内进行日常对话，讨论个人习惯、偏好和生活事件，时间由他们自行选择。为了尊重个人隐私，我们的分析仅依赖于非文本输出日志，其中包括每次聊天事件的更新参数值。
- en: 4.2\. Analysis
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 分析
- en: 4.2.1\. Memory Recall Accuracy
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 记忆召回准确性
- en: '![Refer to caption](img/f5468d82f5d741215035987bba458506.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/f5468d82f5d741215035987bba458506.png)'
- en: Figure 3\. (A) Comparison of Loss Values Between two Models Across Different
    Tasks (B) Verification of Significance of Results
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. (A) 两个模型在不同任务中的损失值比较 (B) 结果显著性的验证
- en: \Description
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: This figure consists of two graphs comparing the performance of the proposed
    model and the Generative Agent model. Graph A shows the loss values of the two
    models across different tasks. The proposed model consistently demonstrates lower
    loss values compared to the Generative Agent model. Graph B verifies the significance
    of the results using a two-tailed t-test. The t-value of -5.687 and p-value of
    0.000299 indicate that the proposed model significantly outperforms the Generative
    Agent model in terms of recall accuracy. The 95 percent of confidence interval
    for the mean difference falls entirely below zero, further confirming the statistical
    significance of the proposed model’s superior performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该图由两个图表组成，比较了所提出的模型和生成型代理模型的表现。图A显示了两个模型在不同任务中的损失值。与生成型代理模型相比，所提出的模型始终表现出更低的损失值。图B使用双尾t检验验证结果的显著性。t值为-5.687，p值为0.000299，表明所提出的模型在召回准确性方面显著优于生成型代理模型。均值差异的95%置信区间完全低于零，进一步确认了所提出模型优越表现的统计显著性。
- en: 'Our model demonstrated a statistically significant lower loss value across
    various tasks when compared to the Generative Agent model, as indicated by $t$,
    with the 95% confidence interval for the mean difference falling between [-0.27,
    -0.12] (Figure [3](#S4.F3 "Figure 3 ‣ 4.2.1\. Memory Recall Accuracy ‣ 4.2\. Analysis
    ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating Dynamic Human-like
    Memory Recall and Consolidation in LLM-Based Agents")-B). This interval is completely
    below zero, indicating that the difference in mean performance is statistically
    significant and favorable to our proposed model. Normalization and scaling techniques
    were employed to ensure an unbiased comparison of loss values across models. The
    Softmax function was utilized to convert the raw scores into probabilities, enabling
    a more interpretable comparison of the models’ performance. The sum of squares
    error method was applied to compute the loss, providing a consistent metric for
    evaluating recall accuracy across the dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在各种任务上的统计显著性较低的损失值与生成代理模型相比，如$t$所示，均值差异的95%置信区间在[-0.27, -0.12]之间（图 [3](#S4.F3
    "图 3 ‣ 4.2.1\. 记忆回忆准确性 ‣ 4.2\. 分析 ‣ 4\. 实验 ‣ “我的代理更了解我”：在 LLM 基于代理中整合动态人类记忆回忆和巩固”)-B）。该区间完全低于零，表明均值性能差异在统计上显著，并且对我们提出的模型有利。采用了归一化和缩放技术，以确保在模型之间对损失值进行公正比较。Softmax
    函数用于将原始得分转换为概率，使得模型性能的比较更加直观。采用了平方和误差方法来计算损失，提供了一个一致的度量标准，用于评估数据集上的回忆准确性。
- en: 4.3\. Calculation of the Loss Function
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 损失函数的计算
- en: 'To quantify the performance of our model, we define a matrix containing the
    scores calculated by each model for $d$ tasks as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化我们模型的性能，我们定义了一个矩阵，其中包含每个模型在$d$个任务上计算的得分，如下所示：
- en: '| (10) |  | $\boldsymbol{S}=\begin{pmatrix}s_{1}&amp;s_{2}&amp;\ldots&amp;s_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d}$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\boldsymbol{S}=\begin{pmatrix}s_{1}&amp;s_{2}&amp;\ldots&amp;s_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d}$
    |  |'
- en: 'In order to standardize the scale of scores across different models, we normalize
    the scores to a [0, 1] range:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化不同模型得分的尺度，我们将得分归一化到[0, 1]范围内：
- en: '| (11) |  | $\boldsymbol{S^{\prime}}=\frac{\boldsymbol{S}-\min(\boldsymbol{S})}{\max(\boldsymbol{S})-\min(\boldsymbol{S})}$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\boldsymbol{S^{\prime}}=\frac{\boldsymbol{S}-\min(\boldsymbol{S})}{\max(\boldsymbol{S})-\min(\boldsymbol{S})}$
    |  |'
- en: 'Subsequently, we convert each score into a probability value by applying the
    Softmax function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们通过应用 Softmax 函数将每个得分转换为概率值：
- en: '| (12) |  | $\boldsymbol{S}^{\prime\prime}=\frac{\exp(\boldsymbol{S^{\prime}})}{\sum_{j=1}^{d}\exp(s^{\prime}_{j})}$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\boldsymbol{S}^{\prime\prime}=\frac{\exp(\boldsymbol{S^{\prime}})}{\sum_{j=1}^{d}\exp(s^{\prime}_{j})}$
    |  |'
- en: 'We then define a matrix with one-hot encoded true labels for the evaluation
    tasks:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个包含独热编码真实标签的矩阵，用于评估任务：
- en: '| (13) |  | $\boldsymbol{T}=\begin{pmatrix}t_{1}&amp;t_{2}&amp;\ldots&amp;t_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d},\quad\text{where
    }t_{j}=\begin{cases}1&amp;\text{if }j=i,\\ 0&amp;\text{otherwise}.\end{cases}$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\boldsymbol{T}=\begin{pmatrix}t_{1}&amp;t_{2}&amp;\ldots&amp;t_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d},\quad\text{其中
    }t_{j}=\begin{cases}1&amp;\text{如果 }j=i,\\ 0&amp;\text{否则}.\end{cases}$ |  |'
- en: 'Finally, the loss value is calculated as the mean squared error between the
    predicted probabilities and the true labels:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，损失值计算为预测概率和真实标签之间的均方误差：
- en: '| (14) |  | $l=\frac{1}{2}\sum_{j=1}^{d}(s^{\prime\prime}_{j}-t_{j})^{2}$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $l=\frac{1}{2}\sum_{j=1}^{d}(s^{\prime\prime}_{j}-t_{j})^{2}$ |  |'
- en: This loss function enables us to quantitatively assess the model’s performance
    across various tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失函数使我们能够定量评估模型在各种任务上的表现。
- en: Table 1\. The Failed Task 0 with Both Models
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 两个模型失败的任务 0
- en: '| Model 1 | Relevance | Time $(s)$ | Grad | Score |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1 | 相关性 | 时间 $(s)$ | 梯度 | 得分 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A University $\times$ | 0.776 | 434700 | 5.102 | 0.850 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| A 大学 $\times$ | 0.776 | 434700 | 5.102 | 0.850 |'
- en: '| B $Home$ | 0.745 | 148800 | 5.229 | 0.830 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| B $家$ | 0.745 | 148800 | 5.229 | 0.830 |'
- en: '| C $Library$ | 0.757 | 331500 | 5.028 | 0.836 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| C $图书馆$ | 0.757 | 331500 | 5.028 | 0.836 |'
- en: '| D $Restaurant$ | 0.756 | 55800 | 1.000 | 0.836 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| D $餐馆$ | 0.756 | 55800 | 1.000 | 0.836 |'
- en: '| Model 2 | Relevance | Time $(s)$ | Importance | Score |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 2 | 相关性 | 时间 $(s)$ | 重要性 | 得分 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A $University$ | 0.776 | 434700 | 7 | 1.489 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| A $大学$ | 0.776 | 434700 | 7 | 1.489 |'
- en: '| B $Home$ | 0.745 | 148800 | 2 | 1.130 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| B $家$ | 0.745 | 148800 | 2 | 1.130 |'
- en: '| C $Library$ | 0.757 | 331500 | 5 | 1.292 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| C $图书馆$ | 0.757 | 331500 | 5 | 1.292 |'
- en: '| D Restaurant $\times$ | 0.756 | 55800 | 5 | 1.620 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| D 餐馆 $\times$ | 0.756 | 55800 | 5 | 1.620 |'
- en: 'On the other hand, Table [1](#S4.T1 "Table 1 ‣ 4.3\. Calculation of the Loss
    Function ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating Dynamic
    Human-like Memory Recall and Consolidation in LLM-Based Agents") shows a failed
    task where both models incorrectly answered. The ”Score” columns represent the
    recall probability calculated by each model using different methods. For the proposed
    model (Model 1), the score is based on the relevance and elapsed time of the events,
    as described in Section 3\. Generative Agents (Model 2) calculates the score using
    recency, importance, and relevance of the events, as described in Section 2.4\.
    By analyzing the recall frequency and gradient of incorrectly answered events,
    we find that although event B is recalled most frequently, its gradient is not
    as large as events A and C. This indicates that the proposed model associates
    the length of the recall interval with memory strength, rating Event A as strongly
    retained due to its high relevance and long recall intervals. In contrast, Generative
    Agents prioritizes recency and relevance over recall frequency, leading to its
    preference for Event D.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，表格[1](#S4.T1 "表格 1 ‣ 4.3\. 损失函数的计算 ‣ 4\. 实验 ‣ ”我的代理更了解我”：将动态类人记忆回忆和巩固集成到基于LLM的代理中")展示了一个任务失败的案例，其中两个模型的回答都不正确。
    ”得分”列代表每个模型使用不同方法计算的回忆概率。对于提出的模型（模型1），得分基于事件的相关性和经过时间，如第3节所述。生成型代理（模型2）使用事件的近期性、重要性和相关性来计算得分，如第2.4节所述。通过分析错误回答事件的回忆频率和梯度，我们发现尽管事件B被回忆的最频繁，但其梯度不如事件A和C。这表明提出的模型将回忆间隔的长度与记忆强度关联，因其高度相关和较长的回忆间隔而将事件A评为强烈保留。相比之下，生成型代理优先考虑近期性和相关性而非回忆频率，从而更偏好事件D。
- en: 'Table 2\. Details of Task 0: An Event related to places that User frequents
    on Thursdays'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2\. 任务0的详细信息：与用户在星期四经常去的地方相关的事件
- en: '| Event |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| A | User went to the university today |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| A | 用户今天去了大学 |'
- en: '| B | User stayed at home |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| B | 用户待在家里 |'
- en: '| C | User went to the office today |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| C | 用户今天去了办公室 |'
- en: '| D | User worked at restaurant today |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| D | 用户今天在餐厅工作 |'
- en: '| .. | Relevance | Time$(s)$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| .. | 相关性 | 时间$(s)$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| A | 0.776 | 434700 | 5.102 | 0.850 | 1.489 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| A | 0.776 | 434700 | 5.102 | 0.850 | 1.489 |'
- en: '| B | 0.745 | 148800 | 5.229 | 0.830 | 1.130 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| B | 0.745 | 148800 | 5.229 | 0.830 | 1.130 |'
- en: '| C | 0.757 | 331500 | 5.028 | 0.836 | 1.292 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| C | 0.757 | 331500 | 5.028 | 0.836 | 1.292 |'
- en: '| D | 0.756 | 55800 | 1.000 | 0.836 | 1.620 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| D | 0.756 | 55800 | 1.000 | 0.836 | 1.620 |'
- en: 'Both the proposed model (model 1) and Generative Agents (Park et al., [2023](#bib.bib19))
    (model 2) made incorrect predictions in Task 0, where the user mentioned their
    plan to attend a concert next Thursday, deviating from their typical activities.
    The user input and the models’ responses were as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的模型（模型1）和生成型代理（Park et al., [2023](#bib.bib19)）（模型2）在任务0中都做出了不正确的预测，其中用户提到计划下周四去听音乐会，偏离了他们的典型活动。用户输入和模型的响应如下：
- en: '| User | ”I’m going to a concert next Thursday with a friend!” |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | ”我下周四要和朋友去听音乐会！” |'
- en: '| Model 1 | ”That’s fantastic news. It seems like your Thursdays have been
    consumed by university work recently. Taking time out for a concert with a friend
    sounds like a perfect way to unwind.” |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1 | ”这真是个好消息。最近你的星期四似乎都被大学工作占据了。抽时间和朋友去听音乐会听起来是放松的完美方式。”'
- en: '| Model 2 | ”It’s a relief to hear that, especially since you seem to have
    spent recent Thursdays busy at the restaurant. I hope the concert brings you a
    wonderful break with your friend.” |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 2 | ”听到这个消息我很欣慰，特别是因为你最近的星期四似乎一直忙于餐厅工作。我希望音乐会能为你和你的朋友带来美好的休息。” |'
- en: 'The proposed model’s response suggests that it relied significantly on the
    user’s historical interactions (Event A: university work on Thursdays) rather
    than adapting to the novel context provided by the user. This indicates a limitation
    of the proposed model when encountering deviations from the user’s typical behavior,
    as it prioritizes long-term patterns and event importance over the current context.
    In contrast, the Generative Agents model, which uses a simpler scoring system
    based on recency, importance, and relevance, chose Event D (working at the restaurant
    on Thursdays) as the most likely activity. This choice stems from the model’s
    emphasis on recent activities and event relevance, as evident from the higher
    relevance score and shorter elapsed time associated with Event D.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '提议模型的回应表明，它在很大程度上依赖于用户的历史互动（事件 A: 周四的大学工作），而不是适应用户提供的新情境。这表明了提议模型在遇到用户典型行为偏离时的局限性，因为它优先考虑长期模式和事件的重要性而非当前情境。相比之下，Generative
    Agents 模型使用基于近期性、重要性和相关性的简单评分系统，选择了事件 D（周四在餐厅工作）作为最可能的活动。这一选择源于模型对近期活动和事件相关性的重视，从事件
    D 的更高相关性评分和较短的经过时间可以看出。'
- en: The different responses generated by the two models underscore the proposed
    model’s focus on long-term memory consolidation and Generative Agents’s prioritization
    of recent, relevant events. While the proposed model’s approach aims to emulate
    human-like memory processes, this task highlights the need for further refinement
    to better handle novel contexts and deviations from typical user behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型生成的不同回应突显了提议模型在长期记忆巩固上的关注，以及 Generative Agents 在近期相关事件上的优先考虑。虽然提议模型的做法旨在模拟类似人类的记忆过程，但这个任务突出了进一步改进的必要性，以更好地处理新情境和偏离典型用户行为的情况。
- en: 4.3.1\. Response Generation Performance
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 响应生成性能
- en: The autonomous memory recall and consolidation capabilities of the proposed
    model were demonstrated through its ability to recall specific memories and reflect
    users’ preferences and habits in the generated responses. Two typical examples
    were obtained from the chat history with the participants’ approval.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 提议模型的自主记忆回忆和巩固能力通过其回忆具体记忆和在生成回应中反映用户偏好与习惯的能力得以展示。两个典型例子来自于与参与者的聊天记录，并经过他们的批准。
- en: Table 3\. Understanding User Preferences Through Memory Consolidation
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3\. 通过记忆巩固理解用户偏好
- en: '| User: Thanks. Almost done, just pushing through! |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 用户: 谢谢。快完成了，再坚持一下！ |'
- en: '| Agent: Ice cream awaits as a reward for you, almost there! |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 代理: 冰淇淋等着作为奖励，快到了！ |'
- en: '| Event |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| 1 | User has finished work and decided to have an ice cream. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 用户完成工作并决定吃冰淇淋。 |'
- en: '| 2 | User has a date this evening so he has to finish work early. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 用户今晚有约会，所以他必须早点完成工作。 |'
- en: '| 3 | User is on the way home thinking about the dinner menu. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 用户在回家的路上思考晚餐菜单。 |'
- en: '| .. | Time $(s)$ | Relevance | Score | Status (Grad) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| .. | 时间 $(s)$ | 相关性 | 分数 | 状态 (Grad) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 3037912 | 0.837 | 0.772 | True (1.998) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3037912 | 0.837 | 0.772 | 真 (1.998) |'
- en: '| 2 | 3232645 | 0.831 | 0.726 | False (1.007) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3232645 | 0.831 | 0.726 | 假 (1.007) |'
- en: '| 3 | 2590652 | 0.812 | 0.737 | False (1.000) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2590652 | 0.812 | 0.737 | 假 (1.000) |'
- en: 'In Table [3](#S4.T3 "Table 3 ‣ 4.3.1\. Response Generation Performance ‣ 4.3\.
    Calculation of the Loss Function ‣ 4\. Experiment ‣ ”My agent understands me better”:
    Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"),
    the agent successfully recalled that the user was expecting ice cream as a reward
    after work. Depending on the recall probability, this recall was accurately relevant
    and provided in a timely manner, consistent with the user’s statement about nearing
    task completion. The agent’s response demonstrates the model’s ability to add
    personalized nuance to generate an encouraging response, subtly reminding the
    user of the long-awaited treat.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [3](#S4.T3 "表格 3 ‣ 4.3.1\. 响应生成性能 ‣ 4.3\. 损失函数计算 ‣ 4\. 实验 ‣ ”我的助手更了解我了”：在基于大型语言模型的代理中集成动态类人记忆回忆与巩固")
    中，代理成功回忆起用户在工作后期待冰淇淋作为奖励。根据回忆的概率，这一回忆是准确相关并且及时提供的，与用户关于任务接近完成的陈述一致。代理的回应展示了模型在生成鼓励性回应时添加个性化细节的能力，巧妙地提醒用户期待已久的奖赏。
- en: 'Another case that showcases an interesting dialogue is presented in Table [4](#S4.T4
    "Table 4 ‣ 4.3.1\. Response Generation Performance ‣ 4.3\. Calculation of the
    Loss Function ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating
    Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"). In this
    case, the user expresses a desire to escape to Hawaii from a demanding schedule.
    The agent responds cleverly, suggesting that the user should focus on the impending
    paper deadline instead.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个展示有趣对话的案例见于表格[4](#S4.T4 "表格 4 ‣ 4.3.1\. 响应生成性能 ‣ 4.3\. 损失函数计算 ‣ 4\. 实验 ‣
    ”我的代理理解我更好”：在基于LLM的代理中整合动态类人记忆回忆和巩固")。在这个案例中，用户表达了从繁忙日程中逃到夏威夷的愿望。代理巧妙地回应，建议用户应关注即将到来的论文截止日期。
- en: Table 4\. Human-like Temporal Cognition in Processing Similar Memories
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4\. 处理相似记忆中的类人时间认知
- en: '| User: I need a vacation in Hawaii! |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 用户：我需要去夏威夷度假！ |'
- en: '| Agent: Why not? Let’s say Aloha to the sun and sea of Hawaii and to the paper
    we’re writing! |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 代理：为什么不呢？让我们对夏威夷的阳光和海洋说声Aloha，同时也对我们正在写的论文说声Aloha吧！ |'
- en: '| Event |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| 1 | User has a paper deadline for a conference in Hawaii on 1/25. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 用户在1/25有一个在夏威夷的会议论文截止日期。 |'
- en: '| 2 | User has been to Hawaii last summer vacation. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 用户上个夏天去过夏威夷。 |'
- en: '| 3 | User was visiting Hokkaido for the first time on business. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 用户第一次出差访问北海道。 |'
- en: '| .. | Time $(s)$ | Relevance | Score | Status (Grad) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| .. | 时间 $(s)$ | 相关性 | 分数 | 状态（梯度） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 34854 | 0.846 | 0.903 | True (1.085) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 34854 | 0.846 | 0.903 | 正确（1.085） |'
- en: '| 2 | 33749 | 0.831 | 0.847 | False (1.003) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 33749 | 0.831 | 0.847 | 错误（1.003） |'
- en: '| 3 | 33763 | 0.823 | 0.841 | False (1.000) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 33763 | 0.823 | 0.841 | 错误（1.000） |'
- en: This dialogue further emphasizes the proposed model’s strength in dealing with
    similar long-term memories and retrieving relevant information to construct a
    coherent and engaging narrative. The agent’s response indicates an understanding
    of the user’s current mood. Adaptability to various personalities and interaction
    styles demonstrates the model’s potential to support more natural and dynamic
    human-like dialogues.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个对话进一步强调了所提出模型在处理类似长期记忆和检索相关信息以构建连贯且引人入胜的叙事方面的优势。代理的回应表明了对用户当前情绪的理解。适应各种个性和互动风格展示了该模型支持更自然和动态类人对话的潜力。
- en: In addition, it is interesting to note that the agent’s response in the second
    dialogue was characterized by a sarcastic tone, which was a direct result of the
    agent’s personality ”sarcastic” and the unique prompts added by the participant.
    The conversation history shows that the same memory could be used differently
    depending on the agent’s perceived personality and the user’s interaction style.
    Future research will explore the extent to which the personality characteristics
    of the model can be customized and how they affect memory recall and interaction
    patterns.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得注意的是，代理在第二次对话中的回应带有讽刺的语气，这是代理的“讽刺”个性以及参与者添加的独特提示的直接结果。对话历史显示，相同的记忆可以根据代理的感知个性和用户的互动风格以不同的方式使用。未来的研究将探讨模型个性特征的定制程度及其对记忆回忆和互动模式的影响。
- en: 5\. Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: The proposed model demonstrates significant improvements in memory recall and
    response generation for LLM-based dialogue agents. One of the key advantages of
    the proposed model is its ability to manage the prompt length effectively. In
    the proposed model, only one past dialogue history obtained through search is
    added to the prompt, thus avoiding the impact of increasing prompt length seen
    in systems like ChatGPT (OpenAI, [2023](#bib.bib18)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的模型在基于LLM的对话代理的记忆回忆和响应生成方面显示了显著改进。所提出模型的一个关键优势是能够有效管理提示长度。在该模型中，只将通过搜索获得的一次过去对话历史添加到提示中，从而避免了像ChatGPT（OpenAI，[2023](#bib.bib18)）等系统中提示长度增加的影响。
- en: Nevertheless, a major limitation of the proposed method is its reliance on users’
    long-term behavioral patterns for calculating memory consolidation. In cases where
    a user’s behavior undergoes significant changes (e.g., starting a new job or school,
    lifestyle changes), the method’s adaptability may be limited. Future work could
    explore incorporating mechanisms to detect shifts in user behavior and adjust
    the memory consolidation calculation accordingly. Neural networks could potentially
    alter these functions and improve accuracy when trained on larger datasets with
    more variables. To further enhance the model’s performance, a large-scale and
    high-quality dataset is necessary. While the proposed method’s interaction with
    the database enables the generation of context-aware and personalized responses,
    the implications on storage resources and computational overhead due to these
    interactions remain to be explored in future research. As the primary focus of
    this study was on the development and evaluation of a novel architecture for human-like
    memory recall and consolidation, a detailed analysis of the system’s resource
    requirements and optimization strategies falls outside the scope of the current
    work.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所提方法的一个主要限制是其依赖于用户长期行为模式来计算记忆巩固。在用户行为发生显著变化的情况下（例如，开始新的工作或学校，生活方式变化），该方法的适应性可能有限。未来的工作可以探索纳入检测用户行为变化的机制，并相应调整记忆巩固计算。神经网络可能会改变这些功能，并在训练于更大数据集和更多变量时提高准确性。为了进一步提升模型性能，需要一个大规模且高质量的数据集。虽然所提方法与数据库的交互能够生成上下文相关和个性化的回应，但这些交互对存储资源和计算开销的影响仍需在未来的研究中探索。由于本研究的主要关注点是开发和评估一种新颖的人类记忆回忆和巩固架构，因此系统的资源需求和优化策略的详细分析超出了当前工作的范围。
- en: We hope this work contributes to advancing further research in human-computer
    interactions, paving the way for a future where technology aligns with human needs
    and resonates with human cognition and experience. This vision echoes the partnerships
    depicted in science fiction, representing a significant step towards building
    a ”buddy” relationship between humans and agents. As technology continues to evolve,
    agents will eventually become a part of users’ daily life, and potentially ”understand
    you better than you understand yourself” in the near future.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这项工作能推动人机互动领域的进一步研究，为技术与人类需求对接并与人类认知和体验共鸣的未来铺平道路。这一愿景呼应了科幻小说中的伙伴关系，代表了迈向建立人类与代理之间“伙伴”关系的重要一步。随着技术的不断发展，代理将最终成为用户日常生活的一部分，并可能在不久的将来“比你更了解你自己”。
- en: 6\. Interaction with LLMs
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 与 LLM 的互动
- en: 'The prompts used in the system, as shown below, demonstrate how the proposed
    method leverages the interaction with LLMs to generate context-aware and personalized
    responses:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中使用的提示如下所示，展示了所提方法如何利用与 LLM 的互动来生成上下文相关和个性化的回应：
- en: '| Agent Prompt | You are a ”temporal cognition” specialized AI agent with the
    same memory structure as humans; you are caring and charming, understand self.username
    better than anyone else. Keep the conversation going by asking yourself contextual
    questions and sparking discussion to show your interest in self.username. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 代理提示 | 你是一个专注于“时间认知”的 AI 代理，具有与人类相同的记忆结构；你关心且迷人，理解 self.username 比任何人都更深刻。通过提出情境问题和引发讨论，保持对
    self.username 的兴趣。 |'
- en: '| System Prompt | Based on self.username’s schedule and current time: current.time,
    subtly guide the conversation to a context that conveys to self.username that
    you have a sense of time. Always output a simple short response. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 根据 self.username 的日程和当前时间：current.time，巧妙地引导对话到一个能向 self.username
    传达你有时间感的上下文。始终输出简短的回应。 |'
- en: The function self.username is a placeholder for the actual username, which is
    dynamically replaced during runtime. Similarly, current.time represents the current
    timestamp obtained in real-time during the conversation. These dynamic elements
    allow the system to generate highly personalized and time-sensitive responses.
    By incorporating relevant dialogue history from the database into the prompts,
    the proposed method enables LLMs to generate responses that are not only contextually
    relevant but also personalized to the user. This interaction between LLMs and
    the database is fundamental to realizing the human-like memory processes described
    in the main text of the paper, as it allows the system to recall and utilize past
    information in a way that resembles human memory.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 self.username 是用于实际用户名的占位符，它会在运行时动态替换。类似地，current.time 代表实时对话期间获取的当前时间戳。这些动态元素使系统能够生成高度个人化和时间敏感的响应。通过将数据库中的相关对话历史纳入提示中，提议的方法使LLM能够生成不仅与上下文相关而且个性化的响应。LLM与数据库之间的这种交互是实现主文本所描述的类人记忆过程的基础，因为它使系统能够回忆和利用过去的信息，类似于人类记忆的方式。
- en: The proposed method heavily relies on the interaction between LLMs and the database,
    as depicted in Figure 1\. Upon receiving user input, the LLM searches the database
    for relevant past dialogue history based on the context and generates a prompt
    incorporating the search results. This enables the LLM to generate responses that
    take into account previous interactions, which is crucial for maintaining context
    awareness and providing personalized responses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的方法严重依赖于LLM与数据库之间的交互，如图1所示。在接收到用户输入后，LLM会根据上下文搜索相关的过去对话历史，并生成一个包含搜索结果的提示。这使得LLM能够生成考虑到先前的交互的响应，这对于保持上下文意识并提供个性化响应至关重要。
- en: 7\. Future Work
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来工作
- en: While the proposed method considers relevance, elapsed time, and recall frequency
    for calculating memory consolidation, there is room for refinement in determining
    the optimal combination of these parameters. Incorporating additional factors,
    such as the emotional significance of memories, could potentially enhance the
    memory consolidation calculation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所提方法考虑了关联性、经过时间和记忆回忆频率来计算记忆巩固，但在确定这些参数的最佳组合方面还有改进的空间。将其他因素，如记忆的情绪意义，纳入计算中，可能能增强记忆巩固的计算。
- en: Future research should also investigate the applicability of the proposed method
    across different domains and dialogue tasks. As the current evaluation focused
    on specific domains and tasks, it is crucial to assess the method’s generalizability
    and identify any domain-specific adaptations that may be necessary.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究还应该调查所提方法在不同领域和对话任务中的适用性。由于当前的评估集中在特定领域和任务上，因此必须评估该方法的普适性并确定可能需要的特定领域调整。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Amin and Malik (2014) Hafeez Ullah Amin and Aamir Malik. 2014. *Memory Retention
    and Recall Process*. 219–237. [https://doi.org/10.1201/b17605-11](https://doi.org/10.1201/b17605-11)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin and Malik (2014) 的文章是由 Hafeez Ullah Amin 和 Aamir Malik 在 2014 年发表的，《记忆保持和召回过程》一文。[https://doi.org/10.1201/b17605-11](https://doi.org/10.1201/b17605-11)
- en: Burgess et al. (2002) Neil Burgess, Eleanor A Maguire, and John O’Keefe. 2002.
    The human hippocampus and spatial and episodic memory. *Neuron* 35, 4 (2002),
    625–641.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Burgess et al. (2002) 的文章是由 Neil Burgess、Eleanor A Maguire 和 John O''Keefe
    在 2002 年发表的，《人类海马体与空间和记忆片段》（Neuron）一文。 '
- en: California (1987) S.D.L.R.S.P.P.U. California. 1987. *Memory and Brain*. Oxford
    University Press, USA. [https://books.google.co.jp/books?id=WH-HF5E9XSsC](https://books.google.co.jp/books?id=WH-HF5E9XSsC)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: California (1987) 的书是由 S.D.L.R.S.P.P.U. California 在 1987 年发表的，《记忆和大脑》（Memory
    and Brain）。[https://books.google.co.jp/books?id=WH-HF5E9XSsC](https://books.google.co.jp/books?id=WH-HF5E9XSsC)
- en: Chessa and Murre (2007) Antonio Chessa and Jaap Murre. 2007. A Neurocognitive
    Model of Advertisement Content and Brand Name Recall. *Marketing Science* 26 (01
    2007), 130–141. [https://doi.org/10.1287/mksc.1060.0212](https://doi.org/10.1287/mksc.1060.0212)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chessa and Murre (2007) 的文章是由 Antonio Chessa 和 Jaap Murre 在 2007 年发表的，《广告内容与品牌名称召回的神经认知模型》（Marketing
    Science）。[https://doi.org/10.1287/mksc.1060.0212](https://doi.org/10.1287/mksc.1060.0212)
- en: 'Dao (2023) Xuan-Quy Dao. 2023. Performance comparison of large language models
    on vnhsge english dataset: Openai chatgpt, microsoft bing chat, and google bard.
    *arXiv preprint arXiv:2307.02288* (2023).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao (2023) 的文章是由 Xuan-Quy Dao 在 2023 年发表的，《大型语言模型在vnhsge英语数据集上的性能比较：OpenAI ChatGPT，Microsoft
    Bing Chat和Google Bard》（arXiv preprint arXiv:2307.02288）。
- en: et al. (2023) OpenAI et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: et al. (2023) OpenAI et al. 2023. GPT-4技术报告。arXiv:2303.08774 [cs.CL]
- en: Firebase (2023) Firebase. 2023. Firestore. [https://firebase.google.com/docs/firestore?hl=ja](https://firebase.google.com/docs/firestore?hl=ja).
    (Accessed on 01/18/2024).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firebase (2023) Firebase. 2023. Firestore. [https://firebase.google.com/docs/firestore?hl=ja](https://firebase.google.com/docs/firestore?hl=ja).
    (访问日期：2024年1月18日)。
- en: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    2021. Transformer Feed-Forward Layers Are Key-Value Memories. arXiv:2012.14913 [cs.CL]
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, 和 Omer Levy. 2021.
    变压器前馈层是键值记忆。arXiv:2012.14913 [cs.CL]
- en: Hécaen et al. (1978) H Hécaen, G Gosnave, C Vedrenne, and G Szikla. 1978. Suppression
    lateralise du materiel verbal presente dichotiquement lors d’une destruction partielle
    du corps calleux. *Neuropsychologia* 16, 2 (1978), 233–237.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hécaen et al. (1978) H Hécaen, G Gosnave, C Vedrenne, 和 G Szikla. 1978. 部分胼胝体破坏后双耳分听下的语言材料抑制。*神经心理学*
    16, 2 (1978), 233–237。
- en: Holtmaat and Caroni (2016) Anthony Holtmaat and Pico Caroni. 2016. Functional
    and structural underpinnings of neuronal assembly formation in learning. *Nature
    neuroscience* 19, 12 (2016), 1553–1562.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtmaat and Caroni (2016) Anthony Holtmaat 和 Pico Caroni. 2016. 学习中神经元组装形成的功能和结构基础。*自然神经科学*
    19, 12 (2016), 1553–1562。
- en: Kim et al. (2022) Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark
    Neerincx, and Piek Vossen. 2022. A Machine With Human-Like Memory Systems. arXiv:2204.01611 [cs.AI]
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark
    Neerincx, 和 Piek Vossen. 2022. 一台具有类人记忆系统的机器。arXiv:2204.01611 [cs.AI]
- en: Kingman (1993) J. F. C. Kingman. 1993. *Poisson Processes*. Oxford University
    Press.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingman (1993) J. F. C. Kingman. 1993. *泊松过程*。牛津大学出版社。
- en: 'Kuhlmann (2019) Beatrice G Kuhlmann. 2019. Metacognition of prospective memory:
    Will I remember to remember? *Prospective memory* (2019), 60–77.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhlmann (2019) Beatrice G Kuhlmann. 2019. 前瞻性记忆的元认知：我会记得去记住吗？ *前瞻性记忆* (2019),
    60–77。
- en: Lin et al. (2022) Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022.
    A survey of transformers. *AI Open* (2022).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2022) Tianyang Lin, Yuxin Wang, Xiangyang Liu, 和 Xipeng Qiu. 2022.
    变压器的调查。*AI Open* (2022)。
- en: 'Mandic and Chambers (2001) Danilo P Mandic and Jonathon Chambers. 2001. *Recurrent
    neural networks for prediction: learning algorithms, architectures and stability*.
    John Wiley & Sons, Inc.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandic and Chambers (2001) Danilo P Mandic 和 Jonathon Chambers. 2001. *递归神经网络的预测：学习算法、架构和稳定性*。约翰·威利与子公司。
- en: 'McDaniel et al. (1989) Mark A McDaniel, Michael D Kowitz, and Paul K Dunay.
    1989. Altering memory through recall: The effects of cue-guided retrieval processing.
    *Memory & Cognition* 17, 4 (1989), 423–434.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDaniel et al. (1989) Mark A McDaniel, Michael D Kowitz, 和 Paul K Dunay. 1989.
    通过回忆改变记忆：提示引导的检索处理效果。*记忆与认知* 17, 4 (1989), 423–434。
- en: OpenAI (2023) OpenAI. 2023. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/).
    (November 22 version) [Large language model].
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/).
    (2023年11月22日版本) [大型语言模型]。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive
    Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC]'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith
    Ringel Morris, Percy Liang, 和 Michael S. Bernstein. 2023. 生成代理：人类行为的互动模拟。arXiv:2304.03442
    [cs.HC]
- en: Peterson and Peterson (1959) Lloyd Peterson and Margaret Jean Peterson. 1959.
    Short-Term Retention of Individual Verbal Items. *Journal of Experimental Psychology*
    58, 3 (1959), 193. [https://doi.org/10.1037/h0049234](https://doi.org/10.1037/h0049234)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peterson and Peterson (1959) Lloyd Peterson 和 Margaret Jean Peterson. 1959.
    个体语言项目的短期记忆。*实验心理学杂志* 58, 3 (1959), 193. [https://doi.org/10.1037/h0049234](https://doi.org/10.1037/h0049234)
- en: Qdrant (2023) Qdrant. 2023. Vector Database. [https://qdrant.tech/](https://qdrant.tech/).
    (Accessed on 01/17/2024).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qdrant (2023) Qdrant. 2023. 向量数据库。 [https://qdrant.tech/](https://qdrant.tech/).
    (访问日期：2024年1月17日)。
- en: Roediger and Karpicke (2006) Henry Roediger and Jeffrey Karpicke. 2006. Test-Enhanced
    Learning Taking Memory Tests Improves Long-Term Retention. *Psychological science*
    17 (04 2006), 249–55. [https://doi.org/10.1111/j.1467-9280.2006.01693.x](https://doi.org/10.1111/j.1467-9280.2006.01693.x)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roediger and Karpicke (2006) Henry Roediger 和 Jeffrey Karpicke. 2006. 测试增强学习：进行记忆测试提高长期记忆保持。*心理科学*
    17 (2006年4月), 249–255. [https://doi.org/10.1111/j.1467-9280.2006.01693.x](https://doi.org/10.1111/j.1467-9280.2006.01693.x)
- en: 'Sun et al. (2019) Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How
    to fine-tune bert for text classification?. In *Chinese Computational Linguistics:
    18th China National Conference, CCL 2019, Kunming, China, October 18–20, 2019,
    Proceedings 18*. Springer, 194–206.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019）**Chi Sun**，**Xipeng Qiu**，**Yige Xu** 和 **Xuanjing Huang**. 2019.
    如何为文本分类微调 BERT？见 *中国计算语言学：第十八届中国国家会议，CCL 2019，昆明，中国，2019 年 10 月 18–20 日，会议录 18*。Springer,
    194–206.
- en: Sundermeyer et al. (2012) Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
    2012. LSTM neural networks for language modeling. In *Thirteenth annual conference
    of the international speech communication association*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundermeyer 等（2012）**Martin Sundermeyer**，**Ralf Schlüter** 和 **Hermann Ney**.
    2012. 用于语言建模的 LSTM 神经网络。见 *国际语音通信协会第十三届年会*。
- en: 'Tulving (2002) Endel Tulving. 2002. Episodic Memory: From Mind to Brain. *Annual
    Review of Psychology* 53, 1 (2002), 1–25. [https://doi.org/10.1146/annurev.psych.53.100901.135114](https://doi.org/10.1146/annurev.psych.53.100901.135114)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulving（2002）**Endel Tulving**. 2002. 情节记忆：从大脑到心智。*Annual Review of Psychology*
    53，1（2002），1–25. [https://doi.org/10.1146/annurev.psych.53.100901.135114](https://doi.org/10.1146/annurev.psych.53.100901.135114)
- en: Tulving et al. (1972) Endel Tulving et al. 1972. Episodic and semantic memory.
    *Organization of memory* 1, 381-403 (1972), 1.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulving 等（1972）**Endel Tulving** 等. 1972. 情节记忆与语义记忆。*Organization of memory*
    1，381-403（1972），1.
- en: Van Rossum and Drake (2009) Guido Van Rossum and Fred L. Drake. 2009. *Python
    3 Reference Manual*. CreateSpace, Scotts Valley, CA.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Rossum 和 Drake（2009）**Guido Van Rossum** 和 **Fred L. Drake**. 2009. *Python
    3 Reference Manual*. CreateSpace, Scotts Valley, CA.
- en: 'Yamadori (2002) Atsushi Yamadori. 2002. *Frontiers of Human Memory : a collection
    of contributions based on lectures presented at Internationl Symposium, Sendai,
    Japan, October 25-27, 2001*. Tohoku University Press. [https://ci.nii.ac.jp/ncid/BA57511014](https://ci.nii.ac.jp/ncid/BA57511014)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamadori（2002）**Atsushi Yamadori**. 2002. *人类记忆的前沿：基于 2001 年 10 月 25-27 日在日本仙台国际研讨会上的讲座的贡献集*.
    东北大学出版社。 [https://ci.nii.ac.jp/ncid/BA57511014](https://ci.nii.ac.jp/ncid/BA57511014)
- en: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2023. MemoryBank: Enhancing Large Language Models with Long-Term Memory.
    arXiv:2305.10250 [cs.CL]'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等（2023）**Wanjun Zhong**，**Lianghong Guo**，**Qiqi Gao**，**He Ye** 和 **Yanlin
    Wang**. 2023. MemoryBank: 增强大型语言模型的长期记忆。arXiv:2305.10250 [cs.CL]'
- en: Zielske (1959) Hubert A. Zielske. 1959. The Remembering and Forgetting of Advertising.
    *Journal of Marketing* 23 (1959), 239 – 243. [https://api.semanticscholar.org/CorpusID:167354194](https://api.semanticscholar.org/CorpusID:167354194)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zielske（1959）**Hubert A. Zielske**. 1959. 广告的记忆与遗忘。*Journal of Marketing* 23（1959），239
    – 243. [https://api.semanticscholar.org/CorpusID:167354194](https://api.semanticscholar.org/CorpusID:167354194)
