- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.16788](https://ar5iv.labs.arxiv.org/html/2401.16788)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Steffi Chern^(2,4)  Ethan Chern^(1,4)  Graham Neubig²  Pengfei Liu^(1,3,4)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Shanghai Jiao Tong University  ²Carnegie Mellon University
  prefs: []
  type: TYPE_NORMAL
- en: ³Shanghai Artificial Intelligence Laboratory  ⁴Generative AI Research Lab (GAIR)
      Corresponding author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Despite the utility of Large Language Models (LLMs) across a wide range of
    tasks and scenarios, developing a method for reliably evaluating LLMs across varied
    contexts continues to be challenging. Modern evaluation approaches often use LLMs
    to assess responses generated by LLMs. However, the meta-evaluation conducted
    to assess the effectiveness of these LLMs as evaluators is typically constrained
    by the coverage of existing benchmarks or requires extensive human annotation.
    This underscores the urgency of methods for scalable meta-evaluation that can
    effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators
    across diverse tasks and scenarios, particularly in potentially new, user-defined
    scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation
    framework that leverages the capabilities of multiple communicative LLM agents.
    This framework supports multi-round discussions to assist human annotators in
    discerning the most capable LLMs as evaluators, which significantly eases their
    workload in cases that used to require large-scale annotations during meta-evaluation.
    We release the code for our framework, which is publicly available at: [https://github.com/GAIR-NLP/scaleeval](https://github.com/GAIR-NLP/scaleeval).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de2acb14ffbc24bbfa20e350b47856aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: We demonstrate ScaleEval, our scalable meta-evaluation framework.
    This is used in assessing the reliability and robustness of employing LLMs as
    evaluators for different evaluative purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) Bubeck et al. ([2023](#bib.bib3)); Gemini Team
    et al. ([2023](#bib.bib12)) have rapidly evolved to the point where they can tackle
    a wide range of tasks with impressive performance. While this has unlocked a variety
    of exciting potential applications, it has also introduced complex challenges
    in evaluating the generated outputs. Current efforts on LLM evaluation primarily
    focus on automated evaluation metrics (Fu et al., [2023](#bib.bib10); Li et al.,
    [2023c](#bib.bib19); Zheng et al., [2023](#bib.bib25); Wang et al., [2023a](#bib.bib23)),
    many of which use LLMs themselves to do evaluation. However, when these LLMs as
    evaluators are applied to a new task, it begs the question: *can LLMs be trusted
    for evaluation?* In many cases, the answer is not clear.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are a few fortunate tasks where meta-evaluation (evaluation
    of evaluation metrics) has been performed rigorously (§[2](#S2 "2 Related Work
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate")). This meta-evaluation typically involves
    the collection of human-annotated judgements for particular criteria (e.g. fluency
    of outputs, semantic adherence to the input). For instance, for machine translation
    quality metrics, there is an extensive meta-evaluation data from the WMT metrics
    task Freitag et al. ([2022](#bib.bib9)), and for summarization there are datasets
    like TAC and RealSum Dang et al. ([2008](#bib.bib8)); Bhandari et al. ([2020](#bib.bib2)).
    Once such a dataset is collected, meta-evaluation can be performed by measuring
    the correlation between automatic evaluation metrics and the human gold-standard
    (§[3](#S3 "3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
  prefs: []
  type: TYPE_NORMAL
- en: However, these datasets are extremely costly to collect, as they require meticulous
    annotation by skilled human experts. With the increasing use of LLMs for various
    purposes such as math problem solving Hendrycks et al. ([2021](#bib.bib14)), reading
    comprehension Zhong et al. ([2023](#bib.bib26)), creative writing Zheng et al.
    ([2023](#bib.bib25)), multilingual applications Hu et al. ([2020](#bib.bib15));
    Bang et al. ([2023](#bib.bib1)), and many more, it is not feasible to create these
    human-judged datasets for every new task. As a result, LLMs as evaluators are
    used without proper vetting, and in many cases the evaluators themselves are highly
    unreliable (Wang et al., [2023b](#bib.bib24); Huang et al., [2023](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose ScaleEval, a *scalable meta-evaluation framework*
    for the era of LLMs, which creates meta-evaluation benchmarks across various tasks
    and scenarios (§[4](#S4 "4 Methodology ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
    Concretely, ScaleEval relies on debate between multiple LLM agents, followed by
    minimal human oversight in cases where the agent LLMs do not agree (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")). Since our
    framework allows users to use their own prompts and responses while applying the
    framework to any scenario or criterion that they define, it offers flexibility
    and adaptability in various evaluation contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In experiments, we conduct meta-meta evaluation (§[6](#S6 "6 Exp-I: Meta-Meta-Evaluation
    of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")) demonstrating that our
    proposed approach correlates well with when meta-evaluation is performed entirely
    by human expert annotators. Further, we assess the reliability and cost-performance
    trade-off of various LLMs as evaluators under a variety of scenarios, and closely
    examine their specific capabilities and limitations as evaluators (§[7](#S7 "7
    Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
    We also examine the impact that variations in prompts used for evaluation can
    have on the performance of LLMs as evaluators (§[8](#S8 "8 Exp-III: Meta-Evaluation
    with Criteria Prompt Format Variations ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).'
  prefs: []
  type: TYPE_NORMAL
- en: All code from our framework is made available open-source, enabling the community
    to conduct meta-evaluation on LLMs as evaluators using their own prompts, LLM
    responses, criteria, and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | Meta-Eval | # Scenarios | Custom. | Scala. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-as-a-Judge | Human | High | ✗ | Low |'
  prefs: []
  type: TYPE_TB
- en: '| FairEval | Human | Low | ✗ | Low |'
  prefs: []
  type: TYPE_TB
- en: '| ChatEval | Human | Low | ✗ | Low |'
  prefs: []
  type: TYPE_TB
- en: '| ScaleEval | Agent Debate | High | ✓ | High |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison of the meta-evaluation processes across different strategies
    using LLMs as evaluators: LLM-as-a-Judge Zheng et al. ([2023](#bib.bib25)), FairEval
    Wang et al. ([2023b](#bib.bib24)), ChatEval Chan et al. ([2023](#bib.bib4)), and
    our own work, ScaleEval. “Custom.” denotes whether the evaluation criterion could
    be customized. “Scala.” refers to scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Automatic Evaluation of LLM Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common paradigm for evaluating LLMs is to evaluate their capabilities
    on standard benchmarks for tasks such as reasoning (e.g. BigBench Srivastava et al.
    ([2022](#bib.bib22))), common sense QA (e.g. MMLU Hendrycks et al. ([2020](#bib.bib13))),
    or code generation (e.g. HumanEval Chen et al. ([2021b](#bib.bib6))). These are
    indicative of the capabilities of the models, but do not measure model abilities
    for open-ended tasks requiring generation of free-form text.
  prefs: []
  type: TYPE_NORMAL
- en: To adapt to the rapid growth in the capabilities of LLMs for open-ended tasks,
    LLM evaluation has started to shift towards evaluating generated text directly,
    often using LLMs themselves as evaluators Fu et al. ([2023](#bib.bib10)); Li et al.
    ([2023c](#bib.bib19)); Zheng et al. ([2023](#bib.bib25)); Wang et al. ([2023a](#bib.bib23)).
    In addition, there are a few recent works that perform LLM-based multi-agent debate
    to improve the fidelity of evaluation Chan et al. ([2023](#bib.bib4)); Li et al.
    ([2023b](#bib.bib18)). While these methods take advantage of the instruction-following
    capabilities and versatility of LLMs, directly using LLMs as evaluators or communicative
    agents out-of-the-box in diverse, unseen user-defined scenarios provides no guarantees
    with respect to the accuracy of these methods. We aim to address this issue by
    introducing scalable meta-evaluation to ensure the reliability of the evaluation
    protocol under diverse scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Another widely used evaluation platform, Chatbot Arena Zheng et al. ([2023](#bib.bib25))
    supports a crowd-sourcing method to collect diverse user prompts from various
    scenarios. However, the process of evaluating LLMs’ performance in Chatbot Arena
    relies heavily on human evaluations, which may not be readily accessible to everyone
    interested in assessing LLMs’ abilities for a specific tasks or scenario. In addition,
    the human evaluators involved are not subject to a uniform set of standards or
    explicit evaluation guidelines, which could lead to biased or imprecise evaluation
    assessments.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Meta-Evaluation of LLMs as Evaluators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous research proposing methods for LLMs as evaluators usually involves
    conducting meta-evaluation in 3 different ways: (i) leveraging existing NLP meta-evaluation
    benchmarks Fu et al. ([2023](#bib.bib10)); Chan et al. ([2023](#bib.bib4)), (ii)
    conducting small-scale meta-evaluations on expert-annotated datasets for specific
    tasks or scenarios Chiang and Lee ([2023](#bib.bib7)); Wang et al. ([2023a](#bib.bib23));
    Zheng et al. ([2023](#bib.bib25)), or (iii) using crowd-sourcing platforms to
    collect human annotations Zheng et al. ([2023](#bib.bib25)). However, due to the
    lack of coverage in existing datasets and annotation budgets, both (i) and (ii)
    are inherently limited in their comprehensiveness. (iii) can provide more comprehensive
    meta-evaluation via crowd-sourcing, but the amount of human annotation required
    in the meta-evaluation process limits the scalability of the approach, and crowd
    workers may not be particularly accurate at more complex tasks. To address these
    issues, we propose an agent-debate-assisted meta-evaluation approach to mitigate
    this effort.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide an introduction to the concepts of automatic evaluation
    and meta-evaluation systems, particularly focused on evaluation of LLM-generated
    outputs in the era of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Key Terms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first define some key terms that will be used throughout our paper.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Criterion: A criterion defines a standard that measures the quality of the
    response generated by LLMs based on the user prompt. Some examples include: helpfulness,
    fluency, factuality, or creativity, among others.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scenario: A scenario describes the real-world situations in which users are
    interacting with LLMs. For example, brainstorming, coding, and dialog, among others.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Automatic Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Automatic evaluation using LLMs measures the quality of LLM-generated responses
    given prompts under different criteria. Usually, automatic evaluation is conducted
    with one of two different protocols: single-response evaluation and pairwise response
    comparison Ouyang et al. ([2022](#bib.bib21)); Zheng et al. ([2023](#bib.bib25));
    Li et al. ([2023a](#bib.bib17)). In this paper, we focus on pairwise response
    comparison. Pairwise response comparison is intuitive for both humans and LLMs
    as evaluators when conducting assessments. It could be further extended to provide
    win-rates and Elo scores across models Zheng et al. ([2023](#bib.bib25)), offering
    a straightforward leaderboard to understand the relative performance of different
    models under various scenarios. Formally, given an automatic evaluation metric
    $E$, evaluation for pairwise response comparison is done in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $o=E(c,p,r_{1},r_{2}).$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: $o\in\{1,0,-1\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Meta-Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Meta-evaluation assesses the quality of an automatic evaluation metric. Formally,
    we define a gold-standard evaluation metric $G$.
  prefs: []
  type: TYPE_NORMAL
- en: In pairwise response comparison, the meta-evaluation measures the example-level
    agreement rate or the system-level agreement rate between $E$ is a good automatic
    evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example-level agreement rate, we calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $0\leq\textsc{meta}(E)\leq 1$ refers to the Kronecker delta function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the system-level agreement rate, given that $\mathcal{E}=\{E(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$,
    we calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textsc{meta}(E)=\delta_{\mathrm{mode}(\mathcal{E}),\mathrm{mode}(\mathcal{G})},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\textsc{meta}(E)\in\{0,1\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we detail the frameworks that ScaleEval employs for meta-evaluation,
    evaluation, and human expert meta-meta evaluation. For meta-evaluation, we generally
    follow the pairwise response comparison setting described in §[3.3](#S3.SS3 "3.3
    Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). Notably, instead
    of relying solely on human labor to construct the meta-evaluation benchmark $\mathcal{G}$.
    For evaluation, we follow the pairwise response comparison setting outlined in
    §[3.2](#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language
    Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators
    via Agent Debate"). The meta-meta evaluation process also follows the rules for
    meta-evaluation, as described in §[3.3](#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate"). The process is included to ensure the
    reliability of using the agent-debate assisted meta-evaluation framework.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Meta-Evaluation Framework via Multi-Agent Debate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The meta-evaluation framework involves multiple communicative agents $\{A_{j}\}_{j=1}^{m}$
    to make a comprehensive assessment of LLMs under different scenarios and criteria.
    Each LLM agent is capable of providing an evaluation result regarding which response
    is better, along with its corresponding justifications. Note that each LLM agent
    can also review other agents’ evaluation results and justifications after the
    initial round of discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the initial round of discussion $d=0$, each LLM agent independently provides
    an evaluation result and justification:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{A}_{0}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'indicates whether $r_{1,i}$ round of discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where similarly to $\mathcal{A}_{0}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{A}_{d}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: The detailed prompt template for meta-evaluation can be found in Table [6](#A1.T6
    "Table 6 ‣ Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")
    under Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where agents fail to reach a consensus after $d=D-1$ rounds of discussions,
    a human evaluator intervenes. The human evaluator reviews the assessment reports
    provided by the agents and makes a final decision. Through this process, we incorporate
    an element of human oversight, thereby increasing the reliability of the final
    decision. This approach strikes a balance between efficiency and the need for
    human judgment, ensuring that evaluations are done in a timely and accurate manner.
    An example of the multi-agent debate process during meta-evaluation is demonstrated
    in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Meta-Evaluation Framework via Multi-Agent Debate
    ‣ 4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/27c70a5fa3443a5ef0623e8b0858ded5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An example of the multi-agent debate process during meta-evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evaluation Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We follow the pairwise response comparison setting outlined in §[3.2](#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    Note that in the LLM era, the automatic evaluation metric $E$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Human Expert Meta-Meta Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test the reliability of our proposed meta-evaluation framework, we apply
    meta-meta evaluation. The meta-meta evaluation process also follows the meta-evaluation
    process described in §[3.3](#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can
    Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs
    as Evaluators via Agent Debate"), where $E$ is instantiated as the human expert
    annotation protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Examined Scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Establishing real-life scenarios that reflect individuals’ daily usage is key
    to assess the performance and limitations of LLMs in a comprehensive manner. In
    the current instantiation of ScaleEval, we include 8 different scenarios that
    are closely related to everyday situations and tasks Liang et al. ([2022](#bib.bib20));
    Li et al. ([2023a](#bib.bib17)). Some example prompts for each defined scenario
    is shown in Table [2](#S5.T2 "Table 2 ‣ 5 Examined Scenarios ‣ Can Large Language
    Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators
    via Agent Debate"). We describe more about exactly how we collect data for each
    of these scenarios below. Individuals interested in evaluating LLMs with our framework
    can supplement their assessment with additional scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Brainstorming | - Can you tell me how to make chocolate chip cookies? |'
  prefs: []
  type: TYPE_TB
- en: '| - Make a list of snacks and foods to serve as party snacks on a game day!
    |'
  prefs: []
  type: TYPE_TB
- en: '| Coding | - What is the difference between HTML and JavaScript? |'
  prefs: []
  type: TYPE_TB
- en: '| - Implement a binary search algorithm to find a specific element in a sorted
    array. |'
  prefs: []
  type: TYPE_TB
- en: '| Dialog | - Act as the Norse Goddess Freyja. |'
  prefs: []
  type: TYPE_TB
- en: '| - Can you think and feel like a human? |'
  prefs: []
  type: TYPE_TB
- en: '| Judgement | - What if the Aztecs had successfully repelled the Spanish conquistadors?
    |'
  prefs: []
  type: TYPE_TB
- en: '| - How can you determine if a person is genuinely interested in a conversation
    or simply being polite? |'
  prefs: []
  type: TYPE_TB
- en: '| Math | - Given that f(x) = 5$x^{3}$ + 3, find the value of f(2). |'
  prefs: []
  type: TYPE_TB
- en: '| - If the endpoints of a line segment are (2, -2) and (10, 4), what is the
    length of the segment? |'
  prefs: []
  type: TYPE_TB
- en: '| ODG | - Is there a meaning for Christmas wreaths? |'
  prefs: []
  type: TYPE_TB
- en: '| - What are some of the best universities for studying robotics? |'
  prefs: []
  type: TYPE_TB
- en: '| ODS | - What causes the northern lights? |'
  prefs: []
  type: TYPE_TB
- en: '| - What do the different octane values of gasoline mean? |'
  prefs: []
  type: TYPE_TB
- en: '| Writing | - Can you help me write a formal email to a potential business
    partner proposing a joint venture? |'
  prefs: []
  type: TYPE_TB
- en: '| - Take MLK speech "I had a dream" but turn it into a top 100 rap song. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Examined scenarios and corresponding selected examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Brainstorming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The brainstorming scenario is designed to test the LLMs’ ability to engage in
    problem-solving, creative ideation, and generation of insightful responses, especially
    in situations that require critical thinking and detailed, step-by-step reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code scenario evaluates LLMs’ ability to comprehend, produce, and debug
    code, as well as answering coding-related questions.
  prefs: []
  type: TYPE_NORMAL
- en: Dialog
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dialog scenario measures LLMs’ ability to engage with users in a manner
    that is intuitive, human-like, and dynamic, testing their proficiency through
    context-sensitive conversations and role-playing that require maintaining a consistent
    persona throughout a series of interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Judgement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The judgement scenario assesses LLMs‘ ability to make inferences and formulate
    opinions, including soliciting insights on diverse situations or emotions, and
    posing questions that require logical thinking or reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Math
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The math scenario evaluates the LLMs’ proficiency in understanding and solving
    mathematical problems, emphasizing their accuracy in tasks ranging from simple
    calculations to complex reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Open-Domain General (ODG)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ODG scenario measures LLMs’ proficiency in applying diverse knowledge and
    exercising reasoning across a wide array of topics, such as answering questions
    with definitive answers.
  prefs: []
  type: TYPE_NORMAL
- en: Open-Domain Science (ODS)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ODS scenario tests the LLMs’ application of scientific knowledge, and gauges
    their ability to accurately interpret and respond to queries related to scientific
    disciplines like biology, chemistry, physics, astronomy, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Writing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The writing scenario evaluates LLMs’ ability to summarize, translate, and generate
    various texts, testing their core language processing and production skills.
  prefs: []
  type: TYPE_NORMAL
- en: '6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first perform meta-meta-evaluation, examining whether the
    meta-evaluation results of using ScaleEval match closely to those resulting from
    meta-evaluation using human evaluators.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For our ScaleEval meta-evaluation framework (as described in §[4.1](#S4.SS1
    "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")), we deploy three LLM agents to perform multi-agent
    debate: gpt-4-turbo, claude-2, and gpt-3.5-turbo.¹¹1Results collected in December
    2023\. Specific models used are: gpt-4-1106-preview, claude-2, and gpt-3.5-turbo-1106.
    In our meta-evaluation experiment, we analyze a total of 160 prompts. This set
    is comprised 137 prompts from AlpacaEval Li et al. ([2023c](#bib.bib19)), 10 coding
    problem prompts from HumanEval Chen et al. ([2021a](#bib.bib5)), and 13 math problem
    prompts from GSM-Hard Gao et al. ([2022](#bib.bib11)). We categorize these prompts
    into four distinct scenarios: brainstorming, coding, math, and writing, where
    each scenario contains 40 prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each scenario is evaluated based on the following criteria, respectively: helpfulness,
    interpretability, reasoning, and creativity. We evaluate the generated responses
    from the following three LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro.
    We select the above LLMs to evaluate due to their rather similar performances
    according to past research and public user feedback, which can help us establish
    a more nuanced understanding of their performance in various real-world scenarios,
    and to identify specific contexts where one may outperform the others.'
  prefs: []
  type: TYPE_NORMAL
- en: Our meta-meta evaluation involves having human experts annotate which LLM submission
    they think is better based on a defined criterion during pairwise comparisons.
    A total of seven human experts were selected from a pool of Carnegie Mellon University
    students who have the relevant expertise in answering the queries in each scenario.
    Different groups of three human experts are responsible for answering the prompts
    in each scenario, where they are assigned to the scenario that relates to their
    expertise. Each expert received identical instructions for the task – they were
    asked to decide which submission is better based on our defined criteria, and
    for each comparison, label either 0 (neither submission is better), 1 (submission
    1 is better), or 2 (submission 2 is better). The label 2 corresponds to the label
    -1 as denoted in section [3.2](#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate"). The experts were tasked to conduct 30
    comparisons for each of the four different scenarios (brainstorming, coding, math,
    and writing), based on their corresponding defined criteria (helpfulness, interpretability,
    reasoning, and creativity). This results in a total of 120 final judgements. The
    question prompts, LLM responses, and criteria utilized for human expert annotations
    were consistent with those used during our meta-evaluation experiment. All the
    details were presented in a google sheet that allowed experts to record their
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Can LLM agents with multi-agent debate be used as meta-evaluators in new
    user-defined scenarios?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To validate the reliability of ScaleEval’s meta-evaluation framework, we perform
    comparisons between the results from human experts and ScaleEval’s multi-agent
    debate by two key metrics: the example-level agreement rate and the system-level
    agreement rate, as mentioned in §[3.3](#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate"). The example-level agreement rate measures
    the proportion of instances where the multi-agent debate results correspond with
    the human experts judgements. On the other hand, the system-level agreement rate
    assesses whether the human experts and multi-agents concur in their overall evaluation
    of which LLMs produce the best responses for each scenario. A high agreement rate
    in both metrics would suggest a strong reliability and validity of our meta-evaluation
    framework, indicating that both human and LLM agents consistently recognize and
    agree on the quality of responses generated by LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From Table [3](#S6.T3 "Table 3 ‣ Results ‣ 6 Exp-I: Meta-Meta-Evaluation of
    Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we generally observe
    a higher example-level agreement rate between human experts and ScaleEval, compared
    to the agreement rate between human experts and individual LLM evaluations. The
    consistently high agreement rates observed suggest that our meta-evaluation framework
    aligns well with human expert judgments in these areas, indicating a reliable
    performance of the collective use of LLMs in meta-evaluating complex scenarios.
    Across all LLM submission comparisons in our experiment, we observe higher agreement
    rates in decisions between ScaleEval outcomes and those of human experts, particularly
    in coding and math scenarios. This observed trend could be attributed to the inherently
    objective nature of these subjects, which have relatively clear, definitive answers
    unlike more subjective areas like creative writing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Fig. [3](#S6.F3 "Figure 3 ‣ Results ‣ 6 Exp-I: Meta-Meta-Evaluation
    of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we notice a consistent
    "preference in the same direction" between human experts and multi-agent debates
    across all LLM pairwise comparisons and scenarios. Notably, gpt-3.5-turbo is favored
    (higher win rates) in brainstorming, math, and writing scenarios when compared
    with claude-instant. Similarly, gemini-pro is also preferred over claude-instant
    in all scenarios. When comparing gpt-3.5-turbo with gemini-pro, a varied pattern
    in decision outcomes is observed: both human experts and multi-agent systems agree
    that gpt-3.5-turbo outperforms gemini-pro in scenarios involving math and writing.
    Conversely, gemini-pro is deemed superior in brainstorming and coding scenarios.
    The high agreement of multi-agent preferences with human expert judgement results
    verifies the reliability of using multiple LLMs agents as meta-evaluators in various
    user-defined scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM Pairwise Comparisons | Criterion | Scenario | Meta-Evaluation | GPT-4-Turbo
    | Claude-2 | GPT-3.5-Turbo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo vs. Claude-Instant | Helpfulness | Brainstorming | 0.600 |
    0.633 | 0.433 | 0.267 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.733 | 0.700 | 0.533 | 0.567 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.867 | 0.600 | 0.400 | 0.367 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.700 | 0.667 | 0.400 | 0.333 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-Instant vs. Gemini-Pro | Helpfulness | Brainstorming | 0.667 | 0.533
    | 0.467 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.833 | 0.600 | 0.500 | 0.567 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.767 | 0.667 | 0.330 | 0.367 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.733 | 0.633 | 0.400 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo vs. Gemini-Pro | Helpfulness | Brainstorming | 0.733 | 0.600
    | 0.467 | 0.467 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.833 | 0.733 | 0.567 | 0.667 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.867 | 0.767 | 0.500 | 0.433 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.767 | 0.667 | 0.500 | 0.433 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Example-level agreement rate comparison between human expert and ScaleEval’s
    meta-evaluation vs. human expert and single LLM evaluation across four scenarios
    and criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94ac091abcc90812a9ce178f6dd82fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) GPT-3.5-Turbo vs. Claude-Instant
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58187d6c92a28030224f0235c478094c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Claude-Instant vs. Gemini-Pro
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb0296cc95403bd1c082863792bc87be.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) GPT-3.5-Turbo vs. Gemini-Pro
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: System-level agreement – win rates for each LLM pairwise comparison.
    Left bars in each scenario represent human expert results; right bars represent
    ScaleEval’s meta-evaluation results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d349af3b522373a9f3d62c9f5827bd11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Human Fleiss Kappa for each LLM pairwise comparison under four scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '7 Exp-II: Meta-Evaluation vs. LLM Evaluators'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we use the fact that ScaleEval allows for reliable and scalable meta-evaluation
    to examine the traits of LLMs as evaluators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: What are the capabilities and limitations of each LLM evaluator?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To effectively evaluate the performance of each LLM in its role as an evaluator,
    we adopt an approach that involves comparing the outcomes from our meta-evaluation
    process with the evaluations made independently by each LLM evaluator, which uncovers
    any disagreements or alignments between them. In the process, we aim to shed light
    on the performance characteristics of each LLM evaluator, which helps us identify
    which of them demonstrate superior evaluative abilities, thereby contributing
    to our understanding of their reliability in evaluating responses under each scenario.
    In addition, we provide a comprehensive cost-performance analysis to decide which
    LLM evaluator is the most suitable choice in each scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For meta-evaluation, we employed three LLMs (gpt-4-turbo, claude-2, and gpt-3.5-turbo)
    as evaluators to perform pairwise comparisons of responses from three distinct
    LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro. Previous studies have highlighted
    the presence of positional biases when LLMs are used as evaluators Wang et al.
    ([2023b](#bib.bib24)). In response to these findings, we have implemented a strategy
    of randomization to mitigate such biases. Specifically, the sequence in which
    submissions from LLMs are presented to the agent evaluators is randomized. Additionally,
    we also randomize the order of discussions for each agent evaluator in every case.
    These approaches ensure that the process is fair and unbiased as much as possible,
    allowing for a more accurate assessment of the LLM evaluators’ performance. The
    meta-evaluations were done under the following 8 scenarios: brainstorming, coding,
    dialog, judgement, open-domain general, open-domain science, and writing, with
    the same set of 4 criteria used during human expert annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [4](#S7.T4 "Table 4 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") compares the agreement rate between ScaleEval’s
    meta-evaluation and each LLM evaluator across criteria and scenarios. We observe
    that gpt-4-turbo, when serving as an evaluator, has the highest agreement rates
    with our meta-evaluation, particularly in the scenarios of brainstorming, dialog,
    and ODG with the helpfulness criterion. It stands out with the highest overall
    average score of 0.780\. However, our selected open-source model evaluator, auto-j,
    outperforms gpt-4-turbo in evaluating coding questions based on the helpfulness
    criterion. In addition, it exhibits the highest agreement rate with our meta-evaluation
    in the judgement scenario, according to the helpfulness criterion, indicating
    it as the most capable evaluator in this setting. It also achieves comparable
    results with other closed-source models like claude-2 and gpt-3.5-turbo in most
    of the other scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: While gpt-4-turbo performs the best as an evaluator in a majority of scenarios,
    it is not necessarily the best choice when we take into consideration its relatively
    high API costs. In fact, both the more affordable version (gpt-3.5-turbo) and
    our selected free, open-source model (auto-j) show comparable performance in scenarios
    like judgement and writing. For coding-related evaluations, the slightly less
    expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: '| Criterion | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo | Auto-J |'
  prefs: []
  type: TYPE_TB
- en: '| Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 | 0.575 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Coding | 0.600 | 0.725 | 0.675 | 0.675 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dialog | 0.800 | 0.700 | 0.700 | 0.625 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Judgement | 0.725 | 0.625 | 0.725 | 0.750 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Math | 0.825 | 0.650 | 0.600 | 0.350 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ODG | 0.850 | 0.525 | 0.575 | 0.700 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ODS | 0.875 | 0.525 | 0.575 | 0.675 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Writing | 0.750 | 0.600 | 0.750 | 0.600 |'
  prefs: []
  type: TYPE_TB
- en: '| Interpretability | Coding | 0.825 | 0.600 | 0.550 | 0.525 |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning | Math | 0.650 | 0.525 | 0.475 | 0.450 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Judgement | 0.750 | 0.650 | 0.700 | 0.675 |'
  prefs: []
  type: TYPE_TB
- en: '| Creativity | Writing | 0.775 | 0.600 | 0.575 | 0.650 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Brainstorming | 0.800 | 0.525 | 0.550 | 0.625 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dialog | 0.875 | 0.750 | 0.700 | 0.800 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | Overall | 0.780 | 0.607 | 0.629 | 0.619 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Agreement rate between ScaleEval’s meta-evaluation and each LLM evaluator
    for comparing GPT3.5-Turbo vs. Claude-Instant.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Criteria Format | Criteria | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| General | Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.825 | 0.600 | 0.550 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.650 | 0.525 | 0.475 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.800 | 0.600 | 0.575 |'
  prefs: []
  type: TYPE_TB
- en: '| Shortened | Helpfulness | Brainstorming | 0.675 | 0.500 | 0.575 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.675 | 0.325 | 0.425 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.625 | 0.425 | 0.400 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.675 | 0.250 | 0.525 |'
  prefs: []
  type: TYPE_TB
- en: '| Gibberish | Helpfulness | Brainstorming | 0.575 | 0.450 | 0.575 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.700 | 0.275 | 0.525 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.650 | 0.200 | 0.400 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.550 | 0.150 | 0.450 |'
  prefs: []
  type: TYPE_TB
- en: '| Shuffled | Helpfulness | Brainstorming | 0.625 | 0.550 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.600 | 0.400 | 0.525 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.625 | 0.225 | 0.600 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.625 | 0.275 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| Flipped | Helpfulness | Brainstorming | 0.725 | 0.325 | 0.550 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.725 | 0.425 | 0.300 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.575 | 0.250 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.750 | 0.075 | 0.550 |'
  prefs: []
  type: TYPE_TB
- en: '| Masked | Helpfulness | Brainstorming | 0.725 | 0.300 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Interpretability | Coding | 0.650 | 0.225 | 0.475 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reasoning | Math | 0.575 | 0.150 | 0.375 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Creativity | Writing | 0.575 | 0.200 | 0.400 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Agreement rate between ScaleEval’s meta-evaluation results and each
    LLM evaluator under various criteria prompt formats and scenarios comparing GPT3.5-Turbo
    vs. Claude-Instant.'
  prefs: []
  type: TYPE_NORMAL
- en: '8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Q3: How do the qualities of criteria prompts influence the robustness of LLMs
    as evaluators in different scenarios?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prior studies have revealed that variations in prompts can substantially affect
    the behavior of LLMs, particularly with the text they generate. With this in mind,
    we define various formatted criteria for evaluating LLM responses under each scenario.
    This approach aims to examine the extent to which different formats of criteria
    prompts influence both the performance and robustness of LLMs as evaluators.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We define five variations of the same criteria prompts: shortened, gibberish,
    shuffled, flipped, and masked (see Table [7](#A1.T7 "Table 7 ‣ Appendix A Meta-Evaluation
    Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") under Appendix [A](#A1 "Appendix A Meta-Evaluation
    Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") for detailed format). With these criteria
    format variations, we intend to observe how the LLMs as evaluators would respond
    differently when conducting evaluation. We compare the example-level agreement
    rate between ScaleEval’s meta-evaluation results and each LLM evaluator.'
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on Table [5](#S7.T5 "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs.
    LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we observe that the
    performance of LLMs as evaluators generally deteriorates when certain letters
    in the criteria prompts are masked. Furthermore, the removal of guiding phrases
    at the beginning, such as "Not Helpful" or "Highly Helpful", can also diminish
    their effectiveness as evaluators. Both gpt-4-turbo and gpt-3.5-turbo demonstrate
    some resilience to these adversarially formatted criteria prompts, maintaining
    a relatively consistent agreement rates across various criteria formats. In contrast,
    Claude-2 often showcases confusion and refuses to evaluate, particularly in cases
    with gibberish and masked criteria prompts, where it rejects answering about half
    of the questions. It typically responds with statements like, "Unfortunately I
    do not have enough information here to provide a fair evaluation… The criteria
    describe different quality levels, but there is no detail on what specific aspects
    of the responses should be assessed… any judgement risks being arbitrary or biased…".
    None of the LLMs as evaluators we tested maintained very similar evaluation capabilities
    when faced with these adversarially formatted criteria prompts, indicating a limitation
    in these LLMs as evaluators’ current design and application. Despite their advanced
    capabilities in fulfilling a variety of tasks, they may still struggle with understanding
    and responding accurately to substituted criteria information, highlighting an
    area for potential improvement in future iterations of LLM technology. Among all
    the different formatted criteria, we highlight the cases where the LLMs perform
    the best as evaluators in Table [5](#S7.T5 "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation
    vs. LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate").'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose ScaleEval, a scalable, agent-debate assisted meta-evaluation
    framework for assessing the reliability and robustness of LLMs as evaluators.
    This approach addresses the expensive and time-intensive challenges inherent in
    traditional meta-evaluation methods, particularly pertinent as the usage of LLMs
    expands, necessitating a more scalable solution. Through our research, we have
    not only demonstrated the reliability of our proposed meta-evaluation framework,
    but also shed light on the capabilities and limitations of LLMs as evaluators
    in various scenarios. We observe how the results from these LLMs as evaluators
    vary based on modifications to the same criteria prompts. By open-sourcing our
    framework, we aim to foster further research in this field and encourage the development
    of more advanced and reliable LLMs as evaluators in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Chunting Zhou, Weizhe Yuan, Chunpu Xu, Yan Ma, and Binjie Wang for
    the helpful discussions and feedback.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale. Fung. 2023. A multitask, multilingual, multimodal evaluation
    of chatgpt on reasoning, hallucination, and interactivity. *arXiv:2302.04023v3*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhandari et al. (2020) Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu,
    and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. *arXiv
    preprint arXiv:2010.07100*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based
    evaluators through multi-agent debate. *arXiv preprint arXiv:2308.07201*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021a. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021b. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chiang and Lee (2023) Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language
    models be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dang et al. (2008) Hoa Trang Dang, Karolina Owczarzak, et al. 2008. Overview
    of the tac 2008 update summarization task. In *TAC*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freitag et al. (2022) Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
    Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and
    André FT Martins. 2022. Results of wmt22 metrics shared task: Stop using bleu–neural
    metrics are better and more robust. In *Proceedings of the Seventh Conference
    on Machine Translation (WMT)*, pages 46–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language
    models. *arXiv preprint arXiv:2211.10435*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemini Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui
    Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M
    Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig,
    Orhan Firat, and Melvin. Johnson. 2020. Xtreme: A massively multilingual multi-task
    benchmark for evaluating cross-lingual generalization. *arXiv:2003.11080v5*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot
    self-correct reasoning yet. *arXiv preprint arXiv:2310.01798*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint
    arXiv:2310.05470*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Ruosen Li, Teerth Patel, and Xinya Du. 2023b. Prd: Peer rank
    and discussion improve large language model based evaluations. *arXiv preprint
    arXiv:2307.02762*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023c. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator?
    a preliminary study. *arXiv preprint arXiv:2303.04048*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are
    not fair evaluators. *ArXiv*, abs/2305.17926.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan. Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv:2304.06364v2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Meta-Evaluation Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| <Initial Evaluation> |'
  prefs: []
  type: TYPE_TB
- en: '| Compare the two submissions based on the criteria above. Which one is better?
    First, provide a step-by-step explanation of your evaluation reasoning according
    to the criteria. Avoid any potential bias. Ensure that the order in which the
    submissions were presented does not affect your judgement. Keep your explanation
    strictly under 150 words. Afterwards, choose one of the following options: |'
  prefs: []
  type: TYPE_TB
- en: '| Submission 1 is better: "1" |'
  prefs: []
  type: TYPE_TB
- en: '| Submission 2 is better: "2" |'
  prefs: []
  type: TYPE_TB
- en: '| Neither is better: "0" |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds
    to your reasoning. At the end, repeat just the number again by itself on a new
    line. |'
  prefs: []
  type: TYPE_TB
- en: '| [Question]: {question} |'
  prefs: []
  type: TYPE_TB
- en: '| [Submission 1]: {submission_1} |'
  prefs: []
  type: TYPE_TB
- en: '| [Submission 2]: {submission_2} |'
  prefs: []
  type: TYPE_TB
- en: '| [Criteria]: {criteria} |'
  prefs: []
  type: TYPE_TB
- en: '| [User]: {user_prompt} |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| You are evaluating two submissions for a particular question, using a specific
    set of criteria. Above is the data. |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| <Discussion Rounds> |'
  prefs: []
  type: TYPE_TB
- en: '| Always remember you are Speaker 1/2/3. Review again your own previous evaluations/discussions
    first, then answer user’s request from Speaker 1/2/3’s perspective. |'
  prefs: []
  type: TYPE_TB
- en: '| [Question]: {question} |'
  prefs: []
  type: TYPE_TB
- en: '| [Submission 1]: {submission_1} |'
  prefs: []
  type: TYPE_TB
- en: '| [Submission 2]: {submission_2} |'
  prefs: []
  type: TYPE_TB
- en: '| [Criteria]: {criteria} |'
  prefs: []
  type: TYPE_TB
- en: '| [Speaker 1’s Initial Evaluation]: {evaluation_1} |'
  prefs: []
  type: TYPE_TB
- en: '| [Speaker 2’s Initial Evaluation]: {evaluation_2} |'
  prefs: []
  type: TYPE_TB
- en: '| [Speaker 3’s Initial Evaluation]: {evaluation_3} |'
  prefs: []
  type: TYPE_TB
- en: '| [Speaker {speaker_number}’s Discussion -- Round {round_number}]: {discussion_reasoning}
    |'
  prefs: []
  type: TYPE_TB
- en: '| ... |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Read the question, submissions, criteria, and evaluations above. First, explain
    your thoughts step-by-step about other speakers’ evaluations. Second, explain
    your reasoning step-by-step regarding whether or not to change your original answer
    about which submission you think is better after considering other speakers’ perspectives.
    Keep your reasoning strictly under 150 words. Afterwards, choose one of the following
    options: |'
  prefs: []
  type: TYPE_TB
- en: '| Submission 1 is better: "1" |'
  prefs: []
  type: TYPE_TB
- en: '| Submission 2 is better: "2" |'
  prefs: []
  type: TYPE_TB
- en: '| Neither is better: "0" |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds
    to your reasoning. At the end, repeat just the number again by itself on a new
    line. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Prompt template for meta-evaluation via multi-agent debate'
  prefs: []
  type: TYPE_NORMAL
- en: '| <Type 1: General Format Version> |'
  prefs: []
  type: TYPE_TB
- en: '| "1": "Not Helpful - The response is completely unrelated, lacks coherence,
    and fails to provide any meaningful information." |'
  prefs: []
  type: TYPE_TB
- en: '| "2": "Somewhat Helpful - The response bears some relevance but remains largely
    superficial and unclear, addressing only the peripheral aspects of the user’s
    needs." |'
  prefs: []
  type: TYPE_TB
- en: '| "3": "Moderately Helpful - The response is mostly relevant and clear, covering
    the basic aspects of the query, but lacks depth and comprehensive elucidation."
    |'
  prefs: []
  type: TYPE_TB
- en: '| "4": "Helpful - The response is on-point, detailed, and well-articulated,
    offering valuable information and clarifications that meet the user’s primary
    needs and enhance understanding." |'
  prefs: []
  type: TYPE_TB
- en: '| "5": "Highly Helpful - The response is exceptionally thorough and precise,
    providing additional insights and valuable supplementary information." |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| <Type 2: Shortened Format Version> |'
  prefs: []
  type: TYPE_TB
- en: '| "1": "The response is completely unrelated, lacks coherence, and fails to
    provide any meaningful information." |'
  prefs: []
  type: TYPE_TB
- en: '| "2": "The response bears some relevance but remains largely superficial and
    unclear, addressing only the peripheral aspects of the user’s needs." |'
  prefs: []
  type: TYPE_TB
- en: '| "3": "The response is mostly relevant and clear, covering the basic aspects
    of the query, but lacks depth and comprehensive elucidation." |'
  prefs: []
  type: TYPE_TB
- en: '| "4": "The response is on-point, detailed, and well-articulated, offering
    valuable information and clarifications that meet the user’s primary needs and
    enhance understanding." |'
  prefs: []
  type: TYPE_TB
- en: '| "5": "The response is exceptionally thorough and precise, providing additional
    insights and valuable supplementary information." |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| <Type 3: Gibberish Format Version> |'
  prefs: []
  type: TYPE_TB
- en: '| "1": "N*t H$l%ful - Th$ r$sp0n$e is c mplt$l? unr€la7$d, la$ks c()h$r$n(€,
    and f#i/s t# p$o&id$ any m€an*&gful !format$on." |'
  prefs: []
  type: TYPE_TB
- en: '| "2": "S#m$*ha+ H$%*fu/ - Th$ r#s0!n$ b%ars $o/e re$ev*nc$ b$t r$ma$n$ l#rg$l4
    $u/7$r7cial an* !ncl=4r, a6r$ss@n4 o7ly th$ p$r4ph@r$l a5p$cts #f th$ $s*r’s n**ds."
    |'
  prefs: []
  type: TYPE_TB
- en: '| "3": "M$!7r$t#ly H$lpfu& - Th$ r@s0*n$@ !s m$%stl€ r$’$van7 an cl$ar, c$%$r$n4
    th$ ba$!c a$%cts of th$ qu€ry, b$t l#cks d$pth an cmpr$h$ns$v$ lu$7$dat!on." |'
  prefs: []
  type: TYPE_TB
- en: '| "4": "H$lpfu& - Th$ r!s0*n$e !s o/7-p$!nt, d$ta$!l$d, an w$l/-a&!u/at$d,
    #ff$r!n4 v#l$%bl$ #nformat$on and cl*r$!cat!ons th#t m=t th$ u/7$rś pr!/ary n$$ds
    an* @n7anc$ un#rstand!n4." |'
  prefs: []
  type: TYPE_TB
- en: '| "5": "H4#h7y H$!p%u& - Th$ r$s&*n!e !s $xc$pt$#nally th#r#7gh an* pr$c$%$,
    pr#v$d$n# a4*!t$#nal !ns$4hts an* v#lu%bl$ @*pp%$%ntary #n%ormat$on." |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| <Type 4: Shuffled Format Version> |'
  prefs: []
  type: TYPE_TB
- en: '| "1": "coherence fails provide unrelated, completely response - and the meaningful
    any to lacks Not Helpful is The information." |'
  prefs: []
  type: TYPE_TB
- en: '| "2": "superficial response largely addressing unclear, remains only needs.
    - relevance user’s and the Helpful the peripheral some bears but aspects Somewhat
    The of" |'
  prefs: []
  type: TYPE_TB
- en: '| "3": "basic aspects query, lacks Moderately covering clear, - Helpful is
    depth response and comprehensive elucidation. relevant mostly the The and the
    of but" |'
  prefs: []
  type: TYPE_TB
- en: '| "4": "clarifications the is response information needs enhance and Helpful
    - on-point, valuable well-articulated, offering understanding. The and detailed,
    primary that user’s meet" |'
  prefs: []
  type: TYPE_TB
- en: '| "5": "valuable Highly response is providing - the exceptionally Helpful information.
    insights thorough and additional precise, supplementary and The" |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| <Type 5: Flipped Format Version> |'
  prefs: []
  type: TYPE_TB
- en: '| "1": "toN lufpleH - ehT esnopser si yletelpmoc detalernu, skcal ecnerehoc,
    dna sliaf ot edivorp yna lufgninaem noitamrofni." |'
  prefs: []
  type: TYPE_TB
- en: '| "2": "tamewoS lufpleH - ehT esnopser sraeb emos ecnaveler tub sniamer ylegral
    laicifrepus dna raelcnu, gnisserdda ylno eht larehpirep stcepsa fo eht s’resu
    sdeen." |'
  prefs: []
  type: TYPE_TB
- en: '| "3": "yletaredoM lufpleH - ehT esnopser si yltsom tnaveler dna raelc, gnirevoc
    eht cisab stcepsa fo eht yreuq, tub skcal htped dna evisneherpmoc noitadicule."
    |'
  prefs: []
  type: TYPE_TB
- en: '| "4": "lufpleH - ehT esnopser si tniop-no, deliated, dna detalucitra-llew,
    gnireffo elbaulav noitamrofni dna snoitacifralc taht teem eht s’resu yramirp sdeen
    dna ecnahne gnidnatsrednu." |'
  prefs: []
  type: TYPE_TB
- en: '| "5": "ylhgiH lufpleH - ehT esnopser si yllanoitpecxe hguoroht dna esicerp,
    gnidivorp lanoitidda sthgisni dna elbaulav yratnemelppus noitamrofni." |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| <Type 6: Masked Format Version> |'
  prefs: []
  type: TYPE_TB
- en: '| "1": "N__ H_l_ful - The r__pnse is c_m__et__y unr_l_te_, lacks _ohe_en_e,
    _nd _ai_s to p_ov_de _ny m_a__ngfu_ _nfo_ma_ion." |'
  prefs: []
  type: TYPE_TB
- en: '| "2": "_om_w_at He_p_ul - T_e re_ponse be_rs _ome rel__a_ce but r__ains la__ely
    s__erfi__al and u_cle__, ad_res__ng onl_ _he __ri__er_l a_pe_ts of t__ u_e_’s
    ne_ds." |'
  prefs: []
  type: TYPE_TB
- en: '| "3": "Mod___tely _elp__l - Th_ _esp__se is mos__y re__va_t an_ _le_r, c_v__ing
    the ba_ic _spe_ts of the q_e_y, but __cks _e_th and co_preh_ns_ve el_c_d_t_on."
    |'
  prefs: []
  type: TYPE_TB
- en: '| "4": "__lpful - _he respo_se is on-p_in_, d___iled, and we_l-ar_icu_ated,
    of_er_ng val_ab_e __for_ation and cl_r_fi__t_ons t_at mee_ the _se_’s p_im_r_
    _eeds and en__nce u_de__tan_ing." |'
  prefs: []
  type: TYPE_TB
- en: '| "5": "Hi_h_y H__p_ul - The r_spon_e is e_c_p_io_al__ th_r_ugh and p_ec_se,
    pr_vi_ing a_di__on_l ins_g_ts and va_u_b_e _upp_e_en_a_y inf_rma_io_." |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Criteria prompt format variations for helpfulness'
  prefs: []
  type: TYPE_NORMAL
