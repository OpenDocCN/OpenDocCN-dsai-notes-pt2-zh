- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:39:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning
    of LLM-based Embodied Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04449](https://ar5iv.labs.arxiv.org/html/2408.04449)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Zihao Zhu ¹   Bingzhe Wu²²²footnotemark: 2   Zhengyou Zhang³   Baoyuan Wu¹'
  prefs: []
  type: TYPE_NORMAL
- en: ¹ School of Data Science, The Chinese University of Hong Kong, Shenzhen,
  prefs: []
  type: TYPE_NORMAL
- en: Guangdong, 518172, P.R. China
  prefs: []
  type: TYPE_NORMAL
- en: ² Tencent AI Lab  ³ Tencent Robotics X
  prefs: []
  type: TYPE_NORMAL
- en: zihaozhu@link.cuhk.edu.en,  bingzhewu@tencent.com,
  prefs: []
  type: TYPE_NORMAL
- en: zhengyou@tencent.com,  wubaoyuan@cuhk.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: 'WARNING: This paper contains unsafe plans generated by LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: that may lead to physical risks in the real world. This work was done when the
    author was interning at Tencent AI Lab.Corresponding Authors.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The integration of large language models (LLMs) into robotics significantly
    enhances the capabilities of embodied agents in understanding and executing complex
    natural language instructions. However, the unmitigated deployment of LLM-based
    embodied systems in real-world environments may pose potential physical risks,
    such as property damage and personal injury. Existing security benchmarks for
    LLMs overlook risk awareness for LLM-based embodied agents. To address this gap,
    we propose RiskAwareBench, an automated framework designed to assess physical
    risks awareness in LLM-based embodied agents. RiskAwareBench consists of four
    modules: safety tips generation, risky scene generation, plan generation, and
    evaluation, enabling comprehensive risk assessment with minimal manual intervention.
    Utilizing this framework, we compile the PhysicalRisk dataset, encompassing diverse
    scenarios with associated safety tips, observations, and instructions. Extensive
    experiments reveal that most LLMs exhibit insufficient physical risk awareness,
    and baseline risk mitigation strategies yield limited enhancement, which emphasizes
    the urgency and cruciality of improving risk awareness in LLM-based embodied agents
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the long-term goals of AI and robotics is to enable embodied agents to
    understand natural language instructions and perform complex tasks [[14](#bib.bib14)].
    Recent advances in large language models (LLMs) have demonstrated a profound capacity
    for understanding, reasoning, and planning leading to significant enhancements
    across various domains [[21](#bib.bib21)]. LLMs have acquired an extensive repository
    of world knowledge and task execution strategies by learning from vast amounts
    of multimodal data. Consequently, contemporary research is investigating the application
    of LLMs within the realm of robotics [[27](#bib.bib27)], positioning them as the
    brain for embodied AI systems, which equips the agents with high-level task plans,
    enabling robots to exhibit a human-like understanding and decision-making proficiency
    in task execution [[3](#bib.bib3), [19](#bib.bib19), [4](#bib.bib4), [22](#bib.bib22),
    [9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, deploying embodied agents into the real physical world carries
    potential safety hazards that could pose risks to the environment, property, and
    even human safety, dubbed as physical risks. For instance, in a kitchen scenario
    equipped solely with metal utensils, where a housekeeping robot instructed to
    “heat food with the microwave and utensils”, we empirically find that most LLMs
    fail to recognize the implicit danger that “microwaving metal can lead to the
    production of electric arcs, potentially damaging the microwave or even starting
    a fire”. As a result, these robots may formulate high-level plans with potential
    physical risks, such as “place a metal plate inside the microwave” and “turn on
    the microwave”. Therefore, prior to the practical deployment of LLM-based embodied
    agents, assessing their awareness of physical risks among the high-level task
    plans is crucial for achieving safe embodied intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Recent benchmarks have been proposed to evaluate the safety of LLMs. For example,
    SafetyBench [[28](#bib.bib28)] presents a comprehensive benchmark for evaluating
    the safety of LLMs using multiple-choice questions covering various safety concerns.
    ToolEmu [[16](#bib.bib16)] introduces a framework for scalable testing of LLM
    agents by emulating tool execution and evaluating safety risks. ASSERT [[10](#bib.bib10)]
    emphasizes the robustness of LLMs through Automated Safety ScEnario Red Teaming,
    which generates a suite of prompts to evaluate the model’s performance under various
    robustness settings. R-Judge [[26](#bib.bib26)] focuses on evaluating the behavioral
    safety of LLM agents within interactive environments by analyzing agent interaction
    records. Although existing studies have made significant strides in evaluating
    the safety and robustness of LLMs, they often overlook the specific aspect of
    physical risk awareness for embodied agents operating in diverse environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the limitations of current safety evaluations, in this paper, we
    propose RiskAwareBench, which aims to fill this gap by providing an automated
    framework for benchmarking the capability of LLM-based embodied agents to identify
    and mitigate potential physical risks in real-world environments. This framework
    comprises four key modules: safety tip generation module, scene generation module,
    plan generation module, and evaluation module. The safety tip generation module
    is responsible for generating safety tips and corresponding explanations for common
    environments. The scene generation module creates the detailed scene information
    and observation as well as natural language instructions for embodied agents based
    on the specific safety tip. The format of scene observation varies, depending
    on the type of LLM that serves as the core of the embodied agent. In the plan
    generation module, the LLM-based embodied agent generates high-level plans based
    on scene observations and instructions for guiding the downstream low-level control.
    The evaluation module includes risk evaluation and effectiveness evaluation, where
    the former evaluates whether the task planning contains implicit physical risks
    and identifies specific dangerous steps, while the latter evaluates whether the
    plans can efficiently complete the task as instructed. All the aforementioned
    modules are driven by large foundation models, enabling the automated evaluation
    of physical risk awareness for embodied agents, thereby reducing the requirement
    for manual intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing RiskAwareBench, we collect the PhysicalRisk dataset, which encompasses
    safety tips, scene observations, and robotic instructions across various scenarios.
    Moreover, we conduct extensive experiments to assess the physical risk awareness
    of LLM-based embodied agents. The baseline experiments explore the influence of
    various popular LLMs with different sizes. The findings indicate that most LLMs
    lack physical risk awareness. Furthermore, we introduced several baseline risk
    mitigation strategies to enhance the risk awareness of LLM-based embodied agents.
    The results show minimal improvement, underscoring the urgency of advancing physical
    risk awareness in embodied intelligent systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are as follows: 1) We reveal the potential physical
    risks of deploying LLM-based embodied intelligent systems in the real world. 2)
    We propose a benchmark framework that enables automatic evaluation of physical
    risk awareness of LLM-based embodied agents. 3) Based on the proposed framework,
    we construct a dataset that contains various scenarios along with risky scenes
    and instructions. 4) Extensive experiments are conducted to compare various popular
    LLMs as the high-level planner of embodied agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLMs in Embodied Task Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building on the demonstrated prowess of LLMs in intricate reasoning and contextual
    generalization, recent years have seen a growing interest in leveraging LLMs for
    embodied task planning [[25](#bib.bib25), [23](#bib.bib23)]. Brohan et al. [[3](#bib.bib3)]
    highlight the potential of combining LLMs with pretrained robotic skills to ground
    high-level instructions in real-world contexts, enabling feasible and contextually
    appropriate actions. Xie et al. [[22](#bib.bib22)] explore the use of ChatGPT
    in robotics, showing how prompt engineering and function libraries help adapt
    to various tasks, from logical reasoning to complex manipulation, primarily through
    natural language interactions. Shi et al. [[18](#bib.bib18)] introduce Robotic
    Vision-Language Planning (ViLa), which integrates perceptual data into LLM reasoning,
    enhancing understanding of spatial layouts and object attributes for better action
    planning. Zhu et al. [[29](#bib.bib29)] present Robotics with Fast and Slow Thinking
    (RFST), a dual-process framework that manages tasks requiring quick responses
    and deliberate reasoning by aligning vision-language models with policy networks.
    Shi et al. [[17](#bib.bib17)] propose OPEx, a framework dissecting core components
    of embodied instruction following tasks, emphasizing the impact of visual perception
    and low-level action execution, and enhancing performance through a multi-agent
    dialogue strategy. Despite these advancements, current research often overlooks
    physical risk awareness of LLM-based embodied agents. Our work specifically addresses
    this gap by evaluating the ability of LLM-based embodied agents to recognize and
    mitigate potential physical risks in real-world environments.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Safety Evaluation of LLM Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evaluation of safety in LLM Agents has garnered significant attention, with
    various studies proposing benchmarks and frameworks to assess different safety
    aspects of these models. SAFETEXT [[8](#bib.bib8)] highlights the susceptibility
    of state-of-the-art LLMs to generating unsafe text and their difficulty in rejecting
    unsafe advice, emphasizing the need for further research in commonsense physical
    safety. SafetyBench [[28](#bib.bib28)] provides a comprehensive benchmark for
    evaluating the safety of LLMs using multiple-choice questions across seven distinct
    safety categories. This benchmark facilitates evaluation in both Chinese and English,
    revealing substantial room for improving the safety of current LLMs despite the
    advantages shown by models like GPT-4. ToolEmu [[16](#bib.bib16)] offers a scalable
    testing framework by emulating tool execution and evaluating safety risks associated
    with LLM agents. This framework identifies potential failures and quantifies associated
    risks, providing a quantitative risk analysis of current LM agents. However, it
    primarily focuses on tool interactions rather than physical risk awareness in
    real-world scenarios. ASSERT [[10](#bib.bib10)] emphasizes the robustness of LLMs
    through Automated Safety ScEnario Red Teaming, generating prompts to evaluate
    model performance under various robustness settings. This approach provides a
    fine-grained analysis of model performance across different safety domains but
    does not specifically address the physical risks encountered by embodied agents.
    R-Judge [[26](#bib.bib26)] benchmarks the behavioral safety of LLM agents within
    interactive environments by analyzing agent interaction records. It evaluates
    the proficiency of LLMs in judging safety risks and highlights the importance
    of salient safety risk feedback. Despite its comprehensive evaluation, R-Judge
    centers on interaction records rather than proactive physical risk identification.
    While these studies have advanced LLM safety evaluations, they often overlook
    physical risk awareness for LLM-based embodied agents, which RiskAwareBench specifically
    addresses by benchmarking their ability to identify and mitigate real-world physical
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da75aa9171156e67d06584bca6a49bf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The fundamental framework of LLM-based embodied intelligence system.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 LLM-based Embodied AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLM-based embodied AI refers to the integration of Large Language Models (LLMs)
    within the realm of robotics to create embodied agents, which are capable of interacting
    with the physical environment in a more human-like manner, thanks to the advanced
    understanding and planning capabilities of LLMs. According to recent studies [[5](#bib.bib5),
    [23](#bib.bib23)], , as shown in Figure [1](#S3.F1 "Figure 1 ‣ 3 Preliminaries
    ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning
    of LLM-based Embodied Agents") a fundamental framework of an LLM-based embodied
    intelligence system consists of three key components, including perception, planning,
    and control.'
  prefs: []
  type: TYPE_NORMAL
- en: Perception.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perception serves as the cornerstone of embodied AI, tasked with environmental
    comprehension. It mirrors the human sensory system, enabling robotic entities
    to harness an array of sensors, such as cameras, lidar, and microphones, to transmute
    raw sensory inputs into digestible observations. These observations, which may
    manifest as text, images, or alternative data constructs, are then processed and
    understood by the robot, facilitating interaction with its environment.
  prefs: []
  type: TYPE_NORMAL
- en: Planning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The planning module is the cognitive core of the embodied intelligence system,
    similar to the human brain, which is responsible for orchestrating high-level
    plans based on observations made by the perception module along with the task
    instruction. This involves understanding instructions and formulating a sequence
    of actions to achieve the specified goal. Leveraging the advanced natural language
    understanding and reasoning capabilities of LLMs, the planning module effectively
    serves as the decision-maker of the robotic system, endowing robots with a human-like
    proficiency in decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Control.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The control module, functions as the robot’s executor, translating the high-level
    plans into granular, low-level controls. Analogous to the human motor system,
    it is charged with the precise calibration of the robot’s manipulative and locomotive
    parameters, such as motor speed, the positioning of the robotic arm’s end-effector,
    and the articulation of joint angles.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Physical Risk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The high-level plans generated by the planning module, while indispensable for
    guiding the robot’s actions, can inadvertently introduce physical risks if not
    meticulously managed. Physical risk refers to the propensity for the robot to
    inflict harm upon humans, property, or the environment as a consequence of executing
    these plans. This risk can emanate from a multitude of sources, including but
    not limited to, collisions with objects, mishandling of fragile items, non-standard
    operations on hazardous goods, combinations of incompatible objects, and etc.
    Given the critical implications of physical risks, this paper focuses on evaluating
    the risk awareness of LLM-based embodied agents to ensure their future safe deployment
    in real-world environments.
  prefs: []
  type: TYPE_NORMAL
- en: 4 RiskAwareBench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To comprehensively evaluate the physical risk awareness of LLM-based embodied
    agents, we introduce an automated evaluation framework, named RiskAwareBench.
    Then, with the help of RiskAwareBench, we construct a PhysicalRisk dataset, which
    contains various risky scenes and corresponding task instructions which imply
    potential physical risks. In the following sections, we will describe the framework
    of RiskAwareBench and the construction of the PhysicalRisk dataset in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Overall Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Overall Framework ‣ 4 RiskAwareBench
    ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning
    of LLM-based Embodied Agents"), the framework of RiskAwareBench consists of four
    key modules, including safety tip generation module, risky scene generation module,
    plan generation module, and evaluation module. First, the safety tip generation
    module is responsible for generating safety tips based on the risky scenes and
    task instructions. Then, the risky scene generation module creates diverse scenes
    and task instructions that may lead to potential physical risks. Afterwards, the
    plan generation module generates high-level plans to complete the instructed task
    based on the observation of risky scenes. Finally, the evaluation module evaluates
    riskiness and effectiveness of the generated plans. The detailed description of
    each module is as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e462b7b9f2d4d878b035f2f595919f27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The framework of our proposed RiskAwareBench.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Safety Tips Generation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The safety tips generation module plays a pivotal role in the RiskAwareBench
    framework by proactively identifying and communicating potential safety hazards
    to prevent physical risks in various environments. This module is inspired by
    the notion that in real life, adherence to safety standards and precautions, as
    established through expert consensus and experiential learning, significantly
    reduces the likelihood of hazards. For instance, the user manual of a microwave
    oven explicitly warns against heating metal objects, guiding users away from potential
    hazards. Drawing on this principle, the safety tip generation module is designed
    to generate comprehensive safety tips for given scenes. Our approach encompasses
    two distinct strategies for generating these safety tips: summarizing safety tips
    from existing materials and generating new safety tips. The prompts used in this
    module are presented in Appendix [A.1](#A1.SS1 "A.1 Prompts of Safety Tips Generation
    Module ‣ Appendix A Details of the RiskAwareBench ‣ RiskAwareBench: Towards Evaluating
    Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 1: Summarizing Safety Tips from Existing Materials.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recognizing the vast array of safety manuals and guidelines available online,
    the first strategy involves a two-step process. Initially, the module conducts
    a targeted search to locate relevant safety instructions pertaining to the given
    scene. Subsequently, the module extracts valuable safety tips from these documents
    based on the understanding abilities of LLMs. This process not only ensures the
    provision of established safety measures but also filters out irrelevant information,
    thereby optimizing the relevance and utility of the safety tips generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 2: Generating New Safety Tips.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second strategy is employed particularly in scenarios where existing online
    materials are sparse or insufficient. In such cases, the module uses the existing
    safety tips extracted with strategy 1 and leverages the in-context learning capabilities
    of LLMs to generate new safety tips that are tailored to the specific needs and
    contexts of the scenario at hand. This innovative approach allows for the comprehensive
    generation of safety tips, filling the gaps left by existing materials and ensuring
    comprehensive coverage of potential physical risks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Risky Scene Generation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following the generation of safety tips, it is critical to contextualize these
    precautions within environments where the physical risks they address may occur.
    The Risky scene generation module is devised to automatically create such environments,
    along with detailed instructions for embodied agents to execute tasks within these
    potentially hazardous settings. The effectiveness of a scene is gauged by the
    presence of four fundamental elements: objects, positions of objects, attributions
    of objects. Each element contributes to constructing a realistic and challenging
    scenario for the agent. In addition, this module also need to generate task instructions
    that may result in physical hazards, which are used as inputs to the agent. The
    following paragraphs elucidate the significance and composition of each element.
    The prompts used in this module are presented in Appendix [A.2](#A1.SS2 "A.2 Prompt
    of Risky Scene Generation module ‣ Appendix A Details of the RiskAwareBench ‣
    RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning
    of LLM-based Embodied Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Objects.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the core of every scene are the objects, the tangible entities that populate
    the environment. These include items directly related to the generated safety
    tips, like the microwave and metal utensils for the tip "Do not heat metal items
    in a microwave." To mimic the complexity of real-world environments and to add
    layers of challenge, objects not associated with the safety tip are also integrated,
    such as tables, cabinets, glassware, sinks, and faucets. Each object is uniquely
    identified with a name_id format (e.g., , spoon_1, spoon_2), allowing for distinction
    between identical objects within the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Positions of Objects.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The spatial arrangement of objects is a crucial aspect of scene generation,
    dictating how the objects relate to one another within the space. This is described
    using the format $<$, where the Relation is a preposition defining spatial relationships,
    like “in, above, or beside”. An example could be “microwave_1 above cabinet_1,"
    establishing a clear and structured layout of the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Attributes of Objects.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To enhance the diversity and authenticity of the scenes, attributions are assigned
    to objects. These characteristics can include material properties (e.g., , the
    chair is made of wooden), states (e.g., , the door of the microwave is opened),
    or contents (e.g., , the glass is filled with orange juice). These attributions
    not only enrich the scene’s description but also influence the decision-making
    process of the agent, as different attributes can present varying levels of risk.
  prefs: []
  type: TYPE_NORMAL
- en: Task Instructions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apart from above necessary scene information, this module is also responsible
    for generating task instructions for embodied agents to follow. The instructions
    should be described in natural language, mimicking tasks that users might perform
    in their daily lives. The instructions need be explicit and challenging to test
    decision-making skills in complex situations. Crucially, since the framework’s
    objective is to assess the agent’s risk awareness, the instructions are designed
    to potentially lead the agent to plan actions with inherent physical risks. An
    example might be “Place the metal tray inside the microwave and set the timer
    for 3 minutes on high power.” This instrustion, while straightforward, invites
    the agent to engage in an action that poses a marked safety hazard, thereby evaluating
    its risk assessment capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, this module need to produce observations about the simulated scene, which
    serve as the input for the agents’ subsequent planning process. These observations
    are carefully designed to be compatible with various types of embodied agents,
    ensuring broad applicability of the RiskAwareBench framework. We introduce two
    distinct observation modalities: textual observation and visual observation.'
  prefs: []
  type: TYPE_NORMAL
- en: Textual Observation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The textual observation synthesizes the scene’s elements into a coherent natural
    language description. This format is particularly suited for agents whose core
    is text-only LLMs. For example, a textual observation might be “In the kitchen,
    you see microwave_1 mounted above cabinet_1\. On the countertop lies spoon_1 with
    a metal bowl_1, …” This detailed narrative enables the agent to comprehend the
    scene’s complexity and identify the risks associated with the environment and
    the tasks to be performed.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Observation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To extend the framework’s utility to multimodal LLMs, visual observations are
    generated using text-to-image diffusion models. These models translate the detailed
    information of the scene into a visual context, akin to the perspective a robot’s
    camera might capture. This visual data encapsulates the scene’s spatial dynamics
    and the interplay of various elements, providing a rich visual context for agents
    that process image input.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Plan Generation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The plan generation module is a crucial component of the framework, where the
    LLM serves as the decision-maker within the emobodied agent, responsible for generating
    high-level plans from observations of the scene and task instruction. To ensure
    that the generated plans are executable for downstream low-level control, we additionally
    provide the LLM with a predefined skill set. This skill set delineates the repertoire
    of actions available to the robot, from basic locomotion to complex manipulative
    tasks such as “move to, hold on, put down, etc”. The prompts used in this module
    are presented in Appendix [A.3](#A1.SS3 "A.3 Prompts of Plan Generation Module
    ‣ Appendix A Details of the RiskAwareBench ‣ RiskAwareBench: Towards Evaluating
    Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Evaluation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The evaluation module is the critical final step in the framework, where the
    high-level plans generated by the previous module are evaluated for risk and effectiveness.
    Drawing inspiration from related works that have successfully demonstrated the
    capabilities of LLMs in achieving human-paralleled accuracy in assessing complex
    tasks. we adopts LLMs as an evaluator, which enables automatic evaluation. The
    prompts used in this module are presented in Appendix [A.4](#A1.SS4 "A.4 Prompts
    of Evaluation Module. ‣ Appendix A Details of the RiskAwareBench ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Risk Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While LLM-based embodied agents have a powerful understanding of language and
    context, may not inherently possess an awareness of physical risks. Hence, it
    is imperative to assess where these plans could lead to hazardous outcomes, especially
    considering the potential physical hazards embedded within the scenes and instructions
    through scene generation module. With carefully designed prompts, the evaluator
    can output not only whether there is a risk, but also which steps are risky.
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The effectiveness evaluation aims to assess the quality of the generated plans
    in terms of robot executableness. Due to the LLM Hallucinations, the LLM-based
    embodied agents may generate plans than include actions outside the robot’s skill
    set, resulting in execution failures, which means that the plans are ineffective.
    On the contrary, if all the plans are valid and executable according to the skill
    set, the plans are considered effective.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PhysicalRisk Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Utilizing the RiskAwareBench framework’s first two modules, we have meticulously
    compiled the PhysicalRisk dataset, which is an extensive collection of samples
    from diverse environments such as kitchen, bathroom, laboratory, factory, and
    etc. Each sample is a comprehensive unit consisting of an environment name, associated
    safety tip with an explanation, corresponding detailed scene information (including
    the list of objects, their positions, properties, and various modalities of scene
    observation), and task instruction that could violate the safety tip. The PhysicalRisk
    dataset encompasses a comprehensive collection of 4,605 samples, distributed across
    15 distinct environments. In particular, we generate a total of 307 safety tips,
    each paired with a constructed risky scene encompassing objects, positions, and
    attributes. For every risky scene, we formulate one textual observation and four
    visual observations. Furthermore, we create multiple distinct task instructions
    for each safety tip. As a result, this methodology has produced a total of 921
    textual risk scenarios and 3,684 multimodal scenarios, consequently augmenting
    the thoroughness and dependability of our evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To comprehensively evaluate the risk awareness of LLMs as high-level planners
    for embodied agents, we assessed two categories of LLMs: unimodal text-based LLMs
    and multimodal LLMs. We included LLMs of varying scales, encompassing both open-source
    and proprietary models. The unimodal text-based LLMs include gpt-3.5-turbo-1106 [[11](#bib.bib11)],
    Llama-3-8b-chat-hf and Llama-3-70b-chat-hf [[20](#bib.bib20)], Qwen1.5-7B-Chat
    and Qwen1.5-72B-Chat [[2](#bib.bib2)], Qwen2-72B-Instruct [[24](#bib.bib24)],
    Mistral-7B-Instruct-v0.1 [[6](#bib.bib6)] and Mistral-8x7B-Instruct-v0.1 [[7](#bib.bib7)].
    The multimodal LLMs include gpt-4o [[12](#bib.bib12)] and gpt-4o-mini [[13](#bib.bib13)],
    claude-3-haiku [[1](#bib.bib1)], gemini-1.5-flash [[15](#bib.bib15)]. All LLMs
    were evaluated on the text subset of the PhysicalRisk dataset, while only the
    multimodal LLMs were further assessed on the multimodal subset.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As described in Section [4.5](#S4.SS5 "4.5 Evaluation Module ‣ 4 RiskAwareBench
    ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning
    of LLM-based Embodied Agents"), the evaluation module is divided into risk evaluation
    and effectiveness evaluation. We use the Task Risk Rate (TRR) to measure the risk
    awareness of LLMs. The TRR is defined as the proportion of cases where the plans
    generated by the planner do not violate the corresponding safety tips. Additionally,
    we use the Task Effective Rate (TER) to measure the effectiveness of the plans,
    which is defined as the proportion of cases where where the planner successfully
    completes the instructed task.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the safety tip generation module, we employ gpt-4o as the LLM. In the risk
    scene generation module, gpt-4o is also utilized for the risk scene generation
    module. We leverage MidJourney as the text-to-image diffusion model to generate
    visual observations. For the evaluation module, we use gpt-4o as the evaluator.
    We predefine a skill set for embodied agents to ensure that the planner generates
    plans within this scope, thereby avoiding arbitrary actions that could affect
    the evaluation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Main results of text-based and multimodal LLMs on textual portion
    of PhysicalRisk dataset, where the scene information is represented as textual
    observation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Task Risk Rate (%) | Task Effective Rate (%) |'
  prefs: []
  type: TYPE_TB
- en: '| Open-Source LLMs | Llama-3-8b-chat-hf | 96.005 | 86.301 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3-70b-chat-hf | 95.662 | 81.735 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen1.5-7B-Chat | 95.548 | 75.799 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen1.5-72B-Chat | 94.977 | 77.968 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen2-72B-Instruct | 95.434 | 83.904 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B-Instruct-v0.1 | 98.288 | 83.219 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B-Instruct-v0.1 | 96.119 | 77.626 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 96.005 | 80.936 |'
  prefs: []
  type: TYPE_TB
- en: '| Closed-Source LLMs | gpt-3.5-turbo-1106 | 94.521 | 83.676 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4o | 93.950 | 82.763 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4o mini | 94.863 | 83.105 |'
  prefs: []
  type: TYPE_TB
- en: '| claude-3-haiku | 93.151 | 83.105 |'
  prefs: []
  type: TYPE_TB
- en: '|  | gemini-1.5-flash | 94.292 | 88.950 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average | 94.155 | 84.320 |'
  prefs: []
  type: TYPE_TB
- en: 'Tables [1](#S5.T1 "Table 1 ‣ 5.2 Main Results ‣ 5 Experiments ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents") and [2](#S5.T2 "Table 2 ‣ Visual observation is more challenging
    than textual observation. ‣ 5.2 Main Results ‣ 5 Experiments ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents") present the main results of RiskAwareBench. Based on the results,
    we derive the following key findings.'
  prefs: []
  type: TYPE_NORMAL
- en: All LLMs exhibit poor risk awareness.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5.2 Main Results ‣ 5 Experiments ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents") presents the performance of well-known unimodal text-based LLMs
    and multimodal LLMs on the textual cases of the PhysicalRisk dataset. Meanwhile,
    Table [2](#S5.T2 "Table 2 ‣ Visual observation is more challenging than textual
    observation. ‣ 5.2 Main Results ‣ 5 Experiments ‣ RiskAwareBench: Towards Evaluating
    Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents")
    displays the results of multimodal LLMs on the multimodal portion of the dataset.
    We find that the Task Risk Rate (TRR) for all LLMs exceeds 90%, indicating that
    current implementations of LLMs as decision-makers for embodied agents lack robust
    risk awareness. For instance, gpt-4o exhibited a TRR of 93.950%, while Llama-3-70b-chat-hf
    showed a TRR of 95.662%. Such high TRR values underscore the inadequate performance
    of LLMs in generating safe high-level plans.'
  prefs: []
  type: TYPE_NORMAL
- en: Bigger LLMs achieve better risk awareness in general.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We further analyze the impact of different model sizes on risk awareness. The
    results suggest that within the same model series, bigger models generally exhibit
    better risk awareness. This trend is evident when comparing models of different
    scales within the same family. For example, Llama-3-70b-chat-hf has a TRR of 95.662%,
    which is slightly lower than the 96.005% TRR of Llama-3-8b-chat-hf. Similarly,
    the Qwen1.5-72B-Chat model, with a TRR of 94.977%, outperforms its smaller counterparts
    such as Qwen1.5-7B-Chat, which has TRR of 95.548%. This finding holds true for
    the Mixtral series as well. Such results indicate that scaling laws are also valid
    for the risk awareness of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Closed-source LLMs show relatively better risk awareness than open-source LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Despite the overall poor performance of all LLMs in risk awareness, a comparison
    between closed-source and open-source LLMs reveals that closed-source models tend
    to perform slightly better than open-source LLMs. As shown in Table [1](#S5.T1
    "Table 1 ‣ 5.2 Main Results ‣ 5 Experiments ‣ RiskAwareBench: Towards Evaluating
    Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents"),
    the average TRR for closed-source LLMs is 94.155%, compared to 96.005% for open-source
    LLMs. For example, gpt-4o and Claude-3-haiku have TRRs of 93.950% and 93.151%,
    respectively, showing a slight improvement over many open-source counterparts.
    However, it is crucial to note that even the closed-source models have TRRs well
    above 90%, indicating that none of LLMs achieve satisfactory risk awareness under
    the baseline settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual observation is more challenging than textual observation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By comparing the results in Table [1](#S5.T1 "Table 1 ‣ 5.2 Main Results ‣
    5 Experiments ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness for
    High-level Planning of LLM-based Embodied Agents") and Table [2](#S5.T2 "Table
    2 ‣ Visual observation is more challenging than textual observation. ‣ 5.2 Main
    Results ‣ 5 Experiments ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness
    for High-level Planning of LLM-based Embodied Agents"), we find that, for the
    same model, the task risk rate is higher when embodied agents receive textual
    observations compared to visual observations. For example, the TPP of gpt-4o in
    Table [1](#S5.T1 "Table 1 ‣ 5.2 Main Results ‣ 5 Experiments ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents") is 93.950%, while in Table  [2](#S5.T2 "Table 2 ‣ Visual observation
    is more challenging than textual observation. ‣ 5.2 Main Results ‣ 5 Experiments
    ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning
    of LLM-based Embodied Agents"), the TRR is 97.516%. Similar results can also be
    found for claude-3-haiku and gemini-1.5-flash.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, such findings underscore the challenges and potential directions
    for improving the risk awareness of LLM-based planners under embodied settings.
    While larger and closed-source models show relative improvements, there is a clear
    need for further research and development to enhance the safety and effectiveness
    of these models in high-level planning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Main results of multimodal LLMs on multimodal portion of PhysicalRisk
    dataset, where the scene information is represented as visual observation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Task Risk Rate (%) | Task Effective Rate (%) |'
  prefs: []
  type: TYPE_TB
- en: '| Closed-source LLMs | gpt-4o | 97.516 | 82.734 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4o mini | 95.748 | 84.047 |'
  prefs: []
  type: TYPE_TB
- en: '| claude-3-haiku | 96.832 | 87.003 |'
  prefs: []
  type: TYPE_TB
- en: '| gemini-1.5-flash | 97.995 | 89.716 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Average | 97.023 | 85.875 |'
  prefs: []
  type: TYPE_TB
- en: 5.3 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3](#S5.F3 "Figure 3 ‣ 5.3 Case Study ‣ 5 Experiments ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents") showcases examples of safety tips, scene information, task instructions,
    and observations generated for a kitchen environment. Constructing such detailed
    and contextually accurate scene information manually is a challenging and time-consuming
    task. However, leveraging carefully designed prompts, LLMs can accurately generate
    diverse and detailed scene information automatically, which significantly enhances
    the efficiency and scalability of creating realistic and varied environments for
    evaluating risk awareness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5.3 Case Study ‣ 5 Experiments ‣ RiskAwareBench:
    Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based
    Embodied Agents") shows high-level plans generated by LLM-based embodied agents
    under three different settings. The left image shows plans without any risk mitigation
    strategy. Notably, steps 10-13 involve a risk operation: heating eggs in a microwave.
    If executed in the real world, these steps could pose significant risks. The middle
    image shows plans with an implicit risk mitigation strategy, which still includes
    risk steps. The right one, however, shows plans with an explicit risk mitigation
    strategy, where the LLM provides a warning to the user. These examples highlight
    the weak risk awareness of LLM-based embodied agents and underscores the necessity
    of advanced strategies to mitigate potential risks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1041b0ecc00d4e5a9ac3e9437d32014d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of safety tips, scene information, task instructions, and
    observations of the environment generated for the kitchen environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b53cdf77a87cde357db48dddb0f39fac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Examples of high-level plans generated by LLM-based embodied agents
    under different settings: 1) Left: without risk mitigation strategy; 2) Middle:
    with implicit risk mitigation strategy; Right: with explicit risk mitigation strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Risk Mitigation Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To enhance the risk awareness of LLM-based embodied agents, we propose two
    fundamental prompt-based risk mitigation strategies: the implicit risk mitigation
    strategy (RM-Implicit)and the explicit risk mitigation (RM-Explicit) strategy.
    Given the robust instruction comprehension capabilities of LLMs, these strategies
    aim to incorporate additional safety constraints into the input prompts to guide
    the model in generating risk-free plans. Below, we detail each strategy and present
    the experimental results.'
  prefs: []
  type: TYPE_NORMAL
- en: Implicit risk mitigation strategy (RM-Implicit).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the RM-Implicit strategy, we incorporate general safety guidelines into
    the prompt to implicitly remind the model to consider potential risks when generating
    plans. The added prompt is structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '“When generating the plans, you should consider the safety of the plans to
    avoid any potential risks or hazards in the real world. The risks include object
    damage, human injury, environmental damage, generation of toxic substances and
    pollutants, or any other negative consequences that may arise from the actions
    performed by the robot. The risks may come from various factors, including but
    not limited to collisions with objects, improper handling of fragile objects,
    unsafe movements in dynamic environments, non-standard operations on objects,
    or combinations of incompatible objects. When you think one step may cause a potential
    risk, you should avoid generating such a step in the plans and add "Warning: "
    at the beginning of the step to point out the risk.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This strategy leverages the model’s inherent understanding of general safety
    principles to mitigate risks without specifying detailed instructions for each
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit risk mitigation strategy (RM-Explicit).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contrary to the RM-Implicit strategy, the RM-Explicit strategy involves incorporating
    more detailed and scenario-specific safety rules into the prompt, explicitly requiring
    the model to consider these risks. Specifically, we directly include the safety
    tips generated by the safety tip module for the current scenario into the prompt,
    ensuring that the model adheres to these safety tips when generating plans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Task risk rates (TRR %) of gpt-3.5-turbo-1106 and gpt-4o on textual
    portion of PhysicalRisk dataset with different risk mitigation strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | gpt-3.5-turbo-1106 | gpt-4o |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 94.521 | 93.95 |'
  prefs: []
  type: TYPE_TB
- en: '| RM-Implicit | 79.365 | 49.206 |'
  prefs: []
  type: TYPE_TB
- en: '| RM-Explicit | 90.651 | 43.915 |'
  prefs: []
  type: TYPE_TB
- en: Mitigation results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [3](#S6.T3 "Table 3 ‣ Explicit risk mitigation strategy (RM-Explicit).
    ‣ 6 Risk Mitigation Strategies ‣ RiskAwareBench: Towards Evaluating Physical Risk
    Awareness for High-level Planning of LLM-based Embodied Agents") presents the
    task risk rate of different risk mitigation strategies in comparison to the baseline.
    The results reveal significant variations in the effectiveness of different risk
    mitigation strategies across various LLMs. For gpt-4o, both RM-Implicit and RM-Explicit
    strategies markedly enhance the model’s risk awareness, with TRR values of 49.206%
    and 43.915%, respectively. This improvement can be attributed to the superior
    instruction comprehension capabilities of gpt-4o, which allows it to better integrate
    and act upon the provided safety guidelines. In contrast, for the less advanced
    gpt-3.5-turbo-1106, the instruction comprehension ability is limited, resulting
    in inconsistent improvements in risk awareness using these prompt-based strategies.
    The RM-Implicit strategy achieves a TRR of 79.365%, whereas the RM-Explicit strategy
    results in a TRR of 90.651%. These findings suggest that while prompt-based strategies
    can be beneficial, their effectiveness is contingent upon the underlying model’s
    capacity to interpret and follow complex instructions. Overall, these strategies
    provide limited enhancement, underscoring the importance of developing more effective
    Risk Mitigation Strategies in the future. Further research is necessary to devise
    advanced methods that can consistently improve the risk awareness of LLM-based
    embodied agents across different model architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose RiskAwareBench, an innovative framework designed to
    evaluate the physical risk awareness of LLM-based embodied AI system. Through
    the creation of the PhysicalRisk dataset and a series of comprehensive experiments,
    we have demonstrated that current LLMs, despite their advanced language processing
    capabilities, often lack the necessary risk awareness to operate safely in real-world
    environments. The high Task Risk Rates (TRR) across various models, both open-source
    and closed-source, highlight a significant gap in safety that must be addressed.
    Moreover, the risk mitigation strategies proposed in this paper represent initial
    steps towards enhancing safety. However, the limited improvements observed suggest
    that these strategies are not sufficiently robust to address the complex risks
    inherent in physical environments. This calls for more sophisticated and context-aware
    approaches that can dynamically adapt to the nuances of different scenarios. As
    LLMs merge with robotics, safety must be prioritized to ensure that technological
    advancement does not come at the expense of human well-being or environmental
    integrity. The RiskAwareBench framework and the associated dataset provide a valuable
    tool for the AI and robotics community to further explore and enhance the safety
    of LLM-based embodied AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic [2024] Anthropic. Introducing the next generation of claude, 2024.
    URL [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2023] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. *arXiv
    preprint arXiv:2309.16609*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brohan et al. [2023] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    et al. Do as i can, not as i say: Grounding language in robotic affordances. In
    *Conference on robot learning*, pp.  287–318\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In *International
    Conference on Machine Learning*, pp.  8469–8488\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2023] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look
    before you leap: Unveiling the power of gpt-4v in robotic vision-language planning.
    *arXiv preprint arXiv:2311.17842*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levy et al. [2022] Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia Chilton,
    Desmond Patton, Kathleen Mckeown, and William Yang Wang. Safetext: A benchmark
    for exploring physical safety in language models. In *Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing*, pp.  2407–2421,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lv et al. [2024] Qi Lv, Hao Li, Xiang Deng, Rui Shao, Michael Yu Wang, and
    Liqiang Nie. Robomp$2$: A robotic multimodal perception-planning framework with
    mutlimodal large language models. In *International Conference on Machine Learning*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mei et al. [2023] Alex Mei, Sharon Levy, and William Wang. Assert: Automated
    safety scenario red teaming for evaluating the robustness of large language models.
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pp. 
    5831–5847, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-3.5-turbo, 2023. URL [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2024a] OpenAI. Hello gpt-4o, 2024a. URL [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI [2024b] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence,
    2024b. URL [https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajan & Saffiotti [2017] Kanna Rajan and Alessandro Saffiotti. Towards a science
    of integrated ai and robotics, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reid et al. [2024] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruan et al. [2023] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao
    Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. Identifying
    the risks of lm agents with an lm-emulated sandbox. In *The Twelfth International
    Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2024a] Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre Côté,
    and Bang Liu. Opex: A component-wise analysis of llm-centric agents in embodied
    instruction following. *arXiv preprint arXiv:2403.03017*, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. [2024b] Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon Shaolei Du, and Huazhe
    Xu. Unleashing the power of pre-trained language models for offline reinforcement
    learning. In *The Twelfth International Conference on Learning Representations*,
    2024b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szot et al. [2023] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure,
    Rin Metcalf, Walter Talbott, Natalie Mackraz, R Devon Hjelm, and Alexander T Toshev.
    Large language models as generalizable policies for embodied tasks. In *The Twelfth
    International Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2023] Bing Xie, Xiangming Xi, Xinan Zhao, Yuhan Wang, Wei Song,
    Jianjun Gu, and Shiqiang Zhu. Chatgpt for robotics: A new approach to human-robot
    interaction and task planning. In *International Conference on Intelligent Robotics
    and Applications*, pp.  365–376\. Springer, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2024] Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping
    Che, and Jian Tang. A survey on robotics with foundation models: toward embodied
    ai. *arXiv preprint arXiv:2402.02385*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2024] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
    Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical
    report. *arXiv preprint arXiv:2407.10671*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2023] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel,
    and Dale Schuurmans. Foundation models for decision making: Problems, methods,
    and opportunities. *arXiv preprint arXiv:2303.04129*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2024] Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie
    Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, et al. R-judge:
    Benchmarking safety risk awareness for llm agents. *arXiv preprint arXiv:2401.10019*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. [2023] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and
    Philip S Yu. Large language models for robotics: A survey. *arXiv preprint arXiv:2311.07226*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang,
    Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating
    the safety of large language models with multiple choice questions. *arXiv preprint
    arXiv:2309.07045*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2024] Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan Xu,
    Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, et al. Language-conditioned
    robotic manipulation with fast and slow thinking. *arXiv preprint arXiv:2401.04181*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Details of the RiskAwareBench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Prompts of Safety Tips Generation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [A.4](#A1.SS4 "A.4 Prompts of Evaluation Module. ‣ Appendix A Details
    of the RiskAwareBench ‣ RiskAwareBench: Towards Evaluating Physical Risk Awareness
    for High-level Planning of LLM-based Embodied Agents") shows the prompt used for
    summarizing safety tips from existing materials and Table LABEL:tab:prompt_generate_safety_tip
    shows the prompt used for generating new safety tips.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompt of Risky Scene Generation module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table LABEL:tab:prompt_generate_scene shows the prompt used for generating risky
    scene information.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Prompts of Plan Generation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table LABEL:tab:prompt_plan_generation shows the prompt used for generating
    high-level plans with different LLMs and Table LABEL:tab:skill_set presents the
    pre-defined skill set of robots.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Prompts of Evaluation Module.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table LABEL:tab:prompt_evaluation shows the prompt used for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The prompt used for summarizing safety tips from existing materials,
    where the content wrapped in { } are placeholders that need to be replaced with
    specific content.'
  prefs: []
  type: TYPE_NORMAL
