- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:47:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A Unified Debugging Approach via LLM-Based Multi-Agent Synergy
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过基于LLM的多智能体协作的统一调试方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17153](https://ar5iv.labs.arxiv.org/html/2404.17153)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17153](https://ar5iv.labs.arxiv.org/html/2404.17153)
- en: 'Cheryl Lee1, Chunqiu Steven Xia2, Jen-tse Huang1, Zhouruixin Zhu3, Lingming
    Zhang2, and Michael R. Lyu1 1The Chinese University of Hong Kong. Email: cheryllee@link.cuhk.edu.hk,
    {jthuang, lyu}@cse.cuhk.edu.hk 2University of Illinois Urbana-Champaign. Email:
    {chunqiu2, lingming}@illinois.edu 3The Chinese University of Hong Kong, Shenzhen.
    Email: zhouruixingzhu@link.cuhk.edu.cn'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Cheryl Lee1、Chunqiu Steven Xia2、Jen-tse Huang1、Zhouruixin Zhu3、Lingming Zhang2
    和 Michael R. Lyu1 1香港中文大学。邮箱：cheryllee@link.cuhk.edu.hk，{jthuang, lyu}@cse.cuhk.edu.hk
    2伊利诺伊大学厄本那-香槟分校。邮箱：{chunqiu2, lingming}@illinois.edu 3香港中文大学（深圳）。邮箱：zhouruixingzhu@link.cuhk.edu.cn
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Tremendous efforts have been devoted to automating software debugging, a time-consuming
    process involving fault localization and repair generation. Recently, Large Language
    Models (LLMs) have shown great potential in automated debugging. However, we identified
    three challenges posed to traditional and LLM-based debugging tools: 1) the upstream
    imperfection of fault localization affects the downstream repair, 2) the deficiency
    in handling complex logic errors, and 3) the ignorance of program contexts. In
    this context, we propose the first automated, unified debugging framework, FixAgent,
    via LLM agent synergy. FixAgent can perform end-to-end localization, repair, and
    analysis of bugs. Our insight is that LLMs can benefit from general software engineering
    principles recognized by human developers in debugging, such as rubber duck debugging,
    enabling a better understanding of program functionality and logic bugs. Hence,
    we create three designs inspired by rubber ducking to address these challenges.
    They are LLM agent specialization and synergy, key variable tracking, and program
    context comprehension, which request LLMs to provide explicit explanations and
    force them to focus on crucial program logic information. Experiments on the widely
    used dataset QuixBugs show that FixAgent correctly fixes 79 out of 80 bugs, 9
    of which have never been fixed. It also plausibly patches 1.9X more defects than
    the best-performing repair tool on Codeflaws, even with no bug location information
    and fewer than 0.6% sampling times. On average, FixAgent increases about 20% plausible
    and correct fixes compared to its base model using different LLMs, showing the
    effectiveness of our designs. Moreover, the correctness rate of FixAgent reaches
    remarkably 97.26%, indicating that FixAgent can potentially overcome the overfitting
    issue of the existing approaches.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化软件调试一直是一个耗时的过程，涉及故障定位和修复生成。最近，大型语言模型（LLMs）在自动化调试中显示出巨大的潜力。然而，我们发现了传统和基于LLM的调试工具面临的三个挑战：1）故障定位的上游不完美影响下游修复，2）处理复杂逻辑错误的不足，3）忽视程序上下文。在这种情况下，我们提出了第一个自动化的统一调试框架FixAgent，通过LLM智能体协作实现。FixAgent可以执行端到端的故障定位、修复和分析。我们的见解是，LLMs可以从人类开发者在调试中认识到的一般软件工程原则中受益，例如橡皮鸭调试，从而更好地理解程序功能和逻辑错误。因此，我们创建了三个受橡皮鸭调试启发的设计来解决这些挑战。它们是LLM智能体专门化和协作、关键变量跟踪以及程序上下文理解，这些设计要求LLMs提供明确的解释，并强制其关注关键的程序逻辑信息。对广泛使用的数据集QuixBugs的实验表明，FixAgent正确修复了80个错误中的79个，其中9个从未被修复过。它还可以比Codeflaws上的最佳修复工具修复多出1.9倍的缺陷，即使没有错误定位信息和少于0.6%的采样次数。与其基础模型使用不同LLMs相比，FixAgent平均增加了约20%的合理且正确的修复，显示了我们设计的有效性。此外，FixAgent的正确率达到了显著的97.26%，表明FixAgent有可能克服现有方法的过拟合问题。
- en: I Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In an era where software systems are ubiquitous, permeating every facet of modern
    life, the inevitability of software bugs is a stark reality. These bugs, far from
    being mere nuisances, have the potential to cause catastrophic failures [[1](#bib.bib1)].
    Identifying and rectifying these bugs falls upon developers, who must be ensnared
    in the time-consuming and complex process of debugging [[2](#bib.bib2)]. It is
    reported that developers usually spend over 50% of their programming time on debugging,
    and the cost of debugging amounts to billions of dollars per year [[3](#bib.bib3)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个软件系统无处不在、渗透到现代生活各个方面的时代，软件缺陷的不可避免性是一个严峻的现实。这些缺陷不仅仅是简单的麻烦，它们有可能导致灾难性的失败 [[1](#bib.bib1)]。识别和修复这些缺陷的责任落在开发者身上，他们必须陷入耗时且复杂的调试过程 [[2](#bib.bib2)]。有报告指出，开发者通常将超过50%的编程时间花费在调试上，调试的成本每年高达数十亿美元 [[3](#bib.bib3)]。
- en: 'The debugging demand calls for automated tools to relieve the manual burden.
    Automated debugging generally consists of two sequent stages: Fault Localization
    (FL) and Automated Program Repair (APR). FL aims to identify precise buggy statements
    and provide a ranked list of suspicious code lines, often the first step in debugging.
    Classic FL analyzes test outcomes to localize faults, using either statistical
    analysis [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] or mutation analysis [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)] to qualify the faulty likelihood of code statements.
    Its effectiveness relies highly on human-written test cases and is thus variable.
    On the other hand, APR attempts to generate correct patches to replace faulty
    code segments. Traditional APR explores the space of possible patches [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)] built by pre-defined fix patterns or synthesizes
    patches via symbolic execution [[13](#bib.bib13), [14](#bib.bib14)] based on human-written
    test cases. However, the search spaces may only contain very few correct patches [[15](#bib.bib15)],
    and such methods require extensive customization to re-implement fix patterns
    when transformed across different programming languages. Learning-based techniques
    have shown promise in both areas. Learning-based FL [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)] models program behavior from source code,
    execution features, and test outcomes to localize bugs. Learning-based APR [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)] often “translates” buggy code snippets into
    fixes via neural machine translation (NMT), despite its heavy reliance on high-quality
    bug-fix pairs for training or fine-tuning [[23](#bib.bib23)]. Large Language Models
    (LLMs) have been regarded as the most effective learning models for coding-related
    tasks, including debugging. A recent study [[24](#bib.bib24)] shows that directly
    applying LLMs can significantly outperform advanced APR techniques. Other LLM-based
    FL [[19](#bib.bib19)] and APR studies [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] also obtain promising results.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 调试需求呼吁自动化工具来减轻手动负担。自动化调试通常包括两个连续的阶段：故障定位（FL）和自动程序修复（APR）。FL旨在识别精确的错误语句，并提供可疑代码行的排序列表，这通常是调试的第一步。经典的FL分析测试结果以定位故障，使用统计分析 [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)] 或变异分析 [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]
    来评估代码语句的故障可能性。其有效性高度依赖于人工编写的测试用例，因此具有一定的变数。另一方面，APR试图生成正确的修补程序以替代故障的代码片段。传统的APR探索通过预定义修复模式建立的可能修补程序空间 [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)] 或通过符号执行 [[13](#bib.bib13), [14](#bib.bib14)]
    合成修补程序，这些方法基于人工编写的测试用例。然而，搜索空间可能只包含很少的正确修补程序 [[15](#bib.bib15)]，而且这些方法在不同编程语言之间转换时需要大量定制以重新实现修复模式。基于学习的技术在这两个领域都显示出了前景。基于学习的FL [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)] 从源代码、执行特征和测试结果中建模程序行为，以定位缺陷。基于学习的APR [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)] 通常通过神经机器翻译（NMT）将有缺陷的代码片段“翻译”为修复，尽管它高度依赖于高质量的错误修复对进行训练或微调 [[23](#bib.bib23)]。大型语言模型（LLMs）被认为是最有效的学习模型，用于编码相关任务，包括调试。最近的一项研究 [[24](#bib.bib24)]
    显示，直接应用LLMs可以显著超越先进的APR技术。其他基于LLMs的FL [[19](#bib.bib19)] 和APR研究 [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)] 也取得了令人鼓舞的结果。
- en: 'However, both traditional and LLM-based debugging tools still face three main
    challenges: 1) Imperfect fault localization Previous studies assume off-the-shelf
    FL tools perfectly identify bug locations, so APR should only patch the suspicious
    code statements. Yet, existing FL techniques show limited effectiveness in practice [[29](#bib.bib29),
    [30](#bib.bib30)], and the performance of APR could be largely biased by FL results [[31](#bib.bib31)].
    2) Struggling with complex logic bugs. Though LLMs have shown human-like logic
    understanding abilities [[32](#bib.bib32)], they still struggle to repair complex
    logic errors, as debugging is a multi-step reasoning process, which is challenging
    for models that rely on pattern recognition rather than genuine thinking. When
    the program structure is complex or poorly documented, LLMs may perform even worse
    than small models [[33](#bib.bib33)]. 3) Context ignorance. Debugging demands
    understanding both the purpose and the broader operational context of a program,
    including intended outcomes and potential side effects. Most LLMs are trained
    on file-level source code, lacking the ability to analyze dependencies. Plus,
    LLM-based APR considers the code and test outcomes only, ignoring the informative
    program contexts, such as variable scopes, function definitions, and external
    libraries, limiting the debugging capability of LLMs.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论是传统调试工具还是基于 LLM 的调试工具仍然面临三个主要挑战：1) 不完美的故障定位。以往研究假设现成的 FL 工具能够完美地识别错误位置，因此
    APR 应仅修补可疑的代码语句。然而，现有的 FL 技术在实践中效果有限 [[29](#bib.bib29), [30](#bib.bib30)]，APR
    的性能可能会受到 FL 结果的较大偏差 [[31](#bib.bib31)]。2) 处理复杂逻辑错误的困难。尽管 LLM 已展示出类似人类的逻辑理解能力 [[32](#bib.bib32)]，但它们仍难以修复复杂的逻辑错误，因为调试是一个多步骤推理过程，这对依赖模式识别而非真正思考的模型来说是一个挑战。当程序结构复杂或文档不足时，LLMs
    可能表现得甚至比小模型更差 [[33](#bib.bib33)]。3) 上下文忽视。调试需要理解程序的目的和更广泛的操作上下文，包括预期结果和潜在副作用。大多数
    LLM 是在文件级源代码上训练的，缺乏分析依赖关系的能力。此外，基于 LLM 的 APR 仅考虑代码和测试结果，忽略了信息丰富的程序上下文，如变量范围、函数定义和外部库，限制了
    LLM 的调试能力。
- en: 'Our insight: LLMs closely mimic developers when performing coding-related tasks,
    so they can benefit from general software engineering principles. We adopt the
    principle of rubber duck debugging (or rubber ducking), a debugging method where
    developers articulate their code in spoken natural languages, often line by line.
    Inspired by rubber ducking, we propose FixAgent, the first unified, automated
    debugging framework via LLM multi-agent synergy.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的见解：LLMs 在执行编码相关任务时 closely mimic 开发者，因此它们可以从通用的软件工程原则中受益。我们采用了橡皮鸭调试原则（或橡皮鸭调试），这是一种开发者通过口头自然语言表达其代码的调试方法，通常是一行行地进行。受到橡皮鸭调试的启发，我们提出了
    FixAgent，这是第一个通过 LLM 多代理协同实现的统一自动化调试框架。
- en: 'Specifically, we create three main designs to address the above challenges:
    1) Specialized agent synergy. We first specialize two LLM agents to serve as a
    bug localizer and program repairer, respectively, to complete multi-stage debugging,
    followed by an extra LLM agent to analyze the bug-repair pair. Each agent explains
    its work to a “rubber duck” in detail. Their synergy delivers program repairs
    with explanations without any prior bug locations. In addition, the repairer may
    correct the mistakes made by the localizer, i.e.,, patching code statements beyond
    those identified by the localizer. If a plausible patch is generated and the changed
    code elements are different from the localization, FixAgent will adjust the localization
    results. Moreover, explaining to a rubber duck separately guides the agents in
    focusing on a specific task and better assists developers since over 85% of developers
    want to know the rationale behind automated debugging [[34](#bib.bib34)]. 2) Intermediate
    variable tracking. We prompt each agent to explicitly track key variables at critical
    points in the buggy program and discuss how such tracking guides their task completion.
    This strategy forces agents to analyze the code along the logic execution paths
    and provide more bug-oriented explanations. We do not require a line-by-line explanation
    as the original rubber ducking does because a buggy program (with its context)
    can be very long, which may degrade the performance of LLMs or even extend the
    window length that LLMs can handle  [[35](#bib.bib35)]. 3) Program context construction.
    We construct the program context with respect to its specifications and dependencies.
    The context is provided along with the buggy program to FixAgent. Program specifications
    can include a functionality description, input/output format or examples, variable
    scopes, etc. Afterward, we parse the dependencies between files inside a repository
    to align with real-world projects. These two parts consist of the context. We
    also encourage agents to analyze the buggy program against the given context to
    obtain more attention to the context.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们创建了三个主要设计来应对上述挑战：1）专业化代理协同。我们首先将两个LLM代理专业化，分别作为bug定位器和程序修复器，以完成多阶段调试，之后再加入一个额外的LLM代理来分析bug-修复对。每个代理将其工作详细解释给“橡皮鸭”听。他们的协同能够在没有任何先前bug位置的情况下提供带解释的程序修复。此外，修复器可能会纠正定位器所犯的错误，即修补定位器未识别的代码语句。如果生成了一个合理的修补程序且更改的代码元素与定位不同，FixAgent将调整定位结果。此外，分别向橡皮鸭解释有助于代理集中于特定任务，并更好地协助开发者，因为超过85%的开发者希望了解自动调试背后的原理[[34](#bib.bib34)]。2）中间变量追踪。我们提示每个代理在有缺陷的程序的关键点明确追踪关键变量，并讨论这种追踪如何指导他们完成任务。这一策略迫使代理沿逻辑执行路径分析代码并提供更多面向bug的解释。我们不要求逐行解释，因为有缺陷的程序（及其上下文）可能非常长，这可能会降低LLM的性能甚至扩展LLM能处理的窗口长度[[35](#bib.bib35)]。3）程序上下文构建。我们根据程序的规格和依赖关系构建程序上下文。上下文与有缺陷的程序一起提供给FixAgent。程序规格可以包括功能描述、输入/输出格式或示例、变量作用域等。之后，我们解析存储库中文件之间的依赖关系，以与实际项目对齐。这两个部分组成了上下文。我们还鼓励代理根据给定的上下文分析有缺陷的程序，以便更多关注上下文。
- en: Experimental results demonstrate the superiority of FixAgent. We compare FixAgent
    against 16 baselines, including 10 state-of-the-art APR tools and 6 base LLMs.
    The comparison experiments are conducted on two widely used datasets, QuixBugs [[36](#bib.bib36)]
    and Codeflaws [[37](#bib.bib37)], written in three programming languages (C, Python,
    and Java). Overall, FixAgent patches 2780 bugs (passing all test cases) out of
    3982 defects on these two datasets, with an estimated correctness rate of 96.5%
    on average. It outperforms all baselines significantly, even without any prior
    bug locations, whereas previous APR tools use ground-truth bug location information.
    For QuixBugs, it correctly fixes 79 out of 80 real-world bugs, including 10 bugs
    that have not been fixed before. FixAgent fixes 24 more bugs than the best-performing
    APR baseline (AlphaRepair). Plus, FixAgent patches 586 defects than the best LLM
    competitor (GPT4) on Codeflaws. We also conduct extensive ablation studies on
    Codeflaws and a recently collected dataset, ConDefects (Python and Java), to mitigate
    the threat of data leakage in LLM training. FixAgent can fix 368 bugs out of 600
    sampled bugs in ConDefects, with 375 plausibly patched. Results show that FixAgent
    performs well using various LLMs, not limited to its original setting (GPT4),
    and makes remarkable improvements compared to its base model. The studies further
    demonstrate that each design contributes to FixAgent positively.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果展示了FixAgent的优越性。我们将FixAgent与16个基线方法进行了比较，包括10个最先进的APR工具和6个基础LLM。比较实验在两个广泛使用的数据集QuixBugs
    [[36](#bib.bib36)]和Codeflaws [[37](#bib.bib37)]上进行，这些数据集使用了三种编程语言（C、Python和Java）。总体而言，FixAgent在这两个数据集中修复了2780个错误（通过所有测试用例），在3982个缺陷中，平均估计正确率为96.5%。它显著优于所有基线方法，即使没有任何先前的错误位置，而之前的APR工具使用了真实的错误位置数据。对于QuixBugs，它正确修复了80个实际世界中的79个错误，包括10个以前未被修复的错误。FixAgent比表现最佳的APR基线（AlphaRepair）多修复了24个错误。此外，FixAgent在Codeflaws上修复了比最佳LLM竞争对手（GPT4）多586个缺陷。我们还在Codeflaws和最近收集的数据集ConDefects（Python和Java）上进行了广泛的消融研究，以减少LLM训练中的数据泄露威胁。FixAgent可以修复ConDefects中600个采样错误中的368个，375个补丁合理。结果表明，FixAgent使用各种LLM表现良好，不仅限于其原始设置（GPT4），并且相较于其基础模型有显著改进。这些研究进一步表明每个设计对FixAgent的贡献都是积极的。
- en: 'Our main contributions are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direction: We are the first to propose that LLMs can benefit from a software
    engineering principle, rubber ducking, to enhance debugging performance.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方向：我们是第一个提出LLM可以从软件工程原理橡皮鸭调试法中受益，以增强调试性能的团队。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Approach: We designed the first automated, unified debugging framework based
    on LLM multi-agent synergy, inspired by rubber ducking. Our method can effectively
    produce repairs and explanations without knowing bug locations. The implementation
    is released at [[38](#bib.bib38)].'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法：我们设计了第一个基于LLM多代理协同的自动化统一调试框架，灵感来源于橡皮鸭调试法。我们的方法可以在不知晓错误位置的情况下有效地生成修复和解释。实现已发布于
    [[38](#bib.bib38)]。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evalution: We conduct extensive experiments to evaluate FixAgent. The results
    show that FixAgent can significantly outperform baselines on widely used datasets.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估：我们进行了大量实验来评估FixAgent。结果显示FixAgent在广泛使用的数据集上显著优于基线方法。
- en: II Background
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A Terminology
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 术语
- en: Software debugging is the process of tracking and fixing issues, such as bugs
    and vulnerabilities, where revising the source code is essential. Code debugging
    involves Fault Localization (FL), Automated Program Repair (APR), and possible
    post-error review. FL attempts to precisely identify buggy elements within a faulty
    program,through static or dynamic analysis. It calculates the probability of each
    code element being buggy to automatically produce a ranked list of suspicious
    code elements. Using this ranked list, APR then automatically patches the identified
    buggy code segments, consisting of patch generation and patch verification. A
    plausible patch can pass all human-written test cases, and it is correct when
    manually verified by developers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 软件调试是跟踪和修复问题（如漏洞和缺陷）的过程，其中修订源代码是必不可少的。代码调试包括故障定位（FL）、自动程序修复（APR）和可能的错误后审查。FL尝试通过静态或动态分析精确识别故障程序中的错误元素。它计算每个代码元素是错误的概率，以自动生成可疑代码元素的排名列表。APR使用这个排名列表自动修补识别出的错误代码段，包括补丁生成和补丁验证。一个合理的补丁可以通过所有人工编写的测试用例，并且在由开发人员手动验证时是正确的。
- en: Unified debugging [[39](#bib.bib39)] is a pioneering work that aims to better
    combine FL and APR. It leverages repair information to improve FL, believing that
    if a patch passes originally failing cases, its patch locations should be highly
    correlated with the groundtruth bug locations. Unified debugging highlights the
    important connection between APR and FL, thereby applying to our work. Differently,
    we aim to provide fixed programs using an end-to-end solution, where the FL and
    APR have an interactive but not determinative relationship. Such an architecture
    allows patching code elements beyond those localized by FL, and the FL results
    can also be adjusted based on repairs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 统一调试 [[39](#bib.bib39)] 是一项开创性的工作，旨在更好地结合 FL 和 APR。它利用修复信息来改进 FL，认为如果一个补丁通过了原本失败的测试案例，那么其补丁位置应该与实际的
    bug 位置高度相关。统一调试突出了 APR 和 FL 之间的重要联系，从而应用于我们的工作。不同的是，我们旨在通过端到端解决方案提供修复程序，其中 FL
    和 APR 具有互动但非决定性的关系。这种架构允许修补 FL 定位之外的代码元素，同时 FL 结果也可以根据修复进行调整。
- en: Rubber duck debugging (aka. rubber ducking) is a debugging method that forces
    developers to explain their code by speaking out their expectations and the real
    implementation to find the gap. The original rubber ducking requires explaining
    the code line by line, but following the essential idea—breaking code into pieces
    and articulating them in natural languages—can be beneficial already.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 橡皮鸭调试（也称为橡皮鸭子调试）是一种调试方法，要求开发人员通过口述他们的期望和实际实现来解释代码，以发现差距。原始的橡皮鸭调试要求逐行解释代码，但遵循核心思想——将代码分解并用自然语言表达——已经是有益的。
- en: II-B Large Language Models
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 大型语言模型
- en: Large Language Models (LLMs) have attained significant advancements in natural
    language processing, including text generation, conversational engagement, and
    logical reasoning [[40](#bib.bib40)]. LLMs is trained to predict tokens auto-regressively
    within a given textual context. This paradigm facilitates the unsupervised training
    on massive corpora of text sourced from the internet, removing the need for labeled
    datasets. In light of the remarkable success of general-purpose LLMs, Code LLMs
    have been extensively studied as code generation is exactly like text generation.
    These models are trained on code corpus (perhaps containing related natural languages).
    For example, DeepSeek-Coder [[41](#bib.bib41)] is trained on 2 trillion tokens
    crawled from GitHub and StackExchange, where 87% are code and 10% are code-related
    text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言处理方面取得了显著进展，包括文本生成、对话互动和逻辑推理[[40](#bib.bib40)]。LLMs 被训练以在给定的文本上下文中自回归地预测标记。这一范式支持在来自互联网的大规模文本语料库上进行无监督训练，从而不需要标记数据集。鉴于通用
    LLMs 的显著成功，代码 LLMs 也得到了广泛研究，因为代码生成就像文本生成一样。这些模型是在代码语料库上进行训练的（可能包含相关的自然语言）。例如，DeepSeek-Coder
    [[41](#bib.bib41)] 是在从 GitHub 和 StackExchange 爬取的 2 万亿标记上进行训练的，其中 87% 是代码，10%
    是代码相关文本。
- en: LLMs are typically used with a prompt–an instruction to an LLM, initializing
    the LLM to perform inference and generate text until the model encounters a predetermined
    stop word or surpasses its designated maximum word limit. Through the deliberate
    construction of prompts, i.e., prompt engineering, researchers have harnessed
    the capabilities of models for a myriad of tasks without necessitating retraining
    or fine-tuning. In this paper, we adopt GPT4 [[42](#bib.bib42)] for automated
    debugging via prompt engineering. GPT4 is a state-of-the-art LLM with advanced
    understanding and reasoning capabilities in both natural languages and code.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 通常与提示一起使用——对 LLM 的指令，初始化 LLM 执行推理并生成文本，直到模型遇到预定的停止词或超出其指定的最大字数限制。通过精心构造提示，即提示工程，研究人员已经利用模型的能力来完成各种任务，而无需重新训练或微调。在本文中，我们采用
    GPT4 [[42](#bib.bib42)] 通过提示工程进行自动化调试。GPT4 是一款先进的 LLM，具备出色的自然语言和代码理解及推理能力。
- en: III Motivation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 动机
- en: 'This section motivates FixAgent, an LLM-based unified debugging framework empowered
    by rubber ducking. Our motivation centers on three challenges of existing debugging
    tools: imperfect fault localization, complex bug repair, and context ignorance,
    as introduced in $\S$[I](#S1 "I Introduction ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") Introduction.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节阐述了 FixAgent 的动机，这是一种基于 LLM 的统一调试框架，赋能于橡皮鸭调试。我们的动机集中在现有调试工具的三个挑战上：不完美的故障定位、复杂的
    bug 修复和上下文无知，正如在 $\S$[I](#S1 "I Introduction ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") 引言中介绍的。
- en: III-A Imperfect Fault Localization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 不完美的故障定位
- en: Imperfect FL results include wrong and missing locations. Such imperfection
    can considerably affect the downstream repair.= Firstly, FL tools can incorrectly
    identify non-buggy code as the source of the error, making the repair approach
    generate patches that are not only unnecessary but could also introduce new errors.
    As reported in [[19](#bib.bib19)], even state-of-the-art FL approaches suffer
    from low accuracy. Prior techniques can only achieve less than 22.3%, and 46.3%
    real faulty statements are ranked as the top-1 and top-5 suspicious ones, respectively,
    on the real-world dataset Defects4J V1.2.0 [[43](#bib.bib43)]. The result is far
    from satisfactory, and the patch space established based on such FL results can
    be problematic. The overwhelming majority of APR tools are confined to replacing
    the identified statements with those produced by APR, potentially assuming perfect
    localization is easily available. Such an assumption is unrealistic and affects
    the debugging performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不完美的FL结果包括错误和遗漏的位置。这种不完美可以显著影响后续修复。= 首先，FL工具可能错误地将非错误代码识别为错误源，从而使修复方法生成的不仅是多余的补丁，还可能引入新的错误。正如[[19](#bib.bib19)]中报告的，即使是最先进的FL方法也会遭遇低准确率。以前的技术只能实现低于22.3%，而46.3%的真实故障语句在现实世界数据集Defects4J
    V1.2.0[[43](#bib.bib43)]中被排在前1和前5的可疑位置上。结果远未令人满意，基于这些FL结果建立的修复空间可能存在问题。绝大多数APR工具限于用APR生成的代码替换识别出的语句，可能假设完美的局部化易于获取。这种假设不切实际，并影响调试性能。
- en: Moreover, FL tools may miss faulty statements. First, most FL and APR tools
    assume that each program only contains one bug existing in an existing code statement [[44](#bib.bib44)].
    This is because 1) identifying multi-line bugs and making edits at multiple and
    non-contiguous locations is especially challenging [[45](#bib.bib45)]; and 2)
    APR tools often rely on spectrum-based FL, which can only isolate single-line
    bugs [[46](#bib.bib46)]. Second, failing case-dependent FL cannot identify bugs
    caused by non-existing statements. Such methods count the execution times of a
    certain statement, so they have difficulties finding missing conditions. However,
    real-world bugs are complex and diverse, which may be multi-line or missing-line,
    calling for more sophisticated debugging to mitigate the threat of localization
    imperfection.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，FL工具可能会遗漏故障语句。首先，大多数FL和APR工具假设每个程序仅包含一个存在于现有代码语句中的错误[[44](#bib.bib44)]。这是因为1)
    识别多行错误并在多个不连续的位置进行编辑尤其具有挑战性[[45](#bib.bib45)]；2) APR工具通常依赖于基于光谱的FL，这只能隔离单行错误[[46](#bib.bib46)]。其次，失败案例依赖的FL不能识别由于不存在的语句引起的错误。这些方法计算某个语句的执行次数，因此难以发现缺失的条件。然而，现实世界的错误是复杂多样的，可能是多行或缺失行，这需要更复杂的调试来减轻局部化不完美的威胁。
- en: III-B Complex Logic Bug Fixing
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 复杂逻辑错误修复
- en: The emergence of LLMs shed light on a potential solution to debugging. A single
    LLM can already complete debugging. It simply regards debugging as producing tokens
    of a repair by calculating the probabilities from left to right, conditioned on
    the buggy program. A recent study comprehensively evaluates the debugging capability
    of LLMs [[47](#bib.bib47)]. It prompts LLMs to fix a buggy program without any
    other prior knowledge. Results show that the most advanced model, GPT4, can achieve
    comparable performance with humans on LeetCode, an online programming platform,
    submissions. However, LLMs still face huge difficulties in fixing logic errors,
    and even runtime information of failing cases is unhelpful for such errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的出现为调试问题提供了潜在的解决方案。一个LLM已经能够完成调试。它简单地将调试视为通过根据有缺陷的程序从左到右计算概率来生成修复的标记。最近的一项研究全面评估了LLMs的调试能力[[47](#bib.bib47)]。它促使LLMs在没有任何其他先验知识的情况下修复有缺陷的程序。结果显示，最先进的模型GPT4在LeetCode这样一个在线编程平台上的提交表现可以与人类相媲美。然而，LLMs在修复逻辑错误时仍然面临巨大困难，甚至失败案例的运行时信息对于这些错误也无济于事。
- en: '![Refer to caption](img/1e75324c84cad63543470ad072a5b7b6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1e75324c84cad63543470ad072a5b7b6.png)'
- en: 'Figure 1: Complex bug fixing is still challenging for LLMs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：复杂错误修复对于LLMs仍然具有挑战性。
- en: We also observe a similar phenomenon. Figure [1](#S3.F1 "Figure 1 ‣ III-B Complex
    Logic Bug Fixing ‣ III Motivation ‣ A Unified Debugging Approach via LLM-Based
    Multi-Agent Synergy") displays a complex program with a logic bug and the wrong
    repair of GPT4\. The wrong repair can pass most test cases but fail to calculate
    the number of tank-tap pairs given three houses and only one pipe, a corner case.
    The key to this problem is to construct a directional chain using nodes and edges
    to represent the water-related objects, a thought of abstracting complex real-world
    systems into code. Such insight requirement goes beyond language understanding
    and is challenging to LLMs. Thus, simply applying LLMs cannot meet the growing
    demand for complex software debugging, motivating us to create designs that better
    unleash their capability.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也观察到类似的现象。图 [1](#S3.F1 "Figure 1 ‣ III-B Complex Logic Bug Fixing ‣ III Motivation
    ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy") 显示了一个带有逻辑错误的复杂程序以及
    GPT4 的错误修复。这个错误修复可以通过大多数测试用例，但无法计算给定三个房屋和仅一个管道的水箱配对数，这是一个边界情况。解决这个问题的关键是使用节点和边构造一个定向链来表示与水相关的对象，这是一种将复杂的现实世界系统抽象为代码的思路。这种洞察要求超越了语言理解，对
    LLMs 来说具有挑战性。因此，简单地应用 LLMs 不能满足对复杂软件调试的日益增长的需求，这促使我们创造更好地发挥其能力的设计。
- en: III-C Context Ignorance
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 上下文忽略
- en: Successful debugging requires a deep understanding of not just the syntax but
    the semantic purpose of code. However, existing APR tools focus on source code
    and test outcomes only, ignoring the important program contexts such as the intended
    functionality, the data flow, and the expected behaviors. Figure [2](#S3.F2 "Figure
    2 ‣ III-C Context Ignorance ‣ III Motivation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") presents a repair generated by an advanced APR
    tool. This repair is also regarded as “ground-truth” provided by the dataset Codeflaws.
    Yet, it ignores the variable scope, so it failed when $m\geq 99$, even an omniscient
    APR tool has trouble producing a correct repair. This case highlights the importance
    of program contexts. Programming is a problem-solving task, so we should conduct
    context-aware program debugging for real-world software.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的调试不仅需要深入理解代码的语法，还要了解其语义目的。然而，现有的APR工具仅关注源代码和测试结果，忽略了重要的程序上下文，如预期功能、数据流和期望行为。图 [2](#S3.F2
    "Figure 2 ‣ III-C Context Ignorance ‣ III Motivation ‣ A Unified Debugging Approach
    via LLM-Based Multi-Agent Synergy") 展示了一个由高级APR工具生成的修复。这个修复也被认为是数据集 Codeflaws
    提供的“真实情况”。然而，它忽略了变量范围，因此当 $m\geq 99$ 时失败，甚至一个全知的APR工具也难以生成正确的修复。这一案例突显了程序上下文的重要性。编程是一个解决问题的任务，因此我们应该对现实世界的软件进行上下文感知的程序调试。
- en: '![Refer to caption](img/d3fa478bb5981b0ff8aaf7efb50fbcdc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d3fa478bb5981b0ff8aaf7efb50fbcdc.png)'
- en: 'Figure 2: The repair made by an APR tool (also regarded as correct in the dataset)
    ignores the variable scope requirement.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：APR 工具所做的修复（也被数据集认为是正确的）忽略了变量范围要求。
- en: '<svg id="S3.SS3.p2.pic1" class="ltx_picture" height="89.67" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,89.67) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="62.11" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Insight: LLMs have the potential
    for unified debugging but face challenges of bug localization imperfection, complex
    error fixing, and context ignorance. They motivate us to create designs to unleash
    the debugging capabilities of LLMs. We propose to adopt rubber ducking to boost
    LLMs just like developers.</foreignobject></g></g></svg>'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="S3.SS3.p2.pic1" class="ltx_picture" height="89.67" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,89.67) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="62.11" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">洞察：LLMs 具有统一调试的潜力，但面临着缺陷定位不完美、复杂错误修复和上下文忽略的挑战。它们激励我们创造设计以释放
    LLMs 的调试能力。我们建议采用橡皮鸭调试法来提升 LLMs，就像开发者一样。</foreignobject></g></g></svg>
- en: IV Methodology
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 方法论
- en: This section introduces the key ideas behind FixAgent, our LLM-based unified
    debugging framework. Figure [3](#S4.F3 "Figure 3 ‣ IV Methodology ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy") displays the overview of
    FixAgent, consisting of three specialized agents serving as bug localizer, patch
    generator, and post-error reviewer, respectively ($\S$[IV-D2](#S4.SS4.SSS2 "IV-D2
    Feedback-supported re-sampling ‣ IV-D Secondary Designs ‣ IV Methodology ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy")).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了FixAgent背后的关键理念，这是我们基于LLM的统一调试框架。图[3](#S4.F3 "Figure 3 ‣ IV Methodology
    ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy")展示了FixAgent的概述，包括三个专业代理，分别担任错误定位器、补丁生成器和错误后评审者（$\S$[IV-D2](#S4.SS4.SSS2
    "IV-D2 Feedback-supported re-sampling ‣ IV-D Secondary Designs ‣ IV Methodology
    ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy")）。
- en: '![Refer to caption](img/b63ef81ed04197e8ccc83a00717f0a5c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b63ef81ed04197e8ccc83a00717f0a5c.png)'
- en: 'Figure 3: Overview of FixAgent.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：FixAgent概述。
- en: IV-A Specialized Agent Synergy
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 专业代理协同
- en: 'We specialize three LLM agents, each responsible for a stage in debugging separately:
    fault localization (localizer), patch generation (repairer), and post-error analysis
    (revisitor). Localizer identifies faulty code statements. It can even point out
    missing statements and label them in the buggy program like “<INFILL> // missing
    this line causes a bug”. Repairer aims to generate an executable and correct patch.
    Revisitor analyzes why the original code was buggy and the rationale behind the
    patch. The agents work sequentially. Each agent passes its response to the downstream
    agent, and responses from these agents make up the final response.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专门化了三个LLM代理，每个代理负责调试中的一个阶段：故障定位（定位器）、补丁生成（修复者）和错误后分析（复访者）。定位器识别有缺陷的代码语句。它甚至可以指出缺失的语句，并在有缺陷的程序中标记为“<INFILL>
    // 缺少这一行会导致错误”。修复者的目标是生成一个可执行且正确的补丁。复访者分析原始代码为什么有缺陷及补丁的理由。这些代理顺序工作。每个代理将其响应传递给下游代理，这些代理的响应组成最终的响应。
- en: 'Each prompt is a triplet consisting of a role profile, program specifications,
    and instructions. First, each agent is promoted with a clear role profile for
    task-oriented role-playing. LLMs can act like an expected agent if given detailed
    role descriptions[[48](#bib.bib48)]. A role profile consists of an expert identification
    and an agent description. Expert identification determines the role of an agent,
    e.g., “You are an expert in identifying specific faulty code elements.” The agent
    description introduces the specific task objective, e.g., “Your task is to fix
    buggy code snippets.” The role profile is followed by program specifications,
    including the buggy program, failing test case information, program contexts,
    and the response from the previous agent(s), if applicable. For example, we prompt
    the revisitor with identified bug locations generated by the localizer and a repair
    produced by the repairer. Afterward, we underline the task objective and provide
    detailed step-by-step instructions. For example, the repairer has the objective
    of returning the patch with an explanation in the desired format. It should carry
    out a series of steps: context comprehending, program analysis (including variable
    tracking introduced in $\S$[IV-B](#S4.SS2 "IV-B Intermediate Variable Tracking
    ‣ IV Methodology ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"))
    against the failing cases, making minimal changes for a patch, and double-checking
    the identified buggy statements.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示都是一个三元组，由角色简介、程序规格和指令组成。首先，每个代理都有一个清晰的角色简介，用于任务导向的角色扮演。如果提供详细的角色描述，LLMs可以表现得像预期中的代理一样[[48](#bib.bib48)]。角色简介包括专家识别和代理描述。专家识别确定代理的角色，例如，“你是一个识别特定错误代码元素的专家。”代理描述介绍了具体的任务目标，例如，“你的任务是修复有缺陷的代码片段。”角色简介之后是程序规格，包括有缺陷的程序、失败的测试用例信息、程序上下文，以及之前代理的响应（如果适用）。例如，我们用本地化器生成的已识别的错误位置和修复者提供的修复提示来提示复访者。随后，我们强调任务目标并提供详细的逐步指令。例如，修复者的目标是以所需格式返回修补程序及其解释。它应执行一系列步骤：理解上下文、程序分析（包括在$\S$[IV-B](#S4.SS2
    "IV-B Intermediate Variable Tracking ‣ IV Methodology ‣ A Unified Debugging Approach
    via LLM-Based Multi-Agent Synergy")中引入的变量跟踪）针对失败的用例，做出最小更改以生成修补程序，并仔细检查识别出的有缺陷的语句。
- en: The downstream agent relies on upstream results while influencing them in turn.
    If the repairer generates a plausible patch that changes code statements different
    from the identified ones, the localizer also adjusts its results based on such
    changes, though it can also present the original responses upon user demand. Similarly,
    the analyzer may contain a better patch to boost the repairer or wiser localization,
    enabling answer adjusting for the localizer. Besides, our pipeline enables FixAgent
    to handle multiple programs in parallel, like an assembly line. The final answer
    returned to the user comes from their synergy, consisting of the identified buggy
    statements, the patch, and the post-error analysis.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下游代理依赖于上游结果，同时又反过来影响它们。如果修复者生成了一个可能的补丁，改变了与识别的代码语句不同的代码语句，本地化器也会根据这些变化调整其结果，尽管在用户要求时，它也可以呈现原始响应。同样，分析器可能包含一个更好的补丁来提升修复者或更明智的本地化，允许本地化器调整答案。此外，我们的管道使FixAgent能够并行处理多个程序，像一个流水线。最终返回给用户的答案来自它们的协同，包括识别的有缺陷的语句、补丁和后错误分析。
- en: IV-B Intermediate Variable Tracking
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 中间变量跟踪
- en: We prompt each agent to track critical intermediate variable values against
    failed test cases and compare them to expected outcomes. Such prompting is positioned
    in the instructions mentioned in the previous section. Each agent is requested
    to explicitly present such tracking in its response, accompanied by a comprehensive
    explanation of how it facilitates the derivation of its answer. This design is
    inspired by the rubber ducking, using explanations to enhance programming. Compared
    with requiring line-by-line explanations as the original rubber ducking does,
    our design prioritizes information with significant impact on the program’s behavior,
    such as the core logic executions and states, helping LLM to concentrate on the
    parts that are most likely to influence the outcome, making it easier to identify
    and solve errors. It also corresponds to chain-of-thought (CoT) [[32](#bib.bib32)],
    whose idea is that decomposing a complex question into pieces can improve the
    reasoning capability of LLMs, and even simply adding “think it step by step” can
    lead to significant improvement. Our tracking design allows LLMs to decompose
    a complex program with multiple logic modules into several intermediate states
    and conduct extra calculations during debugging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提示每个代理跟踪关键的中间变量值与失败的测试用例，并将其与预期结果进行比较。这样的提示在上一节提到的指令中定位。每个代理被要求在其响应中明确呈现这些跟踪，并附上如何促进答案推导的详细解释。这种设计受到橡皮鸭式编程的启发，通过解释来增强编程能力。与原橡皮鸭式编程要求逐行解释不同，我们的设计优先考虑对程序行为有重大影响的信息，例如核心逻辑执行和状态，帮助LLM集中于最可能影响结果的部分，便于识别和解决错误。这也对应于思考链（CoT）[[32](#bib.bib32)]，其理念是将复杂问题分解成若干部分可以提高LLM的推理能力，甚至仅仅添加“逐步思考”也能带来显著改善。我们的跟踪设计允许LLMs将一个包含多个逻辑模块的复杂程序分解为几个中间状态，并在调试过程中进行额外的计算。
- en: Moreover, this design enhances the transparency of LLM decision-making. We can
    see how an agent reaches its answer, enabling potential human interactions. For
    example, in a dynamic programming problem, developers can ask the model to focus
    on the state transfer equation and edge conditions. Even if the agent cannot eventually
    generate a correct repair, the thinking path will still provide insights to developers.
    Such interpretation also helps win the trust of developers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种设计提升了LLM决策过程的透明度。我们可以看到代理如何得出答案，从而实现潜在的人机互动。例如，在动态编程问题中，开发人员可以要求模型专注于状态转移方程和边缘条件。即使代理最终无法生成正确的修复方案，思维路径仍能为开发人员提供见解。这种解释也有助于赢得开发人员的信任。
- en: IV-C Context Construction
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 上下文构建
- en: FixAgent mines two aspects of the program context, i.e., requirements and dependencies.
    First, for programs with detailed documentation, we adopt descriptions of the
    program functionality, input/output format, precision requirement, and other related
    information to clarify the expected behavior of the program. If the program implements
    a well-known algorithm without documentation, we request a general LLM (may not
    be the base model of FixAgent) to introduce the algorithm given its name (usually
    the function name in the program). The introduction serves as a requirement description.
    Second, we parse the dependencies of the buggy program and extract the code of
    these dependent files. The extracted code is put on to the top of the program.
    This operation, though simple, can work well by ensuring LLMs handles the dependent
    code first and then the buggy program, as most LLMs deal with tokens from left
    to right and are trained on file-level code. These two strategies can construct
    an informative context for the program.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: FixAgent 挖掘程序上下文的两个方面，即需求和依赖性。首先，对于具有详细文档的程序，我们采用程序功能、输入/输出格式、精度要求和其他相关信息的描述，以明确程序的预期行为。如果程序实现了一个没有文档的知名算法，我们请求一个通用的
    LLM（可能不是 FixAgent 的基础模型）在给定算法名称（通常是程序中的函数名）的情况下介绍该算法。该介绍作为需求描述。其次，我们解析有缺陷程序的依赖性并提取这些依赖文件的代码。提取的代码被放置在程序的顶部。这种操作虽然简单，但可以通过确保
    LLM 先处理依赖代码然后处理有缺陷程序来有效工作，因为大多数 LLM 从左到右处理令牌，并且是在文件级代码上进行训练的。这两种策略可以为程序构建信息丰富的上下文。
- en: IV-D Secondary Designs
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 次级设计
- en: IV-D1 Test input generation
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 测试输入生成
- en: We specialize an extra agent, crafter, for test case generation beyond the human-written
    test cases. It is launched when a plausible patch is generated to mitigate the
    overfitting issue, i.e., plausible repairs generated by APR tools pass given test
    cases but cannot generalize on others, especially for test-dependent APR tools [[49](#bib.bib49)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为超越人工编写的测试用例的测试用例生成专门设立了一个额外的代理，称为 crafter。当生成了一个可能的补丁以缓解过拟合问题时，它会启动，即 APR
    工具生成的可能修复通过了给定的测试用例，但不能在其他测试用例上泛化，特别是对于测试依赖的 APR 工具[[49](#bib.bib49)]。
- en: We also follow the $\langle$n1 2 ... 10000” is generated in an abbreviated form
    to save tokens and cannot be processed by the program. Still, we can extend it
    to a valid input with its explanation “1000 employees, 10000 applications, ....”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也遵循 $\langle$n1 2 ... 10000” 以缩略形式生成以节省令牌，这些形式不能被程序处理。不过，我们可以将其扩展为有效输入，并附上说明“1000
    名员工，10000 个应用程序，....”
- en: Note that we need external efforts to calculate the expected outputs of the
    generated inputs to obtain test cases. We do not use LLMs for such calculation
    because they are not good at arithmetic problem-solving [[40](#bib.bib40)] and
    are unsuitable to serve as a standard criterion. In practice, developers can calculate
    the expected outputs for the insightful generated inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要外部工作来计算生成输入的预期输出以获得测试用例。我们不使用 LLMs 进行此类计算，因为它们不擅长算术问题解决[[40](#bib.bib40)]，并且不适合作为标准标准。实际上，开发人员可以计算有洞察力生成输入的预期输出。
- en: IV-D2 Feedback-supported re-sampling
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 反馈支持的重新采样
- en: Errors are inevitable in debugging. We adopt a feedback-supported design to
    reduce wrong repairs. If FixAgent generates an implausible patch, we re-sample
    FixAgent to get another patch, prompted with feedback of failing test information
    in a conversational manner, inspired by [[28](#bib.bib28)]. Assume we allow for
    $m$ that passes the most (or all) test cases serves as the final solution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 调试中的错误是不可避免的。我们采用反馈支持的设计来减少错误修复。如果 FixAgent 生成了一个不切实际的补丁，我们会重新采样 FixAgent 以获取另一个补丁，反馈失败的测试信息以对话方式进行提示，灵感来源于
    [[28](#bib.bib28)]。假设我们允许通过最多（或所有）测试用例的 $m$ 作为最终解决方案。
- en: This design aligns FixAgent with previous APR tools, which usually sample thousands
    of candidate patches. Notably, our strategy is different from [[28](#bib.bib28)],
    which keeps one conversation going until reaching a plausible fix, so the prompt
    contains all repairs and their failing information. Instead, we only input one
    repair and its feedback for one sampling, avoiding the possibility of extending
    the token window of an LLM. Lastly, the best repair with bug-fix analysis will
    be returned to the user. Users can also request the specific response of any agent,
    including the corresponding answer and explanation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该设计使 FixAgent 与以往的 APR 工具对齐，这些工具通常会采样数千个候选修复。值得注意的是，我们的策略与 [[28](#bib.bib28)]
    不同，后者保持一个对话直到找到一个可行的修复，因此提示包含所有修复及其失败信息。相反，我们只输入一个修复及其反馈进行一次采样，从而避免了扩展 LLM 的标记窗口的可能性。最后，最佳的修复和错误修复分析将返回给用户。用户还可以请求任何代理的特定响应，包括相应的答案和解释。
- en: V Evaluation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 评估
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: How does FixAgent compare against state-of-the-art APR tools and base
    LLMs?'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ1: FixAgent 如何与最先进的 APR 工具和基础 LLMs 进行比较？'
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: How capable is FixAgent in different LLMs and how does FixAgent make improvement
    on its base model?'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ2: FixAgent 在不同的 LLMs 中表现如何，FixAgent 如何改进其基础模型？'
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3: How does each design contribute to FixAgent?'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ3: 每种设计如何对 FixAgent 做出贡献？'
- en: V-A Experiment Setup
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 实验设置
- en: We compare FixAgent with advanced APR approaches and general LLMs on public
    databases with pairs of bug-fix. The comparison refers to APR metrics, using the
    number of plausible patches and correct patches, as well as the correctness rate
    (the correct patch count over generated plausible patches) to gauge the effectiveness
    of FixAgent. This is because our work is the first unified debugging framework,
    and the eventual goal is to obtain a correct and executable repair.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 FixAgent 与先进的 APR 方法和通用 LLMs 在公开的错误修复对数据库上进行比较。该比较参考了 APR 评估指标，包括可行修复数和正确修复数，以及正确率（正确修复数与生成的可行修复数的比率），以评估
    FixAgent 的有效性。这是因为我们的工作是第一个统一的调试框架，最终目标是获得一个正确且可执行的修复。
- en: V-A1 Datasets
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 数据集
- en: We use three datasets for evaluation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三个数据集进行评估。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Codeflaws [[37](#bib.bib37)] consists of 3902 faulty programs (2952 one-line
    bugs) and their corresponding repairs written in C collected from Codeforces,
    an online programming platform. These bugs are classified into 40 classes, including
    control flow, data flow, function call, pointer, variable type, etc.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Codeflaws [[37](#bib.bib37)] 包含 3902 个故障程序（2952 个单行错误）及其相应的修复，这些程序用 C 语言编写，来自在线编程平台
    Codeforces。这些错误被分为 40 类，包括控制流、数据流、函数调用、指针、变量类型等。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: QuixBugs [[36](#bib.bib36)] consists of 40 faulty programs in Java and Python.
    Each program implements a classic algorithm in one function with a one-line defect.
    The bugs involve incorrect operators, variables, and field dereferences, as well
    as missing conditions, arithmetic expressions, etc.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: QuixBugs [[36](#bib.bib36)] 包含 40 个 Java 和 Python 的故障程序。每个程序在一个函数中实现一个经典算法，并具有一个单行缺陷。这些错误涉及不正确的操作符、变量和字段解引用，以及缺失的条件、算术表达式等。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ConDefects [[51](#bib.bib51)] consists of 1254 Java and 1625 Python faulty programs
    covering diverse task difficulties, sourced from AtCoder, an online programming
    competition platform. Each faulty program is paired with its repair.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ConDefects [[51](#bib.bib51)] 包含 1254 个 Java 和 1625 个 Python 故障程序，涵盖了多种任务难度，来源于在线编程竞赛平台
    AtCoder。每个故障程序都配有其修复。
- en: 'The Codeflaws and QuixBugs are widely used in previous studies, whereas ConDefects
    is recently collected, making it impossible for direct data leakage in the training
    data of existing popular LLMs. Since no studies have reported experimental results
    on this new dataset and repairing all of the faulty programs is especially expensive,
    we sample 300 faulty programs for each programming language and only use them
    to compare FixAgent against base LLMs in RQ2 ($\S$[V-C](#S5.SS3 "V-C RQ2: Using
    different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy")).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'Codeflaws 和 QuixBugs 在以前的研究中广泛使用，而 ConDefects 是最近收集的，避免了现有流行 LLM 训练数据中的直接数据泄漏。由于没有研究报告该新数据集上的实验结果，并且修复所有故障程序尤其昂贵，我们为每种编程语言抽取了
    300 个故障程序，仅用它们来比较 FixAgent 和基础 LLMs 在 RQ2 ($\S$[V-C](#S5.SS3 "V-C RQ2: Using different
    LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based
    Multi-Agent Synergy"))。'
- en: V-A2 Baselines
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 基线
- en: We compare FixAgent against 16 baselines, including 10 APR tools and 6 base
    LLMs. APR methods include three advanced C tools (Angelix [[14](#bib.bib14)],
    Prophet [[52](#bib.bib52)], SPR [[53](#bib.bib53)], CVC4 [[54](#bib.bib54)]),
    two genetic programming-based techniques (Semfix [[13](#bib.bib13)], GenProg [[10](#bib.bib10),
    [11](#bib.bib11)]), two recent NMT-based approaches (CoCoNuT [[21](#bib.bib21)],
    CURE [[22](#bib.bib22)]), and an LLM-based method (AlphaRepair [[27](#bib.bib27)]).
    We adopt the results reported by their original papers and follow-up surveys [[49](#bib.bib49),
    [55](#bib.bib55), [24](#bib.bib24)]. LLM baselines include three general-purpose
    models (Gemini [[56](#bib.bib56)], ChatGPT [[57](#bib.bib57)], GPT4 [[42](#bib.bib42)])
    and three code LLMs (DeepSeek-Coder [[41](#bib.bib41)], CodeLlama [[58](#bib.bib58)],
    Codex [[59](#bib.bib59)]). We also use the reported results of Codex since OpenAI
    no longer supports it. Other LLM baselines are implemented via their official
    APIs, whose versions are Gemini 1.0-Pro, GPT-3.5-Turbo-0125 (about 175 billion
    parameters, i.e., 175B, for simplicity), GPT-4-0125-preview, DeepSeek-Coder-Base
    (33B), and Phind-CodeLlama-V2 (34B), respectively.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 FixAgent 与 16 个基线进行比较，包括 10 种 APR 工具和 6 个基础 LLM。APR 方法包括三种先进的 C 工具（Angelix [[14](#bib.bib14)]，Prophet [[52](#bib.bib52)]，SPR [[53](#bib.bib53)]，CVC4 [[54](#bib.bib54)]），两种基于遗传编程的技术（Semfix [[13](#bib.bib13)]，GenProg [[10](#bib.bib10)，[11](#bib.bib11)]），两种最近的
    NMT 方法（CoCoNuT [[21](#bib.bib21)]，CURE [[22](#bib.bib22)]），以及一种基于 LLM 的方法（AlphaRepair [[27](#bib.bib27)]）。我们采用其原始论文和后续调查的报告结果 [[49](#bib.bib49)，[55](#bib.bib55)，[24](#bib.bib24)]。LLM
    基线包括三种通用模型（Gemini [[56](#bib.bib56)]，ChatGPT [[57](#bib.bib57)]，GPT4 [[42](#bib.bib42)]）和三种代码
    LLM（DeepSeek-Coder [[41](#bib.bib41)]，CodeLlama [[58](#bib.bib58)]，Codex [[59](#bib.bib59)]）。由于
    OpenAI 不再支持 Codex，我们也使用了 Codex 的报告结果。其他 LLM 基线通过其官方 API 实现，其版本分别为 Gemini 1.0-Pro，GPT-3.5-Turbo-0125（约
    1750 亿参数，即 175B，为简便起见），GPT-4-0125-preview，DeepSeek-Coder-Base（33B）和 Phind-CodeLlama-V2（34B）。
- en: 'TABLE I: Comparison with APR tools and base LLMs. #Correct and #Plausible represent
    the number of bugs correctly and plausibly patched, respectively. #Patch/bug is
    the sampling number per bug. Cells filled in by $(x,y)$ sampled bugs. Other cells
    are obtained on the whole dataset.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I：与 APR 工具和基础 LLM 的比较。#Correct 和 #Plausible 分别表示正确和可行修补的错误数量。#Patch/bug 是每个错误的采样数量。由
    $(x,y)$ 填充的单元格是采样错误。其他单元格是基于整个数据集获得的。'
- en: 'Fault Localization Tools #Patch/bug Codeflaws-C (3902 bugs) QuixBugs-Java (40
    bugs) QuixBugs-Python (40 bugs) #Correct #Plausible #Correct #Plausible #Correct
    #Plausible Standard Angelix [[14](#bib.bib14)] 1000 318 591 - - - - Prophet [[52](#bib.bib52)]
    1000 310 839 - - - - SPR [[53](#bib.bib53)] 1000 283 783 - - - - CVC4 [[54](#bib.bib54)]
    - (15, 665) (91, 665) - - - - Semfix [[13](#bib.bib13)] - (38, 665) (56, 655)
    - - - - Perfect GenProg [[10](#bib.bib10), [11](#bib.bib11)] 1000 255-369 1423
    1 4 - - RewardRepair [[60](#bib.bib60)] 200 - - 20 - - - CoCoNuT [[21](#bib.bib21)]
    20000 423 716 13 20 19 21 CURE [[22](#bib.bib22)] 5,000 - - 26 35 - - AlphaRepair [[27](#bib.bib27)]
    5000 - - 28 30 27 32 Codex [[59](#bib.bib59)] 600 - - 32 35 37 37 None^⋆ CodeLlama [[58](#bib.bib58)]
    3 91^† 1353 25 28 33 33 Gemini [[56](#bib.bib56)] 3 89^† 1186 29 32 29 35 DeepSeek-Coder [[41](#bib.bib41)]
    3 93^† 1741 30 34 25 38 ChatGPT [[57](#bib.bib57)] 3 94^† 1927 33 34 34 36 GPT4 [[42](#bib.bib42)]
    3 93^† 2115 35 36 39 39 FixAgent 3 93^† 2701 39 39 40 40'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '故障定位工具 #Patch/bug Codeflaws-C (3902 错误) QuixBugs-Java (40 错误) QuixBugs-Python
    (40 错误) #Correct #Plausible #Correct #Plausible #Correct #Plausible 标准 Angelix [[14](#bib.bib14)]
    1000 318 591 - - - - Prophet [[52](#bib.bib52)] 1000 310 839 - - - - SPR [[53](#bib.bib53)]
    1000 283 783 - - - - CVC4 [[54](#bib.bib54)] - (15, 665) (91, 665) - - - - Semfix [[13](#bib.bib13)]
    - (38, 665) (56, 655) - - - - 完美 GenProg [[10](#bib.bib10)，[11](#bib.bib11)] 1000
    255-369 1423 1 4 - - RewardRepair [[60](#bib.bib60)] 200 - - 20 - - - CoCoNuT [[21](#bib.bib21)]
    20000 423 716 13 20 19 21 CURE [[22](#bib.bib22)] 5,000 - - 26 35 - - AlphaRepair [[27](#bib.bib27)]
    5000 - - 28 30 27 32 Codex [[59](#bib.bib59)] 600 - - 32 35 37 37 无^⋆ CodeLlama [[58](#bib.bib58)]
    3 91^† 1353 25 28 33 33 Gemini [[56](#bib.bib56)] 3 89^† 1186 29 32 29 35 DeepSeek-Coder [[41](#bib.bib41)]
    3 93^† 1741 30 34 25 38 ChatGPT [[57](#bib.bib57)] 3 94^† 1927 33 34 34 36 GPT4 [[42](#bib.bib42)]
    3 93^† 2115 35 36 39 39 FixAgent 3 93^† 2701 39 39 40 40'
- en: $\star$
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\star$
- en: For LLMs understanding both code and natural languages, no fault localization
    information is provided for unified debugging.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于同时理解代码和自然语言的 LLM，未提供统一调试的故障定位信息。
- en: $\dagger$
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\dagger$
- en: We randomly select 100 plausible patches to check their correctness because
    of the huge number of plausible patches.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于可行补丁数量巨大，我们随机选择 100 个可能的补丁来检查其正确性。
- en: V-A3 Implementations
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 实现
- en: A single base LLM performs the whole debugging process without our designs.
    To ensure a fair comparison, we replace the variable tracking prompt with “Think
    it step by step,” a well-known prompt enhancing the reasoning ability of LLMs [[32](#bib.bib32)].
    We also retain the program specifications when prompting the base models. Plus,
    FixAgent is implemented by Python3.9 with 1500+ lines of code. Following prior
    work [[27](#bib.bib27)], the generation uses the top-p nucleus sampling [[61](#bib.bib61)]
    with $p=1.0$.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 单个基准 LLM 在没有我们设计的情况下执行整个调试过程。为了确保公平比较，我们将变量跟踪提示替换为“逐步思考”，这是一个广泛认可的提示，能够增强 LLM
    的推理能力 [[32](#bib.bib32)]。我们还在提示基准模型时保留了程序规范。此外，FixAgent 使用 Python3.9 实现，代码量超过
    1500 行。按照之前的工作 [[27](#bib.bib27)]，生成使用了 top-p 核心采样 [[61](#bib.bib61)]，其中 $p=1.0$。
- en: Previous APR usually generates hundreds to thousands of patches per bug, whereas
    we only sample at most three times per bug for LLM baselines due to the high cost
    of token generation. Comparing thousands of samples (APR studies) with only three
    samples (LLM baselines) seems unfair. Nevertheless, we follow the tradition of
    sampling only several times in code generation evaluation on LLMs [[62](#bib.bib62)],
    and the performance of LLMs is already incredible. For evaluation purposes, we
    manually check the plausible patches. A patch is correct only when semantically
    equivalent to its corresponding reference patch or official answer in the programming
    contest platform. We carefully check the correctness of each plausible patch of
    QuixBugs and 100 sampled plausible patches of Codeflaws in RQ1. We do not check
    all patches of Codeflaws because 1) LLMs and FixAgent generate too many (thousands
    of) plausible patches, so it is extremely time-consuming to check them one by
    one; and 2) without ground-truth fault locations, these LLMs rewrite programs
    and change code statements, for example, replacing multiple “if else” by “switch
    case”, making it very different to manually check the correctness. Thus, we follow [[21](#bib.bib21)]
    to check sampled patches and report the correctness rate instead of the correct
    patch counts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的 APR 通常为每个错误生成数百到数千个修补程序，而我们仅对 LLM 基准进行最多三次采样，因为生成令牌的成本较高。将数千个样本（APR 研究）与仅三个样本（LLM
    基准）进行比较似乎不公平。然而，我们遵循在 LLM 的代码生成评估中仅进行少量采样的传统 [[62](#bib.bib62)]，LLM 的表现已经相当出色。为了评估目的，我们手动检查了可能的修补程序。只有当修补程序在语义上等同于其对应的参考修补程序或编程竞赛平台上的官方答案时，才认为修补程序是正确的。我们仔细检查了
    QuixBugs 的每个可能修补程序的正确性以及 RQ1 中 Codeflaws 的 100 个样本可能修补程序。我们没有检查 Codeflaws 的所有修补程序，因为
    1) LLM 和 FixAgent 生成了过多（成千上万的）可能修补程序，因此逐一检查非常耗时；2) 没有真实的错误位置，这些 LLM 会重写程序并更改代码语句，例如，将多个“if
    else”替换为“switch case”，使得手动检查正确性非常困难。因此，我们遵循 [[21](#bib.bib21)] 检查采样修补程序，并报告正确率而不是正确修补程序的数量。
- en: 'V-B RQ1: Overall Effectiveness of FixAgent'
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'V-B RQ1: FixAgent 的总体有效性'
- en: This section presents the overall comparison between FixAgent and baselines
    on Codeflaws and QuixBugs among C, Java, and Python. Since different APR approaches
    adopted different FL tools, we separate them according to the FL types adopted
    by their original papers, where standard FL refers to a traditional spectrum-based
    FL. Perfect FL assumes a APR tool knows the ground-truth bug locations, though
    it is unrealistic. Our work aims at an end-to-end solution without knowing the
    bug locations, so we do not provide FL results to our method and the base LLM
    baselines (i.e., None for the first column).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节呈现了 FixAgent 与基准方法在 C、Java 和 Python 中的 Codeflaws 和 QuixBugs 数据集上的整体比较。由于不同的
    APR 方法采用了不同的 FL 工具，我们根据原始论文采用的 FL 类型将它们分开，其中标准 FL 指的是传统的基于谱的 FL。Perfect FL 假设
    APR 工具知道真实的错误位置，尽管这不切实际。我们的工作旨在提供一个端到端的解决方案，而不依赖于错误位置的知识，因此我们没有提供 FL 结果给我们的方法和基准
    LLM（即第一列为 None）。
- en: 'The results are shown in Table [I](#S5.T1 "Table I ‣ V-A2 Baselines ‣ V-A Experiment
    Setup ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent
    Synergy"). Overall, FixAgent plausibly patches 2780 out of 3982 bugs on Codeflaws
    and QuixBugs, performing the best compared to all baselines. In particular, on
    the Codeflaws dataset, FixAgent significantly outperforms existing APR methods
    and LLMs. It produces 1.9X plausible patches as the best APR method, GenProg.
    Moreover, it improves the correctness rate by 57.42% compared to that of CoCoNuT,
    which correctly fixes the most bugs among APR methods. We can also roughly estimate
    that FixAgent correctly fixes about 2512 bugs based on the correctness rate. Moreover,
    FixAgent outperforms all compared approaches on QuixBugs. It correctly fixes 39
    bugs in QuixBugs-Java, 7 of which have never been fixed before. FixAgent correctly
    fixes all bugs in QuixBugs-Python, showing superiority over all baselines. Figure [4](#S5.F4
    "Figure 4 ‣ V-B RQ1: Overall Effectiveness of FixAgent ‣ V Evaluation ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy") displays an example bug
    of QuixBugs that is uniquely fixed by FixAgent. The bug wrongly returns an empty
    array, but it is desired as a nested empty array. It is difficult to fix by traditional
    and learning-based APR because it requires adding multi-line edits, which simple
    modifications cannot achieve, and goes beyond typical bug-fix templates. Since
    QuixBugs only contains very brief descriptions of the programs, the debugging
    method must abstract the desired returning format from other returning statements
    in the program or the failing information (if any), requiring a comprehensive
    overview and deep analysis of the program. In fact, even FixAgent generates the
    correct patch upon the original failing test that triggers returning an empty
    set nested with an array. Note that FixAgent fixes four extra bugs in QuixBugs
    compared with its base model GPT4\. We will discuss the reasons in RQ2 ($\S$[V-C](#S5.SS3
    "V-C RQ2: Using different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging
    Approach via LLM-Based Multi-Agent Synergy")).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '结果显示在表格[I](#S5.T1 "Table I ‣ V-A2 Baselines ‣ V-A Experiment Setup ‣ V Evaluation
    ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy")中。总体来说，FixAgent
    在 Codeflaws 和 QuixBugs 上合理地修复了3982个漏洞中的2780个，与所有基线方法相比表现最佳。特别是在 Codeflaws 数据集上，FixAgent
    显著优于现有的 APR 方法和 LLM。它产生的合理修复补丁是最佳 APR 方法 GenProg 的 1.9 倍。此外，与 CoCoNuT 相比，它将正确修复率提高了
    57.42%，CoCoNuT 在 APR 方法中正确修复了最多漏洞。我们还可以粗略估计 FixAgent 正确修复了约2512个漏洞。此外，FixAgent
    在 QuixBugs 上超越了所有比较方法。它在 QuixBugs-Java 中正确修复了39个漏洞，其中7个是以前从未修复的。FixAgent 正确修复了
    QuixBugs-Python 中的所有漏洞，显示出对所有基线的优势。图[4](#S5.F4 "Figure 4 ‣ V-B RQ1: Overall Effectiveness
    of FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent
    Synergy")展示了 FixAgent 独特修复的 QuixBugs 示例漏洞。该漏洞错误地返回一个空数组，但期望返回一个嵌套的空数组。传统和基于学习的
    APR 很难修复此问题，因为它需要添加多行编辑，简单的修改无法实现，并且超出了典型的错误修复模板。由于 QuixBugs 仅包含非常简短的程序描述，调试方法必须从程序中的其他返回语句或失败信息（如果有的话）抽象出期望的返回格式，这需要对程序进行全面概述和深度分析。事实上，即使是
    FixAgent 也会生成正确的修复补丁，以应对返回一个嵌套数组的原始失败测试。请注意，FixAgent 与其基础模型 GPT4 比，额外修复了 QuixBugs
    中的四个漏洞。我们将在 RQ2 ($\S$[V-C](#S5.SS3 "V-C RQ2: Using different LLMs with FixAgent
    ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"))
    中讨论原因。'
- en: '![Refer to caption](img/9de81bfa73f0a9a86c6108b60b739730.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9de81bfa73f0a9a86c6108b60b739730.png)'
- en: 'Figure 4: Example of bug fixed by FixAgent in QuixBugs.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：FixAgent 在 QuixBugs 中修复的漏洞示例。
- en: Furthermore, we find that LLMs achieve much better performance than traditional
    and NMT-based APR tools, with respect to both plausible patch generation and correctness
    rate, similar to previous findings [[24](#bib.bib24)]. LLMs achieve such results
    even without knowing fault locations, while APR baselines assume perfect FL or
    use a FL tool to facilitate repair generation. Even the worst-performing LLM baseline,
    Gemini, achieves comparable results to the best traditional tool, GenProg. We
    attribute this to the capabilities of LLMs in understanding the semantics and
    contexts of code. LLMs can leverage the knowledge learned from previous source
    code to understand the program semantics and requirements deeply, enabling them
    to fix not only syntactic errors but also semantic errors that previous approaches
    might miss, especially when the bug does not correspond to predefined rules or
    templates that most APR baselines rely on. Besides, LLMs more fully embody superiority
    on Codeflaws. This is because Codeflaws challenges require a deeper understanding
    of real-world contexts and conditions under which programs are written, demanding
    more context understanding. Every program in Codeflaws is written to solve a certain
    question described in natural languages connected with the real world, while QuixBugs
    consists of classic algorithms with one-line bugs that are more easily repaired
    by systematic modifications.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现 LLMs 在可行补丁生成和正确率方面的表现明显优于传统的和基于 NMT 的 APR 工具，这与之前的发现类似[[24](#bib.bib24)]。LLMs
    在不知道故障位置的情况下也能取得这样的结果，而 APR 基线则假设完美的 FL 或使用 FL 工具来辅助生成修复。即使是表现最差的 LLM 基线 Gemini，也能取得与最好的传统工具
    GenProg 相当的结果。我们将这归因于 LLMs 理解代码语义和上下文的能力。LLMs 可以利用从之前源代码中学到的知识深入理解程序语义和需求，使它们能够修复不仅是语法错误，还有之前方法可能遗漏的语义错误，尤其是当错误不符合大多数
    APR 基线依赖的预定义规则或模板时。此外，LLMs 在 Codeflaws 上更充分体现了其优越性。这是因为 Codeflaws 的挑战需要对程序编写的现实世界背景和条件有更深入的理解，要求更强的上下文理解。Codeflaws
    中的每个程序都是为了回答与现实世界相关的自然语言描述的问题，而 QuixBugs 包含经典算法和单行错误，这些错误更容易通过系统性修改进行修复。
- en: FixAgent performs the best among LLM-based methods. We attribute this to our
    designs. LLM competitors conduct the whole debugging (or patch generation of AlphaRepair)
    in one turn, while FixAgent divides this complex task into several steps and requires
    explicit explanations focusing on crucial information. We instantiate agents to
    conduct different stages of debugging separately. Each agent can focus on its
    own and produce better results, so their synergy delivers better debugging performance.
    In addition, we prompt the agents to pay attention to key variables. Intuitively,
    if the program’s output does not conform to the expected, the bug usually appears
    before its final printing statement (though there are printing bugs, they are
    easy to fix), manifested by the intermediate values of variables, especially those
    in key logic expressions. Our prompt forces the agents to conduct reasoning along
    the program’s logic execution path, making bugs easily revealed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: FixAgent 在基于 LLM 的方法中表现最佳。我们将此归因于我们的设计。LLM 竞争者在一次性完成整个调试（或 AlphaRepair 的补丁生成），而
    FixAgent 将这个复杂任务分解成几个步骤，并要求明确的解释，重点关注关键的信息。我们实例化了代理来分别进行调试的不同阶段。每个代理可以专注于自己的任务并产生更好的结果，因此它们的协同作用提供了更好的调试性能。此外，我们提示代理关注关键变量。从直观上看，如果程序的输出不符合预期，错误通常会出现在最终打印语句之前（尽管有打印错误，但这些错误容易修复），表现为变量的中间值，特别是那些在关键逻辑表达式中的值。我们的提示迫使代理沿程序的逻辑执行路径进行推理，使错误更容易被揭示。
- en: Note that the debugging results on QuixBugs among LLMs are very close, so we
    omit it in the following RQs. This is because 1) QuixBugs programs implement classic
    algorithms with single-line bugs that are relatively easy to fix, and 2) LLMs
    may have been trained on these algorithm implementations, so they deliver remarkable
    results due to data leakage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LLMs 在 QuixBugs 上的调试结果非常接近，因此我们在以下的 RQs 中省略了它。这是因为 1) QuixBugs 程序实现了经典算法，具有单行错误，这些错误相对容易修复，2)
    LLMs 可能已在这些算法实现上进行过训练，因此由于数据泄漏它们能够提供显著的结果。
- en: 'V-C RQ2: Using different LLMs with FixAgent'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C RQ2：使用不同的 LLMs 和 FixAgent
- en: 'We evaluate the capability of FixAgent using different base LLMs on ConDefects,
    collected after the LLM releases, making it impossible to compose the training
    data, thereby avoiding data leakage. Table [II](#S5.T2 "Table II ‣ V-C RQ2: Using
    different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") shows the results, where FixAgent-$[model]$ bugs.
    FixAgent can correctly fix 115–205 bugs and plausibly patch 124–207 bugs out of
    the sampled 300 Java bugs using different LLMs. It can also fix 89–163 and patch
    99–168 Python bugs. The results indicate that FixAgent can work effectively with
    various base LLMs, though influenced by their capabilities. FixAgent with a larger
    model should perform better in experience, but FixAgent-DeepSeek-Coder outperforms
    FixAgent-ChatGPT and FixAgent-Gemini, while Gemini and ChatGPT have much larger
    model sizes. We attribute this to the specialized training data and objective
    of DeepSeek-Coder. DeepSeek-Coder is a code LLM trained on a real-world code corpus,
    making it proficient at coding-related tasks, including debugging. Though DeepSeek-Coder
    and CodeLlama are code LLM with similar model sizes, FixAgent-DeepSeek-Coder outperforms
    FixAgent-CodeLlama considerably. Their inherent coding ability gap can explain
    this. As reported in [[41](#bib.bib41)], DeepSeek-Coder performs much better than
    CodeLlama on solving programming contests. The debugging performance of LLM is
    strongly related to its coding ability, considering that debugging can be regarded
    as generating a correct program prompted by a buggy one.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了FixAgent使用不同基础LLM在ConDefects上的能力，这些数据在LLM发布后收集，因此无法构建训练数据，从而避免了数据泄露。表[II](#S5.T2
    "Table II ‣ V-C RQ2: Using different LLMs with FixAgent ‣ V Evaluation ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy")显示了结果，其中FixAgent-$[model]$
    bug。FixAgent可以正确修复115到205个bug，并合理修复124到207个bug，在抽样的300个Java bug中使用不同LLM。它还可以修复89到163个Python
    bug，并修补99到168个Python bug。结果表明，FixAgent可以与各种基础LLM有效配合，尽管受其能力的影响。FixAgent使用更大模型的体验应更好，但FixAgent-DeepSeek-Coder的表现优于FixAgent-ChatGPT和FixAgent-Gemini，而Gemini和ChatGPT的模型大小要大得多。我们将此归因于DeepSeek-Coder的专门训练数据和目标。DeepSeek-Coder是一个在实际代码语料库上训练的代码LLM，使其在编码相关任务，包括调试方面非常娴熟。尽管DeepSeek-Coder和CodeLlama是具有类似模型大小的代码LLM，但FixAgent-DeepSeek-Coder的表现明显优于FixAgent-CodeLlama。这可以通过固有的编码能力差距来解释。如在[[41](#bib.bib41)]中报道，DeepSeek-Coder在解决编程竞赛中的表现远好于CodeLlama。LLM的调试性能与其编码能力密切相关，因为调试可以被视为生成一个正确的程序，以解决一个有缺陷的程序。'
- en: 'TABLE II: Effectiveness of FixAgent using different LLMs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：FixAgent使用不同LLM的有效性。
- en: Models ConDefects-Java ConDefects-Python 300 bugs 300 bugs FixAgent-CodeLlama
    115$/$168
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 ConDefects-Java ConDefects-Python 300个bug 300个bug FixAgent-CodeLlama 115$/$168
- en: 'We also evaluate how FixAgent improves its base model on Codeflaws, as shown
    in Table [III](#S5.T3 "Table III ‣ V-C RQ2: Using different LLMs with FixAgent
    ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"),
    where we sample 300 bugs in this evaluation for efficiency. Compared to its base
    models, FixAgent plausibly patches 17–35 more bugs and correctly fixes 12–33 more
    bugs. Among all compared pairs, FixAgent improves the most on GPT4, increasing
    correctly and plausibly patched bugs by 33 and 35, respectively. Overall, applying
    FixAgent can make a 20% improvement on the original LLM, on average. This enhancement
    highlights the effectiveness of our designs that significantly improve the debugging
    abilities of LLMs in a non-invasive manner without any re-training or fine-tuning,
    demonstrating our insight that LLMs can benefit from software principles, such
    as rubber ducking.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还评估了FixAgent在Codeflaws上如何改进其基础模型，如表[III](#S5.T3 "Table III ‣ V-C RQ2: Using
    different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy")所示，我们在此评估中抽样了300个bug以提高效率。与基础模型相比，FixAgent可能修复了17到35个更多的bug，并且正确修复了12到33个更多的bug。在所有对比的模型中，FixAgent在GPT4上的提升最大，正确和合理修复的bug数量分别增加了33个和35个。总体而言，应用FixAgent可以使原始LLM的性能提高20%，平均而言。这一改进突显了我们设计的有效性，显著提升了LLM的调试能力，以一种非侵入性的方式，没有任何重新训练或微调，体现了我们的见解，即LLM可以从软件原则中获益，例如橡皮鸭调试。'
- en: 'TABLE III: Improvement made by FixAgent compared to its base model under different
    LLMs on 300 samples in Codeflaws.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：FixAgent在Codeflaws的300个样本中与其基础模型相比的改进情况。
- en: Models Codeflaws-C, sampled 300 bugs Base Model (CoT) Applying FixAgent CodeLlama
    105$/$262
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 Codeflaws-C，抽样300个bug 基础模型（CoT） 应用FixAgent CodeLlama 105$/$262
- en: 'V-D RQ3: Ablation Studies on FixAgent'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D RQ3：关于FixAgent的消融研究
- en: 'This section studies the contribution of each design to FixAgent. Table [IV](#S5.T4
    "Table IV ‣ V-D RQ3: Ablation Studies on FixAgent ‣ V Evaluation ‣ A Unified Debugging
    Approach via LLM-Based Multi-Agent Synergy") shows the results. Each row represents
    removing one design, the decrease in the number of correct or plausible generated
    patches. FixAgent $w/o$ feedback repeatedly samples FixAgent three times independently
    without failing information.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '本节研究了每个设计对 FixAgent 的贡献。表 [IV](#S5.T4 "表 IV ‣ V-D RQ3: FixAgent 的消融研究 ‣ V 评估
    ‣ 基于 LLM 的多代理协同统一调试方法") 显示了结果。每一行表示去除一个设计，生成的正确或可行修补程序的减少。FixAgent $w/o$ feedback
    反复独立采样 FixAgent 三次而没有失败信息。'
- en: 'TABLE IV: Ablation study results of FixAgent.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：FixAgent 的消融研究结果。
- en: 'Models Codeflaws-C, 300 bugs #Correct #Plausible FixAgent $w/o$ feedback -5
    -4'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 Codeflaws-C, 300 错误 #正确 #可行 FixAgent $w/o$ feedback -5 -4'
- en: 'We find that context construction contributes the most to FixAgent. Removing
    contexts reduces the correctly fixed bugs by 122. This highlights the significance
    of contexts in debugging as they provide the potential to understand the underlying
    problem domain, infer the intended functionality, and apply appropriate repair.
    Such degradation also illustrates why LLMs significantly outperform APR tools
    even without perfect fault localization, as shown in RQ1 ($\S$[V-B](#S5.SS2 "V-B
    RQ1: Overall Effectiveness of FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach
    via LLM-Based Multi-Agent Synergy")).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现上下文构建对 FixAgent 的贡献最大。去除上下文会减少 122 个正确修复的错误。这突显了上下文在调试中的重要性，因为它们提供了理解潜在问题领域、推断预期功能和应用适当修复的可能性。这种退化也说明了为什么即使没有完美的故障定位，LLM
    也能显著超越 APR 工具，如 RQ1 所示（$\S$[V-B](#S5.SS2 "V-B RQ1: FixAgent 的整体有效性 ‣ V 评估 ‣ 基于
    LLM 的多代理协同统一调试方法"))。'
- en: 'The second important design is variable tracking. FixAgent generates 37 and
    38 fewer plausible and correct patches, respectively, without tracking. The foundation
    of this advancement is the enhancement of error diagnosis and reduction of reasoning
    overload in LLMs. By verbally articulating the role and expected behavior of each
    variable, LLMs are more likely to pinpoint discrepancies between expected and
    actual outcomes, facilitating a targeted approach to repair, mirroring the effective
    rubber ducking strategy. Furthermore, the structured explanation reduces the model’s
    reasoning overload. Traditional CoT prompting, while effective in encouraging
    step-by-step reasoning, does not inherently prioritize information in a way that
    minimizes reasoning strain. Instead, our strategy streamlines the process of LLMs
    in analyzing the program logic. Using one agent to replace multi-agent synergy
    has a similar negative effect, which reduces 28 plausible and correct fixes. The
    division of labor mirrors a well-established principle in software: specialization,
    which forces each agent to focus on its own task and reduces cognitive load.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要设计是变量跟踪。FixAgent 在没有跟踪的情况下生成的可行和正确修补程序分别少了 37 和 38 个。这一进展的基础是增强错误诊断和减少
    LLM 中的推理过载。通过口头阐述每个变量的作用和预期行为，LLM 更有可能准确指出预期结果与实际结果之间的差异，从而促使有针对性的修复方法，类似于有效的橡皮鸭策略。此外，结构化解释减少了模型的推理过载。传统的
    CoT 提示虽然有效地鼓励了逐步推理，但并不会本质上优先考虑信息，从而减少推理负担。相反，我们的策略简化了 LLM 分析程序逻辑的过程。使用一个代理代替多代理协同有类似的负面效果，减少了
    28 个可行和正确的修复。劳动分工反映了软件中的一个成熟原则：专业化，这迫使每个代理专注于自己的任务并减少认知负担。
- en: The least useful design is feedback-supported re-sampling. Though the added
    feedback provides more information, it expands the dialogue window, and the model
    must allocate its finite attention span to this new information, potentially at
    the expense of other critical details. This trade-off may explain why the additional
    feedback does not significantly enhance the debugging capability of LLMs compared
    to independent sampling, as our results show.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最不实用的设计是反馈支持的重采样。尽管添加的反馈提供了更多信息，但它扩大了对话窗口，模型必须将其有限的注意力分配给这些新信息，可能会以牺牲其他关键细节为代价。这种权衡可能解释了为什么额外的反馈没有显著增强
    LLM 的调试能力，与独立采样相比，如我们的结果所示。
- en: VI Discussion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 讨论
- en: VI-A Limitations
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 限制
- en: VI-A1 Ceiling Improvement
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 顶点改进
- en: Though FixAgent significantly improves the debugging performance of multiple
    LLMs, its effectiveness is intrinsically tied to the foundational capabilities
    of the base model. Indeed, our method can not fundamentally alter the intrinsic
    capabilities of LLMs; instead, it optimizes their existing skills within a proportional
    range. For example, FixAgent improves Gemini by plausibly and correctly fixing
    26 and 17 more bugs on Codeflaws, but its performance is still poorer than the
    simple application of GPT4. Thus, models with inherently limited debugging abilities
    will not experience a significant leap in performance using FixAgent despite improvements.
    The ceiling enhancement is ultimately bounded by the capabilities of the LLM itself.
    Hence, we adopt GPT4 as the default base model due to its advanced performance
    in a broad range of cognitive tasks [[63](#bib.bib63)], though FixAgent is compatible
    with any LLM.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 External Test Output Calculation
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We introduce an extra agent for test input generation to mitigate the overfitting
    problem, but it cannot directly calculate the corresponding expected outputs or
    test oracles, where a test oracle is a set of assertions that should pass when
    the program behaves as expected. This is primarily due to the intrinsic probabilistic
    mechanisms of LLMs, making it challenging to accurately deliver precise answers
    to computational questions. Addressing such problems akin to mathematical reasoning
    represents one of the significant challenges faced by LLMs. Our approach cannot
    overcome their inherent limitations; therefore, additional information, such as
    correct code or manually computed answers, must be introduced to obtain the outputs
    for generated inputs and form complete test cases.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Threats to Validity
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Internal First, we share the same major internal threat to validity with previous
    LLMs-based coding-related techniques where the training data of closed-sourced
    LLMs may overlap with our evaluation datasets. Since we do not have access to
    the training data, we mitigate this threat from three steps. 1) We choose a recently
    collected dataset (ConDefects) for evaluation proposed to mitigate the data leakage
    problem of LLMs as its collection is posterior to the model release. 2) The other
    two datasets we use are believed to be not part of the training data as discussed
    in [[24](#bib.bib24)] because 1) they have a low number of stars on GitHub, and
    2) they focus on classic algorithms (QuixBugs) and programming assignments (Codeflaws)
    that do not belong to larger real-world projects. 3) RQ2 [V-C](#S5.SS3 "V-C RQ2:
    Using different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach
    via LLM-Based Multi-Agent Synergy") demonstrates that our framework significantly
    improves the debugging effectiveness compared with its base LLM, where the improvement
    is orthogonal to the data leakage issue. Second, we cannot directly determine
    the correct patches and rely on manual validation of plausible patches. For Codeflaws
    and ConDefects, we have to check a sampled subset of the patches because of their
    huge size. To mitigate this threat, we carefully validate the randomly sampled
    patches and all the patches for QuixBugs. Also, we generate more inputs using
    the crafter agent to test the plausible patches.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: External Threat The main external threat lies in the evaluation dataset, where
    the superiority of FixAgent may not generalize to other datasets, especially for
    those less widely used programming languages that have less open-source code data
    for LLM training. To mitigate this, we compare our method against advanced baselines,
    including both general LLMs and APR approaches on five benchmarks in three datasets,
    covering three programming languages. Experiments demonstrate the effectiveness
    of FixAgent among all these benchmarks. We will also evaluate our approach to
    more datasets across more diverse programming languages in the future.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: VII Related Work
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VII-A Debugging
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spectrum-based and mutation-based are twopopular FL techniques. Spectrum-based
    tools [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] calculate the suspiciousness
    scores of each line of code based on the correlation between the execution of
    a code line and the occurrence of test failures. However, it relies solely on
    test coverage data, specifically the binary distinction of whether a test case
    executes a code line. This reliance implicitly assumes that all test cases contribute
    equally to uncovering faults, which oversimplifies the complex nature of software
    errors. Mutation-based [[8](#bib.bib8), [7](#bib.bib7), [9](#bib.bib9)] mitigates
    the above limitation by implementing simple modifications on the buggy program
    based on pre-defined rules (mutation operators), such as altering a conditional
    statement from equality (==) to inequality (!=), thereby creating variants of
    the program that slightly differ from the original buggy one. However, its efficacy
    is contingent upon the ability to generate meaningful mutants for each code element
    under scrutiny. Learning-based FL tools learn program behaviors from rich data
    sources, including code coverage matrix [[16](#bib.bib16)], statement-level calls [[64](#bib.bib64)],
    structural intricacies [[17](#bib.bib17)], or their combination [[18](#bib.bib18)]
    via multiple types of neural networks. A recent advancement, LLMAO [[19](#bib.bib19)],
    proposes to parse Abstract Syntax Tree and utilizes LLMs to achieve test-free
    FL with high confidence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基于谱系和基于变异的技术是两种流行的缺陷定位（FL）技术。基于谱系的工具[[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]
    根据代码行的执行与测试失败的发生之间的相关性，计算每行代码的可疑度分数。然而，它仅依赖于测试覆盖数据，具体来说，就是测试用例是否执行某行代码的二元区分。这种依赖隐含地假设所有测试用例在发现缺陷方面的贡献是相等的，这简化了软件错误的复杂性。基于变异的技术[[8](#bib.bib8),
    [7](#bib.bib7), [9](#bib.bib9)] 通过根据预定义规则（变异操作符）对有缺陷的程序进行简单修改来缓解上述限制，例如将条件语句从等于（==）改为不等于（!=），从而生成与原始有缺陷程序略有不同的变体。然而，它的有效性依赖于为每个受审代码元素生成有意义的变异体的能力。基于学习的缺陷定位工具从丰富的数据源中学习程序行为，包括代码覆盖矩阵[[16](#bib.bib16)]、语句级调用[[64](#bib.bib64)]、结构复杂性[[17](#bib.bib17)]，或它们的组合[[18](#bib.bib18)]，通过多种类型的神经网络。最近的进展，LLMAO[[19](#bib.bib19)]，提出了解析抽象语法树并利用大型语言模型（LLMs）实现高信度的无测试缺陷定位。
- en: APR techniques can be broadly categorized into search-based and generate-and-validate
    (G&V)-based. Search-based employs meta-heuristic search algorithms, such as genetic
    programming [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)], to find suitable
    solutions by exploring the space of possible patches. The patch space can be built
    by fix synthesis through syntactic code modifications and human-defined or automatically
    mined templates. Such approaches assume that the correct expression exists in
    the program or a pre-defined template set, limiting their wider applications [[65](#bib.bib65)].
    G&V APR [[14](#bib.bib14), [13](#bib.bib13)] aims to generate patches logically
    and semantically coherent with the intended functionality. Traditional G&V methods
    represent the repair process as an explicit specification inference [[66](#bib.bib66)].
    Recent studies regard it as a translation from faulty code to correct code using
    NMT, such as CoCoNuT [[21](#bib.bib21)], CURE [[22](#bib.bib22)], and RewardRepair [[60](#bib.bib60)].
    Despite their effectiveness, they rely on bug-fixing training datasets usually
    crawled from commits of open-source repositories, which may contain irrelevant
    changes. To filter such noise, NMT tools are usually trained on small commit data,
    limiting their ability to fix more diverse bugs. Researchers have explored directly
    applying LLMs in APR using an infilling paradigm [[27](#bib.bib27), [24](#bib.bib24)],
    showing promising results. However, they still struggle to fix complex bugs requiring
    a deep understanding of program contexts or logic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: APR技术可以大致分为基于搜索和基于生成与验证（G&V）的两类。基于搜索的方法使用元启发式搜索算法，如遗传编程[[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]，通过探索可能补丁的空间来寻找合适的解决方案。补丁空间可以通过语法代码修改和人工定义或自动挖掘的模板构建。这些方法假设程序中或预定义模板集中存在正确的表达式，限制了它们的广泛应用[[65](#bib.bib65)]。G&V
    APR[[14](#bib.bib14), [13](#bib.bib13)]旨在生成在逻辑和语义上与预期功能一致的补丁。传统的G&V方法将修复过程表示为明确的规范推导[[66](#bib.bib66)]。近期研究将其视为使用NMT（神经机器翻译）从故障代码到正确代码的翻译，如CoCoNuT[[21](#bib.bib21)]、CURE[[22](#bib.bib22)]和RewardRepair[[60](#bib.bib60)]。尽管这些方法有效，但它们依赖于通常从开源代码库提交中抓取的错误修复训练数据，这些数据可能包含无关的更改。为了过滤这些噪声，NMT工具通常在小规模提交数据上进行训练，这限制了它们修复更多样化错误的能力。研究人员已经探索了在APR中直接应用LLMs（大型语言模型）使用填充范式[[27](#bib.bib27),
    [24](#bib.bib24)]，显示出有希望的结果。然而，它们仍然难以修复需要深刻理解程序上下文或逻辑的复杂错误。
- en: Unified debugging is a pioneer concept [[39](#bib.bib39)] that aims to unify
    FL and APR to boost both areas. A recent study [[67](#bib.bib67)] has shown that
    FL can benefit from 16 different APR systems. However, existing studies only consider
    the contribution to FL made by APR. We are the first to propose a unified framework
    enabling end-to-end debugging, where stages of the debugging have an interactional
    rather than determined relationship.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 统一调试是一个开创性的概念[[39](#bib.bib39)]，旨在统一FL（故障定位）和APR（自动程序修复）以提升这两个领域的效果。最近的一项研究[[67](#bib.bib67)]表明FL可以从16种不同的APR系统中受益。然而，现有研究仅考虑APR对FL的贡献。我们首次提出了一种统一框架，支持端到端调试，其中调试阶段之间具有交互关系而非确定关系。
- en: VII-B Large Language Models
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 大型语言模型
- en: LLMs have revolutionized the field of software development with their rapid
    advancement. LLMs are trained on vast amounts of text data, enabling them to understand
    and generate human-like text [[42](#bib.bib42), [41](#bib.bib41)]. Existing studies
    have shown that a well-crafted prompt can lead to accurate, insightful, and useful
    domain-specific text generation [[32](#bib.bib32), [68](#bib.bib68)]. In addition
    to general-purpose LLMs [[57](#bib.bib57), [42](#bib.bib42), [56](#bib.bib56)],
    code LLMs are mainly trained on source code data, such as Codex [[59](#bib.bib59)],
    DeepSeek-Coder [[41](#bib.bib41)], and CodeLlama [[58](#bib.bib58)]. These models
    are proposed to automate and streamline software engineering, enhance productivity,
    and reduce human errors. Many studies have investigated their capability on coding
    tasks, including code generation [[62](#bib.bib62)], APR [[24](#bib.bib24), [33](#bib.bib33),
    [47](#bib.bib47)], and test generation [[69](#bib.bib69)]. On top of the direct
    application of APR [[24](#bib.bib24)], recent studies have researched prompt engineering [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [28](#bib.bib28), [73](#bib.bib73), [27](#bib.bib27)]
    or the combination of other techniques [[25](#bib.bib25), [69](#bib.bib69), [19](#bib.bib19)]
    to better unleash the capability of LLMs. For example, Repilot [[25](#bib.bib25)]
    copilots the LLMs via a code completion engine to synthesize more valid patches.
    AlphaRepair [[27](#bib.bib27)] proposes the first cloze-style APR approach that
    directly prompts LLMs to predict the correct code given context information with
    faulty code masked. A recent code generator [[70](#bib.bib70)] asks the LLM to
    explain the generated code line-by-line as rubber ducking, achieving promising
    performance. InferFix [[73](#bib.bib73)] augmented the prompt by incorporating
    similar fixes identified in a historical database of bugs through a dense retrieval
    model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Our method, FixAgent, provides an end-to-end solution for automated debugging
    with high effectiveness. It performs a universal prompt design inspired by rubber
    ducking, which can significantly improve the debugging ability of LLMs in a non-intrusive
    style without any retraining/fine-tuning or historical bug-fix information. FixAgent
    shows that LLMs can benefit from general software engineering principles recognized
    by developers, potentially besides rubber ducking.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose FixAgent, the first unified debugging framework via LLM agent synergy.
    It conducts fault localization, patch generation, and post-error analysis in an
    end-to-end manner. Our insight is that LLMs can benefit from software engineering
    principles recognized by developers. Thus, we follow the principle of rubber duck
    debugging-explaining the code in detail to create novel designs that unleash the
    debugging capability of LLMs and mitigate previous challenges. The evaluation
    of two widely used datasets demonstrates the superiority of FixAgent over APR
    tools and LLM-based competitors, and extra experiments on recently collected data
    (avoiding data leakage) further show our generalization and effectiveness in debugging
    compared with base LLMs. Our code and generated patches are publicly available
    for further research and reproduction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] D. Yuan, Y. Luo, X. Zhuang, G. R. Rodrigues, X. Zhao, Y. Zhang, P. Jain,
    and M. Stumm, “Simple testing can prevent most critical failures: An analysis
    of production failures in distributed data-intensive systems,” in *11th USENIX
    Symposium on Operating Systems Design and Implementation, OSDI ’14, Broomfield,
    CO, USA, October 6-8, 2014*.   USENIX Association, 2014, pp. 249–265.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Alaboudi and T. D. LaToza, “What constitutes debugging? an exploratory
    study of debugging episodes,” *Empir. Softw. Eng.*, vol. 28, no. 5, p. 117, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. Boulder, “University of cambridge study: Failure to adopt reverse debugging
    costs global economy $41 billion annually.” University of Cambridge, London, Technical
    Report, 2013.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] R. Abreu, P. Zoeteweij, and A. J. C. van Gemund, “Spectrum-based multiple
    fault localization,” in *ASE 2009, 24th IEEE/ACM International Conference on Automated
    Software Engineering, Auckland, New Zealand, November 16-20, 2009*.   IEEE Computer
    Society, 2009, pp. 88–99.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] ——, “An evaluation of similarity coefficients for software fault localization,”
    in *12th IEEE Pacific Rim International Symposium on Dependable Computing (PRDC
    2006), 18-20 December, 2006, University of California, Riverside, USA*.   IEEE
    Computer Society, 2006, pp. 39–46.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Zhang, M. Kim, and S. Khurshid, “Localizing failure-inducing program
    edits based on spectrum information,” in *IEEE 27th International Conference on
    Software Maintenance, ICSM 2011, Williamsburg, VA, USA, September 25-30, 2011*.   IEEE
    Computer Society, 2011, pp. 23–32.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] X. Li and L. Zhang, “Transforming programs and tests in tandem for fault
    localization,” *Proc. ACM Program. Lang.*, vol. 1, no. OOPSLA, pp. 92:1–92:30,
    2017.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Moon, Y. Kim, M. Kim, and S. Yoo, “Ask the mutants: Mutating faulty
    programs for fault localization,” in *Seventh IEEE International Conference on
    Software Testing, Verification and Validation, ICST 2014, March 31 2014-April
    4, 2014, Cleveland, Ohio, USA*.   IEEE Computer Society, 2014, pp. 153–162.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] L. Zhang, L. Zhang, and S. Khurshid, “Injecting mechanical faults to localize
    developer faults for evolving software,” in *Proceedings of the 2013 ACM SIGPLAN
    International Conference on Object Oriented Programming Systems Languages & Applications,
    OOPSLA 2013, part of SPLASH 2013, Indianapolis, IN, USA, October 26-31, 2013*.   ACM,
    2013, pp. 765–784.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] C. L. Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer, “A systematic study
    of automated program repair: Fixing 55 out of 105 bugs for $8 each,” in *34th
    International Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich,
    Switzerland*.   IEEE Computer Society, 2012, pp. 3–13.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest, “Automatically finding
    patches using genetic programming,” in *31st International Conference on Software
    Engineering, ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings*.   IEEE,
    2009, pp. 364–374.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] ——, “Automatically finding patches using genetic programming,” in *31st
    International Conference on Software Engineering, ICSE 2009, May 16-24, 2009,
    Vancouver, Canada, Proceedings*.   IEEE, 2009, pp. 364–374.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, “Semfix: program
    repair via semantic analysis,” in *35th International Conference on Software Engineering,
    ICSE ’13, San Francisco, CA, USA, May 18-26, 2013*.   IEEE Computer Society, 2013,
    pp. 772–781.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix: scalable multiline
    program patch synthesis via symbolic analysis,” in *Proceedings of the 38th International
    Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016*.   ACM,
    2016, pp. 691–701.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] F. Long and M. C. Rinard, “An analysis of the search spaces for generate
    and validate patch generation systems,” in *Proceedings of the 38th International
    Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016*.   ACM,
    2016, pp. 702–713.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y. Li, S. Wang, and T. N. Nguyen, “Fault localization with code coverage
    representation learning,” in *43rd IEEE/ACM International Conference on Software
    Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021*.   IEEE, 2021, pp. 661–673.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Lou, Q. Zhu, J. Dong, X. Li, Z. Sun, D. Hao, L. Zhang, and L. Zhang,
    “Boosting coverage-based fault localization via graph-based representation learning,”
    in *ESEC/FSE ’21: 29th ACM Joint European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28,
    2021*.   ACM, 2021, pp. 664–676.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Li, W. Li, Y. Zhang, and L. Zhang, “Deepfl: integrating multiple fault
    diagnosis dimensions for deep fault localization,” in *Proceedings of the 28th
    ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2019,
    Beijing, China, July 15-19, 2019*.   ACM, 2019, pp. 169–180.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Z. H. Yang, C. L. Goues, R. Martins, and V. J. Hellendoorn, “Large
    language models for test-free fault localization,” in *Proceedings of the 46th
    IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon,
    Portugal, April 14-20, 2024*.   ACM, 2024, pp. 17:1–17:12.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu, and C. Hu, “Template-based
    neural program repair,” in *45th IEEE/ACM International Conference on Software
    Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023*.   IEEE, 2023,
    pp. 1456–1468.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, “Coconut:
    combining context-aware neural translation models using ensemble for program repair,”
    in *ISSTA ’20: 29th ACM SIGSOFT International Symposium on Software Testing and
    Analysis, Virtual Event, USA, July 18-22, 2020*.   ACM, 2020, pp. 101–114.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] N. Jiang, T. Lutellier, and L. Tan, “CURE: code-aware neural machine translation
    for automatic program repair,” in *43rd IEEE/ACM International Conference on Software
    Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021*.   IEEE, 2021, pp. 1161–1173.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Silva, S. Fang, and M. Monperrus, “Repairllama: Efficient representations
    and fine-tuned adapters for program repair,” *CoRR*, vol. abs/2312.15698, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. S. Xia, Y. Wei, and L. Zhang, “Automated program repair in the era
    of large pre-trained language models,” in *45th IEEE/ACM International Conference
    on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023*.   IEEE,
    2023, pp. 1482–1494.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Wei, C. S. Xia, and L. Zhang, “Copiloting the copilots: Fusing large
    language models with completion engines for automated program repair,” in *Proceedings
    of the 31st ACM Joint European Software Engineering Conference and Symposium on
    the Foundations of Software Engineering, ESEC/FSE 2023, San Francisco, CA, USA,
    December 3-9, 2023*.   ACM, 2023, pp. 172–184.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] H. Joshi, J. P. C. Sánchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radicek,
    “Repair is nearly generation: Multilingual program repair with llms,” in *Thirty-Seventh
    AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference
    on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium
    on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC,
    USA, February 7-14, 2023*.   AAAI Press, 2023, pp. 5131–5140.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] C. S. Xia and L. Zhang, “Less training, more repairing please: revisiting
    automated program repair via zero-shot learning,” in *Proceedings of the 30th
    ACM Joint European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18,
    2022*.   ACM, 2022, pp. 959–971.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] ——, “Keep the conversation going: Fixing 162 out of 337 bugs for $0.42
    each using chatgpt,” *CoRR*, vol. abs/2304.00385, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] P. S. Kochhar, X. Xia, D. Lo, and S. Li, “Practitioners’ expectations
    on automated fault localization,” in *Proceedings of the 25th International Symposium
    on Software Testing and Analysis, ISSTA 2016, Saarbrücken, Germany, July 18-20,
    2016*.   ACM, 2016, pp. 165–176.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Sohn and S. Yoo, “Fluccs: using code and change metrics to improve
    fault localization,” in *Proceedings of the 26th ACM SIGSOFT International Symposium
    on Software Testing and Analysis, Santa Barbara, CA, USA, July 10 - 14, 2017*.   ACM,
    2017, pp. 273–283.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] K. Liu, A. Koyuncu, T. F. Bissyandé, D. Kim, J. Klein, and Y. L. Traon,
    “You cannot fix what you cannot find! an investigation of fault localization bias
    in benchmarking automated program repair systems,” in *12th IEEE Conference on
    Software Testing, Validation and Verification, ICST,Xi’an, China, April 22-27,
    2019*.   IEEE, 2019, pp. 102–113.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi,
    Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large
    language models,” in *NeurIPS*, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li, and Y. Zhang, “An
    empirical study on fine-tuning large language models of code for automated program
    repair,” in *38th IEEE/ACM International Conference on Automated Software Engineering,
    ASE 2023, Luxembourg, September 11-15, 2023*.   IEEE, 2023, pp. 1162–1174.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Kirbas, E. Windels, O. McBello, K. Kells, M. W. Pagano, R. Szalanski,
    V. Nowack, E. R. Winter, S. Counsell, D. Bowes, T. Hall, S. Haraldsson, and J. R.
    Woodward, “On the introduction of automatic program repair in bloomberg,” *IEEE
    Softw.*, vol. 38, no. 4, pp. 43–51, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Kumar. (2023) Understanding tokens in chatgpt. [Online]. Available:
    [https://medium.com/@manav.kumar87/understanding-tokens-in-chatgpt-32845987858d](https://medium.com/@manav.kumar87/understanding-tokens-in-chatgpt-32845987858d)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, “Quixbugs: a multi-lingual
    program repair benchmark set based on the quixey challenge,” in *Proceedings Companion
    of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages,
    and Applications: Software for Humanity, SPLASH 2017, Vancouver, BC, Canada, October
    23 - 27, 2017*.   ACM, 2017, pp. 55–56.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. H. Tan, J. Yi, Yulis, S. Mechtaev, and A. Roychoudhury, “Codeflaws:
    a programming competition benchmark for evaluating automated program repair tools,”
    in *Proceedings of the 39th International Conference on Software Engineering,
    ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 - Companion Volume*.   IEEE
    Computer Society, 2017, pp. 180–182.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Author(s). (2024) Rudra. [Online]. Available: [https://github.com/afortunado-aceptado/Rudra](https://github.com/afortunado-aceptado/Rudra)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Lou, A. Ghanbari, X. Li, L. Zhang, D. Hao, and L. Zhang, “Can automated
    program repair refine fault localization?” *CoRR*, vol. abs/1910.01270, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, “Evaluating the
    logical reasoning ability of chatgpt and GPT-4,” *CoRR*, vol. abs/2304.03439,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu,
    Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder: When the large language
    model meets programming - the rise of code intelligence,” *CoRR*, vol. abs/2401.14196,
    2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] OpenAI, “Gpt-4a technical report,” *CoRR*, vol. abs/2303.08774, 2023\.
    [Online]. Available: [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of existing
    faults to enable controlled testing studies for java programs,” in *International
    Symposium on Software Testing and Analysis, ISSTA ’14, San Jose, CA, USA - July
    21 - 26, 2014*.   ACM, 2014, pp. 437–440.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] H. Ye and M. Monperrus, “ITER: iterative neural repair for multi-location
    patches,” in *Proceedings of the 46th IEEE/ACM International Conference on Software
    Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024*.   ACM, 2024, pp.
    10:1–10:13.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Wong, P. Santiesteban, C. Kästner, and C. L. Goues, “Varfix: balancing
    edit expressiveness and search effectiveness in automated program repair,” in
    *ESEC/FSE ’21: 29th ACM Joint European Software Engineering Conference and Symposium
    on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021*.   ACM,
    2021, pp. 354–366.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] R. Abreu, P. Zoeteweij, and A. J. van Gemund, “On the accuracy of spectrum-based
    fault localization,” in *Testing: Academic and Industrial Conference Practice
    and Research Techniques - MUTATION (TAICPART-MUTATION 2007)*, 2007, pp. 89–98.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Y. Pan, Y. Wu, Z. Liu, and M. Sun,
    “Debugbench: Evaluating debugging capability of large language models,” *CoRR*,
    vol. abs/2401.04621, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, and Z. Mao, “Expertprompting:
    Instructing large language models to be distinguished experts,” *CoRR*, vol. abs/2305.14688,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. D. Le, F. Thung, D. Lo, and C. L. Goues, “Overfitting in semantics-based
    automated program repair,” in *Proceedings of the 40th International Conference
    on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018*.   ACM,
    2018, p. 163.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
    R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
    M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Wu, Z. Li, J. M. Zhang, and Y. Liu, “Condefects: A new dataset to address
    the data leakage concern for llm-based fault localization and program repair,”
    *CoRR*, vol. abs/2310.16253, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] F. Long, “Automatic patch generation via learning from successful human
    patches,” Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge,
    USA, 2018.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] F. Long and M. C. Rinard, “Staged program repair with condition synthesis,”
    in *Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,
    ESEC/FSE 2015, Bergamo, Italy, August 30 - September 4, 2015*.   ACM, 2015, pp.
    166–178.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Reynolds, M. Deters, V. Kuncak, C. Tinelli, and C. Barrett, “Counterexample-guided
    quantifier instantiation for synthesis in smt,” in *Computer Aided Verification:
    27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015,
    Proceedings, Part II 27*.   Springer, 2015, pp. 198–216.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] H. Ye, M. Martinez, T. Durieux, and M. Monperrus, “A comprehensive study
    of automatic program repair on the quixbugs benchmark,” *J. Syst. Softw.*, vol.
    171, p. 110825, 2021\. [Online]. Available: [https://doi.org/10.1016/j.jss.2020.110825](https://doi.org/10.1016/j.jss.2020.110825)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk,
    A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou,
    J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. P. Lillicrap, A. Lazaridou,
    O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Viola, M. Reynolds,
    Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira, K. Ayoub,
    M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,
    B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,
    M. Khalman, J. Sygnowski, and et al., “Gemini: A family of highly capable multimodal
    models,” *CoRR*, vol. abs/2312.11805, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] OpenAI. (2023) Gpt-3.5 turbo. [Online]. Available: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,
    J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer,
    A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin,
    N. Usunier, T. Scialom, and G. Synnaeve, “Code llama: Open foundation models for
    code,” *CoRR*, vol. abs/2308.12950, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” *CoRR*, vol.
    abs/2107.03374, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Ye, M. Martinez, and M. Monperrus, “Neural program repair with execution-based
    backpropagation,” in *44th IEEE/ACM 44th International Conference on Software
    Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022*.   ACM, 2022, pp.
    1506–1518.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Holtzman, J. Buys, M. Forbes, and Y. Choi, “The curious case of neural
    text degeneration,” *CoRR*, vol. abs/1904.09751, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” *CoRR*, vol.
    abs/2107.03374, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. Mao, G. Chen, X. Zhang, F. Guerin, and E. Cambria, “Gpteval: A survey
    on assessments of chatgpt and GPT-4,” *CoRR*, vol. abs/2308.12488, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Li, S. Wang, and T. N. Nguyen, “Fault localization to detect co-change
    fixing locations,” in *Proceedings of the 30th ACM Joint European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE
    2022, Singapore, Singapore, November 14-18, 2022*.   ACM, 2022, pp. 659–671.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] K. Liu, S. Wang, A. Koyuncu, K. Kim, T. F. Bissyandé, D. Kim, P. Wu, J. Klein,
    X. Mao, and Y. L. Traon, “On the efficiency of test suite based program repair:
    A systematic assessment of 16 automated repair systems for java programs,” in
    *ICSE ’20: 42nd International Conference on Software Engineering, Seoul, South
    Korea, 27 June - 19 July, 2020*.   ACM, 2020, pp. 615–627.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Gao, Y. Noller, and A. Roychoudhury, “Program repair,” *CoRR*, vol.
    abs/2211.12787, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. Benton, X. Li, Y. Lou, and L. Zhang, “On the effectiveness of unified
    debugging: An extensive study on 16 program repair systems,” in *35th IEEE/ACM
    International Conference on Automated Software Engineering, ASE 2020, Melbourne,
    Australia, September 21-25, 2020*.   IEEE, 2020, pp. 907–918.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang,
    and H. Chen, “Reasoning with language model prompting: A survey,” in *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*.   Association for
    Computational Linguistics, 2023, pp. 5368–5393.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large language models
    are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models,”
    in *Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing
    and Analysis, ISSTA 2023, Seattle, WA, USA, July 17-21, 2023*.   ACM, 2023, pp.
    423–435.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] X. Chen, M. Lin, N. Schärli, and D. Zhou, “Teaching large language models
    to self-debug,” *CoRR*, vol. abs/2304.05128, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] N. Jain, S. Vaidyanath, A. S. Iyer, N. Natarajan, S. Parthasarathy, S. K.
    Rajamani, and R. Sharma, “Jigsaw: Large language models meet program synthesis,”
    in *44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
    2022, Pittsburgh, PA, USA, May 25-27, 2022*.   ACM, 2022, pp. 1219–1231.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. Wang, Z. Liu, S. Wang, G. Cui, N. Ding, Z. Liu, and G. Yu, “INTERVENOR:
    prompt the coding ability of large language models with the interactive chain
    of repairing,” *CoRR*, vol. abs/2311.09868, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy,
    “Inferfix: End-to-end program repair with llms,” *CoRR*, vol. abs/2303.07263,
    2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
