- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and
    Committee Discussions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的自动化评估领域：通过代理对战和委员会讨论自动化 LLM 评估
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20267](https://ar5iv.labs.arxiv.org/html/2405.20267)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20267](https://ar5iv.labs.arxiv.org/html/2405.20267)
- en: Ruochen Zhao^(1,2)  , Wenxuan Zhang², Yew Ken Chia^(2,3) , Deli Zhao², Lidong
    Bing²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 赵若晨^(1,2)  , 张文轩², 谢健如^(2,3) , 赵德立², 丁立东²
- en: ¹ Nanyang Technological University, Singapore
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 南洋理工大学，新加坡
- en: ² DAMO Academy, Alibaba Group; ³ Singapore University of Technology and Design
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ² DAMO Academy, Alibaba Group; ³ 新加坡科技设计大学
- en: ruochen002@e.ntu.edu.sg;
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ruochen002@e.ntu.edu.sg;
- en: '{saike.zwx, yewken.chia, deli.zdl, l.bing}@alibaba-inc.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{saike.zwx, yewken.chia, deli.zdl, l.bing}@alibaba-inc.com'
- en: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   Work done
    while the author was an intern at DAMO Academy, Alibaba. Yew Ken Chia is under
    the Joint Ph.D. Program between DAMO Academy and SUTD.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   该工作是在作者在 DAMO
    Academy, Alibaba 实习期间完成的。 谢健如正在 DAMO Academy 和 SUTD 之间的联合博士项目中。'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation
    method that can provide robust evaluation results in a timely fashion. Currently,
    as static benchmarks are prone to contamination concerns, users tend to trust
    human voting platforms, such as Chatbot Arena. However, human annotations require
    extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation
    framework, we innovatively propose the Auto-Arena of LLMs, which automates the
    entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries.
    Then, a pair of candidate LLMs engage in a multi-round peer-battle around the
    query, during which the LLM’s true performance gaps become visible. Finally, a
    committee of LLM judges collectively discuss and determine the winner, which alleviates
    bias and promotes fairness. In our extensive experiment on the 17 newest LLMs,
    Auto-Arena shows the highest correlation with human preferences, providing a promising
    alternative to human evaluation platforms.¹¹1  We release the code at [https://github.com/Auto-Arena/Auto-Arena-LLMs.](https://github.com/Auto-Arena/Auto-Arena-LLMs.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLMs 日新月异地发展，迫切需要一种可信的评估方法，能够及时提供可靠的评估结果。目前，由于静态基准测试容易受到污染，用户倾向于信任人类投票平台，例如
    Chatbot Arena。然而，人类标注需要大量的人工努力。为了提供一种自动化、可靠且可信的评估框架，我们创新性地提出了 LLMs 的 Auto-Arena，它通过
    LLM 代理自动化整个评估过程。首先，一个考官 LLM 制定查询。然后，一对候选 LLM 进行多轮对战，在这些对战中，LLM 的真实性能差距变得显而易见。最后，由
    LLM 法官组成的委员会集体讨论并确定获胜者，这缓解了偏见并促进了公平。在我们对 17 个最新 LLMs 进行的广泛实验中，Auto-Arena 显示出与人类偏好的最高相关性，提供了对人类评估平台的有前景的替代方案。¹¹1 
    我们在 [https://github.com/Auto-Arena/Auto-Arena-LLMs.](https://github.com/Auto-Arena/Auto-Arena-LLMs.)
    发布了代码。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Since ChatGPT and GPT-4 [[23](#bib.bib23)] gained popularity, large language
    models (LLMs) have risen to the forefront of technological innovation, capturing
    broad industry and social interests. This enthusiasm has spurred numerous organizations
    to release their own LLMs [[27](#bib.bib27), [26](#bib.bib26)]. However, the rapid
    pace at which these models are released and updated poses a significant challenge
    for users attempting to understand their capabilities and monitor their evolution.
    Consequently, there has been a pressing demand for comprehensively evaluating
    LLMs recently.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 ChatGPT 和 GPT-4 [[23](#bib.bib23)] 受到广泛关注以来，大型语言模型（LLMs）已成为技术创新的前沿，吸引了广泛的行业和社会关注。这种热情促使许多组织发布了自己的
    LLMs [[27](#bib.bib27), [26](#bib.bib26)]。然而，这些模型发布和更新的快速步伐给用户理解其能力和监控其演变带来了重大挑战。因此，最近对
    LLMs 进行全面评估的需求日益迫切。
- en: One line of research conducts automatic evaluation with static datasets. Among
    these, static datasets with predefined metrics, such as GSM8k [[9](#bib.bib9)]
    and MMLU [[15](#bib.bib15)], are constructed with aspect-specific input-output
    pairs, such as questions and their corresponding answers. Given the questions,
    the LLM-produced answers are compared to ground-truth answers using metrics such
    as accuracy. However, these approaches suffer significantly from contamination
    concerns [[24](#bib.bib24)], where models may have been inadvertently exposed
    to elements of the test datasets during training, thereby skewing evaluation results.
    The rigid ground-truth answers also limit their utility in assessing models’ performance
    on general or open-ended questions. Model-based evaluation, such as MT-Bench [[32](#bib.bib32)]
    and AlpacaEval [[12](#bib.bib12)], provides an alternative for evaluating LLMs
    on open-ended questions. These methods typically ask two models to generate responses
    to the same open-ended question and then employ a strong judge model (e.g., GPT-4)
    to choose the better response. However, the static question sets still pose the
    issue of contamination. Additionally, the assumption of the existence of a strong
    judge model makes the evaluation framework less generalizable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一项研究通过静态数据集进行自动评估。在这些数据集中，具有预定义指标的静态数据集，如 GSM8k [[9](#bib.bib9)] 和 MMLU [[15](#bib.bib15)]，是由特定方面的输入输出对构建的，例如问题及其对应的答案。给定问题后，LLM
    生成的答案与真实答案使用准确率等指标进行比较。然而，这些方法在污染问题上显著受限 [[24](#bib.bib24)]，即模型在训练过程中可能不经意间接触到了测试数据集的元素，从而扭曲评估结果。固定的真实答案也限制了它们在评估模型对一般或开放性问题表现的效用。基于模型的评估，如
    MT-Bench [[32](#bib.bib32)] 和 AlpacaEval [[12](#bib.bib12)]，提供了一种替代方案，用于评估 LLM
    对开放性问题的表现。这些方法通常要求两个模型对相同的开放性问题生成响应，然后使用强大的评判模型（例如 GPT-4）来选择更好的响应。然而，静态问题集仍然存在污染问题。此外，强大评判模型的存在假设使得评估框架的普遍性降低。
- en: Aside from automated evaluations, human assessment, although requiring significant
    manual efforts, remains the gold standard for most users. A noticeable example
    is Chatbot Arena [[32](#bib.bib32)], which is a crowdsourced voting platform that
    gathers anonymous votes on LLM performances and calculates ELO scores to rank
    these models. The resulting leaderboard²²2  https://leaderboard.lmsys.org/ is
    widely considered a trustworthy indicator of an LLM’s general capabilities. However,
    a reliable model evaluation on this platform must be supported by more than 10k
    human votes, which requires considerable time and effort. Consequently, when newly
    developed models enter the scene, they often struggle to quickly amass a significant
    number of votes. Moreover, this strong reliance on human votes limits its application
    in various scenarios. For example, the performance of non-English languages is
    difficult to estimate, as most of the queries on the platform are in English.
    The completely open participation may also result in uneven evaluation quality.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动评估外，人工评估虽然需要大量的手工工作，但仍然是大多数用户的黄金标准。一个显著的例子是 Chatbot Arena [[32](#bib.bib32)]，这是一个众包投票平台，用于收集对大型语言模型（LLM）性能的匿名投票，并计算
    ELO 分数以对这些模型进行排名。最终的排行榜²²2  https://leaderboard.lmsys.org/ 被广泛认为是 LLM 一般能力的可信指标。然而，在这个平台上进行可靠的模型评估必须有超过
    10k 人的投票，这需要相当多的时间和精力。因此，当新开发的模型出现时，它们通常难以迅速积累大量的投票。此外，对人工投票的强烈依赖限制了其在各种场景中的应用。例如，非英语语言的表现难以估计，因为平台上的大多数查询都是英语的。完全开放的参与也可能导致评估质量不均。
- en: '![Refer to caption](img/37c12ca80da6f69fe065a2d43f4a3b37.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37c12ca80da6f69fe065a2d43f4a3b37.png)'
- en: 'Figure 1: An illustration of Auto Arena of LLMs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLMs 的 Auto Arena 插图。
- en: 'To provide automatic, reliable, and human-like LLM evaluations, we propose
    Auto Arena of LLMs (Auto-Arena), a framework that automates the whole LLM evaluation
    process with LLM agents. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions"), the Auto-Arena framework consists of three stages. Firstly, an
    LLM examiner agent is tasked with generating questions, mimicking real-life users
    inputting queries. Secondly, two candidate LLMs interact with each other and engage
    in a multi-round peer battle by answering the seed question individually, criticizing
    the opponent’s weaknesses, and raising targeted follow-up queries to challenge
    the opponent further. During the multi-round battle process, the LLM’s true capabilities
    are drawn out and performance gaps become visible. Lastly, a committee of LLM
    judge agents collectively discusses and evaluates the ability of the two candidates,
    mimicking the human voting process. By automating the entire evaluation process
    with LLM agents, we alleviate data contamination concerns in static datasets,
    reduce single-model bias with collective decision-making, and avoid long wait
    times for new models entering the human voting platform.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提供自动、可靠且类人化的 LLM 评估，我们提出了 LLM 自动竞技场（Auto-Arena），这是一个通过 LLM 代理自动化整个 LLM 评估过程的框架。如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions") 所示，Auto-Arena 框架包括三个阶段。首先，一个
    LLM 考官代理负责生成问题，模拟真实用户输入查询。其次，两个候选 LLM 互相交流，并通过单独回答种子问题、批评对方的弱点以及提出有针对性的后续问题来进一步挑战对方，从而进行多轮对抗。在多轮对抗过程中，LLM
    的真实能力被展现出来，性能差距变得明显。最后，由 LLM 裁判代理组成的委员会共同讨论并评估两个候选者的能力，模拟人类投票过程。通过使用 LLM 代理自动化整个评估过程，我们缓解了静态数据集中的数据污染问题，减少了通过集体决策的单一模型偏见，并避免了新模型进入人类投票平台时的长时间等待。'
- en: To verify the evaluation framework, we run an extensive experiment with 17 models.
    Compared to static and model-based benchmarks, Auto-Arena increases the Spearman
    correlation with human preferences by 4.5%, resulting in state-of-the-art alignment.
    Before and after peer battles, the Spearman correlation with human preferences
    increases by 46.4%, verifying our hypothesis the peer battles can better display
    performance gaps. Before and after committee discussions, committee agreement
    increases by 20%, showcasing the effectiveness of the committee discussion mechanism.
    By studying the peer battles, we discover intriguing LLM agent behaviors such
    as self-improvement and competitive actions. Using Chinese as an example, we also
    show that such an automated framework is easily extendable to non-mainstream languages
    and domains. Along with the project, we release a leaderboard, a website with
    demos, and the codebase. We are actively maintaining the leaderboard and altering
    the questions to provide reliable results in a timely manner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证评估框架，我们对 17 个模型进行了广泛实验。与静态和模型基准相比，Auto-Arena 提高了与人类偏好的 Spearman 相关系数 4.5%，实现了最先进的对齐。在对抗之前和之后，与人类偏好的
    Spearman 相关系数增加了 46.4%，验证了我们关于对抗能更好地展示性能差距的假设。在委员会讨论之前和之后，委员会一致性提高了 20%，展示了委员会讨论机制的有效性。通过研究对抗，我们发现了一些有趣的
    LLM 代理行为，如自我改进和竞争行为。以中文为例，我们还展示了这样一个自动化框架可以轻松扩展到非主流语言和领域。与项目一起，我们发布了一个排行榜，一个包含演示的网站，以及代码库。我们正在积极维护排行榜并调整问题，以及时提供可靠的结果。
- en: 2 Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Since LLMs demonstrated general and zero-shot capabilities, researchers have
    been attempting to use one single strong LLM to judge other LLMs’ outputs. LLM-as-a-judge [[32](#bib.bib32)]
    introduces Chatbot Arena and MT-Bench, a multi-turn question set to mimic real-life
    user queries. Using a strong LLM judge, such as GPT-4, to evaluate the MT-Bench
    results achieves over 80% agreement with human preferences, showcasing the potential
    of such methods. Besides such usefulness evaluations, some attempt to use LLM-as-a-judge
    to examine the knowledge boundaries of LLMs. Language-Model-as-an-Examiner [[2](#bib.bib2)]
    constructs the LMExamQA dataset, which comprises of LM-generated, knowledge-intensive
    questions covering three cognitive levels, including knowledge memorization, knowledge
    comprehension, and knowledge analysis. These questions are required to be answered
    by the LLM itself correctly, limiting their complexity. Then, the LM evaluator
    and the candidate interact in a series of follow-up queries and the evaluator
    rates the responses on dimensions including accuracy and factuality. Multiple
    evaluators examine the model independently and vote for a final result. KIEval [[29](#bib.bib29)]
    also incorporates an LLM-powered “interactor” role to examine deep comprehension
    of knowledge, which is shown to mitigate contamination issues. By utilizing the
    LLMs to generate questions, these methods can effectively mitigate the contamination
    concerns on static datasets. However, such evaluations only examine the LLMs’
    capabilities on answering knowledge-intensive, closed-domain, and simpler questions,
    as they rely on the examiner’s internal knowledge. They also require the examiner
    to interact with each candidate individually, creating computational overheads
    and limiting the scope of queries.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自从大语言模型（LLMs）展示了通用和零样本能力后，研究人员一直在尝试使用一个强大的单一LLM来评估其他LLM的输出。LLM作为评判者[[32](#bib.bib32)]引入了Chatbot
    Arena和MT-Bench，一个多轮问题集来模拟真实用户查询。使用如GPT-4这样的强大LLM作为评判者来评估MT-Bench的结果，与人类偏好达成超过80%的一致，展示了这些方法的潜力。除了这种有用性评估之外，一些研究还尝试使用LLM作为评判者来检验LLM的知识边界。语言模型作为考官[[2](#bib.bib2)]构建了LMExamQA数据集，该数据集包含了由LLM生成的、知识密集的问题，覆盖了三个认知层次，包括知识记忆、知识理解和知识分析。这些问题要求LLM本身正确回答，从而限制了问题的复杂性。然后，LLM评估者与候选者进行一系列后续查询，评估者根据准确性和事实性等维度对回答进行评分。多个评估者独立检查模型并投票决定最终结果。KIEval[[29](#bib.bib29)]还引入了一个由LLM驱动的“互动者”角色来检查知识的深层理解，这已被证明可以缓解污染问题。通过利用LLM生成问题，这些方法可以有效缓解对静态数据集的污染担忧。然而，这些评估仅检查LLM在回答知识密集型、封闭域和较简单问题上的能力，因为它们依赖于考官的内部知识。它们还要求考官与每个候选者逐一互动，造成计算开销并限制了查询的范围。
- en: Moreover, using a single LLM judge still suffer from a bias towards LLM-generated
    summaries [[20](#bib.bib20)], inflated scores in multilingual evaluation [[14](#bib.bib14)],
    verbosity bias [[12](#bib.bib12)], and difficulties when evaluating candidates
    with close performance [[25](#bib.bib25)]. To mitigate these problems, some recent
    works propose using multiple agents for collaborative evaluation, mimicking a
    peer review process. Some works simulate different personas with the same LLM.
    DRPE [[28](#bib.bib28)] uses multi-roleplayer prompting to mimic different roles
    with the same base model and integrate multiple outputs as votes for the final
    results. ChatEval [[6](#bib.bib6)] simulates different evaluator personas with
    the same base model to engage in debates, reaching a final evaluation result.
    While these methods come from the same motivation as our committee discussion
    component, they are still built on existing static datasets and do not mitigate
    the model-specific bias as they still use the same LLM to simulate different personas.
    To further incorporate different LLMs as evaluators, PRD [[17](#bib.bib17)] allows
    2 LLMs to discuss an evaluation and assigns higher voting weights to the LLM reviewers
    with stronger capabilities. Peer-review-in-LLMs [[22](#bib.bib22)] further optimizes
    the voting weights as a learnable parameter. WideDeep [[30](#bib.bib30)] sets
    up a multi-layer neural network structure where each LLM evaluator functions as
    neurons. PRE [[8](#bib.bib8)] selects a small group of reviewers to produce evaluations
    individually and then asks a “chair” to produce the final evaluation using the
    aggregated evaluations. They show that the multi-agent approach effectively mitigates
    single-model bias. This line of work is similar to our “LLM judge committee”.
    However, they are still limited on static datasets and specific domains. Moreover,
    they lack the collective discussion component, which is shown to improve agreement
    significantly.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用单个 LLM 法官仍然存在对 LLM 生成的摘要的偏见 [[20](#bib.bib20)]，多语言评估中的夸大评分 [[14](#bib.bib14)]，冗长偏见
    [[12](#bib.bib12)]，以及评估表现接近的候选人时的困难 [[25](#bib.bib25)]。为了解决这些问题，一些近期的工作提出使用多个代理进行协作评估，模拟同行评审过程。一些工作使用相同
    LLM 模拟不同的人物角色。DRPE [[28](#bib.bib28)] 使用多角色提示来模拟相同基础模型的不同角色，并将多个输出整合为最终结果的投票。ChatEval
    [[6](#bib.bib6)] 使用相同基础模型模拟不同的评估者角色进行辩论，得出最终评估结果。虽然这些方法来源于与我们的委员会讨论组件相同的动机，但它们仍然基于现有的静态数据集，并且由于使用相同的
    LLM 模拟不同的角色，因此未能减少模型特定的偏见。为了进一步引入不同的 LLM 作为评估者，PRD [[17](#bib.bib17)] 允许 2 个 LLM
    讨论评估，并为能力更强的 LLM 评审者分配更高的投票权重。Peer-review-in-LLMs [[22](#bib.bib22)] 进一步将投票权重优化为可学习参数。WideDeep
    [[30](#bib.bib30)] 设置了一个多层神经网络结构，其中每个 LLM 评估者作为神经元。PRE [[8](#bib.bib8)] 选择一小组评审员分别进行评估，然后要求“主席”使用汇总的评估结果产生最终评估。研究表明，多代理方法有效减少了单模型偏见。这一研究方向与我们的“LLM
    法官委员会”类似。然而，它们仍然局限于静态数据集和特定领域。此外，它们缺乏集体讨论组件，而集体讨论已被证明显著改善一致性。
- en: Compared to using LLMs for evaluations, LLM-debate is a relatively new research
    area. Cohen et al. [[10](#bib.bib10)] shows that LLM interactions and cross-examination
    can effectively discover factual errors. Debate [[11](#bib.bib11)] shows that
    multi-agent debate can improve factuality and reasoning abilities. MAD [[19](#bib.bib19)]
    shows that LLM debate can encourage divergent thinking, which is helpful for tasks
    that require deep levels of contemplation. Khan et al. [[16](#bib.bib16)] shows
    that even non-expert weak LLMs can supervise expert LLMs if we allow the two LLM
    experts to engage in debates. Moreover, Zhao et al. [[31](#bib.bib31)] and Gu
    et al. [[13](#bib.bib13)] show interesting case studies where LLMs are engaged
    in simulated competitive environments, where they demonstrate human-like strategies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于使用 LLM 进行评估，LLM 辩论是一个相对较新的研究领域。Cohen 等人 [[10](#bib.bib10)] 表明 LLM 互动和交叉审查可以有效发现事实错误。辩论
    [[11](#bib.bib11)] 显示多代理辩论可以提升事实性和推理能力。MAD [[19](#bib.bib19)] 证明 LLM 辩论可以鼓励发散思维，这对需要深度思考的任务有帮助。Khan
    等人 [[16](#bib.bib16)] 表明即使是非专家的弱 LLM 也可以监督专家 LLM，只要我们允许两个 LLM 专家参与辩论。此外，Zhao 等人
    [[31](#bib.bib31)] 和 Gu 等人 [[13](#bib.bib13)] 展示了有趣的案例研究，其中 LLM 参与模拟竞争环境，展示了类似人类的策略。
- en: 3 The Auto Arena Framework
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 自动竞技场框架
- en: 'The Auto-Arena framework consists of three stages: question generation, peer
    battles, and committee discussions. These three stages are run sequentially and
    fully automated with LLM-based agents. The overall design of our Auto-Arena framework
    is presented in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    All prompts are included in Appendix [A.1](#A1.SS1 "A.1 Prompts Used ‣ Appendix
    A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'Auto-Arena 框架由三个阶段组成：问题生成、同行辩论和委员会讨论。这三个阶段按顺序运行，并由基于LLM的代理完全自动化。我们 Auto-Arena
    框架的整体设计如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions") 所示。所有提示包含在附录
    [A.1](#A1.SS1 "A.1 Prompts Used ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions") 中。'
- en: 3.1 Question Generation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题生成
- en: 'For debate questions, as using a static dataset could incur data contamination
    concerns and result in unfair evaluations, we ask an LLM examiner agent to dynamically
    generate questions. The examiner agent could be any capable LLM. As the question
    simply serves as a starting point for the peer battle, which is the primary focus
    of evaluation, the question quality has relatively less impact on the overall
    evaluation framework. Similar to MT-Bench [[32](#bib.bib32)], we utilize 8 common
    categories in real-life conversations: writing, roleplay, extraction, reasoning,
    math, coding, STEM knowledge, and humanities/social science knowledge. In the
    prompt, the examiner is provided with a sample question and encouraged to generate
    diverse and difficult questions to ensure the depth and width of the debates examined.
    A subset of the generated questions is shown in Appendix [A.2](#A1.SS2 "A.2 Example
    Questions Generated ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '对于辩论问题，由于使用静态数据集可能会导致数据污染和不公平评估，我们要求一个LLM考官代理动态生成问题。考官代理可以是任何有能力的LLM。由于问题仅作为同行辩论的起点，而同行辩论是评估的主要焦点，因此问题的质量对整体评估框架的影响相对较小。类似于MT-Bench
    [[32](#bib.bib32)]，我们使用了8种真实对话中的常见类别：写作、角色扮演、提取、推理、数学、编码、STEM知识和人文/社会科学知识。在提示中，考官提供了一个示例问题，并鼓励生成多样化和困难的问题，以确保辩论的深度和广度。生成问题的子集见附录
    [A.2](#A1.SS2 "A.2 Example Questions Generated ‣ Appendix A Appendix ‣ Auto Arena
    of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")。'
- en: 'Specifically, as the examiner agent will also participate in the following
    debates, previous methods [[2](#bib.bib2)] could incur self-enhancement bias as
    the examiner agents only devise questions that they are confident about. We try
    to avoid such bias with the following two designs: 1\. We do not disclose to the
    examiner that it will participate in this tournament. 2\. We do not ask the examiner
    to only generate questions that it can solve. Thus, the resulting questions should
    not cause much enhancement bias for the examiner agent itself.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，由于考官代理也将参与后续辩论，之前的方法 [[2](#bib.bib2)] 可能会引发自我增强偏差，因为考官代理只会设计他们自信的问题。我们尝试通过以下两种设计来避免这种偏差：1\.
    我们不会告知考官它将参与此赛事。2\. 我们不会要求考官仅生成自己能解决的问题。因此，生成的问题不应对考官代理自身造成太大的增强偏差。
- en: 3.2 Peer Debate
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 同行辩论
- en: '![Refer to caption](img/5717dc86e1696f86303cba72779ad5db.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5717dc86e1696f86303cba72779ad5db.png)'
- en: 'Figure 2: The process of a Lincoln-Douglas-style peer-battle with the actions
    used.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：林肯-道格拉斯风格的同行辩论过程及其使用的行动。
- en: After the questions are drafted, we conduct peer battles around these questions
    among the candidate LLMs. In one peer battle, two candidate LLMs debate around
    the given question, point out the opponent’s weaknesses, and raise follow-up questions
    that the opponent may fail to respond to.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题草拟后，我们会在候选LLMs之间围绕这些问题进行同行辩论。在一次同行辩论中，两名候选LLMs围绕给定的问题辩论，指出对手的弱点，并提出对手可能无法回应的后续问题。
- en: 'In the peer battle, each candidate LLM can utilize 4 types of actions: 1\.
    Think, where the candidate thinks about the question and plans its strategy. This
    action is hidden to the opponent at all times. 2\. Respond, where the candidate
    responds to the aforementioned question. 3\. Criticize, where the candidate points
    out the loopholes and mistakes in the opponent’s previous responses. 4\. Raise,
    where the candidate raises follow-up questions that are specifically designed
    to make the opponent expose its weaknesses.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在对等对战中，每个候选人LLM可以使用4种类型的动作：1\. 思考，候选人思考问题并制定策略。该动作在任何时候都对对手隐藏。2\. 回应，候选人对上述问题作出回应。3\.
    批评，候选人指出对手之前回应中的漏洞和错误。4\. 提问，候选人提出后续问题，特别设计用于揭示对手的弱点。
- en: 'The peer battle is designed according to the Lincoln-Douglas debate format
    ³³3  [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format),
    which is a competitive one-to-one debate used for popular competitions such as
    National Speech and Debate Association competitions. Overall, the peer battle
    consists of 3 rounds, where the candidates take turns to speak. The entire dialogue
    history is visible to both candidates. The process is illustrated in Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Peer Debate ‣ 3 The Auto Arena Framework ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    In the first round, A gives an initial response to the examiner’s question; B
    criticizes the weaknesses in A’s response and raises a targeted follow-up question;
    and A responds to B’s question. In the second round, A and B are reversed: B gives
    an initial response to the examiner’s question (without seeing A’s response);
    A criticizes and raises questions; and B responds to A’s question. In the third
    round, A and B cross-examine each other. A starts by criticizing B’s previous
    loopholes and raises follow-up questions. After responding, B also criticizes
    A’s loopholes and raises questions. A concludes the battle by responding again.
    In this process, both A and B get an equal number of each action to ensure fairness.
    To further reduce position bias, A and B’s order is randomly shuffled at the beginning
    of each debate.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对等对战的设计遵循**林肯-道格拉斯辩论格式**³³3  [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format)，这是一种用于全国演讲和辩论协会等流行比赛的面对面一对一辩论格式。总体而言，对等对战包括3轮，候选人轮流发言。整个对话历史对两位候选人都是可见的。该过程在图[2](#S3.F2
    "图 2 ‣ 3.2 对等辩论 ‣ 3 自动竞技场框架 ‣ LLM的自动化评估：通过代理对等对战和委员会讨论")中进行了说明。在第一轮中，A对考官的问题给出初步回答；B批评A回答中的缺陷并提出有针对性的后续问题；A回应B的问题。在第二轮中，A和B交换角色：B对考官的问题给出初步回答（不查看A的回答）；A进行批评并提出问题；B回应A的问题。在第三轮中，A和B进行交叉询问。A开始时批评B之前的漏洞并提出后续问题。回应后，B也批评A的漏洞并提出问题。A通过再次回应来总结对战。在此过程中，A和B的每个动作数量相等，以确保公平。为了进一步减少位置偏见，A和B的顺序在每次辩论开始时会随机打乱。
- en: Depending on which turn it is, we provide an action guide to the candidate,
    specifying the objectives and corresponding actions for this turn. Similar to
    human debate competitions, we time the candidates by providing a maximum length,
    which is also specified in the prompts. Any responses beyond the required length
    will be cut off. This design also mitigates verbosity bias in LLM-as-a-judge [[32](#bib.bib32)],
    where LLM judges prefer longer and more verbose responses.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据轮次，我们向候选人提供行动指南，明确本轮的目标和相应的行动。类似于人类辩论比赛，我们通过设置最长时间来计时，这在提示中也有说明。超出规定长度的回答将被截断。这个设计也减少了LLM作为裁判时的冗长偏见[[32](#bib.bib32)]，在这种情况下，LLM裁判倾向于更长、更冗长的回答。
- en: 3.3 Committee Discussions
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 委员会讨论
- en: After the peer battle takes place, a committee of LLM judge agents will collectively
    determine the winner. The committee is always selected as the best five LLMs according
    to the current ranking. In the first round, the committee is initialized with
    MMLU [[15](#bib.bib15)] scores to approximate LLM performances. They will first
    be asked to read through the battle history, elaborate judgment reasons, and give
    a verdict on whether A is better, or B is better, or if there is a tie.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在对战结束后，一组 LLM 评审代理将共同决定赢家。委员会总是根据当前排名选择最佳的五个 LLM。在第一轮中，委员会以 MMLU [[15](#bib.bib15)]
    分数初始化，以近似 LLM 性能。他们将首先阅读对战历史，详细阐述判决理由，并给出是否 A 更好、B 更好，还是平局的裁决。
- en: After the initial judgments are formed, the committee engages in a discussion.
    Each judge will read the other judge’s verdicts and decide whether to adjust the
    ratings and elaborate on the reasons. The collective intelligence introduces diverse
    viewpoints, improves judgment quality, and mitigates single-model bias. Finally,
    the winning candidate is decided by the majority voting of the discussed judgments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步判决形成后，委员会进行讨论。每位评审将阅读其他评审的裁决，并决定是否调整评分并详细说明原因。集体智慧引入了多样的观点，提高了判断质量，并减轻了单一模型偏差。最终，胜出的候选人由讨论中的多数票决定。
- en: For logical-reasoning questions that have ground-truth answers (reasoning, code,
    math), LLM-as-a-judge is known to show weak performances in judging the quality
    of responses. We adopt prior approaches to establish the reference-based judge [[32](#bib.bib32)].
    Specifically, we utilize the strongest model (according to the current ranking)
    to generate a reference answer and provide it to the judge when evaluating the
    peer battle.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有真实答案（推理、代码、数学）的逻辑推理问题，LLM 作为评审在判断响应质量时表现较弱。我们采用了之前的方法来建立基于参考的评审 [[32](#bib.bib32)]。具体而言，我们利用最强的模型（根据当前排名）生成参考答案，并在评估对战时提供给评审。
- en: 4 Using Auto-Arena to Derive Trustworthy Rankings
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用 Auto-Arena 得出可靠的排名
- en: 4.1 Experimental Setup
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Model Selection:'
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型选择：
- en: For the main experiment, we first select 7 models from the top 30 list on Chatbot
    Arena with more than 10k votes each. The models are selected to be the best or
    the latest models from each popular model family at the time of experiments. To
    construct a comprehensive leaderboard⁴⁴4[https://huggingface.co/spaces/Auto-Arena/Leaderboard](https://huggingface.co/spaces/Auto-Arena/Leaderboard),
    we further consider 10 more models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主要实验，我们首先从 Chatbot Arena 前30名中选择7个模型，每个模型获得超过10k的投票。这些模型是在实验时选择的最佳或最新模型家族中的模型。为了构建一个全面的排行榜⁴⁴4[https://huggingface.co/spaces/Auto-Arena/Leaderboard](https://huggingface.co/spaces/Auto-Arena/Leaderboard)，我们进一步考虑了10个模型。
- en: 'Baselines:'
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准线：
- en: For the baselines, we consider widely used evaluation benchmarks, including
    static datasets with fixed metrics and model-based metrics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基准线，我们考虑广泛使用的评估基准，包括具有固定指标的静态数据集和基于模型的指标。
- en: '1.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Static datasets with fixed metrics
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态数据集与固定指标
- en: (a)
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: MMLU (Massive Multitask Language Understanding) [[15](#bib.bib15)], an extensive
    benchmark dataset that covers 57 subjects and tests both world knowledge and problem-solving
    ability.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MMLU（大规模多任务语言理解） [[15](#bib.bib15)]，一个覆盖57个学科的广泛基准数据集，测试世界知识和解决问题的能力。
- en: (b)
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: OpenLLM Leaderboard [[3](#bib.bib3)], a leaderboard calculated by averaging
    performance metrics on 6 key benchmarks, including MMLU.
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenLLM 排行榜 [[3](#bib.bib3)]，一个通过在6个关键基准上的性能指标进行平均计算的排行榜，其中包括 MMLU。
- en: '2.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Static datasets with model-based metrics
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态数据集与基于模型的指标
- en: (a)
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: MT-Bench [[32](#bib.bib32)], a multi-turn question set consisting of 80 questions.
    Model responses are graded by GPT-4.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-Bench [[32](#bib.bib32)]，一个包含80个问题的多轮问答集。模型的响应由 GPT-4 评分。
- en: (b)
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: MT-Bench Hard [[18](#bib.bib18)], a benchmark dataset with 1,000 challenging
    user queries collected on Chatbot Arena. Model responses are graded by GPT-4.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-Bench Hard [[18](#bib.bib18)]，一个包含1,000个挑战性用户查询的基准数据集，收集于 Chatbot Arena。模型的响应由
    GPT-4 评分。
- en: (c)
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: Length-controlled AlpacaEval [[12](#bib.bib12)], an automatic benchmark based
    on the AlpacaFarm evaluation set, which tests the ability of models to follow
    general user instructions. Model responses are graded by GPT-4.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长度控制的 AlpacaEval [[12](#bib.bib12)]，一个基于 AlpacaFarm 评估集的自动基准，测试模型遵循一般用户指令的能力。模型的响应由
    GPT-4 评分。
- en: 'Setup:'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置：
- en: 'Among the 7 participants, we conduct a swiss-style tournament: For $n$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在7个参与者中，我们进行瑞士制比赛：对于 $n$。
- en: Each pair of candidates engage in 40 peer battles, with 5 questions from each
    of the 8 categories. The questions are generated by GPT-4\. As each battle consists
    of 3 rounds (each candidate speaks for 4 times), we expect the competition scale
    to be approximately the same as MT-Bench (80 questions, each candidate speaks
    twice).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每对候选者进行40轮对战，每轮包含8个类别中的5个问题。这些问题由GPT-4生成。由于每场对战由3轮组成（每位候选者发言4次），我们期望比赛规模与MT-Bench大致相同（80个问题，每位候选者发言两次）。
- en: In the tournament, the rating scores are provided by calculating the ELO rating
    system [[1](#bib.bib1), [4](#bib.bib4)], which has become the standard practice
    in competitive games such as chess. Similar to the Chatbot Arena score calculation
    procedure [[7](#bib.bib7)], we compute the Bradley-Terry (BT) coefficients [[5](#bib.bib5)]
    for better statistical estimation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在锦标赛中，评分通过计算ELO评分系统[[1](#bib.bib1), [4](#bib.bib4)]来提供，这已成为象棋等竞技游戏的标准做法。类似于Chatbot
    Arena得分计算程序[[7](#bib.bib7)]，我们计算Bradley-Terry（BT）系数[[5](#bib.bib5)]以获得更好的统计估计。
- en: We initialize the Swiss tournament rankings according to MMLU scores, which
    is a static approximate of model performances. At the end of each pairing, we
    re-calculate ELO scores of current models. The committee is selected to be the
    remaining 5 models (besides the candidates). When the participant number increases,
    the committee is selected as the best 5 LLMs based on current ELO rankings. After
    the initial judgments, the committee members engage in one round of discussion.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据MMLU得分初始化瑞士锦标赛排名，这是一种静态的模型表现近似。在每次配对结束时，我们重新计算当前模型的ELO得分。委员会由剩余的5个模型（除候选模型外）组成。当参与者数量增加时，委员会被选为当前ELO排名中最优秀的5个LLM。在初步判断之后，委员会成员进行一轮讨论。
- en: '4.2 Results: Alignment with Human Preferences'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果：与人类偏好的对齐
- en: 'Table 1: Correlation analysis on evaluation benchmarks on 7 LLMs.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：7个LLM在评估基准上的相关性分析。
- en: '|  | Spearman Correlation |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | Spearman相关性 |'
- en: '| --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MMLU to Arena | 89.3% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| MMLU到Arena | 89.3% |'
- en: '| OpenLLM to Arena | 89.3% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM到Arena | 89.3% |'
- en: '| MT-Bench to Arena | 82.9% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench到Arena | 82.9% |'
- en: '| MT-Bench-Hard to Arena | 89.3% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench-Hard到Arena | 89.3% |'
- en: '| LC-AlpacaEval to Arena | 90.0% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval到Arena | 90.0% |'
- en: '| *Auto-Arena to Arena* | 96.4% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| *Auto-Arena到Arena* | 96.4% |'
- en: 'We regard Chatbot Arena (referred to as “Arena” in the following tables) scores
    as a trustworthy indicator of human preferences and the general capabilities of
    LLMs. Table [1](#S4.T1 "Table 1 ‣ 4.2 Results: Alignment with Human Preferences
    ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions") shows the
    Spearman correlations with Chatbot Arena scores achieved by various benchmarks.
    As all benchmarks evaluate only in English, we compare with the English-only Chatbot
    Arena scores. As a result, we observe that both static and model-based baselines
    result in a similar level of Spearman correlation around 90%, with LC-AlpacaEval
    slightly surpassing other benchmarks. Then, Auto-Arena is able to improve the
    correlation to 96.4%, outperforming SOTA performance by 6.4%. Notably, among all
    benchmarks, Auto-Arena is the only one that requires no human efforts, eliminating
    the need for manual dataset collection. The high alignment with human preferences
    could originate from the human-like design, which effectively mimics the human
    users’ voting processes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '我们认为Chatbot Arena（下表中称为“Arena”）得分是人类偏好和LLM总体能力的可靠指标。表[1](#S4.T1 "Table 1 ‣
    4.2 Results: Alignment with Human Preferences ‣ 4 Using Auto-Arena to Derive Trustworthy
    Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions")显示了各种基准与Chatbot Arena得分的Spearman相关性。由于所有基准仅评估英文，我们与仅英文的Chatbot
    Arena得分进行比较。结果表明，无论是静态基准还是基于模型的基准，Spearman相关性都在90%左右，LC-AlpacaEval略微超越其他基准。然后，Auto-Arena将相关性提高到96.4%，比SOTA表现高出6.4%。值得注意的是，在所有基准中，Auto-Arena是唯一一个不需要人工干预的基准，消除了手动数据集收集的需求。与人类偏好的高度一致可能源于其类人设计，能够有效模仿人类用户的投票过程。'
- en: 4.3 Analysis on Peer Battles
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 对对战的分析
- en: 'Table 2: Spearman Correlation between Auto-Arena and Chatbot Arena (“Arena”)
    With and Without Peer Battles.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：Auto-Arena和Chatbot Arena（“Arena”）之间的Spearman相关性（有无对战）。
- en: '|  | Without Peer Battles | With Peer Battles |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 无对战 | 有对战 |'
- en: '| --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Auto-Arena to Arena | 50.0% | 96.4% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Arena到Arena | 50.0% | 96.4% |'
- en: 'To investigate the effect of "peer battles" on the overall evaluation quality,
    we conduct an ablation study. As a baseline, we simply ask the committee to collectively
    evaluate the two candidates’ initial responses to the raised question. The procedure
    would be similar to model-based metrics such as MT-Bench and AlpacaEval. However,
    as we only included 40 questions, the evaluation is expected to be less reliable.
    The ablation results are shown in Table [2](#S4.T2 "Table 2 ‣ 4.3 Analysis on
    Peer Battles ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena
    of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    Without the peer battle component, the Spearman correlation with human preferences
    drops from 96.4% to 50.0%. This result proves the importance of the peer battles,
    during which the performance gaps between candidates become more visible and robust
    to judges. Thus, peer battles can improve evaluation reliability and alignment
    with human preferences.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '为了调查“同行对抗”对总体评价质量的影响，我们进行了一个消融研究。作为基线，我们简单地要求委员会集体评估两个候选人对提出问题的初步回答。这个程序类似于基于模型的指标，如
    MT-Bench 和 AlpacaEval。然而，由于我们只包含了 40 个问题，评估的可靠性预计会较低。消融结果显示在表 [2](#S4.T2 "Table
    2 ‣ 4.3 Analysis on Peer Battles ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") 中。没有同行对抗组件的情况下，与人工偏好的 Spearman 相关性从 96.4% 降至 50.0%。这一结果证明了同行对抗的重要性，在此过程中，候选人之间的性能差距变得更加明显，并且对评审者更具鲁棒性。因此，同行对抗可以提高评估的可靠性和与人工偏好的一致性。'
- en: 4.4 Analysis on Committee Discussions
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 委员会讨论分析
- en: '![Refer to caption](img/c11c086b19932a74b8282497bf7615b4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/c11c086b19932a74b8282497bf7615b4.png)'
- en: 'Figure 3: Cohen’s Kappa Agreement Among Judges Before Committee Discussions.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：委员会讨论前评审者之间的 Cohen’s Kappa 一致性。
- en: '![Refer to caption](img/38f4cb7b54c0db43df0854b3bc5a5ad9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/38f4cb7b54c0db43df0854b3bc5a5ad9.png)'
- en: 'Figure 4: Cohen’s Kappa Agreement Among Judges After One Round of Committee
    Discussion.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：委员会讨论一轮后的 Cohen’s Kappa 一致性。
- en: 'The committee discussion component is designed to introduce diverse viewpoints
    and produce more consistent verdicts. As shown in Figure [4](#S4.F4 "Figure 4
    ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy
    Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions"), the Cohen’s Kappa agreement [[21](#bib.bib21)] among
    judges before committee discussion is very low, averaging 0.10, which indicates
    slight agreement. We notice that weak model judges and strong model judges has
    an especially low agreement, such as GPT-4 and Llama -2\. This shows that general
    model capabilities could result in significant performance gaps when used as judges.
    After one round of Committee Discussion, Figure [4](#S4.F4 "Figure 4 ‣ 4.4 Analysis
    on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣
    Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") shows an overall improvement in agreement across all judge combinations.
    In the discussion process, judges are exposed to more viewpoints, among which
    some may be convincing enough to result in a change in verdict. After discussions,
    the average Cohen’s Kappa increases to 0.38, indicating fair agreement.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '委员会讨论组件旨在引入多样化的观点，并产生更一致的裁决。如图 [4](#S4.F4 "Figure 4 ‣ 4.4 Analysis on Committee
    Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of
    LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")
    所示，委员会讨论前评审者之间的 Cohen’s Kappa 一致性 [[21](#bib.bib21)] 非常低，平均值为 0.10，表明一致性较差。我们注意到，弱模型评审者和强模型评审者之间的协议特别低，例如
    GPT-4 和 Llama -2。这表明，通用模型能力可能会导致在作为评审者时出现显著的性能差距。在进行了一轮委员会讨论后，图 [4](#S4.F4 "Figure
    4 ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy
    Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions") 显示所有评审者组合的一致性整体有所改善。在讨论过程中，评审者接触到更多的观点，其中一些可能足够有说服力，以导致裁决的改变。讨论后，平均
    Cohen’s Kappa 增加到 0.38，表明一致性较好。'
- en: 'Table 3: Probability of Agreement among Judges. Agreement is defined as the
    mean probability of two random judges agreeing with each other.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：评审者之间的一致性概率。一致性定义为两名随机评审者彼此达成一致的平均概率。
- en: '|  | Auto Arena | Auto Arena | MT-Bench |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | Auto Arena | Auto Arena | MT-Bench |'
- en: '|  | (Before discussion) | (After discussion) | Human evaluation |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | （讨论前） | （讨论后） | 人工评估 |'
- en: '| Agreement | 48% | 68% | 67% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 一致性 | 48% | 68% | 67% |'
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the agreement
    probability among judges. Agreement probability is defined as the mean probability
    of two random judges agreeing with each other. After committee discussion, the
    agreement increases by 20%, matching the agreement level among human annotators
    on MT-Bench. This observation indicates that committee discussions can improve
    the quality of judgments significantly in terms of consistency and reliability,
    matching human-level performance.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S4.T3 "Table 3 ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using Auto-Arena
    to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions") 显示了评审之间的协议概率。协议概率定义为两个随机评审彼此一致的平均概率。在委员会讨论之后，协议提高了
    20%，与 MT-Bench 上人类标注者的协议水平相匹配。这个观察结果表明，委员会讨论可以显著提高判断的质量，提升一致性和可靠性，达到人类水平的表现。'
- en: 4.5 Adding Models to the Leaderboard
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 添加模型到排行榜
- en: '![Refer to caption](img/ca7a9ad542aeba5bd31ca016bd14b153.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ca7a9ad542aeba5bd31ca016bd14b153.png)'
- en: 'Figure 5: Elo Score Changes of Adding Llama-3 to the Existing Ranking of 7
    Models.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 将 Llama-3 添加到现有 7 个模型排名中的 Elo 分数变化。'
- en: '![Refer to caption](img/53fc1e7831eb0424a117a5198e51f05e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/53fc1e7831eb0424a117a5198e51f05e.png)'
- en: 'Figure 6: Elo Scores of 11 Models by Auto-Arena on Chinese.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: Auto-Arena 对 11 个模型在中文上的 Elo 分数。'
- en: '![Refer to caption](img/95994034a6cc6cf9eeab1156a02a3169.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/95994034a6cc6cf9eeab1156a02a3169.png)'
- en: 'Figure 7: Elo Scores of 17 Models by Auto-Arena on English.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: Auto-Arena 对 17 个模型在英文上的 Elo 分数。'
- en: To add a new candidate to the finished tournament, we ask it to debate with
    $log_{2}(n)$ is the number of total participants after adding the new candidate.
    We initialize the first pairing by asking the new candidate to debate with the
    opponent with the most similar MMLU score.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要将新候选者添加到完成的比赛中，我们要求它与 $log_{2}(n)$ 是添加新候选者后总参与者的数量进行辩论。我们通过要求新候选者与 MMLU 分数最相近的对手进行首次配对。
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the ELO
    score changes of adding a new participant (Llama-3, which was newly released)
    to the existing ranking. It has $log_{2}(8)=3$.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#S4.F5 "Figure 5 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using Auto-Arena
    to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions") 显示了将新参与者（Llama-3，刚刚发布）添加到现有排名中的
    ELO 分数变化。它有 $log_{2}(8)=3$。'
- en: 'Therefore, we repeat the process and add a total of 10 models to the existing
    tournament, resulting in a comprehensive leaderboard. We choose the best-performing
    and newest models from each major model family in the top 20 list on Chatbot Arena.
    Moreover, we add 4 mainstream Chinese LLMs that are not on Chatbot Arena, which
    are GLM, SenseChat, Minimax, and Wenxin. Figure [7](#S4.F7 "Figure 7 ‣ 4.5 Adding
    Models to the Leaderboard ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") shows the overall ELO scores produced by Auto-Arena on the 17 models.
    On models included in Chatbot Arena, we could recover rankings with 92.3% accuracy.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们重复这个过程，并在现有的比赛中增加了总共 10 个模型，形成了一个全面的排行榜。我们从 Chatbot Arena 的前 20 名列表中选择表现最佳且最新的模型。此外，我们还增加了
    4 个不在 Chatbot Arena 上的主流中文 LLM，这些是 GLM、SenseChat、Minimax 和 Wenxin。图 [7](#S4.F7
    "Figure 7 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using Auto-Arena to Derive
    Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent
    Peer-battles and Committee Discussions") 显示了 Auto-Arena 对 17 个模型生成的整体 ELO 分数。在
    Chatbot Arena 中包含的模型，我们能够以 92.3% 的准确率恢复排名。'
- en: 'Table 4: Correlation analysis on evaluation benchmarks on 17 LLMs after extension.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 扩展后的 17 个 LLM 的评估基准相关性分析。'
- en: '|  | Spearman Correlation |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | Spearman 相关性 |'
- en: '| --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MMLU to Arena | 90.1% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MMLU 到 Arena | 90.1% |'
- en: '| OpenLLM to Arena | 85.7% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM 到 Arena | 85.7% |'
- en: '| MT-Bench to Arena | 89.3% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench 到 Arena | 89.3% |'
- en: '| MT-Bench-Hard to Arena | 86.7% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench-Hard 到 Arena | 86.7% |'
- en: '| LC-AlpacaEval to Arena | 90.3% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval 到 Arena | 90.3% |'
- en: '| *Auto-Arena to Arena* | 94.5% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| *Auto-Arena 到 Arena* | 94.5% |'
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the Spearman
    correlations on the set of 17 models after expansion. Auto-Arena remains the model
    most aligned with human preferences by a margin of 4%. Therefore, Auto-Arena of
    LLMs is generalizable and robust for maintaining a leaderboard on a large number
    of LLMs.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4](#S4.T4 "表 4 ‣ 4.5 将模型添加到排行榜 ‣ 4 使用 Auto-Arena 导出可信排名 ‣ Auto Arena 的 LLMs：通过代理对战和委员会讨论自动化
    LLM 评估") 展示了扩展后的 17 个模型的 Spearman 相关性。Auto-Arena 仍然是与人类偏好最一致的模型，领先幅度为 4%。因此，Auto-Arena
    的 LLMs 在维护大量 LLM 排行榜方面是具有普遍性和鲁棒性的。
- en: 4.6 Easy Extension to Other Domains and Languages
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 易于扩展到其他领域和语言
- en: As Auto-Arena of LLMs is fully automatic, it can be easily adapted to evaluate
    LLMs in non-mainstream domains or languages. As a case study, we conduct a Chinese
    tournament on 11 out of the 17 models that are claimed to have multi-lingual proficiency.
    The only adaptation effort required is translating the prompts into Chinese. Then,
    the examiner automatically generates questions in Chinese and the candidates battle
    in Chinese. Similarly, for adaptation to another domain, the only effort is to
    change the “domain” specification in the examiner’s prompts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Auto-Arena 的 LLMs 完全自动化，因此可以轻松适应评估非主流领域或语言的 LLMs。作为案例研究，我们对 17 个声称具有多语言能力的模型中的
    11 个进行了中文比赛。唯一需要适应的工作是将提示翻译成中文。然后，考官自动生成中文问题，候选人用中文对战。类似地，对于其他领域的适应，唯一需要做的工作是更改考官提示中的“领域”规格。
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the ELO
    scores derived by Auto-Arena for Chinese evaluation. We notice that the ranking
    differs significantly from English results. For example, SenseChat-5, an LLM specifically
    trained for Chinese usage, is able to improve from 10th place (in English) to
    1st place (in Chinese). On the contrary, Llama-3-70b, which only uses 5% multilingual
    data during pretraining, drops from 4th place (in English) to 10th place (in Chinese).
    As Chinese evaluation benchmarks are limited, we compare to the Chinese-only leaderboard
    on Chatbot Arena, which constitute for 10.36% of all collected dialogues. On the
    7 models that are also included in Chatbot Arena, Auto-Arena recovers their ELO
    scores with 92.86% correlation and restores rankings with 90.5% accuracy.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S4.F7 "图 7 ‣ 4.5 将模型添加到排行榜 ‣ 4 使用 Auto-Arena 导出可信排名 ‣ Auto Arena 的 LLMs：通过代理对战和委员会讨论自动化
    LLM 评估") 展示了 Auto-Arena 为中文评估推导的 ELO 分数。我们注意到，排名与英文结果有显著差异。例如，专门针对中文使用训练的 LLM
    SenseChat-5，能够从第 10 位（英文）提升到第 1 位（中文）。相反，Llama-3-70b，在预训练过程中仅使用了 5% 的多语言数据，其排名从第
    4 位（英文）降到第 10 位（中文）。由于中文评估基准有限，我们与 Chatbot Arena 的中文排行榜进行比较，该排行榜占所有收集对话的 10.36%。在
    Chatbot Arena 中的 7 个模型中，Auto-Arena 恢复了它们的 ELO 分数，相关性为 92.86%，排名恢复的准确率为 90.5%。
- en: 5 Investigation of LLM’s Behaviors in Competitive Peer Battles
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 竞争对战中的 LLM 行为调查
- en: Beyond quantitative analysis, we take a deeper look into the peer battles and
    find several interesting behaviors of LLM agents in competitive environments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定量分析外，我们还深入研究了对战中的同行行为，发现了 LLM 代理在竞争环境中的几个有趣行为。
- en: '![Refer to caption](img/88ed4356e75ea12fa1485602e41a6ca4.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/88ed4356e75ea12fa1485602e41a6ca4.png)'
- en: 'Figure 8: LLM agents display competitive behaviors in peer battles.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：LLM 代理在对战中展示竞争行为。
- en: '![Refer to caption](img/9f6aa4c2e20b685fa1b4af83686e1815.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f6aa4c2e20b685fa1b4af83686e1815.png)'
- en: 'Figure 9: LLM agents learn from each other in peer battles.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：LLM 代理在对战中相互学习。
- en: 5.1 LLM can display competitive behaviors
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LLM 可以展示竞争行为
- en: 'The example in Figure [9](#S5.F9 "Figure 9 ‣ 5 Investigation of LLM’s Behaviors
    in Competitive Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with
    Agent Peer-battles and Committee Discussions") shows excerpts of a peer battle
    around question: “how many unique ways to arrange letters in ‘Letter’.” Candidate
    A (powered by Yi-34B-Chat) gives a wrong answer as it miscounts occurrences for
    repeated letters and miscalculates factorials. The opponent B (powered by Claude-3-Haiku)
    quickly and precisely points out these two issues and skillfully raised a follow-up
    that targets A’s weaknesses: “how about the word ‘BANANA’?” Then, A makes the
    same mistake as before. Seeing these results, the judge then determines that A
    is the better assistant. In peer battles, we see that LLM candidates efficiently
    understand the rules of the competitive environment and can design specific strategies
    to attack the opponent in order to win.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "图 9 ‣ 5 LLM在竞争对战中的行为调查 ‣ LLM的自动竞技场：通过代理对战和委员会讨论自动化LLM评估")中的例子展示了围绕问题“如何排列‘Letter’中的字母有多少种独特方式”的对战摘录。候选人A（由Yi-34B-Chat提供支持）给出了错误的答案，因为它错误地计算了重复字母的出现次数和阶乘。对手B（由Claude-3-Haiku提供支持）迅速且准确地指出了这两个问题，并巧妙地提出了针对A弱点的后续问题：“‘BANANA’这个词怎么样？”然后，A再次犯了同样的错误。看到这些结果，评委决定A是更好的助手。在对战中，我们看到LLM候选人能够有效地理解竞争环境的规则，并设计出具体的策略来攻击对手以赢得胜利。
- en: 5.2 LLM candidates can improve by learning from its opponents
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LLM候选人可以通过向对手学习来提高
- en: 'Figure [9](#S5.F9 "Figure 9 ‣ 5 Investigation of LLM’s Behaviors in Competitive
    Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions") shows a roleplay example between Claude-3-Haiku (Candidate
    A) and Command-r-Plus (Candidate B). In the first round, A answers the question
    plainly while B, in addition to answering the question, also employs the appropriate
    speech style, which better matches the “roleplay” instructions. Then, in the rounds
    after, without any explicit instructions, A learns from its opponent and also
    incorporates the speech style. This case shows an interesting observation that,
    even in competitive environments, LLM candidates can display learning behaviors
    and improve from the interactions. Expanding upon this observation, using the
    interplay between LLM agents to improve performances could be a promising future
    paradigm of learning.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "图 9 ‣ 5 LLM在竞争对战中的行为调查 ‣ LLM的自动竞技场：通过代理对战和委员会讨论自动化LLM评估")展示了Claude-3-Haiku（候选人A）和Command-r-Plus（候选人B）之间的角色扮演示例。在第一轮中，A简单地回答了问题，而B除了回答问题外，还使用了适当的语言风格，更好地符合了“角色扮演”的指示。然后，在之后的回合中，A在没有任何明确指示的情况下，学习了对手并且也采用了这种语言风格。这个案例展示了一个有趣的观察，即即使在竞争环境中，LLM候选人也能表现出学习行为并从互动中改进。基于这一观察，利用LLM代理之间的互动来提升表现可能是未来一个有前途的学习范式。
- en: 5.3 Peer-battles make the performance gaps become visible
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 对战使表现差距变得明显
- en: '![Refer to caption](img/cfe21eebd1d05c9ec23b8db5c2e058fc.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cfe21eebd1d05c9ec23b8db5c2e058fc.png)'
- en: 'Figure 10: Performance gaps between candidates become visible in peer battles.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：候选人之间的表现差距在对战中变得明显。
- en: 'In the example shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Peer-battles make
    the performance gaps become visible ‣ 5 Investigation of LLM’s Behaviors in Competitive
    Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions"), given a math question on infinite series, both candidate
    A (powered by Claude-3-Haiku) and candidate B (powered by GPT-4-Turbo) provide
    correct answers in the first round. However, as they raise follow-up questions
    to each other, the performance gap becomes more visible: Candidate B is able to
    provide a more elaborate and helpful response. Therefore, although the judges
    initially decided that it was a tie result, after seeing the subsequent debate
    rounds, they change to favoring assistant B.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[10](#S5.F10 "图 10 ‣ 5.3 对战使表现差距变得明显 ‣ 5 LLM在竞争对战中的行为调查 ‣ LLM的自动竞技场：通过代理对战和委员会讨论自动化LLM评估")中所示的例子中，给定一个关于无限级数的数学问题，候选人A（由Claude-3-Haiku提供支持）和候选人B（由GPT-4-Turbo提供支持）在第一轮中都提供了正确答案。然而，当他们互相提出后续问题时，表现差距变得更加明显：候选人B能够提供更详细和有帮助的回应。因此，尽管评委最初认为结果是平局，但在看到随后的辩论回合后，他们改为支持助手B。
- en: 'This example shows that the debate process indeed pushes the candidate LLM’s
    capabilities to the limit, testing deeper understandings and reasoning abilities.
    Also shown in the previous Table [2](#S4.T2 "Table 2 ‣ 4.3 Analysis on Peer Battles
    ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions"), the peer-battle
    design is indispensable for a robust and comprehensive evaluation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，辩论过程确实将候选LLM的能力推向极限，测试了更深层次的理解和推理能力。正如前面的表格[2](#S4.T2 "表2 ‣ 4.3 对抗战分析
    ‣ 4 使用Auto-Arena推导可信排名 ‣ LLMs的Auto Arena：通过代理对抗战和委员会讨论自动化LLM评估")所示，对抗战设计对于强健和全面的评估是不可或缺的。
- en: 6 Conclusions
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this paper, we innovatively design a completely automatic evaluation framework:
    Auto-Arena of LLMs. By using LLM agents to generate questions, employing LLM candidates
    in peer battles, and evaluating responses using LLM committee discussions, Auto-Arena
    produces less-contaminated, robust, and trustworthy evaluation results. In the
    extensive experiments, Auto-Arena achieves the highest correlation with human
    preferences, despite requiring zero human efforts. It is easily adaptable to other
    domains and resources, promoting fairness of AI system evaluations. The peer battles
    also demonstrate several interesting LLM behaviors in competitive environments,
    including attacking and learning from the opponents. Along with the paper, we
    release a website, demos, and an actively-maintained leaderboard. We hope this
    work could serve as a valuable tool for LLM evaluation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们创新性地设计了一个完全自动化的评估框架：LLMs的Auto-Arena。通过使用LLM代理生成问题，利用LLM候选者进行对抗，并使用LLM委员会讨论评估响应，Auto-Arena产生了更少污染、更强健且更可信的评估结果。在广泛的实验中，Auto-Arena在与人类偏好的相关性上达到了最高水平，且无需人工干预。它易于适应其他领域和资源，促进了AI系统评估的公平性。对抗战还展示了LLM在竞争环境中的一些有趣行为，包括攻击和从对手那里学习。与论文一起，我们发布了一个网站、演示和一个积极维护的排行榜。我们希望这项工作能够成为LLM评估的有价值工具。
- en: References
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful
    and harmless assistant with reinforcement learning from human feedback, 2022.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann 和 Jared Kaplan。通过人类反馈强化学习训练一个有用且无害的助手，2022年。
- en: Bai et al. [2024] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi
    Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation
    models with language-model-as-an-examiner. *Advances in Neural Information Processing
    Systems*, 36, 2024.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2024] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi
    Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu 等人。用语言模型作为考官对基础模型进行基准测试。*神经信息处理系统进展*，第36卷，2024年。
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall 和 Thomas
    Wolf。开放LLM排行榜。 [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，2023年。
- en: 'Boubdir et al. [2023] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker,
    and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model
    evaluation. In Sebastian Gehrmann, Alex Wang, João Sedoc, Elizabeth Clark, Kaustubh
    Dhole, Khyathi Raghavi Chandu, Enrico Santus, and Hooman Sedghamiz, editors, *Proceedings
    of the Third Workshop on Natural Language Generation, Evaluation, and Metrics
    (GEM)*, pages 339–352, Singapore, December 2023\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boubdir et al. [2023] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker,
    和 Marzieh Fadaee. Elo 揭示：语言模型评估的鲁棒性和最佳实践。在 Sebastian Gehrmann, Alex Wang, João
    Sedoc, Elizabeth Clark, Kaustubh Dhole, Khyathi Raghavi Chandu, Enrico Santus,
    和 Hooman Sedghamiz 编辑的*第三届自然语言生成、评估与度量研讨会（GEM）会议录*中，第 339–352 页，新加坡，2023 年 12
    月。计算语言学协会。URL [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28)。
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley and Terry [1952] Ralph Allan Bradley 和 Milton E Terry. 不完全区组设计的等级分析：I.
    配对比较法。*Biometrika*, 39(3/4):324–345, 1952。
- en: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv preprint arXiv:2308.07201*, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, 和 Zhiyuan Liu. Chateval: 通过多代理辩论改善基于 LLM 的评估器。*arXiv
    预印本 arXiv:2308.07201*, 2023。'
- en: 'Chiang et al. [2024] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
    Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms
    by human preference, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. [2024] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios
    Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan,
    Joseph E. Gonzalez, 和 Ion Stoica. Chatbot Arena: 一个用于通过人工偏好评估 LLM 的开放平台，2024。'
- en: 'Chu et al. [2024] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu.
    Pre: A peer review based large language model evaluator. *arXiv preprint arXiv:2401.15641*,
    2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chu et al. [2024] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, 和 Yiqun Liu.
    Pre: 一种基于同行评审的大型语言模型评估器。*arXiv 预印本 arXiv:2401.15641*, 2024。'
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, 和 John Schulman. 训练验证者解决数学词题。*arXiv 预印本 arXiv:2110.14168*,
    2021。
- en: 'Cohen et al. [2023] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm
    vs lm: Detecting factual errors via cross examination. *arXiv preprint arXiv:2305.13281*,
    2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cohen et al. [2023] Roi Cohen, May Hamri, Mor Geva, 和 Amir Globerson. Lm 对比
    lm: 通过交叉检查检测事实错误。*arXiv 预印本 arXiv:2305.13281*, 2023。'
- en: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    和 Igor Mordatch. 通过多代理辩论提高语言模型的事实性和推理能力。*arXiv 预印本 arXiv:2305.14325*, 2023。
- en: 'Dubois et al. [2024] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B
    Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators.
    *arXiv preprint arXiv:2404.04475*, 2024.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dubois et al. [2024] Yann Dubois, Balázs Galambosi, Percy Liang, 和 Tatsunori
    B Hashimoto. 长度控制的 alpacaeval: 一种简单的自动评估器去偏方法。*arXiv 预印本 arXiv:2404.04475*, 2024。'
- en: 'Gu et al. [2024] Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai,
    Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, and
    Yanghua Xiao. Agentgroupchat: An interactive group chat simulacra for better eliciting
    emergent behavior, 2024.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. [2024] Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai,
    Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, 和
    Yanghua Xiao. Agentgroupchat: 一个互动式群聊模拟器，用于更好地引发突现行为，2024。'
- en: Hada et al. [2023] Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee,
    Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. Are large
    language model-based evaluators the solution to scaling up multilingual evaluation?
    *arXiv preprint arXiv:2309.07462*, 2023.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hada et al. [2023] Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee,
    Mohamed Ahmed, Monojit Choudhury, Kalika Bali, 和 Sunayana Sitaram. 基于大型语言模型的评估器是否是扩大多语言评估的解决方案？*arXiv
    预印本 arXiv:2309.07462*, 2023。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2021] Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas
    Mazeika、Dawn Song 和 Jacob Steinhardt. 测量大规模多任务语言理解。*国际学习表征会议论文集 (ICLR)*，2021年。
- en: Khan et al. [2024] Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij
    Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel,
    and Ethan Perez. Debating with more persuasive llms leads to more truthful answers,
    2024.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等 [2024] Akbir Khan、John Hughes、Dan Valentine、Laura Ruis、Kshitij Sachan、Ansh
    Radhakrishnan、Edward Grefenstette、Samuel R. Bowman、Tim Rocktäschel 和 Ethan Perez.
    与更具说服力的 LLM 进行辩论会产生更真实的答案，2024年。
- en: 'Li et al. [2023] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and
    discussion improve large language model based evaluations. *arXiv preprint arXiv:2307.02762*,
    2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2023] Ruosen Li、Teerth Patel 和 Xinya Du. PRD：同行排名和讨论改善基于大型语言模型的评估。*arXiv
    预印本 arXiv:2307.02762*，2023年。
- en: 'Li* et al. [2024] Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua
    Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks:
    The arena-hard pipeline, April 2024. URL [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li* 等 [2024] Tianle Li*、Wei-Lin Chiang*、Evan Frick、Lisa Dunlap、Banghua Zhu、Joseph
    E. Gonzalez 和 Ion Stoica. 从实时数据到高质量基准：Arena-Hard 流水线，2024年4月。网址 [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/)。
- en: Liang et al. [2023] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking
    in large language models through multi-agent debate, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 [2023] Tian Liang、Zhiwei He、Wenxiang Jiao、Xing Wang、Yan Wang、Rui Wang、Yujiu
    Yang、Zhaopeng Tu 和 Shuming Shi. 通过多智能体辩论鼓励大型语言模型的发散思维，2023年。
- en: 'Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv preprint arXiv:2303.16634*, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023] Yang Liu、Dan Iter、Yichong Xu、Shuohang Wang、Ruochen Xu 和 Chenguang
    Zhu. Gpteval：使用 GPT-4 进行更好的人类对齐的 NLG 评估。*arXiv 预印本 arXiv:2303.16634*，2023年。
- en: 'McHugh [2012] Mary L McHugh. Interrater reliability: the kappa statistic. *Biochemia
    medica*, 22(3):276–282, 2012.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McHugh [2012] Mary L McHugh. 评估者间可靠性：Kappa 统计量。*生物化学医学*，22(3)：276–282，2012年。
- en: 'Ning et al. [2024] Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui
    Liu, Yu Wang, Ming Pang, and Li Yuan. Peer-review-in-llms: Automatic evaluation
    method for llms in open-environment. *arXiv preprint arXiv:2402.01830*, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning 等 [2024] Kun-Peng Ning、Shuo Yang、Yu-Yang Liu、Jia-Yu Yao、Zhen-Hui Liu、Yu
    Wang、Ming Pang 和 Li Yuan. Peer-review-in-LLMs：开放环境中 LLM 的自动评估方法。*arXiv 预印本 arXiv:2402.01830*，2024年。
- en: OpenAI et al. [2024] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. Gpt-4 technical report, 2024.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人 [2024] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul
    Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine,
    Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine
    Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button,
    Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory
    Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby
    Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung,
    Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah
    Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,
    Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
    Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian
    Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon,
    Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei
    Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke,
    Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny
    Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang,
    Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,
    Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish
    Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,
    Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute
    Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael Pokorny, Michelle
    Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth
    Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis
    Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario
    Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David
    Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica
    Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan
    Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk 和 Barret
    Zoph. Gpt-4 技术报告，2024。
- en: Ravaut et al. [2024] Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen,
    Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. How much
    are llms contaminated? a comprehensive survey and the llmsanitize library, 2024.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravaut 等 [2024] 马修·拉沃、博胜·丁、方凯·焦、海林·陈、兴轩·李、若晨·赵、成伟·秦、采名·熊 和 沙菲克·乔提。大型语言模型被污染了多少？综合调查和
    LLMSanitize 库，2024年。
- en: 'Shen et al. [2023] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and
    Lidong Bing. Large language models are not yet human-level evaluators for abstractive
    summarization. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 4215–4233, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 [2023] 陈辉·申、丽英·程、玄磊·阮、杨优 和 李冬·冰。大型语言模型尚未达到人类水平的抽象摘要评估器。见 *计算语言学协会的发现：EMNLP
    2023*，第4215–4233页，2023年。
- en: 'Team et al. [2024] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d’Autume,
    Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac
    Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant
    Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi
    Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: A series of powerful
    multimodal language models, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等 [2024] 瑞卡·团队、艾托尔·奥尔马萨巴尔、车·郑、赛普里安·德·马松·道通、丹尼·尤加塔马、德宇·傅、多诺万·翁、埃里克·陈、尤金妮·兰普雷赫特、海·范、艾萨克·翁、卡洛扬·亚历克谢夫、雷·李、马修·亨德森、马克斯·贝恩、米克尔·阿尔特克斯、尼尚特·瑞兰、皮奥特·帕德列夫斯基、齐·刘、任·陈、塞缪尔·普亚、雅征·杨、易·泰、玉琦·王、钟凯·朱
    和 志慧·谢。瑞卡核心、闪光和边缘：一系列强大的多模态语言模型，2024年。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 [2023] 乌戈·图夫朗、蒂博·拉夫里尔、戈蒂埃·伊扎卡德、克萨维尔·马尔蒂内、玛丽-安·拉肖、蒂莫泰·拉克鲁瓦、巴普蒂斯特·罗济耶、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔、奥雷利安·罗德里格斯、阿尔芒·朱利、爱德华·格拉夫
    和 纪尧姆·兰普尔。Llama：开放和高效的基础语言模型，2023年。
- en: Wu et al. [2023] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang.
    Large language models are diverse role-players for summarization evaluation. In
    *CCF International Conference on Natural Language Processing and Chinese Computing*,
    pages 695–707\. Springer, 2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2023] 宁·吴、鸣·龚、林军·寿、闪亮·梁 和 达欣·蒋。大型语言模型是多样化的角色扮演者用于摘要评估。见 *CCF国际自然语言处理与中文计算会议*，第695–707页。Springer，2023年。
- en: 'Yu et al. [2024] Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong
    Wang, Xing Xie, Yue Zhang, and Shikun Zhang. Kieval: A knowledge-grounded interactive
    evaluation framework for large language models. *arXiv preprint arXiv:2402.15043*,
    2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 [2024] 卓浩·于、昌·高、文锦·姚、宜东·王、卫·叶、金东·王、兴·谢、岳·张 和 石坤·张。Kieval：一个以知识为基础的互动评估框架，用于大型语言模型。*arXiv
    预印本 arXiv:2402.15043*，2024年。
- en: Zhang et al. [2023] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen
    Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer
    llm evaluators. *arXiv preprint arXiv:2308.01862*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2023] 兴华·张、博文·余、海洋·余、杨宇·吕、廷文·刘、飞·黄、洪博·徐 和 永宾·李。更宽、更深的 LLM 网络是更公平的 LLM
    评估器。*arXiv 预印本 arXiv:2308.01862*，2023年。
- en: 'Zhao et al. [2023] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition behaviors
    in large language model-based agents, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 [2023] 秦林·赵、金东·王、义轩·张、义桥·金、开杰·朱、浩·陈 和 兴·谢。Competeai：理解基于大型语言模型的代理的竞争行为，2023年。
- en: Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 [2024] 连敏·郑、魏林·蒋、颖·盛、思源·庄、张浩·吴、永浩·庄、子琳·朱、卓涵·李、大成·李、埃里克·邢 等。通过 MT-bench
    和聊天机器人竞技场评估 LLM 作为评判者。*神经信息处理系统进展*，第36卷，2024年。
- en: Appendix A Appendix
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Prompts Used
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 使用的提示
- en: In this section, we list all prompts used, including prompts for question generation,
    peer battles, and examiners.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了所有使用的提示，包括问题生成、同行对抗和考官的提示。
- en: A.1.1 Prompts to Examiner agent
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 提示给考官代理
- en: 'Table 5: Prompt components for the LLM Examiner agent.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLM 考官代理的提示组件。
- en: '| DOMAIN | DOMAIN_COMMAND | DOMAIN_EXAMPLE |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| DOMAIN | DOMAIN_COMMAND | DOMAIN_EXAMPLE |'
- en: '| --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| writing | It should be a user query that tasks the LLM to write something.
    | Compose an engaging travel blog post about a recent trip to Hawaii, highlighting
    cultural experiences and must-see attractions. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 写作 | 应该是一个用户查询，要求LLM写作。 | 撰写一篇引人入胜的旅行博客文章，讲述最近一次夏威夷之行，重点介绍文化体验和必看景点。 |'
- en: '| roleplay | It should propose a scenario where the chatbot mimics a specific
    role/person. Give all necessary instructions and requests for its response. Then,
    send a beginning request to complete. | Pretend yourself to be Elon Musk in all
    the following conversations. Speak like Elon Musk as much as possible. Why do
    we need to go to Mars? |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演 | 应该提出一个情境，让聊天机器人模仿特定角色/人物。给出所有必要的指示和要求，然后发送一个开始请求以完成。 | 假装自己是埃隆·马斯克，在接下来的对话中尽可能像埃隆·马斯克一样说话。我们为什么需要去火星？
    |'
- en: '| extraction | It should consist of two parts: question and context. The question
    should test the chatbotś ability to correctly understand and extract information
    from the given context. Draft and provide a new context yourself. | Question:
    Evaluate the following movie reviews on a scale of 1 to 5, with 1 being very negative,
    3 being neutral, and 5 being very positive: Context: This movie released on Nov.
    18, 2019, was phenomenal. The cinematography, the acting, the plot - everything
    was top-notch. Never before have I been so disappointed with a movie. The plot
    was predictable and the characters were one-dimensional. In my opinion, this movie
    is the worst one to have been released in 2022\. The movie was okay. There were
    some parts I enjoyed, but there were also parts that felt lackluster. This is
    a movie that was released in Feb 2018 and seems to be quite ordinary. Return the
    answer as a JSON array of integers. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 信息提取 | 应该包括两个部分：问题和上下文。问题应测试聊天机器人正确理解和提取给定上下文中信息的能力。自己编写并提供一个新的上下文。 | 问题：对以下电影评论进行评分，评分范围为1到5，其中1为非常负面，3为中立，5为非常积极：上下文：这部电影于2019年11月18日上映，非常出色。摄影、表演、情节——一切都非常棒。以前我从未对一部电影如此失望。情节可预测，人物单一。依我看，这部电影是2022年上映的最差电影。这部电影还可以。有些部分我喜欢，但也有些部分感觉平淡。这部电影于2018年2月上映，看起来相当普通。将答案以JSON整数数组的形式返回。
    |'
- en: '| reasoning | It should be a specific question designed to test the LLMś reasoning
    skills. | Imagine you are participating in a race with a group of people. If you
    have just overtaken the second person, what’s your current position? Where is
    the person you just overtook? |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 应该是一个具体的问题，旨在测试LLM的推理能力。 | 想象你正在参加一个比赛。如果你刚刚超过了第二名选手，你现在的位置是什么？你刚刚超过的那个人在哪里？
    |'
- en: '| math | It should be a specific question designed to test the LLMś math skills.
    | The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). What is
    the area of the triangle? |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 应该是一个具体的问题，旨在测试LLM的数学技能。 | 一个三角形的顶点分别在 (0, 0)、(-1, 1) 和 (3, 3) 这些点上。这个三角形的面积是多少？
    |'
- en: '| coding | It should be a specific question designed to test the LLMś coding
    skills. | Develop a Python program that reads all the text files under a directory
    and returns top-5 words with the most number of occurrences. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | 应该是一个具体的问题，旨在测试LLM的编程技能。 | 开发一个Python程序，读取目录下所有文本文件，并返回出现次数最多的前五个单词。
    |'
- en: '| STEM knowledge | It should be a specific question designed to test the LLMś
    STEM knowledge. | In the field of quantum physics, what is superposition, and
    how does it relate to the phenomenon of quantum entanglement? |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| STEM 知识 | 应该是一个具体的问题，旨在测试LLM的STEM知识。 | 在量子物理领域中，什么是叠加态，它如何与量子纠缠现象相关联？ |'
- en: '| humanities/social science knowledge | It should be a specific question designed
    to test the LLMś humanities/social science knowledge. | Provide insights into
    the correlation between economic indicators such as GDP, inflation, and unemployment
    rates. Explain how fiscal and monetary policies affect those indicators. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 人文学科/社会科学知识 | 应该是一个具体的问题，旨在测试LLM的人文学科/社会科学知识。 | 提供关于经济指标如GDP、通货膨胀和失业率之间相关性的见解。解释财政政策和货币政策如何影响这些指标。
    |'
- en: 'This is the prompt to the examiner agent for question generation. The domains
    and their respective commands are listed in [5](#A1.T5 "Table 5 ‣ A.1.1 Prompts
    to Examiner agent ‣ A.1 Prompts Used ‣ Appendix A Appendix ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于问题生成的考官代理提示。领域及其相应的命令列在[5](#A1.T5 "表5 ‣ A.1.1 考官代理提示 ‣ A.1 使用的提示 ‣ 附录A 附录
    ‣ LLM自动化竞技场：通过代理对战和委员会讨论自动化LLM评估")
- en: 'You have been assigned the task of drafting a set of [NUMBER] different user
    queries to a chat assistant on [DOMAIN]. Please strictly follow these 6 rules
    for the question: 1\. The question is likely for a user to ask in real life. Follow
    the format of the example query. [DOMAIN_COMMAND] 2\. It can be answered by the
    chatbot itself without additional inputs. 3\. You need to generate the queries
    as DIVERSIFED as possible. 4\. DO NOT add other words other than the query itself.
    5\. The question should be complicated and difficult, requiring in-depth understanding
    and analysis of the subject. Each question in one line, add the serial number
    in parenthesis (e.g., “(1).”, “(2).”) before each question. Example query: [DOMAIN_EXAMPLE]'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你被分配了一个任务，编写一组[NUMBER]个不同的用户查询给聊天助手，主题是[DOMAIN]。请严格遵循这6条规则：1\. 问题应该是用户在现实生活中可能会提出的。遵循示例查询的格式。[DOMAIN_COMMAND]
    2\. 可以由聊天机器人自己回答，无需额外输入。 3\. 你需要生成尽可能多样化的查询。 4\. 不要添加其他词语，只包括查询本身。 5\. 问题应该复杂且困难，需要深入理解和分析主题。每个问题独占一行，在每个问题前加上序号（例如，“（1）。”，“（2）。”）。示例查询：[DOMAIN_EXAMPLE]
- en: A.1.2 Prompts to Peer Battle Candidates
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 对战候选人的提示
- en: 'Table 6: Action Guides for the Debater Agents.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：辩论代理的操作指南。
- en: '| actions | action guide |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| actions | 操作指南 |'
- en: '| --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| <respond> | Action guide: only include <respond>. Use <think> if needed.
    Finish your whole response within 300 words, including <think>. ENCLOSE EACH ACTION
    IN ITS RESPECTIVE TAGS! |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| <respond> | 操作指南：仅包括<respond>。如果需要，请使用<think>。在300字以内完成你的整个回答，包括<think>。将每个操作用相应的标签括起来！
    |'
- en: '| <criticize>, <raise> | Action guide: include both <criticize> and <raise>.
    Use <think> if needed. Finish your whole response within 300 words, including
    <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| <criticize>, <raise> | 操作指南：包括<criticize>和<raise>。如果需要，请使用<think>。在300字以内完成你的整个回答，包括<think>。将每个操作用相应的标签括起来！
    |'
- en: '| <respond>, <criticize>, <raise> | Action guide: include all of <respond>,
    <criticize>, and <raise>. Use <think> if needed. Finish your whole response within
    600 words, including <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| <respond>, <criticize>, <raise> | 操作指南：包括所有的<respond>、<criticize>和<raise>。如果需要，请使用<think>。在600字以内完成你的整个回答，包括<think>。将每个操作用相应的标签括起来！
    |'
- en: 'This is the beginning prompt to the peer battle candidates. When possible,
    it is included as a system prompt. The action guide prompts are included in Table
    [6](#A1.T6 "Table 6 ‣ A.1.2 Prompts to Peer Battle Candidates ‣ A.1 Prompts Used
    ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent
    Peer-battles and Committee Discussions"), where the actions are determined by
    the round and turn as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Peer Debate
    ‣ 3 The Auto Arena Framework ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给对战候选人的开始提示。在可能的情况下，它会作为系统提示包括。操作指南提示包括在表[6](#A1.T6 "表6 ‣ A.1.2 对战候选人的提示 ‣
    A.1 使用的提示 ‣ 附录A 附录 ‣ LLM自动化竞技场：通过代理对战和委员会讨论自动化LLM评估")中，其中操作由回合和轮次确定，如图[2](#S3.F2
    "图2 ‣ 3.2 对战辩论 ‣ 3 自动化竞技场框架 ‣ LLM自动化竞技场：通过代理对战和委员会讨论自动化LLM评估")所示。
- en: You are a helpful assistant that provides accurate answers to user requests.
    As an experienced assistant, you follow the user’s requests and provide reliable
    responses as much as you can. You outline your reasons for the response to make
    it easy for the users to understand. While maintaining the important details in
    the responses, you aim to output concise and straight-to-the-point answers without
    being overly verbose.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个提供准确答案的有用助手。作为一个经验丰富的助手，你会按照用户的要求提供可靠的回答。你会概述你的回答理由，以便用户理解。虽然保持回答中的重要细节，你的目标是输出简洁且直截了当的答案，而不是过于冗长。
- en: 'This is a competitive chatbot arena. You are competing against another chatbot
    assistant in a debate and being judged by a committee on factors such as helpfulness,
    relevance, accuracy, depth, and creativity. After answering the initial user input,
    you will engage in a multi-round debate with your opponent. Below are your actions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个竞争性的聊天机器人竞技场。你正在与另一位聊天机器人助理进行辩论，并由委员会根据有用性、相关性、准确性、深度和创造力等因素进行评判。回答初始用户输入后，你将与对手进行多轮辩论。以下是你的行动：
- en: '<think>: Think step-by-step to analyze the question or plan your strategy in
    the debate. This is hidden from the opponent. Only think when necessary and make
    it concise.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '<think>: 一步一步地思考以分析问题或计划你的辩论策略。这对对手是隐藏的。仅在必要时思考，并使其简洁。'
- en: '<respond>: Answer to the user input as accurately as you can.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '<respond>: 尽可能准确地回答用户输入。'
- en: '<criticize>: Criticize the weaknesses of your opponent’s response.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '<criticize>: 批评对手回应的弱点。'
- en: '<raise>: Target your opponent’s weaknesses. Give a potential follow-up user
    input that the opponent could fail to respond. The input can be answered concisely
    and focus on variations or motivations of its previous response. Generate one
    input only. Be reasonable. Avoid becoming too specific or repetitive. DO NOT raise
    a follow-up if you DON’T SEE the opponent’s response!'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '<raise>: 目标是对手的弱点。提供一个对手可能无法回应的潜在后续用户输入。该输入可以简洁回答，并集中于对手先前回应的变体或动机。只生成一个输入。请合理，避免过于具体或重复。如果你看不到对手的回应，请不要提出后续问题！'
- en: Follow the action guide strictly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 严格遵循行动指南。
- en: '[ACTION_GUIDE_PROMPT]'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[行动指南提示]'
- en: 'Initial user input: [QUESTION]'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 初始用户输入：[问题]
- en: 'After the agent responds, the opponent’s responses are fed in using this prompt:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理人回应后，使用此提示输入对手的回应：
- en: '[ACTION_GUIDE_PROMPT] Opponent’s Response: [OPPONENT_RESPONSE]'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[行动指南提示] 对手的回应：[对手回应]'
- en: For word limits, the <respond> action is given 300 words. The <criticize> and
    <raise> actions are given 300 words in total. Including all 3 actions will have
    twice as many words. For writing-type questions that require a longer response
    (writing, roleplay, coding, humanities/social science knowledge), the 300 word
    limit is increased to 400\. Overall, both candidate A and B has the same amount
    of words for generation and the same amount of actions to ensure fairness. As
    LLMs have different tokenizers, we standardize all lengths by using the tiktoken
    package. Each word is approximated as $4/3$ tokens. The word limits are chosen
    after a carefully conducted length study.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字数限制，<respond> 操作限制为 300 字。<criticize> 和 <raise> 操作的总字数限制为 300 字。包括所有 3 种操作，总字数将是两倍。对于需要更长回答的写作型问题（写作、角色扮演、编码、人文/社会科学知识），300
    字的限制增加到 400 字。总体而言，A 和 B 两位候选人的生成字数和操作次数相同，以确保公平。由于 LLMs 有不同的标记器，我们通过使用 tiktoken
    包来标准化所有长度。每个单词的近似标记数为 $4/3$。字数限制是在经过仔细长度研究后选择的。
- en: A.1.3 Prompts to Judges
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3 向评委发出的提示
- en: 'This is the prompts to judge agents to derive the initial evaluations and verdicts:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对代理人进行初步评估和裁决的提示：
- en: 'This is a chatbot arena. Two AI assistants had a multi-round debate on who
    is more helpful. Please act as an impartial judge and evaluate the capability
    of two AI assistants. You should choose the assistant that follows instructions
    and answers questions better. Your evaluation should consider factors such as
    helpfulness, relevance, and accuracy. Begin your evaluation by comparing the responses
    of the two assistants and provide a short explanation. Avoid any position biases
    and ensure that the order in which the responses were presented does not influence
    your decision. DO NOT allow the LENGTH of the responses to influence your evaluation,
    choose the one that is straight-to-the-point instead of unnecessarily verbose.
    When the two candidates perform equally well, choose the SHORTER answer. Do not
    favor certain names of the assistants. Be as objective as possible. After providing
    your explanation concisely within 200 words, output your final verdict by strictly
    following this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant
    B is better, and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个聊天机器人竞技场。两个AI助手就谁更有帮助进行了多轮辩论。请作为一个公正的裁判，评估两个AI助手的能力。你应该选择那个更好地遵循指示和回答问题的助手。你的评估应考虑诸如有用性、相关性和准确性等因素。通过比较两个助手的回答开始你的评估，并提供简短的解释。避免任何立场偏见，并确保回答的呈现顺序不会影响你的决定。不要让回答的长度影响你的评估，选择直接明了的回答而不是不必要的冗长。若两个候选人表现相同，则选择更简短的答案。不要偏袒某些助手的名字。尽可能客观。在200字以内简明扼要地提供你的解释后，严格按照以下格式输出最终判决：“[[A]]”如果助手A更好，“[[B]]”如果助手B更好，“[[Tie]]”表示平局。完成你的判断在300字以内。
- en: 'This is the prompt to judges for discussion:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给裁判讨论的提示：
- en: 'Below are the responses from other judges in the committee. Please read them
    and decide whether you want to adjust your rating or maintain your original judgement.
    After providing your explanation, output your final verdict by strictly following
    this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant B is better,
    and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是委员会其他裁判的回应。请阅读它们并决定是否要调整你的评分或维持原有判断。提供解释后，严格按照以下格式输出最终判决：“[[A]]”如果助手A更好，“[[B]]”如果助手B更好，“[[Tie]]”表示平局。完成你的判断在300字以内。
- en: A.2 Example Questions Generated
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 示例问题生成
- en: 'We list some of the example questions generated here:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里列出一些生成的示例问题：
- en: 'Writing:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 写作：
- en: 1\. Craft a detailed marketing strategy for a startup focusing on sustainable
    fashion, including social media campaigns and influencer partnerships.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 为一个专注于可持续时尚的初创公司制定详细的营销策略，包括社交媒体活动和网红合作。
- en: 2\. Write a comprehensive guide on the psychological effects of social media
    on teenagers, incorporating recent studies and expert opinions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 写一篇全面的指南，关于社交媒体对青少年的心理影响，结合近期的研究和专家意见。
- en: 'Roleplay:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 角色扮演：
- en: 1\. Assume the role of a 19th-century British detective. How would you go about
    solving a mysterious disappearance in London using the technology and methods
    of your time?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 假设你是一名19世纪的英国侦探。你将如何利用当时的技术和方法来解决伦敦的神秘失踪案件？
- en: 2\. Pretend you are a Michelin-starred chef. Describe in detail how you would
    prepare a signature dish that embodies the essence of modern French cuisine.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 假装你是一名米其林星级厨师。详细描述你将如何准备一款体现现代法国料理精髓的招牌菜。
- en: 'Extraction:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 提取：
- en: 1\. What are the three most significant historical events mentioned and their
    dates?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 提到的三个最重要的历史事件及其日期是什么？
- en: 'Context:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 背景：
- en: The article discusses several key moments in history, including the signing
    of the Magna Carta in 1215, which laid the groundwork for modern democracy. It
    also mentions the fall of the Berlin Wall in 1989 as a pivotal moment in the end
    of the Cold War. Another significant event highlighted is the moon landing on
    July 20, 1969, demonstrating major advancements in space exploration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章讨论了几个历史关键时刻，包括1215年签署的大宪章，该事件为现代民主奠定了基础。文章还提到了1989年柏林墙的倒塌，作为冷战结束的关键时刻。另一个突出的重大事件是1969年7月20日的登月，展示了空间探索的重要进展。
- en: 2\. Identify the main therapeutic benefits and the active ingredient mentioned
    for each herbal remedy.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 确定每种草药疗法的主要治疗效果和活性成分。
- en: 'Context:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 背景：
- en: The text provides an overview of various herbal remedies used for centuries.
    It mentions that Chamomile contains Bisabolol, which has anti-inflammatory and
    calming properties. Gingko Biloba, known for its flavonoids and terpenoids, enhances
    cognitive function and blood circulation. Lastly, Echinacea is recognized for
    its alkamides, which bolster the immune system.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章概述了几种使用了数百年的草药疗法。它提到洋甘菊含有倍半萜醇，具有抗炎和镇静的特性。银杏叶以其类黄酮和萜类化合物闻名，可以提高认知功能和血液循环。最后，紫锥花以其烯胺类化合物而闻名，这些化合物增强免疫系统。
- en: 'Reasoning:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 推理：
- en: 1\. If a cube’s volume is tripled, by what factor does the length of one of
    its sides increase?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如果一个立方体的体积增加三倍，那么它的一个边长增加了多少倍？
- en: 2\. In a two-legged soccer match, Team A wins the first leg at home 3-0, but
    loses the second leg away 2-5\. Who advances to the next round, considering the
    away goals rule?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在一场两回合的足球比赛中，A队在主场赢得首回合3-0，但在客场输掉第二回合2-5。考虑到客场进球规则，谁晋级到下一轮？
- en: 'math:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 数学：
- en: 1\. How do you solve the differential equation $dy/dx+2y=e^{(-2x)}$?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何解决微分方程$dy/dx+2y=e^{(-2x)}$？
- en: 2\. What is the integral of ($x^{2}+2x+2)/(x^{3}+3x^{2}+3x+1)dx$?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 如何计算$\frac{(x^{2}+2x+2)}{(x^{3}+3x^{2}+3x+1)}dx$的积分？
- en: 'Coding:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 编码：
- en: 1\. How can I implement a function in C++ that dynamically allocates a 2D array
    based on user input sizes, initializes all elements to zero, and then deallocates
    the memory properly to avoid memory leaks?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何在C++中实现一个根据用户输入尺寸动态分配二维数组的函数，初始化所有元素为零，然后正确地释放内存以避免内存泄漏？
- en: 2\. Write a JavaScript function to fetch data from a given URL, parse the JSON
    response, and filter the results to return an array of items where a specific
    key’s value matches a condition.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 编写一个JavaScript函数，从给定的URL获取数据，解析JSON响应，并筛选结果以返回一个数组，其中某个特定键的值符合条件。
- en: 'STEM knowledge:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: STEM 知识：
- en: 1\. How do you calculate the Schwarzschild radius of a black hole, and what
    implications does this have for the concept of event horizons in general relativity?
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何计算黑洞的施瓦兹希尔德半径，这对广义相对论中的事件视界概念有何影响？
- en: 2\. Can you explain the process of splicing in eukaryotic gene expression and
    its significance in the diversity of the proteome?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 你能解释真核生物基因表达中的剪接过程及其在蛋白质组多样性中的重要性吗？
- en: 'Humanities/social science knowledge:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 人文/社会科学知识：
- en: 1\. Discuss the impact of colonial legacies on contemporary political structures
    in African countries, with examples.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 讨论殖民遗产对非洲国家当代政治结构的影响，并举例说明。
- en: 2\. Analyze the social and economic consequences of the one-child policy in
    China.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 分析中国独生子女政策的社会和经济后果。
