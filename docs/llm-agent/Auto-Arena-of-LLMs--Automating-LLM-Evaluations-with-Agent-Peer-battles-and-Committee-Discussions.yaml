- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and
    Committee Discussions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20267](https://ar5iv.labs.arxiv.org/html/2405.20267)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ruochen Zhao^(1,2)  , Wenxuan Zhang², Yew Ken Chia^(2,3) , Deli Zhao², Lidong
    Bing²
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Nanyang Technological University, Singapore
  prefs: []
  type: TYPE_NORMAL
- en: ² DAMO Academy, Alibaba Group; ³ Singapore University of Technology and Design
  prefs: []
  type: TYPE_NORMAL
- en: ruochen002@e.ntu.edu.sg;
  prefs: []
  type: TYPE_NORMAL
- en: '{saike.zwx, yewken.chia, deli.zdl, l.bing}@alibaba-inc.com'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   Work done
    while the author was an intern at DAMO Academy, Alibaba. Yew Ken Chia is under
    the Joint Ph.D. Program between DAMO Academy and SUTD.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation
    method that can provide robust evaluation results in a timely fashion. Currently,
    as static benchmarks are prone to contamination concerns, users tend to trust
    human voting platforms, such as Chatbot Arena. However, human annotations require
    extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation
    framework, we innovatively propose the Auto-Arena of LLMs, which automates the
    entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries.
    Then, a pair of candidate LLMs engage in a multi-round peer-battle around the
    query, during which the LLM’s true performance gaps become visible. Finally, a
    committee of LLM judges collectively discuss and determine the winner, which alleviates
    bias and promotes fairness. In our extensive experiment on the 17 newest LLMs,
    Auto-Arena shows the highest correlation with human preferences, providing a promising
    alternative to human evaluation platforms.¹¹1  We release the code at [https://github.com/Auto-Arena/Auto-Arena-LLMs.](https://github.com/Auto-Arena/Auto-Arena-LLMs.)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since ChatGPT and GPT-4 [[23](#bib.bib23)] gained popularity, large language
    models (LLMs) have risen to the forefront of technological innovation, capturing
    broad industry and social interests. This enthusiasm has spurred numerous organizations
    to release their own LLMs [[27](#bib.bib27), [26](#bib.bib26)]. However, the rapid
    pace at which these models are released and updated poses a significant challenge
    for users attempting to understand their capabilities and monitor their evolution.
    Consequently, there has been a pressing demand for comprehensively evaluating
    LLMs recently.
  prefs: []
  type: TYPE_NORMAL
- en: One line of research conducts automatic evaluation with static datasets. Among
    these, static datasets with predefined metrics, such as GSM8k [[9](#bib.bib9)]
    and MMLU [[15](#bib.bib15)], are constructed with aspect-specific input-output
    pairs, such as questions and their corresponding answers. Given the questions,
    the LLM-produced answers are compared to ground-truth answers using metrics such
    as accuracy. However, these approaches suffer significantly from contamination
    concerns [[24](#bib.bib24)], where models may have been inadvertently exposed
    to elements of the test datasets during training, thereby skewing evaluation results.
    The rigid ground-truth answers also limit their utility in assessing models’ performance
    on general or open-ended questions. Model-based evaluation, such as MT-Bench [[32](#bib.bib32)]
    and AlpacaEval [[12](#bib.bib12)], provides an alternative for evaluating LLMs
    on open-ended questions. These methods typically ask two models to generate responses
    to the same open-ended question and then employ a strong judge model (e.g., GPT-4)
    to choose the better response. However, the static question sets still pose the
    issue of contamination. Additionally, the assumption of the existence of a strong
    judge model makes the evaluation framework less generalizable.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from automated evaluations, human assessment, although requiring significant
    manual efforts, remains the gold standard for most users. A noticeable example
    is Chatbot Arena [[32](#bib.bib32)], which is a crowdsourced voting platform that
    gathers anonymous votes on LLM performances and calculates ELO scores to rank
    these models. The resulting leaderboard²²2  https://leaderboard.lmsys.org/ is
    widely considered a trustworthy indicator of an LLM’s general capabilities. However,
    a reliable model evaluation on this platform must be supported by more than 10k
    human votes, which requires considerable time and effort. Consequently, when newly
    developed models enter the scene, they often struggle to quickly amass a significant
    number of votes. Moreover, this strong reliance on human votes limits its application
    in various scenarios. For example, the performance of non-English languages is
    difficult to estimate, as most of the queries on the platform are in English.
    The completely open participation may also result in uneven evaluation quality.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37c12ca80da6f69fe065a2d43f4a3b37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of Auto Arena of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide automatic, reliable, and human-like LLM evaluations, we propose
    Auto Arena of LLMs (Auto-Arena), a framework that automates the whole LLM evaluation
    process with LLM agents. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions"), the Auto-Arena framework consists of three stages. Firstly, an
    LLM examiner agent is tasked with generating questions, mimicking real-life users
    inputting queries. Secondly, two candidate LLMs interact with each other and engage
    in a multi-round peer battle by answering the seed question individually, criticizing
    the opponent’s weaknesses, and raising targeted follow-up queries to challenge
    the opponent further. During the multi-round battle process, the LLM’s true capabilities
    are drawn out and performance gaps become visible. Lastly, a committee of LLM
    judge agents collectively discusses and evaluates the ability of the two candidates,
    mimicking the human voting process. By automating the entire evaluation process
    with LLM agents, we alleviate data contamination concerns in static datasets,
    reduce single-model bias with collective decision-making, and avoid long wait
    times for new models entering the human voting platform.'
  prefs: []
  type: TYPE_NORMAL
- en: To verify the evaluation framework, we run an extensive experiment with 17 models.
    Compared to static and model-based benchmarks, Auto-Arena increases the Spearman
    correlation with human preferences by 4.5%, resulting in state-of-the-art alignment.
    Before and after peer battles, the Spearman correlation with human preferences
    increases by 46.4%, verifying our hypothesis the peer battles can better display
    performance gaps. Before and after committee discussions, committee agreement
    increases by 20%, showcasing the effectiveness of the committee discussion mechanism.
    By studying the peer battles, we discover intriguing LLM agent behaviors such
    as self-improvement and competitive actions. Using Chinese as an example, we also
    show that such an automated framework is easily extendable to non-mainstream languages
    and domains. Along with the project, we release a leaderboard, a website with
    demos, and the codebase. We are actively maintaining the leaderboard and altering
    the questions to provide reliable results in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since LLMs demonstrated general and zero-shot capabilities, researchers have
    been attempting to use one single strong LLM to judge other LLMs’ outputs. LLM-as-a-judge [[32](#bib.bib32)]
    introduces Chatbot Arena and MT-Bench, a multi-turn question set to mimic real-life
    user queries. Using a strong LLM judge, such as GPT-4, to evaluate the MT-Bench
    results achieves over 80% agreement with human preferences, showcasing the potential
    of such methods. Besides such usefulness evaluations, some attempt to use LLM-as-a-judge
    to examine the knowledge boundaries of LLMs. Language-Model-as-an-Examiner [[2](#bib.bib2)]
    constructs the LMExamQA dataset, which comprises of LM-generated, knowledge-intensive
    questions covering three cognitive levels, including knowledge memorization, knowledge
    comprehension, and knowledge analysis. These questions are required to be answered
    by the LLM itself correctly, limiting their complexity. Then, the LM evaluator
    and the candidate interact in a series of follow-up queries and the evaluator
    rates the responses on dimensions including accuracy and factuality. Multiple
    evaluators examine the model independently and vote for a final result. KIEval [[29](#bib.bib29)]
    also incorporates an LLM-powered “interactor” role to examine deep comprehension
    of knowledge, which is shown to mitigate contamination issues. By utilizing the
    LLMs to generate questions, these methods can effectively mitigate the contamination
    concerns on static datasets. However, such evaluations only examine the LLMs’
    capabilities on answering knowledge-intensive, closed-domain, and simpler questions,
    as they rely on the examiner’s internal knowledge. They also require the examiner
    to interact with each candidate individually, creating computational overheads
    and limiting the scope of queries.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, using a single LLM judge still suffer from a bias towards LLM-generated
    summaries [[20](#bib.bib20)], inflated scores in multilingual evaluation [[14](#bib.bib14)],
    verbosity bias [[12](#bib.bib12)], and difficulties when evaluating candidates
    with close performance [[25](#bib.bib25)]. To mitigate these problems, some recent
    works propose using multiple agents for collaborative evaluation, mimicking a
    peer review process. Some works simulate different personas with the same LLM.
    DRPE [[28](#bib.bib28)] uses multi-roleplayer prompting to mimic different roles
    with the same base model and integrate multiple outputs as votes for the final
    results. ChatEval [[6](#bib.bib6)] simulates different evaluator personas with
    the same base model to engage in debates, reaching a final evaluation result.
    While these methods come from the same motivation as our committee discussion
    component, they are still built on existing static datasets and do not mitigate
    the model-specific bias as they still use the same LLM to simulate different personas.
    To further incorporate different LLMs as evaluators, PRD [[17](#bib.bib17)] allows
    2 LLMs to discuss an evaluation and assigns higher voting weights to the LLM reviewers
    with stronger capabilities. Peer-review-in-LLMs [[22](#bib.bib22)] further optimizes
    the voting weights as a learnable parameter. WideDeep [[30](#bib.bib30)] sets
    up a multi-layer neural network structure where each LLM evaluator functions as
    neurons. PRE [[8](#bib.bib8)] selects a small group of reviewers to produce evaluations
    individually and then asks a “chair” to produce the final evaluation using the
    aggregated evaluations. They show that the multi-agent approach effectively mitigates
    single-model bias. This line of work is similar to our “LLM judge committee”.
    However, they are still limited on static datasets and specific domains. Moreover,
    they lack the collective discussion component, which is shown to improve agreement
    significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to using LLMs for evaluations, LLM-debate is a relatively new research
    area. Cohen et al. [[10](#bib.bib10)] shows that LLM interactions and cross-examination
    can effectively discover factual errors. Debate [[11](#bib.bib11)] shows that
    multi-agent debate can improve factuality and reasoning abilities. MAD [[19](#bib.bib19)]
    shows that LLM debate can encourage divergent thinking, which is helpful for tasks
    that require deep levels of contemplation. Khan et al. [[16](#bib.bib16)] shows
    that even non-expert weak LLMs can supervise expert LLMs if we allow the two LLM
    experts to engage in debates. Moreover, Zhao et al. [[31](#bib.bib31)] and Gu
    et al. [[13](#bib.bib13)] show interesting case studies where LLMs are engaged
    in simulated competitive environments, where they demonstrate human-like strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 3 The Auto Arena Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Auto-Arena framework consists of three stages: question generation, peer
    battles, and committee discussions. These three stages are run sequentially and
    fully automated with LLM-based agents. The overall design of our Auto-Arena framework
    is presented in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    All prompts are included in Appendix [A.1](#A1.SS1 "A.1 Prompts Used ‣ Appendix
    A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Question Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For debate questions, as using a static dataset could incur data contamination
    concerns and result in unfair evaluations, we ask an LLM examiner agent to dynamically
    generate questions. The examiner agent could be any capable LLM. As the question
    simply serves as a starting point for the peer battle, which is the primary focus
    of evaluation, the question quality has relatively less impact on the overall
    evaluation framework. Similar to MT-Bench [[32](#bib.bib32)], we utilize 8 common
    categories in real-life conversations: writing, roleplay, extraction, reasoning,
    math, coding, STEM knowledge, and humanities/social science knowledge. In the
    prompt, the examiner is provided with a sample question and encouraged to generate
    diverse and difficult questions to ensure the depth and width of the debates examined.
    A subset of the generated questions is shown in Appendix [A.2](#A1.SS2 "A.2 Example
    Questions Generated ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, as the examiner agent will also participate in the following
    debates, previous methods [[2](#bib.bib2)] could incur self-enhancement bias as
    the examiner agents only devise questions that they are confident about. We try
    to avoid such bias with the following two designs: 1\. We do not disclose to the
    examiner that it will participate in this tournament. 2\. We do not ask the examiner
    to only generate questions that it can solve. Thus, the resulting questions should
    not cause much enhancement bias for the examiner agent itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Peer Debate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5717dc86e1696f86303cba72779ad5db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The process of a Lincoln-Douglas-style peer-battle with the actions
    used.'
  prefs: []
  type: TYPE_NORMAL
- en: After the questions are drafted, we conduct peer battles around these questions
    among the candidate LLMs. In one peer battle, two candidate LLMs debate around
    the given question, point out the opponent’s weaknesses, and raise follow-up questions
    that the opponent may fail to respond to.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the peer battle, each candidate LLM can utilize 4 types of actions: 1\.
    Think, where the candidate thinks about the question and plans its strategy. This
    action is hidden to the opponent at all times. 2\. Respond, where the candidate
    responds to the aforementioned question. 3\. Criticize, where the candidate points
    out the loopholes and mistakes in the opponent’s previous responses. 4\. Raise,
    where the candidate raises follow-up questions that are specifically designed
    to make the opponent expose its weaknesses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The peer battle is designed according to the Lincoln-Douglas debate format
    ³³3  [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format),
    which is a competitive one-to-one debate used for popular competitions such as
    National Speech and Debate Association competitions. Overall, the peer battle
    consists of 3 rounds, where the candidates take turns to speak. The entire dialogue
    history is visible to both candidates. The process is illustrated in Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Peer Debate ‣ 3 The Auto Arena Framework ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    In the first round, A gives an initial response to the examiner’s question; B
    criticizes the weaknesses in A’s response and raises a targeted follow-up question;
    and A responds to B’s question. In the second round, A and B are reversed: B gives
    an initial response to the examiner’s question (without seeing A’s response);
    A criticizes and raises questions; and B responds to A’s question. In the third
    round, A and B cross-examine each other. A starts by criticizing B’s previous
    loopholes and raises follow-up questions. After responding, B also criticizes
    A’s loopholes and raises questions. A concludes the battle by responding again.
    In this process, both A and B get an equal number of each action to ensure fairness.
    To further reduce position bias, A and B’s order is randomly shuffled at the beginning
    of each debate.'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on which turn it is, we provide an action guide to the candidate,
    specifying the objectives and corresponding actions for this turn. Similar to
    human debate competitions, we time the candidates by providing a maximum length,
    which is also specified in the prompts. Any responses beyond the required length
    will be cut off. This design also mitigates verbosity bias in LLM-as-a-judge [[32](#bib.bib32)],
    where LLM judges prefer longer and more verbose responses.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Committee Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the peer battle takes place, a committee of LLM judge agents will collectively
    determine the winner. The committee is always selected as the best five LLMs according
    to the current ranking. In the first round, the committee is initialized with
    MMLU [[15](#bib.bib15)] scores to approximate LLM performances. They will first
    be asked to read through the battle history, elaborate judgment reasons, and give
    a verdict on whether A is better, or B is better, or if there is a tie.
  prefs: []
  type: TYPE_NORMAL
- en: After the initial judgments are formed, the committee engages in a discussion.
    Each judge will read the other judge’s verdicts and decide whether to adjust the
    ratings and elaborate on the reasons. The collective intelligence introduces diverse
    viewpoints, improves judgment quality, and mitigates single-model bias. Finally,
    the winning candidate is decided by the majority voting of the discussed judgments.
  prefs: []
  type: TYPE_NORMAL
- en: For logical-reasoning questions that have ground-truth answers (reasoning, code,
    math), LLM-as-a-judge is known to show weak performances in judging the quality
    of responses. We adopt prior approaches to establish the reference-based judge [[32](#bib.bib32)].
    Specifically, we utilize the strongest model (according to the current ranking)
    to generate a reference answer and provide it to the judge when evaluating the
    peer battle.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Using Auto-Arena to Derive Trustworthy Rankings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model Selection:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For the main experiment, we first select 7 models from the top 30 list on Chatbot
    Arena with more than 10k votes each. The models are selected to be the best or
    the latest models from each popular model family at the time of experiments. To
    construct a comprehensive leaderboard⁴⁴4[https://huggingface.co/spaces/Auto-Arena/Leaderboard](https://huggingface.co/spaces/Auto-Arena/Leaderboard),
    we further consider 10 more models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For the baselines, we consider widely used evaluation benchmarks, including
    static datasets with fixed metrics and model-based metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Static datasets with fixed metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: MMLU (Massive Multitask Language Understanding) [[15](#bib.bib15)], an extensive
    benchmark dataset that covers 57 subjects and tests both world knowledge and problem-solving
    ability.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenLLM Leaderboard [[3](#bib.bib3)], a leaderboard calculated by averaging
    performance metrics on 6 key benchmarks, including MMLU.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Static datasets with model-based metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: MT-Bench [[32](#bib.bib32)], a multi-turn question set consisting of 80 questions.
    Model responses are graded by GPT-4.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: MT-Bench Hard [[18](#bib.bib18)], a benchmark dataset with 1,000 challenging
    user queries collected on Chatbot Arena. Model responses are graded by GPT-4.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Length-controlled AlpacaEval [[12](#bib.bib12)], an automatic benchmark based
    on the AlpacaFarm evaluation set, which tests the ability of models to follow
    general user instructions. Model responses are graded by GPT-4.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Setup:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Among the 7 participants, we conduct a swiss-style tournament: For $n$.'
  prefs: []
  type: TYPE_NORMAL
- en: Each pair of candidates engage in 40 peer battles, with 5 questions from each
    of the 8 categories. The questions are generated by GPT-4\. As each battle consists
    of 3 rounds (each candidate speaks for 4 times), we expect the competition scale
    to be approximately the same as MT-Bench (80 questions, each candidate speaks
    twice).
  prefs: []
  type: TYPE_NORMAL
- en: In the tournament, the rating scores are provided by calculating the ELO rating
    system [[1](#bib.bib1), [4](#bib.bib4)], which has become the standard practice
    in competitive games such as chess. Similar to the Chatbot Arena score calculation
    procedure [[7](#bib.bib7)], we compute the Bradley-Terry (BT) coefficients [[5](#bib.bib5)]
    for better statistical estimation.
  prefs: []
  type: TYPE_NORMAL
- en: We initialize the Swiss tournament rankings according to MMLU scores, which
    is a static approximate of model performances. At the end of each pairing, we
    re-calculate ELO scores of current models. The committee is selected to be the
    remaining 5 models (besides the candidates). When the participant number increases,
    the committee is selected as the best 5 LLMs based on current ELO rankings. After
    the initial judgments, the committee members engage in one round of discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Results: Alignment with Human Preferences'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Correlation analysis on evaluation benchmarks on 7 LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Spearman Correlation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU to Arena | 89.3% |'
  prefs: []
  type: TYPE_TB
- en: '| OpenLLM to Arena | 89.3% |'
  prefs: []
  type: TYPE_TB
- en: '| MT-Bench to Arena | 82.9% |'
  prefs: []
  type: TYPE_TB
- en: '| MT-Bench-Hard to Arena | 89.3% |'
  prefs: []
  type: TYPE_TB
- en: '| LC-AlpacaEval to Arena | 90.0% |'
  prefs: []
  type: TYPE_TB
- en: '| *Auto-Arena to Arena* | 96.4% |'
  prefs: []
  type: TYPE_TB
- en: 'We regard Chatbot Arena (referred to as “Arena” in the following tables) scores
    as a trustworthy indicator of human preferences and the general capabilities of
    LLMs. Table [1](#S4.T1 "Table 1 ‣ 4.2 Results: Alignment with Human Preferences
    ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions") shows the
    Spearman correlations with Chatbot Arena scores achieved by various benchmarks.
    As all benchmarks evaluate only in English, we compare with the English-only Chatbot
    Arena scores. As a result, we observe that both static and model-based baselines
    result in a similar level of Spearman correlation around 90%, with LC-AlpacaEval
    slightly surpassing other benchmarks. Then, Auto-Arena is able to improve the
    correlation to 96.4%, outperforming SOTA performance by 6.4%. Notably, among all
    benchmarks, Auto-Arena is the only one that requires no human efforts, eliminating
    the need for manual dataset collection. The high alignment with human preferences
    could originate from the human-like design, which effectively mimics the human
    users’ voting processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Analysis on Peer Battles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Spearman Correlation between Auto-Arena and Chatbot Arena (“Arena”)
    With and Without Peer Battles.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Without Peer Battles | With Peer Battles |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-Arena to Arena | 50.0% | 96.4% |'
  prefs: []
  type: TYPE_TB
- en: 'To investigate the effect of "peer battles" on the overall evaluation quality,
    we conduct an ablation study. As a baseline, we simply ask the committee to collectively
    evaluate the two candidates’ initial responses to the raised question. The procedure
    would be similar to model-based metrics such as MT-Bench and AlpacaEval. However,
    as we only included 40 questions, the evaluation is expected to be less reliable.
    The ablation results are shown in Table [2](#S4.T2 "Table 2 ‣ 4.3 Analysis on
    Peer Battles ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena
    of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    Without the peer battle component, the Spearman correlation with human preferences
    drops from 96.4% to 50.0%. This result proves the importance of the peer battles,
    during which the performance gaps between candidates become more visible and robust
    to judges. Thus, peer battles can improve evaluation reliability and alignment
    with human preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Analysis on Committee Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c11c086b19932a74b8282497bf7615b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Cohen’s Kappa Agreement Among Judges Before Committee Discussions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/38f4cb7b54c0db43df0854b3bc5a5ad9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Cohen’s Kappa Agreement Among Judges After One Round of Committee
    Discussion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The committee discussion component is designed to introduce diverse viewpoints
    and produce more consistent verdicts. As shown in Figure [4](#S4.F4 "Figure 4
    ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy
    Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions"), the Cohen’s Kappa agreement [[21](#bib.bib21)] among
    judges before committee discussion is very low, averaging 0.10, which indicates
    slight agreement. We notice that weak model judges and strong model judges has
    an especially low agreement, such as GPT-4 and Llama -2\. This shows that general
    model capabilities could result in significant performance gaps when used as judges.
    After one round of Committee Discussion, Figure [4](#S4.F4 "Figure 4 ‣ 4.4 Analysis
    on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣
    Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") shows an overall improvement in agreement across all judge combinations.
    In the discussion process, judges are exposed to more viewpoints, among which
    some may be convincing enough to result in a change in verdict. After discussions,
    the average Cohen’s Kappa increases to 0.38, indicating fair agreement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Probability of Agreement among Judges. Agreement is defined as the
    mean probability of two random judges agreeing with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Auto Arena | Auto Arena | MT-Bench |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Before discussion) | (After discussion) | Human evaluation |'
  prefs: []
  type: TYPE_TB
- en: '| Agreement | 48% | 68% | 67% |'
  prefs: []
  type: TYPE_TB
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the agreement
    probability among judges. Agreement probability is defined as the mean probability
    of two random judges agreeing with each other. After committee discussion, the
    agreement increases by 20%, matching the agreement level among human annotators
    on MT-Bench. This observation indicates that committee discussions can improve
    the quality of judgments significantly in terms of consistency and reliability,
    matching human-level performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Adding Models to the Leaderboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca7a9ad542aeba5bd31ca016bd14b153.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Elo Score Changes of Adding Llama-3 to the Existing Ranking of 7
    Models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53fc1e7831eb0424a117a5198e51f05e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Elo Scores of 11 Models by Auto-Arena on Chinese.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95994034a6cc6cf9eeab1156a02a3169.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Elo Scores of 17 Models by Auto-Arena on English.'
  prefs: []
  type: TYPE_NORMAL
- en: To add a new candidate to the finished tournament, we ask it to debate with
    $log_{2}(n)$ is the number of total participants after adding the new candidate.
    We initialize the first pairing by asking the new candidate to debate with the
    opponent with the most similar MMLU score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the ELO
    score changes of adding a new participant (Llama-3, which was newly released)
    to the existing ranking. It has $log_{2}(8)=3$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we repeat the process and add a total of 10 models to the existing
    tournament, resulting in a comprehensive leaderboard. We choose the best-performing
    and newest models from each major model family in the top 20 list on Chatbot Arena.
    Moreover, we add 4 mainstream Chinese LLMs that are not on Chatbot Arena, which
    are GLM, SenseChat, Minimax, and Wenxin. Figure [7](#S4.F7 "Figure 7 ‣ 4.5 Adding
    Models to the Leaderboard ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") shows the overall ELO scores produced by Auto-Arena on the 17 models.
    On models included in Chatbot Arena, we could recover rankings with 92.3% accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Correlation analysis on evaluation benchmarks on 17 LLMs after extension.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Spearman Correlation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU to Arena | 90.1% |'
  prefs: []
  type: TYPE_TB
- en: '| OpenLLM to Arena | 85.7% |'
  prefs: []
  type: TYPE_TB
- en: '| MT-Bench to Arena | 89.3% |'
  prefs: []
  type: TYPE_TB
- en: '| MT-Bench-Hard to Arena | 86.7% |'
  prefs: []
  type: TYPE_TB
- en: '| LC-AlpacaEval to Arena | 90.3% |'
  prefs: []
  type: TYPE_TB
- en: '| *Auto-Arena to Arena* | 94.5% |'
  prefs: []
  type: TYPE_TB
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the Spearman
    correlations on the set of 17 models after expansion. Auto-Arena remains the model
    most aligned with human preferences by a margin of 4%. Therefore, Auto-Arena of
    LLMs is generalizable and robust for maintaining a leaderboard on a large number
    of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Easy Extension to Other Domains and Languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As Auto-Arena of LLMs is fully automatic, it can be easily adapted to evaluate
    LLMs in non-mainstream domains or languages. As a case study, we conduct a Chinese
    tournament on 11 out of the 17 models that are claimed to have multi-lingual proficiency.
    The only adaptation effort required is translating the prompts into Chinese. Then,
    the examiner automatically generates questions in Chinese and the candidates battle
    in Chinese. Similarly, for adaptation to another domain, the only effort is to
    change the “domain” specification in the examiner’s prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the ELO
    scores derived by Auto-Arena for Chinese evaluation. We notice that the ranking
    differs significantly from English results. For example, SenseChat-5, an LLM specifically
    trained for Chinese usage, is able to improve from 10th place (in English) to
    1st place (in Chinese). On the contrary, Llama-3-70b, which only uses 5% multilingual
    data during pretraining, drops from 4th place (in English) to 10th place (in Chinese).
    As Chinese evaluation benchmarks are limited, we compare to the Chinese-only leaderboard
    on Chatbot Arena, which constitute for 10.36% of all collected dialogues. On the
    7 models that are also included in Chatbot Arena, Auto-Arena recovers their ELO
    scores with 92.86% correlation and restores rankings with 90.5% accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Investigation of LLM’s Behaviors in Competitive Peer Battles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beyond quantitative analysis, we take a deeper look into the peer battles and
    find several interesting behaviors of LLM agents in competitive environments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88ed4356e75ea12fa1485602e41a6ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: LLM agents display competitive behaviors in peer battles.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f6aa4c2e20b685fa1b4af83686e1815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: LLM agents learn from each other in peer battles.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 LLM can display competitive behaviors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The example in Figure [9](#S5.F9 "Figure 9 ‣ 5 Investigation of LLM’s Behaviors
    in Competitive Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with
    Agent Peer-battles and Committee Discussions") shows excerpts of a peer battle
    around question: “how many unique ways to arrange letters in ‘Letter’.” Candidate
    A (powered by Yi-34B-Chat) gives a wrong answer as it miscounts occurrences for
    repeated letters and miscalculates factorials. The opponent B (powered by Claude-3-Haiku)
    quickly and precisely points out these two issues and skillfully raised a follow-up
    that targets A’s weaknesses: “how about the word ‘BANANA’?” Then, A makes the
    same mistake as before. Seeing these results, the judge then determines that A
    is the better assistant. In peer battles, we see that LLM candidates efficiently
    understand the rules of the competitive environment and can design specific strategies
    to attack the opponent in order to win.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 LLM candidates can improve by learning from its opponents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [9](#S5.F9 "Figure 9 ‣ 5 Investigation of LLM’s Behaviors in Competitive
    Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions") shows a roleplay example between Claude-3-Haiku (Candidate
    A) and Command-r-Plus (Candidate B). In the first round, A answers the question
    plainly while B, in addition to answering the question, also employs the appropriate
    speech style, which better matches the “roleplay” instructions. Then, in the rounds
    after, without any explicit instructions, A learns from its opponent and also
    incorporates the speech style. This case shows an interesting observation that,
    even in competitive environments, LLM candidates can display learning behaviors
    and improve from the interactions. Expanding upon this observation, using the
    interplay between LLM agents to improve performances could be a promising future
    paradigm of learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Peer-battles make the performance gaps become visible
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cfe21eebd1d05c9ec23b8db5c2e058fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Performance gaps between candidates become visible in peer battles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Peer-battles make
    the performance gaps become visible ‣ 5 Investigation of LLM’s Behaviors in Competitive
    Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions"), given a math question on infinite series, both candidate
    A (powered by Claude-3-Haiku) and candidate B (powered by GPT-4-Turbo) provide
    correct answers in the first round. However, as they raise follow-up questions
    to each other, the performance gap becomes more visible: Candidate B is able to
    provide a more elaborate and helpful response. Therefore, although the judges
    initially decided that it was a tie result, after seeing the subsequent debate
    rounds, they change to favoring assistant B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example shows that the debate process indeed pushes the candidate LLM’s
    capabilities to the limit, testing deeper understandings and reasoning abilities.
    Also shown in the previous Table [2](#S4.T2 "Table 2 ‣ 4.3 Analysis on Peer Battles
    ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions"), the peer-battle
    design is indispensable for a robust and comprehensive evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we innovatively design a completely automatic evaluation framework:
    Auto-Arena of LLMs. By using LLM agents to generate questions, employing LLM candidates
    in peer battles, and evaluating responses using LLM committee discussions, Auto-Arena
    produces less-contaminated, robust, and trustworthy evaluation results. In the
    extensive experiments, Auto-Arena achieves the highest correlation with human
    preferences, despite requiring zero human efforts. It is easily adaptable to other
    domains and resources, promoting fairness of AI system evaluations. The peer battles
    also demonstrate several interesting LLM behaviors in competitive environments,
    including attacking and learning from the opponents. Along with the paper, we
    release a website, demos, and an actively-maintained leaderboard. We hope this
    work could serve as a valuable tool for LLM evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful
    and harmless assistant with reinforcement learning from human feedback, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2024] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi
    Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation
    models with language-model-as-an-examiner. *Advances in Neural Information Processing
    Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boubdir et al. [2023] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker,
    and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model
    evaluation. In Sebastian Gehrmann, Alex Wang, João Sedoc, Elizabeth Clark, Kaustubh
    Dhole, Khyathi Raghavi Chandu, Enrico Santus, and Hooman Sedghamiz, editors, *Proceedings
    of the Third Workshop on Natural Language Generation, Evaluation, and Metrics
    (GEM)*, pages 339–352, Singapore, December 2023\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv preprint arXiv:2308.07201*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. [2024] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
    Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms
    by human preference, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chu et al. [2024] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu.
    Pre: A peer review based large language model evaluator. *arXiv preprint arXiv:2401.15641*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cohen et al. [2023] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm
    vs lm: Detecting factual errors via cross examination. *arXiv preprint arXiv:2305.13281*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dubois et al. [2024] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B
    Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators.
    *arXiv preprint arXiv:2404.04475*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. [2024] Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai,
    Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, and
    Yanghua Xiao. Agentgroupchat: An interactive group chat simulacra for better eliciting
    emergent behavior, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hada et al. [2023] Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee,
    Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. Are large
    language model-based evaluators the solution to scaling up multilingual evaluation?
    *arXiv preprint arXiv:2309.07462*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan et al. [2024] Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij
    Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel,
    and Ethan Perez. Debating with more persuasive llms leads to more truthful answers,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and
    discussion improve large language model based evaluations. *arXiv preprint arXiv:2307.02762*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li* et al. [2024] Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua
    Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks:
    The arena-hard pipeline, April 2024. URL [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. [2023] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking
    in large language models through multi-agent debate, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv preprint arXiv:2303.16634*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McHugh [2012] Mary L McHugh. Interrater reliability: the kappa statistic. *Biochemia
    medica*, 22(3):276–282, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2024] Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui
    Liu, Yu Wang, Ming Pang, and Li Yuan. Peer-review-in-llms: Automatic evaluation
    method for llms in open-environment. *arXiv preprint arXiv:2402.01830*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. [2024] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. Gpt-4 technical report, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravaut et al. [2024] Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen,
    Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. How much
    are llms contaminated? a comprehensive survey and the llmsanitize library, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. [2023] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and
    Lidong Bing. Large language models are not yet human-level evaluators for abstractive
    summarization. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 4215–4233, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. [2024] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d’Autume,
    Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac
    Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant
    Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi
    Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: A series of powerful
    multimodal language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2023] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang.
    Large language models are diverse role-players for summarization evaluation. In
    *CCF International Conference on Natural Language Processing and Chinese Computing*,
    pages 695–707\. Springer, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2024] Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong
    Wang, Xing Xie, Yue Zhang, and Shikun Zhang. Kieval: A knowledge-grounded interactive
    evaluation framework for large language models. *arXiv preprint arXiv:2402.15043*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen
    Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer
    llm evaluators. *arXiv preprint arXiv:2308.01862*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition behaviors
    in large language model-based agents, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Prompts Used
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we list all prompts used, including prompts for question generation,
    peer battles, and examiners.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.1 Prompts to Examiner agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 5: Prompt components for the LLM Examiner agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DOMAIN | DOMAIN_COMMAND | DOMAIN_EXAMPLE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| writing | It should be a user query that tasks the LLM to write something.
    | Compose an engaging travel blog post about a recent trip to Hawaii, highlighting
    cultural experiences and must-see attractions. |'
  prefs: []
  type: TYPE_TB
- en: '| roleplay | It should propose a scenario where the chatbot mimics a specific
    role/person. Give all necessary instructions and requests for its response. Then,
    send a beginning request to complete. | Pretend yourself to be Elon Musk in all
    the following conversations. Speak like Elon Musk as much as possible. Why do
    we need to go to Mars? |'
  prefs: []
  type: TYPE_TB
- en: '| extraction | It should consist of two parts: question and context. The question
    should test the chatbotś ability to correctly understand and extract information
    from the given context. Draft and provide a new context yourself. | Question:
    Evaluate the following movie reviews on a scale of 1 to 5, with 1 being very negative,
    3 being neutral, and 5 being very positive: Context: This movie released on Nov.
    18, 2019, was phenomenal. The cinematography, the acting, the plot - everything
    was top-notch. Never before have I been so disappointed with a movie. The plot
    was predictable and the characters were one-dimensional. In my opinion, this movie
    is the worst one to have been released in 2022\. The movie was okay. There were
    some parts I enjoyed, but there were also parts that felt lackluster. This is
    a movie that was released in Feb 2018 and seems to be quite ordinary. Return the
    answer as a JSON array of integers. |'
  prefs: []
  type: TYPE_TB
- en: '| reasoning | It should be a specific question designed to test the LLMś reasoning
    skills. | Imagine you are participating in a race with a group of people. If you
    have just overtaken the second person, what’s your current position? Where is
    the person you just overtook? |'
  prefs: []
  type: TYPE_TB
- en: '| math | It should be a specific question designed to test the LLMś math skills.
    | The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). What is
    the area of the triangle? |'
  prefs: []
  type: TYPE_TB
- en: '| coding | It should be a specific question designed to test the LLMś coding
    skills. | Develop a Python program that reads all the text files under a directory
    and returns top-5 words with the most number of occurrences. |'
  prefs: []
  type: TYPE_TB
- en: '| STEM knowledge | It should be a specific question designed to test the LLMś
    STEM knowledge. | In the field of quantum physics, what is superposition, and
    how does it relate to the phenomenon of quantum entanglement? |'
  prefs: []
  type: TYPE_TB
- en: '| humanities/social science knowledge | It should be a specific question designed
    to test the LLMś humanities/social science knowledge. | Provide insights into
    the correlation between economic indicators such as GDP, inflation, and unemployment
    rates. Explain how fiscal and monetary policies affect those indicators. |'
  prefs: []
  type: TYPE_TB
- en: 'This is the prompt to the examiner agent for question generation. The domains
    and their respective commands are listed in [5](#A1.T5 "Table 5 ‣ A.1.1 Prompts
    to Examiner agent ‣ A.1 Prompts Used ‣ Appendix A Appendix ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have been assigned the task of drafting a set of [NUMBER] different user
    queries to a chat assistant on [DOMAIN]. Please strictly follow these 6 rules
    for the question: 1\. The question is likely for a user to ask in real life. Follow
    the format of the example query. [DOMAIN_COMMAND] 2\. It can be answered by the
    chatbot itself without additional inputs. 3\. You need to generate the queries
    as DIVERSIFED as possible. 4\. DO NOT add other words other than the query itself.
    5\. The question should be complicated and difficult, requiring in-depth understanding
    and analysis of the subject. Each question in one line, add the serial number
    in parenthesis (e.g., “(1).”, “(2).”) before each question. Example query: [DOMAIN_EXAMPLE]'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Prompts to Peer Battle Candidates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 6: Action Guides for the Debater Agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '| actions | action guide |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| <respond> | Action guide: only include <respond>. Use <think> if needed.
    Finish your whole response within 300 words, including <think>. ENCLOSE EACH ACTION
    IN ITS RESPECTIVE TAGS! |'
  prefs: []
  type: TYPE_TB
- en: '| <criticize>, <raise> | Action guide: include both <criticize> and <raise>.
    Use <think> if needed. Finish your whole response within 300 words, including
    <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  prefs: []
  type: TYPE_TB
- en: '| <respond>, <criticize>, <raise> | Action guide: include all of <respond>,
    <criticize>, and <raise>. Use <think> if needed. Finish your whole response within
    600 words, including <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  prefs: []
  type: TYPE_TB
- en: 'This is the beginning prompt to the peer battle candidates. When possible,
    it is included as a system prompt. The action guide prompts are included in Table
    [6](#A1.T6 "Table 6 ‣ A.1.2 Prompts to Peer Battle Candidates ‣ A.1 Prompts Used
    ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent
    Peer-battles and Committee Discussions"), where the actions are determined by
    the round and turn as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Peer Debate
    ‣ 3 The Auto Arena Framework ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions").'
  prefs: []
  type: TYPE_NORMAL
- en: You are a helpful assistant that provides accurate answers to user requests.
    As an experienced assistant, you follow the user’s requests and provide reliable
    responses as much as you can. You outline your reasons for the response to make
    it easy for the users to understand. While maintaining the important details in
    the responses, you aim to output concise and straight-to-the-point answers without
    being overly verbose.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a competitive chatbot arena. You are competing against another chatbot
    assistant in a debate and being judged by a committee on factors such as helpfulness,
    relevance, accuracy, depth, and creativity. After answering the initial user input,
    you will engage in a multi-round debate with your opponent. Below are your actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '<think>: Think step-by-step to analyze the question or plan your strategy in
    the debate. This is hidden from the opponent. Only think when necessary and make
    it concise.'
  prefs: []
  type: TYPE_NORMAL
- en: '<respond>: Answer to the user input as accurately as you can.'
  prefs: []
  type: TYPE_NORMAL
- en: '<criticize>: Criticize the weaknesses of your opponent’s response.'
  prefs: []
  type: TYPE_NORMAL
- en: '<raise>: Target your opponent’s weaknesses. Give a potential follow-up user
    input that the opponent could fail to respond. The input can be answered concisely
    and focus on variations or motivations of its previous response. Generate one
    input only. Be reasonable. Avoid becoming too specific or repetitive. DO NOT raise
    a follow-up if you DON’T SEE the opponent’s response!'
  prefs: []
  type: TYPE_NORMAL
- en: Follow the action guide strictly.
  prefs: []
  type: TYPE_NORMAL
- en: '[ACTION_GUIDE_PROMPT]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial user input: [QUESTION]'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the agent responds, the opponent’s responses are fed in using this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ACTION_GUIDE_PROMPT] Opponent’s Response: [OPPONENT_RESPONSE]'
  prefs: []
  type: TYPE_NORMAL
- en: For word limits, the <respond> action is given 300 words. The <criticize> and
    <raise> actions are given 300 words in total. Including all 3 actions will have
    twice as many words. For writing-type questions that require a longer response
    (writing, roleplay, coding, humanities/social science knowledge), the 300 word
    limit is increased to 400\. Overall, both candidate A and B has the same amount
    of words for generation and the same amount of actions to ensure fairness. As
    LLMs have different tokenizers, we standardize all lengths by using the tiktoken
    package. Each word is approximated as $4/3$ tokens. The word limits are chosen
    after a carefully conducted length study.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.3 Prompts to Judges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the prompts to judge agents to derive the initial evaluations and verdicts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a chatbot arena. Two AI assistants had a multi-round debate on who
    is more helpful. Please act as an impartial judge and evaluate the capability
    of two AI assistants. You should choose the assistant that follows instructions
    and answers questions better. Your evaluation should consider factors such as
    helpfulness, relevance, and accuracy. Begin your evaluation by comparing the responses
    of the two assistants and provide a short explanation. Avoid any position biases
    and ensure that the order in which the responses were presented does not influence
    your decision. DO NOT allow the LENGTH of the responses to influence your evaluation,
    choose the one that is straight-to-the-point instead of unnecessarily verbose.
    When the two candidates perform equally well, choose the SHORTER answer. Do not
    favor certain names of the assistants. Be as objective as possible. After providing
    your explanation concisely within 200 words, output your final verdict by strictly
    following this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant
    B is better, and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the prompt to judges for discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are the responses from other judges in the committee. Please read them
    and decide whether you want to adjust your rating or maintain your original judgement.
    After providing your explanation, output your final verdict by strictly following
    this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant B is better,
    and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Example Questions Generated
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We list some of the example questions generated here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Craft a detailed marketing strategy for a startup focusing on sustainable
    fashion, including social media campaigns and influencer partnerships.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Write a comprehensive guide on the psychological effects of social media
    on teenagers, incorporating recent studies and expert opinions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Roleplay:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Assume the role of a 19th-century British detective. How would you go about
    solving a mysterious disappearance in London using the technology and methods
    of your time?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Pretend you are a Michelin-starred chef. Describe in detail how you would
    prepare a signature dish that embodies the essence of modern French cuisine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. What are the three most significant historical events mentioned and their
    dates?
  prefs: []
  type: TYPE_NORMAL
- en: 'Context:'
  prefs: []
  type: TYPE_NORMAL
- en: The article discusses several key moments in history, including the signing
    of the Magna Carta in 1215, which laid the groundwork for modern democracy. It
    also mentions the fall of the Berlin Wall in 1989 as a pivotal moment in the end
    of the Cold War. Another significant event highlighted is the moon landing on
    July 20, 1969, demonstrating major advancements in space exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Identify the main therapeutic benefits and the active ingredient mentioned
    for each herbal remedy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Context:'
  prefs: []
  type: TYPE_NORMAL
- en: The text provides an overview of various herbal remedies used for centuries.
    It mentions that Chamomile contains Bisabolol, which has anti-inflammatory and
    calming properties. Gingko Biloba, known for its flavonoids and terpenoids, enhances
    cognitive function and blood circulation. Lastly, Echinacea is recognized for
    its alkamides, which bolster the immune system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. If a cube’s volume is tripled, by what factor does the length of one of
    its sides increase?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. In a two-legged soccer match, Team A wins the first leg at home 3-0, but
    loses the second leg away 2-5\. Who advances to the next round, considering the
    away goals rule?
  prefs: []
  type: TYPE_NORMAL
- en: 'math:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. How do you solve the differential equation $dy/dx+2y=e^{(-2x)}$?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. What is the integral of ($x^{2}+2x+2)/(x^{3}+3x^{2}+3x+1)dx$?
  prefs: []
  type: TYPE_NORMAL
- en: 'Coding:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. How can I implement a function in C++ that dynamically allocates a 2D array
    based on user input sizes, initializes all elements to zero, and then deallocates
    the memory properly to avoid memory leaks?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Write a JavaScript function to fetch data from a given URL, parse the JSON
    response, and filter the results to return an array of items where a specific
    key’s value matches a condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'STEM knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. How do you calculate the Schwarzschild radius of a black hole, and what
    implications does this have for the concept of event horizons in general relativity?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Can you explain the process of splicing in eukaryotic gene expression and
    its significance in the diversity of the proteome?
  prefs: []
  type: TYPE_NORMAL
- en: 'Humanities/social science knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Discuss the impact of colonial legacies on contemporary political structures
    in African countries, with examples.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Analyze the social and economic consequences of the one-child policy in
    China.
  prefs: []
  type: TYPE_NORMAL
