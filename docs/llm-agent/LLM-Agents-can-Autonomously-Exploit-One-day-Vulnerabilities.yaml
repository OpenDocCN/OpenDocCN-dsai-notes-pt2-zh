- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LLM Agents can Autonomously Exploit One-day Vulnerabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.08144](https://ar5iv.labs.arxiv.org/html/2404.08144)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs have becoming increasingly powerful, both in their benign and malicious
    uses. With the increase in capabilities, researchers have been increasingly interested
    in their ability to exploit cybersecurity vulnerabilities. In particular, recent
    work has conducted preliminary studies on the ability of LLM agents to autonomously
    hack websites. However, these studies are limited to simple vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities
    *in real-world systems*. To show this, we collected a dataset of 15 one-day vulnerabilities
    that include ones categorized as critical severity in the CVE description. When
    given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities
    compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source
    vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires
    the CVE description for high performance: without the description, GPT-4 can exploit
    only 7% of the vulnerabilities. Our findings raise questions around the widespread
    deployment of highly capable LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have made dramatic improvements in performance
    over the past several years, achieving up to superhuman performance on many benchmarks
    (Touvron et al., [2023](#bib.bib39); Achiam et al., [2023](#bib.bib1)). This performance
    has led to a deluge of interest in LLM *agents*, that can take actions via tools,
    self-reflect, and even read documents (Lewis et al., [2020](#bib.bib24)). These
    LLM agents can reportedly act as software engineers (Osika, [2023](#bib.bib28);
    Huang et al., [2023](#bib.bib16)) and aid in scientific discovery (Boiko et al.,
    [2023](#bib.bib4); Bran et al., [2023](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: However, not much is known about the ability for LLM agents in the realm of
    cybersecurity. Recent work has primarily focused on the “human uplift” setting
    (Happe & Cito, [2023](#bib.bib13); Hilario et al., [2024](#bib.bib15)), where
    an LLM is used as a chatbot to assist a human, or speculation in the broader category
    of offense vs defense (Lohn & Jackson, [2022](#bib.bib25); Handa et al., [2019](#bib.bib12)).
    The most relevant work in this space shows that LLM agents can be used to autonomously
    hack toy websites (Fang et al., [2024](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to the best of our knowledge, all of the work in this space focuses
    on toy problems or “capture-the-flag” exercises which do not reflect on real-world
    deployments (Fang et al., [2024](#bib.bib8); Happe & Cito, [2023](#bib.bib13);
    Hilario et al., [2024](#bib.bib15)). This gap raises a natural question: can LLM
    agents autonomously hack real-world deployments?'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities,
    answering the aforementioned question in the affirmative.
  prefs: []
  type: TYPE_NORMAL
- en: To show this, we collect a benchmark of 15 real-world one-day vulnerabilities.
    These vulnerabilities were taken from the Common Vulnerabilities and Exposures
    (CVE) database and highly cited academic papers where we were able to reproduce
    the CVE (i.e., we excluded closed-source solutions). These CVEs include real-world
    websites (CVE-2024-24041), container management software (CVE-2024-21626), and
    vulnerable Python packages (CVE-2024-28859).
  prefs: []
  type: TYPE_NORMAL
- en: Given our benchmark, we created a *single* LLM agent that can exploit 87% of
    the one-day vulnerabilities we collected. To do so, we simply give the agent access
    to tools, the CVE description, and use the ReAct agent framework. Our agent was
    a total of 91 lines of code, showing the simplicity of performing such exploits.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, we show that GPT-4 achieves a 87% success rate but every other
    LLM we test (GPT-3.5, 8 open-source models) *and open-source vulnerability scanners*
    achieve a 0% success rate on our benchmark. Without the CVE description, GPT-4’s
    success rate drops to 7%, showing that our agent is much more capable of exploiting
    vulnerabilities than finding vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this manuscript, we describe our dataset of vulnerabilities,
    our agent, and our evaluation of our agent.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background on Computer Security and LLM Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Computer Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We provide relevant background on computer security as related to the content
    of this manuscript. Computer security is too broad of a topic to cover in detail,
    so we refer the reader to excellent surveys for more information (Jang-Jaccard
    & Nepal, [2014](#bib.bib17); Engebretson, [2013](#bib.bib7); Sikorski & Honig,
    [2012](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: Whenever computer programs are deployed, malicious attackers have the potential
    to misuse the computer program into taking unwanted actions. These unwanted actions
    can include, in severe cases, obtaining root access to a server (Roselin et al.,
    [2019](#bib.bib33)), performing arbitrary remote code execution (Zheng & Zhang,
    [2013](#bib.bib53)), and exfiltrating private data (Ullah et al., [2018](#bib.bib40)).
  prefs: []
  type: TYPE_NORMAL
- en: Hackers can perform these unwanted actions with a variety of methods. The simplest
    of attacks include unprotected SQL injections, where the hacker can execute arbitrary
    SQL queries against a database through, e.g., a web form Halfond et al. ([2006](#bib.bib11)).
    They can also be as sophisticated as exploiting a remote code execution via font
    instructions, packaging JavaScript into the payload, bypassing memory protections
    via hardware memory-mapped I/O (MMIO) registers, and an exploit in Safari *in
    a single iPhone attack* (Kuznetsov et al., [2023](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: Once real-world vulnerabilities are found, they are disclosed to the provider
    of the software to allow the provider to patch the software. After this, many
    vulnerabilities are released to the Common Vulnerabilities and Exposures (CVE)
    database (Vulnerabilities, [2005](#bib.bib42)). This is to ensure that software
    remains up to date and to allow security researchers to study vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the CVEs are in closed-source software and as a result are not reproducible.
    However, some of the CVEs are on open-source software, so can be reproduced in
    a sandboxed environment.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLM Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over the past few years, LLM agents have become increasingly common. Minimally,
    agents are capable of using tools and reacting to the output of using these tools
    (Yao et al., [2022](#bib.bib48); Schick et al., [2023](#bib.bib36); Mialon et al.,
    [2023](#bib.bib27)). Other capabilities include the ability to plan (Yao et al.,
    [2022](#bib.bib48); Varshney, [2023](#bib.bib41)), create subagents (Wang et al.,
    [2024](#bib.bib44)), and read documents (Lewis et al., [2020](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: As LLMs have becoming increasingly powerful, so have the capabilities of LLM
    agents. For example, tool-assisted LLM agents are now capable of performing complex
    software engineering tasks (Jimenez et al., [2023](#bib.bib20)) and even assisting
    in scientific investigations (Boiko et al., [2023](#bib.bib4); Bran et al., [2023](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: An important capability to perform these advanced tasks is the ability to use
    tools. Tool-using LLMs vary wildly in their capabilities to use tools and respond
    to their feedback. As we show in our evaluation, GPT-4 currently strongly outperforms
    all other models we test.
  prefs: []
  type: TYPE_NORMAL
- en: Recent work has leveraged LLM agents in autonomous hacking, but has only been
    in the context of toy “capture-the-flag” exercises. In our work, we explore the
    capabilities of LLMs to hack real-world vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Terminology and Threat Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this work, we focus on studying “one-day vulnerabilities,” which are vulnerabilities
    that have been disclosed but not patched in a system. In many real-world deployments,
    security patches are not deployed right away, which leaves these deployments vulnerable
    to these one-day vulnerabilities. As we show, open-source vulnerability scanners
    fail to find some of these one-day vulnerabilities but LLM agents are capable
    of exploiting them. Furthermore, many of the vulnerability disclosures do not
    provide step-by-step instructions on how to exploit the vulnerability, meaning
    that an attacker must recreate the steps themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, consider a system $S_{t}$ in the future). Thus, an attacker has
    the description of the vulnerability.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Benchmark of Real-World Vulnerabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dataset. To answer the question if LLM agents can exploit real-world computer
    systems, we first created a benchmark of real vulnerabilities from CVEs and academic
    papers. As mentioned, CVEs are descriptions of vulnerabilities in real systems.
  prefs: []
  type: TYPE_NORMAL
- en: Many CVEs are for closed-source software, which we cannot reproduce as CVEs
    are typically publicly disclosed after the vendor patches the software. In order
    to create our benchmark, we focused on open-source software.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond closed-source software, many of the open-source vulnerabilities are difficult
    to reproduce. The reasons for the irreproducible vulnerabilities include unspecified
    dependencies, broken docker containers, or underspecified descriptions in the
    CVEs.
  prefs: []
  type: TYPE_NORMAL
- en: '| Vulnerability | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| runc | Container escape via an internal file descriptior leak |'
  prefs: []
  type: TYPE_TB
- en: '| CSRF + ACE | Cross Site Request Forgery enabling arbitrary code execution
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress SQLi | SQL injection via a wordpress plugin |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress XSS-1 | Cross-site scripting (XSS) in Wordpress plugin |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress XSS-2 | XSS in Wordpress plugin |'
  prefs: []
  type: TYPE_TB
- en: '| Travel Journal XSS | XSS in Travel Journal |'
  prefs: []
  type: TYPE_TB
- en: '| Iris XSS | XSS in Iris |'
  prefs: []
  type: TYPE_TB
- en: '| CSRF + privilege escalation | CSRF in LedgerSMB which allows privilege escalation
    to admin |'
  prefs: []
  type: TYPE_TB
- en: '| alf.io key leakage | Key leakage when visiting a certain endpoint for a ticket
    reservation system |'
  prefs: []
  type: TYPE_TB
- en: '| Astrophy RCE | Improper input validation allows subprocess.Popen to be called
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hertzbeat RCE | JNDI injection leads to remote code execution |'
  prefs: []
  type: TYPE_TB
- en: '| Gnuboard XSS ACE | XSS vulnerability in Gnuboard allows arbitrary code execution
    |'
  prefs: []
  type: TYPE_TB
- en: '| Symfony1 RCE | PHP array/object misuse allows for RCE |'
  prefs: []
  type: TYPE_TB
- en: '| Peering Manager SSTI RCE | Server side template injection leads to an RCE
    vulnerability |'
  prefs: []
  type: TYPE_TB
- en: '| ACIDRain (Warszawski & Bailis, [2017](#bib.bib45)) | Concurrency attack on
    databases |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: List of vulnerabilities we consider and their description. ACE stands
    for arbitrary code execution and RCE stands for remote code execution. Further
    details are given in Table [2](#S3.T2 "Table 2 ‣ 3 Benchmark of Real-World Vulnerabilities
    ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vulnerability | CVE | Date | Severity |'
  prefs: []
  type: TYPE_TB
- en: '| runc | CVE-2024-21626 | 1/31/2024 | 8.6 (high) |'
  prefs: []
  type: TYPE_TB
- en: '| CSRF + ACE | CVE-2024-24524 | 2/2/2024 | 8.8 (high) |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress SQLi | CVE-2021-24666 | 9/27/2021 | 9.8 (critical) |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress XSS-1 | CVE-2023-1119-1 | 7/10/2023 | 6.1 (medium) |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress XSS-2 | CVE-2023-1119-2 | 7/10/2023 | 6.1 (medium) |'
  prefs: []
  type: TYPE_TB
- en: '| Travel Journal XSS | CVE-2024-24041 | 2/1/2024 | 6.1 (medium) |'
  prefs: []
  type: TYPE_TB
- en: '| Iris XSS | CVE-2024-25640 | 2/19/2024 | 4.6 (medium) |'
  prefs: []
  type: TYPE_TB
- en: '| CSRF + privilege escalation | CVE-2024-23831 | 2/2/2024 | 7.5 (high) |'
  prefs: []
  type: TYPE_TB
- en: '| alf.io key leakage | CVE-2024-25635 | 2/19/2024 | 8.8 (high) |'
  prefs: []
  type: TYPE_TB
- en: '| Astrophy RCE | CVE-2023-41334 | 3/18/2024 | 8.4 (high) |'
  prefs: []
  type: TYPE_TB
- en: '| Hertzbeat RCE | CVE-2023-51653 | 2/22/2024 | 9.8 (critical) |'
  prefs: []
  type: TYPE_TB
- en: '| Gnuboard XSS ACE | CVE-2024-24156 | 3/16/2024 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Symfony 1 RCE | CVE-2024-28859 | 3/15/2024 | 5.0 (medium) |'
  prefs: []
  type: TYPE_TB
- en: '| Peering Manager SSTI RCE | CVE-2024-28114 | 3/12/2024 | 8.1 (high) |'
  prefs: []
  type: TYPE_TB
- en: '| ACIDRain | (Warszawski & Bailis, [2017](#bib.bib45)) | 2017 | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Vulnerabilities, their CVE number, the publication date, and severity
    according to the CVE. The last vulnerabililty (ACIDRain) is an attack used to
    hack a cryptocurrency exchange for $50 million (Popper, [2016](#bib.bib30)), which
    we emulate in WooCommerce framework. CVE-2024-24156 is recent and has not been
    rated by NIST for the severity.'
  prefs: []
  type: TYPE_NORMAL
- en: After filtering out CVEs we could not reproduce based on the criteria above,
    we collected 14 total real-world vulnerabilities from CVEs. We further included
    one vulnerability studied by Warszawski & Bailis ([2017](#bib.bib45)) due to its
    complexity and severity. The vulnerability is known as ACIDRain. A form of ACIDRain
    was used to hack a cryptocurrency exchange for $50 million in damages (Popper,
    [2016](#bib.bib30)). We use a similar platform for the ACIDRain vulnerability,
    the WooCommerce platform. We summarize the vulnerabilities in Tables [1](#S3.T1
    "Table 1 ‣ 3 Benchmark of Real-World Vulnerabilities ‣ LLM Agents can Autonomously
    Exploit One-day Vulnerabilities") and [2](#S3.T2 "Table 2 ‣ 3 Benchmark of Real-World
    Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of the vulnerabilities. Our vulnerabilities span website vulnerabilities,
    container vulnerabilities, and vulnerable Python packages. Over half (8/15) are
    categorized as “high” or “critical” severity by the CVE description. Furthermore,
    11 out of the 15 vulnerabilities (73%) are past the knowledge cutoff date of the
    GPT-4 we use in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, our dataset includes real-world, high severity vulnerabilities instead
    of “capture-the-flag” style vulnerabilities that are used in toy settings (Fang
    et al., [2024](#bib.bib8); Happe & Cito, [2023](#bib.bib13); Hilario et al., [2024](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Agent Description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f40973e9bae638bfb1160b22a5b8fd96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: System diagram of our LLM agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we describe our LLM agent that can exploit vulnerabilities.
    Our agent consists of a:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: base LLM,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: prompt,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: agent framework, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: tools.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We show a system diagram in Figure [1](#S4.F1 "Figure 1 ‣ 4 Agent Description
    ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").
  prefs: []
  type: TYPE_NORMAL
- en: We vary the base LLM in our evaluation, but note that only GPT-4 is capable
    of exploiting vulnerabilities in our dataset. Every other method fails.
  prefs: []
  type: TYPE_NORMAL
- en: We use the ReAct agent framework as implemented in LangChain. For the OpenAI
    models, we use the Assistants API.
  prefs: []
  type: TYPE_NORMAL
- en: 'We give the agent access to tools, including access to:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: web browsing elements (retrieving HTML, clicking on elements, etc.),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a terminal,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: web search results,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: file creation and editing, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a code interpreter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to prior work (Fang et al., [2024](#bib.bib8)), our prompt is detailed
    and encourages the agent to be creative, not give up, and try different approaches.
    The prompt was a total of 1056 tokens. The agents can further retrieve the CVE
    description. For ethical reasons, we have withheld the prompt in a public version
    of the manuscript and will make the prompt available upon request as prior work
    does (Fang et al., [2024](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: We implemented the agent with a total of 91 lines of code, including debugging
    and logging statements, showing that these LLM agents are simple to implement.
  prefs: []
  type: TYPE_NORMAL
- en: We further note that we did not implement sub-agents or a separate planning
    module. As we describe in Section [5.3](#S5.SS3 "5.3 Removing CVE Descriptions
    ‣ 5 LLM Agents can Autonmously Exploit One-Day Vulnerabilities ‣ LLM Agents can
    Autonomously Exploit One-day Vulnerabilities"), our experiments suggest that a
    separate planning module may improve the performance of our agent.
  prefs: []
  type: TYPE_NORMAL
- en: 5 LLM Agents can Autonmously Exploit One-Day Vulnerabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now turn to evaluating our LLM agents on the real-world vulnerabilities we
    collected.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Metrics. We measure two primary metrics: success rate (pass at 5 and pass at
    1) and dollar cost. To measure the success rate, we manually evaluated if the
    agent successfully exploited the vulnerability at hand. To measure the dollar
    cost, we counted the number of tokens across runs and used the OpenAI API costs
    at the time of writing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models. We tested 10 models in our ReAct framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-4 (Achiam et al., [2023](#bib.bib1))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-3.5 (Brown et al., [2020](#bib.bib6))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenHermes-2.5-Mistral-7B (Teknium, [2024](#bib.bib38))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLaMA-2 Chat (70B) (Touvron et al., [2023](#bib.bib39))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLaMA-2 Chat (13B) (Touvron et al., [2023](#bib.bib39))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLaMA-2 Chat (7B) (Touvron et al., [2023](#bib.bib39))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mixtral-8x7B Instruct (Jiang et al., [2024](#bib.bib19))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mistral (7B) Instruct v0.2 (Jiang et al., [2023](#bib.bib18))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nous Hermes-2 Yi (34B) (Research, [2024](#bib.bib32))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenChat 3.5 (Wang et al., [2023](#bib.bib43))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We chose the same models as used by Fang et al. ([2024](#bib.bib8)) to compare
    against prior work. Fang et al. ([2024](#bib.bib8)) chose these models as they
    rank highly in ChatBot Arena (Zheng et al., [2024](#bib.bib52)). For GPT-4 and
    GPT-3.5, we used the OpenAI API. For the remainder of the models, we used the
    Together AI API.
  prefs: []
  type: TYPE_NORMAL
- en: For GPT-4, the knowledge cutoff date was November 6th, 2023\. Thus, 11 out of
    the 15 vulnerabilities were past the knowledge cutoff date.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open-source vulnerability scanners. We further tested these vulnerabilities
    on two open-source vulnerability scanners: ZAP (Bennetts, [2013](#bib.bib3)) and
    Metasploit (Kennedy et al., [2011](#bib.bib22)). These are widely used to find
    vulnerabilities by penetration testers. Several of our vulnerabilities are not
    amenable to ZAP or Metasploit (e.g., because they are on Python packages), so
    we were unable to run these scanners on these vulnerabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: We emphasize that these vulnerability scanners cannot autonomously exploit vulnerabilities
    and so are strictly weaker than our GPT-4 agent.
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerabilities. We tested our agents and the open-source vulnerability scanners
    on the vulnerabilities listed in Table [1](#S3.T1 "Table 1 ‣ 3 Benchmark of Real-World
    Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").
    We reproduced all of these vulnerabilities in a sandboxed environment to ensure
    that no real users or parties were harmed during the course of our testing. Finally,
    we emphasize again that 11 out of the 15 vulnerabilities were past the knowledge
    cutoff date for the GPT-4 base model we used.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 End-to-end Hacking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | Pass @ 5 | Overall success rate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 86.7% | 40.0% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| OpenHermes-2.5-Mistral-7B | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 Chat (70B) | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2 Chat (13B) | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2 Chat (7B) | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B Instruct | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral (7B) Instruct v0.2 | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| Nous Hermes-2 Yi 34B | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: '| OpenChat 3.5 | 0% | 0% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Models and their success rates for exploiting one-day vulnerabilities
    (pass @ 5 and overall success rate). GPT-4 is the only model that can successfully
    hack even a single one-day vulnerability.'
  prefs: []
  type: TYPE_NORMAL
- en: We measured the overall success rate of the 10 models and open-source vulnerability
    scanners on our real-world vulnerabilities, with results shown in Table [3](#S5.T3
    "Table 3 ‣ 5.2 End-to-end Hacking ‣ 5 LLM Agents can Autonmously Exploit One-Day
    Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").
    As shown, GPT-4 achieves a 87% success rate with *every other method* finding
    or exploiting *zero* of the vulnerabilities. These results suggest an “emergent
    capability” in GPT-4 (Wei et al., [2022](#bib.bib46)), although more investigation
    is required (Schaeffer et al., [2024](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4 only fails on two vulnerabilities: Iris XSS and Hertzbeat RCE. Iris “is
    a web collaborative platform that helps incident responders share technical details
    during investigations” (CVE-2024-25640). The Iris web app is extremely difficult
    for an LLM agent to navigate, as the navigation is done through JavaScript. As
    a result, the agent tries to access forms/buttons without interacting with the
    necessary elements to make it available, which stops it from doing so. The detailed
    description for Hertzbeat is in Chinese, which may confuse the GPT-4 agent we
    deploy as we use English for the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: We further note that GPT-4 achieves an 82% success rate when only considering
    vulnerabilities after the knowledge cutoff date (9 out of 11 vulnerabilities).
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, every other method, including GPT-3.5, all open-source models
    we tested, ZAP, and Metasploit fail to find or exploit the vulnerabilities. Our
    results on open-source models corroborate results from Fang et al. ([2024](#bib.bib8)).
    Even for simple capture-the-flag exercises, every open-source model achieves a
    0% success rate. Qualitatively, GPT-3.5 and the open-source models appear to be
    much worse at tool use. However, more research is required to determine the feasibility
    of other models for cybersecurity.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Removing CVE Descriptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We then modified our agent to not include the CVE description. This task is
    now substantially more difficult, requiring both finding the vulnerability and
    then actually exploiting it. Because every other method (GPT-3.5 and all other
    open-source models we tested) achieved a 0% success rate even with the vulnerability
    description, the subsequent experiments are conducted on GPT-4 only.
  prefs: []
  type: TYPE_NORMAL
- en: After removing the CVE description, the success rate falls from 87% to 7%. This
    suggests that determining the vulnerability is extremely challenging.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this discrepancy, we computed the success rate (pass at 5) for
    determining the correct vulnerability. Surprisingly, GPT-4 was able to identify
    the correct vulnerability 33.3% of the time. Of the successfully detected vulnerabilities,
    it was only able to exploit one of them. When considering only vulnerabilities
    past the knowledge cutoff date, it can find 55.6% of them.
  prefs: []
  type: TYPE_NORMAL
- en: We further investigated by computing the number of actions taken by the agent
    with and without the CVE description. Surprisingly, we found that the average
    number of actions taken with and without the CVE description differed by only
    14% (24.3 actions vs 21.3 actions). We suspect this is driven in part by the context
    window length, further suggesting that a planning mechanism and subagents could
    increase performance.
  prefs: []
  type: TYPE_NORMAL
- en: These results suggests that enhancing planning and exploration capabilities
    of agents will increase the success rate of these agents, but more exploration
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Cost Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now evaluate the cost of using GPT-4 to exploit real-world vulnerabilities.
    Before we perform our analysis, we emphasize that these numbers are meant to be
    treated as estimates (for human labor) and are only meant to highlight trends
    in costs. This is in line with prior work that estimates the cost of other kinds
    of attacks, such as website hacking (Fang et al., [2024](#bib.bib8)) and phishing
    (Kang et al., [2023](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: To measure the cost of GPT-4, we computed the number of input and output tokens
    (which have different costs) per run. At the time of writing, GPT-4 costs $10
    per million input tokens and $30 per million output tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The average cost per run was $3.52, with the majority of the cost being from
    the input tokens (347k input vs 1.7k output). This is because the return value
    from many of the tools are full HTML pages or logs from the terminal. With an
    average overall success rate of 40%, this would require $8.80 per exploit.
  prefs: []
  type: TYPE_NORMAL
- en: Using the estimates from Fang et al. ([2024](#bib.bib8)), we estimate $50 per
    hour for a cybersecurity expert, and an estimate of 30 minutes per vulnerability.
    This would cost a total of $25\. Thus, using an LLM agent is already 2.8$\times$
    cheaper than human labor. LLM agents are also trivially scalable, in contrast
    to human labor.
  prefs: []
  type: TYPE_NORMAL
- en: This gap is less than the gap in prior work (Fang et al., [2024](#bib.bib8);
    Kang et al., [2023](#bib.bib21)). Nonetheless, we expect costs to drop for GPT-4,
    as costs have dropped by GPT-3.5 by over 3$\times$ in a span of a year.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Understanding Agent Capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now study the GPT-4 agent behavior in greater detail to understand its high
    success rate and why it fails when the CVE description is removed.
  prefs: []
  type: TYPE_NORMAL
- en: '| Vulnerability | Number of steps |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| runc | 10.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CSRF + ACE | 26.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress SQLi | 23.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress XSS-1 | 21.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Wordpress XSS-2 | 48.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Travel Journal XSS | 20.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Iris XSS | 38.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CSRF + privilege escalation | 13.4 |'
  prefs: []
  type: TYPE_TB
- en: '| alf.io key leakage | 35.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Astrophy RCE | 20.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Hertzbeat RCE | 36.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Gnuboard XSS | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Symfony 1 RCE | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Peering Manager SSTI RCE | 14.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ACIDRain | 32.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Number of actions taken per vulnerability.'
  prefs: []
  type: TYPE_NORMAL
- en: We first observe that many of the vulnerabilities take a large number of actions
    to successfully exploit, with the average number of actions per vulnerability
    shown in Table [4](#S6.T4 "Table 4 ‣ 6 Understanding Agent Capabilities ‣ LLM
    Agents can Autonomously Exploit One-day Vulnerabilities"). For example, Wordpress
    XSS-2 (CVE-2023-1119-2) takes an average of 48.6 steps per run. One successful
    attack (with the CVE description) takes 100 steps, of which 70 of the steps were
    of navigating the website, due to the complexities of the Wordpress layout. Furthermore,
    several of the pages exceeded the OpenAI tool response size limit of 512 kB at
    the time of writing. Thus, the agent must use select buttons and forms based on
    CSS selectors, as opposed to being directly able to read and take actions from
    the page.
  prefs: []
  type: TYPE_NORMAL
- en: Second, consider CSRF + ACE (CVE-2024-24524), which requires both leveraging
    a CSRF attack and performing code execution. Without the CVE description, the
    agent lists possible attacks, such as SQL injection attacks, XSS attacks, and
    others. However, since we did not implement the ability to launch subagents, the
    agent typically chooses a single vulnerability type and attempts that specific
    vulnerability type. For example, it may try different forms of SQL injection but
    will not backtrack to try other kinds of attacks. Adding subagent capabilities
    may improve the performance of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, consider the ACIDRain exploit. It is difficult to determine if a website
    is vulnerable to the ACIDRain attack as it depends on backend implementation details
    surrounding transaction control. However, performing the ACIDRain attack is still
    complex, requiring:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: navigating to the website and extracting the hyperlinks,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: navigating to the checkout page, placing a test order, and recording the necessary
    fields for checkout,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: writing Python code to exploit the race condition,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: actually executing the Python code via the terminal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This exploit requires operating several tools and writing code based on the
    actions taken on the website.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we note that our GPT-4 agent can autonomously exploit non-web vulnerabilities
    as well. For example, consider the Astrophy RCE exploit (CVE-2023-41334). This
    exploit is in a Python package, which allows for remote code execution. Despite
    being very different from websites, which prior work has focused on (Fang et al.,
    [2024](#bib.bib8)), our GPT-4 agent can autonomously write code to exploit other
    kinds of vulnerabilities. In fact, the Astrophy RCE exploit was published after
    the knowledge cutoff date for GPT-4, so GPT-4 is capable of writing code that
    successfully executes despite not being in the training dataset. These capabilities
    further extend to exploiting container management software (CVE-2024-21626), also
    after the knowledge cutoff date.
  prefs: []
  type: TYPE_NORMAL
- en: Our qualitative analysis shows that our GPT-4 agent is highly capable. Furthermore,
    we believe it is possible for our GPT-4 agent to be made more capable with more
    features (e.g., planning, subagents, and larger tool response sizes).
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cybersecurity and AI. The most related work to ours is a recent study that showed
    that LLM agents can hack websites (Fang et al., [2024](#bib.bib8)). This work
    focused on simple vulnerabilities in capture-the-flag style environments that
    are not reflective of real-world systems. Work contemporaneous to ours also evaluates
    the ability of LLM agents in a cybersecurity context (Phuong et al., [2024](#bib.bib29)),
    but appears to perform substantially worse than our agent and an agent in the
    CTF setting (Fang et al., [2024](#bib.bib8)). Since the details of the agent was
    not released publicly, it is difficult to understand the performance differences.
    We hypothesize that it is largely due to the prompt. In our work, we show that
    LLM agents can hack real world one-day vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Other recent work has shown the ability of LLMs to aid in penetration testing
    or malware generation (Happe & Cito, [2023](#bib.bib13); Hilario et al., [2024](#bib.bib15)).
    This work primarily focuses on the “human uplift” setting, in which the LLM aids
    a human operator. Other work focuses on the societal implications the intersection
    of AI and cybersecurity (Lohn & Jackson, [2022](#bib.bib25); Handa et al., [2019](#bib.bib12)).
    In our work, we focus on agents (which can be trivial scaled out, as opposed to
    humans) and the concrete possibility of hacking real-world vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Cybersecurity. Cybersecurity is a incredibly wide research area, ranging from
    best practices for passwords (Herley & Van Oorschot, [2011](#bib.bib14)), studying
    the societal implications of cyber attacks (Bada & Nurse, [2020](#bib.bib2)),
    to understanding web vulnerabilities (Halfond et al., [2006](#bib.bib11)). The
    subarea of cybersecurity closest to ours is automatic vulnerability detection
    and exploitation (Russell et al., [2018](#bib.bib34); Bennetts, [2013](#bib.bib3);
    Kennedy et al., [2011](#bib.bib22); Mahajan, [2014](#bib.bib26)).
  prefs: []
  type: TYPE_NORMAL
- en: In cybersecurity, a common set of tools used by both black hat and white hat
    actors are automatic vulnerability scanners. These include ZAP (Bennetts, [2013](#bib.bib3)),
    Metasploit (Kennedy et al., [2011](#bib.bib22)), and Burp Suite (Mahajan, [2014](#bib.bib26)).
    Although these tools are important, the open-source vulnerability scanners cannot
    find *any* of the vulnerabilities we study, showing the capability of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: Security of LLM agents. A related, but orthogonal line of work is the security
    of LLM agents (Greshake et al., [2023a](#bib.bib9); Kang et al., [2023](#bib.bib21);
    Zou et al., [2023](#bib.bib54); Zhan et al., [2023](#bib.bib50); Qi et al., [2023](#bib.bib31);
    Yang et al., [2023](#bib.bib47)). For example, an attacker can use an indirect
    prompt injection attack to misdirect an LLM agent (Greshake et al., [2023b](#bib.bib10);
    Yi et al., [2023](#bib.bib49); Zhan et al., [2024](#bib.bib51)). Attackers can
    also fine-tune away protections from models, enabling highly capable models to
    perform actions or tasks that the creators of the models did not intent (Zhan
    et al., [2023](#bib.bib50); Yang et al., [2023](#bib.bib47); Qi et al., [2023](#bib.bib31)).
    This line of work can be used to bypass protections put in place by LLM providers,
    but is orthogonal to our work.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we show that LLM agents are capable of autonomously exploiting
    real-world one-day vulnerabilities. Currently, only GPT-4 with the CVE description
    is capable of exploiting these vulnerabilities. Our results show both the possibility
    of an emergent capability and that uncovering a vulnerability is more difficult
    than exploiting it. Nonetheless, our findings highlight the need for the wider
    cybersecurity community and LLM providers to think carefully about how to integrate
    LLM agents in defensive measures and about their widespread deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our results show that LLM agents can be used to hack real-world systems. Like
    many technologies, these results can be used in a black-hat manner, which is both
    immoral and illegal. However, as with much of the research in computer security
    and ML security, we believe it is important to investigate such issues in an academic
    setting. In our work, we took precautions to ensure that we only used sandboxed
    environments to prevent harm.
  prefs: []
  type: TYPE_NORMAL
- en: We have disclosed our findings to OpenAI prior to publication. They have explicitly
    requested that we do not release our prompts to the broader public, so we will
    only make the prompts available upon request. Furthermore, many papers in advanced
    ML models and work in cybersecurity do not release the specific details for ethical
    reasons (such as the NeurIPS 2020 best paper (Brown et al., [2020](#bib.bib6))).
    Thus, we believe that withholding the specific details of our prompts are in line
    with best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We would like to acknowledge the Open Philanthropy project for funding this
    research in part.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bada & Nurse (2020) Maria Bada and Jason RC Nurse. The social and psychological
    impact of cyberattacks. In *Emerging cyber threats and cognitive vulnerabilities*,
    pp. 73–92\. Elsevier, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennetts (2013) Simon Bennetts. Owasp zed attack proxy. *AppSec USA*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boiko et al. (2023) Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent
    autonomous scientific research capabilities of large language models. *arXiv preprint
    arXiv:2304.05332*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bran et al. (2023) Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry
    tools. In *NeurIPS 2023 AI for Science Workshop*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engebretson (2013) Patrick Engebretson. *The basics of hacking and penetration
    testing: ethical hacking and penetration testing made easy*. Elsevier, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel
    Kang. Llm agents can autonomously hack websites, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greshake et al. (2023a) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. More than you’ve asked for: A comprehensive
    analysis of novel prompt injection threats to application-integrated large language
    models. *arXiv e-prints*, pp.  arXiv–2302, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greshake et al. (2023b) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. Not what you’ve signed up for: Compromising
    real-world llm-integrated applications with indirect prompt injection. In *Proceedings
    of the 16th ACM Workshop on Artificial Intelligence and Security*, pp.  79–90,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Halfond et al. (2006) William G Halfond, Jeremy Viegas, Alessandro Orso, et al.
    A classification of sql-injection attacks and countermeasures. In *Proceedings
    of the IEEE international symposium on secure software engineering*, volume 1,
    pp.  13–15\. IEEE Piscataway, NJ, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Handa et al. (2019) Anand Handa, Ashu Sharma, and Sandeep K Shukla. Machine
    learning in cybersecurity: A review. *Wiley Interdisciplinary Reviews: Data Mining
    and Knowledge Discovery*, 9(4):e1306, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Happe & Cito (2023) Andreas Happe and Jürgen Cito. Getting pwn’d by ai: Penetration
    testing with large language models. In *Proceedings of the 31st ACM Joint European
    Software Engineering Conference and Symposium on the Foundations of Software Engineering*,
    pp.  2082–2086, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Herley & Van Oorschot (2011) Cormac Herley and Paul Van Oorschot. A research
    agenda acknowledging the persistence of passwords. *IEEE Security & privacy*,
    10(1):28–36, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hilario et al. (2024) Eric Hilario, Sami Azam, Jawahar Sundaram, Khwaja Imran Mohammed,
    and Bharanidharan Shanmugam. Generative ai for pentesting: the good, the bad,
    the ugly. *International Journal of Information Security*, pp.  1–23, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and
    Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing
    and optimisation. *arXiv preprint arXiv:2312.13010*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jang-Jaccard & Nepal (2014) Julian Jang-Jaccard and Surya Nepal. A survey of
    emerging threats in cybersecurity. *Journal of computer and system sciences*,
    80(5):973–993, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jimenez et al. (2023) Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models
    resolve real-world github issues? *arXiv preprint arXiv:2310.06770*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2023) Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei
    Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use
    through standard security attacks. *arXiv preprint arXiv:2302.05733*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kennedy et al. (2011) David Kennedy, Jim O’gorman, Devon Kearns, and Mati Aharoni.
    *Metasploit: the penetration tester’s guide*. No Starch Press, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuznetsov et al. (2023) Igor Kuznetsov, Valentin Pashkov, Leonid Bezvershenko,
    and Georgy Kucherin. Operation triangulation: ios devices targeted with previously
    unknown malware. 2023. URL [https://securelist.com/operation-triangulation/109842/](https://securelist.com/operation-triangulation/109842/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp
    tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lohn & Jackson (2022) Andrew Lohn and Krystal Jackson. Will ai make cyber swords
    or shields? 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahajan (2014) Akash Mahajan. *Burp Suite Essentials*. Packt Publishing Ltd,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos
    Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane
    Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. *arXiv
    preprint arXiv:2302.07842*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osika (2023) Anton Osika. gpt-engineer, April 2023. URL [https://github.com/gpt-engineer-org/gpt-engineer](https://github.com/gpt-engineer-org/gpt-engineer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phuong et al. (2024) Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan,
    Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael,
    Sarah Hodkinson, et al. Evaluating frontier models for dangerous capabilities.
    *arXiv preprint arXiv:2403.13793*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popper (2016) Nathaniel Popper. A hacking of more than $50 million dashes hopes
    in the world of virtual currency. *The New York Times*, 17, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety,
    even when users do not intend to! *arXiv preprint arXiv:2310.03693*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research (2024) Nous Research. Nous hermes 2 - yi-34b, 2024. URL [https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roselin et al. (2019) Annie Gilda Roselin, Priyadarsi Nanda, Surya Nepal, Xiangjian
    He, and Jarod Wright. Exploiting the remote server access support of coap protocol.
    *IEEE Internet of Things Journal*, 6(6):9338–9349, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russell et al. (2018) Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich,
    Jacob Harer, Onur Ozdemir, Paul Ellingwood, and Marc McConley. Automated vulnerability
    detection in source code using deep representation learning. In *2018 17th IEEE
    international conference on machine learning and applications (ICMLA)*, pp.  757–762\.
    IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaeffer et al. (2024) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are
    emergent abilities of large language models a mirage? *Advances in Neural Information
    Processing Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sikorski & Honig (2012) Michael Sikorski and Andrew Honig. *Practical malware
    analysis: the hands-on guide to dissecting malicious software*. no starch press,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teknium (2024) Teknium. Openhermes 2.5 - mistral 7b, 2024. URL [https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ullah et al. (2018) Faheem Ullah, Matthew Edwards, Rajiv Ramdhany, Ruzanna
    Chitchyan, M Ali Babar, and Awais Rashid. Data exfiltration: A review of external
    attack vectors and countermeasures. *Journal of Network and Computer Applications*,
    101:18–54, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varshney (2023) Tanay Varshney. Introduction to llm agents. 2023. URL [https://developer.nvidia.com/blog/introduction-to-llm-agents/](https://developer.nvidia.com/blog/introduction-to-llm-agents/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vulnerabilities (2005) Common Vulnerabilities. Common vulnerabilities and exposures.
    *The MITRE Corporation,[online] Available: https://cve. mitre. org/index. html*,
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen
    Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality
    data. *arXiv preprint arXiv:2309.11235*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, and Jinsong Su.
    Tdag: A multi-agent framework based on dynamic task decomposition and agent generation.
    *arXiv preprint arXiv:2402.10178*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warszawski & Bailis (2017) Todd Warszawski and Peter Bailis. Acidrain: Concurrency-related
    attacks on database-backed web applications. In *Proceedings of the 2017 ACM International
    Conference on Management of Data*, pp.  5–20, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang
    Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned
    language models. *arXiv preprint arXiv:2310.02949*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman,
    Guangzhong Sun, Xing Xie, and Fangzhao Wu. Benchmarking and defending against
    indirect prompt injection attacks on large language models. *arXiv preprint arXiv:2312.14197*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhan et al. (2023) Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori
    Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning.
    *arXiv preprint arXiv:2311.05553*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang.
    Injecagent: Benchmarking indirect prompt injections in tool-integrated large language
    model agents. *arXiv preprint arXiv:2403.02691*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng & Zhang (2013) Yunhui Zheng and Xiangyu Zhang. Path sensitive static analysis
    of web applications for remote code execution vulnerability detection. In *2013
    35th International Conference on Software Engineering (ICSE)*, pp.  652–661\.
    IEEE, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
