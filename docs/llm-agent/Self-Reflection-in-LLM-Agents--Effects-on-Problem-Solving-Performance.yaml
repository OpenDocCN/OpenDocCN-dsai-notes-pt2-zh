- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-Reflection in LLM Agents: Effects on Problem-Solving Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06682](https://ar5iv.labs.arxiv.org/html/2405.06682)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Matthew Renze
  prefs: []
  type: TYPE_NORMAL
- en: Johns Hopkins University
  prefs: []
  type: TYPE_NORMAL
- en: mrenze1@jhu.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Erhan Guven'
  prefs: []
  type: TYPE_NORMAL
- en: Johns Hopkins University
  prefs: []
  type: TYPE_NORMAL
- en: eguven2@jhu.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this study, we investigated the effects of self-reflection in large language
    models (LLMs) on problem-solving performance. We instructed nine popular LLMs
    to answer a series of multiple-choice questions to provide a performance baseline.
    For each incorrectly answered question, we instructed eight types of self-reflecting
    LLM agents to reflect on their mistakes and provide themselves with guidance to
    improve problem-solving. Then, using this guidance, each self-reflecting agent
    attempted to re-answer the same questions. Our results indicate that LLM agents
    are able to significantly improve their problem-solving performance through self-reflection
    ($p<0.001$). In addition, we compared the various types of self-reflection to
    determine their individual contribution to performance. All code and data are
    available on GitHub at [https://github.com/matthewrenze/self-reflection](https://github.com/matthewrenze/self-reflection)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1 Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-reflection is a process in which a person thinks about their thoughts,
    feelings, and behaviors. In the context of problem-solving, self-reflection allows
    us to inspect the thought process leading to our solution. This type of self-reflection
    aims to avoid making similar errors when confronted with similar problems in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: Like humans, large language model (LLM) agents can be instructed to produce
    a chain of thought (CoT) before answering a question. CoT prompting has been shown
    to significantly improve LLM performance on a variety of problem-solving tasks
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]. However, LLMs still often make
    errors in their CoT due to logic errors, mathematical errors, hallucination, etc.
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: Also similar to humans, LLM agents can be instructed to reflect on their own
    CoT. This allows them to identify errors, explain the cause of these errors, and
    generate advice to avoid making similar types of errors in the future [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: Our research investigates the use of self-reflection in LLM agents to improve
    their problem-solving capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Prior Literature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over the past few years, we’ve seen the emergence of AI agents based on LLM
    architectures [[16](#bib.bib16), [17](#bib.bib17)]. These agents have demonstrated
    impressive capabilities in solving multi-step problems [[18](#bib.bib18), [19](#bib.bib19),
    [10](#bib.bib10)]. In addition, they’ve been observed successfully using tools,
    including web browsers, search engines, code interpreters, etc. [[20](#bib.bib20),
    [19](#bib.bib19), [10](#bib.bib10), [21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: However, these LLM agents have several imitations. They have limited knowledge,
    make errors in reasoning, hallucinate output, and get stuck in unproductive loops
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: To improve their performance, we can provide them with a series of cognitive
    capabilities. For example, we can provide them with a CoT [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)], access to external memory [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)], and the ability to learn from feedback [[18](#bib.bib18),
    [10](#bib.bib10), [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: Learning from feedback can be decomposed into several components. These components
    include the source of the feedback, the type of feedback, and the strategy used
    to learn from feedback [[11](#bib.bib11)]. There are two sources of feedback (e.g.,
    internal or external feedback) and two main types of feedback (e.g., scalar values
    or natural language) [[11](#bib.bib11), [12](#bib.bib12)].
  prefs: []
  type: TYPE_NORMAL
- en: There are also several strategies for learning from feedback. These strategies
    depend on where they occur in the LLM’s output-generation process. They can occur
    at model-training time, output-generation time, or after the output has been generated.
    Within each of these three phases, there are various techniques available (e.g.,
    model fine-tuning, output re-ranking, and self-correction) [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: In terms of learning from self-correction, various methods are currently being
    investigated. These include iterative refinement, multi-model debate, and self-reflection
    [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: Self-reflection in LLM agents is a meta-cognitive strategy also known as introspection
    [[13](#bib.bib13), [14](#bib.bib14)]. Some research studies have indicated that
    LLMs using self-reflection are able to identify and correct their mistakes [[12](#bib.bib12),
    [10](#bib.bib10), [8](#bib.bib8), [15](#bib.bib15)]. Others have indicated that
    LLMs cannot identify errors in their reasoning; regardless, they still may be
    able to correct them with external feedback [[7](#bib.bib7), [26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Contribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our research builds upon the prior literature by determining which aspects of
    self-reflection are most beneficial in improving an LLM agent’s performance on
    problem-solving tasks. It decomposes the process of self-reflection into several
    components and identifies how each component contributes to the agent’s overall
    increase in performance.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, it provides insight into which types of LLMs and problem domains
    benefit most from each type of self-reflection. These include LLMs like GPT-4,
    Llama 2 70B, and Gemini 1.5 Pro. It also includes various problem domains such
    as math, science, medicine, etc.
  prefs: []
  type: TYPE_NORMAL
- en: This information is useful to AI engineers attempting to build LLM agents with
    self-reflection capabilities. In addition, it is valuable to AI researchers studying
    metacognition in LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our test dataset consists of a set of multiple-choice question-and-answer (MCQA)
    problems derived from popular LLM benchmarks. These benchmarks include ARC, AGIEval,
    HellaSwag, MedMCQA, etc. [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)].
  prefs: []
  type: TYPE_NORMAL
- en: We preprocessed and converted these datasets into a standardized format. Then,
    we randomly selected 100 questions from each of the ten datasets to create a multi-domain
    exam with 1,000 problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a complete list of the source problem sets used to create the MCQA exam,
    see Table [1](#S2.T1 "Table 1 ‣ 2.1 Data ‣ 2 Methods ‣ Self-Reflection in LLM
    Agents: Effects on Problem-Solving Performance"). For a sample of an MCQA problem,
    see Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") in the appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Problem sets used to create the 1,000-question multi-domain MCQA exam.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Problem Set | Benchmark | Domain | Questions | License | Source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ARC Challenge Test | ARC | Science | 1,173 | CC BY-SA | [[27](#bib.bib27)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| AQUA-RAT | AGI Eval | Math | 254 | Apache v2.0 | [[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: '| Hellaswag Val | Hellaswag | Common Sense Reasoning | 10,042 | MIT | [[28](#bib.bib28)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| LogiQA (English) | AGI Eval | Logic | 651 | GitHub | [[30](#bib.bib30), [31](#bib.bib31)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| LSAT-AR | AGI Eval | Law (Analytic Reasoning) | 230 | MIT | [[30](#bib.bib30),
    [32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '| LSAT-LR | AGI Eval | Law (Logical Reasoning) | 510 | MIT | [[30](#bib.bib30),
    [32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '| LSAT-RC | AGI Eval | Law (Reading Comprehension) | 260 | MIT | [[30](#bib.bib30),
    [32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '| MedMCQA Valid | MedMCQA | Medicine | 6,150 | MIT | [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '| SAT-English | AGI Eval | English | 206 | MIT | [[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: '| SAT-Math | AGI Eval | Math | 220 | MIT | [[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: 'Note: The GitHub repository for LogiQA does not include a license file. However,
    both the paper and readme.md file states that "The dataset is freely available."'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluated our agents using nine popular LLMs, including GPT-4, Llama 2 70B,
    Google Gemini, etc. [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47)]. All models were accessed via cloud-based APIs hosted by Microsoft,
    Anthropic, and Google.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these LLMs has its own unique strengths and weaknesses. For example,
    LLMs like GPT-4, Gemini 1.5 Pro, and Claude Opus are powerful LLMs with a large
    number of parameters [[44](#bib.bib44), [40](#bib.bib40), [34](#bib.bib34)]. However,
    they have a significantly higher cost per token than smaller models like GPT-3.5
    and Llama 2 7B [[42](#bib.bib42), [46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: 'For a complete list of LLMs used in our experiment, see Table [2](#S2.T2 "Table
    2 ‣ 2.2 Models ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: LLMs used in the experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Vendor | Released | License | Source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | Anthropic | 2024-03-04 | Closed | [[33](#bib.bib33), [34](#bib.bib34)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Command R+ | Cohere | 2024-04-04 | Open | [[35](#bib.bib35), [36](#bib.bib36)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini 1.0 Pro | Google | 2023-12-06 | Closed | [[37](#bib.bib37), [38](#bib.bib38)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini 1.5 Pro (Preview) | Google | 2024-02-15 | Closed | [[39](#bib.bib39),
    [40](#bib.bib40)] |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 Turbo | OpenAI | 2022-11-30 | Closed | [[41](#bib.bib41), [42](#bib.bib42)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | OpenAI | 2023-03-14 | Closed | [[43](#bib.bib43), [44](#bib.bib44)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 7B Chat | Meta | 2023-07-18 | Open | [[45](#bib.bib45), [46](#bib.bib46)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 70B Chat | Meta | 2023-07-18 | Open | [[45](#bib.bib45), [46](#bib.bib46)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral Large | Mistral AI | 2024-02-26 | Closed | [[47](#bib.bib47)] |'
  prefs: []
  type: TYPE_TB
- en: 2.3 Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We investigated eight types of self-reflecting LLM agents. These agents reflect
    upon their own CoT and then generate self-reflections to use when attempting to
    re-answer questions. Each of these agents uses a unique type of self-reflection
    to assist it. We also included a single non-reflecting (i.e., Baseline) agent
    as our control.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listed below are the various types of agents and the type of self-reflection
    they generate and use to re-answer questions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baseline - no self-reflection capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retry - informed that it answered incorrectly and simply tries again.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keywords - a list of keywords for each type of error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advice - a list of general advice for improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explanation - an explanation of why it made an error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructions - an ordered list of instructions for how to solve the problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution - a step-by-step solution to the problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Composite - all six types of self-reflections.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unredacted - all six types without the answers redacted
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The Baseline agent is our control for the experiment and a lower bound for
    the scores. It informs us how well the base model answers the question without
    using any self-reflection. The Baseline agent used standard prompt-engineering
    techniques, including domain expertise, CoT, conciseness, and few-shot prompting
    [[48](#bib.bib48), [49](#bib.bib49), [1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)].
    The sampling temperature was set to 0.0 for all LLMs to improve reproducibility
    [[50](#bib.bib50)]. See Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Data ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance") in the appendix for an
    example of the Baseline answer prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-reflecting agents used the same prompt-engineering techniques as the
    Baseline agent to re-answer questions. However, they also reflected upon their
    mistakes before attempting to re-answer. While re-answering, the self-reflection
    was injected into the re-answer prompt to allow the agent to learn from its mistakes.
    See Figures [7](#A2.F7 "Figure 7 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") and [8](#A2.F8 "Figure 8 ‣ Appendix B
    Data ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")
    in the appendix for examples of the self-reflection prompt and the re-answer prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: We redacted all of the answer labels (e.g., "A", "B", "C") and answer descriptions
    (e.g., "Baltimore", "Des Moines", "Las Vegas") from the agents’ self-reflections.
    However, the Unredacted agent retains this information. This agent is only used
    to provide an upper bound for the scores. Essentially, the Unredacted agent tells
    us how accurately the LLM could answer the questions when given the correct answer
    in its self-reflection.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, the Baseline agent answered all 1,000 questions. If a question was answered
    correctly, it was added to the Baseline agent’s score. If it was answered incorrectly,
    it was added to a queue of incorrectly answered questions to be reflected upon
    (see Figure [1](#S2.F1.1 "Figure 1 ‣ 2.4 Process ‣ 2 Methods ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance")).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, for each incorrectly answered question, the self-reflecting agents reflected
    upon the problem, their incorrect solution, and the correct answer. Using the
    correct answer as an external feedback signal, they each generated one of the
    eight types of self-reflection feedback described above.
  prefs: []
  type: TYPE_NORMAL
- en: Then, a find-and-replace operation was performed on the text of each self-reflection
    to redact the answer labels and answer descriptions. For example, we replaced
    answer labels (e.g., "A", "B", "C") and answer descriptions (e.g., "Baltimore",
    "Des Moines", "Las Vegas") with the text "[REDACTED]".¹¹1The process we used to
    redact answer labels and descriptions was greedy. It often redacting additional
    text that did not leak the answer. However, we felt it necessary to err on the
    side of caution by eliminating any possible answer leakage. This was done to all
    of the self-reflecting agents, except for the Unredacted agent, to prevent answer
    leakage in the self-reflections.²²2It is important to note that the self-reflections
    generated by the Explanation, Instructions, and Solution agents indirectly leak
    information about the correct answer without directly specifying the correct or
    incorrect answers. However, they generated this information on their own based
    on nothing more than being provided the correct answer during the self-reflection
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for each incorrectly answered question, the self-reflecting agents
    used their specific self-reflection text to assist them in re-answering the question.
    We calculated the scores for all agents and compared them to the Baseline agent
    for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'While LLM agents typically operate over a series of iterative steps, the code
    for this experiment was implemented as batch operations to save time and cost.
    So, each step in the self-reflection process occurred in one of four batch phases
    described above. Conceptually, the experiment represented virtual multi-step agents.
    However, the technical implementation of the experiment was actually a series
    of batch operations (see Algorithm [1](#alg1 "Algorithm 1 ‣ Figure 1 ‣ 2.4 Process
    ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a139eaa788150aa2cdd4733574cc76e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Diagram of the self-reflection experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Self-reflection Experiment (Batch)
  prefs: []
  type: TYPE_NORMAL
- en: 1:for each model, exam, and problem do2:     Create the answer prompt3:     Answer
    the question4:     if the answer is incorrect then5:         Add the problem to
    the incorrect list6:     end if7:end for8:Calculate the Baseline agent scores9:10:for each
    model, exam, and problem do11:     Reflect upon the incorrect solution12:     Generate
    the self-reflections13:     if not the Unredacted agent then14:         Redact
    the answers15:     end if16:     Separate the reflections by type17:end for18:19:for each
    model, agent, exam, and problem do20:     Create the re-answer prompt21:     Inject
    the agent’s reflection22:     Re-answer the question23:end for24:Calculate the
    reflected agent scores
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We used correct-answer accuracy as our primary metric to measure the performance
    of the agents. We divided the number of correctly answered questions by the total
    number of questions.
  prefs: []
  type: TYPE_NORMAL
- en: However, to reduce the cost of running our experiment, we did not have the self-reflecting
    agents re-answer all of the questions that were correctly answered by the Baseline
    agent. Rather, the self-reflecting agents only re-answered the incorrectly answered
    questions. We then added the self-reflecting agent’s correct re-answer score to
    the Baseline agent’s score to create a new total score for the self-reflecting
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculations for accuracy used in our experiment are listed in Equation(s)
    [1](#S2.E1 "In 2.5 Metrics ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects
    on Problem-Solving Performance"). In these equations, the subscript [base] refers
    to the Baseline agent’s correct-answer score, and the subscript [ref] is the reflection
    agent’s correct re-answer score.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Accuracy}_{\text{base}}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 2.6 Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When comparing the scores of the self-reflecting agents to the Baseline agent,
    we performed the McNemar test to determine statistical significance and report
    p-values. This test was specifically chosen because our analysis compared two
    series of binary outcomes (i.e., correct or incorrect answers). These outcomes
    were paired question-by-question across both the Baseline agent and self-reflecting
    agent being compared.
  prefs: []
  type: TYPE_NORMAL
- en: The McNemar test compares the number of discordant pairs in the two sets of
    pair-wise outcomes. To compute the test statistic, we create a $2\times 2$ contains
    correct-incorrect answer pairs (which, in our case, will always be zero) [[51](#bib.bib51)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The McNemar’s test statistic is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\chi^{2}=\frac{(b-c)^{2}}{b+c}\quad\text{where }b\text{ and }c\text{
    are the discordant pairs in }\left[\begin{array}[]{cc}a&amp;b\\ c&amp;d\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{array}\right]$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: 3 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Performance by Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our analysis revealed that agents using various types of self-reflection outperformed
    our Baseline agent. The increase in performance was statistically significant
    ($p<0.001$) for all types of self-reflection across all LLMs. We can use GPT-4
    as an example case. In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Performance by Agent
    ‣ 3 Results ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance"),
    we can see that all types of self-reflection improve the accuracy of the agent
    in solving MCQA problems. See Table [3](#A1.T3 "Table 3 ‣ Appendix A Results ‣
    Self-Reflection in LLM Agents: Effects on Problem-Solving Performance") in the
    appendix for a numerical analysis of the results for GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d4a267d4c4a050a77f794dc19b4a38a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: All self-reflection types improved the accuracy of GPT-4 agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Performance by Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In terms of performance by model, every LLM that we tested demonstrated similar
    increases in accuracy across all self-reflection types. In all cases, the improvement
    in performance was statistically significant ($p<0.001$). See Figure [3](#S3.F3
    "Figure 3 ‣ 3.2 Performance by Model ‣ 3 Results ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") for a plot of accuracy by model and agent.
    See Table [4](#A1.T4 "Table 4 ‣ Appendix A Results ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") for a numerical analysis of accuracy
    across all models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/836be5c7bc33ad9dec69f159f1c373e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: All LLMs we tested showed a similar pattern of improvement across
    self-reflection agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Performance by Exam
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In terms of performance by exam, we saw that self-reflection significantly
    increased performance for some problem domains. However, other problem domains
    were less affected. For example, we saw the largest improvement on the LSAT-AR
    (Analytical Reasoning) exam. Other exams, like the SAT English exam, had much
    smaller effects. See Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Performance by Exam ‣ 3
    Results ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")
    for a plot of accuracy by exam and agent for GPT-4\. See Table [5](#A1.T5 "Table
    5 ‣ Appendix A Results ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance") in the appendix for a numerical analysis of the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6354e0070805e1e1d22fab20790200c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The increase in performance from self-reflection was larger for some
    exams and smaller for others.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Interpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on these results, all types of self-reflection improve the performance
    of LLM agents. In addition, these effects were observed across every LLM we tested.
    Self-reflections that contain more information (e.g., Instructions, Explanation,
    and Solution) outperform types of self-reflection with limited information (e.g.,
    Retry, Keywords, and Advice).
  prefs: []
  type: TYPE_NORMAL
- en: The difference in accuracy between the self-reflecting agents and the Unredacted
    agent demonstrated that we were effectively eliminating direct answer leakage
    from the self-reflections. However, the structure of feedback generated by the
    Instruction, Explanation, Solution, and Composite agents clearly provides indirect
    guidance toward the correct answer without directly giving the answer away.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the Retry agent significantly improved performance across all
    LLMs. As a result, it appears that even the mere knowledge that the agent previously
    made a mistake improves the agent’s performance while re-answering the question.
    We hypothesize that this is either the result of the agent being more diligent
    in its second attempt or choosing the second most likely answer based on its re-answer
    CoT. Further investigation will be required to answer this question.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, the LLM agent we created for this experiment only solved a single-step
    problem. The real value in LLM agents is their ability to solve complex multi-step
    problems by iteratively choosing actions that lead them toward their goal. As
    a result, this experiment does not fully demonstrate the potential of self-reflecting
    LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: Second, API response errors may have introduced a small amount of error into
    our results. API errors typically occurred when content-safety filters were triggered
    by the questions being asked. In most cases, this may have amounted to an error
    in reporting an agent’s accuracy of less than 1%. However, in the case of the
    Gemini 1.0 Pro and the Mistral Large models, this error could be as high as 2.8%.
  prefs: []
  type: TYPE_NORMAL
- en: Third, the top-performing LLMs scored above 90% accuracy for most exams. As
    a result, the increase in scores for the top exams was compressed near the upper
    limit of 100% (i.e., a perfect score). This compression effect makes it difficult
    to assess the performance increase. As a result, our analysis would benefit from
    exams with a higher level of difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for all models and agents, the LSAT-AR (Analytical Reasoning) exam
    was the most difficult and also the most benefited by self-reflection. This large
    increase in performance from a single exam had the potential to skew the aggregate
    results across all exams. Using a set of exams with more uniform difficulty would
    eliminate this skewness.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Implications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our research builds upon previous research in LLM agents and self-reflecting
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: It has practical implications for AI engineers who are building agentic LLM
    systems. Agents that can self-reflect on their own mistakes based on error signals
    from the environment can learn to avoid similar mistakes in the future. This will
    also help prevent the common issue of agents getting stuck in unproductive loops
    because they continue repeating the same mistake indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, our research has theoretical implications for AI researchers studying
    meta-cognition in LLMs. If LLMs are able to self-reflect on their own CoT, other
    similar metacognitive processes may also be leveraged to improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Future Research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we recommend repeating this experiment using a more complex set of problems.
    Using problems as difficult or more difficult than the LSAT-AR exam would better
    reflect the performance improvement from self-reflection by avoiding compression
    of the scores around 100% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we recommend performing an experiment using multi-step problems. This
    would allow the agents to receive feedback from their environment after each step
    to use as external signals for error correction. It would also demonstrate the
    potential of self-reflection on long-horizon problems.
  prefs: []
  type: TYPE_NORMAL
- en: Third, we recommend repeating this experiment while providing the agents with
    access to external tools. This would allow us to see how error signals from the
    tools benefit self-reflection. For example, we could observe how an agent adapts
    to compiler errors from a Python interpreter or low-rank search results from a
    search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, we recommend repeating this experiment with agents the possess external
    memory. Having an agent answer the same questions based on self-reflection is
    only beneficial from an experimental standpoint. Real-world agents need to store
    self-reflections and retrieve them (using Retrieval Augmented Generation) when
    encountering similar but not necessarily identical problems.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we recommend a survey of self-reflection across a wider set of LLMs,
    agent types, and problem domains. This would help us better characterize the effects
    of self-reflection and provide further empirical evidence for the potential benefits
    of self-reflecting LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we investigated the effects of self-reflection in LLM agents
    on problem-solving tasks. Our results indicate that LLMs are able to reflect upon
    their own CoT and produce guidance that can significantly improve problem-solving
    performance. These performance improvements were observed across multiple LLMs,
    self-reflection types, and problem domains. This research has practical implications
    for AI engineers building agentic AI systems as well as theoretical implications
    for AI researchers studying meta-cognition in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Funding for this research was provided by [Microsoft](https://www.microsoft.com/)
    and [Renze AI Research Institute](https://renzeai.org/).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language
    models are zero-shot reasoners,” in *Advances in Neural Information Processing
    Systems*, vol. 35, 5 2022, pp. 22 199–22 213\. [Online]. Available: [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le,
    and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,”
    *arXiv*, 1 2022\. [Online]. Available: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba,
    “Large language models are human-level prompt engineers,” *The Eleventh International
    Conference on Learning Representations*, 11 2023\. [Online]. Available: [https://arxiv.org/abs/2211.01910](https://arxiv.org/abs/2211.01910)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang, D. Chen,
    H. S. Chan, W. Dai, A. Madotto, and P. Fung, “Survey of hallucination in natural
    language generation,” *ACM Computing Surveys*, vol. 55, 2 2022\. [Online]. Available:
    [http://dx.doi.org/10.1145/3571730](http://dx.doi.org/10.1145/3571730)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. U. Hadi, qasem al tashi, R. Qureshi, A. Shah, amgad muneer, M. Irfan,
    A. Zafar, M. B. Shaikh, N. Akhtar, J. Wu, S. Mirjalili, Q. Al-Tashi, and A. Muneer,
    “A survey on large language models: Applications, challenges, limitations, and
    practical usage,” *Authorea Preprints*, 10 2023\. [Online]. Available: [https://doi.org/10.36227/techrxiv.23589741.v1](https://doi.org/10.36227/techrxiv.23589741.v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Payandeh, D. Pluth, J. Hosier, X. Xiao, and V. K. Gurbani, “How susceptible
    are llms to logical fallacies?” *arXiv*, 8 2023\. [Online]. Available: [https://arxiv.org/abs/2308.09853v1](https://arxiv.org/abs/2308.09853v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou,
    “Large language models cannot self-correct reasoning yet,” *arXiv*, 10 2023\.
    [Online]. Available: [https://arxiv.org/abs/2310.01798](https://arxiv.org/abs/2310.01798)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Z. Ji, T. Yu, Y. Xu, N. Lee, E. Ishii, and P. Fung, “Towards mitigating
    hallucination in large language models via self-reflection,” *arXiv*, 10 2023\.
    [Online]. Available: [https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain,
    and J. Gao, “Large language models: A survey,” *arXiv*, 2 2024\. [Online]. Available:
    [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao,
    “Reflexion: Language agents with verbal reinforcement learning,” *arXiv*, 3 2023\.
    [Online]. Available: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang, “Automatically
    correcting large language models: Surveying the landscape of diverse self-correction
    strategies,” *arXiv*, 8 2023\. [Online]. Available: [https://arxiv.org/abs/2308.03188](https://arxiv.org/abs/2308.03188)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon,
    N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck,
    A. Yazdanbakhsh, and P. Clark, “Self-refine: Iterative refinement with self-feedback,”
    *arXiv*, 3 2023\. [Online]. Available: [https://arxiv.org/abs/2303.17651v2](https://arxiv.org/abs/2303.17651v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Toy, J. MacAdam, and P. Tabor, “Metacognition is all you need? using
    introspection in generative agents to improve goal-directed behavior,” *arXiv*,
    1 2024\. [Online]. Available: [https://arxiv.org/abs/2401.10910](https://arxiv.org/abs/2401.10910)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. Wang and Y. Zhao, “Metacognitive prompting improves understanding in
    large language models,” *arXiv*, 8 2023\. [Online]. Available: [https://arxiv.org/abs/2308.05342](https://arxiv.org/abs/2308.05342)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning
    to retrieve, generate, and critique through self-reflection,” *arXiv*, 10 2023\.
    [Online]. Available: [https://arxiv.org/abs/2310.11511v1](https://arxiv.org/abs/2310.11511v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, A. Anandkumar,
    U. Austin, and U. Madison, “Voyager: An open-ended embodied agent with large language
    models,” *arXiv*, 5 2023\. [Online]. Available: [https://arxiv.org/abs/2305.16291v2](https://arxiv.org/abs/2305.16291v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
    E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou,
    X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu,
    X. Huang, and T. Gui, “The rise and potential of large language model based agents:
    A survey,” *arXiv*, 9 2023\. [Online]. Available: [https://arxiv.org/abs/2309.07864v3](https://arxiv.org/abs/2309.07864v3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,
    “React: Synergizing reasoning and acting in language models,” *arXiv*, 10 2022\.
    [Online]. Available: [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] N. Miao, Y. W. Teh, and T. Rainforth, “Selfcheck: Using llms to zero-shot
    check their own step-by-step reasoning,” *arXiv*, 8 2023\. [Online]. Available:
    [https://arxiv.org/abs/2308.00436v3](https://arxiv.org/abs/2308.00436v3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain,
    V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button,
    M. Knight, B. Chess, and J. S. Openai, “Webgpt: Browser-assisted question-answering
    with human feedback,” *arXiv*, 12 2021\. [Online]. Available: [https://arxiv.org/abs/2112.09332v3](https://arxiv.org/abs/2112.09332v3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves
    to use tools,” *arXiv*, 2 2023\. [Online]. Available: [https://arxiv.org/abs/2302.04761v1](https://arxiv.org/abs/2302.04761v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] P. Lewis and et al., “Retrieval-augmented generation for knowledge-intensive
    nlp tasks,” *arXiv*, 5 2020\. [Online]. Available: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang,
    and H. Wang, “Retrieval-augmented generation for large language models: A survey,”
    *arXiv*, 12 2023\. [Online]. Available: [https://arxiv.org/abs/2312.10997v5](https://arxiv.org/abs/2312.10997v5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang, “Memorybank: Enhancing large
    language models with long-term memory,” *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 38, pp. 19 724–19 731, 5 2023\. [Online]. Available:
    [https://arxiv.org/abs/2305.10250v3](https://arxiv.org/abs/2305.10250v3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Z. Wang, A. Liu, H. Lin, J. Li, X. Ma, and Y. Liang, “Rat: Retrieval augmented
    thoughts elicit context-aware reasoning in long-horizon generation,” *arXiv*,
    3 2024\. [Online]. Available: [https://arxiv.org/abs/2403.05313](https://arxiv.org/abs/2403.05313)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] G. Tyen, H. Mansoor, V. Cărbune, P. Chen, and T. Mak, “Llms cannot find
    reasoning errors, but can correct them!” *arXiv*, 11 2023\. [Online]. Available:
    [https://arxiv.org/abs/2311.08516v2](https://arxiv.org/abs/2311.08516v2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning
    challenge,” *ArXiv*, 3 2018\. [Online]. Available: [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:
    Can a machine really finish your sentence?” in *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*, 2019\. [Online]. Available:
    [https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A large-scale multi-subject
    multi-choice dataset for medical domain question answering,” in *Proceedings of
    the Conference on Health, Inference, and Learning*.   PMLR, 2022, pp. 248–260\.
    [Online]. Available: [https://proceedings.mlr.press/v174/pal22a.html](https://proceedings.mlr.press/v174/pal22a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen,
    and N. Duan, “Agieval: A human-centric benchmark for evaluating foundation models,”
    *ArXiv*, 4 2023\. [Online]. Available: [https://arxiv.org/abs/2304.06364](https://arxiv.org/abs/2304.06364)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, “Logiqa: A challenge
    dataset for machine reading comprehension with logical reasoning,” in *International
    Joint Conference on Artificial Intelligence*, 2020\. [Online]. Available: [https://arxiv.org/abs/2007.08124](https://arxiv.org/abs/2007.08124)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, and N. Duan, “From
    lsat: The progress and challenges of complex reasoning,” *IEEE/ACM Transactions
    on Audio, Speech and Language Processing*, vol. 30, pp. 2201–2216, 8 2021\. [Online].
    Available: [https://doi.org/10.1109/TASLP.2022.3164218](https://doi.org/10.1109/TASLP.2022.3164218)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Anthropic, “Introducing the next generation of claude  anthropic,” 2024\.
    [Online]. Available: [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] ——, “The claude 3 model family: Opus, sonnet, haiku,” 2024\. [Online].
    Available: [https://www.anthropic.com/claude-3-model-card](https://www.anthropic.com/claude-3-model-card)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Cohere, “Command r+,” 2024\. [Online]. Available: [https://docs.cohere.com/docs/command-r-plus](https://docs.cohere.com/docs/command-r-plus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] ——, “Model card for c4ai command r+,” 2024\. [Online]. Available: [https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Pichai and D. Hassabis, “Introducing gemini: Google’s most capable
    ai model yet,” 2023\. [Online]. Available: [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Gemini-Team, “Gemini: A family of highly capable multimodal models,” *arXiv*,
    12 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Pichai and D. Hassabis, “Introducing gemini 1.5, google’s next-generation
    ai model,” 2024\. [Online]. Available: [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Gemini-Team, “Gemini 1.5: Unlocking multimodal understanding across millions
    of tokens of context,” 2024\. [Online]. Available: [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] OpenAI, “Introducing chatgpt,” 11 2022\. [Online]. Available: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] ——, “Models - openai api.” [Online]. Available: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] ——, “Gpt-4,” 3 2023\. [Online]. Available: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] ——, “Gpt-4 technical report,” *arXiv*, 3 2023\. [Online]. Available: [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Meta, “Meta and microsoft introduce the next generation of llama | meta,”
    2023\. [Online]. Available: [https://about.meta.com/news/2023/07/llama-2/](https://about.meta.com/news/2023/07/llama-2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen,
    G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami,
    N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa,
    I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich,
    Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton,
    J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian,
    X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov,
    Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov,
    and T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,” *arXiv*,
    7 2023\. [Online]. Available: [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Mistral-AI-Team, “Au large | mistral ai | frontier ai in your hands,”
    2024\. [Online]. Available: [https://mistral.ai/news/mistral-large/](https://mistral.ai/news/mistral-large/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. M. Bsharat, A. Myrzakhan, and Z. Shen, “Principled instructions are
    all you need for questioning llama-1/2, gpt-3.5/4,” *arXiv*, 12 2023\. [Online].
    Available: [https://arxiv.org/abs/2312.16171](https://arxiv.org/abs/2312.16171)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Renze and E. Guven, “The benefits of a concise chain of thought on
    problem-solving in large language models,” *arXiv*, 1 2024\. [Online]. Available:
    [https://arxiv.org/abs/2401.05618v1](https://arxiv.org/abs/2401.05618v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] ——, “The effect of sampling temperature on problem solving in large language
    models,” *arXiv*, 2 2024\. [Online]. Available: [https://arxiv.org/abs/2402.05201v1](https://arxiv.org/abs/2402.05201v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Q. McNemar, “Note on the sampling error of the difference between correlated
    proportions or percentages,” *Psychometrika*, vol. 12, pp. 153–157, 6 1947\. [Online].
    Available: [https://doi.org/10.1007/BF02295996](https://doi.org/10.1007/BF02295996)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Agent Name | Accuracy | Difference | Test Statistic | p-value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 0.786 | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Retry | 0.827 | 0.041 | 39.024 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Keywords | 0.832 | 0.046 | 44.022 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Advice | 0.840 | 0.054 | 52.019 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Instructions | 0.849 | 0.063 | 61.016 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Explanation | 0.876 | 0.090 | 88.011 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Solution | 0.925 | 0.139 | 137.007 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Composite | 0.932 | 0.146 | 144.007 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: '| Unredacted | 0.971 | 0.185 | 183.005 | $<0.001$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of accuracy by agent for GPT-4'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Name | Baseline | Retry | Keywords | Advice | Instruction | Explanation
    | Solution | Composite | Unredacted |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Claude 3 Opus | 0.792 | 0.849 | 0.855 | 0.852 | 0.853 | 0.908 | 0.939 | 0.947
    | 0.971 |'
  prefs: []
  type: TYPE_TB
- en: '| Cohere Command R+ | 0.641 | 0.745 | 0.77 | 0.733 | 0.798 | 0.77 | 0.843 |
    0.874 | 0.937 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini 1.0 Pro | 0.617 | 0.724 | 0.734 | 0.724 | 0.725 | 0.748 | 0.763 |
    0.774 | 0.881 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini 1.5 Pro | 0.751 | 0.813 | 0.807 | 0.824 | 0.804 | 0.812 | 0.818 |
    0.815 | 0.972 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 Turbo | 0.596 | 0.686 | 0.691 | 0.704 | 0.706 | 0.802 | 0.831 | 0.827
    | 0.904 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 0.786 | 0.827 | 0.832 | 0.840 | 0.849 | 0.876 | 0.925 | 0.932 | 0.971
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 70b | 0.376 | 0.481 | 0.564 | 0.591 | 0.575 | 0.655 | 0.600 | 0.672
    | 0.837 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 7b | 0.297 | 0.372 | 0.364 | 0.374 | 0.377 | 0.457 | 0.413 | 0.427
    | 0.495 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral Large | 0.723 | 0.769 | 0.796 | 0.802 | 0.803 | 0.825 | 0.889 | 0.896
    | 0.922 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Accuracy by model and agent'
  prefs: []
  type: TYPE_NORMAL
- en: '| Agent Title | AQUA-RAT | ARC | Hellaswag | LSAT-AR | LSAT-LR | LSAT-RC |
    LogiQA | MedMCQA | SAT-English | SAT-Math |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 0.79 | 0.95 | 0.89 | 0.33 | 0.83 | 0.85 | 0.62 | 0.77 | 0.93 |
    0.90 |'
  prefs: []
  type: TYPE_TB
- en: '| Retry | 0.83 | 0.96 | 0.92 | 0.45 | 0.84 | 0.86 | 0.68 | 0.79 | 0.95 | 0.99
    |'
  prefs: []
  type: TYPE_TB
- en: '| Keywords | 0.85 | 0.97 | 0.91 | 0.45 | 0.85 | 0.88 | 0.69 | 0.81 | 0.94 |
    0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Advice | 0.87 | 0.98 | 0.92 | 0.45 | 0.84 | 0.88 | 0.71 | 0.84 | 0.94 | 0.97
    |'
  prefs: []
  type: TYPE_TB
- en: '| Instructions | 0.86 | 0.98 | 0.93 | 0.48 | 0.86 | 0.88 | 0.71 | 0.88 | 0.95
    | 0.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Explanation | 0.86 | 0.99 | 0.93 | 0.53 | 0.91 | 0.93 | 0.76 | 0.92 | 0.96
    | 0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Solution | 0.88 | 1.00 | 0.96 | 0.76 | 0.94 | 0.95 | 0.87 | 0.94 | 0.97 |
    0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| Composite | 0.87 | 1.00 | 0.99 | 0.72 | 0.99 | 0.96 | 0.88 | 0.95 | 0.98
    | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| Unredacted | 0.91 | 1.00 | 0.99 | 0.92 | 0.99 | 0.98 | 0.95 | 0.99 | 0.99
    | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Accuracy by agent and exam for GPT-4'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 5: Sample of an MCQA problem in JSON-L format – with whitespace added
    for readability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 6: Sample of the answer prompt used by the baseline agent to solve MCQA
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 7: Sample of an MCQA problem in JSON-L format – with whitespace added
    for readability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 8: Sample of the re-answer prompt used by the self-reflecting agents.
    The system prompt, example problem, and example solution are identical to the
    answer prompt and thus omitted for clarity.'
  prefs: []
  type: TYPE_NORMAL
