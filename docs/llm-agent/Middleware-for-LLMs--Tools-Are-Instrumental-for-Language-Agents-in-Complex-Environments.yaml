- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex
    Environments'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14672](https://ar5iv.labs.arxiv.org/html/2402.14672)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yu Gu¹, Yiheng Shu¹, Hao Yu², Xiao Liu², Yuxiao Dong², Jie Tang²,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Jayanth Srinivasa³, Hugo Latapie³, Yu Su¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ¹The Ohio State University  ²Tsinghua University  ³Cisco Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '{gu.826, su.809}@osu.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The applications of large language models (LLMs) have expanded well beyond the
    confines of text processing, signaling a new era where LLMs are envisioned as
    generalist language agents capable of operating within complex real-world environments.
    These environments are often highly expansive, making it impossible for the LLM
    to process them within its short-term memory. Motivated by recent research on
    extending the capabilities of LLMs with tools, this paper investigates the intriguing
    potential of tools to augment LLMs in handling such complexity. To this end, we
    design customized tools to aid in the proactive exploration within these massive
    environments. Such tools can serve as a middleware layer shielding the LLM from
    environmental complexity. In two representative complex environments—knowledge
    bases (KBs) and databases—we demonstrate the significant potential of augmenting
    language agents with tools in complex environments. Notably, equipped with these
    tools, GPT-4 achieves 2.8$\times$ in KB tasks. Our findings illuminate the path
    for advancing language agents in complex real-world applications. ¹¹1Our code
    and data will be released at [OSU_NLP/Fuxi](https://github.com/OSU-NLP-Group/Fuxi).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated revolutionary language capabilities,
    demonstrating a human-like mastery over text OpenAI ([2023a](#bib.bib25), [b](#bib.bib26));
    Touvron et al. ([2023](#bib.bib39)); Jiang et al. ([2024](#bib.bib16)). However,
    the true ambition of AI extends well beyond the realm of text. The goal is to
    ultimately empower LLMs to act as generalist language agents that can aid humans
    across the multitude of complex real-world tasks Yao et al. ([2022](#bib.bib41));
    Schick et al. ([2023](#bib.bib31)); Gu et al. ([2023](#bib.bib8)), which often
    involve handling complex environments, whether it be browsing intricate webpages Deng
    et al. ([2023](#bib.bib6)) or managing vast databases with millions of entries Li
    et al. ([2023a](#bib.bib18)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19c99f4ee7ddc2cb4814281e3e828b41.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (left) When an LLM engages with a complex environment, it can develop
    an understanding by fitting the environment’s description (i.e., linearized tokens)
    into its short-term memory (i.e., the LLM’s input window). However, this method
    encounters drastic scalability issues as the complexity of the environment grows.
    (right) Another option is to furnish the LLM with a set of tools that assist it
    in actively engaging with the environment and acquiring the necessary information.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'For LLMs to effectively serve as agents that ground human instructions into
    accurate actions within the environment, they must develop a robust understanding
    of the environment. The most direct method to achieve it is to linearize the environment
    into a sequence of tokens that fit into the LLM’s short-term memory (i.e., its
    input window) and have the LLM process the environment based on the linearized
    description Tai et al. ([2023](#bib.bib37)); Shridhar et al. ([2021](#bib.bib32));
    Liu et al. ([2023](#bib.bib21)). However, such a method faces steep challenges
    in scaling to more complex environments, primarily due to the input size limitations
    of LLMs. Also, discrete token descriptions may not reflect the most natural perception
    of the environment. Recent work has explored using tools to extend the boundary
    of the LLM’s capacity Li et al. ([2023b](#bib.bib19)); Qin et al. ([2023b](#bib.bib28));
    Schick et al. ([2023](#bib.bib31)). The core idea is that LLMs can actively decide
    a proper tool to use, using language as a powerful vehicle of thought Su ([2023](#bib.bib34)).
    For example, the LLM may invoke a calculator when facing a computationally intensive
    math task. Intuitively, we can also equip the LLM with tools that enable navigating
    complex environments, so that the LLM can proactively invoke different tools to
    explore the environment, thus circumventing limitations posed by its short-term
    memory (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Middleware for LLMs: Tools
    Are Instrumental for Language Agents in Complex Environments")). However, this
    promising paradigm has been thus far underexplored. In this paper, we aim to delve
    into this paradigm and answer an intriguing question: How effectively can LLMs
    handle complex environments with the aid of tools?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Answering this question requires equipping the LLM with a suite of tools designed
    to meet a wide range of needs within the target environment. In this paper, we
    carefully develop such tailored tools for two exemplar complex environments, i.e.,
    databases and knowledge bases (KBs). Unlike readily available Web APIs Qin et al.
    ([2023b](#bib.bib28)) used in prior research, our tools have to be manually invented
    from scratch. In crafting these tools, we capitalize on the intuition of human
    information-gathering behaviors—such as performing keyword searches to identify
    a relevant database column or investigating the connections of a KB entity—to
    fulfill complex tasks in these intricate environments (Section [3.1](#S3.SS1 "3.1
    Tools for Complex Environments ‣ 3 Fuxi ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments")). Ideally, these tools are designed
    to function as a middleware layer between the LLM and the environment, shielding
    the LLM from environmental complexity. With these specialized tools in place,
    we adapt ReAct Yao et al. ([2022](#bib.bib41)), a standard framework that enables
    the LLM to synergistically combine reasoning with tool usage, as our reasoning
    algorithm to allow the LLM to effectively leverage the provided tools (Section
     [3.2](#S3.SS2 "3.2 Reasoning with Tools ‣ 3 Fuxi ‣ Middleware for LLMs: Tools
    Are Instrumental for Language Agents in Complex Environments")). The combination
    of the crafted tools and the reasoning algorithm allows the LLM to actively explore
    the environment and ground human instructions into accurate actions. We call this
    framework Fuxi (i.e., flexible grounding with exploration).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'With Fuxi, we evaluate different LLMs on benchmarks featuring complex tasks
    over the target environments, including a newly curated benchmark for the KB.
    The outcomes of our experiments are revealing: LLMs equipped with customized tools
    demonstrate a significant enhancement in their ability to engage with complex
    environments, markedly surpassing the prior art. In particular, despite its simplicity,
    Fuxi allows GPT-4 OpenAI ([2023a](#bib.bib25)) to achieve 2.8$\times$%) in KB
    tasks. Our findings underscore the integral role of tool augmentation in enabling
    LLMs to handle complex environments.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are as follows: a) We develop Fuxi, a new framework
    with customized tools for two complex environments, to investigate the role of
    tools in handling complex environments with LLMs; b) We extensively evaluate six
    different LLMs on our carefully chosen benchmarks; c) Our analysis highlights
    a critical takeaway: augmenting LLMs with tools is crucial for successfully tackling
    complex environments, opening new possibilities to progress LLMs as generalist
    language agents for practical applications.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Interface Complex Environments with LLMs.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing methods that feed the environment directly into the LLM for grounding Chandu
    et al. ([2021](#bib.bib4)) would fail in complex environments due to scalability
    issues. Specifically, these methods process the environment by linearizing it
    into discrete tokens Hwang et al. ([2019](#bib.bib14)); Shridhar et al. ([2021](#bib.bib32));
    Yu et al. ([2023](#bib.bib43)); Liu et al. ([2023](#bib.bib21)); Tai et al. ([2023](#bib.bib37));
    Song et al. ([2023](#bib.bib33)). However, linearizing expansive environments
    like databases with millions of entries Li et al. ([2023a](#bib.bib18)) or lengthy
    webpage HTML code Deng et al. ([2023](#bib.bib6)) can often exceed an LLM’s input
    length constraints. Alternative studies bypass the LLM’s direct interaction with
    complex environments by generating ungrounded draft plans for post-processing
    grounding Li et al. ([2023c](#bib.bib20)); Nie et al. ([2023](#bib.bib24)) or
    by using the LLM to assess grounded plans created via predefined rules Gu et al.
    ([2023](#bib.bib8)). Such strategies do not fully utilize the LLMs’ innate reasoning
    potential in actively navigating complex environments. In this paper, we explore
    a new paradigm where we can bypass these issues by equipping LLMs with a suite
    of comprehensive tools to actively gather necessary information about the environment
    upon demand, leveraging the LLMs’ inherent reasoning capabilities. The most closely
    related work to ours is StructGPT Jiang et al. ([2023b](#bib.bib17)). However,
    the narrow tool selection of StructGPT (i.e., only two tools for KBs and three
    schema-level tools for databases) largely constrains its flexibility in perceiving
    the complex environment when handling diverse tasks.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接将环境输入LLM进行基础处理的现有方法，如Chandu等人（[2021](#bib.bib4)），由于可扩展性问题，在复杂环境中会失败。具体来说，这些方法通过将环境线性化为离散的标记来处理环境，如Hwang等人（[2019](#bib.bib14)）；Shridhar等人（[2021](#bib.bib32)）；Yu等人（[2023](#bib.bib43)）；Liu等人（[2023](#bib.bib21)）；Tai等人（[2023](#bib.bib37)）；Song等人（[2023](#bib.bib33)）。然而，像具有数百万条记录的数据库（Li等人（[2023a](#bib.bib18)））或长篇网页HTML代码（Deng等人（[2023](#bib.bib6)））这样的扩展环境的线性化，通常会超出LLM的输入长度限制。替代研究通过生成未经过基础处理的草案计划以供后续处理（Li等人（[2023c](#bib.bib20)）；Nie等人（[2023](#bib.bib24)）），或者通过使用LLM评估通过预定义规则创建的基础处理计划（Gu等人（[2023](#bib.bib8)））来绕过LLM与复杂环境的直接交互。这些策略没有充分利用LLM在主动导航复杂环境中的固有推理潜力。在本文中，我们探索了一种新范式，通过为LLM配备一套全面的工具来主动收集环境所需的信息，从而绕过这些问题，充分发挥LLM的固有推理能力。与我们的工作最相关的是StructGPT（Jiang等人（[2023b](#bib.bib17)））。然而，StructGPT狭窄的工具选择（即，仅有两个KB工具和三个用于数据库的模式级工具）在处理多样化任务时大大限制了其在感知复杂环境方面的灵活性。
- en: Tool Learning.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具学习。
- en: Tools are essential for enhancing the capabilities of LLMs Schick et al. ([2023](#bib.bib31));
    Qin et al. ([2023a](#bib.bib27)); Mialon et al. ([2023](#bib.bib23)); Hao et al.
    ([2023](#bib.bib13)). Existing research, such as ToolLLM Qin et al. ([2023b](#bib.bib28))
    and API-Bank Li et al. ([2023b](#bib.bib19)), focuses on open-domain applications
    with a wide array of readily available RESTful APIs. In contrast, this paper specifically
    aims to study the potential of tools in augmenting LLMs to effectively execute
    tasks within complex environments, where we carefully craft the specialized tools
    for different environments by ourselves. In addition, research focusing on RESTful
    APIs typically displays shallow reasoning, while practical tasks within a complex
    environment typically entail a long sequence of actions (e.g., querying a KB or
    browsing a webpage). To enable tool use in more intricate settings within a more
    specific complex environment, StructGPT Jiang et al. ([2023b](#bib.bib17)) employs
    a predefined sequence of tool invocations; Chameleon Lu et al. ([2023](#bib.bib22))
    functions in an open-loop setting where the LLM directly produces a sequence for
    tool usage before any execution occurs. Both of them fail to seamlessly integrate
    the reasoning capacity of the LLM with the use of tools. In this paper, we build
    on ReAct to tightly synergize the generation of a reasoning step and corresponding
    tool use. Additionally, we introduce two simple yet effective strategies aimed
    at improving the accuracy of action prediction.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3 Fuxi
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fuxi equips LLMs with a suite of tools specifically tailored to support an
    extensive variety of operations and cater to the diverse needs within a complex
    environment $\mathcal{E}$, abstracting the LLM from having to directly interact
    with all of its intricacies (Section [3.1](#S3.SS1 "3.1 Tools for Complex Environments
    ‣ 3 Fuxi ‣ Middleware for LLMs: Tools Are Instrumental for Language Agents in
    Complex Environments")). Furthermore, to fully unleash the inherent planning capabilities
    of LLMs in invoking proper tools, Fuxi builds on ReAct Yao et al. ([2022](#bib.bib41))
    to seamlessly integrate chain-of-thought (CoT) reasoning Wei et al. ([2022](#bib.bib40))
    with tool use, with novel strategies to enhance action accuracy (Section [3.2](#S3.SS2
    "3.2 Reasoning with Tools ‣ 3 Fuxi ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments")). This unified framework allows
    us to reliably investigate the potential of LLMs in handling complex environments
    with the aid of tools.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e912ff983064dad562ab08455e1d8411.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The LLM is equipped with an array of tools to facilitate its engagement
    with complex environments (e.g., a KB here). (a) The LLM may produce invalid actions
    (marked in pink). This can be mitigated by prompting it with an error message
    that encourages a reattempt (corrected action marked in green). (b) Alternatively,
    we can have the LLM first generate a thought, then predict an action based on
    it in a separate context (marked in blue), and finally insert the action back
    to the original context. Text marked in yellow are input from the environment.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Tools for Complex Environments
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the potential of LLMs in handling complex environments when equipped
    with tools, we need to first carefully craft the necessary tools for the environments.
    These tools should meet two essential criteria: 1) They should offer comprehensiveness,
    encompassing a broad spectrum of operations and needs. Broad coverage of tools
    is crucial for maximizing the potential of LLMs in planning. 2) The tools should
    prioritize ease of use, enabling the LLM to invoke them mostly with straightforward
    slot filling, thus shielding the LLM from the implementation details of the tools.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In production scenarios, databases typically feature dozens of tables, with
    each table containing thousands of rows or more. A key task in such environments
    is performing data analysis through SQL queries. To bridge the gap between natural
    language instructions and SQL, LLMs are employed to automate the generation of
    SQL queries (i.e., text-to-SQL parsing Yu et al. ([2018](#bib.bib44)); Li et al.
    ([2023a](#bib.bib18))). To support the LLM in crafting complex SQL queries, we
    introduce a set of specialized tools designed for interaction with intricate databases.
    These tools are divided into two main categories: navigational and functional.
    Navigational tools help the LLM to explore the environment (e.g., get_distinct_values()
    and find_columns_containing_value()), while functional tools help check each SQL
    clause composed by the LLM. For example, where() verifies the legality of the
    WHERE clause and determines if the specified conditions can match any entries
    in the database. In total, we craft $12$ tools for databases (Appendix [A.1](#A1.SS1
    "A.1 Databases ‣ Appendix A Detailed Tool Definitions ‣ Middleware for LLMs: Tools
    Are Instrumental for Language Agents in Complex Environments")). The development
    of these tools is grounded in our domain expertise in SQL and databases.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: KBs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modern KBs, such as Freebase Bollacker et al. ([2008](#bib.bib2)), are vast
    repositories storing billions of facts as triples $\langle h,r,t\rangle$ tools
    for KBs (Appendix [A.2](#A1.SS2 "A.2 Knowledge Bases ‣ Appendix A Detailed Tool
    Definitions ‣ Middleware for LLMs: Tools Are Instrumental for Language Agents
    in Complex Environments")). Our design of KB tools tightly adheres to the common
    needs in knowledge base question answering (KBQA) Gu et al. ([2021](#bib.bib9));
    Cao et al. ([2022](#bib.bib3)).'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Reasoning with Tools
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We leverage ReAct to enable the LLM to effectively invoke our crafted tools.
    Unlike existing methods relying on rigid, human-defined workflows that follow
    fixed tool usage sequences  Jiang et al. ([2023b](#bib.bib17)), ReAct allows the
    LLM autonomy in proactively determining tool selection using CoT. Thus, ReAct
    allows us to tap into the full potential of the LLM’s reasoning capabilities.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Formally, at each step $t$, where
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c_{t}$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{a}_{t}$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '$\hat{a}_{t}$ encapsulates the governing rules of the environment (e.g., the
    relation argument for get_neighbors() must be derived from the output of get_relations(),
    which is applied to the specified entity argument in prior steps), infusing prior
    knowledge into the LLM’s decision-making process (see Figure [2](#S3.F2 "Figure
    2 ‣ 3 Fuxi ‣ Middleware for LLMs: Tools Are Instrumental for Language Agents in
    Complex Environments")(b)). The concrete prompts used by us are shown in Appendix [C](#A3
    "Appendix C Prompts ‣ Middleware for LLMs: Tools Are Instrumental for Language
    Agents in Complex Environments").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Req. Cont. (N) | Req. Cont. (Y) | Overall |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '|  | EX | VA | EX | VA | EX | VA |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| w/ Oracle Knowledge |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| API Docs Prompt Rajkumar et al. ([2022](#bib.bib29)) |  |  |  |  |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $38.1$ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $49.5$ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| w/o Oracle Knowledge |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| API Docs Prompt Rajkumar et al. ([2022](#bib.bib29)) |  |  |  |  |  |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo^† | $30.9$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $38.2$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| StructGPT Jiang et al. ([2023b](#bib.bib17)) |  |  |  |  |  |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $36.2$ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $40.7$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Fuxi (error feedback) |  |  |  |  |  |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $38.8$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $45.1$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results on Bird’s dev set. Performance of all baselines is obtained
    under a zero-shot setting. ${\dagger}$ denotes the best method w/o oracle knowledge
    on Bird’s official leaderboard. The predictions with API Docs Prompt are directly
    supplied by the authors of Bird.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Counting | Superlative | None | Overall |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '|  | F1 | VA | F1 | VA | F1 | VA | F1 | VA |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| Pangu^♢ Gu et al. ([2023](#bib.bib8)) |  |  |  |  |  |  |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $10.1$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $12.3$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| KB-Binder Li et al. ([2023c](#bib.bib20)) |  |  |  |  |  |  |  |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo (20-shot) | $0.0$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 (20-shot) | $7.9$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| StructGPT Jiang et al. ([2023b](#bib.bib17)) |  |  |  |  |  |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $4.5$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $2.2$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| Fuxi (error feedback) |  |  |  |  |  |  |  |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $33.7$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $70.7$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| Fuxi (decoupled generation) |  |  |  |  |  |  |  |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-3.5-turbo | $48.9$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '|               w/ GPT-4 | $74.1$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results on KBQA-Agent. All models are provided with one-shot demonstration
    except for KB-Binder, where we provide 20-shot demonstrations for optimal performance.
    $\diamondsuit$ indicates our reimplementation of Pangu, as the original code lacks
    support for chat models. We assume perfect entity linking for all methods.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 4 Benchmarks
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The predominant tasks for databases and KBs are text-to-SQL parsing and KBQA.
    However, popular benchmarks for them may fall short for evaluating language agents
    out-of-box. Specifically, the majority of questions in popular KBQA datasets like
    WebQSP Berant et al. ([2013](#bib.bib1)); Yih et al. ([2016](#bib.bib42)) are
    one-hop or two-hop questions, for which we can effectively handle with existing
    semantic parsing methods Gu et al. ([2022](#bib.bib10)). Additionally, the databases
    featured in Spider Yu et al. ([2018](#bib.bib44)) and WikiSQL Zhong et al. ([2017](#bib.bib47))
    have limited complexity in terms of both schema design and the number of rows
    in the tables. This over-simplification enables the direct feeding of the database
    schema to the LLM, achieving strong performance without the need to access the
    actual content of the database Rajkumar et al. ([2022](#bib.bib29)). Therefore,
    we need different benchmarks with complex environments and instructions that better
    mirror the real-world situations language agents must handle (see statistics of
    our benchmarks in Appendix [B](#A2 "Appendix B Benchmark Statistics ‣ Middleware
    for LLMs: Tools Are Instrumental for Language Agents in Complex Environments")).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For databases, we leverage Bird Li et al. ([2023a](#bib.bib18)), which is a
    recent dataset notable for its complexity, featuring intricate instructions over
    highly complex databases. There are originally two different settings in Bird:
    with and without oracle knowledge, where the oracle knowledge supplies specific
    information about the target database needed to fulfill each task. For instance,
    “Exclusively virtual refers to Virtual = ‘F”’. With such oracle knowledge, the
    complexity of the environments is substantially mitigated; it offers a shortcut
    for the task and eliminates the necessity for deep engagement with the database.
    This cheating setting is also unrealistic for practical applications. As a result,
    we stick to the setting without oracle knowledge. For each of the $1534$ questions)
    and enables fine-grained insights into the LLM’s performance. In addition to execution
    accuracy (EX) used in Bird, which determines if the execution results of the predicted
    SQL match those of the ground truth SQL, we also evaluate whether the predicted
    SQL is a valid SQL query (VA).'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: KBs
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We curate KBQA-Agent, a new test set sourcing from existing KBQA datasets that
    contain complex questions. In particular, we selected $500$ diverse questions
    that involve at least three relations, or two relations coupled with an aggregation
    function (i.e., Counting or Superlative). For each question, we annotate it with
    a ground truth sequence of actions based on the toolset defined by us.³³3We leverage
    the gold S-expressions provided by Gu and Su ([2022](#bib.bib11)). Our dataset
    has been served as part of AgentBench Liu et al. ([2023](#bib.bib21)). Specifically,
    KBQA-Agent comprises questions from three KBQA datasets on Freebase: GrailQA Gu
    et al. ([2021](#bib.bib9)), ComplexWebQ Talmor and Berant ([2018](#bib.bib38)),
    and GraphQ Su et al. ([2016](#bib.bib35)), ensuring a wide range of question types
    and sources. KBQA-Agent is designed to be more representative of challenging,
    real-world scenarios compared to existing benchmarks (Appendix [B](#A2 "Appendix
    B Benchmark Statistics ‣ Middleware for LLMs: Tools Are Instrumental for Language
    Agents in Complex Environments")). It offers an ideal testbed for evaluating language
    agents in interacting with massive KBs. We assess this through two metrics: F1
    of answer entities and Validity (VA), a binary metric evaluating the LLM’s ability
    to find an answer, whether correct or not.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4e96952e7dcfddc84d8b39db46ae180.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The open-source LLMs still largely lag behind GPT-3.5-turbo and GPT-4
    in both environments.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Setup
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To concretely instantiate our tools for the two environments, we employ standard
    query interfaces for databases and KBs, specifically SQLite for databases and
    Virtuoso for KBs. We then prompt the LLM with the tool descriptions together with
    the input task instructions (Appendix [C](#A3 "Appendix C Prompts ‣ Middleware
    for LLMs: Tools Are Instrumental for Language Agents in Complex Environments")).
    Each environment exhibits its own unique characteristics and challenges. In KBQA,
    the arguments for each function are either a variable or an item from the KB schema
    (i.e., a relation or an attribute). In contrast, in text-to-SQL parsing, the arguments
    can be more varied, ranging from a part of a SQL query to a complete query. This
    makes listing potential actions, as needed in decoupled generation, much more
    complex for text-to-SQL parsing. Therefore, we implement error feedback solely
    for text-to-SQL parsing.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the underlying LLMs, we primarily compare Fuxi with baseline methods using
    two of the most advanced LLMs—GPT-3.5-turbo OpenAI ([2023b](#bib.bib26)) and GPT-4 OpenAI
    ([2023a](#bib.bib25))—since our goal is investigating the full potential of tool-enhanced
    LLMs operating within complex environments. In addition, we also explore four
    open-source LLMs to more comprehensively evaluate our framework: Llama2-7B-Chat,
    Llama2-13B-Chat Touvron et al. ([2023](#bib.bib39)), Mistral-7B-Instruct-v0.2 Jiang
    et al. ([2023a](#bib.bib15)), and Mixtral 8$\times$7B-Instruct-v0.1 Jiang et al.
    ([2024](#bib.bib16)).'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于底层的LLMs，我们主要使用两个最先进的LLMs——GPT-3.5-turbo OpenAI ([2023b](#bib.bib26)) 和 GPT-4
    OpenAI ([2023a](#bib.bib25))——将Fuxi与基线方法进行比较，因为我们的目标是**研究工具增强型LLMs**在复杂环境中发挥的**全部潜力**。此外，我们还探索了四个开源LLMs，以更全面地评估我们的框架：Llama2-7B-Chat，Llama2-13B-Chat
    Touvron et al. ([2023](#bib.bib39))，Mistral-7B-Instruct-v0.2 Jiang et al. ([2023a](#bib.bib15))，以及Mixtral
    8$\times$7B-Instruct-v0.1 Jiang et al. ([2024](#bib.bib16))。
- en: '![Refer to caption](img/8f0b6ac2d6d22546fde3d02315c6c4f7.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8f0b6ac2d6d22546fde3d02315c6c4f7.png)'
- en: 'Figure 4: The customized tools can serve as effective middleware between the
    LLM and the environment.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：定制工具可以作为LLM与环境之间的有效中间件。
- en: Baselines
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线
- en: 'To fully understand the potential of tool augmentation for assisting LLMs in
    handling complex environments, we compare Fuxi against an array of strong baselines.
    For text-to-SQL parsing, LLMs demonstrate a strong ability to compose SQL queries
    when properly prompted with the database schema (i.e., API docs prompting Rajkumar
    et al. ([2022](#bib.bib29))). This also represents the current state-of-the-art
    prompting-based method when oracle knowledge is not available on Bird’s leaderboard.⁴⁴4https://bird-bench.github.io
    In addition, we experiment with StructGPT Jiang et al. ([2023b](#bib.bib17)),
    which represents an advanced text-to-SQL parsing method leveraging tools. For
    all methods on text-to-SQL parsing, we adopt the zero-shot setting. Unlike text-to-SQL
    parsing, directly prompting LLMs does not generate reasonable outputs for KBQA
    due to the massive size of the KB schema. Instead, existing KBQA methods based
    on LLMs typically follow two paradigms: either first generating an ungrounded
    program and then grounding the program to the KB schema afterwards Li et al. ([2023c](#bib.bib20));
    Nie et al. ([2023](#bib.bib24)), or gradually constructing a complex program and
    grounding it step by step Gu et al. ([2023](#bib.bib8)). We compare Fuxi with
    the most representative work from each paradigm, namely KB-Binder Li et al. ([2023c](#bib.bib20))
    and Pangu Gu et al. ([2023](#bib.bib8)). We also include StructGPT as an additional
    baseline for tool use. For all KBQA methods except KB-Binder, we provide a one-shot
    demo to obtain more meaningful results.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了**全面理解**工具增强对LLMs处理复杂环境的潜力，我们将Fuxi与一系列强大的基线方法进行了比较。在文本到SQL解析中，当适当地提供数据库模式（即API文档提示
    Rajkumar et al. ([2022](#bib.bib29)))时，LLMs展示了强大的SQL查询生成能力。这也代表了当前最先进的基于提示的方法，当在Bird的排行榜上没有oracle知识时。⁴⁴4https://bird-bench.github.io
    另外，我们还实验了StructGPT Jiang et al. ([2023b](#bib.bib17))，这是一种利用工具的高级文本到SQL解析方法。对于所有文本到SQL解析的方法，我们采用零-shot设置。与文本到SQL解析不同，由于KB模式的庞大规模，直接提示LLMs不会生成合理的KBQA输出。现有的基于LLMs的KBQA方法通常遵循两种范式：要么首先生成一个无基础的程序，然后将程序与KB模式进行对接
    Li et al. ([2023c](#bib.bib20)); Nie et al. ([2023](#bib.bib24))，要么逐步构建复杂程序，并一步步对接
    Gu et al. ([2023](#bib.bib8))。我们将Fuxi与每个范式中最具代表性的工作进行比较，即KB-Binder Li et al. ([2023c](#bib.bib20))
    和 Pangu Gu et al. ([2023](#bib.bib8))。我们还将StructGPT作为工具使用的额外基线。对于所有KBQA方法，除了KB-Binder，我们提供了一个one-shot演示以获得更有意义的结果。
- en: 5.2 Main Results
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 'As shown in Tables [1](#S3.T1 "Table 1 ‣ 3.2 Reasoning with Tools ‣ 3 Fuxi
    ‣ Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments")
    and [2](#S3.T2 "Table 2 ‣ 3.2 Reasoning with Tools ‣ 3 Fuxi ‣ Middleware for LLMs:
    Tools Are Instrumental for Language Agents in Complex Environments"), equipping
    LLMs with customized tools leads to significant improvement over previous standards,
    almost doubling or tripling the performance under multiple metrics. Specifically,
    API docs prompting can only feed the schema information to the LLM due to the
    vast amount of database content. As a result, it fails catastrophically on examples
    that require database content to compose the SQL query. In contrast, Fuxi equips
    the agent with tools to actively navigate the database to collect relevant information
    for composing a SQL query. As a result, Fuxi significantly closes the gap between
    performance on questions requiring database content and questions not requiring
    it when using GPT-4 (i.e., $45.1$% in F1. As for the other two baselines, KB-Binder
    and StructGPT, both fail miserably on our challenging setting. On the one hand,
    KB-Binder only retrieves relations within two hops from the entities for grounding.
    However, most questions in KBQA-Agent involve more than two relations. As a result,
    many of its drafted programs are unable to ground, which explains its low VA.
    On the other hand, StructGPT is heavily limited by its constrained toolset and
    cannot handle complex questions in KBQA-Agent. Therefore, StructGPT frequently
    refuses to provide an answer (as revealed by its low VA) due to insufficient information.
    The strong performance of Fuxi underscores that tools are instrumental for language
    agents in complex environments.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[1](#S3.T1 "表 1 ‣ 3.2 使用工具推理 ‣ 3 Fuxi ‣ LLM的中间件：工具对复杂环境中的语言代理至关重要")和[2](#S3.T2
    "表 2 ‣ 3.2 使用工具推理 ‣ 3 Fuxi ‣ LLM的中间件：工具对复杂环境中的语言代理至关重要")所示，给LLM配备定制工具比以往标准显著提高性能，在多个指标下几乎翻倍或三倍。具体而言，由于海量的数据库内容，API文档提示仅能提供模式信息给LLM。因此，对于需要数据库内容来编写SQL查询的例子，它的表现会极其糟糕。相比之下，Fuxi为代理提供工具，以主动浏览数据库以收集编写SQL查询所需的相关信息。因此，Fuxi显著缩小了在需要数据库内容和不需要数据库内容的问题上的性能差距（即，GPT-4中的F1达到$45.1$%）。至于其他两个基准，KB-Binder和StructGPT，都在我们具有挑战性的设置中表现糟糕。一方面，KB-Binder仅从实体处检索两跳内的关系用于定位。然而，KBQA-Agent中的大多数问题涉及超过两个关系。因此，它草拟的许多程序无法进行定位，这解释了其低VA。另一方面，StructGPT受限于其有限的工具集，无法处理KBQA-Agent中的复杂问题。因此，StructGPT经常因为信息不足而拒绝提供答案（由其低VA揭示）。Fuxi的强大表现强调了工具在复杂环境中对语言代理的重要性。
- en: 5.3 Experiments with Open-Source LLMs
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 与开源LLM的实验
- en: 'To gain a more thorough insight, we also include experiments with four open-source
    LLMs ( Figure [3](#S4.F3 "Figure 3 ‣ 4 Benchmarks ‣ Middleware for LLMs: Tools
    Are Instrumental for Language Agents in Complex Environments")). Our findings
    indicate that Llama2 models generally underperform compared to other LLMs, aligning
    with trends observed in other LLM leaderboards, such as Chatbot Arena Zheng et al.
    ([2023](#bib.bib46)). Specifically, we find Llama2 models struggle with even generating
    grammatical tool use following our instruction. On the other hand, Mistral and
    Mixtral demonstrate much better performance than Llama2. In particular, Mixtral
    represents an advanced mixture-of-experts model that has demonstrated superior
    performance and even surpasses GPT-3.5-turbo on Chatbot Arena Zheng et al. ([2023](#bib.bib46)).
    However, different from answering open-ended questions featured in Chatbot Arena,
    properly engaging with the complex environment demands the language agent to produce
    more precise actions that strictly conform to the task specification. There is
    still a performance gap between Mixtral and GPT-3.5-turbo in terms of predicting
    valid actions over intricate environments. Compared to GPT-3.5-turbo, Mixtral
    tends to output invalid actions more frequently. This also explains why decoupled
    generation, where the output space is strictly constrained to a list of valid
    actions, helps weaker models more. With Fuxi + decoupled generation, using Mistral
    can almost match the best baseline performance with GPT-3.5-turbo, and using Mixtral
    can even match the best baseline with GPT-4. While stronger models like GPT-4
    can effectively recover the mistake via error feedback, weaker models tend to
    benefit more from decoupled generation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Tools as A Middleware Layer
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deepen our understanding of the integral roles of tools in aiding LLMs in
    accessing complex environments (i.e., KB triples and database rows in our setup),
    we conduct further analysis by comparing Fuxi with prompting baselines with different
    amounts of data items directly sampled from the environment (Figure [4](#S5.F4
    "Figure 4 ‣ 5.1 Setup ‣ 5 Experiments ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments")). For the KB, we sample $10$ rows
    per table).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pioneering vision is for language agents to assist humans in tackling intricate
    real-world tasks. This paper demonstrates that with meticulously-crafted tools
    acting as middleware between LLMs and complex environments, LLMs can substantially
    exceed current solutions. Our results spotlight these specialized tools’ indispensable
    role in unlocking the potential of LLMs within complex real-world tasks previously
    posing immense challenges.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we aim to address the compelling question we posed: how effectively
    can LLMs handle complex environments with the aid of tools? We investigate this
    through evaluations in two exemplary environments: KBs and databases. While we
    achieve notable results in these environments, it is important to acknowledge
    that implementing customized tools for KBs and databases presents fewer challenges
    compared to environments without a straightforward query interface, such as a
    webpage or a physical environment. In future work, we plan to extend Fuxi across
    a broader range of environments, aiming to fully realize the potential of language
    agents in complex environments through the integration of customized middleware
    tools.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the tools developed in this study are soley grounded in our experience.
    Despite this, our results already demonstrate the significant potential of augmenting
    LLMs with customized tools in complex environments, aligning with the primary
    objective of this paper. Nonetheless, to enhance performance further, adopting
    a more principled strategy for tool design is essential.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank colleagues from THU KEG and the OSU NLP group for their
    thoughtful comments. In addition, we are grateful to the authors of Bird for kindly
    providing the original predictions of the baselines based on API Docs prompting.
    This research was supported by a sponsored award from Cisco Research.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Berant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
    2013. [Semantic parsing on Freebase from question-answer pairs](https://aclanthology.org/D13-1160).
    In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing*, pages 1533–1544, Seattle, Washington, USA. Association for Computational
    Linguistics.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bollacker et al. (2008) Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh,
    Tim Sturge, and Jamie Taylor. 2008. [Freebase: a collaboratively created graph
    database for structuring human knowledge](https://doi.org/10.1145/1376616.1376746).
    In *Proceedings of the ACM SIGMOD International Conference on Management of Data,
    SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008*, pages 1247–1250\. ACM.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2022) Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong
    Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang. 2022. [KQA Pro: A dataset
    with explicit compositional programs for complex question answering over knowledge
    base](https://doi.org/10.18653/v1/2022.acl-long.422). In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*, pages 6101–6119\. Association
    for Computational Linguistics.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chandu et al. (2021) Khyathi Raghavi Chandu, Yonatan Bisk, and Alan W. Black.
    2021. [Grounding ’grounding’ in NLP](https://doi.org/10.18653/V1/2021.FINDINGS-ACL.375).
    In *Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021,
    Online Event, August 1-6, 2021*, volume ACL/IJCNLP 2021 of *Findings of ACL*,
    pages 4283–4305\. Association for Computational Linguistics.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023. [Teaching large language models to self-debug](https://doi.org/10.48550/ARXIV.2304.05128).
    *CoRR*, abs/2304.05128.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: Towards a generalist agent for
    the web](https://doi.org/10.48550/ARXIV.2306.06070). *CoRR*, abs/2306.06070.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. 2023. [CRITIC: large language models can self-correct
    with tool-interactive critiquing](https://doi.org/10.48550/ARXIV.2305.11738).
    *CoRR*, abs/2305.11738.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2023) Yu Gu, Xiang Deng, and Yu Su. 2023. [Don’t generate, discriminate:
    A proposal for grounding language models to real-world environments](https://doi.org/10.18653/v1/2023.acl-long.270).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*,
    pages 4928–4949\. Association for Computational Linguistics.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021) Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang,
    Xifeng Yan, and Yu Su. 2021. [Beyond I.I.D.: three levels of generalization for
    question answering on knowledge bases](https://doi.org/10.1145/3442381.3449992).
    In *WWW ’21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April
    19-23, 2021*, pages 3477–3488\. ACM / IW3C2.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022) Yu Gu, Vardaan Pahuja, Gong Cheng, and Yu Su. 2022. [Knowledge
    base question answering: A semantic parsing perspective](https://doi.org/10.48550/arXiv.2209.04994).
    In *4th Conference on Automated Knowledge Base Construction*.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu and Su (2022) Yu Gu and Yu Su. 2022. [ArcaneQA: Dynamic program induction
    and contextualized encoding for knowledge base question answering](https://aclanthology.org/2022.coling-1.148).
    In *Proceedings of the 29th International Conference on Computational Linguistics*,
    pages 1718–1731, Gyeongju, Republic of Korea. International Committee on Computational
    Linguistics.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging pre-trained large language models to construct
    and utilize world models for model-based task planning](https://doi.org/10.48550/ARXIV.2305.14909).
    *CoRR*, abs/2305.14909.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2023) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023.
    [Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings](https://doi.org/10.48550/arXiv.2305.11554).
    *CoRR*, abs/2305.11554.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang et al. (2019) Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon
    Seo. 2019. [A comprehensive exploration on wikisql with table-aware word contextualization](http://arxiv.org/abs/1902.01069).
    *CoRR*, abs/1902.01069.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023a. [Mistral 7b](https://doi.org/10.48550/ARXIV.2310.06825).
    *CoRR*, abs/2310.06825.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    L’elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. [Mixtral of
    experts](http://arxiv.org/abs/2401.04088). *CoRR*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023b) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023b. [StructGPT: A general framework for large language
    model to reason over structured data](https://doi.org/10.48550/arXiv.2305.09645).
    *CoRR*, abs/2305.09645.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen
    Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao
    Ma, Guoliang Li, Kevin Chen-Chuan Chang, Fei Huang, Reynold Cheng, and Yongbin
    Li. 2023a. [Can LLM already serve as A database interface? A big bench for large-scale
    database grounded text-to-sqls](https://doi.org/10.48550/ARXIV.2305.03111). *CoRR*,
    abs/2305.03111.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li,
    Fei Huang, and Yongbin Li. 2023b. [API-Bank: A benchmark for tool-augmented llms](https://doi.org/10.48550/arXiv.2304.08244).
    *CoRR*, abs/2304.08244.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023c) Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu
    Chen. 2023c. [Few-shot in-context learning on knowledge base question answering](https://api.semanticscholar.org/CorpusID:258461017).
    In *Annual Meeting of the Association for Computational Linguistics*.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. [AgentBench: Evaluating llms
    as agents](https://doi.org/10.48550/arXiv.2308.03688). *CoRR*, abs/2308.03688.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. [Chameleon: Plug-and-play
    compositional reasoning with large language models](https://doi.org/10.48550/ARXIV.2304.09842).
    *CoRR*, abs/2304.09842.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos
    Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick,
    Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom.
    2023. [Augmented language models: a survey](https://doi.org/10.48550/ARXIV.2302.07842).
    *CoRR*, abs/2302.07842.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2023) Zhijie Nie, Richong Zhang, Zhongyuan Wang, and Xudong Liu.
    2023. [Code-style in-context learning for knowledge-based question answering](https://doi.org/10.48550/ARXIV.2309.04695).
    *CoRR*, abs/2309.04695.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023a) OpenAI. 2023a. [GPT-4 technical report](https://doi.org/10.48550/arXiv.2303.08774).
    *CoRR*, abs/2303.08774.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023b) OpenAI. 2023b. Models - OpenAI API. [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023a) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng
    Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,
    Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning
    Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,
    Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu,
    and Maosong Sun. 2023a. [Tool learning with foundation models](https://doi.org/10.48550/arXiv.2304.08354).
    *CoRR*, abs/2304.08354.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2023b) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian,
    Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    2023b. [ToolLLM: Facilitating large language models to master 16000+ real-world
    apis](https://doi.org/10.48550/ARXIV.2307.16789). *CoRR*, abs/2307.16789.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajkumar et al. (2022) Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau.
    2022. [Evaluating the text-to-sql capabilities of large language models](https://doi.org/10.48550/ARXIV.2204.00498).
    *CoRR*, abs/2204.00498.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-BERT:
    Sentence embeddings using Siamese BERT-networks](https://doi.org/10.18653/v1/D19-1410).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3982–3992, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. [Toolformer:
    Language models can teach themselves to use tools](https://doi.org/10.48550/arXiv.2302.04761).
    *CoRR*, abs/2302.04761.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew J. Hausknecht. 2021. [Alfworld: Aligning text
    and embodied environments for interactive learning](https://openreview.net/forum?id=0IOX0YcCdTn).
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler,
    Wei-Lun Chao, and Yu Su. 2023. LLM-Planner: Few-shot grounded planning for embodied
    agents with large language models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su (2023) Yu Su. 2023. [Language agents: a critical evolutionary step of artificial
    intelligence](https://yusu.substack.com/p/language-agents). *yusu.substack.com*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2016) Yu Su, Huan Sun, Brian M. Sadler, Mudhakar Srivatsa, Izzeddin
    Gur, Zenghui Yan, and Xifeng Yan. 2016. [On generating characteristic-rich question
    sets for QA evaluation](https://doi.org/10.18653/V1/D16-1054). In *Proceedings
    of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2016, Austin, Texas, USA, November 1-4, 2016*, pages 562–572\. The Association
    for Computational Linguistics.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong,
    Bin Chen, and Jian Su. 2023. [Battle of the large language models: Dolly vs LLaMA
    vs vicuna vs guanaco vs bard vs ChatGPT - a text-to-SQL parsing comparison](https://doi.org/10.18653/v1/2023.findings-emnlp.750).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    11225–11238, Singapore. Association for Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2023) Chang-Yu Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, and Huan
    Sun. 2023. [Exploring chain of thought style prompting for text-to-sql](https://aclanthology.org/2023.emnlp-main.327).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2023, Singapore, December 6-10, 2023*, pages 5376–5393\. Association
    for Computational Linguistics.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. [The web as
    a knowledge-base for answering complex questions](https://doi.org/10.18653/V1/N18-1059).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New
    Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*, pages 641–651\.
    Association for Computational Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://doi.org/10.48550/arXiv.2307.09288).
    *CoRR*, abs/2307.09288.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. [Chain-of-thought
    prompting elicits reasoning in large language models](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).
    In *NeurIPS*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. [ReAct: Synergizing reasoning and acting
    in language models](https://doi.org/10.48550/arXiv.2210.03629). *CoRR*, abs/2210.03629.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yih et al. (2016) Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang,
    and Jina Suh. 2016. [The value of semantic parse labeling for knowledge base question
    answering](https://doi.org/10.18653/v1/P16-2033). In *Proceedings of the 54th
    Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
    Papers)*, pages 201–206, Berlin, Germany. Association for Computational Linguistics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo
    Li, Jun Wang, Yiqun Hu, William Yang Wang, Zhiguo Wang, and Bing Xiang. 2023.
    [DecAF: Joint decoding of answers and logical forms for question answering over
    knowledge bases](https://openreview.net/pdf?id=XHc5zRPxqV9). In *The Eleventh
    International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
    May 1-5, 2023*. OpenReview.net.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R.
    Radev. 2018. [Spider: A large-scale human-labeled dataset for complex and cross-domain
    semantic parsing and text-to-sql task](https://doi.org/10.18653/v1/d18-1425).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 3911–3921\.
    Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J.
    Smola, and Le Song. 2018. [Variational reasoning for question answering with knowledge
    graph](https://doi.org/10.1609/AAAI.V32I1.12057). In *Proceedings of the Thirty-Second
    AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications
    of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
    Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
    2-7, 2018*, pages 6069–6076\. AAAI Press.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging LLM-as-a-judge with
    MT-Bench and Chatbot Arena](http://arxiv.org/abs/2306.05685).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    [Seq2SQL: Generating structured queries from natural language using reinforcement
    learning](http://arxiv.org/abs/1709.00103). *CoRR*, abs/1709.00103.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendices
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this supplementary material, we provide further details as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix A](#A1 "Appendix A Detailed Tool Definitions ‣ Middleware for LLMs:
    Tools Are Instrumental for Language Agents in Complex Environments"): Detailed
    Tool Definitions'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix B](#A2 "Appendix B Benchmark Statistics ‣ Middleware for LLMs: Tools
    Are Instrumental for Language Agents in Complex Environments"): Benchmark Statistics'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Appendix C](#A3 "Appendix C Prompts ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments"): Prompts'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix A Detailed Tool Definitions
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we detail the descriptions of our customized tools for both
    environments. Specifically, we implement $12$ different tools for KBs. The tool
    selection is carefully made based on our domain knowledge of these environments.
    Note that, for databases, we direct prompt the LLM with the DB schema information
    in API docs format Rajkumar et al. ([2022](#bib.bib29)), as a result, our tools
    focus on helping the LLM better engage with the database content.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们详细描述了我们为这两种环境定制的工具。具体来说，我们为知识库实现了$12$种不同的工具。工具的选择是基于我们对这些环境的领域知识进行的精心挑选。请注意，对于数据库，我们直接通过API文档格式的DB模式信息来提示LLM（Rajkumar等，[2022](#bib.bib29)），因此，我们的工具重点在于帮助LLM更好地处理数据库内容。
- en: A.1 Databases
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据库
- en: 'Navigational tools for databases:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库的导航工具：
- en: '<svg id="A1.SS1.p2.pic1" class="ltx_picture" height="103.89" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,103.89) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 7.87 77.06)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="239.92" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">find_columns_containing_value(value)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="63.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">This function can help to find columns that contain the given
    cell value, which can help you make better decisions in decoding the right column
    to use. Note that, the value here means cell value in the rows of the column,
    not the column name. Prerequisite: n/a</foreignobject></g></g></svg><svg id="A1.SS1.p3.pic1"
    class="ltx_picture" height="120.49" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,120.49) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87 93.66)"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject
    width="281.81" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">find_columns_containing_value_fuzzy(value)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="80.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Sometimes find_columns_containing_cell_value may not find a column
    with the exact matched cell value. This function can help to find columns that
    potentially contain the target cell value with fuzzy matching. Note that, the
    value here means cell value in the rows of the column, not the column name. Prerequisite:
    n/a</foreignobject></g></g></svg><svg id="A1.SS1.p4.pic1" class="ltx_picture"
    height="87.28" overflow="visible" version="1.1" width="600"><g transform="translate(0,87.28)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 7.87 60.45)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="215.01" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">get_distinct_values(table,
    column)</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="47.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Returns the distinct values
    in the given column. This may mainly help you make better decisions in decoding
    the right value to use. Prerequisite: n/a</foreignobject></g></g></svg><svg id="A1.SS1.p5.pic1"
    class="ltx_picture" height="87.28" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,87.28) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87 60.45)"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject
    width="260.29" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">is_value_in_column(table, column, value)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Returns whether the given value is in the given column. You can
    use this function to better detect the right column to use. Prerequisite: n/a</foreignobject></g></g></svg><svg
    id="A1.SS1.p6.pic1" class="ltx_picture" height="87.28" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,87.28) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    60.45)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="198.75" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">get_date_format(table, column)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Returns an example item of the given Date column. This may help
    you to better understand the date format in the column. Prerequisite: n/a</foreignobject></g></g></svg><svg
    id="A1.SS1.p7.pic1" class="ltx_picture" height="70.68" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,70.68) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    43.85)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="144.87" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">search_by_SQL(query)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="30.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Executing a SQL query to search the table. Prerequisite: n/a</foreignobject></g></g></svg>'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Functional tools for databases:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库的功能工具：
- en: '<svg id="A1.SS1.p9.pic1" class="ltx_picture" height="87.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,87.28) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 7.87 60.45)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="137.37" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">from(from_statement)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="47.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">This function specifies the FROM clause, e.g., from("FROM table1")
    or from("FROM table1 JOIN table2 ON table1.id = table2.id") Prerequisite: n/a</foreignobject></g></g></svg><svg
    id="A1.SS1.p10.pic1" class="ltx_picture" height="69.14" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,69.14) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    42.31)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="151.98" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">where(where_statement)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">This function specifies the WHERE clause, e.g., where("WHERE table1.id
    = 1"). Prerequisite: from</foreignobject></g></g></svg><svg id="A1.SS1.p11.pic1"
    class="ltx_picture" height="69.14" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,69.14) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87 42.31)"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject
    width="147.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">select(select_statement)</foreignobject></g></g></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="28.9"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">This function
    specifies the SELECT clause, e.g., select("SELECT table1.id"). Prerequisite: from,
    where</foreignobject></g></g></svg><svg id="A1.SS1.p12.pic1" class="ltx_picture"
    height="69.14" overflow="visible" version="1.1" width="600"><g transform="translate(0,69.14)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 7.87 42.31)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="200.79" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">group_by(group_by_statement)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="28.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">This function specifies the GROUP BY clause, e.g., group_by("GROUP
    BY table1.id"). Prerequisite: from, where, select</foreignobject></g></g></svg><svg
    id="A1.SS1.p13.pic1" class="ltx_picture" height="69.3" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,69.3) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    42.47)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="161.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">having(having_statement)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="29.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">This function specifies the HAVING clause, e.g., having("HAVING
    table1.id = 1"). Prerequisite: from, where, select, group_by</foreignobject></g></g></svg><svg
    id="A1.SS1.p14.pic1" class="ltx_picture" height="85.75" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,85.75) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    58.92)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="194.72" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">order_by(order_by_statement)</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="45.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">This function specifies an additional constraint like ordering.
    For example, order_by("ORDER BY table1.id DESC LIMIT 3"). Prerequisite: from,
    where, select</foreignobject></g></g></svg>'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Knowledge Bases
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Navigational tools for KBs:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.p2.pic1" class="ltx_picture" height="170.31" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,170.31) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 7.87 143.48)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="253.64" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">get_relations(variable)
    -> list of relations</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="130.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">A variable can be either an
    entity or a set of entities (i.e., the result of a previous query). This function
    helps to navigate all relations in the KB connected to the variable, so you can
    decide which relation is the most useful to find the answer to the question. A
    simple use case can be ‘get_relations(Barack Obama)’, which finds all relations/edges
    starting from the entity Barack Obama. The argument of get_relations should always
    be an entity or a variable (e.g., #0) and not anything else. Prerequisite: n/a</foreignobject></g></g></svg><svg
    id="A1.SS2.p3.pic1" class="ltx_picture" height="135.71" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,135.71) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    108.88)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="190.84" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">get_neighbors(v, r) -> variable</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="95.48" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Given a variable, this function returns all entities connected
    to the variable via the given relation. Note that, get_neighbors() can only be
    used after get_relations() is used to find a set of viable relations. A simple
    use case can be ‘get_neighbors(Barack Obama, people.person.profession)’, which
    returns the profession of Obama in Freebase. Prerequisite: get_relations</foreignobject></g></g></svg><svg
    id="A1.SS2.p4.pic1" class="ltx_picture" height="85.9" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,85.9) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    59.07)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="229" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">get_attributes(v) -> list of
    attributes</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="45.66" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">This function helps to find
    all numerical attributes of the variable. Please only use it if the question seeks
    for a superlative accumulation (i.e., argmax or argmin). Prerequisite: get_neighbors</foreignobject></g></g></svg>'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Functional tools for KBs:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库的功能工具：
- en: '<svg id="A1.SS2.p6.pic1" class="ltx_picture" height="119.11" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,119.11) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 7.87 92.28)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="150.36" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">argmax(v,
    a) -> variable</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="78.87" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Given a variable, this function
    returns the entity with the maximum value of the given attribute. It can only
    be used after get_attributes() is used to find a set of viable attributes. A simple
    use case can be ‘argmax(variable, age)’, which returns the oldest entity belonging
    to the variable. Prerequisite: get_attributes</foreignobject></g></g></svg><svg
    id="A1.SS2.p7.pic1" class="ltx_picture" height="119.11" overflow="visible" version="1.1"
    width="600"><g transform="translate(0,119.11) matrix(1 0 0 -1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87
    92.28)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 13.39 9.96)"><foreignobject width="147.67" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">argmin(v, a) -> variable</foreignobject></g></g></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject
    width="582.65" height="78.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Given a variable, this function returns the entity with the minimum
    value of the given attribute. It can only be used after get_attributes() is used
    to find a set of viable attributes. A simple use case can be ‘argmin(variable,
    age)’, which returns the youngest entity belonging to the variable. Prerequisite:
    get_attributes</foreignobject></g></g></svg><svg id="A1.SS2.p8.pic1" class="ltx_picture"
    height="85.9" overflow="visible" version="1.1" width="600"><g transform="translate(0,85.9)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 7.87 59.07)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject width="189.64" height="13.84"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">intersection(v1,
    v2) -> variable</foreignobject></g></g></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="45.66" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Given two variables, this function
    returns the intersection of the two variables. The two variables must be of the
    same type. Prerequisite: get_neighbors</foreignobject></g></g></svg><svg id="A1.SS2.p9.pic1"
    class="ltx_picture" height="69.3" overflow="visible" version="1.1" width="600"><g
    transform="translate(0,69.3) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 7.87 42.47)"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.39 9.96)"><foreignobject
    width="92.63" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">count(v) -> int</foreignobject></g></g></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 8.67 8.67)"><foreignobject width="582.65" height="29.06"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Given a
    variable, this function returns the number of entities belonging to the variable.
    Prerequisite: get_neighbors</foreignobject></g></g></svg>'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Benchmark Statistics
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Dataset | # Table/DB | # Row/DB | % Require Cont. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| WikiSQL Zhong et al. ([2017](#bib.bib47)) | $1$ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| Spider Yu et al. ([2018](#bib.bib44)) | $5.1$ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Bird Li et al. ([2023a](#bib.bib18)) | $7.3$ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: (a) Databases
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | # Relations/KB | # Triples/KB | # Hops | % Have Aggr. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| MetaQA Zhang et al. ([2018](#bib.bib45)) | $9$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| WebQSP Yih et al. ([2016](#bib.bib42)) | $19$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| GrailQA Gu et al. ([2021](#bib.bib9)) | $19$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| KBQA-Agent (Ours) | $19$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: (b) Knowledge Bases
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Table B.1: Our curated benchmarks more accurately mirror real-world complexity,
    offering a more effective assessment of language agents. Aggr. denotes aggregation
    functions.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [B.1](#A2.T1 "Table B.1 ‣ Appendix B Benchmark Statistics ‣ Middleware
    for LLMs: Tools Are Instrumental for Language Agents in Complex Environments"),
    we present the statistics of Bird and KBQA-Agent, which we have chosen for our
    evaluation. Relative to established benchmarks in text-to-SQL parsing and KBQA,
    Bird and KBQA-Agent exhibit significantly greater complexity, making them more
    suitable for assessing the capabilities of language agents.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompts
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ceb471cccc9187a46d022210653e0e76.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure C.1: Instructions for using database tools. Descriptions of tools are
    omitted.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions and demonstrations for using database tools are shown in Figure
    [C.1](#A3.F1 "Figure C.1 ‣ Appendix C Prompts ‣ Middleware for LLMs: Tools Are
    Instrumental for Language Agents in Complex Environments"). Note that, we also
    include the schema information of the database in API Docs in our prompt, which
    is not shown here. This design choice has been a common practice for text-to-SQL
    parsing with LLMs Tai et al. ([2023](#bib.bib37)); Sun et al. ([2023](#bib.bib36)).
    Instructions and demonstrations for using KB tools are shown in Figure [C.2](#A3.F2
    "Figure C.2 ‣ Appendix C Prompts ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments"). The instruction and demonstration
    for candidate selection in decoupled generation for KB is shown in Figure [C.3](#A3.F3
    "Figure C.3 ‣ Appendix C Prompts ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments"). Additionally, we also show an example
    of input we use for our KB experiments in Section [5.4](#S5.SS4 "5.4 Tools as
    A Middleware Layer ‣ 5 Experiments ‣ Middleware for LLMs: Tools Are Instrumental
    for Language Agents in Complex Environments"). For the input used for databases
    in Section [5.4](#S5.SS4 "5.4 Tools as A Middleware Layer ‣ 5 Experiments ‣ Middleware
    for LLMs: Tools Are Instrumental for Language Agents in Complex Environments"),
    we strictly follow the standard way of prompting with API docs plus exemplar rows Li
    et al. ([2023a](#bib.bib18)); Rajkumar et al. ([2022](#bib.bib29)).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/74b8115b1966a24947462fe41cb642db.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Figure C.2: Instructions and a one-shot demonstration for using KB tools. Descriptions
    of tools are omitted.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '图 C.2: 使用知识库工具的指令和一次性演示。工具的描述被省略。'
- en: '![Refer to caption](img/3d89150fed004aea751ca9621bc136e1.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3d89150fed004aea751ca9621bc136e1.png)'
- en: 'Figure C.3: Prompt for candidate action selection in decoupled generation for
    KB.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '图 C.3: 用于知识库的解耦生成中的候选动作选择的提示。'
- en: '![Refer to caption](img/5a0a1fa716f9e95f707c44eb0ab9135e.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5a0a1fa716f9e95f707c44eb0ab9135e.png)'
- en: 'Figure C.4: Input for question “which song is the longest song of handel: messiah
    (dublin version, 1742)?" with $10$ triples sampled from the KB, which is used
    in Section [5.4](#S5.SS4 "5.4 Tools as A Middleware Layer ‣ 5 Experiments ‣ Middleware
    for LLMs: Tools Are Instrumental for Language Agents in Complex Environments").'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '图 C.4: 问题“哪一首是亨德尔的最长歌曲：弥赛亚（都柏林版，1742）？”的输入，包含从知识库中抽取的 $10$ 个三元组，使用于第[5.4](#S5.SS4
    "5.4 Tools as A Middleware Layer ‣ 5 Experiments ‣ Middleware for LLMs: Tools
    Are Instrumental for Language Agents in Complex Environments")节。'
