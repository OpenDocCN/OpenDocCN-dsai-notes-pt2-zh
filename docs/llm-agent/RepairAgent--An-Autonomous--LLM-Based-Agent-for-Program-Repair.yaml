- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'RepairAgent: An Autonomous, LLM-Based Agent for Program Repair'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17134](https://ar5iv.labs.arxiv.org/html/2403.17134)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  prefs: []
  type: TYPE_NORMAL
- en: Islem Bouzenia
  prefs: []
  type: TYPE_NORMAL
- en: University of Stuttgart
  prefs: []
  type: TYPE_NORMAL
- en: Germany
  prefs: []
  type: TYPE_NORMAL
- en: fi_bouzenia@esi.dz    Premkumar Devanbu
  prefs: []
  type: TYPE_NORMAL
- en: UC Davis
  prefs: []
  type: TYPE_NORMAL
- en: USA
  prefs: []
  type: TYPE_NORMAL
- en: ptdevanbu@ucdavis.edu    Michael Pradel
  prefs: []
  type: TYPE_NORMAL
- en: University of Stuttgart
  prefs: []
  type: TYPE_NORMAL
- en: Germany
  prefs: []
  type: TYPE_NORMAL
- en: michael@binaervarianz.de
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Automated program repair has emerged as a powerful technique to mitigate the
    impact of software bugs on system reliability and user experience. This paper
    introduces RepairAgent, the first work to address the program repair challenge
    through an autonomous agent based on a large language model (LLM). Unlike existing
    deep learning-based approaches, which prompt a model with a fixed prompt or in
    a fixed feedback loop, our work treats the LLM as an agent capable of autonomously
    planning and executing actions to fix bugs by invoking suitable tools. RepairAgent
    freely interleaves gathering information about the bug, gathering repair ingredients,
    and validating fixes, while deciding which tools to invoke based on the gathered
    information and feedback from previous fix attempts. Key contributions that enable
    RepairAgent include a set of tools that are useful for program repair, a dynamically
    updated prompt format that allows the LLM to interact with these tools, and a
    finite state machine that guides the agent in invoking the tools. Our evaluation
    on the popular Defects4J dataset demonstrates RepairAgent’s effectiveness in autonomously
    repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting
    with the LLM imposes an average cost of 270,000 tokens per bug, which, under the
    current pricing of OpenAI’s GPT-3.5 model, translates to 14 cents per bug. To
    the best of our knowledge, this work is the first to present an autonomous, LLM-based
    agent for program repair, paving the way for future agent-based techniques in
    software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Software bugs lead to system failures, security vulnerabilities, and compromised
    user experience. Fixing bugs is a critical task in software development, but if
    done manually, demands considerable time and effort. Automated program repair
    (APR) promises to dramatically reduce this effort by addressing the critical need
    for effective and efficient bug resolution in an automated manner. Researchers
    and practitioners have explored various approaches to address the challenge of
    automatically fixing bugs [[1](#bib.bib1)], including techniques based on manually
    designed [[2](#bib.bib2), [3](#bib.bib3)] and (semi-)automatically extracted [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)] fix patterns, based on symbolic constraints [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)], and various machine learning-based approaches [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: The current state-of-the-art in APR predominantly revolves around large language
    models (LLMs). The first generation of LLM-based repair techniques involve a one-time
    interaction with the model, where the model receives a prompt containing the buggy
    code and produces a fixed version [[17](#bib.bib17), [18](#bib.bib18)]. The second
    and current generation of LLM-based repair techniques introduces iterative approaches,
    which query the LLM repeatedly based on feedback obtained from previous fix attempts [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: A key limitation of current iterative, LLM-based repair techniques is that their
    hard-coded feedback loops do not allow the model to gather information about the
    bug or existing code that may provide ingredients to fix the bug. Instead, these
    approaches fix the context information provided in the prompt, typically to the
    buggy code [[19](#bib.bib19), [21](#bib.bib21)], and sometimes also details about
    the test cases that fail [[20](#bib.bib20)]. The feedback loop then executes the
    tests on different variants of the buggy code and adds any compilation errors,
    test failures, or other output, to the prompt of the next iteration. However,
    this approach fundamentally differs from the way human developers fix bugs, which
    typically involves a temporal interleaving of gathering information to understand
    the bug, searching code that may be helpful for fixing the bug, and experimenting
    with candidate fixes [[22](#bib.bib22), [23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: This paper presents RepairAgent, the first autonomous, LLM-based agent for automated
    program repair. Our approach treats the LLM as an autonomous agent capable of
    planning and executing actions to achieve the goal of fixing a bug. To this end,
    we equip the LLM with a set of bug repair-specific tools that the models can invoke
    to interact with the code base in a way similar to a human developer. For example,
    RepairAgent has tools to extract information about the bug by reading specific
    lines of code, to gather repair ingredients by searching the code base, and to
    propose and validate fixes by applying a patch and executing test cases. Importantly,
    we do not hard-code how and when to use these tools, but instead let the LLM autonomously
    decide which tool to invoke next, based on previously gathered information and
    feedback from previous fix attempts.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach is enabled by three key components. First, a general-purpose LLM,
    such as GPT-3.5, which we query repeatedly with a dynamically updated prompt.
    We contribute a novel prompt format that guides the LLM through the bug repair
    process, and that gets updated based on the commands invoked by the LLM and the
    results of the previous command executions. Second, a set of tools that the LLM
    can invoke to interact with the code base. We present a set of 14 tools designed
    to cover different steps a human developer would take when fixing a bug, such
    as reading specific lines of code, searching the code base, and applying a patch.
    Third, a middleware that orchestrates the communication between the LLM and the
    tools. We present novel techniques for guiding tool invocations through a finite
    state machine and for heuristically interpreting possibly incorrect LLM outputs.
    The iterative loop of RepairAgent continues until the agent declares to have found
    a suitable fix, or until exhausting a budget of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the effectiveness of our approach, we apply it to all 835 bugs in
    the Defects4J [[24](#bib.bib24)] dataset, a widely used benchmark for evaluating
    program repair techniques. RepairAgent successfully fixes 164 bugs, including
    74 and 90 bugs of Defects4J v1.2 and v2.0, respectively. The correctly fixed bugs
    include 49 bugs that require fixing more than one line, showing that RepairAgent
    is capable of fixing complex bugs. Compared to state-of-the-art techniques [[19](#bib.bib19),
    [21](#bib.bib21)], RepairAgent successfully fixes 39 bugs not fixed by prior work.
    Measuring the costs imposed by interacting with the LLM, we find that RepairAgent
    imposes an average cost of 270,000 tokens per bug, which, under the current pricing
    of OpenAI’s GPT-3.5 model, translates to 14 cents per bug. Overall, our results
    show that our agent-based approach establishes a new state of the art in program
    repair.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, this paper contributes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An autonomous, LLM-based agent for program repair.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dynamically updated prompt format that guides the LLM through the bug repair
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of tools that enable a LLM to to perform steps a human developer would
    take when fixing a bug.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A middleware that orchestrates the communication between the LLM and the tools.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Empirical evidence that RepairAgent establishes a new state of the art by successfully
    fixing 164 bugs, including 39 bugs not fixed by prior work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will release the implementation of RepairAgent as open-source to foster future
    work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To the best of our knowledge, there currently is no published work on an autonomous,
    LLM-based agent for any code-generation task. We envision RepairAgent to pave
    the way for future agent-based techniques in software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: II Background on LLM-Based, Autonomous Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By virtue of being trained on vast amounts of web knowledge, including natural
    language and source code, LLMs have demonstrated remarkable abilities in achieving
    human-level performance for various tasks [[25](#bib.bib25)]. A promising way
    of using these abilities are LLM-based agents that autonomously plan and execute
    actions to achieve a goal. The basic idea is to query the LLM with a prompt that
    contains the current state of the world, the goal to be achieved, and a set of
    actions that could be performed next. The model than decides which action to perform,
    and the feedback from performing the action is integrated into the next prompt.
    One way to represent “actions” is through tools that the model can invoke to interact
    with the world [[26](#bib.bib26), [27](#bib.bib27)]. Recent surveys provide a
    comprehensive overview of LLM-based, autonomous agents [[28](#bib.bib28)] and
    of LLM agents equipped with tools invoked via APIs [[29](#bib.bib29)]. The potential
    of such agents for software engineering currently is not well explored, which
    this paper aims to address for the challenging task of automated program repair.
  prefs: []
  type: TYPE_NORMAL
- en: III Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01470337f6802b2b9bfba00691c49dd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of RepairAgent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ III-A Overview ‣ III Approach ‣ RepairAgent:
    An Autonomous, LLM-Based Agent for Program Repair") gives an overview of the RepairAgent
    approach, which consists of three components: an LLM agent, a set of tools, and
    a middleware that orchestrates the communication between the two. Given a bug
    to fix, the middleware initializes the LLM agent with a prompt that contains task
    information and instructions on how to perform it by using the provided tools.
    The LLM responds by suggesting a call to one of the available tools, which the
    middleware parses and then executes. The output of the tool is then integrated
    into the prompt for the next invocation of the LLM, and the process continues
    iteratively until the bug is fixed or a predefined budget is exhausted.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Terminology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RepairAgent proceeds in multiple iterations, which we call cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1  (Cycle).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A *cycle* represents one round of interaction with the LLM agent, which consists
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the agent
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Post-process the response
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the command suggested by the agent
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the dynamic prompt based on the command’s output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In each cycle, the approach queries the LLM once. The input to the model is
    updated based on commands (calls to tools) invoked by the LLM, and their results,
    in previous cycles. We call the model input a dynamic prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2  (Dynamic prompt).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The *dynamic prompt* is a sequence of text sections $P=[s_{0},s_{1},...,s_{n}]$):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *static section*, which remains the same across all cycles, i.e., $s_{i}(c)=s_{i}(c^{\prime})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *dynamic section*, which may differ across cycles, i.e., there may exist $c,c^{\prime}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III-C Dynamic Prompting of the Repair Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE I: Sections of the dynamically updated prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '|         Prompt section |         Nature |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|         Role |         Static |'
  prefs: []
  type: TYPE_TB
- en: '|         Goals |         Static |'
  prefs: []
  type: TYPE_TB
- en: '|         Guidelines |         Static |'
  prefs: []
  type: TYPE_TB
- en: '|         State description |         Dynamic |'
  prefs: []
  type: TYPE_TB
- en: '|         Available tools |         Dynamic |'
  prefs: []
  type: TYPE_TB
- en: '|         Gathered information |         Dynamic |'
  prefs: []
  type: TYPE_TB
- en: '|         Specification of output format |         Static |'
  prefs: []
  type: TYPE_TB
- en: '|         Last executed command and result |         Dynamic |'
  prefs: []
  type: TYPE_TB
- en: 'The repair agent is an LLM trained on natural language and source code, such
    as GPT-3.5. RepairAgent queries the LLM with a dynamic prompt that consists of
    a sequence of static and dynamic sections, as listed in Table [I](#S3.T1 "TABLE
    I ‣ III-C Dynamic Prompting of the Repair Agent ‣ III Approach ‣ RepairAgent:
    An Autonomous, LLM-Based Agent for Program Repair"). We describe each section
    in detail in the following.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Role
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section of the prompt defines the agent’s area of expertise, which is
    to resolve bugs in Java code, and outlines the agent’s primary objective: understanding
    and fixing bugs. The prompt emphasizes that the agent’s decision-making process
    is autonomous and should not rely on user assistance.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Goals
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We define five goals for the agent to pursue, which remain the same across
    all cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Locate the bug:* Execute tests and use fault localization techniques to pinpoint
    the bug’s location. Skip this goal when fault localization information is already
    provided in the prompt.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gather information about the bug:* Analyze the lines of code associated with
    the bug to understand the bug.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Suggest simple fixes to the bug:* Start by suggesting simple fixes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Suggest complex fixes:* If simple fixes prove ineffective, explore and propose
    more complex ones.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Iterate over the previous goals:* Continue to gather information and to suggest
    fixes until finding a fix.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III-C3 Guidelines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We provide a set of guidelines. First, we inform the model that there are diverse
    kinds of bugs, ranging from single-line issues to multi-line bugs that may entail
    changing, removing, or adding lines. Based on the observation that many bugs can
    be fixed by relatively simple, recurring fix patterns [[30](#bib.bib30)], we provide
    a list of recurring fix patterns. The list is based on the patterns described
    in prior work on single-statement bugs in Java [[30](#bib.bib30)]. For each pattern,
    we provide a short natural language description and an example of buggy and fixed
    code. Second, we instruct the model to insert comments above the modified code,
    which serves two purposes. On the one hand, the comments allow the model to explain
    its reasoning, which has been shown to enhance the reasoning abilities of LLMs [[31](#bib.bib31)].
    On the other hand, commenting will ultimately help human developers in understanding
    the nature of the edits. Third, we instruct the model to conclude its reasoning
    with a clearly defined next step that can be translated into a call to a tool.
    Finally, we describe that there is a limited budget of tool invocations, highlighting
    the importance of efficiency in selecting the next steps. Specifically, we specify
    a maximum number of cycles (40 by default).
  prefs: []
  type: TYPE_NORMAL
- en: III-C4 State Description
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8fa1899969d17c1a1712c19f478de94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: State machine to guide selection of tools.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To guide the LLM agent toward using the available tools in an effective and
    meaningful way, we define a finite state machine that constrains which tools are
    available at a given point in time. The motivation is that we observed the LLM
    agent to frequently get lost in aimless exploration in earlier experiments without
    such guidance. Figure [2](#S3.F2 "Figure 2 ‣ III-C4 State Description ‣ III-C
    Dynamic Prompting of the Repair Agent ‣ III Approach ‣ RepairAgent: An Autonomous,
    LLM-Based Agent for Program Repair") shows the finite state machine, which we
    design to mimic the states a human developer would go through when fixing a bug.
    Each state is associated with a set of tools available to the agent, which are
    described in Section [III-D](#S3.SS4 "III-D Tools for the Agent to Use ‣ III Approach
    ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair"). Importantly,
    the agent is free to transition between states at any point in time by using tools.
    That is, despite providing guidance, the state machine does not enforce a strict
    order of tool invocations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The state description section of the prompt informs the agent about its current
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand the bug*: The agent starts in this state, where it can collect
    information related to the failing test cases and the bug’s location. Once the
    agent has an understanding of the bug, it formulates a hypothesis to describe
    the nature of the bug and the reason behind it. Throughout the repair process,
    the agent may refute earlier hypotheses and express new ones. After expressing
    a hypothesis, the agent will automatically switch to the next state.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Collect information to fix the bug*: In this state the agent collects information
    that help suggest a fix for the bug expressed by the hypothesis, e.g., by searching
    for specific repair ingredients or by reading possibly relevant code. Once the
    agent has gathered enough information to attempt a fix, it can transition to the
    next state.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Try to fix the bug*: In this state, the agent tries to fix the bug based on
    its current hypothesis and the collected information. Each fix attempt modifies
    the code base and is validated by executing the test cases. If necessarily, the
    agent can go back to one of the previous states to establish a new hypothesis
    or to gather additional information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to the three above states, RepairAgent has a final state, *“Done”*,
    which the agent can reach by calling a specific command that indicates the success
    of repair.
  prefs: []
  type: TYPE_NORMAL
- en: III-C5 Available Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section of the prompt describes a set of tools that the agent can call
    at the current state. Each tool has a name, a description, and a set of typed
    arguments (Section [III-D](#S3.SS4 "III-D Tools for the Agent to Use ‣ III Approach
    ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair")).'
  prefs: []
  type: TYPE_NORMAL
- en: III-C6 Gathered Information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A key ability of the repair agent is to gather information about the bug and
    the code base, which serves as the basis for deciding which commands to invoke
    next. To make this information available to the agent, we maintain a prompt section
    that lists the information gathered by the different tool invocations. Intuitively,
    this section of the prompt serves as a memory for the agent, allowing it to recall
    information from previous cycles. The gathered information is structured into
    different subsections, where each subsection contains the outputs produced by
    a specific tool.
  prefs: []
  type: TYPE_NORMAL
- en: III-C7 Specification of Output Format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'interface  Response  {thoughts:  string;command:  {name:  string;args:  Record<string,  any>;};}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: JSON format of the response of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the dynamic prompt, the LLM agent provides one response per cycle. To
    enable the middleware to parse the response, we specify the expected output format
    (Figure [3](#S3.F3 "Figure 3 ‣ III-C7 Specification of Output Format ‣ III-C Dynamic
    Prompting of the Repair Agent ‣ III Approach ‣ RepairAgent: An Autonomous, LLM-Based
    Agent for Program Repair")). The “thoughts” field provides a textual description
    of the agent’s reasoning when deciding about the next command. Asking the agent
    to express its thoughts increases the transparency and interpretability of the
    approach, provides a way to debug potential issues in the agent’s decision-making
    process, and helps improve the reasoning abilities of LLMs [[31](#bib.bib31)].
    The “command” field specifies the next command to be executed, consisting of the
    name of the tool to invoke and the set of arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: '{"thoughts":  "With  the  hypothesis  in  mind,it’s  time  to  gather  more  information  toformulate  potential  fixes  for  the  bug.  Ishould  focus  on  understanding  the  contextaround  the  condition’if  (x  <  0  &&  prev  ==  ’-’)’to  come  up  with  effective  solutions.","command":  {"name":  "search_code_base","args":  {"key_words":[  "addNumber","CodeConsumer","if  (x  <  0  &&  prev  ==  ’-’)"  ]}}}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Example of a response of the repair agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, Figure [4](#S3.F4 "Figure 4 ‣ III-C7 Specification of Output Format
    ‣ III-C Dynamic Prompting of the Repair Agent ‣ III Approach ‣ RepairAgent: An
    Autonomous, LLM-Based Agent for Program Repair") shows a response of the LLM agent.
    The model expresses the need to collect more information to understand the bug.
    It then suggests a command that searches the code base with a list of keywords.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C8 Last Executed Command and Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section of the prompt contains the last command (tool name and arguments)
    that was executed (if any) and the output it produced. The rationale is to remind
    the agent of the last step it took, and to make it aware of any problems that
    occurred during the execution of the command. Furthermore, we remind the agent
    how many cycles have already been executed, and how many cycles are left
  prefs: []
  type: TYPE_NORMAL
- en: III-D Tools for the Agent to Use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A key novelty in our approach is to let an LLM agent autonomously decide which
    tools to invoke to fix a bug. The tools we provide to the agent (Table [II](#S3.T2
    "TABLE II ‣ III-D Tools for the Agent to Use ‣ III Approach ‣ RepairAgent: An
    Autonomous, LLM-Based Agent for Program Repair")) are inspired by the tools that
    developers use in their IDEs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Repair-related tools invoked by RepairAgent.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tool | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Read and extract code: |'
  prefs: []
  type: TYPE_TB
- en: '| *read_range* | Read a range of lines in a file. |'
  prefs: []
  type: TYPE_TB
- en: '| *get_classes_and_methods* | Get the names of all classes and methods in a
    file. |'
  prefs: []
  type: TYPE_TB
- en: '| *extract_method* | Given a method name, extract method implementations from
    a file. |'
  prefs: []
  type: TYPE_TB
- en: '| *extract_tests* | Given the failure report from JUnit or ANT, extract the
    code of failing test cases. |'
  prefs: []
  type: TYPE_TB
- en: '| Search and generate code: |'
  prefs: []
  type: TYPE_TB
- en: '| *search_code_base* | Scans all Java files within a project for a list of
    keywords. |'
  prefs: []
  type: TYPE_TB
- en: '| *find_similar_api _calls* | Given a code snippet that calls an API, search
    for similar API calls in the project. |'
  prefs: []
  type: TYPE_TB
- en: '| *generate_method_body* | Ask an LLM (GPT3.5 by default) to generate the body
    of a method based on code proceeding the method. |'
  prefs: []
  type: TYPE_TB
- en: '| Testing and patching: |'
  prefs: []
  type: TYPE_TB
- en: '| *run_tests* | Run the test suite of a project. |'
  prefs: []
  type: TYPE_TB
- en: '| *run_fault_localization* | Retrieve pre-existing localization information
    or run a fault localization tool. |'
  prefs: []
  type: TYPE_TB
- en: '| *write_fix* | Apply a patch to the code base and execute the test suite of
    the project. Changes are reverted automatically if tests fail. Moves the agent
    into the ’Try to fix the bug’ state. |'
  prefs: []
  type: TYPE_TB
- en: '| Control: |'
  prefs: []
  type: TYPE_TB
- en: '| *express_hypothesis* | Express a hypothesis about the bug. Moves the agent
    into the ’Collect information to fix the bug’ state. |'
  prefs: []
  type: TYPE_TB
- en: '| *collect_more_information* | Move the agent back to the ’Collect information
    to fix the bug’ state. |'
  prefs: []
  type: TYPE_TB
- en: '| *discard_hypothesis* | Discard the current hypothesis about the bug and move
    back to the ’Understand the bug’ state. |'
  prefs: []
  type: TYPE_TB
- en: '| *goal_accomplished* | Declare that the goal has been accomplished and exiting
    the repair process. |'
  prefs: []
  type: TYPE_TB
- en: III-D1 Reading and Extracting Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A prerequisite for fixing a bug is to read and understand relevant parts of
    the code base. Instead of hard-coding the context provided to the LLM [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21)], we let the agent decide which parts of the
    code to read, based on four tools. The *read_range* tool allows the agent to extract
    a range of lines from a specific file, which is useful to obtain a focused view
    of a particular section of code. To obtain an overview of the code structure,
    the *get_classes_and_methods* tool retrieves all class and method names within
    a given file. By invoking the *extract_method* tool, the agent can retrieve the
    implementation(s) of methods that match a given method name within a given file.
    Finally, we offer the *extract_tests* tool, which extracts the code of test cases
    that resulted in failure. The tool is crucial to understand details of failing
    tests, such as input values and the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: III-D2 Search and generate code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Motivated by the fact that human developers commonly search for code [[32](#bib.bib32)],
    we present tools that allow the agent to search for specific code snippets. These
    tools are useful for the agent to better understand the context of a bug and to
    gather repair ingredients, i.e., code fragments that could become part of a fix.
    The *search_code_base* tool enables the agent to locate instances of particular
    keywords within the entire code base. For example, the agent can use this tool
    to find occurrences of variables, methods, and classes. Given a set of keywords,
    the tool performs an approximate matching against all source code files in the
    project. Specifically, the tool splits each keyword into subtokens based on camel
    case, underscores, and periods, and then searches for each subtoken in the code.
    For example, searching for quickSortArray yields matches for sortArray, quickSort,
    arrayQuickSort, and other related variations. The output of the tool is a nested
    dictionary, organized by file names, classes, and method names, that provides
    the keywords that match a method’s content. Another search tool, *find_similar_api_calls*,
    allows the agent to identify and extract usages of a method, which is useful to
    fix incorrect method calls. Without such a tool, LLMs tend to hallucinate method
    calls that do not exist in the code base [[33](#bib.bib33)]. Given a code snippet
    that contains a method call, the tool extracts the name of the called method,
    and then searches for calls to methods with the same name. The agent can restrict
    the search to a specific file or search the entire code base.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to searching for existing code, RepairAgent offers a tool that generates
    new code by invoking another LLM. The tool is inspired by the success of LLM-based
    code completion tools, such as Copilot [[34](#bib.bib34)], which human developers
    increasingly use when fixing bugs. Given the code preceding a method and the signature
    of the method, the *generate_method_body* tool asks an LLM to generate the body
    of the method. The query to the code-generating LLM is independent of the dynamic
    prompt used by the overall RepairAgent approach, and may use a different model.
    In our evaluation, we use the same LLM for both the repair agent and as the code-generating
    LLM of this tool. The tool limits the given code context to 12k tokens and sets
    a limit of 4k tokens for the generated code.
  prefs: []
  type: TYPE_NORMAL
- en: III-D3 Testing and Patching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The next category of tools is related to running tests and applying patches.
    The *run_tests* tool allows the agent to execute the test suite of the project.
    It produces a report that indicates whether the tests passed or failed. In case
    of test failures, the tool cleans the output of the test runner, e.g., by removing
    entries of the stack trace that are outside of the current project. The rationale
    is that LLMs have a limited prompt size and that irrelevant information may confuse
    the model. The *run_fault_localization* tool retrieves fault localization information,
    which is useful to understand which parts of the code are likely to contain the
    bug. RepairAgent offers two variants of this tool: Either, it provides perfect
    fault localization information, i.e., the file(s) and line(s) that need to be
    edited to fix the bug, or it invokes an existing fault localization tool, such
    as GZoltar [[35](#bib.bib35)], to calculate fault localization scores. As common
    in the field of program repair, we assume perfect fault localization as the default.'
  prefs: []
  type: TYPE_NORMAL
- en: '[{"file_path":  "jfree/data/time/Week.java","insertions":  [{"line_number":  175,"new_lines":  ["//  ...new  lines  to  insert...\n","//  ...more  new  lines...\n"]}],"deletions":  [179,  183],"modifications":  [{"line_number":  179,"modified_line":  "  if  (dataset  ==  null)  {\n"}]},{"file_path":  "org/jfree/data/time/Day.java","insertions":  [],"deletions":  [307],"modifications":  []}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Example of patch given to the *write_fix* tool.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the agent has gathered sufficient information to fix the bug, it can apply
    a patch to the code base using the *write_fix* tool. RepairAgent aims at repairing
    arbitrarily complex bugs, including multi-line and even multi-file bugs. The *write_fix*
    tool expects a patch in a specific JSON format, which indicates the insertions,
    deletions, and modifications to be made in each file. Figure [5](#S3.F5 "Figure
    5 ‣ III-D3 Testing and Patching ‣ III-D Tools for the Agent to Use ‣ III Approach
    ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair") shows an example
    of a patch in this format. Given a patch, the tool applies the changes to the
    code base and runs the test suite. If the tests fail, the *write_fix* reverts
    the changes, giving the agent a clean code base to try another fix. Motivated
    by the observation that some fix attempts are almost correct, the *write_fix*
    tool requests the LLM to sample multiple variants of the suggested fix. By default,
    RepairAgent samples 30 variants at max. Given the generated variants, the approach
    removes duplicates and launch tests for every variant.'
  prefs: []
  type: TYPE_NORMAL
- en: III-D4 Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The final set of tools do not directly correspond to a tool a human developer
    may use, but rather allow the agent to move between states (Figure [2](#S3.F2
    "Figure 2 ‣ III-C4 State Description ‣ III-C Dynamic Prompting of the Repair Agent
    ‣ III Approach ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair")).
    The *express_hypothesis* tool empowers the agent to articulate a hypothesis regarding
    the nature of the bug and to transition to the ’Collect information to fix the
    bug’ state. Inversely, the *discard_hypothesis* tool allows the agent to discard
    a hypothesis that is no longer viable, which leads back to the ’Understand the
    bug’ state. Together, the two commands enforce a structured approach to hypothesis
    formulation, aligning with work on scientific debugging [[36](#bib.bib36), [20](#bib.bib20)].
    In case the agent has tried multiple fixes without success, the *collect_more_information*
    tool allows the agent to revert to the ’Collect information to fix the bug’ state.
    Finally, once the agent has found at least one fix that passes all tests, it can
    invoke the *goal_accomplished* tool, which terminates RepairAgent.'
  prefs: []
  type: TYPE_NORMAL
- en: III-E Middleware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The middleware component plays a crucial role in RepairAgent, orchestrating
    the communication between the LLM agent and the tools. It performs the steps in
    Definition [1](#Thmdefinition1 "Definition 1 (Cycle). ‣ III-B Terminology ‣ III
    Approach ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair") as
    described in the following.'
  prefs: []
  type: TYPE_NORMAL
- en: III-E1 Parsing and Refining LLM Output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At the beginning of each cycle, the middleware queries the LLM with the current
    prompt. Ideally, the response adheres perfectly to the expected format (Figure [3](#S3.F3
    "Figure 3 ‣ III-C7 Specification of Output Format ‣ III-C Dynamic Prompting of
    the Repair Agent ‣ III Approach ‣ RepairAgent: An Autonomous, LLM-Based Agent
    for Program Repair")). In practice, however, the LLM may produce responses that
    deviate from the expected format, e.g., due to hallucinations or syntactic errors.
    For example, the LLM may provide a “path” argument while the tool expects a “file_path”
    argument.'
  prefs: []
  type: TYPE_NORMAL
- en: RepairAgent tries to heuristically rectify such issues by mapping the output
    to the expected format in three steps. First, it tries to map the tool mentioned
    in the response to one of the available tools. Specifically, the approach checks
    if the predicted tool name $n_{\mathit{predicted}}$ is below a threshold (0.1
    by default). Second, the approach tries to map the argument names provided in
    the response to the expected arguments of the tool, following the same logic as
    above. Third, the approach handles invalid argument values by heuristically mapping
    or replacing them, e.g., by replacing a predicted file path with a valid one.
    If the heuristic mapping fails or produces multiple possible tool invocations,
    the middleware informs the LLM about the issue via the “Last executed command
    and result” prompt section and enters a new cycle.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to rectifying minor mistakes in the response, the middleware also
    checks for repeated invocations of the same tool with the same arguments. If the
    agent suggests the exact same command as in a previous cycle, which would yield
    the same results, the middleware informs the agent about the repetition and enters
    a new cycle.
  prefs: []
  type: TYPE_NORMAL
- en: III-E2 Calling the Tool
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the middleware has received a valid command from the LLM, it calls the
    corresponding tool. To prevent tool executions to interfere with the host environment
    or RepairAgent itself, the middleware executes the command in an isolated environment
    that contains a cloned version of the buggy repository.
  prefs: []
  type: TYPE_NORMAL
- en: III-E3 Updating the Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the output of the tool, the middleware updates all dynamic sections of
    the prompt for the next cycle. In particular, the middleware updates the state
    description and the available tools, appends the tool’s output to the gathered
    information, and replaces the section that shows the last executed command.
  prefs: []
  type: TYPE_NORMAL
- en: IV Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use Python 3.10 as our primary programming language. Docker is used to containerize
    and isolate command executions for enhanced reliability and reproducibility. RepairAgent
    is built on top of the AutoGPT framework and GPT-3.5-0125 from OpenAI. To parse
    and interact with Java code, we use ANTLR.
  prefs: []
  type: TYPE_NORMAL
- en: V Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate our approach we aim to answer the following research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: RQ1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How effective is RepairAgent at fixing real-world bugs?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the costs of the approach?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the LLM agent use the available tools?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: V-A Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We apply RepairAgent to bugs in the Defects4J dataset [[24](#bib.bib24)]. We
    use the entire Defects4J dataset, which consists of 835 real-world bugs from 17
    Java projects, including 395 bugs from 6 projects in Defects4Jv1.2, as well as
    another 440 bugs and 11 projects added in Defects4Jv2. Evaluating on the entire
    dataset allows us to assess the generalization capabilities of RepairAgent to
    different projects and bugs, without restricting the evaluation, e.g., based on
    the number of lines, hunks, or files that need to be fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We compare with three existing repair techniques: ChatRepair [[19](#bib.bib19)],
    ITER [[21](#bib.bib21)], and SelfAPR [[37](#bib.bib37)]. ChatRepair and ITER are
    two very recent approaches and have been shown to be the current state of the
    art. All three baseline approaches follow an iterative approach that incorporates
    feedback from previous patch attempts. Unlike RepairAgent, the baselines do not
    use an autonomous, LLM-based agent. We compare against the baselines based on
    patches provided by the authors of the respective approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics of success
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similar to past work, we report both the number of plausible and correct patches.
    A fix is *plausible* if it passes all test cases, but is not necessarily correct.
    To determine whether a fix is correct, we automatically check whether it syntactically
    matches the developer-created fix. If this is not the case, we manually determine
    whether the RepairAgent-generated fix is semantically consistent with the developer-created
    fix. If and only if either of the two checks succeeds, we consider the fix to
    be *correct*.
  prefs: []
  type: TYPE_NORMAL
- en: 'V-B RQ1: Effectiveness'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-B1 Overall Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE III: Results on Defects4J.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Project | Bugs | Plausible | Correct | ChatRepair | ITER | SelfAPR |'
  prefs: []
  type: TYPE_TB
- en: '| Chart | 26 | 14 | 11 | 15 | 10 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Cli | 39 | 9 | 8 | 5 | 6 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Closure | 174 | 27 | 27 | 37 | 18 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Codec | 18 | 10 | 9 | 8 | 3 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Collections | 4 | 1 | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Compress | 47 | 10 | 10 | 2 | 4 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Csv | 16 | 6 | 6 | 3 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Gson | 18 | 3 | 3 | 3 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| JacksonCore | 26 | 5 | 5 | 3 | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Jacksondatabind | 112 | 18 | 11 | 9 | 0 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| JacksonXml | 6 | 1 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jsoup | 93 | 18 | 18 | 14 | 0 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| JxPath | 22 | 0 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Lang | 63 | 17 | 17 | 21 | 0 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Math | 106 | 29 | 29 | 32 | 0 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| Mockito | 38 | 6 | 6 | 6 | 0 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Time | 26 | 3 | 2 | 3 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Defects4Jv1.2 | 395 | 96 | 74 | 114 | 57 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| Defects4Jv2 | 440 | 90 | 90 | 48 | — | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 835 | 186 | 164 | 162 | 57 | 110 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Distribution of fixes by location type'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bug type | RepairAgent | ChatRepair | ITER | SelfAPR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Single-line | 115 | 133 | 36 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-line* | 46 | 29 | 14 | 24 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-file | 3 | 0 | 4 | 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table [III](#S5.T3 "TABLE III ‣ V-B1 Overall Results ‣ V-B RQ1: Effectiveness
    ‣ V Evaluation ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair")
    summarizes the effectiveness of RepairAgent in fixing the 835 bugs in Defects4J.
    The approach generates plausible fixes for 186 bugs. While not necessarily correct,
    plausible fixes pass all test cases and may still provide developers a hint about
    what should be changed. RepairAgent generates correct fixes for 164 bugs, which
    are semantically consistent with the developer-provided patches. Being able to
    fix bugs from different projects shows that the approach can generalize to code
    bases of multiple domains. Furthermore, RepairAgent creates fixes for bugs of
    different levels of complexity. Specifically, as shown in Table [IV](#S5.T4 "TABLE
    IV ‣ V-B1 Overall Results ‣ V-B RQ1: Effectiveness ‣ V Evaluation ‣ RepairAgent:
    An Autonomous, LLM-Based Agent for Program Repair"), the approach fixes 115 single-line
    bugs, 46 multi-line (single-file) bugs, and 3 multi-file bugs.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 Comparison with Prior Work
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0001e517270f48dafb6713b7842331f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Intersection of the set fixes with related work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of Table [III](#S5.T3 "TABLE III ‣ V-B1 Overall Results
    ‣ V-B RQ1: Effectiveness ‣ V Evaluation ‣ RepairAgent: An Autonomous, LLM-Based
    Agent for Program Repair") compares RepairAgent with the baseline approaches ChatRepair,
    ITER, and SelfAPR. Previous to this work, ChatRepair had established a new state
    of the art in APR by fixing 162 bugs in Defects4J. RepairAgent achieves a comparable
    record by fixing a total of 164 bugs. Our work particularly excels in Defects4Jv2,
    where RepairAgent fixes 90 bugs, while ChatRepair only fixes 48 bugs. To further
    compare the sets of fixed bugs, Figure [6](#S5.F6 "Figure 6 ‣ V-B2 Comparison
    with Prior Work ‣ V-B RQ1: Effectiveness ‣ V Evaluation ‣ RepairAgent: An Autonomous,
    LLM-Based Agent for Program Repair") shows the overlaps between different approaches.
    As often observed in the field of APR, different approaches complement each other
    to some extent. In particular, RepairAgent fixes 39 bugs that were not fixed by
    any of the three baselines. Comparing the complexity of the bug fixes, as shown
    on the right-hand side of Table [IV](#S5.T4 "TABLE IV ‣ V-B1 Overall Results ‣
    V-B RQ1: Effectiveness ‣ V Evaluation ‣ RepairAgent: An Autonomous, LLM-Based
    Agent for Program Repair"), RepairAgent is particularly more effective, compared
    to other tools, for bugs that require more than a single-line fix. We attribute
    this result to the RepairAgent’s ability to autonomously retrieve suitable repair
    ingredients and the fact that the agent can perform edits to an arbitrary number
    of lines and files.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'if  (cfa  !=  null)  {for  (Node  finallyNode  :  cfa.finallyMap.get(parent))  {-  cfa.createEdge(fromNode,  Branch.UNCOND,  finallyNode);+  cfa.createEdge(fromNode,  Branch.ON_EX,  finallyNode);}}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Closure-14, bug fixed by RepairAgent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Separator  sep  =  (Separator)  elementPairs.get(0);+  if  (sep.iAfterParser  ==  null  &&  sep.iAfterPrinter  ==  null)  {PeriodFormatter  f  =  toFormatter(elementPairs.subList(2,  size),  notPrinter,  notParser);sep  =  sep.finish(f.getPrinter(),  f.getParser());return  new  PeriodFormatter(sep,  sep);+  }'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Time-27, bug fixed by RepairAgent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figures [7](#S5.F7 "Figure 7 ‣ V-B3 Examples ‣ V-B RQ1: Effectiveness ‣ V Evaluation
    ‣ RepairAgent: An Autonomous, LLM-Based Agent for Program Repair") and [8](#S5.F8
    "Figure 8 ‣ V-B3 Examples ‣ V-B RQ1: Effectiveness ‣ V Evaluation ‣ RepairAgent:
    An Autonomous, LLM-Based Agent for Program Repair") showcase interesting bugs
    fixed exclusively by RepairAgent. In the example of Figure [7](#S5.F7 "Figure
    7 ‣ V-B3 Examples ‣ V-B RQ1: Effectiveness ‣ V Evaluation ‣ RepairAgent: An Autonomous,
    LLM-Based Agent for Program Repair"), the agent used the tool $find\_similar\_api\_calls$
    to search for calls similar to cfa.createEdge(fromNode, Branch.UNCOND, finallyNode);
    which returns a similar call that is found in another file but passes Branch.ON_EX
    to the method call instead of Branch.UNCOND. The latter was used as the repair
    ingredient by the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, RepairAgent benefeted from the tool $generate\_method\_body$
    to generate the missing if-statement which led to suggesting a correct fix afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: From one side, these examples illustrate the clever and proper usage of available
    tools by the repair agent. From the other side, it shows how useful these tools
    at finding repair ingredients that traditional approaches and previous work failed
    to find.
  prefs: []
  type: TYPE_NORMAL
- en: 'V-C RQ2: Costs of the Approach'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/085940f2b03047d2c8a78a2bea33875b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0a2d5e0a47db4d3f818d2ad359daf4c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Tokens/money consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Distribution of cost metrics per bug (time, number of token, and
    monetary costs).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We measure three kinds of costs imposed by RepairAgent: (i) Time taken to fix
    a bug. (ii) The number of tokens consumed by queries to the LLM, which is relevant
    both for commercial models, such as the GPT-3.5 used here, and for self-hosted
    models, where the number of tokens determines the computational costs. (iii) The
    monetary costs associated with the token consumption, based on OpenAI’s pricing
    as of March 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our findings are summarized in Figure [9](#S5.F9 "Figure 9 ‣ V-C RQ2: Costs
    of the Approach ‣ V Evaluation ‣ RepairAgent: An Autonomous, LLM-Based Agent for
    Program Repair"). The median time taken to address a bug is 920 seconds, with
    minimal variation between fixed (870 seconds) and unfixed bugs. Surprisingly,
    fixed bugs do not consistently exhibit lower repair times. This is due to RepairAgent’s
    autonomous nature, where the repair process continues until the *goal_accomplished*
    command is invoked or the cycles budget is exhausted. The figure shows several
    outliers where bug fixing attempt takes multiple hours. RepairAgent spends 99%
    of the total time in tool executions, mostly running tests. This cost could be
    reduced in the future by executing selected test cases only.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the costs imposed by the LLM, we find a median consumption of approximately
    270,000 tokens, equating to around 14 cents (US dollars). The number of tokens
    consumed by fixed bugs (21,000) is clearly lower than by unfixed bugs (315,000).
    This difference is because the agent continues to extract additional information
    for not yet fixed bugs, saturating the prompt with operations, such as reading
    more lines of code or performing extensive searches.
  prefs: []
  type: TYPE_NORMAL
- en: 'V-D RQ3: Usage of Tools by the Agent'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand the approach, we evaluate how the agent uses the available
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: V-D1 Adherence to Output Format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A notable challenge in designing an LLM-based agent is the inherent informality
    and natural language noise present in the model’s output. We categorize the output
    of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fully parsable output:* Responses that adhere to the JSON format without requiring
    further refinement (87.7%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unparsable output:* Responses that do not conform to the JSON format (2.3%
    of the responses).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Partially parsable output:* Responses in JSON format but with missing or misnamed
    fields requiring refinement by the middleware (9.9%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A parsed output does not guarantee a correctly specified command, which the
    middleware tries to rectify. At this phase, the output may fall into the following
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Correct command name:* The name of the command is correct (97.9%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Non-existent command:* The command could not be found or mapped to an existing
    one (1.4%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mapped command:* The command does not exist but can be mapped to an existing
    command by the middleware (0.7%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Correct arguments:* The arguments are correct (90.1%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unrefinable arguments list:* The command exists or was mapped, but the list
    of arguments is incomplete or has incorrect names (1.9%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Refinable arguments list:* The middleware successfully maps the list of arguments
    into a correct one (8.0%).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Overall, these results show that our heuristic refinement of LLM outputs contributes
    to the effectiveness and robustness of RepairAgent.
  prefs: []
  type: TYPE_NORMAL
- en: V-D2 Frequency of Tools Invocation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf586852e5c0e931e46e51ec585ab8eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Frequency of tool invocations (average per bug).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On average, RepairAgent makes 35 calls per bug, which also corresponds to the
    number of cycles. Figure [10](#S5.F10 "Figure 10 ‣ V-D2 Frequency of Tools Invocation
    ‣ V-D RQ3: Usage of Tools by the Agent ‣ V Evaluation ‣ RepairAgent: An Autonomous,
    LLM-Based Agent for Program Repair") shows the frequency of tool invocations,
    where we distinguish between fixed (i.e., “correct”) and unfixed (i.e., “plausible”
    only or completely unfixed) bugs. The LLM agent uses the full range of tools,
    with the most frequently called tool being *write_fix* (average of 6 calls for
    fixed bugs and 17 calls for unfixed bugs. Around 7% of *write_fix* invocations
    in unfixed bugs produce plausible patches, compared to 44% in fixed bugs.'
  prefs: []
  type: TYPE_NORMAL
- en: VI Threats to Validity and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While RepairAgent shows promising results when repairing Defects4J bugs, we
    acknowledge several potential threats to validity and inherent limitations: *(i)
    Data leakage:* As we evaluate on GPT-3.5 and its training data is not publicly
    known, the LLM may have seen parts of the Java projects during training. While
    we acknowledge this risk, our approach does not solely depend on knowing about
    a bug, but rather the ability to collect information to fix the bug. We also note
    that the closest competitor, ChatRepair, also uses GPT-3.5, and thus faces the
    same risk. *(ii) Missing test cases:* Defects4J has at least one failing test
    case for each bug, which may not be the case for real-world usage scenarios. It
    will be interesting to evaluate RepairAgent on bugs with no a-priori available
    error-revealing test cases in future work. *(iii) Fault localization:* Inaccurate
    or imprecise fault localization could lead to suboptimal repair suggestions or
    incorrect diagnoses. *(iv) Non-deterministic output of LLMs:* The inherently non-deterministic
    nature of LLMs may result in different outcomes between two consecutive runs of
    RepairAgent. The large number of bugs we evaluate on mitigates this risk. Moreover,
    we make logs of all interactions with the LLM available to ensure reproducibility.'
  prefs: []
  type: TYPE_NORMAL
- en: VII Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated program repair
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Automated program repair [[1](#bib.bib1)] has received significant attention.
    Some approaches address it as a search problem based on manually designed code
    mutation rules and fix patterns [[2](#bib.bib2), [38](#bib.bib38), [3](#bib.bib3)].
    Alternatively, transformation rules can be derived (semi-)automatically from human-written
    patches [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Other approaches use
    symbolic constraints to derive fixes [[7](#bib.bib7), [39](#bib.bib39), [8](#bib.bib8),
    [9](#bib.bib9)], integrate repair into a static analysis that identifies bugs [[40](#bib.bib40),
    [41](#bib.bib41), [42](#bib.bib42)], or replace buggy code with similar code from
    the same project [[43](#bib.bib43)]. APR has been successfully deployed in industrial
    contexts [[5](#bib.bib5), [44](#bib.bib44)]. Beyond functional bugs, several techniques
    target other kinds of problems, such as syntax errors [[45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47)], performance bugs [[48](#bib.bib48)], vulnerabilities [[49](#bib.bib49)],
    type errors [[50](#bib.bib50)], common issues in deep learning code [[51](#bib.bib51)],
    and build errors [[52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: Learning-based program repair
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While early work uses machine learning to rank and select candidate fixes [[10](#bib.bib10)],
    more recent work uses machine learning to generate fixes. Approaches include neural
    machine translation models that map buggy code into fixed code [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)], models that predict tree
    transformations [[15](#bib.bib15), [16](#bib.bib16)], neural architectures for
    specific kinds of bugs [[53](#bib.bib53)], and repair-specific training regimes [[54](#bib.bib54),
    [37](#bib.bib37)]. We refer to a recent survey for a more comprehensive discussion [[55](#bib.bib55)].
    Unlike the above work, RepairAgent and the work discussed below use a general-purpose
    LLM, instead of training a task-specific model.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based program repair
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'LLMs have motivated researchers to apply them to program repair, e.g., in studies
    that explore prompts [[18](#bib.bib18), [17](#bib.bib17)] and in a technique that
    prompts the model with error messages [[56](#bib.bib56)]. These approaches perform
    a one-time interaction with the model, where the model receives a prompt with
    code and produces a fix. The most recent repair techniques introduce iterative
    approaches, which query the LLM repeatedly based on feedback obtained from previous
    fix attempts [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)]. RepairAgent
    also queries the model multiple times, but fundamentally differs by pursuing an
    agent-based approach. Section [V](#S5 "V Evaluation ‣ RepairAgent: An Autonomous,
    LLM-Based Agent for Program Repair") empirically compares RepairAgent to the most
    closely related iterative approaches [[19](#bib.bib19), [21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs for code generation and code editing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Beyond program repair, LLMs have been applied to a variety of other code generation
    and code editing tasks, including code completion [[34](#bib.bib34), [57](#bib.bib57)],
    fuzzing [[58](#bib.bib58)], generating and improving unit tests [[59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64)],
    multi-step code editing [[65](#bib.bib65)]. Unlike our work, none of these approaches
    uses an agent-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based agents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The idea to let LLM agents autonomously plan and perform complex tasks is relatively
    new and has been applied to tasks outside of software engineering [[28](#bib.bib28)].
    To the best of our knowledge, our work is the first to apply an LLM-based agent
    to program repair or any other code generation problem in software engineering.
    RepairAgent is inspired by prior work [[29](#bib.bib29)] on augmenting LLMs with
    tools invoked via APIs [[26](#bib.bib26), [27](#bib.bib27)] and with the ability
    to generate and execute code [[66](#bib.bib66)]. Our key contribution in applying
    these ideas to a software engineering task is to define tools that are useful
    for program repair and a prompt format that allows the LLM to interact with these
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents a pioneering technique for bug repair based on an autonomous
    agent powered by Large Language Models (LLMs). Through extensive experimentation,
    we validate the effectiveness and potential of our approach. Further exploration
    and refinement of autonomous agent-based techniques will help generalize to more
    difficult and diverse types of bugs if equipped with the right tools.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. Le Goues, M. Pradel, and A. Roychoudhury, “Automated program repair,”
    *Commun. ACM*, vol. 62, no. 12, pp. 56–65, 2019\. [Online]. Available: https://doi.org/10.1145/3318162'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A generic
    method for automatic software repair,” *IEEE Trans. Software Eng.*, vol. 38, no. 1,
    pp. 54–72, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyandé, “Tbar: revisiting template-based
    automated program repair,” in *Proceedings of the 28th ACM SIGSOFT International
    Symposium on Software Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19,
    2019*, D. Zhang and A. Møller, Eds.   ACM, 2019, pp. 31–42. [Online]. Available:
    https://doi.org/10.1145/3293882.3330577'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Kim, J. Nam, J. Song, and S. Kim, “Automatic patch generation learned
    from human-written patches.” in *International Conference on Software Engineering
    (ICSE)*, 2013, pp. 802–811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getafix: Learning to fix
    bugs automatically,” *Proc. ACM Program. Lang.*, vol. 3, no. OOPSLA, pp. 159:1–159:27,
    2019\. [Online]. Available: https://doi.org/10.1145/3360585'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. Bavishi, H. Yoshida, and M. R. Prasad, “Phoenix: automated data-driven
    synthesis of repairs for static analysis violations,” in *ESEC/SIGSOFT FSE 2019,
    Tallinn, Estonia, August 26-30, 2019*, M. Dumas, D. Pfahl, S. Apel, and A. Russo,
    Eds.   ACM, 2019, pp. 613–624\. [Online]. Available: https://doi.org/10.1145/3338906.3338952'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, “Semfix: program
    repair via semantic analysis,” in *35th International Conference on Software Engineering,
    ICSE ’13, San Francisco, CA, USA, May 18-26, 2013*, 2013, pp. 772–781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L. Marcote, T. Durieux,
    D. Le Berre, and M. Monperrus, “Nopol: Automatic repair of conditional statement
    bugs in java programs,” *IEEE Transactions on Software Engineering*, vol. 43,
    no. 1, pp. 34–55, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix: Scalable multiline program
    patch synthesis via symbolic analysis,” in *Proceedings of the 38th international
    conference on software engineering*, 2016, pp. 691–701.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] F. Long and M. Rinard, “Automatic patch generation by learning correct
    code,” in *Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles
    of Programming Languages, POPL 2016, St. Petersburg, FL, USA, January 20 - 22,
    2016*, 2016, pp. 298–312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] R. Gupta, S. Pal, A. Kanade, and S. K. Shevade, “Deepfix: Fixing common
    C language errors by deep learning,” in *Proceedings of the Thirty-First AAAI
    Conference on Artificial Intelligence*, 2017, pp. 1345–1351. [Online]. Available:
    http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk, “On
    learning meaningful code changes via neural machine translation,” in *Proceedings
    of the 41st International Conference on Software Engineering, ICSE 2019, Montreal,
    QC, Canada, May 25-31, 2019*, 2019, pp. 25–36\. [Online]. Available: https://dl.acm.org/citation.cfm?id=3339509'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, “Coconut:
    combining context-aware neural translation models using ensemble for program repair,”
    in *ISSTA ’20: 29th ACM SIGSOFT Virtual Event, USA, July 18-22, 2020*, S. Khurshid
    and C. S. Pasareanu, Eds.   ACM, 2020, pp. 101–114\. [Online]. Available: https://doi.org/10.1145/3395363.3397369'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk, and M. Monperrus,
    “SequenceR: Sequence-to-sequence learning for end-to-end program repair,” *IEEE
    Trans. Software Eng.*, vol. 47, no. 9, pp. 1943–1959, 2021. [Online]. Available:
    https://doi.org/10.1109/TSE.2019.2940179'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Li, S. Wang, and T. N. Nguyen, “Dlfix: Context-based code transformation
    learning for automated program repair,” in *ICSE*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Q. Zhu, Z. Sun, Y. Xiao, W. Zhang, K. Yuan, Y. Xiong, and L. Zhang, “A
    syntax-guided edit decoder for neural program repair,” in *ESEC/FSE ’21 Athens,
    Greece, August 23-28, 2021*, D. Spinellis, G. Gousios, M. Chechik, and M. D. Penta,
    Eds.   ACM, 2021, pp. 341–353\. [Online]. Available: https://doi.org/10.1145/3468264.3468544'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] C. S. Xia, Y. Wei, and L. Zhang, “Automated program repair in the era
    of large pre-trained language models,” in *2023 IEEE/ACM 45th International Conference
    on Software Engineering (ICSE)*, 2023, pp. 1482–1494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of code language models
    on automated program repair,” in *45th IEEE/ACM International Conference on Software
    Engineering, ICSE*.   IEEE, 2023, pp. 1430–1442\. [Online]. Available: https://doi.org/10.1109/ICSE48619.2023.00125'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing 162 out of
    337 bugs for $0.42 each using ChatGPT,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Kang, B. Chen, S. Yoo, and J. Lou, “Explainable automated debugging
    via large language model-driven scientific debugging,” *CoRR*, vol. abs/2304.02195,
    2023\. [Online]. Available: https://doi.org/10.48550/arXiv.2304.02195'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Ye and M. Monperrus, “Iter: Iterative neural repair for multi-location
    patches,” in *ICSE*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung, “An exploratory
    study of how developers seek, relate, and collect relevant information during
    software maintenance tasks,” *IEEE Transactions on software engineering*, vol. 32,
    no. 12, pp. 971–987, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Böhme, E. O. Soremekun, S. Chattopadhyay, E. Ugherughe, and A. Zeller,
    “Where is the bug and how is it fixed? an experiment with practitioners,” in *Proceedings
    of the 2017 11th joint meeting on foundations of software engineering*, 2017,
    pp. 117–128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of existing
    faults to enable controlled testing studies for java programs,” in *International
    Symposium on Software Testing and Analysis, ISSTA ’14, San Jose, CA, USA - July
    21 - 26, 2014*, 2014, pp. 437–440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
    P. Lee, Y. T. Lee, Y. Li, S. M. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro,
    and Y. Zhang, “Sparks of artificial general intelligence: Early experiments with
    GPT-4,” *CoRR*, vol. abs/2303.12712, 2023\. [Online]. Available: https://doi.org/10.48550/arXiv.2303.12712'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves
    to use tools,” *CoRR*, vol. abs/2302.04761, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2302.04761'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large language
    model connected with massive apis,” *CoRR*, vol. abs/2305.15334, 2023. [Online].
    Available: https://doi.org/10.48550/arXiv.2305.15334'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J.-R. Wen, “A survey on large language
    model based autonomous agents,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu,
    B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz, E. Grave, Y. LeCun, and
    T. Scialom, “Augmented language models: a survey,” *CoRR*, vol. abs/2302.07842,
    2023\. [Online]. Available: https://doi.org/10.48550/arXiv.2302.07842'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] R.-M. Karampatsis and C. Sutton, “How often do single-statement bugs occur?”
    Jun. 2020\. [Online]. Available: http://dx.doi.org/10.1145/3379597.3387491'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in neural information processing systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] L. D. Grazia and M. Pradel, “Code search: A survey of techniques for finding
    code,” *ACM Comput. Surv.*, vol. 55, no. 11, pp. 220:1–220:31, 2023. [Online].
    Available: https://doi.org/10.1145/3565971'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Eghbali and M. Pradel, “De-hallucinator: Iterative grounding for llm-based
    code completion,” *CoRR*, vol. abs/2401.01701, 2024\. [Online]. Available: https://doi.org/10.48550/arXiv.2401.01701'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. T. Mark Chen, “Evaluating large language models trained on code,” *CoRR*,
    vol. abs/2107.03374, 2021\. [Online]. Available: https://arxiv.org/abs/2107.03374'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Campos, A. Riboira, A. Perez, and R. Abreu, “Gzoltar: an eclipse plug-in
    for testing and debugging,” in *Proceedings of the 27th IEEE/ACM international
    conference on automated software engineering*, 2012, pp. 378–381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Zeller, *Why programs fail: a guide to systematic debugging*.   Elsevier,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Ye, M. Martinez, X. Luo, T. Zhang, and M. Monperrus, “Selfapr: Self-supervised
    program repair with test execution diagnostics,” in *ASE 2022, Rochester, MI,
    USA, October 10-14, 2022*.   ACM, 2022, pp. 92:1–92:13\. [Online]. Available:
    https://doi.org/10.1145/3551349.3556926'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. D. Le, D. Lo, and C. Le Goues, “History driven program repair,” in
    *IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering,
    SANER 2016, Suita, Osaka, Japan, March 14-18, 2016 - Volume 1*, 2016, pp. 213–224\.
    [Online]. Available: https://doi.org/10.1109/SANER.2016.76'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Ke, K. T. Stolee, C. Le Goues, and Y. Brun, “Repairing programs with
    semantic code search (t),” in *2015 30th IEEE/ACM International Conference on
    Automated Software Engineering (ASE)*.   IEEE, 2015, pp. 295–306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] R. van Tonder and C. L. Goues, “Static automated program repair for heap
    properties,” in *ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018*, M. Chaudron,
    I. Crnkovic, M. Chechik, and M. Harman, Eds.   ACM, 2018, pp. 151–162\. [Online].
    Available: https://doi.org/10.1145/3180155.3180250'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Y. Liu, S. Mechtaev, P. Subotić, and A. Roychoudhury, “Program repair
    guided by datalog-defined static analysis,” in *Proceedings of the 31st ACM Joint
    European Software Engineering Conference and Symposium on the Foundations of Software
    Engineering*, 2023, pp. 1216–1228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] N. Jain, S. Gandhi, A. Sonwane, A. Kanade, N. Natarajan, S. Parthasarathy,
    S. Rajamani, and R. Sharma, “Staticfixer: From static analysis to static repair,”
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Yang, X. Mao, L. Chen, X. Xu, Y. Lei, D. Lo, and J. He, “Transplantfix:
    Graph differencing-based code transplantation for automated program repair,” in
    *ASE 2022, Rochester, MI, USA, October 10-14, 2022*.   ACM, 2022, pp. 107:1–107:13\.
    [Online]. Available: https://doi.org/10.1145/3551349.3556893'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Marginean, J. Bader, S. Chandra, M. Harman, Y. Jia, K. Mao, A. Mols,
    and A. Scott, “Sapfix: Automated end-to-end repair at scale,” in *ICSE-SEIP*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] K. Wang, R. Singh, and Z. Su, “Search, align, and repair: data-driven
    feedback generation for introductory programming exercises,” in *Proceedings of
    the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation,
    PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018*, 2018, pp. 481–495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] R. Gupta, A. Kanade, and S. K. Shevade, “Deep reinforcement learning for
    syntactic error repair in student programs,” in *The Thirty-Third AAAI Conference
    on Artificial Intelligence, AAAI 2019, Honolulu, Hawaii, USA, January 27 - February
    1, 2019*.   AAAI Press, 2019, pp. 930–937\. [Online]. Available: https://doi.org/10.1609/aaai.v33i01.3301930'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Sakkas, M. Endres, P. J. Guo, W. Weimer, and R. Jhala, “Seq2parse:
    neurosymbolic parse error repair,” *Proc. ACM Program. Lang.*, vol. 6, no. OOPSLA2,
    pp. 1180–1206, 2022\. [Online]. Available: https://doi.org/10.1145/3563330'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. Yu and M. Pradel, “Pinpointing and repairing performance bottlenecks
    in concurrent programs,” *Empirical Software Engineering (EMSE)*, pp. 1–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Harer, O. Ozdemir, T. Lazovich, C. P. Reale, R. L. Russell, L. Y. Kim,
    and S. P. Chin, “Learning to repair software vulnerabilities with generative adversarial
    networks,” in *NeurIPS 2018, 3-8 December 2018, Montréal, Canada.*, 2018, pp.
    7944–7954.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. W. Chow, L. D. Grazia, and M. Pradel, “Pyty: Repairing static type
    errors in python,” in *International Conference on Software Engineering (ICSE)*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] X. Zhang, J. Zhai, S. Ma, and C. Shen, “AUTOTRAINER: an automatic DNN
    training problem detection and repair system,” in *43rd IEEE/ACM International
    Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021*.   IEEE,
    2021, pp. 359–371\. [Online]. Available: https://doi.org/10.1109/ICSE43902.2021.00043'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] D. Tarlow, S. Moitra, A. Rice, Z. Chen, P. Manzagol, C. Sutton, and E. Aftandilian,
    “Learning to fix build errors with graph2diff neural networks,” in *ICSE ’20 Workshops,
    Seoul, Republic of Korea, 27 June - 19 July, 2020*.   ACM, 2020, pp. 19–20\. [Online].
    Available: https://doi.org/10.1145/3387940.3392181'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Vasic, A. Kanade, P. Maniatis, D. Bieber, and R. Singh, “Neural program
    repair by jointly learning to localize and repair,” in *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] H. Ye, M. Martinez, and M. Monperrus, “Neural program repair with execution-based
    backpropagation,” in *ICSE*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Q. Zhang, C. Fang, Y. Ma, W. Sun, and Z. Chen, “A survey of learning-based
    automated program repair,” *ACM Transactions on Software Engineering and Methodology*,
    vol. 33, no. 2, pp. 1–69, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] H. Joshi, J. P. C. Sánchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radicek,
    “Repair is nearly generation: Multilingual program repair with llms,” in *Thirty-Seventh
    AAAI Conference on Artificial Intelligence, AAAI 2023, Washington, DC, USA, February
    7-14, 2023*, B. Williams, Y. Chen, and J. Neville, Eds.   AAAI Press, 2023, pp.
    5131–5140\. [Online]. Available: https://doi.org/10.1609/aaai.v37i4.25642'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. Shrivastava, H. Larochelle, and D. Tarlow, “Repository-level prompt
    generation for large language models of code,” in *International Conference on
    Machine Learning*.   PMLR, 2023, pp. 31 693–31 715.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang, “Fuzz4all:
    Universal fuzzing with large language models,” in *ICSE*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, “Codamosa: Escaping
    coverage plateaus in test generation with pre-trained large language models,”
    in *45th International Conference on Software Engineering, ser. ICSE*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Schäfer, S. Nadi, A. Eghbali, and F. Tip, “An empirical evaluation
    of using large language models for automated unit test generation,” *IEEE Trans.
    Software Eng.*, vol. 50, no. 1, pp. 85–105, 2024. [Online]. Available: https://doi.org/10.1109/TSE.2023.3334955'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan, and B. Ray,
    “Code-aware prompting: A study of coverage guided test generation in regression
    setting using llm,” in *FSE*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman, I. Harper,
    A. Marginean, S. Sengupta, and E. Wang, “Automated unit test improvement using
    large language models at meta,” in *FSE*, vol. abs/2402.09171, 2024\. [Online].
    Available: https://doi.org/10.48550/arXiv.2402.09171'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Kang, J. Yoon, and S. Yoo, “Large language models are few-shot testers:
    Exploring llm-based general bug reproduction,” in *45th IEEE/ACM International
    Conference on Software Engineering, ICSE*, 2023, pp. 2312–2323\. [Online]. Available:
    https://doi.org/10.1109/ICSE48619.2023.00194'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Feng and C. Chen, “Prompting is all your need: Automated android bug
    replay with large language models,” in *ICSE*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] R. Bairi, A. Sonwane, A. Kanade, V. D. C, A. Iyer, S. Parthasarathy, S. Rajamani,
    B. Ashok, and S. Shet, “Codeplan: Repository-level coding using llms and planning,”
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig,
    “PAL: program-aided language models,” *CoRR*, vol. abs/2211.10435, 2022\. [Online].
    Available: https://doi.org/10.48550/arXiv.2211.10435'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
