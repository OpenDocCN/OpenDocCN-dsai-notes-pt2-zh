- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.00993](https://ar5iv.labs.arxiv.org/html/2407.00993)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shihan Deng Weikai Xu Hongda Sun Wei Liu XiaoMi AI Lab Tao Tan Gaoling School
    of Artificial Intelligence, Renmin University of China Jianfeng Liu XiaoMi AI
    Lab Ang Li XiaoMi AI Lab Jian Luan XiaoMi AI Lab Bin Wang XiaoMi AI Lab Rui Yan
    Shuo Shang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the remarkable advancements of large language models (LLMs), LLM-based
    agents have become a research hotspot in human-computer interaction. However,
    there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking
    these agents generally faces three main challenges: (1) The inefficiency of UI-only
    operations imposes limitations to task evaluation. (2) Specific instructions within
    a singular application lack adequacy for assessing the multi-dimensional reasoning
    and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics
    are insufficient to accurately assess the process of sequential actions. To this
    end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities
    of LLM-based mobile agents. First, we expand conventional UI operations by incorporating
    103 collected APIs to accelerate the efficiency of task completion. Subsequently,
    we collect evaluation data by combining real user queries with augmentation from
    LLMs. To better evaluate different levels of planning capabilities for mobile
    agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT,
    reflecting varying levels of task complexity. Mobile-Bench comprises 832 data
    entries, with more than 200 tasks specifically designed to evaluate multi-APP
    collaboration scenarios. Furthermore, we introduce a more accurate evaluation
    metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential
    points during their planning and reasoning steps. Dataset and platform are available
    at [https://github.com/XiaoMi/MobileBench](https://github.com/XiaoMi/MobileBench).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents'
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†footnotetext: ^∗ Equal contribution.^†^†footnotetext: ^($\dagger$) Corresponding
    authors: Rui Yan and Shuo Shang.^†^†footnotetext: ^‡ Work done during the internship
    at XiaoMi.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Interacting with mobile devices using natural language is a long-standing pursuit
    in human-computer interaction  (Bolt, [1980](#bib.bib6); Karat et al., [2002](#bib.bib19);
    Følstad and Brandtzæg, [2017](#bib.bib16)). With the remarkable advancements in
    large language models (LLM)  (Bai et al., [2022](#bib.bib2); Chowdhery et al.,
    [2022](#bib.bib9); Du et al., [2021](#bib.bib13); Touvron et al., [2023](#bib.bib39);
    Ouyang et al., [2022](#bib.bib29)), LLM-driven agents are at the forefront, yet
    their reasoning capability to navigate mobile application functionalities lags
    behind their proficiency with web pages on PCs  (Yao et al., [2022](#bib.bib45);
    Sun et al., [2023](#bib.bib38)). To faithfully replicate a typical mobile environment,
    it’s imperative to incorporate a diverse set of applications and leverage authentic
    data, moving beyond the limitations of purely simulated scenarios. The development
    challenges in the mobile domain stem from a trio of core issues: a limited understanding
    of mobile interfaces, a scarcity of application variety, and a lack of real-world
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3eab3a133c6dda0422d0e6628fb4202a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: For the task of “Setting an alarm for seven thirty.”, accomplishing
    it solely through UI operations requires four steps, while API calls can achieve
    the same task in just one step.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Platform&BenchMark | InfoUI | API&UI | Real APP | Real Query | Multi-APP
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| World of Bits Shi et al. ([2017](#bib.bib35)) | ✓ | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| WebShop Yao et al. ([2022](#bib.bib45)) | ✓ | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AndroidEnv Toyama et al. ([2021](#bib.bib40)) | ✗ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| MobileEnv Zhang et al. ([2023](#bib.bib47)) | ✓ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile-Bench (Ours) | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison of Mobile-Bench with existing LLM-based agent platforms.
    ‘InfoUI’ represents whether UI information is used for interaction with agents,
    ‘API&UI’ represents whether the agent’s actions like API calls and UI interface
    operations, ’Real APP’ represents whether real applications are used, ‘Real Query’
    represents whether real user queries are used, and ‘Multi-APP’ represents whether
    there are tasks involving multiple applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to Google’s breakthrough (Wang et al., [2023](#bib.bib41)) in UI interface
    representation, LLM agent’s understanding of UI pages becomes easier, leading
    to the creation of UI platforms such as Android-Env Toyama et al. ([2021](#bib.bib40))
    and Mobile-Env Zhang et al. ([2023](#bib.bib47)), which tasks are defined within
    individual games or search engines. However, these works collectively face the
    following challenges: (1) UI actions depend on the textual descriptions of interfaces,
    where structured text fails to capture the content of graphical buttons or images
    which can lead to wrong actions. A single API action might be equivalent to dozens
    of UI steps, leading to UI’s inefficiency. (2) Their tasks are far removed from
    real-world task scenarios encountered in daily use, which require cooperation
    between multiple applications, with user commands being ambiguous and not specifying
    target applications. (3) The evaluation of tasks should not solely rely on LLMs,
    without any objective quantitative metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, voice assistants on mobile phones can meet most of the users’ daily
    needs, yet they do not interact directly with UI interfaces but operate by invoking
    the APIs Qin et al. ([2023](#bib.bib30)) behind applications. As shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Mobile-Bench: An Evaluation Benchmark
    for LLM-based Mobile Agents"), in mobile applications, APIs are more efficient
    than UI interfaces; a single API call can be equivalent to multiple UI operations
    to achieve the same outcome. However, a single API is insufficient for more complex
    tasks, especially when user commands are unclear, necessitating reliance on LLMs
    to interpret user intent. Therefore, an agent capable of utilizing both UI and
    APIs would be best suited for the job. Simultaneously, It requires developing
    a strategy for the selection and order of the application usage, with human oversight
    merely focusing on reviewing the outcomes. This is a function that voice assistants
    currently lack Wen et al. ([2023a](#bib.bib43), [b](#bib.bib44)). To this end,
    we develop a combination of API and UI actions to circumvent the limitations of
    UI interfaces, each action can be chosen between UI interactions and API calls;
    all tasks begin from the mobile HOME page rather than from the launch page of
    a specific application, enabling the agent to determine single or multiple applications
    it will use; queries in the task are gathered from real users, and instruction
    generation is only applied to some complex ones which undergo rigorous manual
    review; we draw inspiration from objective metrics in software automation testing,
    named CheckPoint, and have made necessary adjustments to accommodate the unpredictable
    semantic outputs of LLMs. Above all, we propose a mobile phone environment that
    includes a platform supporting both API and UI interactions, and a corresponding
    dataset with multi-APP tasks. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Mobile-Bench:
    An Evaluation Benchmark for LLM-based Mobile Agents") presents a comparison among
    recent platforms and benchmark work based on API and UI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) To the best of our knowledge, we are the first to establish a running platform
    for LLM-based mobile agents that simultaneously supports both UI and API calls.
  prefs: []
  type: TYPE_NORMAL
- en: (2) We propose an evaluation dataset containing diverse tasks for multi-APP
    interactions. Our tasks starting from the home page are more appropriate for testing
    the planning capabilities for agents. Our dataset and platform will be released
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: (3) We introduce a new category-based evaluation metric to assess the task completion
    capabilities of the agent in the context of both UI and API interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Mobile Platforms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior to the emphasis on LLM-based agents, research efforts were directed towards
    RL-based agents, exemplified by the Android-Env platform  Toyama et al. ([2021](#bib.bib40)).
    This open-source platform tailored for reinforcement learning experiments within
    the Android ecosystem, successfully tested various RL-based agents like DDPG  Zhang
    and Van Huynh ([2023](#bib.bib46)), D4PG  Barth-Maron et al. ([2018](#bib.bib4)),
    MPO  Abdolmaleki et al. ([2018](#bib.bib1)), DQN  Mnih et al. ([2015](#bib.bib26)),
    IMPALA  Espeholt et al. ([2018](#bib.bib14)) and R2D2  Kapturowski et al. ([2018](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 'More significant research has focused on LLM-based agents Liu et al. ([2024](#bib.bib24));
    Sun et al. ([2024b](#bib.bib37), [a](#bib.bib36)). Regarding the domain of tool-using
    agents, they can be categorized into three main types:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) For mobile tasks. Platforms like AutoDroid, DroidBot-GPT, GPT-Droid, and
    WebShop Wen et al. ([2023a](#bib.bib43), [b](#bib.bib44)); Liu et al. ([2023b](#bib.bib25));
    Yao et al. ([2022](#bib.bib45)) create an interactive environment enabling LLMs
    to engage with mobile tasks, and generate human-like operations for automation
    test. Mobile-Env  Zhang et al. ([2023](#bib.bib47)) is specifically designed to
    evaluate agents’ capabilities in handling multi-step interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 2) For PC Tasks. Researchers developed Toollama  Qin et al. ([2023](#bib.bib30))
    to evaluate the capabilities to use tools and API calls. AgentBench  Liu et al.
    ([2023a](#bib.bib22)) presents a standardized Agent task evaluation architecture
    with strong decoupling and scalability. PPTC Benchmark  Guo et al. ([2023](#bib.bib17))
    proposed to evaluate the ability of LLM-based agents on PowerPoint tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Other Methods. Toolformer  Schick et al. ([2023](#bib.bib32)) and HuggingGPT
     Shen et al. ([2023](#bib.bib34)) evaluate LLM’s capability to master tools.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Benchmarks for LLM agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess agents’ proficiency in understanding user interfaces, a diverse dataset
    covering various tasks is crucial Liu et al. ([2023a](#bib.bib22)). The widely
    used RICO dataset  Deka et al. ([2017](#bib.bib11)) is commonly employed for this
    purpose, with Screen2Vec Li et al. ([2021](#bib.bib20)) utilizing it to evaluate
    agent performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to the absence of specific standards for evaluating agent performance,
    efforts have focused on designing evaluation frameworks. PPTC Benchmark  Guo et al.
    ([2023](#bib.bib17)) devised 279 multi-round dialogue tasks for PPT file operations.
    DroidTask  Wen et al. ([2023a](#bib.bib43)) and various unnamed datasets  Liu
    et al. ([2023b](#bib.bib25)); Wen et al. ([2023b](#bib.bib44)) covering various
    mobile applications have also been established. Additionally, Screen2Words used
    a sampling method to sample screens from the RICO-SCA  Li et al. ([2020](#bib.bib21))
    dataset and hired professional annotators to generate English summaries for these
    screens  Wang et al. ([2021](#bib.bib42)).
  prefs: []
  type: TYPE_NORMAL
- en: Current evaluation standards align with various works. ToolBench proposes Win
    Rate gauges the model’s solution quality against benchmarks like RoBERTa Liu et al.
    ([2019](#bib.bib23)), GPT-3  Brown et al. ([2020](#bib.bib7)), PaLM  Chowdhery
    et al. ([2023](#bib.bib10)), OPT Zhang et al. ([2022](#bib.bib48)), ChatGPT  Bubeck
    et al. ([2023](#bib.bib8)) and GPT-4  OpenAI ([2023](#bib.bib28)). Although Fan Fan
    et al. ([2024](#bib.bib15)) found that the cost of inference can be reduced by
    using only the necessary layers for inference, it is still expensive to calculate
    the win rate. Mobile-Env  Zhang et al. ([2023](#bib.bib47)) evaluates agent performance
    based on the completion status, average steps, and average rewards in WikiHow
    tasks. PPTC Benchmark  Guo et al. ([2023](#bib.bib17)) uses Turn-based and Session-based
    accuracy. Android in the Wild  Rawles et al. ([2023](#bib.bib31)) makes use of
    Out-of-distribution Generalization. Overall, metrics such as success rate, episode
    length, and match score are currently the most commonly employed.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Our Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Mobile-Bench Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data collection.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The queries in the dataset are divided into the following three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SAST: Single-App-Single-Task. A real dataset containing only one task text,
    including single-task operations such as opening and closing APP, such as "Help
    me open the map".'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SAMT: Single-App-Multi-Task. A real dataset containing multiple task texts,
    as well as constructed single-APP data. A complex multi-task on single APP, such
    as "Help me open the map, and navigate to Eiffel Tower.".'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MAMT: Multi-App-Multi-Task. Constructed multi-APP data, complete a complex
    multi-task, such as "Help me search for the latest technology news and share it
    with friends."'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SAST is directly derived from real voice requests processed by the voice assistants
    loaded on the mobile phone. We select a subset of this query collection, primarily
    filtering out the portion that requires voice assistant processing and involves
    multimodal tools. Additionally, querys that exceed permissions or involve privacy
    are also filtered out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there are fewer SAMT and MAMT data in real data and the quality is not
    high, refer to Toollama  Qin et al. ([2023](#bib.bib30)) method, we use GPT-4
    to construct SAMT and MAMT data. For MAMT, we randomly sample 6 applications from
    the entire application collection, and then provide some examples of real multi-APP
    data to prompt GPT-4 to select 2-4 applications to generate tasks. By integrating
    real and constructed data, we create the final dataset. An example of data is
    shown in Figure [2](#S3.F2 "Figure 2 ‣ Data collection. ‣ 3.1 Mobile-Bench Benchmark
    ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile
    Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3072cb3f0078baaa643b9051bd0dd3b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A test case in MAMT. $\&amp;$ stands for sequential check, SC. The
    package CheckPoint passes when the action history includes either Amap and Ctrip
    Travel, or Amap and Qunar. Key phrase CheckPoint comes from the orange parts in
    the case.'
  prefs: []
  type: TYPE_NORMAL
- en: APP & API collection.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To ensure task comprehensiveness, we select not only the applications included
    in SAST and SAMT but also the most popular free applications from each category
    in the APP Store. Obtaining the API is to analyze the package of each application
    to obtain its external reserved interface Desnos and Gueguen ([2011](#bib.bib12)).
    The advantage of this is that the obtained API is naturally classified for the
    application. Since the description of the API in the decompilation result is not
    as detailed as the development document, we use the ADB(Android Debug Bridge)
    command to verify the feasibility of the API one by one. Owing to its debugging
    properties, system-level APIs can also be invoked normally, allowing access to
    functions such as checking the battery status and performing memory cleaning.
    For more specific application names and categories, please refer to Appendix [B.3](#A2.SS3
    "B.3 APP&API statistics ‣ Appendix B Details of Dataset ‣ Mobile-Bench: An Evaluation
    Benchmark for LLM-based Mobile Agents")'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset statistics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Including several default applications within the system, we collected a total
    of 29 applications. For applications, we collected a total of 103 usable APIs,
    which primarily serve the following functions: system calls, opening pages, closing
    pages, searching for information, viewing details, and controlling device switches.
    These functions are summarized into the following main aspects: page switch, details
    view, broadcast, search. In Table [2](#S3.T2 "Table 2 ‣ Dataset statistics. ‣
    3.1 Mobile-Bench Benchmark ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation Benchmark
    for LLM-based Mobile Agents"), we have tabulated the number of APIs and the functional
    categories covered by APIs, categorized by the type of APP. We organized the available
    APIs and APP descriptions for each APP, and generated an APP list as the basis
    for selecting applications, shown in Appendix [B.3](#A2.SS3 "B.3 APP&API statistics
    ‣ Appendix B Details of Dataset ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Mobile-Bench dataset, we collected a total of 332, 300, 200 queries
    for SAST, SAMT, and MAMT. We sort out the APIs actually used by each task in real
    voice requests. Provide these API as an example to GPT-4 for query generation.
    As shown in Figure [3](#S3.F3 "Figure 3 ‣ Dataset statistics. ‣ 3.1 Mobile-Bench
    Benchmark ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents")(a), we calculated the ratio of tasks calling APIs, ensuring a
    sufficient number of tasks in the dataset that include steps to call API. This
    approach ensures that we have sufficient data to analyze the role of APIs in task
    completion.'
  prefs: []
  type: TYPE_NORMAL
- en: APP Category API Quantity APP Number API Functions Travel Transportation 5 3
    ①, ②, ④ Audiovisual Vision 15 5 ①, ②, ③ Social Communication 3 1 ①, ②, ④ Fashion
    Shopping 14 6 ①, ④ Information News 11 4 ①, ②, ④ Practical Tool 38 8 ①, ②, ③,
    ④, ⑤ Home Life 5 1 ①, ⑤ Book Reading 7 2 ①, ②, ④ Universal Buttons 5 0 ⑤
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Our dataset covers nine major categories of applications, and we compared
    them based on the API function. The above API functions can be summarized into
    five categories: ① Page Navigation, ② Viewing Details, ③ Playback, ④ Searching,
    and ⑤ System Calls.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7b8026164b2a1e75ce51f34823957b5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: (a) The API$\&amp;$. (b) The number of CheckPoints.'
  prefs: []
  type: TYPE_NORMAL
- en: Quality verification.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bolotova-Baranova et al. ([2023](#bib.bib5)) The initial test data originates
    from software automation tests, but some complex data points are generated by
    GPT-4\. To ensure the quality of our dataset, we randomly sampled 100 data points
    from each of the SAST, SAMT, and MAMT, resulting in a total of 300 quality test
    data. We conducted cross-source validation to verify the feasibility of these
    CheckPoints. The specific formula for calculation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Overlap}(CP_{1},CP_{2})=\frac{&#124;CP_{1}\cap CP_{2}&#124;}{&#124;CP_{1}&#124;}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'CP[1],CP[2] representing the CheckPoint sequences generated by CP[instruction]
    and CP[Human], respectively. In Table [3](#S3.T3 "Table 3 ‣ Quality verification.
    ‣ 3.1 Mobile-Bench Benchmark ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation
    Benchmark for LLM-based Mobile Agents"), we list the human evaluation results
    for three types of data. From the table, it can be observed that a higher proportion
    of terminal data corresponds to better data quality. However, all MAMT data is
    generated by instructions, its quality does not exhibit an unacceptable gap compared
    to SAST. See appendix [B.1](#A2.SS1 "B.1 Dataset quality analysis ‣ Appendix B
    Details of Dataset ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile
    Agents") for more analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics SAST SAMT MAMT Total CP[instruction] 395 546 513 1454 CP[Human] 412
    598 623 1633 CP[instruction ​ $\cap$ ​ Human] 372 466 412 1250 Overlap 0.94 0.85
    0.80 0.86
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Human Evaluation Results'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Test Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Overview Mobile-Bench is designed as a universal interaction platform that
    supports hybrid API and UI interactions. Users are able to construct their own
    evaluation data following a fixed format, yet they must adhere to our prescribed
    evaluation method. As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Test Platform
    ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile
    Agents") users can interact with the environment using the following commands.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start: Open the test environment and load the preset snapshot using this command.
    Each test case must start from the same environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stop: Stop the test environment and end test.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Close: Close the test environment and save the test process and results.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check: Capture a screenshot snapshot of the current test environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ReSet: Load a previously saved environment snapshot into the test environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4dab88a650addca618831dc8408563ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Test Platform Overview. The test platform is linked by the user,
    the simulator, and the Agent. After the user’s instructions are issued, the entire
    test execution process is completed by the Agent, which can view and manage the
    test tasks through the preset interface in the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation space To enable the agent to read information on the android emulator
    in a human-like manner, we use Appium to obtain page information. Following the
    method described by Wang (Wang et al., [2023](#bib.bib41)), we convert XML to
    HTML, as the training data for LLMs is predominantly sourced from the Internet,
    which includes numerous HTML files. Therefore, we believe that LLM has a better
    understanding of HTML than XML. Given the tree structure of XML, we initially
    convert the XML into a tree format and subsequently transform the nodes that need
    to be displayed to the agent into HTML. The agent simulates human interaction
    with smartphones, performing three major operations: click, input, and scroll.
    Humans visually identify which elements can be clicked or receive input, and use
    their fingers to determine if they can scroll the screen. Therefore, we provide
    the agent with elements that are visible and scrollable. Due to the limit on context
    length, we only convert the information required by the agent in XML to HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Type: HTML element categories inherited directly from XML formatted information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ID: “ID” inherits from the XML “resource-id” attribute, uniquely identifying
    the existence of an element.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Package: the package name of the current application.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class: the class of the element, such as ImageView, TextView.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Description & text: describe the function and shape of the element.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clickable & Scrollable: whether the element is clickable and scrollable.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bounds: if the element is scrollable, this attribute will be present and scope
    the scroll component, such as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\bm{\left[x_{i},y_{i}\right]\left[x_{j},y_{j}\right]}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The scrollable rectangle ranges from $[x_{i},y_{i}]$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And, there is an example of HTML elements: <button package="com.ximalaya.ting.android"
    class="android.widget.Button" clickable="true"> message </button>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action space Our Mobile-Bench imitates human behavior in using mobile and summarizes
    three actions Zhang et al. ([2023](#bib.bib47)) and imitates the process of calling
    the API on the test platform Sengupta et al. ([2023](#bib.bib33)):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Click: simulate real user click actions by passing in specific elements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scroll: simulate real user scrolling actions by tapping - dragging - releasing.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input: simulate real user input actions by clicking-typing.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API Call: launch an activity or send an intent by invoking an API through ADB
    commands.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19f38f28795306aae6b139823f232101.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Baseline Model Overview. The entire process framework consists of
    sensors, reflection components, controllers, execution components, and environments.
    Once a task starts, these components will run iteratively until the task is completed
    or the maximum number of steps is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Evaluation Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CheckPoint.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Automated test CheckPoint coverage Bajunaid and Menascé ([2018](#bib.bib3))
    is a test metric for the software execution process. It cannot assist in checking
    the software results, but it can visually inspect whether the software runs in
    the specified unit sequence. During data construction, we supply APPs and APIs,
    which naturally serve as detection indicators. Additionally, we incorporated a
    CheckPoint to verify if the UI operation correctly clicks on the intended element.
    After sorting out the above CheckPoints, we constructed the following three CheckPoints:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Package: the unique package name corresponding to the application. Checking
    the package can determine whether the correct application is used.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key phrase: the key phrase extracted from the query, represents key steps in
    the UI execution process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API: API commands that need to be called during the execution process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To evaluate the agent’s selection and execution capabilities, we divide the
    inspection granularity into two levels: CheckPoint[l1] - whether it uses the correct
    application, and CheckPoint[l2] - whether it follows the predefined paths to complete
    the task. For CheckPoint[l1], we check the number of correctly called packages.
    For CheckPoint[l2], we check the number of correctly called package, key phrase,
    API. For CheckPoints, we identify three logical relationships: sequential, conjunctive,
    and disjunctive checks. These correspond to the instability of LLM output and
    its tendency for synonym substitution. The calculation formula for "sequential
    check" is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'SC represent Sequential Check Set and AH represent Actions History. The calculation
    formulas for conjunctive checks is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'CC represent Conjunctive Check Set. The calculation formulas for disjunctive
    checks is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: DC represent Disjunctive Check Set. The weighted sum of the above three scores
    will be the final CheckPoint coverage rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [3](#S3.F3 "Figure 3 ‣ Dataset statistics. ‣ 3.1 Mobile-Bench
    Benchmark ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents"), the number of key phrase CheckPoints is significantly higher
    than that of packages, indicating the need for more semantic information to ensure
    tasks are completed step-by-step. Analyzing the dataset from a proportional perspective,
    we find that the distributions of the three types of CheckPoints are 0.212, 0.493,
    0.294, with key phrase CheckPoints remaining the most predominant method of checking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a test case should include at least the following contents: ID,
    Query, APP List, CheckPoints(Package, Key phrase, API). Figure [2](#S3.F2 "Figure
    2 ‣ Data collection. ‣ 3.1 Mobile-Bench Benchmark ‣ 3 Our Environment ‣ Mobile-Bench:
    An Evaluation Benchmark for LLM-based Mobile Agents") is a test case that contains
    the above three CheckPoints.'
  prefs: []
  type: TYPE_NORMAL
- en: PassRate.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Qin et al. ([2023](#bib.bib30)) We assess an agent’s human-computer interaction
    capabilities by calculating the proportion of queries successfully completed within
    the specified step limits. During this process, we organized the emulator’s current
    state. Subsequently, GPT-4 evaluates the task completion status. We computed the
    percentage of pass tasks, yielding a PassRate as an indicator of agent’s human-computer
    interaction capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Average steps.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhang et al. ([2023](#bib.bib47)) We quantified the step size required by Mobile-Bench
    to complete tasks as a metric for evaluating the efficiency of the agent. In Mobile-Bench,
    a ’step’ is defined as the completion of a UI operation or the execution of an
    API call.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Baseline Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our model’s architecture, illustrated in Algorithm [1](#alg1 "Algorithm 1 ‣
    4.1 Baseline Model ‣ 4 Experiment ‣ Mobile-Bench: An Evaluation Benchmark for
    LLM-based Mobile Agents"), begins by obtaining the smartphone’s UI information
    in XML format through Appium and transforms it into HTML format through a heuristic
    algorithm. Subsequently, as illustrated in Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Test
    Platform ‣ 3 Our Environment ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents") leveraging the HTML, task details, and APP list, LLM generates
    a comprehensive task plan, outlining the necessary applications and corresponding
    sub-tasks. As the collection of APIs is organized based on the classification
    of APPs, we can get the API set that may be used in plan.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The task plan is executed iteratively. In each iteration, the model either
    performs an API call or a UI operation. After each execution, the model records
    the success or failure of the action in its history, generates the subsequent
    thought, and evaluates whether the task has been completed. For the actual running
    process of an algorithm, please refer to the appendix [C.7](#A3.SS7 "C.7 Algorithm
    Examples ‣ Appendix C Details for Baseline Model ‣ Mobile-Bench: An Evaluation
    Benchmark for LLM-based Mobile Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Baseline Model
  prefs: []
  type: TYPE_NORMAL
- en: '1:description of the Task, $Task$22:end while Metric LLaMA-13B LLaMA-70B GPT-3.5-turbo
    GPT-4 SAST SAMT MAMT SAST SAMT MAMT SAST SAMT MAMT SAST SAMT MAMT Average #Steps
    7.43 18.76 49.52 5.97 16.63 48.91 4.53 12.06 48.73 3.79 13.94 44.86 PassRate 44.58
    27.67 8 56.62 54 13.5 64.94 64 15.5 80.96 63 26.5 CheckPoint[l1] 46.08 43.67 28.74
    56.62 61 39.98 66.75 67 43.16 81.57 72.66 61.34 CheckPoint[l2] 34.85 29.13 21.39
    63.12 62.73 41.21 76.21 71.29 44.09 83.76 77.35 52.98'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Results of the agents based on different LLMs on Mobile-Bench dataset.
    On MAMT data, due to context length limitations, a compression is applied to the
    actions history by retaining only the most recent 20 entries.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate four popular LLMs on the proposed Mobile-Bench task set: GPT-3.5-turbo
    Ouyang et al. ([2022](#bib.bib29)), GPT-4 Nori et al. ([2023](#bib.bib27)), LLaMA-13B
    and LLaMA-70BTouvron et al. ([2023](#bib.bib39)), while ChatGPT-3.5 and GPT-4
    are accessed through the online APIs of OpenAI. The experiments are conducted
    with a 3-shot in-context learning under sampling temperature of 0.1. Recognizing
    that task execution incurs costs, we preset different maximum step limits for
    tasks based on their difficulty levels. For the three categories of SAST, SAMT,
    and MAMT, we set the max step to 10, 20, and 50 respectively. Owing to the limit
    of budget, only GPT-3.5 utilizes an interface with a context length of 16K. GPT-4
    uses a standard interface, which necessitated compression and trimming of actions
    history. See Appendix [A](#A1 "Appendix A Settings ‣ Mobile-Bench: An Evaluation
    Benchmark for LLM-based Mobile Agents") for other settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As observed in Table [4](#S4.T4 "Table 4 ‣ 4.1 Baseline Model ‣ 4 Experiment
    ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents"), it can
    be observed that GPT-3.5 outperforms GPT-4 in PassRate on SAMT(64%>63%), and it
    requires fewer steps to complete the task(12.06<13.94). To investigate this phenomenon,
    we analyze the output files and find that models with poorer performance exhibit
    PassRate misjudgments: they prematurely terminate even when the task is not completed.
    This phenomenon is also present in LLaMA, which exhibits a high PassRate (44.58%)
    but low CheckPoint coverage (34.85%). At the same time, we delved into why the
    results for MAMT are so low(15.5%, 26.5%). Our analysis revealed that LLMs often
    exhibit greedy exploration behavior when completing tasks, meaning they struggle
    to determine when to exit the current application and transition to the next one.
    This tendency is particularly prevalent in certain generation tasks. Moreover,
    as the actions history increases, its ability to accurately judge task progress
    becomes increasingly challenging. For more detailed result, please refer to Table
    [7](#A2.T7 "Table 7 ‣ B.5 Supplementary experiments ‣ Appendix B Details of Dataset
    ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Settings Average #Steps CheckPoint[l2] PassRate SAST (GPT-4) 3.79 83.76 80.96
    SAMT (GPT-4) 13.94 77.35 63 MAMT (GPT-4) 44.86 52.98 26.5 SAST (w/o API) 6.13
    72.73 74.39 SAMT (w/o API) 16.86 56.74 48 MAMT (w/o API) 49.17 31.69 9.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: API Ablation Study based on GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Impact of API Calls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'API Calls can accelerate task execution, as a single call often replaces several
    sequential UI steps. From another perspective, the ability of the agent to select
    appropriate APIs and input parameters warrants further investigation. Choosing
    the wrong API may lead the task in an incorrect direction or require a significant
    number of steps to rectify. Therefore, in Table [5](#S4.T5 "Table 5 ‣ 4.3 Results
    ‣ 4 Experiment ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents"),
    we evaluate and analyze the impact of introducing APIs on task completion based
    on GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Table [5](#S4.T5 "Table 5 ‣ 4.3 Results ‣ 4 Experiment ‣ Mobile-Bench:
    An Evaluation Benchmark for LLM-based Mobile Agents"), it can be seen that even
    in SAST, the PassRate has decreased by 6.57% (from 80.96 to 74.39). Furthermore,
    the values for CheckPoints[l2] exhibit a more pronounced decrease after API removal,
    with a drop exceeding 20% in SAMT. Simultaneously, we have observed varying increases
    in the average number of steps, which align with our expectations. We analyzed
    the results and found that the inability to accurately scroll pages, inefficient
    exploration of page functionality, and failure to click graphical buttons are
    the primary reasons for the low efficiency of UI operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Settings Average #Steps CheckPoint[l1] CheckPoint[l2] PassRate SAST (GPT-4)
    3.63 82 79.74 76 SAST (w/o thought) 8.86 82 29.16 24 SAST (w/o plan) 3.98 76 74.54
    72 SAMT (GPT-4) 13.94 63 72.66 77 SAMT (w/o thought) 19.54 63 18.31 20 SAMT (w/o
    plan) 17.09 52 58.02 62'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Thought and Plan Ablation Study on SAST (subset 50) and SAMT (subset
    200) based on GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Impact of Plan and Thought
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since observation-thought-action is already a standardized process in the agent
    directionQin et al. ([2023](#bib.bib30)), and verified by experimental results,
    planning and thought before action are essential. From the experimental results,
    we can find that without the observation-thought step, the agent is almost unable
    to complete the task(77->20, 76->24), which is because it cannot determine the
    next action category and the current task status. In more complex tasks SAMT,
    losing the plan has more negative consequences(77->62). But they will have almost
    no impact on CheckPoint[l1](82->82 63->63), because the application selection
    is almost done by the API Call.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work we have proposed an agent capability testing environment that
    supports API and UI interaction on mobile phone. This holds significant importance
    for exploring how LLMs can be integrated with mobile operating systems. Additionally,
    it can serve as a valuable reference for developing testing platforms for operating
    systems to evaluate the capabilities of LLM agents. We collected and released
    a test dataset containing tasks for multiple APPs, ensuring its quality through
    human verification. Based on this data set and environment, we tested the planning,
    decision-making and execution of various LLM-based agents. Please refer to the
    Section [6](#S6 "6 Limitations ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents") for the limitations of our benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While general large models exhibit strong capabilities in reasoning and planning,
    they tend to have pronounced illusions in API calls. As a result, the language
    model may become confused about the application’s functionality, leading to a
    reluctance to continue and complete the task. Therefore, fine-tuning a model for
    instructions is highly necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic CheckPoint is a process evaluation metric, making it challenging to
    assess the quality of the final outcome. This depends on whether the agent has
    obtained the necessary information (actions) on the required pages.
  prefs: []
  type: TYPE_NORMAL
- en: The enhancement of the agent’s capabilities relies on extensive API and SDK
    libraries, requiring substantial support from application development companies.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have rigorously refined our dataset to remove any elements that could compromise
    personal privacy, thereby guaranteeing the highest level of protection for individual
    data. The evaluation of our work was carried out through a meticulously randomized
    selection of IT professionals. This process ensured a gender-balanced and educationally
    diverse panel, reflecting a wide spectrum of perspectives and expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank the Xiaoai Voice Department of Xiaomi Technology Corporation for their
    raw data support for this project. We additionally thank our crowd annotators
    for their diligent work, Junfeng Peng and Yifan Cheng for contributing to the
    human performance estimates, and the anonymous reviewers for their constructive
    comments. This work was supported by the NSFC (U2001212, 62032001, and 61932004).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abdolmaleki et al. (2018) Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval
    Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. 2018. Maximum a posteriori
    policy optimisation. *arXiv preprint arXiv:1806.06920*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2022) Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones,
    A Chen, A Goldie, A Mirhoseini, C McKinnon, et al. 2022. Constitutional ai: Harmlessness
    from ai feedback (arxiv: 2212.08073). arxiv.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bajunaid and Menascé (2018) Noor Bajunaid and Daniel A Menascé. 2018. Efficient
    modeling and optimizing of checkpointing in concurrent component-based software
    systems. *Journal of Systems and Software*, 139:1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barth-Maron et al. (2018) Gabriel Barth-Maron, Matthew W Hoffman, David Budden,
    Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy
    Lillicrap. 2018. Distributed distributional deterministic policy gradients. *arXiv
    preprint arXiv:1804.08617*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bolotova-Baranova et al. (2023) Valeriia Bolotova-Baranova, Vladislav Blinov,
    Sofya Filippova, Falk Scholer, and Mark Sanderson. 2023. Wikihowqa: A comprehensive
    benchmark for multi-document non-factoid question answering. In *Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 5291–5314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolt (1980) Richard A Bolt. 1980. “put-that-there” voice and gesture at the
    graphics interface. In *Proceedings of the 7th annual conference on Computer graphics
    and interactive techniques*, pages 262–270.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deka et al. (2017) Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman,
    Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A mobile
    app dataset for building data-driven design applications. In *Proceedings of the
    30th annual ACM symposium on user interface software and technology*, pages 845–854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Desnos and Gueguen (2011) Anthony Desnos and Geoffroy Gueguen. 2011. Android:
    From reversing to decompilation. *Proc. of Black Hat Abu Dhabi*, 1:1–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2021) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2021. Glm: General language model pretraining with
    autoregressive blank infilling. *arXiv preprint arXiv:2103.10360*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Espeholt et al. (2018) Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,
    Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al.
    2018. Impala: Scalable distributed deep-rl with importance weighted actor-learner
    architectures. In *International conference on machine learning*, pages 1407–1416\.
    PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2024) Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo
    Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. 2024. Not all layers of llms
    are necessary during inference. *arXiv preprint arXiv:2403.02181*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Følstad and Brandtzæg (2017) Asbjørn Følstad and Petter Bae Brandtzæg. 2017.
    Chatbots and the new world of hci. *interactions*, 24(4):38–42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, and Duan
    Nan. 2023. Pptc benchmark: Evaluating large language models for powerpoint task
    completion. *arXiv preprint arXiv:2311.01767*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kapturowski et al. (2018) Steven Kapturowski, Georg Ostrovski, John Quan, Remi
    Munos, and Will Dabney. 2018. Recurrent experience replay in distributed reinforcement
    learning. In *International conference on learning representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karat et al. (2002) Clare-Marie Karat, John Vergo, and David Nahamoo. 2002.
    Conversational interface technologies. *The human-computer interaction handbook*,
    pages 169–186.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and Brad A
    Myers. 2021. Screen2vec: Semantic embedding of gui screens and gui components.
    In *Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems*,
    pages 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge.
    2020. Mapping natural language instructions to mobile ui action sequences. *arXiv
    preprint arXiv:2005.03776*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023a. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang,
    and Rui Yan. 2024. From skepticism to acceptance: Simulating the attitude dynamics
    toward fake news. *arXiv preprint arXiv:2403.09498*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu
    Wu, Xing Che, Dandan Wang, and Qing Wang. 2023b. Chatting with gpt-3 for zero-shot
    human-like mobile automated gui testing. *arXiv preprint arXiv:2305.09434*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
    learning. *nature*, 518(7540):529–533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nori et al. (2023) Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan,
    and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. *arXiv
    preprint arXiv:2303.13375*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rawles et al. (2023) Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana
    Riva, and Timothy Lillicrap. 2023. Android in the wild: A large-scale dataset
    for android device control. *arXiv preprint arXiv:2307.10088*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sengupta et al. (2023) Aritro Sengupta, Amit Singh, and BM Vinjit. 2023. A platform
    independent and forensically sound method to extract whatsapp data from mobile
    phones. *International Journal of Electronic Security and Digital Forensics*,
    15(3):259–280.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface. *arXiv preprint arXiv:2303.17580*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2017) Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez,
    and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents.
    In *International Conference on Machine Learning*, pages 3135–3144\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2024a) Hongda Sun, Hongzhan Lin, Haiyu Yan, Chen Zhu, Yang Song,
    Xin Gao, Shuo Shang, and Rui Yan. 2024a. Facilitating multi-role and multi-behavior
    collaboration of large language models for online job seeking and recruiting.
    *arXiv preprint arXiv:2405.18113*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2024b) Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai,
    Xin Gao, Shuo Shang, and Rui Yan. 2024b. Harnessing multi-role capabilities of
    large language models for open-domain question answering. In *Proceedings of the
    ACM on Web Conference 2024*, pages 4372–4382.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo
    Shang, Ji-Rong Wen, and Rui Yan. 2023. Determlr: Augmenting llm-based logical
    reasoning from indeterminacy to determinacy. *arXiv preprint arXiv:2310.18659*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Toyama et al. (2021) Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe
    Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina
    Precup. 2021. Androidenv: A reinforcement learning platform for android. *arXiv
    preprint arXiv:2105.13231*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational
    interaction with mobile ui using large language models. In *Proceedings of the
    2023 CHI Conference on Human Factors in Computing Systems*, pages 1–17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman,
    and Yang Li. 2021. Screen2words: Automatic mobile ui summarization with multimodal
    learning. In *The 34th Annual ACM Symposium on User Interface Software and Technology*,
    pages 498–510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2023a) Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu,
    Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2023a.
    Empowering llm to use smartphone for intelligent task automation. *arXiv preprint
    arXiv:2308.15272*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2023b) Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. 2023b.
    Droidbot-gpt: Gpt-powered ui automation for android. *arXiv preprint arXiv:2304.07061*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Van Huynh (2023) Bolun Zhang and Nguyen Van Huynh. 2023. Deep deterministic
    policy gradient for end-to-end communication systems without prior channel knowledge.
    *arXiv preprint arXiv:2305.07448*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Danyang Zhang, Lu Chen, and Kai Yu. 2023. Mobile-env: A
    universal platform for training and evaluation of mobile interaction. *arXiv preprint
    arXiv:2305.08144*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct experiments on the Android 14.0 version emulator and use Appium UiAutomator2
    Driver for automated testing. Before each execution of a task, we load a snapshot
    to ensure the emulator in the same environment every time. For all applications,
    we have logged in to the account in advance to ensure that the full function of
    the application can be used. Since we tests in the real world, we filtered out
    any tasks that included payments.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Details of Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Dataset quality analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The root cause of low-quality data often lies in the inaccuracies in the descriptions
    of applications. Additionally, ambiguity in query generation also plays a significant
    role. For example, in the query ”Help me find pictures related to Beijing”, although
    the user has not explicitly specified the source application, for a human, the
    expected result would likely be a search engine or a map application, as the images
    are not likely to be from the user themselves. However, for LLM, because the statement
    includes the word “pictures”, it might be reasonable for it to spend all its time
    searching for pictures in the gallery application, even though this effort would
    ultimately be in vain. CheckPoint coverage is calculated as the weighted sum of
    the scores for the three types of CheckPoints mentioned above.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Prompts for Instruction Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Below we list the detailed prompt for instruction generation, including single-APP-multi-task
    description, multi-APP-multi-task description.
  prefs: []
  type: TYPE_NORMAL
- en: 'single-APP-multi-task description:'
  prefs: []
  type: TYPE_NORMAL
- en: You will be provided with an application with descriptions, an available API
    list including adb command, function description and parameter information. You
    should create 5 varied, innovative, and detailed multi task queries that employ
    this application as a tool, API can be used as an auxiliary.
  prefs: []
  type: TYPE_NORMAL
- en: Each query should include the necessary parameters. Note that you shouldn’t
    ask ‘which APP to use’, rather, simply state your needs that can be addressed
    by these APPs. You should also avoid asking for the input parameters required
    by the APP call, but instead directly provide the parameter in your query. Those
    related APP and APIs have to strictly come from the provided lists.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, you also need to provide the CheckPoint of this query, including
    package, key phrase and API. The package comes from the package corresponding
    to the APP to be used. Key phrase is the key click element or key input character
    that the Android emulator will perform when executing this query, which is used
    to check whether the query has been completed. Key phrase should be noun and part
    of query, should be kept as short as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Key phrase can contain multiple pieces of information, "$|$", and the count
    increases by one for each passed element. The "ADB Command" to be used is stored
    in the API, which may also be empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deliver your response in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: $[\{$
  prefs: []
  type: TYPE_NORMAL
- en: '"id": "number"'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "text"'
  prefs: []
  type: TYPE_NORMAL
- en: '"APP": "APP name"'
  prefs: []
  type: TYPE_NORMAL
- en: '"CheckPoint": $\{$'
  prefs: []
  type: TYPE_NORMAL
- en: '"package": "APP package name"'
  prefs: []
  type: TYPE_NORMAL
- en: '"key phrase": $[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"API: $[$"'
  prefs: []
  type: TYPE_NORMAL
- en: $\}$
  prefs: []
  type: TYPE_NORMAL
- en: $\ \}$
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: $]$
  prefs: []
  type: TYPE_NORMAL
- en: 'multi-APP-multi-task description:'
  prefs: []
  type: TYPE_NORMAL
- en: You will be provided with some APPs with descriptions, available API list including
    adb command, function description and parameter information. You should create
    3 varied, innovative, and detailed multi queries that employ multi-APP as a tool,
    API can be used as an auxiliary.
  prefs: []
  type: TYPE_NORMAL
- en: Each query should include the necessary parameters. Note that you shouldn’t
    ask ‘which APP to use’, rather, simply state your needs that can be addressed
    by these APPs. You should also avoid asking for the input parameters required
    by the APP call, but instead directly provide the parameter in your query. Those
    related APPs and APIs have to strictly come from the provided lists. You should
    first think about possible related APP combinations, then give your query. Keep
    in mind that each query should call upon two to four APPs.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, you also need to provide the CheckPoint of this query, including
    package, key phrase and API. The package comes from the package corresponding
    to the APP to be used. Key phrase is the key click element or key input character
    that the Android emulator will perform when executing this query, which is used
    to check whether the query has been completed. Key phrase should be noun and part
    of query, should be kept as short as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Key phrase can contain multiple pieces of information, "$|$", and the count
    increases by one for each passed element. The "ADB Command" to be used is stored
    in the API, which may also be empty. For different queries, overlap of related
    APPs should be as little as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deliver your response in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: $[\{$
  prefs: []
  type: TYPE_NORMAL
- en: '"id": "number"'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "text"'
  prefs: []
  type: TYPE_NORMAL
- en: '"APP": $[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"CheckPoint": $\{$'
  prefs: []
  type: TYPE_NORMAL
- en: '"package": $[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"key phrase": $[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"API: $[$"'
  prefs: []
  type: TYPE_NORMAL
- en: $\}$
  prefs: []
  type: TYPE_NORMAL
- en: $\ \}$
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: $]$
  prefs: []
  type: TYPE_NORMAL
- en: B.3 APP&API statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18e1b5128992422c239719f4a14328d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: APP classification and quantity chart: The largest category is utility
    tools, where we categorize fundamental mobile applications. Their distinctive
    feature is the use of standard API interfaces, and the API functionality is more
    comprehensive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from Figure [6](#A2.F6 "Figure 6 ‣ B.3 APP&API statistics ‣
    Appendix B Details of Dataset ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents"), each functional area contains at least one application and its
    corresponding API. These applications are sufficient to meet the daily needs of
    users. In other words, our simulation environment is almost consistent with the
    real daily use environment, and it is consistent with the real daily use environment.
    Open world information exchange. There are so many practical tools that are the
    basic functions of mobile . They have been automatically installed and completed
    during system installation, and standard API interfaces for tools are easier to
    obtain. Our next step is to increase the number of APIs and SDKs for third-party
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CheckPoints is a group of words, including packages, key phases, and API, which
    represent the package name, action keywords, and API instructions of the application
    respectively. We regularize these words and action histories to check whether
    they select a sufficient and correct number of applications, UI elements, and
    APIs to accomplish the given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will give an example of CheckPoints in Figure [8](#A2.F8 "Figure 8
    ‣ B.4 Case study ‣ Appendix B Details of Dataset ‣ Mobile-Bench: An Evaluation
    Benchmark for LLM-based Mobile Agents") and Figure [8](#A2.F8 "Figure 8 ‣ B.4
    Case study ‣ Appendix B Details of Dataset ‣ Mobile-Bench: An Evaluation Benchmark
    for LLM-based Mobile Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dcbad2e205cff31309c0ac8cf0e9faa2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A test case in SAST.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9eae1d1682d0f6fefbd4ba14fd69ff2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A test case in SAMT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [10](#A2.F10 "Figure 10 ‣ B.4 Case study ‣ Appendix B Details of Dataset
    ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents") and Figure
    [10](#A2.F10 "Figure 10 ‣ B.4 Case study ‣ Appendix B Details of Dataset ‣ Mobile-Bench:
    An Evaluation Benchmark for LLM-based Mobile Agents") is an example of a data
    set and action history. Note that CheckPoints only check successfully executed
    instructions in the action history. From the action history, we can see that the
    emulator successfully opened the application by API, perform tasks in ctrip package,
    and selected "air ticket", "Beijing", and "Shanghai" elements, but failed to input
    the correct date. According to the definitions of level 1 and level 2 CheckPoints,
    level 1 CheckPoint score counts package CheckPoints covered, and the score of
    the example is 1/1, level 2 CheckPoint score counts all CheckPoints covered, and
    the score of the example is 5/6.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/167a3dbed6a02d72a0c33028e91d8ce7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A test case in SAMT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0a74e4977cc2ea3e6197b071cc32c791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A action history of a test case in SAMT.'
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Supplementary experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As can be seen from the table [7](#A2.T7 "Table 7 ‣ B.5 Supplementary experiments
    ‣ Appendix B Details of Dataset ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based
    Mobile Agents"), categories with smaller average execution steps generally have
    higher success rates and CheckPoints scores. Among them, the travel transportation
    task has the largest average number of execution steps and the lowest PassRate.
    We can think that more complex tasks require longer execution steps, and the PassRate
    and CheckPoint score of complex tasks are lower. Travel transportation task contains
    more uncertainties and it is difficult to determine whether it is completed, so
    the PassRate is the lowest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'APP Category Case Quantity Average #Steps PassRate(%) CheckPoint[l1] CheckPoint[l2]
    Travel Transportation 18 8.17 39 83 68 Audiovisual Vision 34 4.03 82 68 72 Social
    Communication 30 6.40 77 57 63 Fashion Shopping 35 7.97 54 63 61 Information News
    24 6.46 67 83 68 Practical Tool 61 2.08 89 87 89 Home Life 46 1.67 89 72 91 Book
    Reading 23 4.17 78 74 84 Universal Buttons 61 1.20 98 98 99'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Results on SAST classified by APP categories'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Details for Baseline Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Examples for HTML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8774ead21c41ca51539b98c5deee7e2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example for HTML. The orange box illustrates clickable elements,
    and the blue frame illustrates the scrollable range.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [11](#A3.F11 "Figure 11 ‣ C.1 Examples for HTML ‣ Appendix C Details
    for Baseline Model ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile
    Agents") shows the correspondence between the components in the UI page and the
    corresponding HTML code. It is easy to find that most components have text descriptions,
    but the switch of the alarm clock does not have a corresponding text description,
    and LLM will hardly think of it. To click this button, therefore, component function
    exploration is what we need to do next.'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Prompts for application Selection and Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are a large language model agent stored on a mobile phone, below I will
    provide you with a task, the environment of the current mobile phone interface(Apps
    information).
  prefs: []
  type: TYPE_NORMAL
- en: Please help me choose the correct APP to perform the task based on the Apps
    information. If the APP you want is not available on the current page, you can
    go to play store and download a suitable APP.
  prefs: []
  type: TYPE_NORMAL
- en: On this basis, you should make a simple plan for completing the task.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Begin!
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Prompts for API Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are the greatest large language model agent stored on a mobile phone. You
    will be provided with a API list that can be called by mobile phone, the task
    you need to complete, the thought about what have done and what need to do now.
  prefs: []
  type: TYPE_NORMAL
- en: You are just the first step to interact with the phone, and your follow-up is
    UI interaction components. If you find that there is no suitable API and the next
    step is UI interaction, please answer directly sorry. You should not use the API
    to complete the work that has been completed by the UI interactive components
    in the previous steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your decision should consider the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. You need to first judge based on the UI information and actions complete
    whether the planned action has been completed.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. You must only choose one API that should be executed most at present to
    finish the first action in next actions.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. If there is no suitable API, you can just say sorry without providing any
    additional suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: Strings within "$<></math>$
  prefs: []
  type: TYPE_NORMAL
- en: Scroll the screen from $[x_{start},y_{start}]$.
  prefs: []
  type: TYPE_NORMAL
- en: '[Examples]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Click and input have higher priority than scrolling. Scrolling is only considered
    when all elements of the current interface are indeed irrelevant to the task.
  prefs: []
  type: TYPE_NORMAL
- en: 2.When you fail to try repeatedly in one interface, maybe you can try to turn
    back to select other options.
  prefs: []
  type: TYPE_NORMAL
- en: 3.When you need to switch APPs, you need to return to the desktop first.
  prefs: []
  type: TYPE_NORMAL
- en: 4.When input fails multiple times, you should first select it with click.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Begin!
  prefs: []
  type: TYPE_NORMAL
- en: C.5 Prompts for Thought Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are a large language model agent stored on a mobile phone, below I will
    provide you with a task, a plan, the environment of the current mobile phone interface
    before action (Previous UI information), current action, the environment of the
    current mobile phone interface(Now UI information), action history. Action history
    records completed operations, including click, input, scroll and API list.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to summarize these four aspects: changes in the UI page, actions that
    have been completed, task progress, one next action.'
  prefs: []
  type: TYPE_NORMAL
- en: '[one next action] need to choose one among click, input, scroll and one API
    as the next action, and give one and only one operation object. [One next action]
    strictly refer to [current action] and [action history] result to do the next
    action.'
  prefs: []
  type: TYPE_NORMAL
- en: '[action history] are all previous historical actions, and [current action]
    is the current action that causes the UI page to change.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Examples]:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Begin!
  prefs: []
  type: TYPE_NORMAL
- en: C.6 Prompts for Task Completion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are a large language model agent stored on a mobile phone, below I will
    provide you with a task, the environment of the current mobile phone interface(UI
    information), historical action information. You need to judge whether the current
    task has been completed based on the current environment and historical action
    information.
  prefs: []
  type: TYPE_NORMAL
- en: C.7 Algorithm Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a running process of the algorithm on a test case of SAMT
  prefs: []
  type: TYPE_NORMAL
- en: '$[$:'
  prefs: []
  type: TYPE_NORMAL
- en: $\ \{$
  prefs: []
  type: TYPE_NORMAL
- en: '"id": 2,'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": $[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"Play recent records in history with Himalaya."'
  prefs: []
  type: TYPE_NORMAL
- en: $]$,
  prefs: []
  type: TYPE_NORMAL
- en: '"check$\_$'
  prefs: []
  type: TYPE_NORMAL
- en: '"activity":$[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"com.ximalaya.ting.android.host.activity.MainActivity",'
  prefs: []
  type: TYPE_NORMAL
- en: $\&amp;$ "com.ximalaya.ting.android.host.activity.MainActivity"
  prefs: []
  type: TYPE_NORMAL
- en: $]$,
  prefs: []
  type: TYPE_NORMAL
- en: '"key phrase": $[$'
  prefs: []
  type: TYPE_NORMAL
- en: '"Playing history" | "history "'
  prefs: []
  type: TYPE_NORMAL
- en: $]$,
  prefs: []
  type: TYPE_NORMAL
- en: '"package": "com.ximalaya.ting.android"'
  prefs: []
  type: TYPE_NORMAL
- en: $\}$,
  prefs: []
  type: TYPE_NORMAL
- en: '"domain": "smartApp/Ximalaya"'
  prefs: []
  type: TYPE_NORMAL
- en: $\}$
  prefs: []
  type: TYPE_NORMAL
- en: 'According to algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Baseline Model ‣ 4 Experiment
    ‣ Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents"), LLM generates
    a plan based on the query in data as a task and a given list of available applications
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '$[$: Play recent records in history with Himalaya.'
  prefs: []
  type: TYPE_NORMAL
- en: '$[$:'
  prefs: []
  type: TYPE_NORMAL
- en: $[$
  prefs: []
  type: TYPE_NORMAL
- en: $\{$
  prefs: []
  type: TYPE_NORMAL
- en: '"name": "ctrip",'
  prefs: []
  type: TYPE_NORMAL
- en: '"function$\_$description": "As an authoritative online travel service company
    in the industry, Ctrip’s travel hotel booking platform covers approximately 1.4
    million hotels around the world; air ticket products cover more than 300 international
    airlines; these airlines operate flights to major cities around the world, providing
    more than 2 million Air routes connect more than 5,000 cities in about 200 countries
    and regions…"'
  prefs: []
  type: TYPE_NORMAL
- en: $\}$,
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: $]$
  prefs: []
  type: TYPE_NORMAL
- en: '$[$: I should open the Himalaya app and find the "Playback History" option
    in the app to view recent playback history.'
  prefs: []
  type: TYPE_NORMAL
- en: Then the task plan is executed iteratively. In each iteration, the model accepts
    task, app list, action history, plan, and current HTML as input, and outputs execution
    API calls or UI operation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A short example of AH(action history), thought, Html will be shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '$[$:'
  prefs: []
  type: TYPE_NORMAL
- en: $\{$
  prefs: []
  type: TYPE_NORMAL
- en: $\{$down
  prefs: []
  type: TYPE_NORMAL
- en: $\_$
  prefs: []
  type: TYPE_NORMAL
- en: $\{$
  prefs: []
  type: TYPE_NORMAL
- en: '$[$:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changes: The current page is the main page of the Ximalaya app. There is no
    change between the previous and current pages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task progress: From the current action and action history, I am currently on
    the main page of the Ximalaya app, ready to check the play history.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actions completed: From the current action and action history, I have opened
    the Ximalaya app.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One next action: Click on the "play history" option to view the recent play
    history.'
  prefs: []
  type: TYPE_NORMAL
- en: '$[$:'
  prefs: []
  type: TYPE_NORMAL
- en: <p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true">
    membership </p>
  prefs: []
  type: TYPE_NORMAL
- en: <p id="com.ximalaya.ting.android:id/main$\_$level" package="com.ximalaya.ting.android"
    class="android.widget.TextView" clickable="true"> user level </p>
  prefs: []
  type: TYPE_NORMAL
- en: <p id="com.ximalaya.ting.android:id/main$\_$title" package="com.ximalaya.ting.android"
    class="android.widget.TextView" clickable="true"> Listen (minutes) </p>
  prefs: []
  type: TYPE_NORMAL
- en: <p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true">
    fans </p>
  prefs: []
  type: TYPE_NORMAL
- en: <p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true">
    Follow </p>
  prefs: []
  type: TYPE_NORMAL
- en: <p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true">
    live </p>
  prefs: []
  type: TYPE_NORMAL
- en: <button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true">
    message </button>
  prefs: []
  type: TYPE_NORMAL
- en: <button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true">
    history </button>
  prefs: []
  type: TYPE_NORMAL
- en: <button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true">
    favorite </button>
  prefs: []
  type: TYPE_NORMAL
- en: <button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true">
    local </button>
  prefs: []
  type: TYPE_NORMAL
- en: <button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true">
    purchased </button>
  prefs: []
  type: TYPE_NORMAL
- en: <img package="com.ximalaya.ting.android" class="android.widget.ImageView" description="play"
    clickable="true"> </img>
  prefs: []
  type: TYPE_NORMAL
- en: 'A example of api action or ui action will be shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: $[$
  prefs: []
  type: TYPE_NORMAL
- en: '$[$: click(<button package="com.ximalaya.ting.android" class="android.widget.Button"
    clickable="true"> history </button>)'
  prefs: []
  type: TYPE_NORMAL
- en: After successful execution, the current action will be added to the action history,
    the updated HTML of the emulator will be read, and handed over to LLM to generate
    a new thought and determine whether the task is over.
  prefs: []
  type: TYPE_NORMAL
- en: '$[$:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changes: The current page is the "My" page in the Ximalaya app.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actions Complete: I have opened the Ximalaya app and clicked the "my" button,
    then clicked the "play history" button.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task progress: The current mission progress is to view the play history.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One next action: Click on the "play" item to continue playing.'
  prefs: []
  type: TYPE_NORMAL
- en: '$[$: No, task is not finished.'
  prefs: []
  type: TYPE_NORMAL
