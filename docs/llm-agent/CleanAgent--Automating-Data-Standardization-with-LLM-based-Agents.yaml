- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'CleanAgent: Automating Data Standardization with LLM-based Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08291](https://ar5iv.labs.arxiv.org/html/2403.08291)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Danrui Qi, Jiannan Wang Simon Fraser University
  prefs: []
  type: TYPE_NORMAL
- en: '{dqi, jnwang}@sfu.ca'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data standardization is a crucial part in data science life cycle. While tools
    like Pandas offer robust functionalities, their complexity and the manual effort
    required for customizing code to diverse column types pose significant challenges.
    Although large language models (LLMs) like ChatGPT have shown promise in automating
    this process through natural language understanding and code generation, it still
    demands expert-level programming knowledge and continuous interaction for prompt
    refinement. To solve these challenges, our key idea is to propose a Python library
    with declarative, unified APIs for standardizing column types, simplifying the
    LLM’s code generation with concise API calls. We first propose Dataprep.Clean
    which is written as a component of the Dataprep Library, offers a significant
    reduction in complexity by enabling the standardization of specific column types
    with a single line of code. Then we introduce the CleanAgent framework integrating
    Dataprep.Clean and LLM-based agents to automate the data standardization process.
    With CleanAgent , data scientists need only provide their requirements once, allowing
    for a hands-free, automatic standardization process. To demonstrate the practical
    utility of CleanAgent , it has been developed as a user-friendly web application,
    allowing VLDB attendees to interact with it using real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data standardization, which is pivotal in the realm of data science, aims to
    transform heterogeneous data formats within a single column into an unified data
    format. This crucial data preprocessing step is essential for enabling effective
    data integration, data analysis, and decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1. We illustrate the data standardization task in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ CleanAgent: Automating Data Standardization with
    LLM-based Agents"). Given an input table $T$, data in the three cells of “Admission
    Date” column follows only one date format, i.e. the “MM/DD/YYYY” format.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, data scientists heavily rely on libraries such as Pandas (McKinney
    et al., [2024](#bib.bib2)) for data standardization tasks. Even though Pandas
    is a powerful tool, achieving data standardization often requires writing hundreds
    or thousands of lines of code. The standardization process for single column involves
    identifying the column type, applying intricate methods such as regular expressions
    to each cell within this column for validation, and converting each cell into
    desired formats. Moreover, a table may contain multiple columns, each possibly
    of a different type, requiring bespoke standardization code for each column type.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the emergence of large language models (LLMs), especially ChatGPT,
    has shown potential in revolutionizing this process. By leveraging their natural
    language understanding and code generation ability, these models could significantly
    aid data scientists by autonomously generating standardization code in response
    to conversational prompts. However, this method still necessitates detailed prompt
    crafting and often involves multi-turn dialogues, which limits efficiency and
    practicality.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba8ea4cfa416e2872d34615fce97faa8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An example of automatic data standardization process with CleanAgent.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these limitations, our key idea is to introduce a Python library
    involving declarative and unified APIs specifically designed for standardizing
    different column types. This idea simplifies the LLM’s task to converting natural
    language (NL) instructions into succinct, declarative API calls. Such an approach
    streamlines the LLM’s code generation process, requiring just a few lines of code,
  prefs: []
  type: TYPE_NORMAL
- en: This above idea, however, introduces two primary challenges. The first challenge
    (C1) is the design of the declarative and unified APIs for data standardization,
    ensuring it can effectively reduce the intricacies involved in standardizing specific
    column types (ideally one line of code per column type). The second challenge
    (C2) centers on optimizing the interaction between data scientists and LLMs. Our
    goal is to minimize human involvement, ideally allowing data scientists to input
    their standardization requirements in one instance, thereby enabling an autonomous
    and hands-off data standardization process.
  prefs: []
  type: TYPE_NORMAL
- en: To solve C1, we propose the type-specific Clean module in Dataprep Library¹¹1[https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep),
    named Dataprep.Clean²²2[https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean](https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean).
    By observing the common steps of data standardization for specific column types,
    we design unified APIs clean_type(df, column_name, target_format), where the type
    represents the desired standardization type, such as date, address and phone,
    etc. These unified APIs offer enhanced expressiveness compared to raw Pandas code,
    reducing the complexity of standardizing specific column types. With Dataprep.Clean,
    data scientists can standardize one column type with only one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve C2, we propose the CleanAgent framework which automates data standardization
    with Dataprep.Clean and LLM-based Agents  (Xi et al., [2023](#bib.bib4); Wu et al.,
    [2023](#bib.bib3)). Once users have entered their final goals, the LLM-based Agents
    can free their hands, autonomously generate thoughts and execute particular tasks.
    CleanAgent leverages both the capabilities of both Dataprep.Clean and LLM-based
    Agents. Data scientists only need to input the table that needs to be standardized
    and their requirements, CleanAgent will complete the data standardization process
    automatically with three steps: annotating the type of each column, generating
    concise Python code for standardization and executing the generated Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2. Continuing with Example 1\. Given an input table $T$.
  prefs: []
  type: TYPE_NORMAL
- en: CleanAgent is built as a web application. We allow the VLDB attendees to choose
    sample data and communicate with CleanAgent for standardization. We provide the
    demonstration video which can be found on Youtube³³3[https://youtu.be/fSYXVM6qeqM](https://youtu.be/fSYXVM6qeqM).
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we make the following contributions: (1) We propose Dataprep.Clean,
    an open-sourced library showing reducing the complexity of implementing standardization
    for specific column types with type-specific standardization functions. (2) We
    propose CleanAgent , which automates the data standardization process by combining
    both the advantages of Dataprep.Clean and LLM-based Agents. (3) We deploy CleanAgent as
    a web application with user-friendly interface and demonstrate its utility. We
    also open-sourced the implementation of CleanAgent on GitHub⁴⁴4[https://github.com/sfu-db/CleanAgent](https://github.com/sfu-db/CleanAgent).'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Type-Specific Standardization API Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first describe the common steps of data standardization.
    Then we introduce the type-specific API design of Dataprep.Clean.
  prefs: []
  type: TYPE_NORMAL
- en: Common Steps of Data Standardization. Inspired by the steps human standardizing
    data cell, we identify three common steps of data standardization. We take the
    datetime column type as an example to illustrate these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Assume a data scientist is dealing with an datetime column including two records
    ”Thu Sep 25 10:36:28 2003” and ”1996.07.10 AD at 15:08:56”. The data scientist
    wants to unify the chaotic column into a target format ”YYYY-MM-DD hh:mm:ss”.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Split. At the beginning, the data scientist needs to split the datetime
    string into several single parts which include one kind of specific information.
    In our example, the data scientist can get several tokens {’Thu’,’Sep’,’25’,’10’,’36’,’28’,’2003’}
    from the first record by using space and colon as separators. Different type has
    its own splitting strategy which is not always splitting into tokens. For example,
    the data scientist will split the email string into the username part and the
    domain part.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Validate. Standardization can only be performed on valid inputs. Thus the
    second step should be validation. For example, if the string ”little cat” is an
    instance of the datetime column, this string is invalid and the data scientist
    will transform it to a default value like NaN. Intuitively, a string is valid
    means that each part of this string after splitting is valid. Hence, validation
    of each split part is important. Usually, the data scientist will recognize and
    validate each part by their domain knowledge, some corpus or some rules. If every
    split part is valid, the string is also valid. For instance, the token ’Sep’ can
    be recognized as a valid representation of month, and ’2003’ can be recognized
    as a valid year.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Transform. The last step of standardization is to transform each split part
    and combine them into target format. In our example, because the target format
    is ”YYYY-MM-DD hh:mm:ss”, the month Sep is transformed into number 09 and recombined
    with other parts to the target "2003-09-25 10:36:28".
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c834ff80a47ae5828bc64b62acb3573.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Basic Structure of LLM-based Agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Design of Unified APIs. The goal of our API design is to enable data scientists
    to complete all the common steps of data standardization of one column with a
    single function call. The simplicity and consistency are considerdered as the
    pinciple of API design. The observation on the common steps of data standardization
    brings the type-specific API design idea. Thus, we design the API in the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | clean_type(df, column_name, target_format) |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/893b4838c0ed4f61acbb2cbe4ef4e601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. The Workflow of CleanAgent.
  prefs: []
  type: TYPE_NORMAL
- en: where clean_type is the function name, type represents the type of the current
    column. The first argument df represents the input DataFrame, the second argument
    column_name is the column needs to be standardized and the third argument target_type
    is the target standardization format users specified. Our APIs design is flexible
    and extensible, which is convenient for users to add new standardization functions
    dealing with new data types. Currently, we have 142 standardization functions
    in Dataprep.Clean, representing 142 data types.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. CleanAgent Workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the basic structure of the LLM-based agent.
    Then we describe the CleanAgent workflow constructed by four agents. The automatic
    data standardization process can be completed by the cooperation of the four agents
    in CleanAgent .
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic Structure of LLM-based Agent. According to the previous surveys on LLM-based
    Agent (Xi et al., [2023](#bib.bib4)), we conclude the basic structure of LLM-based
    Agent shown in Figure [2](#S2.F2 "Figure 2 ‣ 2\. Type-Specific Standardization
    API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents").
    An LLM-based agent includes four main components: (1) an LLM used to generate
    replies for input information (2) a memory used to store historical conversation
    messages (3) a system message defining the role of the agent (4) a set of external
    tools which can be called by the LLM-based agent to complete specific tasks, such
    as web searching, code running, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detailed Workflow. The detailed workflow of CleanAgent is shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents"). The CleanAgent is composed of four
    agents, including a Chat Manager, a Column-type Annotator, a Python Programmer
    and a Code Executor. They can communicate with each other and automatically complete
    the data standardization process by cooperation. As mentioned in Figure [2](#S2.F2
    "Figure 2 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents"), each agent has its own memory to
    store the historical conversational messages between it and other agents. It is
    important to highlight that the memory of the Chat Manager is uniquely comprehensive,
    encompassing the entire historical conversational messages from all agents within
    the CleanAgent system. This extensive memory enables every agent in the CleanAgent to
    generate responses that are informed by the complete historical messages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11d4e810b45c064f19923ce532cd8ac3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. User interface of CleanAgent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input of CleanAgent includes a table $T$ is returned. If the generated
    code executes with errors, the error message is returned to the Chat Manager and
    stored in its memory (⑥ in Figure [3](#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization
    API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")).
    Then CleanAgent will retry the whole workflow until it can complete the data standardization
    process successfully.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Demonstration Scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The demonstration setup includes a table need to be standardized and a laptop.
    The laptop must connect to the Internet for visitors can use CleanAgent smoothly
    with OpenAI’s GPT service. If the conference Internet fails, a mobile hotspot
    (established via cell phone) can also be used for running CleanAgent .
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ 3\. CleanAgent Workflow ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents") shows the user interface of CleanAgent .
    As area ① shows, users must first upload a CSV file that needs to be cleaned.
    Then CleanAgent receives and shows the basic information of the uploaded file
    (number of rows and number of columns). If users want CleanAgent to start the
    data standardization process, they should just click “Start Standardization” button.'
  prefs: []
  type: TYPE_NORMAL
- en: After clicking the “Start Standardization” button, as area ② shows, the User_Proxy
    generates three detailed steps to complete the data standardization task. Firstly,
    the Column-type Annotator receives messages from the Chat Manager, annotates and
    outputs the type of each column, as area ③ shows. Then the Python Programmer picks
    up standardization functions from Dataprep.Clean with respect to the type of each
    column, and write proper Python code to standardize columns in the input table,
    as area ④ shows. Thirdly, the Code Executor executes the Python code generated
    by the Python Programmer and collects the execution messages, as area ⑤ shows.
    If the Code Executor gets the error message when executing generated Python code,
    the error message is sent to the Chat Manager and becomes part of the prompt of
    the next try. If the Code Executor gets the message of successful execution, CleanAgent will
    report that the data standardization is completed, as area ⑥ shows. Moreover,
    users can click the “Show Cleaned Table” button to check whether the standardized
    table matches users’ requirements. If yes, users can directly download the standardized
    table. Otherwise, users can input their extra requirements with natural language.
    CleanAgent will start a new data standardization process according to users’ input.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this demo paper, we proposed CleanAgent to automate the data standardization
    process with Dataprep.Clean and LLM-based Agents. We implemented CleanAgent as
    a web service to visualize the conversations among agents. Other tasks in data
    science life cycle such as data cleaning and data visualization can also be completed
    by LLM-based agents (Xue et al., [2023](#bib.bib5)). In the future, it is promising
    that the data science life cycle can be automatically planned and completed by
    LLM-based agents’ cooperation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McKinney et al. (2024) Wes McKinney et al. 2024. pandas: powerful Python data
    analysis toolkit. [https://pandas.pydata.org/](https://pandas.pydata.org/) Accessed:
    2024-01-25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling
    Next-Gen LLM Applications via Multi-Agent Conversation Framework. *CoRR* abs/2308.08155
    (2023). [https://doi.org/10.48550/ARXIV.2308.08155](https://doi.org/10.48550/ARXIV.2308.08155)
    arXiv:2308.08155'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The
    Rise and Potential of Large Language Model Based Agents: A Survey. *CoRR* abs/2309.07864
    (2023). [https://doi.org/10.48550/ARXIV.2309.07864](https://doi.org/10.48550/ARXIV.2309.07864)
    arXiv:2309.07864'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2023) Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting
    Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang
    Zhao, Fan Zhou, Danrui Qi, Hong Yi, Shaodong Liu, and Faqiang Chen. 2023. DB-GPT:
    Empowering Database Interactions with Private Large Language Models. *CoRR* abs/2312.17449
    (2023). [https://doi.org/10.48550/ARXIV.2312.17449](https://doi.org/10.48550/ARXIV.2312.17449)
    arXiv:2312.17449'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
