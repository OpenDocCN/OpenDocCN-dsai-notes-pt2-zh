- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01887](https://ar5iv.labs.arxiv.org/html/2407.01887)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \pdfcolInitStack
  prefs: []
  type: TYPE_NORMAL
- en: tcb@breakable
  prefs: []
  type: TYPE_NORMAL
- en: Fanzeng Xia fanzengxia@link.cuhk.edu.cn, litongxin@cuhk.edu.cn Hao Liu {hliu3,
    yyue}@caltech.edu Yisong Yue {hliu3, yyue}@caltech.edu Tongxin Li fanzengxia@link.cuhk.edu.cn,
    litongxin@cuhk.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In-context decision-making is an important capability of artificial general
    intelligence, which Large Language Models (LLMs) have effectively demonstrated
    in various scenarios. However, LLMs often face challenges when dealing with numerical
    contexts, and limited attention has been paid to evaluating their performance
    through preference feedback generated by the environment. This paper investigates
    the performance of LLMs as decision-makers in the context of Dueling Bandits (DB).
    We first evaluate the performance of LLMs by comparing GPT-3.5 Turbo, GPT-4, and
    GPT-4 Turbo against established DB algorithms. Our results reveal that LLMs, particularly
    GPT-4 Turbo, quickly identify the Condorcet winner, thus outperforming existing
    state-of-the-art algorithms in terms of weak regret. Nevertheless, LLMs struggle
    to converge even when explicitly prompted to do so, and are sensitive to prompt
    variations. To overcome these issues, we introduce an LLM-augmented algorithm,
    IF-Enhanced LLM, which takes advantage of both in-context decision-making capabilities
    of LLMs and theoretical guarantees inherited from classic DB algorithms. The design
    of such an algorithm sheds light on how to enhance trustworthiness for LLMs used
    in decision-making tasks where performance robustness matters. We show that IF-Enhanced
    LLM has theoretical guarantees on both weak and strong regret. Our experimental
    results validate that IF-Enhanced LLM is robust even with noisy and adversarial
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we are particularly interested in studying the ability of Large
    Language Model (LLM) agent to behave optimally in online decision-making settings,
    i.e., have low regret. In such settings, notable failure cases have been reported [[1](#bib.bib1),
    [2](#bib.bib2)], e.g., LLM agents being vulnerable to adversarial loss functions
    and suffer from high regret compared to classic algorithms such as Follow-The-Regularized-Leader
    (FTRL) [[1](#bib.bib1)], and failures in exploration for classic Multi-Armed Bandit
    (MAB) problems [[2](#bib.bib2)]. These results suggest that carefully designed
    prompts (particularly with external summarization) and non-trivial algorithmic
    interventions are needed to elicit desirable behavior of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The failure cases encountered by LLMs may be attributed to intrinsic difficulties
    of handling numerical rewards, and there has been a notable lack of emphasis on
    evaluating the relative comparisons among the decisions they generate. To disentangle
    the complexities introduced by numerical rewards, this paper focuses on the problem
    of dueling bandits (DB) [[3](#bib.bib3)] as an important variant of the MAB problem.
    The problem of DB[[3](#bib.bib3), [4](#bib.bib4)] extends the classic MAB model[[5](#bib.bib5)]
    by querying for preference feedback between selected pairs of arms to identify
    the best one. In DB, a decision-maker chooses a pair of arms every time, but only
    observes the binary outcome (win or lose) of a noisy comparison between the two
    selected arms. Figure[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") shows a toy example illustrating
    the in-context interaction between an LLM agent and an environment. As a variant
    of the standard MAB problem, DB has attracted significant attention in recent
    years due to its applicability in information retrieval[[6](#bib.bib6)], recommendation
    systems [[7](#bib.bib7)], and online ranker evaluation [[4](#bib.bib4)]. This
    setup is particularly useful when eliciting explicit feedback is challenging or
    when the feedback is inherently comparative, like taste of food and product attractiveness [[3](#bib.bib3)].
    We frame our investigation with the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: Are LLMs effective in-context agents for solving the problem of dueling bandits?
  prefs: []
  type: TYPE_NORMAL
- en: We highlight two aspects where the capabilities and limitations of LLMs intersect
    with the challenges of the dueling bandit problem. First, dueling bandits pose
    distinctive challenges as a decision-making instance, notably due to the sparse
    nature of the relative rewards they provide. This sparsity complicates the in-context
    decision-making process, as it restricts the feedback obtained from interactions,
    introducing a level of difficulty not typically seen in conventional bandit problems.
    Even though reduction from DB to standard MAB exists [[8](#bib.bib8), [9](#bib.bib9)],
    it is unclear how LLMs would perform in DB with relative comparison feedback rather
    than numerical rewards. There are conceptual differences between them, similar
    to those for Reinforcement Learning from Human Feedback (RLHF) [[10](#bib.bib10)]
    and standard RL, where impossibility results can be found in[[11](#bib.bib11)].
    Additionally, LLMs often struggle with numerical contexts, and there has been
    insufficient focus on assessing their performance using relative comparison feedback.
    Besides these aspects, it is worth noting that dueling bandits have broad applications,
    notably in areas such as healthcare decision support[[12](#bib.bib12)], legal
    judgment prediction [[13](#bib.bib13)], and stock movement prediction [[14](#bib.bib14)].
    We summarize our main results below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation of LLMs’ capabilities for preference feedback. First, going beyond
    numeric rewards, in Section[4](#S4 "4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), we thoroughly
    evaluate the performance of LLM agents in terms of both strong and weak regret
    for making decisions in DB by comparing against various baseline DB algorithms
    via a case study. LLMs exhibit behaviors that significantly differ from those
    in classic MAB settings [[2](#bib.bib2)], as summarized in Table [1](#S4.T1 "Table
    1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents"). Notably, GPT-4 Turbo can serve as an effective decision-maker
    for dueling bandits in the short term, quickly identifying and exploiting the
    best arm with low variance across different instances. However, the long-term
    performance is hindered by over-estimation bias in the exploration stage and lack
    of convergence criterion in the exploitation stage. This highlights the need for
    more robust and effective strategies to use LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effective and Robust LLM-augmented DB algorithm. To enhance the trustworthiness
    of integrating LLMs in decision-making problems where performance robustness is
    critical, in Section[5.1](#S5.SS1 "5.1 Algorithm Design of IF-Enhanced LLM ‣ 5
    Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), we propose a design of LLM-augmented algorithm,
    IF-Enhanced LLM that combines a classic DB algorithm, Interleaved Filter2 (IF2),
    with LLM agents. We show that the proposed algorithm has theoretical guarantees,
    with experiments demonstrating its efficacy and robustness with various prompting
    scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/494de5e57126f1567c46d23cad991ee5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: In-context interaction at time $t$ between an LLM agent and a dueling
    bandit environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we briefly introduce the problem of dueling bandits (DB) and
    establish the necessary notation for this paper. Additional useful definitions
    can be found in Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas
    for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and
    Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Dueling Bandits. In a $K$.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Agents for Dueling Bandits. We consider an LLM agent with policy $\pi_{\mathrm{LLM}}$
    (see Figure[6](#A2.F6 "Figure 6 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")), consisting of three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context $C$, and the task objective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: History $H_{t}$, which includes a sequence of pairwise dueling results and the
    empirical probabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reasoning $R$: the zero-shot chain-of-thought (CoT) reasoning [[15](#bib.bib15)]
    that encourages the LLM agent to reason about the problem in a structured manner.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The LLM agent’s policy can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t)\right)=\pi_{\mathrm{LLM}}\left(\mathtt{Prompt}(C,H_{t},R)\right).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'The goal is to maximize the cumulative reward over some time horizon $T$, where
    the reward is the sum of the unknown probabilities of the two chosen arms beating
    the best arm (Condorcet winner). We can quantify performance as minimizing the
    cumulative regret, either in the strong or weak sense (see Eq.([2](#S2.E2 "In
    2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"))
    and Eq.([3](#S2.E3 "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents"))).'
  prefs: []
  type: TYPE_NORMAL
- en: Strong and Weak Regret. We assume the standard setting that a Condorcet winner
    exists[[7](#bib.bib7), [16](#bib.bib16), [4](#bib.bib4), [3](#bib.bib3)]. The
    Condorcet winner $b^{*}$. Throughout this paper, we consider two important performance
    metrics. The first is the strong regret of a given algorithm ALG, defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $T$ is the time horizon. The second is the weak regret of ALG, defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'which only compares $b^{*}$. It is worth highlighting that LLM agents exhibit
    significantly different behaviors with respect to the two defined notions of regret,
    as detailed in Section [4.2](#S4.SS2 "4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents"). More useful definitions and settings are provided in Appendix [A.2.1](#A1.SS2.SSS1
    "A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dueling Bandits. The problem of dueling bandits was initially introduced in
    [[3](#bib.bib3)]. Various methods have been proposed to tackle the task since
    then. These methods can be broadly classified into two categories as Explore-Then-Exploit
    methods and Ongoing Regret Minimization methods according to [[4](#bib.bib4)].
    Explore-Then-Exploit methods focus on identifying the best arm with high confidence
    before exploiting it, such as Interleaved Filter (IF) [[3](#bib.bib3)] and Beat
    the Mean (BTM) [[17](#bib.bib17)], etc. In contrast, Ongoing Regret Minimization
    methods explicitly target the objective of minimizing cumulative regret, including
    Relative Upper Confidence Bound (RUCB) [[18](#bib.bib18)] and Self-Sparring[[7](#bib.bib7)],
    etc. Dueling bandit problem and preference feedback in general has a wide variety
    of applications, including recommendation systems [[3](#bib.bib3)], robotics [[19](#bib.bib19)],
    and most recently, the training algorithm of large language models, such as Reinforcement
    Learning from Human Feedback (RLHF) [[20](#bib.bib20)].
  prefs: []
  type: TYPE_NORMAL
- en: LLM Agents for Multi-Armed Bandits. Several recent works have explored evaluating
    the capabilities of LLMs in bandit problems. For example, [[21](#bib.bib21)] proposed
    an approach to enhance contextual bandits by integrating LLMs as encoders. The
    LLMs’ ability to capture rich semantic and syntactic information from textual
    contexts is leveraged to provide the algorithm with a more informative representation
    of the context. The LLM-augmented algorithm transforms the raw context into a
    latent space vector using the LLM’s encoding capabilities. This encoded context
    is then used to guide the decision-making process. [[2](#bib.bib2)] investigates
    whether LLMs can engage in exploration in simple MAB environments without additional
    training. They compared various prompt designs and found that GPT-4 with zero-shot
    chain-of-thought (CoT) reasoning and an externally summarized interaction history
    performed the best, while other configurations failed in exploration, either by
    never selecting the best arm after initial rounds or by selecting all arms nearly
    equally often[[2](#bib.bib2)]. Different from the previous results, in this work
    we go beyond the settings of numeric rewards and investigate the capabilities
    of LLMs under preference feedback.
  prefs: []
  type: TYPE_NORMAL
- en: In-Context LLMs for Decision-Making. Beyond bandit problems, LLM agents have
    demonstrated strong capabilities in complex reasoning across a wide range of in-context
    decision-making tasks[[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    Various existing works aim to understand LLM agents’ capabilities for in-context
    decision-making, with notable examples including planning [[26](#bib.bib26), [27](#bib.bib27)].
    Additionally, LLM agents have been shown to enhance embodied agents in various
    robotic applications by providing advanced task planning abilities [[28](#bib.bib28)]
    and reward designing[[29](#bib.bib29)], further enabling the development of lifelong
    learning agents [[30](#bib.bib30)]. Besides these empirical successes, the authors
    of [[1](#bib.bib1)] analyzed LLMs’ interactions in online learning and game theory
    settings through the lens of the regret metrics. They identified simple cases
    where LLMs fail to be no-regret. Another line of research incorporates LLMs into
    classic decision-making frameworks to create LLM-augmented online decision-makers.
    For instance, Liu et al.[[31](#bib.bib31)] utilized LLMs to enhance the components
    of warm starting, sampling candidates, and surrogate modeling in Bayesian optimization.
    Our work contributes to this broad area by integrating LLM agents with the Interleaved
    Filter algorithm to enhance the utilization of preference feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 4 LLMs as Standalone In-Context Decision-Makers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate the efficacy of LLMs for solving DB problems, in this section,
    we use LLMs as standalone in-context decision-making agents and compare them with
    classic baseline algorithms. Our evaluation is two-fold: First, in Figures [2](#S4.F2
    "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"), we compare the performance
    of LLM agents and classic algorithms in terms of the strong and weak regret (see
    Eq. ([2](#S2.E2 "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")) and Eq. ([3](#S2.E3 "In 2 Preliminaries ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")), with standard deviation).
    Second, we delve into the experimental results and analyze the advantages and
    disadvantages of LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Implementation Details of Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prompts and Configurations of LLMs. We employ an interactive zero-shot chain-of-thought
    (CoT) prompt $\mathtt{Prompt}(C,H_{t},R)$. We adopt the prompting template that
    leads to the best performance among all prompt variations explored in a recent
    study [[2](#bib.bib2)] for the classic MAB problem. The LLM agents interact with
    dueling bandit environments in a round-based manner, with the prompt guiding their
    decision-making process. We conduct experiments with three LLMs: GPT-3.5 Turbo,
    GPT-4, and GPT-4 Turbo through the OpenAI API with temperature = 0\. The detailed
    prompt is provided in Appendix[B.1.1](#A2.SS1.SSS1 "B.1.1 Design of Prompts ‣
    B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. We compare LLMs against eight well-established baseline algorithms
    to evaluate their efficacy. The baselines include Interleaved Filter (IF2) [[3](#bib.bib3)],
    Beat the Mean (BTM) [[17](#bib.bib17)], Sensitivity Analysis of VAriables for
    Generic Exploration (SAVAGE) [[32](#bib.bib32)], Relative Upper Confidence Bound
    (RUCB) [[18](#bib.bib18)], Relative Confidence Sampling (RCS) [[4](#bib.bib4)],
    Relative Minimum Empirical Divergence (RMED) [[33](#bib.bib33)], Self-Sparring [[7](#bib.bib7)],
    and Double Thompson Sampling (DTS) [[16](#bib.bib16)]. Each of these algorithms
    employs distinct strategies for selecting arms and estimating preferences, with
    the ultimate goal of efficiently identifying the Condorcet winner. We assess the
    performance of LLMs and baseline algorithms using strong regret and weak regret
    metrics defined in Section [2](#S2 "2 Preliminaries ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Environments. We evaluate the regret performance of LLMs and baselines across
    two stochastic environments, each characterized by a distinct preference matrix
    $P$ parameters given by:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Easy instance: $\theta(1)=1,\ \theta(i)=0.5-(i-1)/2K$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hard instance: $\theta(i)=1-(i-1)/K$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note that the datasets generated in this way satisfy the Strong Stochastic
    Transitivity (SST) and Stochastic Triangle Inequality (STI) properties [[3](#bib.bib3)]
    (see Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") for more details). The settings of the BTL model used in our experiments
    also imply the existence of a Condorcet winner.'
  prefs: []
  type: TYPE_NORMAL
- en: Random Tests. The scale of our experiments is chosen to balance computational
    feasibility while preserving the ability of obtaining meaningful conclusions.
    We set the time horizon to $T=2000$ times for the baseline algorithms, enabling
    an understanding of their average behaviors and reliable performance estimates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/097221bb53277accfd3f1731acb7a9a0.png)![Refer to caption](img/ca9563161504d0368d178e7b32888c74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparisons between LLM agents and various classic DB algorithms.
    Left and Right: strong and weak regret for the Easy instance. Results for the
    Hard instance are presented in Figure [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experimental results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For brevity, we present our initial analysis focused on the Easy instance (Figure [2](#S4.F2
    "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).
    The analysis is qualitatively similar for the Hard instance (Figure [8](#A2.F8
    "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") in Appendix [B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")). We use $\gamma=0.5$ for
    RUCB, RCS and DTS. We analyze the results in terms of the strong and weak regret
    defined in Section [2](#S2 "2 Preliminaries ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strong Regret. The strong regret results in Figure[2](#S4.F2 "Figure 2 ‣ 4.1
    Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") (Left) reveal
    that among the LLMs, GPT-4 Turbo demonstrates the best performance in both the
    Easy and Hard instances. Comparing GPT-4 Turbo with the state-of-the-art baseline
    algorithms (DTS and Self-Sparring), we observe the following: (i) In the first
    $20$ time steps, GPT-4 Turbo maintains a performance level comparable to the state-of-the-art
    baselines in the Hard instance and slightly underperforms compared to the state-of-the-art
    baselines in the Easy instance, yet still outperforms all the other baseline algorithms.
    (iii) Throughout the entire time horizon, the strong regret of LLMs grows approximately
    linearly in both instances, indicating a non-converging behavior of the LLM agents.
    The results for the Hard instance (Figure[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")) are qualitatively similar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weak Regret. In terms of the weak regret metric, Figure[2](#S4.F2 "Figure 2
    ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    (Right) indicates the exceptional performance of GPT-4 Turbo. It consistently
    outperforms all other LLMs and state-of-the-art baseline algorithms throughout
    the entire time horizon in both instances, exhibiting the lowest mean weak regret
    with small variance. The results for the Hard instance (Figure [8](#A2.F8 "Figure
    8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣
    Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")) are qualitatively similar.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To more thoroughly evaluate the capabilities of LLMs, we delve into the success
    and failure cases found in our experiments, with more results presented in Appendix [B.2](#A2.SS2
    "B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). In the
    following analyses, we focus on GPT-4 Turbo, which is the top-performing LLM as
    shown in Figures [2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Advantages of LLM Agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Efficient Best Arm Identification. As evident in Figures[2](#S4.F2 "Figure
    2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") (Right), GPT-4 Turbo demonstrates
    strong zero-shot chain-of-thought (CoT) reasoning capabilities in terms of the
    weak regret. This can be attributed to its ability to quickly identify and consistently
    include the best arm in its duels. Figure [9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") in Appendix[B.2](#A2.SS2 "B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") illustrates the fraction of duels including
    the best arm across different time intervals. GPT-4 Turbo outperforms other LLMs
    and the state-of-the-art baselines throughout the entire timeline in both instances.
    These findings suggest that GPT-4 Turbo is able to effectively process the feedback
    obtained from duels and make informed decisions to identify and select the best
    arm, leading to superior weak regret performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Low Variance of Regret.  GPT-4 Turbo demonstrates low variance compared to
    other LLMs and baseline algorithms. As shown in Figure [10](#A2.F10 "Figure 10
    ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") (Left) in Appendix[B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"), GPT-4 Turbo exhibits the
    lowest average generalized variance of strong and weak regret in the Easy instance,
    suggesting that it can make consistent decisions when the preference structure
    is simple. For the Hard instance shown in Figure [10](#A2.F10 "Figure 10 ‣ B.2.1
    Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") (Right), GPT-4 Turbo has a variance level comparable
    to the state-of-the-art baseline algorithms (except BTM and SAVAGE, which are
    still in an early stage of exploration), indicating that its performance stability
    remains competitive even when faced with a more complex preference structure.
    This low variance result highlights GPT-4 Turbo’s ability to adapt to varying
    levels of difficulty and maintain a stable decision-making process in DB setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Disadvantages of LLM Agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figures [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2
    Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") and[12](#A2.F12
    "Figure 12 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") in Appendix[B.2.2](#A2.SS2.SSS2 "B.2.2
    Duel Selection Trajectory ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") present the arm selection traces of GPT-4 Turbo and state-of-the-art
    baselines on the Easy and Hard instances to analyze the failure cases. The trace
    plots suggest that GPT-4 Turbo exhibits a two-stage Explore-Then-Exploit pattern[[4](#bib.bib4)]
    with occasional robustness checks. Analysis of the two stages reveals two failure
    modes of LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploration Vulnerability. In the exploration stage, we observe that GPT-4
    Turbo tends to quickly narrow down to a small subset of arms (usually containing
    the Condorcet winner) and repeatedly compare these arms. In contrast, the baselines
    exhibit more diverse and extended exploration patterns. This behavior suggests
    that GPT-4 Turbo may overestimate the quality of arms that win their initial comparisons
    based on limited historical data. Unlike the baselines, which have explicit exploration
    mechanisms, LLMs rely on their inherent randomness (via sampling from their output
    distribution) to explore. Based on these observations, we hypothesize that if
    GPT-4 Turbo happens to sample a sequence of comparisons that favors suboptimal
    arms early on, it can get stuck comparing these arms indefinitely. To test this
    hypothesis, we conducted experiments using noisy prompts with biased history.
    Our results in Figure[12](#A2.F12 "Figure 12 ‣ LLM Agents and Baseline Algorithms
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") confirm
    that GPT-4 Turbo’s exploration strategy is indeed vulnerable to biased history
    initialization and can converge to local optima.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convergence Instability. Despite GPT-4 Turbo’s outstanding weak regret performance,
    it fails to consistently converge to a single best arm to duel against itself,
    even when the prompt setting explicitly calls for it. This behavior highlights
    a fundamental limitation of LLMs: they are primarily designed and trained for
    word token prediction rather than decision-making. Unlike baselines with explicit
    stopping conditions, GPT-4 Turbo relies on its inherent language modeling capabilities
    to determine when to stop exploring. Consequently, in the later exploitation stage,
    GPT-4 Turbo keeps comparing the same top arms without committing to a single winner
    (see Figure [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2
    Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")), reflecting
    an intrinsic uncertainty in its decision-making process for DB. The patterns learned
    during pre-training do not lead to converged decisions, even if the prompt includes
    emphatic commands, exclamation marks, quotation marks, or other forms of emphasis.
    This suggests that the language modeling objective alone may not be sufficient
    for LLMs to achieve optimal control in complex decision-making tasks like DB.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.T1.pic1" class="ltx_picture ltx_centering" height="197.41" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,197.41) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 8.89 177.45)"><foreignobject width="582.23"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Type
                                                  Experimental Results</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.89 8.89)"><foreignobject
    width="582.23" height="152.76" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Pros Best Arm Identification Condorcet Winner is identified and
    consistently selected
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: Figure[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons with Different Metrics
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") Low Variance
    Low variance across instances of varying difficulty'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: Figure[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [10](#A2.F10 "Figure 10 ‣ B.2.1 Comparisons with Different Metrics
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") Cons Exploration
    Vulnerability Overestimates the quality of initially winning arms'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: Figure[12](#A2.F12 "Figure 12 ‣ LLM Agents and Baseline Algorithms
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") Convergence
    Instability Cannot guarantee convergence as a black-box language model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: Figure[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of GPT-4 Turbo’s pros and cons in our experimental results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary of Experimental Results. Table [1](#S4.T1 "Table 1 ‣ 4.2.2 Disadvantages
    of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") summarizes
    the success and failure cases of using GPT-4 Turbo as an in-context decision-maker
    for the DB problem. GPT-4 Turbo can serve as an effective decision-maker in the
    short term, by quickly identifying and exploiting the best arm with low variance
    across different instances. However, the long-term performance is hindered by
    over-estimation bias in exploration stage and lack of convergence criterion in
    exploitation stage. This highlights the need for more robust and effective strategies
    to use LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further investigate the algorithmic behavior of LLMs in DB and develop more
    robust and effective strategies, we seek to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Q1]  Can we develop an LLM-augmented approach for DB with a theoretical guarantee?'
  prefs: []
  type: TYPE_NORMAL
- en: '[Q2]  How does it perform compared to standalone LLM agents and classic DB
    algorithms?'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Algorithm-Enhanced LLMs for Dueling Bandits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Classic DB algorithms such as Interleaved Filter 2 (IF2) [[3](#bib.bib3)] are
    known to be near-optimal, with matching regret upper and lower bounds up to multiplicative
    constants. To address the challenges identified in Table[1](#S4.T1 "Table 1 ‣
    4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") of using LLM agents as standalone decision-makers for DB, we
    propose an LLM-augmented approach to demonstrate the possibility of taking advantage
    of both LLM agents and classic DB algorithms. Our algorithm, IF-Enhanced LLM,
    enjoys both a regret guarantee and strong empirical performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Algorithm Design of IF-Enhanced LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4c5f9a2601dbbe54f173d0b5e714e35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Main components of the proposed IF-Enhanced LLM agent in Algorithm [1](#algorithm1
    "Algorithm 1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") are illustrated: (i) The blue-colored part represents the LLM phase.
    (ii) The grey-colored part indicates the IF2 phase. (iii) The Match Arms Procedure
    and IF2 Procedure are detailed in Appendix [A.1](#A1.SS1 "A.1 Detailed Procedure
    Description ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents"). (iv) The black arrows
    denote shared interactions between components. (v) The dotted arrows represent
    the input and output.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we present the design intuitions of IF-Enhanced LLM. We begin
    by discussing the limitations of a naive intervention approach and the desirable
    properties for an effective LLM augmentation framework. Based on these considerations,
    we identify IF2 [[3](#bib.bib3)] as the ideal candidate algorithm to serve as
    the foundation for IF-Enhanced LLM. Finally, we illustrate the components of IF-Enhanced
    LLM and provide a detailed algorithm description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Limitations of Naive Intervention. A straightforward approach to addressing
    the limitations of LLMs is to use a simple if-else condition that forces the LLMs
    to converge when they appear to select two identical arms, which we call the Convergence-Triggered
    (CT) intervention strategy. However, CT fails to guarantee the selection of the
    true Condorcet winner and can reinforce local optima (see Figure[13](#A2.F13 "Figure
    13 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") in Appendix[B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") for a failure example). This
    suggests that relying on the LLM’s internal convergence behavior for such a mechanism
    is not robust, as the LLM’s exploration is largely driven by its inherent sampling
    noise rather than a structured exploration policy. Thus, handling this limitation
    with theoretical guarantees remains challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Desirable Properties for LLM Augmentation. To address [Q1], we seek an algorithmic
    framework with the following properties: (i) A clear, symbolic logical structure
    that allows for easy integration with LLM suggestions. (ii) A well-defined exploration-exploitation
    trade-off that leverages the LLMs’ exploration behavior while ensuring convergence.
    (iii) Strong theoretical guarantees to maintain robustness with various prompting
    scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'IF2 as an Ideal Candidate. Among classic DB algorithms, IF2 stands out as particularly
    well-suited for LLM augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Its Explore-Then-Exploit [[4](#bib.bib4)] structure naturally aligns with the
    LLMs’ tendency to keep exploring without converging (see Figure[11](#A2.F11 "Figure
    11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")), allowing for effective leveraging the LLMs’
    exploration behavior while mitigating their exploration vulnerability and convergence
    instability (see Table [1](#S4.T1 "Table 1 ‣ 4.2.2 Disadvantages of LLM Agents
    ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its symbolic representation of the algorithm’s logic enables clear integration
    of LLM suggestions at specific points without disrupting the overall structure
    and theoretical guarantees. In contrast, algorithms like Self-Sparring [[7](#bib.bib7)]
    are less symbolic, making them less suitable for direct LLM augmentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Its strong theoretical guarantees, with an expected regret bound of $O((K/\epsilon_{\mathrm{bad}})\log
    T)$ up to constants (see Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions
    and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")), and empirical performance (see Figures [2](#S4.F2
    "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")) provide a robust foundation,
    ensuring convergence and bounded regret.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By selecting IF2 as the base algorithm and leveraging its Explore-Then-Exploit
    pattern, we address [Q1]. IF2’s alignment with the observed behavior of LLMs (see
    Figure [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")) allows for a natural integration
    that takes advantage of the LLMs’ exploration capabilities while maintaining theoretical
    guarantees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm Description. The procedures of the IF2-enhanced LLM agent are illustrated
    in Figure [3](#S5.F3 "Figure 3 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), presented in Algorithm [1](#algorithm1 "Algorithm 1 ‣ 5.1 Algorithm
    Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents") (see more details
    in Appendix[A.1](#A1.SS1 "A.1 Detailed Procedure Description ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")). IF-Enhanced LLM maintains a confidence parameter $\delta$
    that control the algorithm’s confidence of matches between arms. The key components
    of IF-Enhanced LLM are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Phase 1 (LLM Phase):  *Utilization of LLM recommended arms*: The algorithm
    maintains a set of candidate arms $B$ is defeated by another arm, indicating that
    the LLM’s suggestions are no longer trusted.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Phase 2 (IF2 Phase):  *Roll back to IF2*: If $b_{\mathrm{LLM}}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After Phase 2, the LLM-augmented agent repeats Phase 1 until $B$ only contains
    the best arm. Algorithm[1](#algorithm1 "Algorithm 1 ‣ 5.1 Algorithm Design of
    IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") and Figure[3](#S5.F3 "Figure
    3 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    summarize the phases above, with detailed procedures defined in Appendix[A.1](#A1.SS1
    "A.1 Detailed Procedure Description ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 LLM-augmented Agent: IF-Enhanced LLM
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize :'
  prefs: []
  type: TYPE_NORMAL
- en: $T,B=\{b_{1},\ldots,b_{K}\}$
  prefs: []
  type: TYPE_NORMAL
- en: while *$|B|\geq 2$* do
  prefs: []
  type: TYPE_NORMAL
- en: $\textit{TrustLLM}\leftarrow\text{True}$
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Theoretical Guarantees for IF-Enhanced LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we present two main theoretical results. First, we characterize
    the vulnerability of using standalone LLM agents for dueling bandits in Assumption [1](#Thmassumption1
    "Assumption 1 (Worst-Case Behavior). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and Theorem[5.1](#S5.Thmtheorem1 "Theorem 5.1
    (Vulnerability). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"). Then, we provide the theoretical guarantees of IF-Enhanced LLM in
    Theorem[5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical
    Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), demonstrating
    its robustness and efficacy.'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 1  (Worst-Case Behavior).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Under the original prompt (see Figure [6](#A2.F6 "Figure 6 ‣ B.1.1 Design of
    Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")), the worst-case
    behavior of an LLM agent in the dueling bandit setting is equivalent to a randomizer
    that selects actions uniformly at random.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vulnerability of Standalone LLM Agents. Inspired by the adversarial corruptions
    framework introduced in [[36](#bib.bib36)] for the classic MAB problem, we investigate
    the vulnerability of standalone LLM agents in the DB setting under adversarial
    prompts. We consider an attacker with a budget $\Phi(T)$ rounds. This adversarial
    strategy compels the LLM agent to select suboptimal arms, resulting in poor performance,
    as formalized in the following theorem with Assumption[1](#Thmassumption1 "Assumption
    1 (Worst-Case Behavior). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5
    Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 5.1  (Vulnerability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For the dueling bandits problem with $K$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof of Theorem [5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣
    5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    is provided in Appendix[A.2.2](#A1.SS2.SSS2 "A.2.2 Theoretical Guarantees of IF-Enhanced
    LLM ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced
    LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The
    theorem underscores the suboptimality of standalone LLM agents in DB problems,
    particularly when input prompts are subjected to adversarial attacks. This vulnerability
    highlights the need for a more robust approach that harnesses the power of LLMs
    while offering theoretical guarantees under diverse prompting scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected Regret Bounds of IF-Enhanced LLM. Following the algorithm design of
    IF-Enhanced LLM introduced in Section[5.1](#S5.SS1 "5.1 Algorithm Design of IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), IF-Enhanced LLM inherits the theoretical guarantees
    of IF2 (see Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas
    for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and
    Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")), while leveraging the benefits of LLM-enhanced exploration.
    Specifically, IF-Enhanced LLM offers the following theoretical guarantee:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 5.2  (Expected Regret Bounds).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose after $T_{\mathrm{LLM}}$, the expected strong regret of IF-Enhanced
    LLM satisfies that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and the expected weak regret of IF-Enhanced LLM is bounded from above by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\left[\mathsf{WR}(\textsc{IF-Enhanced LLM})\right]\leq\min\left\{O(T_{\mathrm{LLM}}),\widetilde{O}\left(\frac{K\log
    K}{\epsilon_{1,2}}\log T\right)\right\},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\widetilde{O}(\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof of Theorem [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds).
    ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    is provided in Appendix[A.2.2](#A1.SS2.SSS2 "A.2.2 Theoretical Guarantees of IF-Enhanced
    LLM ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced
    LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The
    required assumptions are precisely stated in Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1
    Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣
    Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"). Theorem [5.2](#S5.Thmtheorem2 "Theorem
    5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM
    ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") establishes the efficacy and robustness of IF-Enhanced
    LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficacy: Both the strong and weak regret for IF-Enhanced LLM are guaranteed
    to satisfy a worst-case upper bound of $\widetilde{O}\left(({K\log K}/{\epsilon_{1,2}})\log
    T\right)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Robustness: The worst-case upper bounds on the strong and weak regret hold
    regardless of the specific prompting scenario, ensuring that IF-Enhanced LLM maintains
    its theoretical guarantees even in the presence of noisy or adversarial prompts,
    as considered in Theorem[5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣
    5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").
    This robustness is particularly important in practical applications, where the
    prompts provided to the LLM agent may be subject to various uncertainties or manipulations.
    By leveraging the strengths of LLMs while incorporating a robust algorithmic framework,
    IF-Enhanced LLM offers a reliable and theoretically grounded solution for DB problems
    in real-world settings, where the prompting quality may not always be optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Empirical Evaluation of IF-Enhanced LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regarding [Q2], we design a two-fold evaluation to assess efficacy and robustness.
    The evaluation is conducted on the Easy instance, which provides higher distinguishability,
    allowing us to observe convergence and regret differences within a practical number
    of steps. First, we compare the strong regret of IF-Enhanced LLM against state-of-the-art
    baseline algorithms to validate its efficacy. Second, we investigate the robustness
    of IF-Enhanced LLM with noisy and adversarial prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.1 Efficacy Evaluation: Strong Regret and Weak Regret'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f37b49fb96bdba081c63143b3279ceda.png)![Refer to caption](img/b133b5d77bb7123a073a72d40a37d598.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparisons between IF-Enhanced LLM, GPT-4 Turbo, IF2, and state-of-the-art
    baseline algorithms (Self-Sparring and DTS). Left and Right: strong and weak regret
    on the Easy instance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyper-parameters. In our implementation of IF-Enhanced LLM (see Algorithm[1](#algorithm1
    "Algorithm 1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")), there are two hyper-parameters: the threshold parameter $t$ provided
    the best performance in terms of cumulative regret. These hyper-parameter values
    strike a balance between the number of comparisons required to identify the best
    arm and the confidence level for pruning suboptimal arms, enabling IF-Enhanced
    LLM to efficiently explore and exploit the available arms in the dueling bandits
    setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the cumulative strong and weak regret performance of the proposed
    IF-Enhanced LLM with different confidence parameter settings ($\delta=0.1,0.2,0.4$:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strong Regret. Figure[4](#S5.F4 "Figure 4 ‣ 5.3.1 Efficacy Evaluation: Strong
    Regret and Weak Regret ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") (Left) demonstrates that IF-Enhanced LLM exhibits competitive performance
    across different $\delta$ allows for more aggressive exploration, enabling IF-Enhanced
    LLM to identify the optimal arm sooner, leading to reduced cumulative strong regret.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weak Regret. Figure[4](#S5.F4 "Figure 4 ‣ 5.3.1 Efficacy Evaluation: Strong
    Regret and Weak Regret ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") (Right) showcases the superior weak regret performance of IF-Enhanced
    LLM. When $\delta=0.2$, IF-Enhanced LLM consistently achieves lower cumulative
    weak regret, demonstrating its ability to effectively identify and include the
    optimal arm in its comparisons. As emphasized in Table[1](#S4.T1 "Table 1 ‣ 4.2.2
    Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents"), this can be attributed to GPT-4 Turbo’s ability to consistently
    identify the best arm.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.2 Robustness Evaluation: Noisy and Adversarial Prompts'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0054e53e36684dc1a53e1da4ec3efd27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Robustness evaluation under prompt perturbations. The specific noisy
    (biased history) and adversarial (reversed goal) prompts are shown in Figure [7](#A2.F7
    "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent studies [[2](#bib.bib2), [37](#bib.bib37)] have emphasized the importance
    of varying prompts to elicit the desired behavior from LLMs in decision-making
    tasks, highlighting the potential limitations of prompt quality. Results obtained
    from a single prompt template may lead to unreliable conclusions that cannot generalize
    to real-world situations where optimal prompts are often unavailable. Thus, we
    evaluate the robustness of IF-Enhanced LLM in DB tasks by employing two types
    of prompt perturbations (see Figure[7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts
    ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")) along with the original
    prompt (see Figure [6](#A2.F6 "Figure 6 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Original Prompt. Under the initial prompt, IF-Enhanced LLM leverages the LLM’s
    ability to quickly identify the best arm through exploration. As shown in Figures
    [14](#A2.F14 "Figure 14 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and [15](#A2.F15 "Figure 15 ‣ IF-Enhanced LLM
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") (Top Row),
    we observe that: (i) Compared to IF2, IF-Enhanced LLM benefits from the LLM’s
    exploration ability by initializing with the best arm as the incumbent when entering
    the IF2 phase. (ii) Compared to GPT-4 Turbo, convergence to the Condorcet winner
    is guaranteed for IF-Enhanced LLM with high probability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Biased History. We inject an incorrect history into the prompt, where each
    non-optimal arm initially wins against the best arm 10 times, while keeping the
    underlying preference matrix unchanged. In this scenario, standalone LLM agents
    are observed to get trapped in local optima for extended periods. In contrast,
    IF-Enhanced LLM overcomes this limitation by employing uniform comparisons in
    the IF2 phase to escape such suboptimal exploration modes (see Figure [15](#A2.F15
    "Figure 15 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") (Middle Row)). This effectively addresses the local optima problem
    highlighted in the disadvantages of standalone LLMs in Table[1](#S4.T1 "Table
    1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reversed Goal. When the prompt is adversarially modified from maximizing reward
    to minimizing, the LLM consistently recommends non-optimal arms after its exploration
    stage. Nevertheless, even with adversarial prompts, IF-Enhanced LLM still maintains
    a robust behavior and achieves near-optimal cumulative strong regret, as shown
    in Figure [15](#A2.F15 "Figure 15 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") (Bottom Row). Since the LLM’s exploration
    capability is only utilized within the bounded length of the Match Arms procedure,
    the impact of the reversed goal on the exploitation phase is eliminated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.3.2 Robustness Evaluation: Noisy and Adversarial
    Prompts ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") presents the cumulative strong regret results comparing IF-Enhanced LLM
    against standalone LLM agents and the IF2 algorithm across three prompt designs.
    Notably, IF-Enhanced LLM with $\delta=1/(TK^{2})$ (consistent with IF2 to showcase
    the robust behavior) achieves near-optimal cumulative regret with low variance
    even with noisy and adversarial prompts, validating the regret bounds stated in
    Theorem [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical
    Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The experiment
    in this section is extended from 2000 steps to 3000 steps to illustrate GPT-4
    Turbo’s non-convergence and growing regret. IF-Enhanced LLM and IF2 converge to
    the best arm within 2000 steps, while GPT-4 Turbo’s cumulative expected regret
    continues to increase, indicating the instability of standalone LLM agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work evaluates LLM agents as in-context decision-makers for the problem
    of dueling bandits (DB) through case studies. We find that the top-performing
    LLM (GPT-4 Turbo) exhibits strong capabilities in identifying the Condorcet winner
    and achieving low weak regret while maintaining minimal variance. Yet, stemming
    from the fact that LLMs are primarily designed and trained for word token prediction
    rather than optimizing decision-making objectives, GPT-4 Turbo struggles to consistently
    converge to a single optimal solution in the long run and is vulnerable to prompt
    design variations. To address these limitations, we propose IF-Enhanced LLM, an
    LLM-augmented approach that takes advantages of both LLMs and classic DB algorithms,
    providing theoretical guarantees and robust empirical performance even with noisy
    and adversarial prompts. Our findings shed light on the capabilities and challenges
    of LLMs as in-context decision-makers, and pave the way for the employment of
    LLMs in complex decision-making tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Limitations and Future Works. Due to computational constraints and financial
    costs, our case studies focus on a relatively small number of arms ($K=5$). To
    address this limitation, it is worth investigating the scalability of the proposed
    approach to larger decision spaces with more arms as certified by Theorem [5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"). In addition, while IF2 is employed as a representative
    Explore-Then-Exploit[[18](#bib.bib18)] algorithm to design an LLM-augmented algorithm
    in this work, utilizing other ongoing regret-minimization algorithms, such as
    RUCB [[18](#bib.bib18)], would be an interesting future direction. Moreover, considering
    more complex preference structures, such as non-transitive preferences, would
    provide insights on algorithm design. Investigating other variants of dueling
    bandits, including contextual dueling bandits, multi-dueling bandits, and adversarial
    dueling bandits, would further expand the applicability of this work. Finally,
    we have only experimented with some of the models available to us as of May 2024,
    selecting high-profile closed-source models over open-source models. Exploring
    the performance of a wider range of LLMs, including open-source models, would
    provide a more comprehensive understanding of the capabilities and limitations
    of LLMs in the dueling bandits setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Broader Impacts. The ability of LLMs to encode complex contextual information
    enables their application in various decision-making scenarios beyond the scope
    of traditional DB algorithms. This could lead to more informed and context-aware
    decision-making in domains such as healthcare, finance, and recommender systems,
    where incorporating user feedback and adaptivity is crucial for handling dynamic
    relative preferences. However, the substantial computational resources required
    to train and deploy large-scale LLMs for decision-making tasks raise concerns
    about energy consumption and environmental impact, especially when dealing with
    a large set of arms.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang. Do llm agents
    have regret? a case study in online learning and games. arXiv preprint arXiv:2403.16843,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, and Aleksandrs
    Slivkins. Can large language models explore in-context? arXiv preprint arXiv:2403.15371,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The
    k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538--1556,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and Remi Munos. Relative
    confidence sampling for efficient on-line ranker evaluation. In Proceedings of
    the 7th ACM international conference on Web search and data mining, pages 73--82,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University
    Press, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Yisong Yue and Thorsten Joachims. Interactively optimizing information
    retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual
    International Conference on Machine Learning, pages 1201--1208, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue. Multi-dueling
    bandits with dependent arms. arXiv preprint arXiv:1705.00253, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits
    to cardinal bandits. In International Conference on Machine Learning, pages 856--864\.
    PMLR, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both-world
    analyses for online learning from preferences. In ICML 2022-39th International
    Conference on Machine Learning, pages 1--25, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize
    with human feedback. Advances in Neural Information Processing Systems, 33:3008--3021,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult than standard
    rl? a theoretical perspective. Advances in Neural Information Processing Systems,
    36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Roy H Perlis, Joseph F Goldberg, Michael J Ostacher, and Christopher D
    Schneck. Clinical decision support for bipolar depression using large language
    models. Neuropsychopharmacology, pages 1--5, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Cong Jiang and Xiaolei Yang. Legal syllogism prompting: Teaching large
    language models for legal judgment prediction. In Proceedings of the Nineteenth
    International Conference on Artificial Intelligence and Law, pages 417--421, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. The
    wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement
    prediction challenges. arXiv preprint arXiv:2304.05351, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
    Iwasawa. Large language models are zero-shot reasoners. Advances in neural information
    processing systems, 35:22199--22213, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. Advances
    in neural information processing systems, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Yisong Yue and Thorsten Joachims. Beat the mean bandit. In Proceedings
    of the 28th international conference on machine learning (ICML-11), pages 241--248\.
    Citeseer, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative
    upper confidence bound for the k-armed dueling bandit problem. In International
    conference on machine learning, pages 10--18\. PMLR, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Maegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui, Yisong Yue,
    Joel W Burdick, and Aaron D Ames. Preference-based learning for exoskeleton gait
    optimization. In 2020 IEEE international conference on robotics and automation
    (ICRA), pages 2351--2357\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in neural information processing systems, 35:27730--27744, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Ali Baheri and Cecilia O Alm. Llms-augmented contextual bandit. arXiv
    preprint arXiv:2311.02268, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824--24837,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
    reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi
    Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most
    prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan
    Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with
    large language models. Advances in Neural Information Processing Systems, 36,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,
    Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue:
    Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    and Zhiting Hu. Reasoning with language model is planning with world model. arXiv
    preprint arXiv:2305.14992, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander
    Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do
    as i can, not as i say: Grounding language in robotic affordances. In Conference
    on robot learning, pages 287--318\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani,
    Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level
    reward design via coding large language models. arXiv preprint arXiv:2310.12931,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar.
    Large language models to enhance bayesian optimization. arXiv preprint arXiv:2402.03921,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Tanguy Urvoy, Fabrice Clerot, Raphael Féraud, and Sami Naamane. Generic
    exploration and k-armed voting bandits. In International conference on machine
    learning, pages 91--99\. PMLR, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret
    lower bound and optimal algorithm in dueling bandit problem. In Conference on
    learning theory, pages 1141--1154\. PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block
    designs: I. the method of paired comparisons. Biometrika, 39(3/4):324--345, 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical
    Society Series C: Applied Statistics, 24(2):193--202, 1975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John Lui, Wing Shing Wong,
    et al. Adversarial bandits with corruptions: Regret lower bound and no-regret
    algorithm. Advances in Neural Information Processing Systems, 33:19943--19952,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Manikanta Loya, Divya Anand Sinha, and Richard Futrell. Exploring the
    sensitivity of llms’ decision-making capabilities: Insights from prompt variation
    and hyperparameters. arXiv preprint arXiv:2312.17476, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This appendix provides supplementary information and additional experimental
    results to support the main text. The content is organized into two main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Theoretical Part: Algorithm Design and Analysis of IF-Enhanced LLM'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix [A.1](#A1.SS1 "A.1 Detailed Procedure Description ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents") describes the IF-Enhanced LLM algorithm stated in Section[5.1](#S5.SS1
    "5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),
    detailing its key features and implementation remarks.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") presents the necessary assumptions and lemmas for the theoretical analysis
    of IF-Enhanced LLM in Section[5.2](#S5.SS2 "5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix [A.2.2](#A1.SS2.SSS2 "A.2.2 Theoretical Guarantees of IF-Enhanced
    LLM ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced
    LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") proves
    Theorem[5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣ 5.2 Theoretical Guarantees
    for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") and[5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), establishing IF-Enhanced LLM’s regret bounds.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experimental Part: Prompt Design and Supplementary Results'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix [B.1.1](#A2.SS1.SSS1 "B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") illustrates the prompt design and
    prompt perturbations logic.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix [B.1.2](#A2.SS1.SSS2 "B.1.2 Exemplars of GPT-4 Turbo ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") provides a five consecutive time
    steps example of GPT-4 Turbo to showcase its behavior.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appendix [B.2](#A2.SS2 "B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") presents supplementary experimental results, providing further
    insights into the performance and behavior of the algorithms in Sections[4](#S4
    "4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and [5](#S5 "5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix A Algorithm Design and Analysis of IF-Enhanced LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we detail the design principles and implementation of the
    IF-Enhanced LLM algorithm. We also provide a rigorous proof of Theorem[5.1](#S5.Thmtheorem1
    "Theorem 5.1 (Vulnerability). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM
    ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and[5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected
    Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), establishing the theoretical guarantees of IF-Enhanced LLM under
    the assumptions outlined in Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions
    and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Detailed Procedure Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Procedure [1](#algorithm1a "Procedure 1 ‣ A.1 Detailed Procedure Description
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") below, we describe the Match
    Arms procedure used in IF-Enhanced LLM (see Algorithm[1](#algorithm1 "Algorithm
    1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and Figure[3](#S5.F3 "Figure 3 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Match Arms
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Two arms $a,a^{\prime}$'
  prefs: []
  type: TYPE_NORMAL
- en: if *$a\neq a^{\prime}$* then
  prefs: []
  type: TYPE_NORMAL
- en: while *$\nexists\ (b,b^{\prime})\in B$*
  prefs: []
  type: TYPE_NORMAL
- en: We reprise the IF2 procedure in[[3](#bib.bib3)] below to complement the presentation
    of IF-Enhanced LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 IF2 Procedure
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Incumbent arm $a$'
  prefs: []
  type: TYPE_NORMAL
- en: if *$t\leq(8K/\epsilon_{1,2}^{2})\log(TK)$* then
  prefs: []
  type: TYPE_NORMAL
- en: while *$\nexists\ (b,b^{\prime})\in B$*
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting the following features of Algorithm [1](#algorithm1 "Algorithm
    1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    in its practical implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The LLM Phase allows for flexible exploration design within the bounded length
    of the Match Arms procedure, not limiting the number of prompts and comparisons
    performed by the LLM to identify an empirically best arm.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The bound length in the Match Arms procedure can be adjusted based on empirical
    requirements. Modifying the confidence parameter $\delta$ will affect the regret
    bound and the algorithm’s performance. These parameters can be tuned to balance
    exploration and exploitation, depending on the specific application and desired
    level of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.2.1 Useful Assumptions and Lemmas for Dueling Bandits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We introduce the useful assumptions and lemmas for Dueling Bandits that are
    necessary for the theoretical analysis of our proposed algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 2  (Total Ordering).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preference matrix $P=(\epsilon_{ij})$.
  prefs: []
  type: TYPE_NORMAL
- en: With the TO property satisfied, we assume the preference matrix $P$ further
    satisfies the following two standard properties[[6](#bib.bib6), [17](#bib.bib17),
    [3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 3  (Strong Stochastic Transitivity).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preference matrix $P=(\epsilon_{ij})$.
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 4  (Stochastic Triangle Inequality).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preference matrix $P=(\epsilon_{ij})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the Bradley-Terry-Luce (BTL) model [[34](#bib.bib34)] used in our
    experiments[4.1](#S4.SS1 "4.1 Implementation Details of Experiments ‣ 4 LLMs as
    Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents") satisfies Assumption[3](#Thmassumption3 "Assumption
    3 (Strong Stochastic Transitivity). ‣ A.2.1 Useful Assumptions and Lemmas for
    Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") and[4](#Thmassumption4 "Assumption 4 (Stochastic Triangle Inequality).
    ‣ A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"). We restate the following
    theoretical guarantees for IF2 that is useful in the proof of Theorem[5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"). Let $\epsilon_{\mathrm{bad}}\coloneq\min_{b\neq
    b^{*}}\epsilon(b,b^{*})$.'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 1  (Theorem 2 in [[3](#bib.bib3)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assuming the preference matrix $P$ satisfies the SST and STI, then IF2 has its
    expected regret (both weak and strong) bounded from above by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{IF2})]\leq O\left(\frac{K}{\epsilon_{\mathrm{bad}}}\log
    T\right).$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: The following expected regret bound achieved by IF2 is tight up to multiplicative
    constants, as indicated by the lower bound (Theorem 4) in[[3](#bib.bib3)] such
    that any algorithm Alg for DB satisfies $\mathbb{E}[\mathsf{SR}(\textsc{Alg})]=\Omega\left((K/\epsilon_{\mathrm{bad}})\log
    T\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 Theoretical Guarantees of IF-Enhanced LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Part I: Vulnerability of Standalone LLM Agents'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof of Theorem [5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣ 5.2
    Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the following Dueling Bandit instance with $K\geq 3$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$P_{i,j}=\begin{cases}0.5+\epsilon,&amp;\text{if }b_{i}=b^{*}\text{ and
    }b_{j}\neq b^{*},\\ 0.5-\epsilon,&amp;\text{if }b_{i}\neq b^{*}\text{ and }b_{j}=b^{*},\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0.5,&amp;\text{otherwise.}\end{cases}$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $0<\epsilon<0.5$.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider an attacker strategy with budget $\Phi(T)$ rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Let $N(T)$.
  prefs: []
  type: TYPE_NORMAL
- en: In the rounds where $b^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the expected cumulative regret of the LLM agent after $T$ rounds is at
    least:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the first inequality follows from the regret incurred in rounds where
    $b^{*}$ is eliminated from the duel, and the second inequality holds due to Jensen’s
    inequality and the linearity of expectation.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the Assumption [1](#Thmassumption1 "Assumption 1 (Worst-Case Behavior).
    ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),
    in the worst case, the LLM agent’s behavior is equivalent to randomly selecting
    a duel in each round. For $K$. The regret bound becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}[\text{Regret}(T)]\geq p\cdot\min\left\{\frac{2T}{K},\Phi(T)\right\}\cdot\epsilon=\Omega\left(\min\left\{\frac{T}{K},\Phi(T)\right\}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, any standalone LLM agent whose policy is represented by Eq. ([1](#S2.E1
    "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")) under the worst-case assumption will suffer an expected regret of $\Omega\left(\min\left\{\Phi(T),\frac{T}{K}\right\}\right)$.
    This lower bound demonstrates the vulnerability of solely relying on LLM agents
    for DB in adversarial environments when the attacker can manipulate the input
    prompts. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part II: Expected Regret Bounds of IF-Enhanced LLM'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose at each step $t\leq T$. Before proceeding to prove Theorem [5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), we first state a useful lemma from [[3](#bib.bib3)]
    as a result of the Hoeffding’s inequality.'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 2  (Generalized Lemma 1 in [[3](#bib.bib3)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With a confidence parameter $\delta=1/(K^{2}\log T)$ number of comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that Lemma [2](#Thmlemma2 "Lemma 2 (Generalized Lemma 1 in [3]). ‣ A.2.2
    Theoretical Guarantees of IF-Enhanced LLM ‣ A.2 Theoretical Analysis ‣ Appendix
    A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") can be directly implied by Lemma 1 in[[3](#bib.bib3)].
    Now, under Assumption [3](#Thmassumption3 "Assumption 3 (Strong Stochastic Transitivity).
    ‣ A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") and[4](#Thmassumption4 "Assumption
    4 (Stochastic Triangle Inequality). ‣ A.2.1 Useful Assumptions and Lemmas for
    Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") such that the preference matrix $P$ satisfies the SST and STI properties,
    we prove Theorem[5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds). ‣
    5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof of Theorem [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds).
    ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose the arms suggested by LLM agent includes the best arm $b^{*}$ steps.
    We prove the two bounds shown in Theorem [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected
    Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") one-by-one.'
  prefs: []
  type: TYPE_NORMAL
- en: First Bound. The first $T_{\mathrm{LLM}}$ can be bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{IF-Enhanced LLM}\text{
    after }T_{\mathrm{LLM}})]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\leq$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\ $ |  |'
  prefs: []
  type: TYPE_TB
- en: since there are at most $K+1$ matches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second Bound. Furthermore, consider the adversarial selection of arms from
    the LLM agent. According to Lemma [2](#Thmlemma2 "Lemma 2 (Generalized Lemma 1
    in [3]). ‣ A.2.2 Theoretical Guarantees of IF-Enhanced LLM ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"), the IF2 procedure with $\smash{O((K/\epsilon_{1,2}^{2})\log(TK))}$
    comparisons of pairs. Applying the expected regret bound in Lemma [1](#Thmlemma1
    "Lemma 1 (Theorem 2 in [3]). ‣ A.2.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") completes the proof.'
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Prompt Design and Supplementary Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 LLM Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide the detailed design of the prompts used in our experiments
    and provide additional results to support our findings. We begin by presenting
    the original prompt used in the LLM-Env interaction. We then introduce the perturbed
    prompts, which include both noisy and adversarial variations to test the robustness
    of our approach. Finally, we provide a five consecutive time steps example of
    GPT-4 Turbo using original prompt to showcase its behavior in the dueling bandits
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: B.1.1 Design of Prompts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de5b13870ada0a533dc09a843dd3e3a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Original prompt for LLM-Env interaction in dueling bandit setting
    with temperature = 0, including context $C$ (see Section [2](#S2 "2 Preliminaries
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83caf2726cca83ac1cb38f96a3a8f110.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Perturbed prompts used to evaluate the robustness of IF-Enhanced
    LLM. The Biased History prompt (Left) injects an incorrect dueling history favoring
    non-optimal arms, while the Reversed Goal prompt (Right) adversarially modifies
    the objective from maximizing to minimizing reward. Both prompts maintain the
    zero-shot chain-of-thought (CoT) reasoning, and temperature setting of 0.'
  prefs: []
  type: TYPE_NORMAL
- en: B.1.2 Exemplars of GPT-4 Turbo
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We provide an example of five consecutive time steps using the original prompt
    (see Figure [6](#A2.F6 "Figure 6 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")) to demonstrate GPT-4 Turbo’s decision-making
    process in the dueling bandits setting. This example highlights GPT-4 Turbo’s
    capability to analyze available information, reason about the relative strengths
    of the arms, and make informed decisions on which pair of arms to duel next.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A2.SS1.SSS2.p2.pic1" class="ltx_picture" height="45341.08" overflow="visible"
    version="1.1" width="45341.08"><g transform="translate(0,45341.08) matrix(1 0
    0 -1 0 0) translate(22670.54,0) translate(0,22670.54)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-rule="evenodd"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.07 13.19)"><foreignobject width="557.86" height="163.28" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer at Time Step 0: 1\.
    Analysis: Since no duels have been conducted yet, all empirical probabilities
    between arms are at their initial value of 0.50, indicating no preference or historical
    data to suggest one arm is better than another. To begin identifying the Condorcet
    Winner, we need to start collecting data by conducting duels between different
    arms. A balanced approach would be to start dueling pairs that have not yet competed,
    but since all pairs are at the same starting point, we can choose any pair. 2\.
    Next Pair: (1, 2)<svg id="A2.SS1.SSS2.p3.pic1" class="ltx_picture" height="45341.08"
    overflow="visible" version="1.1" width="45341.08"><g transform="translate(0,45341.08)
    matrix(1 0 0 -1 0 0) translate(22670.54,0) translate(0,22670.54)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-rule="evenodd"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.07 13.19)"><foreignobject width="557.86"
    height="296.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    at Time Step 1: 1\. Analysis: From the results provided, the only conclusive duel
    so far is between Arm 1 and Arm 2, where Arm 1 won, giving P(1 > 2) = 1.00\. This
    suggests that Arm 1 might be stronger than Arm 2, but more data is needed to confirm
    its strength relative to other arms. All other pairwise comparisons have not yet
    resulted in any duels, so their probabilities remain at the initial estimate of
    0.50, indicating no preference between the arms based on the data. To better understand
    the relative strengths of the arms and potentially identify the Condorcet Winner,
    it is crucial to increase the number of duels among arms with less information.
    2\. Next Pair: Given that Arm 1 has shown some potential by winning against Arm
    2, it would be informative to test Arm 1 against another arm. Since Arm 1 has
    not yet been tested against Arm 3, Arm 4, or Arm 5 in a conclusive duel, any of
    these could be a good choice. However, to balance the exploration among other
    arms and gather more data on their performance, let’s choose a pair that has not
    been tested yet. Arm 3 and Arm 4 have not dueled conclusively, and understanding
    their relative strength could provide insights into the overall hierarchy of the
    arms. Choose: (Arm 3, Arm 4)<svg id="A2.SS1.SSS2.p4.pic1" class="ltx_picture"
    height="45341.08" overflow="visible" version="1.1" width="45341.08"><g transform="translate(0,45341.08)
    matrix(1 0 0 -1 0 0) translate(22670.54,0) translate(0,22670.54)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-rule="evenodd"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.07 13.19)"><foreignobject width="557.86"
    height="213.09" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    at Time Step 2: 1\. Analysis: - Arm 1 vs Arm 2: Arm 1 has a clear advantage over
    Arm 2 with a win probability of 1.00. - Arm 3 vs Arm 4: Arm 3 has a clear advantage
    over Arm 4 with a win probability of 1.00. - Other Duels: All other duels have
    resulted in a 50% win probability for each arm, indicating no clear advantage
    based on the limited data available. Given these results, Arm 1 and Arm 3 appear
    to be stronger contenders in their respective duels. However, more data is needed
    to confidently identify the Condorcet Winner. To gather more information, it would
    be strategic to pit arms against each other where the win probabilities are still
    uncertain (i.e., 0.50), especially focusing on those arms that have shown potential
    strength. 2\. Next Pair: (Arm 1, Arm 3)<svg id="A2.SS1.SSS2.p5.pic1" class="ltx_picture"
    height="45341.08" overflow="visible" version="1.1" width="45341.08"><g transform="translate(0,45341.08)
    matrix(1 0 0 -1 0 0) translate(22670.54,0) translate(0,22670.54)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g fill-rule="evenodd"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.07 13.19)"><foreignobject width="557.86"
    height="262.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer
    at Time Step 3: 1\. Analysis: - Arm 1 has shown strong performance against Arm
    2 and Arm 3, winning all duels against them. However, it has not yet won against
    Arm 4 and Arm 5, with the results still uncertain (P=0.50). - Arm 3 has won against
    Arm 4, suggesting it might be stronger than Arm 4. - Arms 4 and 5 have not won
    any duels yet, and their probabilities against other arms are still at 0.50, indicating
    uncertainty and lack of data. - Arms 2, 4, and 5 have not won any duels, and their
    probabilities against other arms are mostly at 0.50, indicating a need for more
    data to determine their strengths. Given this analysis, it seems prudent to focus
    on gathering more information about the arms that have uncertain outcomes and
    have not shown strong winning probabilities. Specifically, more data is needed
    for Arm 1 against Arms 4 and 5, as well as between Arms 4 and 5 themselves, to
    better understand their relative strengths. 2\. Next Pair: (Arm 1, Arm 4)<svg
    id="A2.SS1.SSS2.p6.pic1" class="ltx_picture" height="45341.08" overflow="visible"
    version="1.1" width="45341.08"><g transform="translate(0,45341.08) matrix(1 0
    0 -1 0 0) translate(22670.54,0) translate(0,22670.54)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g fill-rule="evenodd"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.07 13.19)"><foreignobject width="557.86" height="279.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Answer at Time Step 4: 1\.
    Analysis: - Arm 1 has shown strong performance against Arms 2, 3, and 4, winning
    all duels against them. However, it has not yet won against Arm 5, with the probability
    of winning currently estimated at 0.50. - Arm 3 has also shown some strength by
    winning against Arm 4. - The other arms (2, 4, 5) have not won any duels yet,
    and their probabilities of winning against each other are all estimated at 0.50,
    indicating no clear evidence of superiority among them. - Given the current data,
    Arm 1 appears to be a strong candidate for the Condorcet Winner, but more evidence
    is needed, particularly in its performance against Arm 5. To further clarify the
    potential of Arm 1 and to explore the capabilities of Arm 5, which has not lost
    yet but also has not won (except in an undecided duel against Arm 1), it would
    be strategic to have them duel again. This will help in determining if Arm 1 can
    consistently outperform Arm 5 or if Arm 5 has potential that hasn’t been fully
    revealed yet. 2\. Next Pair: (Arm 1, Arm 5)'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Supplementary Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: B.2.1 Comparisons with Different Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present supplementary results to complement our case studies. (i) Figure [8](#A2.F8
    "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") shows the strong and weak regret
    comparisons for the Hard instance. (ii) Figure[9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") introduces the Best Arm Ratio, measuring how often the algorithms
    select the best arm (Condorcet winner) in a duel for both instances. (iii) Figure[10](#A2.F10
    "Figure 10 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") examines the generalized variance
    of the strong and weak regret for both instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39d84c5958672e46020c4be2d5e6a223.png)![Refer to caption](img/17ef2c5d735c9eb02482faa5ad558cec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparisons between LLM agents and various classic DB algorithms.
    Left and Right: strong and weak regret for the Hard instance. Results for the
    Easy instance is presented in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Implementation
    Details of Experiments ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e79459b60cbc4b5f77508b9222633ad2.png)![Refer to caption](img/9145a6d3edcc28dbfb533dfef85b49dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The Best Arm Ratio represents the fraction of duels that include
    the best arm (Condorcet winner) over different time intervals in the Easy instance
    (Left) and the Hard instance (Right). This metric measures how often an algorithm
    selects the best arm to participate in a duel. A higher Best Arm Ratio indicates
    that the algorithm is more effective at identifying and exploiting the best arm.
    Three LLMs (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo) and two state-of-the-art baselines
    (Self-Sparring and DTS) are compared against each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a78d9f88ceb8d532bb44ab1592f8808.png)![Refer to caption](img/e7b3d8f8d51d56881c050271860542a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Comparison of the generalized variance of strong and weak regret
    between LLMs and baseline algorithms on the Easy (Left) and Hard (Right) instances.
    In the Easy instance, GPT-4 Turbo exhibits the lowest average generalized variance.
    For the Hard instance, GPT-4 Turbo maintains a variance level comparable to state-of-the-art
    baseline algorithms (except BTM and SAVAGE, which are in an early exploration
    stage).'
  prefs: []
  type: TYPE_NORMAL
- en: B.2.2 Duel Selection Trajectory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We visualize the duel selection trajectory in representative experiments to
    better understand the behavior of LLM agents and baseline algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Duel Selection Trajectory Explanation: The reshuffled arm order is $b_{5}\succ
    b_{3}\succ b_{2}\succ b_{1}\succ b_{4}$, with arm indices from bottom to top:
    5, 4, 3, 2, 1\. Each filled black cell represents a selected arm at that time
    step. For instance, black lines in arms 5 and 3 indicate the selection of the
    duel between (arm 5, arm 3) at that particular time step.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Agents and Baseline Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed42d5de3ab2e7c58eb96536d2c9516e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Comparison of duel selection trajectories among GPT-4 Turbo, Self-Sparring,
    and DTS on the Easy (Top Row) and Hard (Bottom Row) instances. The decision trajectories
    of GPT-4 Turbo exhibit a clear pattern of continuous exploration without converging
    to the best arm. In contrast, Self-Sparring and DTS demonstrate more structured
    exploration patterns and convergence properties on both instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b066c3c8835e363d4c6dbcfbad3b724.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Local optima trajectories of GPT-3.5 Turbo (Left), GPT-4 (Middle),
    and GPT-4 Turbo (Right, with noisy prompt) on the Hard instance. Less capable
    LLMs, such as GPT-3.5 Turbo and GPT-4, could get stuck comparing suboptimal arms
    on hard preference structure. Even for GPT-4 Turbo, noisy prompts with biased
    history (see Figure [7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")) can lead it to be trapped in bad
    tournaments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f7690d0e459b3dc6b3c76f25d527d61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Comparison of success (Left) and failure (Right) cases for the Convergence-Triggered
    GPT-4 Turbo intervention strategy discussed in Section [5.1](#S5.SS1 "5.1 Algorithm
    Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents"). While it works for
    most cases due to GPT-4 Turbo’s strong capability (Left), sometimes this naive
    intervention can reinforce suboptimal choices (Right) on the Hard instance.'
  prefs: []
  type: TYPE_NORMAL
- en: IF-Enhanced LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6abdd423fcee43d588c9ff02a16370ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Trajectory of the classic IF2 algorithm with different initial incumbent
    arm selections. The left plot shows the behavior of IF2 when a good initial incumbent
    arm is randomly selected, leading to faster convergence and better overall performance.
    In contrast, the right plot illustrates the case where a bad initial incumbent
    arm is chosen, resulting in slower convergence and suboptimal performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3830a04acf5915464e4514a7e1980d21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Duel selection trajectory comparison between GPT-4 Turbo and IF-Enhanced
    LLM under different prompt settings (see Figures [6](#A2.F6 "Figure 6 ‣ B.1.1
    Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and [7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")). (i) Top Row: With the original prompt, IF-Enhanced
    LLM leverages GPT-4 Turbo’s exploration ability and guarantees convergence through
    the IF2 phase. (ii) Middle Row: With a noisy prompt (biased history), IF-Enhanced
    LLM overcomes the limitation of standalone GPT-4 Turbo getting trapped in local
    optima by employing uniform comparisons in the IF2 phase. (iii) Bottom Row: With
    an adversarial prompt (reversed goal), IF-Enhanced LLM maintains robust behavior
    despite the adversarial modification. Across all scenarios, IF-Enhanced LLM demonstrates
    superior performance and robustness compared to standalone GPT-4 Turbo.</foreignobject></g></g></g></svg></foreignobject></g></g></g></svg></foreignobject></g></g></g></svg></foreignobject></g></g></g></svg></foreignobject></g></g></g></svg></foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
