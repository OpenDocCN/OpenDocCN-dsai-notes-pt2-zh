- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'AIOS: LLM Agent Operating System'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.16971](https://ar5iv.labs.arxiv.org/html/2403.16971)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kai Mei
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Rutgers University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Zelong Li
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Rutgers University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Shuyuan Xu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Rutgers University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Ruosong Ye
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Rutgers University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Yingqiang Ge
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Rutgers University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Yongfeng Zhang
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Rutgers University ^∗Author Affiliations: Department of Computer Science, Rutgers
    University, New Brunswick, NJ 08854; Author Emails: kai.mei, zelong.li, shuyuan.xu,
    ruosong.ye, yingqiang.ge, yongfeng.zhang@rutgers.edu'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The integration and deployment of large language model (LLM)-based intelligent
    agents have been fraught with challenges that compromise their efficiency and
    efficacy. Among these issues are sub-optimal scheduling and resource allocation
    of agent requests over the LLM, the difficulties in maintaining context during
    interactions between agent and LLM, and the complexities inherent in integrating
    heterogeneous agents with different capabilities and specializations. The rapid
    increase of agent quantity and complexity further exacerbates these issues, often
    leading to bottlenecks and sub-optimal utilization of resources. Inspired by these
    challenges, this paper presents AIOS, an LLM agent operating system, which embeds
    large language model into operating systems (OS) as the brain of the OS, enabling
    an operating system “with soul”—an important step towards AGI. Specifically, AIOS
    is designed to optimize resource allocation, facilitate context switch across
    agents, enable concurrent execution of agents, provide tool service for agents,
    and maintain access control for agents. We present the architecture of such an
    operating system, outline the core challenges it aims to resolve, and provide
    the basic design and implementation of the AIOS. Our experiments on concurrent
    execution of multiple agents demonstrate the reliability and efficiency of our
    AIOS modules. Through this, we aim to not only improve the performance and efficiency
    of LLM agents but also to pioneer for better development and deployment of the
    AIOS ecosystem in the future. The project is open-source at [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of autonomous agents, research endeavors [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)] are directed towards systems that can operate independently, make
    decisions, and perform tasks with no or minimal human intervention. These agents
    are designed to understand instructions, process information, make decisions and
    take action to achieve a state of autonomy. The advent of large language models
    (LLMs) [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] has brought new possibilities
    to the agent development [[7](#bib.bib7)]. Current LLMs have shown great power
    in understanding instructions [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)], reasoning and solving problems [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)], and interacting with human
    users [[17](#bib.bib17)] as well as external environments [[18](#bib.bib18), [19](#bib.bib19)].
    Built upon these powerful LLMs, emergent LLM-based agents [[7](#bib.bib7), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)] can present strong task fulfillment abilities
    in diverse environments, ranging from virtual assistants to more sophisticated
    systems involving complex and creative problem solving, planning and reasoning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主智能体领域，研究工作[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]致力于开发可以独立操作、做出决策并执行任务的系统，这些系统不需要或只需最少的人类干预。这些智能体被设计成能够理解指令、处理信息、做出决策并采取行动以实现自主状态。大型语言模型（LLMs）的出现[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]为智能体的发展带来了新的可能性[[7](#bib.bib7)]。当前的LLMs在理解指令[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]、推理和解决问题[[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]以及与人类用户[[17](#bib.bib17)]和外部环境[[18](#bib.bib18),
    [19](#bib.bib19)]互动方面展现了强大的能力。基于这些强大的LLMs，涌现出的LLM智能体[[7](#bib.bib7), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]在各种环境中展现了强大的任务完成能力，从虚拟助手到涉及复杂和创造性问题解决、规划和推理的更复杂系统。
- en: 'One compelling example of how an LLM-based agent solves real-world tasks can
    be seen from [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AIOS: LLM Agent Operating
    System"). Given the trip organization request from the user, the travel agent
    decomposes the task into executable steps. Then, it follows the steps sequentially
    to book flights, reserve hotels, process payments, and update calendars based
    on the user’s preferences. During the plan execution, agents show the reasoning
    and decision-making abilities, which sets it apart from the traditional software
    applications that are constrained to a pre-defined set of functions or workflow.
    To realize this travel scenario, the agent needs to interact with both LLM services
    (e.g, retrieving and understanding user preferences, deciding which tool API to
    call, generating reviews and responses) and traditional operating system (OS)
    services (e.g., accessing disk driver and executing software).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '一个引人注目的例子是基于LLM的智能体如何解决现实世界任务，这可以从[图1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    AIOS: LLM Agent Operating System")中看到。根据用户的旅行组织请求，旅行代理将任务分解成可执行的步骤。然后，它按照这些步骤顺序进行，包括预订航班、预订酒店、处理支付和根据用户的偏好更新日历。在计划执行过程中，智能体展示了推理和决策能力，这使其区别于受限于预定义功能或工作流程的传统软件应用。要实现这一旅行场景，智能体需要与LLM服务（例如，检索和理解用户偏好、决定调用哪个工具API、生成评论和响应）和传统操作系统（OS）服务（例如，访问磁盘驱动程序和执行软件）进行交互。'
- en: '![Refer to caption](img/004ae1e187f5d160d7851080b3d1f342.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/004ae1e187f5d160d7851080b3d1f342.png)'
- en: 'Figure 1: A motivating example of how an agent (i.e., Travel Agent) requires
    both LLM level and OS level resources and functions to complete a task.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个激励性例子，展示了一个智能体（即旅行代理）如何需要LLM级别和OS级别的资源和功能来完成任务。
- en: Accompanied by the exponential growth in the agent quantity and complexity,
    there is an increasing strain on the functionalities of LLM and OS. For example,
    scheduling and prioritizing agent requests in limited LLM resources poses a significant
    challenge. Moreover, the LLM’s generation process can become time-intensive when
    dealing with lengthy contexts, occasionally resulting in the generation being
    suspended by the scheduler. This raises the problem of devising a mechanism to
    snapshot the LLM’s current generation result, thereby enabling pause/resume behavior
    even when the LLM has not finalized the response generation for the current request.
    Furthermore, once an agent has obtained the list of available calling tools, determining
    the optimal sequence for invoking these tools presents yet another challenge since
    multiple agents may need to call the same tool. Additionally, the concurrent operation
    of multiple agents necessitates a robust system for memory management across different
    agents, while also ensuring stringent enforcement of privacy and access control
    measures.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着代理数量和复杂性的指数增长，对LLM和操作系统功能的压力也在增加。例如，在有限的LLM资源下调度和优先排序代理请求是一项重大挑战。此外，当处理较长的上下文时，LLM的生成过程可能会变得耗时，偶尔导致生成被调度器挂起。这就提出了制定机制快照LLM当前生成结果的问题，从而在LLM尚未完成当前请求的响应生成时实现暂停/恢复行为。此外，一旦代理获得了可用调用工具的列表，确定调用这些工具的最佳顺序也是一个挑战，因为多个代理可能需要调用相同的工具。此外，多个代理的并发操作需要一个强大的系统来进行不同代理之间的内存管理，同时还要确保严格执行隐私和访问控制措施。
- en: 'To address the above mentioned challenges, we propose AIOS, an LLM agent operating
    system ([Figure 2](#S2.F2 "Figure 2 ‣ LLM-based Multi-Agent Systems. ‣ 2.2 Large
    Language Model Agents ‣ 2 Related Work ‣ AIOS: LLM Agent Operating System")) to
    provide module isolation and aggregations of LLM and OS functionalities. To address
    the potential conflicts arising between tasks associated with LLM and those unrelated
    to LLM, we propose the design of an LLM-specific kernel. This kernel segregates
    the OS-like duties, particularly those related to the oversight of LLM agents,
    their corresponding resources, and development tool-kits. Through this segregation,
    the LLM kernel aims to enhance the management and coordination of LLM-related
    activities. Within the proposed LLM kernel, we have devised a suite of modules,
    each dedicated to addressing the distinct functions pertinent to LLM operations.
    An overview of these modules and their respective functionalities is presented
    in the following, with the comprehensive discussion of their underlying mechanisms
    detailed in Section [4](#S4 "4 AIOS Implementation ‣ AIOS: LLM Agent Operating
    System").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对上述挑战，我们提出了AIOS，一个LLM代理操作系统（[图 2](#S2.F2 "Figure 2 ‣ LLM-based Multi-Agent
    Systems. ‣ 2.2 Large Language Model Agents ‣ 2 Related Work ‣ AIOS: LLM Agent
    Operating System")），以提供模块隔离和LLM与操作系统功能的整合。为了应对LLM相关任务和与LLM无关任务之间可能出现的冲突，我们提出了设计一个专门的LLM内核。这个内核将操作系统类似的职责，特别是与LLM代理、其相应资源和开发工具包的监督相关的职责进行隔离。通过这种隔离，LLM内核旨在提升LLM相关活动的管理和协调。在提议的LLM内核中，我们设计了一套模块，每个模块都专注于解决与LLM操作相关的不同功能。以下将介绍这些模块及其各自的功能，详细的机制讨论将在第[4](#S4
    "4 AIOS Implementation ‣ AIOS: LLM Agent Operating System")节中进行。'
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Agent Scheduler: Prioritizes and schedules agent requests to optimize LLM utilization.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理调度器：优先排序和调度代理请求，以优化LLM的使用。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Context Manager: Supports snapshot and restore the intermediate generation
    status in LLM and context window management of LLM.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文管理器：支持快照和恢复LLM中的中间生成状态以及LLM的上下文窗口管理。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Manager: Provides short-term memory for each agent’s interaction logs.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存管理器：为每个代理的交互日志提供短期内存。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Storage Manager: Persists agent interaction logs to long-term storage for future
    retrieval.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 存储管理器：将代理交互日志持久化到长期存储中，以便将来检索。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tool Manager: Manages agent’s calling of external API tools (e.g., search,
    scientific computing).'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具管理器：管理代理对外部API工具（如搜索、科学计算）的调用。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Access Manager: Enforces privacy and access control policies between agents.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 访问管理器：执行代理之间的隐私和访问控制策略。
- en: Aside from the modules, the kernel exposes an LLM system call interface through
    which agents can transparently leverage these services. Moreover, we design the
    AIOS SDK to further encapsulate the LLM system calls, providing more convenient
    agent library functions for agent developers. With the AIOS architecture, an agent
    like the travel planner can break down its task into steps that fluidly combine
    LLM reasoning (e.g., plan generation and tool calling decision) and OS-level actions
    (e.g., accessing storage and executing software services). This synergistic combination
    of capabilities equips multiple LLM agents to tackle increasingly complex, multi-modal
    tasks that require reasoning, execution, and interaction with the physical world.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模块之外，内核还提供了LLM系统调用接口，代理可以通过该接口透明地利用这些服务。此外，我们设计了AIOS SDK以进一步封装LLM系统调用，为代理开发者提供更便捷的代理库函数。借助AIOS架构，像旅行规划器这样的代理可以将任务分解为流畅结合LLM推理（例如，计划生成和工具调用决策）与操作系统级操作（例如，访问存储和执行软件服务）的步骤。这种能力的协同组合使多个LLM代理能够处理越来越复杂的多模态任务，这些任务需要推理、执行以及与物理世界的互动。
- en: Looking ahead, we envision extending the AIOS to support even tighter agent-world
    integration (e.g., through robotic control), more intelligent resource management,
    and safer multi-agent collaboration. Ultimately, AIOS serves as the crucial platform
    to facilitate the development, deployment and usage of various complex LLM agents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，我们设想将AIOS扩展以支持更紧密的代理-世界集成（例如，通过机器人控制）、更智能的资源管理以及更安全的多代理协作。**最终**，AIOS作为促进各种复杂LLM代理开发、部署和使用的关键平台。
- en: 2 Related Work
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Evolution of Operating Systems
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 操作系统的演变
- en: The evolution of operating systems (OS) has unfolded in a progressive way, evolving
    from rudimentary systems to the complex and interactive OS of today. Initially,
    operating systems served to bridge the gap between the user-level tasks and the
    binary functionality of computer hardware, such as electron and gate manipulation.
    Their evolution saw a transition from simple batch job processing [[23](#bib.bib23)]
    to more advanced process management techniques like time-sharing [[24](#bib.bib24)]
    and multi-task processing [[25](#bib.bib25), [26](#bib.bib26)], which facilitated
    the handling of increasingly complex tasks. The progress moved toward modularization
    within the OS, delineating specific responsibilities such as process scheduling [[27](#bib.bib27),
    [28](#bib.bib28)], memory management [[29](#bib.bib29), [30](#bib.bib30)], and
    filesystem management [[31](#bib.bib31), [32](#bib.bib32)], enhancing efficiency
    and manageability. The further advent of graphical user interfaces (GUIs), e.g.,
    Macintosh¹¹1[http://apple-history.com/128k](http://apple-history.com/128k), Windows²²2[https://winworldpc.com/product/windows-3/31](https://winworldpc.com/product/windows-3/31)
    and GNOME³³3[https://www.gnome.org/](https://www.gnome.org/), makes operating
    systems more interactive and user-centric. Meanwhile, the operating system ecosystem
    has also expanded, offering a comprehensive suite of developer tools (OS SDKs)
    and runtime libraries. These tools enable application developers to design, implement,
    and run their applications efficiently within the OS environment [[33](#bib.bib33)].
    Notable examples of OS ecosystems include Android Studio⁴⁴4[https://developer.android.com/studio](https://developer.android.com/studio),
    XCode⁵⁵5[https://developer.apple.com/xcode/](https://developer.apple.com/xcode/)
    and Cloud SDK⁶⁶6[https://cloud.google.com/sdk](https://cloud.google.com/sdk).
    In these ecosystems, the OS provides numerous resources to facilitate software
    development and serves as a platform for deploying and hosting software applications,
    leading to a thriving OS-application ecosystem. Nowadays, we stand at the transformative
    phase to see the potential of intelligent operating systems. With the incorporation
    of large language models (LLMs), These advanced systems promise to further narrow
    the communication gap between humans and machines, forwarding a new era of user-computer
    interaction.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Large Language Model Agents
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large language model (LLM) based autonomous agents take natural language instructions
    as input for complex task solving. The research on LLM-based agents can be generally
    classified into single-agent systems and multi-agent systems [[33](#bib.bib33)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based Single-Agent Systems.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM-based single-agent systems (SAS) use a single LLM agent for complex task
    solving, such as travel planning, personalized recommendation, and artistic design
    [[7](#bib.bib7)]. The agent takes natural language instruction from users as input
    and decomposes the task into a multistep plan for task solving, where each step
    may call external tools to be completed, such as collecting information, executing
    specialized models, or interacting with the external world. Single-agent applications
    may engage with either digital environment or physical environment or both, depending
    on the task to solve. For example, agents in virtual or digital environment may
    invoke APIs [[7](#bib.bib7), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)], browse websites [[38](#bib.bib38), [22](#bib.bib22)], or execute
    codes [[39](#bib.bib39)], while agents in the physical environment may manipulate
    objects [[19](#bib.bib19), [40](#bib.bib40), [41](#bib.bib41)], carry out lab
    experiments [[42](#bib.bib42), [43](#bib.bib43)], or make actionable decisions
    [[44](#bib.bib44), [45](#bib.bib45)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based Multi-Agent Systems.
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM-based multi-agent systems (MAS) leverage the interaction among multiple
    agents for problem solving. The relationship among the multiple agents could be
    cooperative, competitive, or a mixture of cooperation and competition [[33](#bib.bib33)].
    In cooperative multi-agent systems, each agent takes and assesses the information
    provided by other agents, thereby working together to solve complex tasks, such
    as role playing [[46](#bib.bib46)], social simulation [[47](#bib.bib47)] and software
    development [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)].
    In competitive multi-agent systems, agents may detabe, negotiate and compete with
    each other in a game environment to achieve their goals, such as improving negotiation
    skills [[52](#bib.bib52)] and debating about the correct answer [[53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)]. Some multi-agent systems may exhibit both
    cooperation and competition among agents. For example, WarAgent [[56](#bib.bib56)]
    models each country as an LLM-based agent to study how the interaction between
    countries can lead to international conflicts, where countries may cooperate with
    each other, such as establishing alliances and making peace agreements, or compete
    with each other, such as arms race, mobilization, and declaring wars.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf49a4180ea8ea50fd878115ed57976c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the AIOS architecture.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 3 AIOS Layers
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As depicted in [Figure 2](#S2.F2 "Figure 2 ‣ LLM-based Multi-Agent Systems.
    ‣ 2.2 Large Language Model Agents ‣ 2 Related Work ‣ AIOS: LLM Agent Operating
    System"), the architecture of our AIOS is organized into three distinct layers:
    the application layer, the kernel layer, and the hardware layer. This layered
    architecture ensures a clear delineation of responsibilities across the system.
    Each higher layer abstracts the complexities of the layers below it, facilitating
    interaction through interfaces or specific modules, thereby enhancing modularity
    and simplifying system interactions across different layers.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Application Layer.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the application layer, agent applications, such as travel agent or math agent,
    are developed and deployed. In this layer, AIOS provides the AIOS SDK, with a
    higher abstraction of system calls that simplifies the development process for
    agent developers. This SDK allows for development of agent applications by offering
    a rich toolkit that abstract away the complexities of lower-level system functions.
    This enables developers to dedicate their focus to the essential logic and functionalities
    of their agents, facilitating a more efficient development process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Layer.
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The kernel layer is divided into two primary components: the OS Kernel and
    the LLM Kernel, each serving the unique requirements of non-LLM and LLM-specific
    operations, respectively. This distinction allows the LLM kernel to focus on LLM
    specific tasks such as context management and agent scheduling, which are essential
    for handling LLM-related activities and are not typically within the purview of
    standard OS kernel functions. Our work primarily concentrates on enhancing the
    LLM kernel without making significant alterations to the existing OS kernel structure.
    The LLM kernel is equipped with several key modules, including the LLM system
    call interface, agent scheduler, context manager, memory manager, storage manager,
    tool manager, and access manager. These components are designed to address the
    diverse execution needs of agent applications, ensuring efficient management and
    execution within the AIOS framework. The specifics of these modules will be further
    detailed in Section [4](#S4 "4 AIOS Implementation ‣ AIOS: LLM Agent Operating
    System").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Layer.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The hardware layer comprises the physical components of the system, including
    the CPU, GPU, memory, disk, and peripheral devices. It is crucial to note that
    the LLM kernel’s system calls cannot directly interact with the hardware. Instead,
    these calls interface with the OS’s system calls, which in turn manage the hardware
    resources. This indirect interaction ensures a layer of abstraction and security,
    allowing the LLM kernel to leverage hardware capabilities without requiring direct
    hardware management, thus maintaining the system’s integrity and efficiency.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 4 AIOS Implementation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we begin with an overview of the fundamental design and implementation
    of each module within the LLM kernel. Subsequently, we present the LLM system
    calls, which encompass essential functions for each module. At last, we discuss
    the exploration of the AIOS SDK, aiming to facilitate the development process
    for agent developers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Agent Scheduler
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83a35c78c5066e25ef338d8401411efd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An illustration of the agent scheduler.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Agent scheduler is designed to manage the agent requests in an efficient way.
    Consider the various agents (denoted as A, B, and C) in [Figure 3](#S4.F3 "Figure
    3 ‣ 4.1 Agent Scheduler ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating System"),
    each of which has several execution steps. In the sequential execution paradigm,
    the agent tasks are processed in a linear order, where steps from a same agent
    will be processed first. This can lead to potential increased waiting times for
    tasks queued later in the sequence.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The agent scheduler employs strategies such as First-In-First-Out (FIFO)⁷⁷7[https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)](https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)),
    Round Robin (RR)⁸⁸8[https://en.wikipedia.org/wiki/Round-robin_scheduling](https://en.wikipedia.org/wiki/Round-robin_scheduling),
    and other scheduling algorithms to optimize this process. Through concurrent execution,
    the scheduler significantly balances waiting time and turnaround time of each
    agent, as tasks from different agents are interleaved and executed in parallel.
    This concurrent approach is visualized through a timeline where tasks from different
    agents are processed in an interleaved manner (e.g., A1, B1, C1, B2, A2, A3, C2,
    C3), ensuring that no single agent monopolizes the processing resources and that
    idle times are minimized. Apart from implementing the traditional scheduling algorithms,
    more complex scheduling algorithms considering the dependency relationships between
    agent requests can also be incorporated, which can be considered in the future.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Context Manager
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/678cd58596a629cfb06388fcfa565e87.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Context snapshot and restoration, where we use beam search (beam
    width = 1) as an example search algorithm to illustrate this generative decoding
    process.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'The context manager is responsible for managing the context provided to LLM
    and the generation process given certain context. It primarily involves two crucial
    functions: context snapshot and restoration, and context window management.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/28fc33432b34a0afd0e82bd190c2c581.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Storage of interaction history with memory manager and storage manager.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Context Snapshot and Restoration. Consider that the scheduler algorithms may
    involve time quantum operations (e.g., Round-Robin) and agent requests may be
    suspended by the scheduler. This suspension happens even if the response has not
    been fully generated yet by the LLM. Therefore, it necessitates a mechanism to
    preserve the state of the LLM’s generation process, ensuring that it can be accurately
    resumed once resources are available again.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'AIOS provides the snapshot and restoration mechanisms in the context manager
    to address this issue, which can be seen from [Figure 4](#S4.F4 "Figure 4 ‣ 4.2
    Context Manager ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating System").
    We use the beam search process⁹⁹9[https://en.wikipedia.org/wiki/Beam_search](https://en.wikipedia.org/wiki/Beam_search),
    a typical practice in LLMs [[10](#bib.bib10), [57](#bib.bib57), [58](#bib.bib58)],
    to illustrate the generative decoding process. For simplicity of illustration,
    we set beam width as 1. Specifically, consider the agent request as: Determine
    whether there will be a rain in the destination of flight UA057. At each step,
    the LLM evaluates multiple potential candidates, with the most promising paths
    kept for further expansion based on the predefined beam width.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'When such generation process has been suspended by the scheduler at an intermediate
    step, the context manager uses the snapshot function to capture and store the
    current state of the LLM’s beam search tree, including all intermediate probabilities
    and paths being explored for generating the response. Upon resumption, the restoration
    function is employed to reload the saved state from the snapshot, allowing the
    LLM to continue its generation process exactly from the point of suspension to
    reach the final answer: Search weather in Paris. In this way, the context manager
    ensures that the temporary suspension of one agent’s request does not lead to
    a loss of progress, thereby optimizing resource use without compromising the quality
    and efficiency of response generation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Context Window Management. To address challenges posed by long contexts that
    surpass the context window limit of LLMs, context manager also needs to manage
    potential expansion of context window. Specifically, context manager in AIOS supports
    basic text summarization and incorporates other expansion techniques [[59](#bib.bib59),
    [60](#bib.bib60)] to manage the context window. In this way, it can help enhance
    the LLM’s ability to process and understand extensive contexts without compromising
    the integrity or relevance of the information.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Memory Manager
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in [Figure 5](#S4.F5 "Figure 5 ‣ 4.2 Context Manager ‣ 4 AIOS Implementation
    ‣ AIOS: LLM Agent Operating System"), memory manager manages short-term memory
    within an agent’s lifecycle, ensuring that data is stored and accessible only
    while the agent is active, either waiting for execution or during runtime. The
    current AIOS supports storing each agent’s memory independently, each of which
    other agents have no direct access to, unless it is authorized by the access manager.
    More complicated memory mechanisms such as shared memory pools among agents or
    hierarchical caches can be considered and integrated into AIOS in the future.
    Compared with the storage manager introduced in the following, the memory manager
    enables rapid data retrieval and processing, facilitating swift responses to user
    queries and interactions without overburdening the storage of AIOS.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图 5](#S4.F5 "Figure 5 ‣ 4.2 Context Manager ‣ 4 AIOS Implementation ‣ AIOS:
    LLM Agent Operating System")所示，内存管理器在智能体的生命周期内管理短期记忆，确保数据仅在智能体处于活跃状态时存储和可访问，无论是在等待执行还是在运行时。当前的
    AIOS 支持独立存储每个智能体的内存，其他智能体无法直接访问，除非获得访问管理器的授权。更复杂的内存机制，如智能体之间的共享内存池或分层缓存，可以考虑并在未来集成到
    AIOS 中。与后续介绍的存储管理器相比，内存管理器能够快速检索和处理数据，从而在不增加 AIOS 存储负担的情况下，快速响应用户查询和交互。'
- en: 'Table 1: Managed tools in AIOS. The last column shows each tool’s required
    input and output format.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：AIOS 中管理的工具。最后一列显示了每个工具所需的输入和输出格式。
- en: Category Tool Name Description Input $\rightarrow$ Text
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 工具名称 描述 输入 $\rightarrow$ 文本
- en: 4.4 Storage Manager
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 存储管理器
- en: In constrast, the storage manager is responsible for the long-term preservation
    of data, overseeing the storage of information that needs to be retained indefinitely,
    beyond the active lifespan of any single agent. This permanent storage in AIOS
    is achieved through a variety of durable mediums such as local files, databases,
    or cloud-based solutions, ensuring data integrity and availability for future
    reference or analysis. The storage manager supports retrieval augmentation [[61](#bib.bib61)].
    Through storing user preferences and maintaining historical interaction logs,
    the storage manager can enrich the agent knowledge update and enhancing the long-term
    user experience.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，存储管理器负责数据的长期保存，监督需要无限期保留的信息的存储，超出任何单一智能体的活跃生命周期。AIOS 中的这种永久存储是通过各种耐用介质实现的，如本地文件、数据库或基于云的解决方案，确保数据的完整性和未来的参考或分析的可用性。存储管理器支持检索增强[[61](#bib.bib61)]。通过存储用户偏好和维护历史交互日志，存储管理器可以丰富智能体的知识更新，并提升长期用户体验。
- en: 4.5 Tool Manager
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 工具管理器
- en: 'The tool manager in the AIOS system manages a diverse array of API tools that
    enhance the functionality of LLMs. As shown in [Table 1](#S4.T1 "Table 1 ‣ 4.3
    Memory Manager ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating System"), the
    tool manager integrates commonly-used tools from various sources [[7](#bib.bib7),
    [62](#bib.bib62), [63](#bib.bib63)] and classify them into different categories,
    which covers web search, scientific computing, database retrieval, image processing,
    etc. In this way, the managed tools can cover different modalities of input and
    output (image and text), thus facilitating agent development within the AIOS ecosystem.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'AIOS 系统中的工具管理器管理着多种 API 工具，这些工具增强了 LLM 的功能。如[表 1](#S4.T1 "Table 1 ‣ 4.3 Memory
    Manager ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating System")所示，工具管理器整合了来自不同来源的常用工具[[7](#bib.bib7),
    [62](#bib.bib62), [63](#bib.bib63)]，并将它们分类为不同类别，包括网页搜索、科学计算、数据库检索、图像处理等。通过这种方式，管理的工具可以涵盖不同的输入和输出模式（图像和文本），从而促进了
    AIOS 生态系统中的智能体开发。'
- en: 'Table 2: Instances of LLM system calls.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LLM 系统调用实例。
- en: Category Name Description Arguments Agent syscall get_aid get ID of an agent
    - set_aid set ID of an agent int aid get_status get status of an agent int aid
    set_status get status of an agent int aid, string status get_priority get priority
    of an agent int aid set_priority set priority of an agent int aid, int priority
    suspend_agent suspend an agent’s operation int aid resume_agent resume an agent’s
    operation int aid Context syscall gen_snapshot snapshot intermediate LLM generation
    status - gen_restore restore intermediate LLM generation status - get_context
    get context window length - exp_context expand context window length - clr_context
    clear current context window - set_context set a specific context window context
    c Memory syscall mem_write write the interaction record into memory int aid, memory
    m mem_read read the interaction record from memory int aid, memory m mem_clear
    clear specific memory record int aid, memory m mem_alloc allocate memory for an
    agent int aid, size s Storage syscall sto_write write the interaction record into
    storage int aid, storage s sto_read load the interaction record from storage int
    aid, storage s sto_delete delete the interaction record from storage int aid,
    storage s sto_alloc allocate storage for an agent int aid, size s
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Access Manager
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The access manager orchestrates access control operations among distinct agents
    by administering a dedicated privilege group for each agent. Those other agents
    that are excluded from an agent’s privilege group are denied access to its resources,
    such as the interaction history. To further enhance system transparency, the access
    manager compiles and maintains auditing logs. These logs capture detailed information
    about access requests, agent activities, and any modifications to the access control
    parameters, which help to safeguard against potential privilege attacks [[64](#bib.bib64),
    [65](#bib.bib65)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 LLM System Call
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLM system call interface within the LLM kernel is designed to offer basic
    LLM call operation functions. This interface acts as a bridge between complex
    agent requests and the execution of different kernel’s modules. As is shown in [Table 2](#S4.T2
    "Table 2 ‣ 4.5 Tool Manager ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating
    System"), analogous to OS system calls, LLM system calls offers a suite of basic
    functions that span across the kernel’s modules, including agent management, context
    handling, memory and storage operations, and access control. The LLM system call
    list can be further expanded in the future to support more operations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 AIOS SDK
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The AIOS SDK is designed to equip developers with a versatile toolkit for crafting
    sophisticated agent applications within the AIOS. This SDK encompasses a broad
    spectrum of functionalities, from initializing agents and managing agent lifecycles
    to facilitating complex operations like resource monitoring, and generation plan
    for an agent task. Like any operating system, enriching the SDK to be comprehensive
    and developer-friendly is a long-term and never-ending endeavor. The current SDK
    functions supported in AIOS are shown in [Table 3](#S4.T3 "Table 3 ‣ 4.8 AIOS
    SDK ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating System"), which will be
    continuously updated and expanded to fulfill the needs of evolving agent applications.
    This development effort aims to provide developers with the tools they need to
    harness the full potential of their agent applications within the AIOS framework.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'AIOS SDK 旨在为开发人员提供一个多功能工具包，以在 AIOS 内创建复杂的代理应用程序。该 SDK 涵盖了广泛的功能，从初始化代理和管理代理生命周期，到执行复杂的操作，如资源监控和生成代理任务的计划。与任何操作系统一样，使
    SDK 变得全面且对开发者友好是一个长期且永无止境的工作。当前在 AIOS 中支持的 SDK 功能见 [表 3](#S4.T3 "Table 3 ‣ 4.8
    AIOS SDK ‣ 4 AIOS Implementation ‣ AIOS: LLM Agent Operating System")，这些功能将持续更新和扩展，以满足不断发展的代理应用程序的需求。这一开发工作旨在为开发者提供所需工具，以充分发挥其代理应用程序在
    AIOS 框架内的潜力。'
- en: 'Table 3: The list of SDK functions in AIOS SDK'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：AIOS SDK 中的 SDK 函数列表
- en: SDK Function Name Description Return Type initializeAgent() set up environment
    for agent execution, including resource allocation Void registerAgent() register
    a new agent to the system, providing it with ID and permissions Boolean queueForExecution()
    enqueue the agent for execution Void generatePlan() generate a plan of a given
    agent task Object terminateAgent() terminate an agent’s execution with cleanup
    Void createAgentSnapshot() Create a snapshot of the agent’s current state for
    rollback Object rollbackAgentState() Rollback the agent’s state to a previous
    snapshot Void updateAgentConfig() update the configuration settings for an agent
    Boolean authenticateAgent() authenticate an agent’s credentials before execution
    Boolean monitorResourceUsage() track and report the usage of system resources
    by agents Object logEvent() provide logging of agent-specific events for debugging
    and auditing Void listActiveAgents() list all currently active agents within the
    system List queryAgentHistory() query the historical operations of an agent Object
    encryptData() encrypt sensitive data generated by an agent Object decryptData()
    decrypt sensitive data for an agent’s consumption Object
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: SDK 功能名称 描述 返回类型 initializeAgent() 设置代理执行环境，包括资源分配 Void registerAgent() 注册一个新的代理到系统中，为其提供
    ID 和权限 Boolean queueForExecution() 将代理加入执行队列 Void generatePlan() 为给定的代理任务生成计划
    Object terminateAgent() 终止代理的执行并进行清理 Void createAgentSnapshot() 创建代理当前状态的快照以便回滚
    Object rollbackAgentState() 将代理状态回滚到先前的快照 Void updateAgentConfig() 更新代理的配置设置 Boolean
    authenticateAgent() 在执行前验证代理的凭据 Boolean monitorResourceUsage() 跟踪并报告系统资源的使用情况
    Object logEvent() 提供代理特定事件的日志记录，用于调试和审计 Void listActiveAgents() 列出系统内所有当前活动的代理
    List queryAgentHistory() 查询代理的历史操作 Object encryptData() 加密代理生成的敏感数据 Object decryptData()
    解密代理消费的敏感数据 Object
- en: 5 Evaluation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: 'In this section, we evaluate both the correctness and the performance of AIOS
    modules when multiple agents are running in parallel in AIOS. Our investigation
    is guided by two research questions: firstly, whether the LLM responses to agent
    requests are consistent after agent suspension and transition to another agent,
    and secondly, how is the performance of AIOS scheduling in improving balance of
    waiting and turnaround time relative to non-scheduled (sequential) execution.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了 AIOS 模块在多个代理并行运行时的正确性和性能。我们的调查由两个研究问题指导：首先，LLM 对代理请求的响应在代理挂起后及转移到另一个代理后是否一致；其次，AIOS
    调度在提高等待和周转时间的平衡相对于未调度（顺序）执行的性能如何。
- en: 5.1 Setup
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: 'Our experiments are conducted in Python 3.9 with PyTorch 2.0.1 and CUDA 11.8
    on an Ubuntu 22.04 machine equipped with 8 NVIDIA RTX A5000 GPUs. We employ the
    publicly available LLMs (i.e., Gemma-2b-it and Gemma-7b-it [[66](#bib.bib66)],
    LLaMA-2-13b-chat-hf [[10](#bib.bib10)]) as the backbone of AIOS. This selection
    is driven by the advantage of local deployment of open-sourced models, which aids
    in the accurate measurement of time latency. For the evaluation, we configure
    three specialized agents: a Math Agent for solving mathematical challenges, a
    Narrative Agent for generating novel narratives, and a Rec Agent tasked with providing
    restaurant recommendations. Each agent is designed to send 2 to 3 requests to
    the backbone LLM during running.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验在配备8个NVIDIA RTX A5000 GPU的Ubuntu 22.04机器上，使用Python 3.9、PyTorch 2.0.1和CUDA
    11.8进行。我们采用公开可用的LLM（即Gemma-2b-it和Gemma-7b-it[[66](#bib.bib66)]，LLaMA-2-13b-chat-hf[[10](#bib.bib10)]）作为AIOS的骨干。这一选择是基于本地部署开源模型的优势，有助于准确测量时间延迟。为了评估，我们配置了三个专用代理：一个用于解决数学问题的数学代理，一个用于生成新叙事的叙事代理，以及一个负责提供餐馆推荐的推荐代理。每个代理在运行期间设计为向骨干LLM发送2到3个请求。
- en: 5.2 Experimental Results
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实验结果
- en: 'Table 4: Consistency of LLM generated responses when running multiple agents
    in parallel compared with LLM generated responses when running a single agent
    one by one.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在并行运行多个代理与逐个运行单个代理时，LLM生成的响应的一致性。
- en: LLM-backbone Math Agent Narrative Agent Rec Agent BLEU Score BERT Score BLEU
    Score BERT Score BLEU Score BERT Score Gemma-2b-it 1.0 1.0 1.0 1.0 1.0 1.0 Gemma-7b-it
    1.0 1.0 1.0 1.0 1.0 1.0 LLaMA-2-13b-chat-hf 1.0 1.0 1.0 1.0 1.0 1.0
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-backbone 数学代理 叙事代理 推荐代理 BLEU分数 BERT分数 BLEU分数 BERT分数 BLEU分数 BERT分数 Gemma-2b-it
    1.0 1.0 1.0 1.0 1.0 1.0 Gemma-7b-it 1.0 1.0 1.0 1.0 1.0 1.0 LLaMA-2-13b-chat-hf
    1.0 1.0 1.0 1.0 1.0 1.0
- en: Consistency Analysis.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一致性分析。
- en: 'To answer the consistency question, we first run each of the three constructed
    agents individually to generate results. Subsequently, we execute these agents
    in parallel, capturing their outputs at each step. To assess the consistency of
    outputs under the running of multiple agents in parallel and under the running
    of single agents one by one, we utilize the BLEU score [[67](#bib.bib67)] and
    BERT score [[68](#bib.bib68)] as evaluative metrics. Both metrics span from 0.0
    to 1.0, with outputs produced in a single-agent context serving as the reference
    standard and we set the temperature parameter as 0 to eliminate the impact of
    randomness. As demonstrated in Table [4](#S5.T4 "Table 4 ‣ 5.2 Experimental Results
    ‣ 5 Evaluation ‣ AIOS: LLM Agent Operating System"), BLEU and BERT scores all
    achieve the value of 1.0, indicating a perfect alignment between outputs generated
    in multi-agent and single-agent configurations. This result affirms the consistency
    of our design in effectively facilitating concurrent multi-agent operations.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解答一致性问题，我们首先分别运行三个构建的代理以生成结果。随后，我们并行执行这些代理，捕捉它们在每一步的输出。为了评估在多个代理并行运行和单个代理逐个运行下输出的一致性，我们利用BLEU分数[[67](#bib.bib67)]和BERT分数[[68](#bib.bib68)]作为评估指标。这两个指标的范围为0.0到1.0，单代理上下文中生成的输出作为参考标准，我们将温度参数设为0，以消除随机性的影响。如表[4](#S5.T4
    "Table 4 ‣ 5.2 Experimental Results ‣ 5 Evaluation ‣ AIOS: LLM Agent Operating
    System")所示，BLEU和BERT分数均达到了1.0，表明在多代理和单代理配置中生成的输出完全一致。该结果确认了我们的设计在有效促进并发多代理操作中的一致性。'
- en: Performance Analysis.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能分析。
- en: 'To answer the efficiency question, we conduct a comparative analysis between
    AIOS employing FIFO scheduling and a non-scheduled approach, wherein the aforementioned
    three agents run concurrently. In the non-scheduled setting, the three agents
    are executed following a predefined sequential order: Math Agent, Narrative Agent,
    and Rec Agent. We employ two metrics for assessing temporal efficiency: waiting
    time (the interval from agent request submission to commencement) and turnaround
    time (the duration from agent request submission to completion). As each agent
    will send multiple requests to the LLM, the waiting time and turnaround time of
    each agent are calculated as the average of waiting time and turnaround time of
    all its sent requests, respectively. To mitigate randomness, we execute these
    three agents, both with and without scheduling, in five separate trials to report
    the results. As shown in  [Table 5](#S5.T5 "Table 5 ‣ Performance Analysis. ‣
    5.2 Experimental Results ‣ 5 Evaluation ‣ AIOS: LLM Agent Operating System"),
    the non-scheduled approach exhibits good performance for agents earlier in the
    sequence, but at the expense of extended waiting time and turnaround time for
    agents later in the sequence. Conversely, AIOS’s scheduling mechanism efficiently
    regulates both waiting time and turnaround time, a benefit that becomes particularly
    evident for agent requests submitted later by agents, especially when LLM is large.
    This suggests the importance of our scheduling to accommodate parallel operations
    of multiple agents.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决效率问题，我们对使用FIFO调度的AIOS与非调度方法进行了比较分析，其中上述三种代理同时运行。在非调度设置中，三种代理按照预定义的顺序执行：数学代理、叙事代理和推荐代理。我们使用两种指标来评估时间效率：等待时间（从代理请求提交到开始的间隔）和周转时间（从代理请求提交到完成的持续时间）。由于每个代理会向LLM发送多个请求，因此每个代理的等待时间和周转时间分别计算为其发送的所有请求的等待时间和周转时间的平均值。为了减少随机性，我们对这三种代理进行了五次独立试验，无论是有调度还是无调度。结果显示在[表5](#S5.T5
    "Table 5 ‣ Performance Analysis. ‣ 5.2 Experimental Results ‣ 5 Evaluation ‣ AIOS:
    LLM Agent Operating System")中，未调度的方法对于序列中的早期代理表现良好，但牺牲了序列中后期代理的等待时间和周转时间。相反，AIOS的调度机制有效地调节了等待时间和周转时间，这一优势在代理请求较晚提交时尤为明显，特别是在LLM较大的情况下。这表明我们的调度机制在适应多个代理的并行操作方面的重要性。'
- en: 'Table 5: Effectiveness of agent scheduling, compared with non-scheduled (sequential)
    execution.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：代理调度的有效性，与未调度（顺序）执行相比。
- en: LLM backbone Agent Sequential execution (non-scheduled) Concurrent execution
    (scheduled) Waiting time (s) Turnaround time (s) Waiting time (s) Turnaround time
    (s) Gemma-2b-it Math Agent 0.002$\pm$2.43
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LLM主干代理 顺序执行（未调度） 并发执行（调度） 等待时间（秒） 周转时间（秒） 等待时间（秒） 周转时间（秒） Gemma-2b-it 数学代理
    0.002$\pm$2.43
- en: 6 Conclusions
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper proposes the AIOS architecture, demonstrating the potential to facilitate
    the development and deployment of LLM-based agents, fostering a more cohesive,
    effective and efficient AIOS-Agent ecosystem. The insights and methodologies presented
    herein contribute to the ongoing discourse in both AI and system research, offering
    a viable solution to the integration challenges posed by the diverse landscape
    of AI Agents. Diverse future work can be built upon this foundation, exploring
    innovative ways to refine and expand the AIOS architecture to meet the evolving
    needs of developing and deploying LLM agents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了AIOS架构，展示了促进基于LLM的代理开发和部署的潜力，促进了一个更具凝聚力、高效和有效的AIOS-代理生态系统。本文提出的见解和方法为AI和系统研究的持续讨论做出了贡献，提供了一种解决多样化AI代理领域整合挑战的可行解决方案。在此基础上，可以开展多样化的未来工作，探索创新的方法来改进和扩展AIOS架构，以满足开发和部署LLM代理的不断变化的需求。
- en: 7 Future Work
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来工作
- en: Beginning with AIOS, there are many directions for future research to pursue.
    This section outlines potential areas of study that expand upon the foundational
    features of AIOS.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从AIOS开始，未来的研究方向有很多。本节概述了可能的研究领域，这些领域扩展了AIOS的基础功能。
- en: Advanced Scheduling Algorithms.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高级调度算法。
- en: The scheduling function of AIOS lays the groundwork for the development of more
    advanced algorithms. Future research could focus on algorithms that perform dependency
    analysis among agent requests, optimizing the allocation of computational resources.
    Additionally, some of the tool resources are locally deployed models, which can
    also be incorporated into the scheduling paradigm. This includes the management
    of tool status and snapshots, suggesting a move towards a unified scheduling framework
    that encompasses both agents and their tools.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency of Context Management.
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: More efficient mechanisms can be devised to assist context management. For example,
    the pursuit of time-efficient context management techniques could significantly
    augment user experience by expediting the processes of context snapshotting and
    restoration. Also, context compression techniques can also be leveraged prior
    to snapshotting, which can yield a more space-efficient solution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of Memory and Storage Architecture.
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the context of agent collaboration and communication, the future design of
    memory and storage systems can adopt a shared approach, enabling the sharing of
    memory and storage between agents. Such an architecture would enable agents to
    access a communal pool of memory and storage, thereby improving the agents’ decision-making
    ability since one agent can benefit from other agents’ memory or storage. Moreover,
    future work can explore hierarchical storage solutions, designed to optimize data
    retrieval and storage efficiency. This could involve prioritizing quicker access
    and reduced storage allocation for frequently accessed data, and vice versa for
    less frequently accessed information.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Safety and Privacy Enhancements.
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The aspect of safety in AIOS necessitates protective measures against various
    attacks, ensuring the system’s resilience against malicious attacks, such as jailbreaking
    of LLM or unauthorized access of other agents’ memory. In the realm of privacy,
    the exploration of advanced encryption techniques is vital for safeguarding data
    transmission within AIOS, thus maintaining the confidentiality of agent communications.
    Furthermore, the implementation of watermarking techniques could serve to protect
    the intellectual property of agent developers by embedding unique identifiers
    in outputs, facilitating the tracing of data lineage.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In a netshell, AIOS stands as a motivating body of work that brings a broad
    spectrum of research opportunities. Each outlined direction can not only build
    upon the foundational elements of AIOS but also contribute to the advancement
    of the field at large.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Jian Zhang, Zhenting Wang, and Wenyue Hua for their valuable discussions
    and suggestions during the project.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory
    and practice. The knowledge engineering review, 10(2):115–152, 1995.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Nicholas R Jennings, Katia Sycara, and Michael Wooldridge. A roadmap of
    agent research and development. Autonomous agents and multi-agent systems, 1:7–38,
    1998.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Paolo Bresciani, Anna Perini, Paolo Giorgini, Fausto Giunchiglia, and John
    Mylopoulos. Tropos: An agent-oriented software development methodology. Autonomous
    Agents and Multi-Agent Systems, 8:203–236, 2004.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] OpenAI. Gpt-4. [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4),
    2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Facebook. Meta. introducing llama: A foundational, 65-billion-parameter
    large language model. [https://ai.facebook.com/blog/largelanguage-model-llama-meta-ai](https://ai.facebook.com/blog/largelanguage-model-llama-meta-ai),
    2022.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, and
    Yongfeng Zhang. OpenAGI: When LLM Meets Domain Experts. Advances in Neural Information
    Processing Systems, 36, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
    Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training
    language models to follow instructions with human feedback. Advances in Neural
    Information Processing Systems, 35:27730–27744, 2022.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
    language models. arXiv preprint arXiv:2210.11416, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang.
    Recommendation as language processing (rlp): A unified pretrain, personalized
    prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender
    Systems, page 299–315, 2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
    Iwasawa. Large language models are zero-shot reasoners. Advances in neural information
    processing systems, 35:22199–22213, 2022.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for
    code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
    Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica:
    A large language model for science. arXiv preprint arXiv:2211.09085, 2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting
    Hu. Reasoning with language model is planning with world model. In Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing, pages
    8154–8173, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve
    computer tasks. Advances in Neural Information Processing Systems, 36, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and
    Justin D Weisz. The programmer’s assistant: Conversational interaction with a
    large language model for software development. In Proceedings of the 28th International
    Conference on Intelligent User Interfaces, pages 491–514, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
    Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e:
    an embodied multimodal language model. In Proceedings of the 40th International
    Conference on Machine Learning, pages 8469–8488, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander
    Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do
    as i can, not as i say: Grounding language in robotic affordances. In Conference
    on robot learning, pages 287–318\. PMLR, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. International
    Conference on Learning Representations, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and
    Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances
    in Neural Information Processing Systems, 36, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang,
    Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances
    in Neural Information Processing Systems, 36, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] UW:CSE451. History of Operating Systems, 2023. [https://courses.cs.washington.edu/courses/cse451/16wi/readings/lecture_readings/LCM_OperatingSystemsTimeline_Color_acd_newsize.pdf](https://courses.cs.washington.edu/courses/cse451/16wi/readings/lecture_readings/LCM_OperatingSystemsTimeline_Color_acd_newsize.pdf).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Dennis M. Ritchie and Ken Thompson. The unix time-sharing system. Commun.
    ACM, 17(7):365–375, jul 1974.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Charles Antony Richard Hoare. Monitors: An operating system structuring
    concept. Communications of the ACM, 17(10):549–557, 1974.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Dawson R Engler, M Frans Kaashoek, and James O’Toole Jr. Exokernel: An
    operating system architecture for application-level resource management. ACM SIGOPS
    Operating Systems Review, 29(5):251–266, 1995.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Chung Laung Liu and James W Layland. Scheduling algorithms for multiprogramming
    in a hard-real-time environment. Journal of the ACM (JACM), 20(1):46–61, 1973.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Edsger W Dijkstra. Cooperating sequential processes. In The origin of
    concurrent programming: from semaphores to remote procedure calls, pages 65–138\.
    Springer, 2002.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Peter J Denning. The working set model for program behavior. Communications
    of the ACM, 11(5):323–333, 1968.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Robert C Daley and Jack B Dennis. Virtual memory, processes, and sharing
    in multics. Communications of the ACM, 11(5):306–312, 1968.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Mendel Rosenblum and John K Ousterhout. The design and implementation
    of a log-structured file system. ACM Transactions on Computer Systems (TOCS),
    10(1):26–52, 1992.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Marshall K McKusick, William N Joy, Samuel J Leffler, and Robert S Fabry.
    A fast file system for unix. ACM Transactions on Computer Systems (TOCS), 2(3):181–197,
    1984.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng
    Zhang. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent
    Ecosystem. arXiv:2312.03815, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models
    can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Shunyu Yao and Karthik Narasimhan. Language agents in the digital world:
    Opportunities and risks. princeton-nlp.github.io, Jul 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language
    models. arXiv preprint arXiv:2205.12255, 2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun.
    Toolalpaca: Generalized tool learning for language models with 3000 simulated
    cases. arXiv preprint arXiv:2306.05301, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang,
    Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin
    Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human
    feedback, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Kechi Zhang, Ge Li, Jia Li, Zhuo Li, and Zhi Jin. Toolcoder: Teach code
    generation models to use apis with search tools. arXiv preprint arXiv:2305.04032,
    2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi
    Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building
    open-ended embodied agents with internet-scale knowledge. Advances in Neural Information
    Processing Systems, 35:18343–18362, 2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. In Intrinsically-Motivated and Open-Ended Learning Workshop@
    NeurIPS2023, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous
    scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332,
    2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow:
    Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language
    models as zero-shot planners: Extracting actionable knowledge for embodied agents.
    In International Conference on Machine Learning, pages 9118–9147\. PMLR, 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang,
    and Zhiting Hu. Language models meet world models: Embodied experiences enhance
    language models. Advances in neural information processing systems, 36, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
    Ghanem. Camel: Communicative agents for "mind" exploration of large language model
    society. Advances in Neural Information Processing Systems, 36, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng,
    Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt:
    Meta programming for multi-agent collaborative framework. In The Twelfth International
    Conference on Learning Representations, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen
    llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei Li, Saibo Geng, Julian Paul
    Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, and Robert West. Flows: Building
    blocks of reasoning and collaborating ai. arXiv preprint arXiv:2308.01285, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language
    model negotiation with self-play and in-context learning from ai feedback. arXiv
    preprint arXiv:2305.10142, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.
    Improving factuality and reasoning in language models through multiagent debate.
    arXiv preprint arXiv:2305.14325, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang
    Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. In The Twelfth International Conference on Learning
    Representations, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu
    Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language
    models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge,
    Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based
    multi-agent simulation of world wars. arXiv preprint arXiv:2311.17227, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,
    Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
    Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models
    across training and scaling. In International Conference on Machine Learning,
    pages 2397–2430\. PMLR, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. arXiv preprint
    arXiv:2306.15595, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn:
    Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis,
    Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu,
    Asli Celikyilmaz, et al. Augmented language models: a survey. Transactions on
    Machine Learning Research, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] LangChain. Langchain. [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain),
    2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Rapid. Rapid api hub. [https://rapidapi.com/hub](https://rapidapi.com/hub),
    2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Ken Thompson. Reflections on trusting trust. Communications of the ACM,
    27(8):761–763, 1984.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Sven Bugiel, Lucas Davi, Alexandra Dmitrienko, Thomas Fischer, Ahmad-Reza
    Sadeghi, and Bhargava Shastry. Towards taming privilege-escalation attacks on
    android. In NDSS, volume 17, page 19, 2012.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Tris Warkentin Jeanine Banks. Gemma: Introducing new state-of-the-art
    open models. [https://blog.google/technology/developers/gemma-open-models/](https://blog.google/technology/developers/gemma-open-models/),
    2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method
    for automatic evaluation of machine translation. In Proceedings of the 40th annual
    meeting of the Association for Computational Linguistics, pages 311–318, 2002.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
    Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675,
    2019.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
