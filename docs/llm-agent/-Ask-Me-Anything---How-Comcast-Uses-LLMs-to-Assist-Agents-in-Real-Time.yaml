- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '“Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “问我任何问题”：康卡斯特如何利用LLMs实时辅助客服
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00801](https://ar5iv.labs.arxiv.org/html/2405.00801)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00801](https://ar5iv.labs.arxiv.org/html/2405.00801)
- en: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier") ,  Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
     and  Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com)
    [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier")，Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    康卡斯特AI技术 费城 宾夕法尼亚州 美国，Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    康卡斯特AI技术 费城 宾夕法尼亚州 美国，Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
    以及 Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com)
    [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X "ORCID identifier")
    康卡斯特AI技术 费城 宾夕法尼亚州 美国（2024）
- en: '“Ask Me Anything”: How Comcast Uses LLMs to'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “问我任何问题”：康卡斯特如何利用LLMs实时辅助客服
- en: Assist Agents in Real Time
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实时辅助客服
- en: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier") ,  Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA ,  Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
     and  Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com)
    [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X "ORCID identifier")
    Comcast AI TechnologiesPhiladelphiaPennsylvaniaUSA(2024)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Scott Rome [scott˙rome@comcast.com](mailto:scott%CB%99rome@comcast.com) [0000-0003-0270-7192](https://orcid.org/0000-0003-0270-7192
    "ORCID identifier")，Tianwen Chen [tianwen˙chen@comcast.com](mailto:tianwen%CB%99chen@comcast.com)
    [0009-0008-0477-7366](https://orcid.org/0009-0008-0477-7366 "ORCID identifier")
    康卡斯特AI技术 费城 宾夕法尼亚州 美国，Raphael Tang [raphael˙tang@comcast.com](mailto:raphael%CB%99tang@comcast.com)
    [0009-0007-2873-892X](https://orcid.org/0009-0007-2873-892X "ORCID identifier")
    康卡斯特AI技术 费城 宾夕法尼亚州 美国，Luwei Zhou [luwei˙zhou@comcast.com](mailto:luwei%CB%99zhou@comcast.com)
    [0009-0009-1862-3058](https://orcid.org/0009-0009-1862-3058 "ORCID identifier")
    以及 Ferhan Ture [ferhan˙ture@comcast.com](mailto:ferhan%CB%99ture@comcast.com)
    [0000-0002-5585-157X](https://orcid.org/0000-0002-5585-157X "ORCID identifier")
    康卡斯特AI技术 费城 宾夕法尼亚州 美国（2024）
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Customer service is how companies interface with their customers. It can contribute
    heavily towards the overall customer satisfaction. However, high-quality service
    can become expensive, creating an incentive to make it as cost efficient as possible
    and prompting most companies to utilize AI-powered assistants, or ”chat bots”.
    On the other hand, human-to-human interaction is still desired by customers, especially
    when it comes to complex scenarios such as disputes and sensitive topics like
    bill payment.¹¹1[https://bit.ly/3yaNO9t](https://bit.ly/3yaNO9t)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 客户服务是公司与客户接口的方式。这对整体客户满意度的贡献极大。然而，高质量服务可能变得昂贵，这促使公司尽可能提高成本效益，并促使大多数公司使用AI驱动的助手或“聊天机器人”。另一方面，客户仍然希望有人与人之间的互动，特别是在处理复杂情境如争议和敏感话题如账单支付时。¹¹1[https://bit.ly/3yaNO9t](https://bit.ly/3yaNO9t)
- en: This raises the bar for customer service agents. They need to accurately understand
    the customer’s question or concern, identify a solution that is acceptable yet
    feasible (and within the company’s policy), all while handling multiple conversations
    at once.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这提高了客户服务代理的标准。他们需要准确理解客户的问题或关切，找到一个既可接受又可行的解决方案（并符合公司政策），同时处理多个对话。
- en: In this work, we introduce “Ask Me Anything” (AMA) as an add-on feature to an
    agent-facing customer service interface. AMA allows agents to ask questions to
    a large language model (LLM) on demand, as they are handling customer conversations—the
    LLM provides accurate responses in real-time, reducing the amount of context switching
    the agent needs. In our internal experiments, we find that agents using AMA versus
    a traditional search experience spend approximately 10% fewer seconds per conversation
    containing a search, translating to millions of dollars of savings annually. Agents
    that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating
    its usefulness as an AI-assisted feature for customer care.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入了“随问随答”（AMA）作为面向客服的接口的附加功能。AMA允许代理在处理客户对话时按需向大型语言模型（LLM）提问——LLM实时提供准确的响应，减少了代理所需的上下文切换。在我们的内部实验中，我们发现使用AMA的代理相比传统搜索体验每次对话节省了大约10%的时间，这转化为每年数百万美元的节省。使用AMA功能的代理大约80%的时间提供了积极反馈，证明其作为客户服务的AI辅助功能的有效性。
- en: 'rag, llm, customer care, assistive AI, vector db, reranking^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: Proceedings of the 47th International
    ACM SIGIR Conference on Research and Development in Information Retrieval; July
    14–18, 2024; Washington, DC, USA^†^†doi: 10.1145/3626772.3661345^†^†isbn: 979-8-4007-0431-4/24/07^†^†ccs:
    Information systems Retrieval models and ranking^†^†ccs: Information systems Language
    models'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'rag, llm, customer care, assistive AI, vector db, reranking^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: Proceedings of the 47th International
    ACM SIGIR Conference on Research and Development in Information Retrieval; July
    14–18, 2024; Washington, DC, USA^†^†doi: 10.1145/3626772.3661345^†^†isbn: 979-8-4007-0431-4/24/07^†^†ccs:
    Information systems Retrieval models and ranking^†^†ccs: Information systems Language
    models'
- en: 1\. Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Comcast, like many other companies, provides customer service through various
    communication channels. Many self-service solutions are available on the mobile
    “Xfinity” app (e.g., reviewing latest bill) which also has an option to chat with
    an AI-powered bot named “Xfinity Assistant”. While these digital automation capabilities
    have been replacing human customer representatives (also referred to as ”agents”)
    for many tasks, there are still many situations that require human-to-human interactions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Comcast和许多其他公司一样，通过各种沟通渠道提供客户服务。许多自助服务解决方案可以在移动端的“Xfinity”应用上找到（例如，查看最新账单），其中还可以选择与名为“Xfinity
    Assistant”的AI驱动机器人聊天。虽然这些数字化自动化能力已经取代了许多任务中的人工客服代表（也称为“代理”），但仍有许多情况需要人与人之间的互动。
- en: A customer trying to simply look up information about their profile, internet
    services, or bill, they should be able to do it without an agent’s assistance.
    This also holds true if they are trying to carry out a relatively straightforward
    task like rescheduling their appointment or make a change to their services.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 客户如果只是想查询关于他们的个人资料、互联网服务或账单的信息，他们应该能够在不需要代理帮助的情况下完成。这同样适用于他们试图执行像重新安排预约或更改服务这样相对简单的任务。
- en: Past studies show a human-human interaction is preferred over a human-computer
    one in certain customer service situations(Wowak, [[n. d.]](#bib.bib22)). For
    example, agents might outperform bots in situations that require creative problem
    solving. In other situations, the customer might simply prefer to talk to a agent
    to benefit from their empathy and emotional intelligence, or to navigate through
    cultural sensitivities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的研究表明，在某些客户服务场景中，人际互动优于人与计算机的互动（Wowak, [[n. d.]](#bib.bib22)）。例如，在需要创造性解决问题的情况下，代理可能会胜过机器人。在其他情况下，客户可能仅仅偏好与代理交谈，以利用其同理心和情感智力，或应对文化敏感问题。
- en: At Comcast, an internal custom tool suite aims to help agents to effectively
    and efficiently handle such conversations. However, it still often requires manually
    looking up information in multiple places, relating it to what the customer is
    saying, then crafting a relevant response that aligns with the communication guidelines.
    In this paper, we introduce a new feature to this tool suite called “Ask Me Anything”
    (AMA). It leverages large language models (LLMs) following a retrieval-augmented
    generation (RAG) approach to generate contextually relevant responses by combining
    internal knowledge sources, indexing existing knowledge articles efficiently at
    build time, retrieving relevant chunks of text for a given question at query time,
    then feeding them to a *Reader* LLM to generate a succinct answer with citations
    provided as reference. In the next section, we describe the methodology in more
    detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在康卡斯特（Comcast），一个内部定制工具套件旨在帮助代理有效且高效地处理此类对话。然而，它仍然经常需要手动在多个地方查找信息，将其与客户所说的内容相关联，然后制定符合沟通指南的相关回应。在本文中，我们介绍了这个工具套件中的一项新功能——“问我任何事”（AMA）。它利用大型语言模型（LLMs），采用检索增强生成（RAG）方法，通过结合内部知识来源、在构建时高效地索引现有知识文章、在查询时检索相关文本片段，然后将其提供给一个*Reader*
    LLM，以生成简洁的回答，并提供作为参考的引用。接下来的部分，我们将详细描述这一方法论。
- en: 2\. Methodology
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 方法论
- en: 'Our system follows a typical RAG implementation with modifications to improve
    performance on proprietary questions. First, the documents are preprocessed to
    text and chunked, the chunks are embedded then stored with metadata (e.g., associated
    URL for citations, an identifier, the title, etc.) in a vector database. We describe
    our specific choices for processing and embeddings in Section [2.1](#S2.SS1 "2.1\.
    Document Preprocessing ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses
    LLMs to Assist Agents in Real Time") and Section [2.2](#S2.SS2 "2.2\. Retrieving
    Relevant Text Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses
    LLMs to Assist Agents in Real Time") respectively with some experimental justification.
    Next, we detail how we train and evaluate a reranking model using synthetic data
    to improve search result relevancy in Section [2.3](#S2.SS3 "2.3\. Reranking Search
    Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist
    Agents in Real Time"). Finally, we discuss how we generate answers followed by
    how we evaluate the system in Section [2.4](#S2.SS4 "2.4\. Generating the Answer
    from Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to
    Assist Agents in Real Time") and [2.5](#S2.SS5 "2.5\. Offline Response Evaluation
    ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents
    in Real Time").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统遵循典型的 RAG 实现，并做了一些修改以提高对专有问题的性能。首先，文档被预处理为文本并分块，块被嵌入后存储在带有元数据（例如，引用的相关
    URL、标识符、标题等）的向量数据库中。我们在[2.1](#S2.SS1 "2.1\. 文档预处理 ‣ 2\. 方法论 ‣ “问我任何事”：康卡斯特如何利用
    LLM 实时协助代理")节和[2.2](#S2.SS2 "2.2\. 检索相关文本片段 ‣ 2\. 方法论 ‣ “问我任何事”：康卡斯特如何利用 LLM 实时协助代理")节详细描述了我们在处理和嵌入中的具体选择，并提供了一些实验性理由。接下来，我们在[2.3](#S2.SS3
    "2.3\. 重新排序搜索结果 ‣ 2\. 方法论 ‣ “问我任何事”：康卡斯特如何利用 LLM 实时协助代理")节详细说明了如何使用合成数据训练和评估重新排序模型，以提高搜索结果的相关性。最后，我们在[2.4](#S2.SS4
    "2.4\. 从片段生成答案 ‣ 2\. 方法论 ‣ “问我任何事”：康卡斯特如何利用 LLM 实时协助代理")节和[2.5](#S2.SS5 "2.5\.
    离线回应评估 ‣ 2\. 方法论 ‣ “问我任何事”：康卡斯特如何利用 LLM 实时协助代理")节讨论了如何生成答案以及如何评估系统。
- en: 2.1\. Document Preprocessing
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 文档预处理
- en: We receive documents from various internal clients in different formats. We
    standardize the documents into plain text and chunk each document into snippets
    using Deepset.ai’s Haystack library (Pietsch et al., [2019](#bib.bib14)). In order
    to uniquely reference each chunk of every document after retrieval, we assign
    an origin identifier to each document and a local identifier to each chunk. Finally,
    we implement role-based access control on each document, so different users can
    only view the documents for which they have permission.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接收来自不同内部客户的各种格式的文档。我们将这些文档标准化为纯文本，并使用 Deepset.ai 的 Haystack 库（Pietsch et al.,
    [2019](#bib.bib14)）将每个文档分块。为了在检索后唯一引用每个文档的每个块，我们为每个文档分配一个源标识符，为每个块分配一个本地标识符。最后，我们在每个文档上实施基于角色的访问控制，以便不同的用户只能查看他们有权限访问的文档。
- en: 'In [1(a)](#S2.T1.st1 "1(a) ‣ 2.1\. Document Preprocessing ‣ 2\. Methodology
    ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time"), we
    show various chunking parameters for Haystack’s preprocessor and their evaluation
    scores. The metric derivation is explained in Section [2.5](#S2.SS5 "2.5\. Offline
    Response Evaluation ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs
    to Assist Agents in Real Time") (Answer quality assumed the top 3 items were passed
    to the LLM). We observed a large improvement from setting a higher `max_chars_check`,
    which we used as a proxy for limiting the size of each snippet given to the LLM.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[1(a)](#S2.T1.st1 "1(a) ‣ 2.1\. 文档预处理 ‣ 2\. 方法论 ‣ “问我任何问题”：康卡斯特如何利用LLMs实时协助代理")中，我们展示了Haystack预处理器的各种切分参数及其评估分数。指标推导详见[2.5节](#S2.SS5
    "2.5\. 离线响应评估 ‣ 2\. 方法论 ‣ “问我任何问题”：康卡斯特如何利用LLMs实时协助代理")（答案质量假设前3个条目传递给LLM）。我们观察到，通过设置更高的`max_chars_check`，即限制每个片段的大小，对LLM的输入有了显著改进。
- en: Table 1. Chunking parameters and evaluation of three different settings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 三种不同设置的切分参数和评估。
- en: '| Parameter | A | B | C |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | A | B | C |'
- en: '| clean_empty_lines | true |  |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 清理空行 | true |  |  |'
- en: '| clean_whitespace | true |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 清理空白字符 | true |  |  |'
- en: '| clean_header_footer | true |  |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 清理标题和尾部 | true |  |  |'
- en: '| split_by | word |  |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 按词切分 | word |  |  |'
- en: '| split_length | 300 | 100 |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 切分长度 | 300 | 100 |  |'
- en: '| split_overlap | 50 | 25 |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 切分重叠 | 50 | 25 |  |'
- en: '| split_respect_sentence_boundary | true |  |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 尊重句子边界切分 | true |  |  |'
- en: '| max_chars_check | 1000 |  | 3000 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 最大字符检查 | 1000 |  | 3000 |'
- en: '| Metric |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 指标 |  |  |  |'
- en: '| Answer Quality | - | -5.7% | +13.2% |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 答案质量 | - | -5.7% | +13.2% |'
- en: '| MRR | - | -13.3% | 0.0% |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| MRR | - | -13.3% | 0.0% |'
- en: '| R@3 | - | -7.9% | 0.0% |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| R@3 | - | -7.9% | 0.0% |'
- en: '| NDCG | - | -10.0% | 0.0% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| NDCG | - | -10.0% | 0.0% |'
- en: '(a) For clarity, only changes from setting $A$. Metrics are defined in Section
    [2.5](#S2.SS5 "2.5\. Offline Response Evaluation ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了清晰起见，仅展示了来自设置 $A$ 的变化。指标定义见[2.5节](#S2.SS5 "2.5\. 离线响应评估 ‣ 2\. 方法论 ‣ “问我任何问题”：康卡斯特如何利用LLMs实时协助代理")。
- en: 2.2\. Retrieving Relevant Text Snippets
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 检索相关文本片段
- en: To inform the choice of our retriever model, we conducted pilot experiments
    on a curated evaluation set of fifty question–answer pairs. We searched the in-production
    system logs for queries starting with a WH-word (who, what, how, etc.) or ending
    with a question mark, roughly following the procedure on Bing query logs from
    WikiQA (Yang et al., [2015](#bib.bib25)). For each question, we then located the
    relevant passage and answer span in our internal knowledge base used by agents.
    Queries without answers were also labeled as such. Crucially, this process avoids
    back-formulation (Sakai et al., [2004](#bib.bib18)), where queries are manually
    written by annotators based on known passages rather than crawled from logs, resulting
    in biased evaluation sets.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通知我们检索模型的选择，我们在一个包含五十对问答对的精选评估集上进行了初步实验。我们从生产系统日志中搜索以WH词（who, what, how等）开头或以问号结尾的查询，基本遵循WikiQA (Yang
    et al., [2015](#bib.bib25))上Bing查询日志的程序。对于每个问题，我们随后在代理使用的内部知识库中定位相关的段落和答案范围。没有答案的查询也被标记为此。关键是，这个过程避免了反向构造(Sakai
    et al., [2004](#bib.bib18))，即由注释员根据已知片段手动编写查询，而不是从日志中抓取，从而避免了偏见的评估集。
- en: 'We experimented with both dense and sparse retrieval models. For the sparse
    model, we used Okapi BM25 (Robertson et al., [2009](#bib.bib17)) with $k_{1}=1.0$.
    For the dense ones, we experimented with four: dense passage retrieval (DPR) (Karpukhin
    et al., [2020](#bib.bib10)), fine-tuned on Natural Questions (Kwiatkowski et al.,
    [2019](#bib.bib11)); MPNet-base (v1) (Song et al., [2020](#bib.bib19)), trained
    on 160GB of text corpora including Wikipedia, BookCorpus (Zhu et al., [2015](#bib.bib27)),
    and OpenWebText (Gokaslan and Cohen, [2019](#bib.bib7)); OpenAI’s state-of-the-art
    `ada-002` embeddings model; and MPNet-base v2, trained further on one billion
    sentence pairs for better embedding quality.²²2Nils Reimers’s open-source contribution: 
    [https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)
    Each was deemed to satisfy our computational and financial constraints at inference
    time.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对密集和稀疏检索模型进行了实验。对于稀疏模型，我们使用了Okapi BM25 (Robertson et al., [2009](#bib.bib17))，设置$k_{1}=1.0$。对于密集模型，我们进行了四种实验：密集片段检索
    (DPR) (Karpukhin et al., [2020](#bib.bib10))，在Natural Questions上微调 (Kwiatkowski
    et al., [2019](#bib.bib11))；MPNet-base (v1) (Song et al., [2020](#bib.bib19))，在包括Wikipedia、BookCorpus (Zhu
    et al., [2015](#bib.bib27))和OpenWebText (Gokaslan and Cohen, [2019](#bib.bib7))在内的160GB文本语料上训练；OpenAI最先进的`ada-002`嵌入模型；以及MPNet-base
    v2，进一步在十亿对句子上训练，以提高嵌入质量。²²2Nils Reimers的开源贡献：[https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354)每个模型在推理时都符合我们的计算和财务限制。
- en: 'In [2(a)](#S2.T2.st1 "2(a) ‣ 2.2\. Retrieving Relevant Text Snippets ‣ 2\.
    Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real
    Time"), we report the recall@3 (R@3) and the mean reciprocal rank (MRR) of these
    models on our evaluation set. The choice of recall@3 (versus recall@5 or 10) is
    from us feeding the top-three retrieved passages into the LLM. As a sanity check,
    we also ran a baseline that randomly drew a passage, which unsurprisingly yielded
    low scores. Mirroring prior work (Yang et al., [2019](#bib.bib24)), we found that
    BM25 remains a strong baseline, outperforming DPR in R@3 and MRR, respectively.
    We conjecture that this results from Natural Questions being substantially out
    of domain from our data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2(a)](#S2.T2.st1 "2(a) ‣ 2.2\. 检索相关文本片段 ‣ 2\. 方法论 ‣ “问我任何问题”：康卡斯特如何利用LLM实时协助代理")中，我们报告了这些模型在我们的评估集上的recall@3
    (R@3)和平均倒排排名 (MRR)。选择recall@3（与recall@5或10相比）是因为我们将前三个检索到的片段输入到LLM中。作为 sanity
    check，我们还运行了一个基线模型，随机抽取片段，结果显然得分较低。与之前的工作 (Yang et al., [2019](#bib.bib24)) 相似，我们发现BM25仍然是一个强大的基线模型，在R@3和MRR上均优于DPR。我们推测这可能是因为Natural
    Questions与我们的数据领域差距较大。
- en: Table 2. Results of various retrievers on our pilot evaluation set
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 我们的试点评估集上各种检索器的结果
- en: '| Method | Recall@3 | MRR |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Recall@3 | MRR |'
- en: '| Random | -71.4% | -83.9% |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | -71.4% | -83.9% |'
- en: '| BM25 | - | - |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | - | - |'
- en: '| DPR (`single-nq`) | -42.8% | -42.9% |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DPR (`single-nq`) | -42.8% | -42.9% |'
- en: '| DPR (`multiset-nq`) | -23.8% | -29.0% |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| DPR (`multiset-nq`) | -23.8% | -29.0% |'
- en: '| Multi-QA MPNet-base | +33.0% | +39.7% |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Multi-QA MPNet-base | +33.0% | +39.7% |'
- en: '| OpenAI embeddings (`ada-002`) | +33.0% | +53.9% |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI嵌入 (`ada-002`) | +33.0% | +53.9% |'
- en: '| MPNet-base v2 | +38.1% | +54.9% |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| MPNet-base v2 | +38.1% | +54.9% |'
- en: (a) Statistics presented as relative difference from BM25, i.e., $100\cdot(\mu-\mu_{BM25})/\mu_{BM25}$
    . Underline denotes statistical significance relative to DPR.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 统计数据以相对差异呈现，相对于BM25，即 $100\cdot(\mu-\mu_{BM25})/\mu_{BM25}$。下划线表示相对于DPR的统计显著性。
- en: We observe MPNet-base (v1), OpenAI’s `ada-002`, and MPNet-base (v2) to perform
    similarly. Signed-rank tests for R@3 and $t$) from DPR. Due to operational convenience
    and the high performance of OpenAI’s ADA embeddings, we used ADA for the retriever
    component for the final system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到MPNet-base (v1)、OpenAI的`ada-002`和MPNet-base (v2)的表现类似。对DPR的R@3和$t$进行了签名秩检验。由于OpenAI的ADA嵌入的操作便利性和高性能，我们在最终系统中选择了ADA作为检索器组件。
- en: For our production retrieval step, we embedded both the title of the article
    and the text of the individual chunk and added them together prior to storage
    in the vector database. Anecdotally, we found this to yield a more comprehensive
    retrieval for a variety of queries, especially when chunks were missing some descriptive
    context of the topic of the article.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的生产检索步骤，我们将文章标题和单个片段的文本嵌入在一起，并在存储到向量数据库之前将其合并。根据经验，我们发现这能在各种查询中提供更全面的检索，特别是在片段缺少一些关于文章主题的描述性上下文时。
- en: 2.3\. Reranking Search Results
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 重新排序搜索结果
- en: We found that reranking results using models finetuned on synthetic data improved
    the retrieval step. Our approach was inspired by previous synthetic data generation
    approaches (Bonifacio et al., [2022](#bib.bib2); Dai et al., [2022](#bib.bib4)).
    First, we used GPT-4 to generate synthetic questions from each snippet in our
    dataset. We then ran each question through our search system using OpenAI’s `text-embeddings-ada-002`
    (Greene et al., [2022](#bib.bib9)) embeddings. Any questions where the original
    snippet used for question generation did not appear in the top 20 results were
    discarded. For each synthetic question, we stored the top 20 items retrieved,
    their relevance as scored by `BGE-reranker-large` (Xiao et al., [2023](#bib.bib23)),
    and an indicator that the snippet was the source of the question. The final rankings
    were determined by first placing the source snippet as the ”most relevant” result,
    followed by the snippets in most relevant order as scored by the `BGE-reranker-large`
    model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，使用在合成数据上微调的模型进行重排名可以改善检索步骤。我们的方法受到以前合成数据生成方法的启发 (Bonifacio et al., [2022](#bib.bib2);
    Dai et al., [2022](#bib.bib4))。首先，我们使用 GPT-4 从数据集中的每个片段生成合成问题。然后，我们使用 OpenAI 的
    `text-embeddings-ada-002` (Greene et al., [2022](#bib.bib9)) 嵌入将每个问题运行通过我们的搜索系统。任何原始片段用于生成问题的片段未出现在前
    20 个结果中的问题都会被丢弃。对于每个合成问题，我们存储了检索到的前 20 项、由 `BGE-reranker-large` (Xiao et al.,
    [2023](#bib.bib23)) 评分的相关性，以及一个指示片段是问题来源的标志。最终排名是通过首先将来源片段置于“最相关”结果中，然后按 `BGE-reranker-large`
    模型评分的最相关顺序排列片段来确定的。
- en: 'For training, we used RankNet (Burges et al., [2005](#bib.bib3)) to distill
    these rankings into a finetuned MPNet (Song et al., [2020](#bib.bib19)), in particular
    `all-mpnet-base-v2` from `sentence-transformer` (Reimers and Gurevych, [2019](#bib.bib16)),
    which has fewer parameters requiring less computational resources to deploy into
    production than `BGE-reranker-large`. The final dataset after constructing the
    necessary pairs for RankNet consisted of over 10 million examples. We set aside
    0.5% of the examples as validation dataset. Our training parameters were listed
    in Table [3](#S2.T3 "Table 3 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology
    ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time"). We
    used `DistributedDataParallel` from PyTorch (Paszke et al., [2017](#bib.bib13))
    for distributed training, so the effective batch size is the number of GPUs multiplied
    by the batch size. We found the ”Linear Scaling Rule”, where one scales the learning
    rate when the batch size increases, to not apply to our use case (Goyal et al.,
    [2018](#bib.bib8)), but we suspect it is because the original MPNet architecture
    was trained with a much larger batch size than we used for finetuning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练过程中，我们使用了 RankNet (Burges et al., [2005](#bib.bib3)) 将这些排名精炼为一个微调的 MPNet
    (Song et al., [2020](#bib.bib19))，特别是 `sentence-transformer` (Reimers 和 Gurevych,
    [2019](#bib.bib16)) 的 `all-mpnet-base-v2`，它比 `BGE-reranker-large` 需要更少的参数和计算资源来部署到生产环境中。在为
    RankNet 构建必要的对后，最终的数据集包含了超过 1000 万个样本。我们将 0.5% 的样本作为验证数据集。我们的训练参数列在表 [3](#S2.T3
    "Table 3 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time") 中。我们使用了 PyTorch (Paszke
    et al., [2017](#bib.bib13)) 的 `DistributedDataParallel` 进行分布式训练，因此有效的批量大小是 GPU
    数量乘以批量大小。我们发现“线性缩放规则”，即在批量大小增加时缩放学习率，并不适用于我们的使用案例 (Goyal et al., [2018](#bib.bib8))，但我们怀疑这可能是因为原始
    MPNet 架构是在比我们用于微调的更大批量大小上训练的。'
- en: '[b] Parameter Specification Learning Rate $5\times 10^{-6}$ Batch Size 8 Number
    of GPUs¹ 10 Warmup Steps 4000 Weight Decay 0.001 Epochs 1 Total Training Steps
    171391 Learning Rate Scheduler Warmup-constant'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[b] 参数规格 学习率 $5\times 10^{-6}$ 批量大小 8 GPU 数量¹ 10 热身步骤 4000 权重衰减 0.001 训练周期
    1 总训练步骤 171391 学习率调度器 热身-常数'
- en: Table 3. Training hyperparameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 训练超参数。
- en: '1'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: GPU type: g4dn.xlarge (Nvidia T4)
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GPU 类型: g4dn.xlarge (Nvidia T4)'
- en: 'To further evaluate the performance of our reranker model, we randomly sampled
    10,000 real questions asked by customer service agents in our production system.
    For every retrieved document, we followed the approach in (Thomas et al., [2023](#bib.bib21)),
    which showed that an LLM can accurately predict the relevancy of search results.
    Specifically, GPT-4 was used to evaluate the overall quality of each document
    to the question, which combined the scores from how the document matches the intent
    of the question as well as how trustworthy the document is. The final integer
    score ranged between 0 and 2, with higher score meaning higher overall quality.
    Table [4](#S2.T4 "Table 4 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology ‣
    “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time") compares
    multiple metrics between ADA vs. reranker. Since the overall score is non-binary,
    we compute MRR using the rank of first document with a score of 2, and recall@3
    examines whether the top 3 documents contain any documents with a score of 2\.
    The results indicate an improvement in retrieval performance with the reranker
    model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步评估我们的重新排序模型的性能，我们随机抽取了在生产系统中由客服人员提问的10,000个真实问题。对于每个检索到的文档，我们遵循了（Thomas
    et al., [2023](#bib.bib21)）的方法，该方法展示了大型语言模型（LLM）能够准确预测搜索结果的相关性。具体而言，使用了GPT-4来评估每个文档与问题的整体质量，这结合了文档与问题意图匹配的分数以及文档的可信度。最终的整数分数在0到2之间，分数越高，整体质量越高。表[4](#S2.T4
    "Table 4 ‣ 2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time")比较了ADA与重新排序模型的多个指标。由于总体评分是非二元的，我们使用第一个得分为2的文档的排名来计算MRR，并且recall@3检查前3个文档中是否包含得分为2的文档。结果表明，重新排序模型在检索性能上有所提升。'
- en: Table 4. ADA vs. Reranker Search Results using Production Questions
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 使用生产问题的ADA与重新排序搜索结果
- en: '| Metric | ADA | Reranker |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | ADA | 重新排序模型 |'
- en: '| --- | --- | --- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Recall@3 | - | +12% |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Recall@3 | - | +12% |'
- en: '| MRR | - | +15% |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| MRR | - | +15% |'
- en: '| NDCG | - | +4.8% |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| NDCG | - | +4.8% |'
- en: (a) For clarity, only changes from setting ADA are found in the table. The metric
    values are the relative difference from ADA.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了清晰起见，表中仅列出了相对于ADA的变化。指标值是相对于ADA的相对差异。
- en: 2.4\. Generating the Answer from Snippets
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 从片段中生成答案
- en: In generating the answer, we follow the conventional wisdom approach in the
    RAG literature. We begin our prompt with a preamble of guidelines for the model,
    followed by the task description. Due to the length of our snippets of text from
    the knowledge base, we are unable to provide few-shot examples. We have anecdotally
    found it better to include more of the text to avoid necessary information being
    cut off at random. To avoid the ”lost in the middle” problem (Liu et al., [2023](#bib.bib12)),
    we reverse the order of the Top K results when passed into the LLM, formatted
    as XML capturing the ID, title and content of the result. We used OpenAI’s `gpt-3.5-turbo`
    for our production Reader component. As a final step in our prompt, we ask the
    LLM to answer the given question using the search results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成答案时，我们遵循RAG文献中的传统智慧方法。我们以模型指南的前言开始提示，然后是任务描述。由于知识库中的文本片段较长，我们无法提供少量示例。我们发现最好包括更多的文本，以避免必要信息被随机截断。为了避免“中间丢失”的问题（Liu
    et al., [2023](#bib.bib12)），我们在传递给LLM时，反转Top K结果的顺序，并格式化为XML，捕捉结果的ID、标题和内容。我们在生产的阅读组件中使用了OpenAI的`gpt-3.5-turbo`。作为我们提示的最后一步，我们要求LLM使用搜索结果回答给定的问题。
- en: 2.4.1\. Citations
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 引用
- en: 'An important product feature of of the AMA solution is providing references
    to agents so they can learn more about the answer given. This can be seen in various
    RAG implementations, such as Microsoft Copilot. In addition, the goal was to build
    confidence in the system’s output and drive adoption internally. Inspired by the
    Fact-Checking Rail (Rebedea et al., [2023](#bib.bib15)), our Citation Rail was
    accomplished by prompting the LLM to cite its sources in a specific manner (c.f.,
    Figure [1](#S2.F1 "Figure 1 ‣ 2.4.1\. Citations ‣ 2.4\. Generating the Answer
    from Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses LLMs to
    Assist Agents in Real Time")) combined with a post processing step where the citations
    were removed from the text. If no citations were found, then the system would
    not return the answer. Practically, there was another benefit from an observability
    perspective: through this approach, we identified most ”no answer” responses from
    the LLM, as typically the LLM would response similarly to ”I’m sorry. I was unable
    to find the answer in the documents” without a citation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'AMA 解决方案的一个重要产品特性是向代理提供参考，以便他们可以更深入地了解所给出的答案。这一点可以在各种 RAG 实现中看到，例如 Microsoft
    Copilot。此外，目标是建立对系统输出的信心并推动内部采纳。受到 Fact-Checking Rail (Rebedea et al., [2023](#bib.bib15))
    的启发，我们的 Citation Rail 通过以特定方式提示 LLM 引用其来源（见图 [1](#S2.F1 "Figure 1 ‣ 2.4.1\. Citations
    ‣ 2.4\. Generating the Answer from Snippets ‣ 2\. Methodology ‣ “Ask Me Anything”:
    How Comcast Uses LLMs to Assist Agents in Real Time")）完成，然后通过去除文本中的引用进行后处理。如果没有找到引用，系统将不返回答案。从可观察性的角度来看，还有另一个好处：通过这种方法，我们识别出了大多数
    LLM 的“无答案”响应，因为 LLM 通常会对“对不起。我无法在文档中找到答案”做出类似的回应，而没有引用。'
- en: '[⬇](data:text/plain;base64,UGxlYXNlIGluY2x1ZGUgYSBzaW5nbGUgc291cmNlIGF0IHRoZSBlbmQgb2YgeW91ciBhbnN3ZXIsIGkuZS4sIFtEb2N1bWVudDBdIGlmIERvY3VtZW50MCBpcyB0aGUgc291cmNlLiBJZiB0aGVyZSBpcyBtb3JlIHRoYW4gb25lIHNvdXJjZSwgdXNlIFtEb2N1bWVudDBdW0RvY3VtZW50MV0gaWYgRG9jdW1lbnQwIGFuZCBEb2N1bWVudDEgYXJlIHRoZSBzb3VyY2VzLg==)Please  include  a  single  source  at  the  end  of  your  answer,  i.e.,  [Document0]  if  Document0  is  the  source.  If  there  is  more  than  one  source,  use  [Document0][Document1]  if  Document0  and  Document1  are  the  sources.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,UGxlYXNlIGluY2x1ZGUgYSBzaW5nbGUgc291cmNlIGF0IHRoZSBlbmQgb2YgeW91ciBhbnN3ZXIsIGkuZS4sIFtEb2N1bWVudDBdIGlmIERvY3VtZW50MCBpcyB0aGUgc291cmNlLiBJZiB0aGVyZSBpcyBtb3JlIHRoYW4gb25lIHNvdXJjZSwgdXNlIFtEb2N1bWVudDBdW0RvY3VtZW50MV0gaWYgRG9jdW1lbnQwIGFuZCBEb2N1bWVudDEgYXJlIHRoZSBzb3VyY2VzLg==)请在答案末尾包含一个来源，例如，[Document0]
    如果 Document0 是来源。如果有多个来源，请使用 [Document0][Document1] 如果 Document0 和 Document1 是来源。'
- en: Figure 1. An example component of a prompt to encourage citations from the LLM
    used in the system prompt section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 系统提示部分用于鼓励 LLM 引用的提示示例组件。
- en: 2.5\. Offline Response Evaluation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 离线响应评估
- en: To evaluate the system’s responses, we follow the LLM-as-a-judge methodology
    (Zhu et al., [2023](#bib.bib26)), in addition to metrics around retrieval quality
    typical of a search system. In particular, a random sample of questions from customers
    were pulled from production traffic. Human annotators then wrote correct answers
    to each query using internal knowledge bases that are also available to the AMA
    system. We were able to compare system answers to correct responses given by human
    annotators using GPT-4 to compute ”Answer Quality”.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估系统的响应，我们遵循了 LLM-as-a-judge 方法论 (Zhu et al., [2023](#bib.bib26))，以及检索质量的相关指标，这些指标通常与搜索系统相关。特别地，我们从生产流量中抽取了客户的随机问题。然后，人工注释者使用
    AMA 系统也能访问的内部知识库为每个查询编写了正确答案。我们能够使用 GPT-4 计算“答案质量”，将系统答案与人工注释者提供的正确答案进行比较。
- en: For each question, the annotators also provided a citation from which their
    answers were based. We used this to calculate ”Citation Match Rate”:  the percentage
    of cases in which the citation from the AMA system matched the ground truth. Given
    that our retrieval step returns a list, we calculated Recall@K by assuming the
    annotated citation is the only relevant document.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个问题，注释者还提供了他们回答的引用来源。我们使用这些引用计算“引用匹配率”： AMA 系统中的引用与真实情况相匹配的案例百分比。鉴于我们的检索步骤返回一个列表，我们通过假设注释引用是唯一相关文档来计算
    Recall@K。
- en: 'Table [5](#S2.T5 "Table 5 ‣ 2.5\. Offline Response Evaluation ‣ 2\. Methodology
    ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time") shows
    key metrics for the same two approaches as in Table [4](#S2.T4 "Table 4 ‣ 2.3\.
    Reranking Search Results ‣ 2\. Methodology ‣ “Ask Me Anything”: How Comcast Uses
    LLMs to Assist Agents in Real Time") (`text-embedding-ada-002` for dense retrieval
    of relevant documents and rerankering the ADA-retrieved documents using our finetuned
    model). We observe that using reranked documents, LLM is able to achieve a higher
    answer quality meaning that the answer from a different document ranking is more
    accurate according to GPT-4\. The improvement can also be explained by the increased
    Citation Match Rate and Recall@3 from the reranked documents directly influencing
    the LLM’s ability to answer accurately.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [5](#S2.T5 "表格 5 ‣ 2.5\. 离线响应评估 ‣ 2\. 方法论 ‣ “Ask Me Anything”: Comcast 如何使用LLMs实时辅助代理人")
    展示了与表格 [4](#S2.T4 "表格 4 ‣ 2.3\. 重新排序搜索结果 ‣ 2\. 方法论 ‣ “Ask Me Anything”: Comcast
    如何使用LLMs实时辅助代理人") 中相同两种方法的关键指标（`text-embedding-ada-002`用于稠密检索相关文档并使用我们微调过的模型对ADA检索的文档进行重新排名）。我们观察到，使用重新排名的文档，LLM能够实现更高的答案质量，这意味着根据GPT-4，不同文档排序的答案更准确。这一改善还可以通过重新排名的文档中的引文匹配率和召回率@3的增加来解释，这直接影响LLM的准确回答能力。'
- en: Table 5. Response Quality
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5. 响应质量
- en: '| Metric | ADA | Reranker |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | ADA | 重新排序器 |'
- en: '| --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Answer Quality | - | +5.9% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 答案质量 | - | +5.9% |'
- en: '| Citation Match Rate | - | +2.5% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 引文匹配率 | - | +2.5% |'
- en: '| Recall@3 | - | +16.5% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 召回率@3 | - | +16.5% |'
- en: (a) For clarity, only changes from setting ADA are found in the table. The metric
    values are the relative difference from ADA.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了清晰起见，表格中仅显示与设置ADA不同的更改。指标值是相对于ADA的差异。
- en: 3\. Deploying AMA to Customer Service Agents
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 将AMA应用于客服代理
- en: Due to business sensitivity purposes, this section will obscure some details
    related to monetary business metrics. The system was piloted with hundreds of
    chat agents in late 2023\. Over the course of a month-long trial, chat handling
    time improved 10% when agents used AMA versus the traditional search option, which
    required the agent to open a new tool and perform a search. We believe this is
    a good proxy metric for answer quality because an inaccurate or incomplete response
    from AMA would require the agent to start over and revert to the traditional option,
    duplicating work and taking more time overall. Explicit feedback, via a simple
    thumbs up/thumbs down UI element, was also collected from agents, with nearly
    an 80% positive feedback rate (there is no baseline for this rate as such feedback
    was not requested before the release of this feature). Shortly after the trial
    period, the system was rolled out to all chat agents (in thousands), with AMA-driven
    search becoming the preferred way of searching, accounting for two thirds of all
    typed queries.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 出于业务敏感性目的，本节将隐藏与货币业务指标相关的某些细节。该系统在2023年底与数百名聊天代理人进行了试点。在为期一个月的试用期内，当代理人使用AMA时，与传统搜索选项相比，聊天处理时间提高了10%，传统搜索选项需要代理人打开新的工具并进行搜索。我们认为这是一个很好的答案质量的代理指标，因为AMA提供的不准确或不完整的响应将要求代理人重新开始并返回传统选项，重复工作并花费更多时间。代理人还通过简单的点赞/点踩用户界面收集了反馈，其中近80%的反馈是积极的（由于此功能发布之前未要求此类反馈，因此没有基线数据）。试用期结束后不久，该系统推广到所有聊天代理人（数千人），AMA驱动的搜索成为首选搜索方式，占所有键入查询的三分之二。
- en: 4\. Online Reranker Experiment
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 在线重新排序实验
- en: 'Shortly after the trial from Section [3](#S3 "3\. Deploying AMA to Customer
    Service Agents ‣ “Ask Me Anything”: How Comcast Uses LLMs to Assist Agents in
    Real Time") concluded, we began an A/B test of the reranker module described in
    Section [2.3](#S2.SS3 "2.3\. Reranking Search Results ‣ 2\. Methodology ‣ “Ask
    Me Anything”: How Comcast Uses LLMs to Assist Agents in Real Time"). The control
    variant used only the ADA embeddings for vector retrieval with no reranking component,
    and the treatment utilized the reranker component on the top $20$ when metrics
    considered user feedback, as responses were sparse. Due to the limited pool of
    agents, our randomization unit, we utilized an agent-day randomization similar
    to the cookie-day randomization found in other large systems (Tang et al., [2010](#bib.bib20))
    to increase statistical power. It has been shown in the literature (Deng et al.,
    [2017](#bib.bib6)) that violations of the independent and identically distributed
    (IID) assumption can lead to underestimation of the variance, but these tests
    can still be considered trustworthy in practice by using smaller significance
    thresholds and when observing larger effect sizes. The delta method (Deng et al.,
    [2018](#bib.bib5)) was employed to estimate the variance from question-level metrics.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [3](#S3 "3\. 部署 AMA 到客户服务代理 ‣ “随问随答”：康卡斯特如何利用 LLM 实时协助代理") 节试验结束后不久，我们开始对第
    [2.3](#S2.SS3 "2.3\. 重新排序搜索结果 ‣ 2\. 方法论 ‣ “随问随答”：康卡斯特如何利用 LLM 实时协助代理") 节中描述的重新排序模块进行
    A/B 测试。对照组仅使用 ADA 嵌入进行向量检索，没有重新排序组件，而处理组在考虑用户反馈时对前 $20$ 项使用了重新排序组件，因为回答稀疏。由于代理池的限制，我们的随机化单位，我们采用了类似于其他大型系统中发现的
    cookie-day 随机化的 agent-day 随机化 (Tang et al., [2010](#bib.bib20)) 以提高统计功效。文献中已显示
    (Deng et al., [2017](#bib.bib6)) 违反独立同分布 (IID) 假设可能会导致方差低估，但通过使用较小的显著性阈值和观察较大的效应大小，这些测试在实践中仍然可以被认为是可信的。我们使用了
    delta 方法 (Deng et al., [2018](#bib.bib5)) 来估计问题级别指标的方差。
- en: We observed a statistically significant increase in two of our metrics: namely
    the ”No Answer Rate”, which is the number of queries with no answer divided by
    the total number of queries, and the ”Positive Feedback Rate”, defined as the
    number of thumbs up divided by the count of feedback received. Downstream business
    metrics like average handle time and escalation rate showed no significant difference.
    However, the improvement in No Answer Rate implies that the system was able to
    handle more questions than before by providing the relevant documents to the LLM
    while also increasing the rate of positive feedback.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到两个指标有统计学上显著的增加：即“无答案率”，它是没有答案的查询数量除以总查询数量，以及“积极反馈率”，定义为点赞数量除以收到的反馈数量。下游商业指标如平均处理时间和升级率没有显著差异。然而，无答案率的改进表明，该系统能够处理比以前更多的问题，通过向
    LLM 提供相关文档，同时也提高了积极反馈的比例。
- en: Table 6. A/B Test Results
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. A/B 测试结果
- en: '| Metric | Effect | p-value |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 效果 | p 值 |'
- en: '| --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| No Answer Rate | -11.9% | p ¡ .001 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 无答案率 | -11.9% | p ¡ .001 |'
- en: '| Positive Feedback Rate | +8.9% | p ¡ .05 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 积极反馈率 | +8.9% | p ¡ .05 |'
- en: (a) Table contains relative change from control as the effect. Lower is better
    for No Answer Rate.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 表格包含相对于对照组的效果的相对变化。无答案率越低越好。
- en: 5\. Conclusions
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: In this paper, we introduced AMA, a large-scale solution to a common business
    need: efficient high-quality customer care. Through the use of third-party LLMs
    and proven RAG methodology, we were able to build AMA pretty quickly and demonstrate
    clear value as an assistive feature. We showed improvements to retrieval and answer
    quality with specific choices for the document preprocessing, the retrieval model
    and its embeddings, as well as a custom reranker model. As we deploy AMA to thousands
    of agents with tangible business benefits, we believe that this provides a good
    example of how humans and AI can collaborate to better serve customers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 AMA，这是一个解决常见商业需求的规模化方案：高效的高质量客户服务。通过使用第三方 LLM 和经过验证的 RAG 方法，我们能够迅速构建
    AMA，并展示其作为辅助功能的明确价值。我们通过对文档预处理、检索模型及其嵌入、以及自定义重新排序模型的特定选择，展示了检索和回答质量的改进。随着我们将 AMA
    部署到数千名代理人，并带来实际的商业利益，我们相信这提供了一个很好的例子，说明人类和 AI 如何协作以更好地服务客户。
- en: References
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Bonifacio et al. (2022) Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and
    Rodrigo Nogueira. 2022. InPars: Data Augmentation for Information Retrieval using
    Large Language Models. arXiv:2202.05144 [cs.CL]'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bonifacio 等（2022）Luiz Bonifacio、Hugo Abonizio、Marzieh Fadaee 和 Rodrigo Nogueira。2022年。《InPars:
    使用大型语言模型的数据增强以进行信息检索》。arXiv:2202.05144 [cs.CL]'
- en: Burges et al. (2005) Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt
    Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient
    descent. In *Proceedings of the 22nd International Conference on Machine Learning*
    (Bonn, Germany) *(ICML ’05)*. Association for Computing Machinery, New York, NY,
    USA, 89–96. [https://doi.org/10.1145/1102351.1102363](https://doi.org/10.1145/1102351.1102363)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burges 等（2005）Chris Burges、Tal Shaked、Erin Renshaw、Ari Lazier、Matt Deeds、Nicole
    Hamilton 和 Greg Hullender。2005年。《使用梯度下降的排序学习》。发表于 *第22届国际机器学习大会*（波恩，德国） *(ICML
    ’05)*。计算机协会，纽约，NY，美国，89–96。 [https://doi.org/10.1145/1102351.1102363](https://doi.org/10.1145/1102351.1102363)
- en: 'Dai et al. (2022) Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing
    Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2022. Promptagator:
    Few-shot Dense Retrieval From 8 Examples. arXiv:2209.11755 [cs.CL]'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等（2022）Zhuyun Dai、Vincent Y. Zhao、Ji Ma、Yi Luan、Jianmo Ni、Jing Lu、Anton
    Bakalov、Kelvin Guu、Keith B. Hall 和 Ming-Wei Chang。2022年。《Promptagator: 从 8 个示例中进行少样本密集检索》。arXiv:2209.11755
    [cs.CL]'
- en: 'Deng et al. (2018) Alex Deng, Ulf Knoblich, and Jiannan Lu. 2018. Applying
    the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas. In *Proceedings
    of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining* (London, United Kingdom) *(KDD ’18)*. Association for Computing Machinery,
    New York, NY, USA, 233–242. [https://doi.org/10.1145/3219819.3219919](https://doi.org/10.1145/3219819.3219919)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2018）Alex Deng、Ulf Knoblich 和 Jiannan Lu。2018年。《在度量分析中应用 Delta 方法：具有新颖想法的实用指南》。发表于
    *第24届 ACM SIGKDD 国际知识发现与数据挖掘大会*（伦敦，英国） *(KDD ’18)*。计算机协会，纽约，NY，美国，233–242。 [https://doi.org/10.1145/3219819.3219919](https://doi.org/10.1145/3219819.3219919)
- en: 'Deng et al. (2017) Alex Deng, Jiannan Lu, and Jonthan Litz. 2017. Trustworthy
    Analysis of Online A/B Tests: Pitfalls, challenges and solutions. In *Proceedings
    of the Tenth ACM International Conference on Web Search and Data Mining* (Cambridge,
    United Kingdom) *(WSDM ’17)*. Association for Computing Machinery, New York, NY,
    USA, 641–649. [https://doi.org/10.1145/3018661.3018677](https://doi.org/10.1145/3018661.3018677)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2017）Alex Deng、Jiannan Lu 和 Jonthan Litz。2017年。《可靠的在线 A/B 测试分析：陷阱、挑战与解决方案》。发表于
    *第十届 ACM 国际网络搜索与数据挖掘大会*（剑桥，英国） *(WSDM ’17)*。计算机协会，纽约，NY，美国，641–649。 [https://doi.org/10.1145/3018661.3018677](https://doi.org/10.1145/3018661.3018677)
- en: Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. OpenWebText
    Corpus. [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gokaslan 和 Cohen（2019）Aaron Gokaslan 和 Vanya Cohen。2019年。《OpenWebText Corpus》。
    [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus)。
- en: 'Goyal et al. (2018) Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis,
    Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
    2018. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv:1706.02677 [cs.CV]'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等（2018）Priya Goyal、Piotr Dollár、Ross Girshick、Pieter Noordhuis、Lukasz
    Wesolowski、Aapo Kyrola、Andrew Tulloch、Yangqing Jia 和 Kaiming He。2018年。《准确的大型小批量
    SGD：在 1 小时内训练 ImageNet》。arXiv:1706.02677 [cs.CV]
- en: Greene et al. (2022) Ryan Greene, Ted Sanders, Lilian Weng, and Arvind Neelakantan.
    2022. *New and improved embedding model*. [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greene 等（2022）Ryan Greene、Ted Sanders、Lilian Weng 和 Arvind Neelakantan。2022年。*新改进的嵌入模型*。
    [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model)
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage
    Retrieval for Open-Domain Question Answering. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*. 6769–6781.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin 等（2020）Vladimir Karpukhin、Barlas Oguz、Sewon Min、Patrick Lewis、Ledell
    Wu、Sergey Edunov、Danqi Chen 和 Wen-tau Yih。2020年。《开放域问答的密集段落检索》。发表于 *2020 年自然语言处理实证方法会议（EMNLP）*。6769–6781。
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question
    answering research. *Transactions of the Association for Computational Linguistics*
    7 (2019), 453–466.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski 等人（2019）Tom Kwiatkowski、Jennimaria Palomaki、Olivia Redfield、Michael
    Collins、Ankur Parikh、Chris Alberti、Danielle Epstein、Illia Polosukhin、Jacob Devlin、Kenton
    Lee 等。2019年。《自然问题：问答研究的基准》。*计算语言学学会会刊* 7（2019），453–466。
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle:
    How Language Models Use Long Contexts. arXiv:2307.03172 [cs.CL]'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）Nelson F. Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio
    Petroni 和 Percy Liang。2023年。《迷失在中间：语言模型如何使用长上下文》。arXiv:2307.03172 [cs.CL]
- en: Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,
    Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam
    Lerer. 2017. Automatic differentiation in PyTorch. (2017).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人（2017）Adam Paszke、Sam Gross、Soumith Chintala、Gregory Chanan、Edward
    Yang、Zachary DeVito、Zeming Lin、Alban Desmaison、Luca Antiga 和 Adam Lerer。2017年。《PyTorch中的自动微分》。
    (2017)。
- en: 'Pietsch et al. (2019) Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch,
    Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir
    Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee. 2019. Haystack:
    the end-to-end NLP framework for pragmatic builders. [https://github.com/deepset-ai/haystack](https://github.com/deepset-ai/haystack).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pietsch 等人（2019）Malte Pietsch、Timo Möller、Bogdan Kostic、Julian Risch、Massimiliano
    Pippi、Mayank Jobanputra、Sara Zanzottera、Silvano Cerza、Vladimir Blagojevic、Thomas
    Stadelmann、Tanay Soni 和 Sebastian Lee。2019年。《Haystack：面向实用构建者的端到端 NLP 框架》。 [https://github.com/deepset-ai/haystack](https://github.com/deepset-ai/haystack)。
- en: 'Rebedea et al. (2023) Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher
    Parisien, and Jonathan Cohen. 2023. NeMo Guardrails: A Toolkit for Controllable
    and Safe LLM Applications with Programmable Rails. arXiv:2310.10501 [cs.CL]'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebedea 等人（2023）Traian Rebedea、Razvan Dinu、Makesh Sreedhar、Christopher Parisien
    和 Jonathan Cohen。2023年。《NeMo Guardrails：一个用于可控和安全的 LLM 应用程序的工具包，具有可编程 Rails》。arXiv:2310.10501
    [cs.CL]
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics. [http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych（2019）Nils Reimers 和 Iryna Gurevych。2019年。《Sentence-BERT：使用
    Siamese BERT 网络的句子嵌入》。发表于 *2019年自然语言处理领域方法学会议论文集*。计算语言学协会。[http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084)
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: BM25 and beyond. *Foundations and Trends in
    Information Retrieval* 3, 4 (2009), 333–389.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson 等人（2009）Stephen Robertson、Hugo Zaragoza 等。2009年。《概率相关框架：BM25及其扩展》。*信息检索基础与趋势*
    3，4（2009），333–389。
- en: Sakai et al. (2004) Tetsuya Sakai, Yoshimi Saito, Yumi Ichimura, Tomoharu Kokubu,
    and Makoto Koyama. 2004. The effect of back-formulating questions in question
    answering evaluation. In *Proceedings of the 27th annual international ACM SIGIR
    conference on Research and development in information retrieval*. 474–475.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakai 等人（2004）Tetsuya Sakai、Yoshimi Saito、Yumi Ichimura、Tomoharu Kokubu 和 Makoto
    Koyama。2004年。《在问答评估中回溯公式化问题的效果》。发表于 *第27届国际ACM SIGIR信息检索研究与发展年会论文集*。474–475。
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    2020. MPNet: Masked and Permuted Pre-training for Language Understanding. arXiv:2004.09297 [cs.CL]'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2020）Kaitao Song、Xu Tan、Tao Qin、Jianfeng Lu 和 Tie-Yan Liu。2020年。《MPNet：用于语言理解的掩蔽和置换预训练》。arXiv:2004.09297
    [cs.CL]
- en: 'Tang et al. (2010) Diane Tang, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer.
    2010. Overlapping Experiment Infrastructure: More, Better, Faster Experimentation.
    In *Proceedings 16th Conference on Knowledge Discovery and Data Mining*. Washington,
    DC, 17–26.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2010）Diane Tang、Ashish Agarwal、Deirdre O’Brien 和 Mike Meyer。2010年。《重叠实验基础设施：更多、更好、更快的实验》。发表于
    *第16届知识发现与数据挖掘会议论文集*。华盛顿特区，17–26。
- en: Thomas et al. (2023) Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar
    Mitra. 2023. Large language models can accurately predict searcher preferences.
    arXiv:2309.10621 [cs.IR]
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomas 等人（2023）Paul Thomas、Seth Spielman、Nick Craswell 和 Bhaskar Mitra。2023年。《大型语言模型可以准确预测搜索者偏好》。arXiv:2309.10621
    [cs.IR]
- en: 'Wowak ([n. d.]) Kaitlin Wowak. [n. d.]. Humans vs. automation: Service center
    agents can outperform technology, study shows. [https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/](https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wowak ([n. d.]) 凯特琳·沃瓦克 Kaitlin Wowak。[n. d.]。人类与自动化：服务中心代理能够超越技术，研究表明。[https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/](https://news.nd.edu/news/humans-vs-automation-service-center-agents-can-outperform-technology-study-shows/)
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
    2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL]'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) 萧世涛 Shitao Xiao、刘正 Zheng Liu、张佩天 Peitian Zhang 和穆尼霍夫 Niklas
    Muennighoff。2023年。C-Pack: 推进通用中文嵌入的打包资源。arXiv:2309.07597 [cs.CL]'
- en: 'Yang et al. (2019) Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically
    examining the ”neural hype”: weak baselines and the additivity of effectiveness
    gains from neural ranking models. In *Proceedings of the 42nd international ACM
    SIGIR conference on research and development in information retrieval*. 1129–1132.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) 杨伟 Wei Yang、卢匡 Kuang Lu、杨佩林 Peilin Yang 和吉米·林 Jimmy Lin。2019年。批判性地检验“神经炒作”：神经排序模型的弱基线和效果增益的可加性。在
    *第42届国际ACM SIGIR信息检索研究与发展会议论文集* 中。1129–1132。
- en: 'Yang et al. (2015) Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA:
    A challenge dataset for open-domain question answering. In *Proceedings of the
    2015 conference on empirical methods in natural language processing*. 2013–2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2015) 杨懿 Yi Yang、叶文涛 Wen-tau Yih 和克里斯托弗·米克 Christopher Meek。2015年。WikiQA:
    一个用于开放领域问答的挑战数据集。在 *2015年自然语言处理经验方法会议论文集* 中。2013–2018。'
- en: 'Zhu et al. (2023) Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. JudgeLM:
    Fine-tuned Large Language Models are Scalable Judges. arXiv:2310.17631 [cs.CL]'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) 梁辉 Zhu、王兴刚 Xinggang Wang 和王欣龙 Xinlong Wang。2023年。JudgeLM:
    微调的大型语言模型是可扩展的裁判。arXiv:2310.17631 [cs.CL]'
- en: 'Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,
    Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies:
    Towards story-like visual explanations by watching movies and reading books. In
    *Proceedings of the IEEE international conference on computer vision*. 19–27.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2015) 朱宇坤 Yukun Zhu、瑞安·基罗斯 Ryan Kiros、理查德·泽梅尔 Rich Zemel、鲁斯兰·萨拉赫图丁诺夫
    Ruslan Salakhutdinov、拉奎尔·乌尔塔孙 Raquel Urtasun、安东尼奥·托拉尔巴 Antonio Torralba 和桑贾·费德勒
    Sanja Fidler。2015年。对齐书籍与电影：通过观看电影和阅读书籍来实现故事般的视觉解释。在 *IEEE国际计算机视觉会议论文集* 中。19–27。
