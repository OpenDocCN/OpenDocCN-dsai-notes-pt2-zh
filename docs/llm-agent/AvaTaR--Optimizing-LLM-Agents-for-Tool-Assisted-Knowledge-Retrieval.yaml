- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11200](https://ar5iv.labs.arxiv.org/html/2406.11200)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shirley Wu^§, Shiyu Zhao^§, Qian Huang^§, Kexin Huang^§, Michihiro Yasunaga^§,
    Kaidi Cao^§
  prefs: []
  type: TYPE_NORMAL
- en: Vassilis N. Ioannidis^†, Karthik Subbian^†, Jure Leskovec^(∗§), James Zou^(∗§)
  prefs: []
  type: TYPE_NORMAL
- en: ^∗Equal senior authorship.
  prefs: []
  type: TYPE_NORMAL
- en: ^§Department of Computer Science, Stanford University   ^†Amazon
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language model (LLM) agents have demonstrated impressive capability in
    utilizing external tools and knowledge to boost accuracy and reduce hallucinations.
    However, developing the prompting techniques that make LLM agents able to effectively
    use external tools and knowledge is a heuristic and laborious task. Here, we introduce
    AvaTaR, a novel and automatic framework that optimizes an LLM agent to effectively
    use the provided tools and improve its performance on a given task/domain. During
    optimization, we design a comparator module to iteratively provide insightful
    and holistic prompts to the LLM agent via reasoning between positive and negative
    examples sampled from training data. We demonstrate AvaTaR on four complex multimodal
    retrieval datasets featuring textual, visual, and relational information. We find
    AvaTaR consistently outperforms state-of-the-art approaches across all four challenging
    tasks and exhibits strong generalization ability when applied to novel cases,
    achieving an average relative improvement of 14% on the Hit@1 metric. Code and
    dataset are available at [https://github.com/zou-group/avatar.](https://github.com/zou-group/avatar)
    ^†^†footnotetext: Correspondence: {shirwu, jure, jamesz}@cs.stanford.edu'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autonomous agents powered by large language models (LLMs) offer substantial
    promise for complex problem-solving [[37](#bib.bib37), [48](#bib.bib48), [56](#bib.bib56),
    [35](#bib.bib35), [7](#bib.bib7)]. These agents demonstrate remarkable capabilities
    in reasoning [[42](#bib.bib42), [41](#bib.bib41), [48](#bib.bib48), [47](#bib.bib47)]
    and planning [[13](#bib.bib13), [53](#bib.bib53), [14](#bib.bib14), [8](#bib.bib8)].
    Moreover, their functionality is extended by employing external tools that provide
    access to external/private data and specialized operations, such as APIs for interacting
    with knowledge bases and search engines. These tools empower agents to perform
    complex tasks like multi-step problem-solving and retrieving diverse information,
    which is essential for complex question-answering and retrieval [[36](#bib.bib36),
    [34](#bib.bib34), [43](#bib.bib43), [22](#bib.bib22), [13](#bib.bib13), [19](#bib.bib19),
    [29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the promising capabilities of LLM agents, it remains challenging to
    engineer effective prompts that guide these agents through a multi-stage procedure
    for real-world problem-solving. This procedure includes (1) decomposing a complex
    question into an actionable plan with simpler steps, (2) strategically utilizing
    provided tools to gather relevant information, and, finally, (3) synthesize the
    intermediate results to produce a coherent and accurate response. Each step requires
    extensive manual effort and numerous iterations of trial and error to perfect
    the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Current approaches have primarily focused on directly deploying agents using
    complex human-designed “mega-prompts” [[20](#bib.bib20), [48](#bib.bib48), [16](#bib.bib16)],
    which requires lots of manual trial-and-error. Nevertheless, such hand-engineered
    mega-prompts may also result in brittle implementations with suboptimal accuracy
    (see Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ AvaTaR: Optimizing LLM Agents
    for Tool-Assisted Knowledge Retrieval") (a)), where the ReAct agent [[48](#bib.bib48)]
    easily produces trivial and misleading answers to customers’ queries about specific
    products. Furthermore, existing research [[45](#bib.bib45), [40](#bib.bib40),
    [5](#bib.bib5), [6](#bib.bib6), [55](#bib.bib55), [49](#bib.bib49), [52](#bib.bib52)]
    on employing LLMs as optimizers often fails to adequately refine the complex strategies
    for enhancing tool integration and usage. This lack of strategic optimization
    can lead to less effective, non-generalizable agent applications in complex real-world
    scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Present work: AvaTaR. To address these challenges, we introduce AvaTaR, an
    automated framework that optimizes Agents for effective Tool utilization and excellent
    task performance. We demonstrate our framework on challenging and common tasks
    of knowledge base Retrieval, which involves the complex multi-stage procedure
    and extensive tool usage. To highlight, we leverage the key insights from contrastive
    learning and build a comparator module (“trainer”) to generate holistic instructions/prompts
    (i.e., compute robust “gradient”) to optimize an actor LLM. Specifically, AvaTaR
    includes two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1781230d4db98a5648bdb9f01076ef1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of AvaTaR. AvaTaR consists of an actor LLM and a comparator
    LLM. (a) During optimization, the actor generates actions to answer queries leveraging
    the provided tools. Then the comparator contrasts a set of well-performing (positive)
    and poorly-performing (negative) queries, and automatically generates holistic
    prompts to teach the actor more effective retrieval strategies and tool usage
    (cf. Section [4](#S4 "4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step
    Tasks ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval")).
    (b) At deployment, the optimized actor can be effectively utilized to answer new
    queries.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimization phase. The core of our optimization framework (Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge
    Retrieval")) is a comparator LLM that automatically generates holistic prompts
    to teach an actor LLM to differentiate between best-performing and poor-performing
    tool usage. The comparator takes positive and negative data samples on which the
    current agent performs well and poorly, respectively, to identify the overall
    gap and systematic errors exhibited by the agent. Unlike per-sample instructions
    which easily lead to overfitting on individual data point, by constructing multiple
    samples as a “batch”, comparator can extract a more robust “gradient” to “backpropagate”
    to the actor. In other words, the comparator can provide more effective and adaptive
    prompts through such batch-wise contrastive reasoning, promoting the agent to
    identify flaws in solving the challenging multi-stage problem. Following previous
    methods [[37](#bib.bib37), [26](#bib.bib26), [54](#bib.bib54), [49](#bib.bib49)],
    we also maintain a memory bank with selected past instructions to prevent the
    actor LLM from repeating previous mistakes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deployment phase. The iterative optimization through our AvaTaR framework updates
    actor for more effective and generalizable knowledge extraction, allowing direct
    generalization to novel user inquiries in the deployment time. In Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge
    Retrieval") (b), the optimized actor creates three novel strategies 1) the precise
    decomposition of problems via extracting multifaceted attributes, 2) effective
    tool usage via a sophisticated and robust scoring system, and 3) The strategic
    combination of different scores, determined by the learned coefficients, ensures
    accurate and comprehensive retrieval.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Experimental evaluation. We conduct extensive experiments on four retrieval
    datasets. The retrieval tasks are of high complexity with multimodal data, including
    textual, visual, and relational information. AvaTaR consistently outperforms state-of-the-art
    methods, showing a substantial 14% improvement in the Hit@1 metric. Impressively,
    with only 25 iterations, AvaTaR boosts the Hit@1 metric from an initial 5.1% to
    28.6% on Flickr30K-Entities[[31](#bib.bib31)] and the Recall@20 metric from 30.3%
    to 39.3% on STaRK-Prime[[44](#bib.bib44)]. These improvements, achieved through
    iterative updates to the prompts, underscore AvaTaR ’s ability to optimize agents
    for complex task and tool usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our key contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce AvaTaR, a novel framework that optimizes an actor for effective
    tool utilization through a comparator module that automatically generate holistic
    prompts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We demonstrate that AvaTaR on four challenging retrieval tasks, which significantly
    outperforms existing agent methods in terms of the task performance and generalization
    ability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comprehensive analysis on the actor’s evolution during optimization,
    highlighting how comparator automatically provides targeted instructions that
    improve and generalize the actor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f8072563ea71a5308d3f710e70104c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison between AvaTaR and ReAct. (a) The ReAct agent exhibits
    incomplete task decomposition and employs suboptimal tool combinations like lengthy
    string matching, leading to poor task performance. (b) AvaTaR decomposes the task
    into multiple steps such as type filtering and flexible token matching. Moreover,
    it implements robust tool usage and precise synthesis with learned parameters
    from the optimization phase to achieve excellent performance on new queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM Agents. Recent research has been leveraging the remarkable language understanding
    and reasoning abilities of LLMs [[42](#bib.bib42), [47](#bib.bib47), [2](#bib.bib2),
    [37](#bib.bib37), [48](#bib.bib48)] to complete downstream tasks. For complex
    tasks that require more capabilities, previous works regard LLMs as agents that
    can interact with the environments [[7](#bib.bib7), [36](#bib.bib36), [22](#bib.bib22),
    [43](#bib.bib43), [13](#bib.bib13), [19](#bib.bib19), [16](#bib.bib16), [48](#bib.bib48),
    [5](#bib.bib5), [23](#bib.bib23)] and leverage external tools [[35](#bib.bib35),
    [29](#bib.bib29), [57](#bib.bib57), [34](#bib.bib34), [58](#bib.bib58), [7](#bib.bib7),
    [27](#bib.bib27), [32](#bib.bib32), [24](#bib.bib24)]. For example, ReAct [[48](#bib.bib48)]
    conducts reasoning and action in an interleaved way, which retrieves information
    from Wikipedia to support reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Agents for Retrieval. Previous research has applied LLM agents for Information
    Retrieval (IR) systems, through pretraining [[9](#bib.bib9), [50](#bib.bib50),
    [3](#bib.bib3), [15](#bib.bib15)], reranking [[12](#bib.bib12), [38](#bib.bib38)],
    and prompting techniques [[16](#bib.bib16), [11](#bib.bib11)]. In the IR systems,
    retriever module directly influences the performance of downstream tasks, such
    as retrieval-augmented generation [[18](#bib.bib18), [25](#bib.bib25), [26](#bib.bib26)]
    and knowledge-intensive question answering [[30](#bib.bib30), [46](#bib.bib46)].
    For example, EHRAgent [[36](#bib.bib36)] is designed for EHR question-answering,
    capable of retrieving relevant clinical knowledge through a structured tool-use
    planning process and an interactive coding mechanism. However, these LLM agents
    usually take heuristic (zero-shot) prompts or rely on few-shot examples [[21](#bib.bib21),
    [48](#bib.bib48), [16](#bib.bib16), [36](#bib.bib36)] to apply to downstream tasks,
    which lack more informed guidance on generating effective retrieval strategies
    and tool-assisted actions.
  prefs: []
  type: TYPE_NORMAL
- en: Agent Optimization. In the field of improving LLM agents, previous works have
    modified the parameters of LLM backbones through finetuning or instruction tuning
    to enhance agent capability [[29](#bib.bib29), [51](#bib.bib51), [33](#bib.bib33),
    [4](#bib.bib4), [17](#bib.bib17), [28](#bib.bib28)] or generated better prompts
    through iterative prompt tuning [[45](#bib.bib45), [49](#bib.bib49), [40](#bib.bib40),
    [16](#bib.bib16), [11](#bib.bib11)]. Recently, Zhang et al. [[52](#bib.bib52)]
    conducted agent training by iteratively updating the agents’ functions according
    to the execution history. However, these methods do not explicitly consider targeted
    optimization on tool usage and the impact on complex multi-stage tasks. Moreover,
    improving the generalization ability of agents [[10](#bib.bib10), [39](#bib.bib39),
    [27](#bib.bib27)], which is essential for real-world applications, has received
    less attention. In our work, we focus on automatically generating holistic instructions
    via a novel contrastive reasoning mechanism, targeting effective tool usage and
    agents’ generalization ability.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Definition 1: Tools. We define tools or APIs as a set of implemented functions
    with specified input and output variables. We denote the abstract tool space as
    $\mathcal{T}=\{f_{k}:\mathcal{I}_{f_{k}}\rightarrow\mathcal{O}_{f_{k}}\mid k=1,2,\ldots\}$.
    For example, the tools can be APIs used for accessing external knowledge given
    a search index, an encoder model that generates vector representations from text
    or image data, or a task-specific classifier that outputs probabilities over a
    list of classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 2: Agents. An LLM agent, defined as $\mathcal{A}:\mathcal{P}\rightarrow\alpha$
    renders the results for the task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-step problem-solving. Real-world problems are inherently complex and
    cannot be effectively addressed with straightforward solution or simple tool usage
    alone. Using LLM agents for solving real-world problems can be structured into
    a multi-stage procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decomposition of the problem: The procedure begins by breaking down a complex
    question into an actionable plan characterized by simpler steps. This decomposition
    is crucial for setting clear objectives and facilitating focused problem-solving.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tool-assisted subproblem solving: In the subsequent phase, agents strategically
    utilize tools from the established tool space $\mathcal{T}$ to gather solutions
    for each step. This stage is essential for acquiring the necessary information
    required to address each subproblem of the decomposed problem effectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Synthesis and response formulation: The final stage involves synthesizing the
    intermediate results to construct a precise response. This synthesis not only
    combines the data but may also refine the response through trials and adjustments,
    ensuring the solution’s accuracy and relevancy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, retrieval tasks are inherently complex and demanding. Given a user
    query $q$, which are used to compute the quality of the prediction. Specifically,
    the LLM agent is required to 1) comprehend a user’s request, followed by 2) using
    the provided tools to identify and analyze relevant information in the large knowledge
    space, which may contain multi-modal data source. Finally, it requires 3) the
    integration of all gathered information to reason and generate an accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: '4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Key differences between AvaTaR and prevailing agent methods. AvaTaR
    demonstrates the capabilities to: 1) self-improve on specific tasks, 2) retain
    memory throughout the optimization process, 3) enhance the agent’s ability to
    generalize, and 4) autonomously generate holistic, high-quality prompts for better
    tool usage. Please refer to Section [4](#S4 "4 Our Method: Optimizing Agents for
    Tool-Assisted Multi-Step Tasks ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted
    Knowledge Retrieval") for details.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Self-Improvement | Memory | Generalization | Holistic Prompt Generation
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | (on Tool Usage) |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct [[48](#bib.bib48)] | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Self-refine [[23](#bib.bib23)] | ✔ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion [[37](#bib.bib37)] | ✔ | ✔ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR (Ours) | ✔ | ✔ | ✔ | ✔ |'
  prefs: []
  type: TYPE_TB
- en: 'Each step in the multi-stage problem-solving process (formulated in Section [3](#S3
    "3 Problem Formulation ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge
    Retrieval")) requires effective prompts to identify key flaws and improve task
    performance. However, perfecting the agents’ prompts requires extensive manual
    effort and numerous iterations of trial and error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we offer an automatic and novel optimization framework, AvaTaR,
    to generate prompts to improve agents’ tool usage and task performance. In Table [1](#S4.T1
    "Table 1 ‣ 4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks
    ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval"), we highlight
    four critical aspects of our approach compared with the prevailing agent frameworks [[48](#bib.bib48),
    [37](#bib.bib37), [23](#bib.bib23)]. Here we introduce two main LLM components
    in AvaTaR, an actor LLM (Section [4.1](#S4.SS1 "4.1 Actor Construction and Challenges
    ‣ 4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks ‣ AvaTaR:
    Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval")) and a comparator
    LLM (Section [4.2](#S4.SS2 "4.2 Automate Holistic Instruction Generation with
    Comparator ‣ 4 Our Method: Optimizing Agents for Tool-Assisted Multi-Step Tasks
    ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Actor Construction and Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Actor. The actor agent follows the definition in Section [3](#S3 "3 Problem
    Formulation ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval"),
    which is responsible for generating initial actions given the initial instructions/prompts
    and modifying actions given other instructions. Specifically, the initial instructions
    provide details about the task and the existing tools, where the tools can be
    introduced in programming languages such as Python. During optimization, the prompts
    further incorporate the previous action sequence and updated instructions to modify
    these actions. The actor then generates revised actions, which could be a blend
    of tool usage through the programming language (code generation), along with natural
    language explanations of how the tools are used.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges on multi-step complex tasks. A common approach to updating instructions
    uses execution results or performance data from a specific instance, often through
    techniques like self-explanation [[23](#bib.bib23), [5](#bib.bib5)] or self-reflection [[37](#bib.bib37),
    [49](#bib.bib49)]. However, this may not be suitable for complex tasks involving
    tool usage. As complex multi-step tasks include several interacting factors that
    influence overall performance, such as problem decomposition and tool selections,
    these per-sample instructions tend to be narrow and fail to identify flaws across
    all components of a complex solution. Additionally, while certain tool combinations
    may be effective for one type of input, their effectiveness can vary with others,
    leading to decreased performance when applied across different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Automate Holistic Instruction Generation with Comparator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address the challenges, we construct a comparator LLM to update the instructions
    for the actor. Instead of optimizing on a sampled instance, comparator aims to
    identify systematic flaws throughout the structured actions/solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Constructing positive and negative queries. To realize this goal, in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AvaTaR: Optimizing LLM Agents for
    Tool-Assisted Knowledge Retrieval"), the comparator samples a group of data samples
    (question-answer pairs), executes the current actions for each question, and constructs
    well-performing (positive) and poorly-performing (negative) queries based on their
    execution results. Specifically, we define two thresholds, $\ell$ as a hyperparameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Generating instruction by contrastive reasoning. After that, the comparator
    is asked to contrast the two groups of queries based on their key characteristics,
    attribute the performance gap to particular tool usage in the complex solution,
    and finally suggest general modifications that can lead to overall improvement
    on task performance. The instructions generated by the comparator will then be
    appended to the initial prompts to update the actor.'
  prefs: []
  type: TYPE_NORMAL
- en: Insights/Justification for the comparator. To illustrate the insights, we draw
    an analogy from deep neural network training, where extremely small batch sizes
    can introduce significant noise in gradient estimates and high variance in model
    updates. By adopting a batched training strategy and sampling positive and negative
    queries as two “mini-batches”, comparator is able to extract a robust “gradient”
    to update the actor. This approach motivates comparator to generate more general
    and comprehensive instructions on the complex action sequence, including problem
    decomposition, solutions to subproblems, and the final synthesis. Moreover, as
    contrastive reasoning directly targets disentangling the performance gap related
    to input patterns and how they are handled differently by the tools, it is particularly
    effective in helping comparator differentiate and select tools for use. Finally,
    by identifying systemic flaws across a wide array of negative queries, comparator
    generates modifications that are not only tailored to individual samples but also
    to diverse data samples, offering benefits for better generalization to novel
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ad3aa1bc1e1028d30398fab59918b35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Demonstration example during optimization. Best viewed in color.
    The task of the comparator is to automatically generate instructions based on
    sampled positive and negative queries. Then comparator provides holistic instructions
    that guide the actor to improve query decomposition, utilize better tools, and
    incorporate more comprehensive information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demonstration example. Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Automate Holistic
    Instruction Generation with Comparator ‣ 4 Our Method: Optimizing Agents for Tool-Assisted
    Multi-Step Tasks ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval")
    shows an example where comparator contrasts the patterns of positive and negative
    queries, identifying discrepancies in tool usage within the action sequence. It
    reveals that compared to positive queries, negative queries feature more complex
    product descriptions, more subtle brand mentions, and additional relevant product
    mentions. These observations suggest: 1) an incomplete problem decomposition involving
    query attributes like detailed product features, 2) a potentially imprecise brand
    match using embedding similarity, and 3) a lack of consideration for related products
    in the results. Informed by these insights, actor updates its action sequence
    to include the additional sbuproblems and use the tools more effectively for the
    task, such as replacing the embedding tool with an LLM verification tool.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Logistic Instructions and Memory Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic instructions. While instructions from the comparator are designed to
    improve task performance, we incorporate two types of orthogonal instructions
    to ensure the actions are valid and can be executed efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validity check: This instruction is triggered internally during the execution
    of each action. It ensures the validity of the actor’s actions, such as verifying
    the proper use of function calls.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timeout Error: To preclude inefficient action sequences that may stall the
    actor, we implement a timeout mechanism that triggers an error when processing
    exceeds a threshold. This error prompt the actor to use more efficient strategies,
    such as eliminating redundant operations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Memory Bank. During optimization, we utilize a memory bank inspired by human
    decision-making processes, where humans typically address current problems by
    analyzing the current situation and referencing past experiences. The memory bank
    stores tuples of action sequences, instructions from comparator, and the performance
    of these action sequences on a small QA set (conducted during sampling positive
    and negative queries). To manage the context size input to actor , we take only
    the top-$5$ action sequences with the best performance. This memory bank enables
    actor to learn from both immediate instructions and historical results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment. At deployment (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval")), we apply
    the optimized actor /action sequence, which includes effective tool usage and
    problem-solving strategies, to directly answer user queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tasks and Evaluation. We conduct experiments on four challenging retrieval
    datasets (Appendix [A](#A1 "Appendix A Retrieval Tasks ‣ AvaTaR: Optimizing LLM
    Agents for Tool-Assisted Knowledge Retrieval")) from STaRK [[44](#bib.bib44)]
    and Flickr30K-Entities [[31](#bib.bib31)] to demonstrate AvaTaR in handling complex
    real-world task. For each query, the task is to retrieve relevant entities such
    as nodes in the knowledge graph or images in the knowledge bases. We assess task
    performance by comparing the consistency of the results with the ground truth
    answers in the datasets, where we use Hit@1, Hit@5, Recall@20, and Mean Reciprocal
    Rank (MRR) as the metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. Following Wu et al. [[44](#bib.bib44)], we employ several embedding-based
    retriever models for our evaluation: Vector Similarity Search (VSS) and Multi-Vector
    Similarity Search (Multi-VSS) using text-embedding-ada-002 from OpenAI; a relation-aware
    model, QAGNN [[50](#bib.bib50)], for the STaRK benchmark. Moreover, we include
    two prevailing agent frameworks to further enrich our evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReAct [[48](#bib.bib48)] conducts reasoning and action in an in-context and
    interleaved manner to enhance LLMs with the capability to interactively analyze
    observed information and perform actions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflexion [[37](#bib.bib37)] utilizes self-reflection on the current task completion
    and store these reflections in an episodic memory buffer to enhance decision-making
    in subsequent trials.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We also include an ablation model, AvaTaR-C, which removes the comparator in
    our optimization pipeline. By default, we use Claude 3 Opus as the backbone LLM
    for all agent approaches in the main paper and report part of results using GPT-4
    Turbo (0125) in Appendix [B](#A2 "Appendix B Additional Experimental Results ‣
    AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Function library. Our function library (Appendix [D](#A4 "Appendix D Function
    library ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval"))
    consists of twenty-eight functions that facilitate access to, operation on, and
    reasoning over the knowledge information by LLM agents. We used the same function
    library for ReAct, Reflexion, and AvaTaR agents.'
  prefs: []
  type: TYPE_NORMAL
- en: General pipeline. For AvaTaR, we leverage the same implementation, including
    the same structure of the initial prompts, the metric Recall@20 for constructing
    positive and negative queries, and hyperparameters ($\ell=h=0.5$), for all four
    datasets. Initially developed for STaRK-Amazon , this pipeline was then directly
    applied to the other datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Textual and Relational Retrieval Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We employ the Amazon, MAG, and Prime datasets from the STaRK benchmark [[44](#bib.bib44)],
    a large-scale semi-structured retrieval benchmark that integrates textual and
    relational knowledge (cf. detailed description in Appendix [A](#A1 "Appendix A
    Retrieval Tasks ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval")).
    Here, the entities to be retrieved are defined as nodes in a graph structure,
    the knowledge associated with each entity includes both textual descriptions and
    relational data about the entities. We used the official splits in STaRK benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway 1: AvaTaR outperforms the state-of-the-art models. Table [2](#S5.T2
    "Table 2 ‣ 5.1 Textual and Relational Retrieval Tasks ‣ 5 Experiments ‣ AvaTaR:
    Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval") shows that AvaTaR
    substantially outperforms leading models such as Reflexion across all metrics
    on the STaRK benchmark. To highlight, the average improvement of AvaTaR is 15.6%
    on Hit@1 and 9.5% on MRR. ReAct agents, however, can not optimized from instructions
    on better tool usage and tend to select tools based on the LLM’s prior knowledge,
    which may not be optimal for the given task. We observe that ReAct agents applied
    highly similar tools across various queries and are hard to “jump out of the box”
    for better tool usage even with extensive in-context reasoning. In Appendix [B](#A2
    "Appendix B Additional Experimental Results ‣ AvaTaR: Optimizing LLM Agents for
    Tool-Assisted Knowledge Retrieval"), we provide results of the agent methods using
    GPT-4 Turbo, which has similar conclusions discussed here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway 2: Comparator greatly impacts the actor’s performance. The comparison
    of AvaTaR with its ablation variant, AvaTaR-C, highlights the significant advantages
    of the comparator module. Although AvaTaR-C conducts validity and timeout checks,
    integrating Comparator into AvaTaR adds a comprehensive instruction mechanism
    that is crucial for identify clear directions to improve the agents, underlining
    comparator’s key role in optimizing actor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Retrieval performance (%) on STaRK benchmark. Last row shows the relative
    improvements over the best metric value in each column.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Amazon | MAG | Prime |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hit@1 | Hit@5 | R@20 | MRR | Hit@1 | Hit@5 | R@20 | MRR | Hit@1 | Hit@5
    | R@20 | MRR |'
  prefs: []
  type: TYPE_TB
- en: '| Dense Retr. | 15.29 | 47.93 | 44.49 | 30.20 | 10.51 | 35.23 | 42.11 | 21.34
    | 4.46 | 21.85 | 30.13 | 12.38 |'
  prefs: []
  type: TYPE_TB
- en: '| QAGNN | 26.56 | 50.01 | 52.05 | 37.75 | 12.88 | 39.01 | 46.97 | 29.12 | 8.85
    | 21.35 | 29.63 | 14.73 |'
  prefs: []
  type: TYPE_TB
- en: '| VSS | 39.16 | 62.73 | 53.29 | 50.35 | 29.08 | 49.61 | 48.36 | 38.62 | 12.63
    | 31.49 | 36.00 | 21.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-VSS | 40.07 | 64.98 | 55.12 | 51.55 | 25.92 | 50.43 | 50.80 | 36.94
    | 15.10 | 33.56 | 38.05 | 23.49 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 42.14 | 64.56 | 50.81 | 52.30 | 31.07 | 49.49 | 47.03 | 39.25 | 15.28
    | 31.95 | 33.63 | 22.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion | 42.79 | 65.05 | 54.70 | 52.91 | 40.71 | 54.44 | 49.55 | 47.06
    | 14.28 | 34.99 | 38.52 | 24.82 |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR-C | 40.92 | 63.63 | 53.68 | 51.73 | 33.25 | 52.17 | 47.88 | 41.34
    | 8.82 | 23.82 | 30.32 | 16.20 |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR | 49.87 | 69.16 | 60.57 | 58.70 | 44.36 | 59.66 | 50.63 | 51.15 |
    18.44 | 36.73 | 39.31 | 26.73 |'
  prefs: []
  type: TYPE_TB
- en: '| Relative | 16.6% | 6.3% | 9.9% | 12.2% | 9.6% | 2.1% | -0.3% | 8.7% | 20.7%
    | 5.0% | 2.1% | 7.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Improvement | ![Refer to caption](img/38cc352de7f161f974ca07996f71a23f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Optimization dynamics of AvaTaR agents on STaRK. The figures show
    the validation performance (solid line) and their moving average (dash line) during
    the optimization of AvaTaR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway 3: AvaTaR effectively improves the agents during optimization. Figure [4](#S5.F4
    "Figure 4 ‣ 5.1 Textual and Relational Retrieval Tasks ‣ 5 Experiments ‣ AvaTaR:
    Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval") illustrates the
    agents’ performance on the validation set during optimization. Impressively, we
    found that AvaTaR agents can significantly enhance performance, e.g., improving
    from 35% to 75% on Amazon and from 20% to 78% on MAG. This evidence strongly supports
    the effectiveness of the instructions generated by our comparator. Moreover, our
    memory bank, which stores past best-performing actions, encourages AvaTaR agents
    to gradually converge by the end of the optimization process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway 4: AvaTaR can generalize to real-world tasks. Comparator suggests
    instructions tailored to a group of queries, which encourages agents to make more
    general modifications and generalize to novel queries. We validate this improved
    generalization capability by directly applying the optimized actions to the leave-out
    queries generated by humans from STaRK benchmark. These human-generated queries
    exhibit a more distinct distribution than the question-answering pairs used to
    optimize the agents in our framework. We report the results in Table [4](#A2.T4
    "Table 4 ‣ Appendix B Additional Experimental Results ‣ AvaTaR: Optimizing LLM
    Agents for Tool-Assisted Knowledge Retrieval") (Appendix [B](#A2 "Appendix B Additional
    Experimental Results ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge
    Retrieval")) which shows that AvaTaR significantly outperforms other models, achieving
    an average improvement of 20.9% on Hit@1, demonstrating strong generalization
    across diverse human-generated queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Image Retrieval Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We further experiment on Flickr30K Entities [[31](#bib.bib31)], which focuses
    on image retrieval with 30k images representing the entity set. This dataset (Appendix [A](#A1
    "Appendix A Retrieval Tasks ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted
    Knowledge Retrieval")) enriches the entity information with pixel values and annotated
    bounding boxes along with their descriptive phrases. In Table [2](#S5.T2 "Table
    2 ‣ 5.1 Textual and Relational Retrieval Tasks ‣ 5 Experiments ‣ AvaTaR: Optimizing
    LLM Agents for Tool-Assisted Knowledge Retrieval"), AvaTaR again presents significant
    improvements. Particularly, the Reflexion agents struggle with this task due to
    “overfitting” where they are easily misled by specific image data, leading to
    inappropriate actions. For example, they might attempt to “extract the color of
    a hat” from images that do not contain a hat. In contrast, AvaTaR effectively
    avoids such pitfalls by employing the batch-wise contrastive reasoning that incorporates
    a more global perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway 5: AvaTaR generates impressive and generalizable actions. The final
    actions of the AvaTaR agent achieve the advanced performance reported in Figure [5](#S5.F5
    "Figure 5 ‣ 5.2 Image Retrieval Task ‣ 5 Experiments ‣ AvaTaR: Optimizing LLM
    Agents for Tool-Assisted Knowledge Retrieval") (left). We present these actions
    in Figure [8](#A3.F8 "Figure 8 ‣ Appendix C Prompts ‣ AvaTaR: Optimizing LLM Agents
    for Tool-Assisted Knowledge Retrieval") (Appendix [B](#A2 "Appendix B Additional
    Experimental Results ‣ AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge
    Retrieval")) due to space constraints. Notably, AvaTaR effectively handles the
    input query and impressively performs actions that leverage Inverse Document Frequency
    (IDF) scores to reweight phrase matching scores, ultimately synthesizing the final
    answers. Besides utilizing existing tools, AvaTaR agents can also develop high-level
    tools based on existing ones, such as the tool for reweighting phrase match scores
    by IDF scores. This capability suggests a future direction to maintain a dynamic
    tool library and generate instructions to enhance tool generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Hit@1 | Hit@5 | R@20 | MRR |'
  prefs: []
  type: TYPE_TB
- en: '| VSS (clip-vit-large-patch14) | 37.2 | 56.4 | 72.8 | 46.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct (claude3) | 38.8 | 54.8 | 71.6 | 46.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion (claude3) | 28.4 | 53.2 | 75.2 | 41.2 |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR-C (claude3) | 28.8 | 53.2 | 78.4 | 40.0 |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR (claude3) | 42.4 | 63.0 | 79.2 | 52.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Relative Improvement | 9.2% | 11.7% | 5.3% | 13.0% | ![Refer to caption](img/e2da32c7a288e84d408e37bcd01f8dbd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Performance (left) and AvaTaR’s optimization dynamics (right) on
    Flickr30K-Entities.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c86b117bdfef93e5a5ae576ae9de7858.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Representative instruction types from the comparator. We provide
    three cases where the comparator guides the actor towards (1) better divide-and-conquer
    strategies for multi-step problem-solving, (2) more sensible differentiation between
    good and bad tool usage/combinations, and (3) adjustments in the weights to generate
    the final answers. We record the number of occurrences $X$/25).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway 6: Emerging Behaviors during Optimization. In Figure [6](#S5.F6 "Figure
    6 ‣ 5.2 Image Retrieval Task ‣ 5 Experiments ‣ AvaTaR: Optimizing LLM Agents for
    Tool-Assisted Knowledge Retrieval"), we present concrete cases illustrating key
    interactions between actor and comparator . In each instance, comparator identifies
    critical flaws, including information omission, ineffective tool usage, and suboptimal
    synthesis of varying scores. The instructions subsequently prompt actor to enhance
    retrieval strategies, tool selection, and precise score combinations. Furthermore,
    the frequent references to tool usage underscore comparator ’s dedicated examination
    of tool utilization during optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we introduce AvaTaR, a novel framework that automates the optimization
    of LLM agents for enhanced tool utilization in multi-step problems with a focus
    on complex retrieval tasks. AvaTaR demonstrates remarkable improvements across
    four diverse datasets. This success can largely be attributed to the comparator
    module, which effectively refines agent performance through the iterative generation
    of holistic and strategic prompts. A key innovation of comparator is its use of
    contrastive reasoning with batch-wise sampling, enabling it to identify systemic
    flaws and extract robust "gradients" for comprehensive agent improvement across
    diverse scenarios. Future work can explore extending this methodology to other
    agent tasks and more dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank lab members in Zou and Leskovec’s labs for discussions and for providing
    feedback on our manuscript. We also gratefully acknowledge the support of DARPA
    under Nos. N660011924033 (MCS); NSF under Nos. OAC-1835598 (CINES), CCF-1918940
    (Expeditions), DMS-2327709 (IHBEM); Stanford Data Applications Initiative, Wu
    Tsai Neurosciences Institute, Stanford Institute for Human-Centered AI, Chan Zuckerberg
    Initiative, Amazon, Genentech, GSK, Hitachi, SAP, and UCB. The content is solely
    the responsibility of the authors and does not necessarily represent the official
    views of the funding entities.
  prefs: []
  type: TYPE_NORMAL
- en: REFERENCES
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besta et al. [[n. d.]] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, and Torsten Hoefler. [n. d.]. Graph of Thoughts: Solving Elaborate
    Problems with Large Language Models. ([n. d.]). arXiv:2308.09687'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borgeaud et al. [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick,
    Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer,
    Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,
    Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language
    Models by Retrieving from Trillions of Tokens. In *ICML*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023b] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, and Shunyu Yao. 2023b. FireAct: Toward Language Agent Fine-tuning.
    arXiv:2310.05915'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023a. Teaching Large Language Models to Self-Debug. (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. 2022. RLPrompt:
    Optimizing Discrete Text Prompts with Reinforcement Learning. In *EMNLP*. ACL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Durante et al. [[n. d.]] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong,
    Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos,
    Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, and Jianfeng Gao. [n. d.]. Agent
    AI: Surveying the Horizons of Multimodal Interaction. ([n. d.]). arXiv:2401.03568'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [[n. d.]] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante,
    Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and
    Jianfeng Gao. [n. d.]. MindAgent: Emergent Gaming Interaction. ([n. d.]). arXiv:2309.09971'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guu et al. [2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *ICML*. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2024] Simon Jerome Han, Keith J. Ransom, Andrew Perfors, and Charles
    Kemp. 2024. Inductive reasoning in humans and large language models. *Cogn. Syst.
    Res.* (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2024] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas
    Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-Augmented
    Generation for Textual Graph Understanding and Question Answering. (2024). arXiv:2402.07630'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. [2023] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie,
    Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot
    rankers for recommender systems. *arXiv:2305.08845* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022a] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
    Mordatch. 2022a. Language Models as Zero-Shot Planners: Extracting Actionable
    Knowledge for Embodied Agents. In *ICML*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022b] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre
    Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman,
    and Brian Ichter. 2022b. Inner Monologue: Embodied Reasoning through Planning
    with Language Models. In *CoRL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ioannidis et al. [2022] Vassilis N Ioannidis, Xiang Song, Da Zheng, Houyu Zhang,
    Jun Ma, Yi Xu, Belinda Zeng, Trishul Chilimbi, and George Karypis. 2022. Efficient
    and effective training of language and graph neural network models. *arXiv preprint
    arXiv:2206.10781* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khattab et al. [2023] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2023. Demonstrate-Search-Predict:
    Composing retrieval and language models for knowledge-intensive NLP. arXiv:2212.14024 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. [2022] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese,
    and Steven Chu-Hong Hoi. 2022. CodeRL: Mastering Code Generation through Pretrained
    Models and Deep Reinforcement Learning. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. [2021] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023. CAMEL: Communicative Agents for "Mind" Exploration
    of Large Scale Language Model Society. abs/2303.17760 (2023). arXiv:2303.17760'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs
    as Agents. (2023). arXiv:2308.03688'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lo et al. [2023] Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, and Shuyan
    Zhou. 2023. Hierarchical Prompting Assists Large Language Model on Web Navigation.
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2023] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-Play
    Compositional Reasoning with Large Language Models. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. [2023] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck,
    Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with
    Self-Feedback. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted
    question-answering with human feedback. *ArXiv* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. [[n. d.]] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff
    Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. [n. d.]. WebGPT: Browser-assisted
    question-answering with human feedback. ([n. d.]). arXiv:2112.09332'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Packer et al. [2023] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin,
    Sarah Wooders, and Joseph E. Gonzalez. 2023. MemGPT: Towards LLMs as Operating
    Systems. 2310.08560 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paranjape et al. [2023] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh
    Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023. ART: Automatic multi-step
    reasoning and tool-use for large language models. *arXiv:2303.09014* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parisi et al. [2022] Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM: Tool
    Augmented Language Models. arXiv:2205.12255'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patil et al. [2023] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. 2023. Gorilla: Large Language Model Connected with Massive APIs. *CoRR*
    (2023). arXiv:2305.15334'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. [2023] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao.
    2023. Check Your Facts and Try Again: Improving Large Language Models with External
    Knowledge and Automated Feedback. *arxiv* 2302.12813 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plummer et al. [2017] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C.
    Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2017. Flickr30k Entities: Collecting
    Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. *Int. J.
    Comput. Vis.* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. [2023a] Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun
    Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan
    Liu, Maosong Sun, and Jie Zhou. 2023a. WebCPM: Interactive Web Search for Chinese
    Long-form Question Answering. In *Proceedings of ACL 2023*. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian,
    Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    2023b. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world
    APIs. *arxiv* 2307.16789 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2023. Toolformer: Language Models Can Teach Themselves to Use Tools. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its
    Friends in Hugging Face. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2024] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang
    Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D Wang. 2024. EHRAgent: Code Empowers
    Large Language Models for Complex Tabular Reasoning on Electronic Health Records.
    *arXiv:2401.07128* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement
    learning. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [[n. d.]] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie
    Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. [n. d.]. Is ChatGPT Good at Search?
    Investigating Large Language Models as Re-Ranking Agents. In *EMNLP, year = 2023*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2024] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu,
    Nick Haber, and Noah D. Goodman. 2024. Hypothesis Search: Inductive Reasoning
    with Language Models. *ICLR* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo,
    Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, and Zhiting Hu. 2023a. PromptAgent:
    Strategic Planning with Language Models Enables Expert-level Prompt Optimization.
    (2023). arXiv:2310.16427'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-Consistency
    Improves Chain of Thought Reasoning in Language Models. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling
    Next-Gen LLM Applications via Multi-Agent Conversation Framework. (2023). arXiv:2308.08155'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2024] Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi
    Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec.
    2024. STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases.
    (2024). arXiv:2404.13207'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2024] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V.
    Le, Denny Zhou, and Xinyun Chen. 2024. Large Language Models as Optimizers. (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset
    for diverse, explainable multi-hop question answering. *EMNLP* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2023a. Tree of Thoughts: Deliberate Problem
    Solving with Large Language Models. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2024] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei
    Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit,
    Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. 2024. Retroformer:
    Retrospective Large Language Agents with Policy Gradient Optimization. (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yasunaga et al. [2021] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy
    Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with Language Models and Knowledge
    Graphs for Question Answering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. [[n. d.]] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu,
    Yuxiao Dong, and Jie Tang. [n. d.]. AgentTuning: Enabling Generalized Agent Abilities
    for LLMs. ([n. d.]). arXiv:2310.12823'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2024] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi
    Wang, Ranjay Krishna, and Qingyun Wu. 2024. Training Language Model Agents without
    Modifying Language Models. (2024). arXiv:2402.11359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023] Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang,
    Yihan Xi, Dejia Xu, and Zhangyang Wang. 2023. Outline, then details: Syntactically
    guided coarse-to-fine code generation. *ICML* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. [2024] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2024. MemoryBank: Enhancing Large Language Models with Long-Term Memory.
    In *AAAI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large Language Models are Human-Level
    Prompt Engineers. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan
    Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models
    for information retrieval: A survey. *arXiv:2308.07107* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. [2023] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao
    Zhang. 2023. ToolQA: A Dataset for LLM Question Answering with External Tools.
    In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zong et al. [[n. d.]] Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian
    Shao, and Yueting Zhuang. [n. d.]. Triad: A Framework Leveraging a Multi-Role
    LLM-based Agent to Solve Knowledge Base Question Answering. ([n. d.]). arXiv:2402.14320'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Retrieval Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: STaRK. On STaRK benchmark, we are given a relation-text knowledge base, based
    on a knowledge graph $G=(V,E)$ is in.
  prefs: []
  type: TYPE_NORMAL
- en: The query set $Q$ satisfied the relational requirements in knowledge graph and
    textual requirements in its text documents.
  prefs: []
  type: TYPE_NORMAL
- en: Flickr30K Entities. On Flickr30K Entities dataset, we are given a image-text
    knowledge base. We denote an image-text knowledge base of size $n$ represents
    a set of text phrases that describe the entity in the corresponding bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: In our task, the image captions is served as the text query, therefore all the
    $q_{i}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f13e641706b4a25b1eb162da0f45af69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Example data on Flickr30K Entities. Every entity is an image along
    with its included image patches and associated phrases with the image patches.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Experimental Results ‣
    AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval"), we provide
    the results on STaRK using GPT-4 Turbo (0125) as the backbone LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional Experimental Results ‣
    AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval"), we demonstrate
    AvaTaR’s ability in generalizing to testing queries with different distributions
    than the question-answering pairs used to optimize the actor agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [8](#A3.F8 "Figure 8 ‣ Appendix C Prompts ‣ AvaTaR: Optimizing LLM
    Agents for Tool-Assisted Knowledge Retrieval"), we present the final actions which
    are made'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Retrieval performance (%) on STaRK benchmark. Last row shows the relative
    improvements over the best metric value among the baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Amazon | MAG | Prime |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hit@1 | Hit@5 | R@20 | MRR | Hit@1 | Hit@5 | R@20 | MRR | Hit@1 | Hit@5
    | R@20 | MRR |'
  prefs: []
  type: TYPE_TB
- en: '| Dense Retr. (roberta) | 15.29 | 47.93 | 44.49 | 30.20 | 10.51 | 35.23 | 42.11
    | 21.34 | 4.46 | 21.85 | 30.13 | 12.38 |'
  prefs: []
  type: TYPE_TB
- en: '| QAGNN (roberta) | 26.56 | 50.01 | 52.05 | 37.75 | 12.88 | 39.01 | 46.97 |
    29.12 | 8.85 | 21.35 | 29.63 | 14.73 |'
  prefs: []
  type: TYPE_TB
- en: '| VSS (ada-002) | 39.16 | 62.73 | 53.29 | 50.35 | 29.08 | 49.61 | 48.36 | 38.62
    | 12.63 | 31.49 | 36.00 | 21.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-VSS (ada-002) | 40.07 | 64.98 | 55.12 | 51.55 | 25.92 | 50.43 | 50.80
    | 36.94 | 15.10 | 33.56 | 38.05 | 23.49 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct (gpt4) | 38.83 | 62.50 | 50.39 | 49.16 | 23.50 | 46.50 | 43.11 | 33.91
    | 10.83 | 30.83 | 32.16 | 19.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion (gpt4) | 41.45 | 64.83 | 53.98 | 52.22 | 33.44 | 51.33 | 49.14
    | 41.34 | 14.27 | 35.11 | 39.29 | 23.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Reranker (gpt4) | 44.79 | 71.17 | 55.35 | 55.69 | 40.90 | 58.18 | 48.60 |
    49.00 | 18.28 | 37.28 | 34.05 | 26.55 |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR-C (gpt4) | 32.03 | 58.46 | 54.03 | 44.00 | 25.97 | 45.62 | 46.68 |
    35.12 | 9.52 | 26.04 | 32.62 | 17.58 |'
  prefs: []
  type: TYPE_TB
- en: '| AIR (gpt4) | 48.82 | 72.03 | 56.04 | 57.17 | 46.08 | 59.32 | 49.70 | 52.01
    | 20.10 | 39.89 | 42.23 | 29.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Relative Improvement | 9.0% | 1.2% | 1.3% | 2.7% | 12.7% | 2.1% | -2.2% |
    6.1% | 10.0% | 7.0% | 11.0% | 9.9% |'
  prefs: []
  type: TYPE_TB
- en: '| (over Best Baseline) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Retrieval performance (%) on the leave-out sets of human-generated
    queries in STaRK.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Amazon | MAG | Prime |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hit@1 | Hit@5 | MRR | Hit@1 | Hit@5 | MRR | Hit@1 | Hit@5 | MRR |'
  prefs: []
  type: TYPE_TB
- en: '| VSS | 39.50 | 64.19 | 52.65 | 28.57 | 41.67 | 35.81 | 21.20 | 40.37 | 29.84
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-VSS | 46.91 | 72.84 | 58.74 | 23.81 | 41.67 | 31.43 | 25.67 | 40.37
    | 33.77 |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct | 45.65 | 71.73 | 58.81 | 27.27 | 40.00 | 33.94 | 21.73 | 33.33 | 28.20
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion | 49.38 | 64.19 | 58.96 | 28.57 | 39.29 | 36.53 | 16.52 | 33.03
    | 23.99 |'
  prefs: []
  type: TYPE_TB
- en: '| AvaTaR | 58.02 | 76.54 | 65.91 | 33.33 | 42.86 | 38.62 | 33.03 | 51.37 |
    41.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Rel. Impr. | 17.5% | 5.1% | 11.8% | 16.7% | 2.9% | 5.7% | 28.7% | 27.3% |
    21.4% |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We keep only two prompt templates for our framework on all tasks: (1) The prompt
    template given to actor as initially instructions, and (2) the prompt template
    given to the comparator to conduct contrastive reasoning and generate the instructions
    for the actor. Below are the complete templates:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the prompt given to actor as initially instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,WW91IGFyZSBhbiBleHBlcnQgdXNlciBvZiBhIGtub3dsZWRnZSBiYXNlLCBhbmQgeW91ciB0YXNrIGlzIHRvIGFuc3dlciBhIHNldCBvZiBxdWVyaWVzLiBJIHdpbGwgcHJvdmlkZSB5b3VyIHdpdGggdGhlIHNjaGVtYSBvZiB0aGlzIGtub3dsZWRnZSBiYXNlOgo8a25vd2xlZGdlX2Jhc2Vfc2NoZW1hPgoKWW91IGhhdmUgYWNjZXNzIHRvIHNldmVyYWwgQVBJcyB0aGF0IGFyZSBwcmUtaW1wbGVtZW50ZWQgZm9yIGludGVyYWN0aW9uIHdpdGggdGhlIGtub3dsZWRnZSBiYXNlOgo8ZnVuY19jYWxsX2Rlc2NyaXB0aW9uPgoKSW5mb3JtYXRpb24gb2YgcXVlcmllczogQmVsb3cgYXJlIHNldmVyYWwgcXVlcnkgZXhhbXBsZXMgdGhhdCB5b3UgbmVlZCB0byBjYXJlZnVsbHkgcmVhZCB0aHJvdWdoOgoiCjxleGFtcGxlX3F1ZXJpZXM+CiIKClRhc2s6IEdpdmVuIGFuIGlucHV0IHF1ZXJ5LCB5b3Ugc2hvdWxkIHdyaXRlIHRoZSBhY3Rpb25zIGluIFB5dGhvbiBjb2RlIHRvIGNhbGN1bGF0ZSBhIGBub2RlX3Njb3JlX2RpY3RgIGZvciA8bl9pbml0X2NhbmRpZGF0ZXM+IG5vZGUgSURzLCB3aGljaCBhcmUgaW5wdXQgYXMgYSBsaXN0LiBUaGVzZSBub2RlIElEcywgcmVmZXJyZWQgdG8gYXMgYGNhbmRpZGF0ZV9pZHNgLCBhcmUgYSBzdWJzZXQgb2Ygbm9kZSBJRHMgZnJvbSB0aGUga25vd2xlZGdlIGJhc2UsIGFuZCB0aGUgbm9kZXMgYmVsb25nIHRvIHRoZSB0eXBlKHMpIDxjYW5kaWRhdGVfdHlwZXM+LiBJbiBgbm9kZV9zY29yZV9kaWN0OiBEaWN0W2ludCwgZmxvYXRdYCwgZWFjaCBrZXkgc2hvdWxkIGJlIGEgbm9kZSBJRCwgYW5kIGVhY2ggdmFsdWUgc2hvdWxkIGJlIHRoZSBjb3JyZXNwb25kaW5nIG5vZGUgc2NvcmUuIFRoaXMgc2NvcmUgc2hvdWxkIGluZGljYXRlIHRoZSBsaWtlbGlob29kIG9mIHRoZSBub2RlIGJlaW5nIHRoZSBjb3JyZWN0IGFuc3dlciB0byB0aGUgcXVlcnkuCgpPdXRwdXQgZm9ybWF0OiBGaXJzdGx5LCB5b3Ugc2hvdWxkIGVzdGFibGlzaCBhIGNvbm5lY3Rpb24gYmV0d2VlbiB0aGUgZ2l2ZW4gcXVlcmllcyBhbmQgdGhlIHF1ZXJ5IHBhdHRlcm5zIHRvIHRoZSBzY2hlbWEgb2YgdGhlIGtub3dsZWRnZSBiYXNlLiBTZWNvbmRseSwgZ2VuZXJhdGUgYW4gb3V0bGluZSBmb3IgdGhlIGNvZGUgdGhhdCB3aWxsIGNvbXB1dGUgdGhlIHNjb3JlcyBmb3IgYWxsIHRoZSBjYW5kaWRhdGUgbm9kZXMgcHJvdmlkZWQgaW4gdGhlIHF1ZXJ5IGV4YW1wbGVzLiBGaW5hbGx5LCBkZXZlbG9wIHRoZSBtYWluIGZ1bmN0aW9uIG5hbWVkIGBnZXRfbm9kZV9zY29yZV9kaWN0YCwgd2hpY2ggdGFrZXMgdHdvIHJlcXVpcmVkIHBhcmFtZXRlcnM6IGBxdWVyeWAgYW5kIGBjYW5kaWRhdGVfaWRzYCwgYW5kIG9wdGlvbmFsIHBhcmFtZXRlcnMgZGVjbGFyZWQgaW4gYHBhcmFtZXRlcl9kaWN0YC4gTm90ZSB0aGF0IGBwYXJhbWV0ZXJfZGljdGAgaXMgYSBkaWN0aW9uYXJ5IG9mIHBhcmFtZXRlcnMgYW5kIHRoZWlyIGRlZmF1bHQgdmFsdWVzIHdoZXJlIHlvdSBjYW4gZGVjbGFyZSBhbnkgcGFyYW1ldGVycyBvciB3ZWlnaHRzIHVzZWQgZHVyaW5nIGNvbXB1dGluZyB0aGUgbm9kZSBzY29yZXMuIElmIG5vIG9wdGlvbmFsIHBhcmFtZXRlcnMgYXJlIG5lZWRlZCwgbGVhdmUgYHBhcmFtZXRlcl9kaWN0YCBhcyBhbiBlbXB0eSBkaWN0aW9uYXJ5LiBPdmVyYWxsLCB5b3VyIG91dHB1dCBzaG91bGQgZm9sbG93IHRoZSBzdHJ1Y3R1cmU6CgpgYGBweXRob24KIyA8Y29kZSBvdXRsaW5lcz4KaW1wb3J0IDxwYWNrYWdlMT4KLi4uCgpwYXJhbWV0ZXJfZGljdCA9IHs8cGFyYW1ldGVyX25hbWUxPjogPGRlZmF1bHRfdmFsdWUxPiwKICAgICAgICAgICAgICAgICAgPHBhcmFtZXRlcl9uYW1lMj46IDxkZWZhdWx0X3ZhbHVlMj4sCiAgICAgICAgICAgICAgICAgIC4uLn0KCmRlZiBnZXRfbm9kZV9zY29yZV9kaWN0KHF1ZXJ5LCBjYW5kaWRhdGVfaWRzLCAqKnBhcmFtZXRlcl9kaWN0KToKICAgIG5vZGVfc2NvcmVfZGljdCA9IHt9CiAgICAjIHlvdXIgY29kZQogICAgcmV0dXJuIG5vZGVfc2NvcmVfZGljdApgYGAKCkhpbnRzOgotIE9ic2VydmUgdGhlIGV4YW1wbGUgcXVlcmllcyBjYXJlZnVsbHkgYW5kIGNvbnNpZGVyIHRoZSBrZXkgYXR0cmlidXRlcyB0byBleHRyYWN0LgotIFVzZSBgYGBweXRob24gYW5kIGBgYCB0byB3cmFwIHRoZSBjb21wbGV0ZSBjb2RlLCBhbmQgZG8gbm90IHVzZSBhbnkgb3RoZXIgZGVsaW1pdGVycy4KLSBZb3UgY2FuIHVzZSBhbnkgb2YgdGhlIHByZS1pbXBsZW1lbnRlZCBBUElzIGJ1dCBzaG91bGQgYXZvaWQgbW9kaWZ5aW5nIHRoZW0uCi0gWW91IGNhbiBpbmNsdWRlIG90aGVyIGZ1bmN0aW9ucyBiZXNpZGVzIGBnZXRfbm9kZV9zY29yZV9kaWN0YCwgYnV0IGVuc3VyZSB0aGV5IGFyZSBmdWxseSBpbXBsZW1lbnRlZC4KLSBUaGUgY29kZSBzaG91bGQgYmUgY29tcGxldGUgd2l0aG91dCBwbGFjZWhvbGRlcnMgYW5kIGR1bW15IGZ1bmN0aW9ucy4KLSBPcHRpbWl6ZSB0aGUgaW50ZWdyaXR5IG9mIHRoZSBjb2RlLCBlLmcuLCBjb3JuZXIgY2FzZXMuCi0gTWluaW1pemUgY29tcHV0YXRpb25hbCBleHBlbnNlcyBieSBlYXJseSBlbGltaW5hdGlvbiBvZiBjYW5kaWRhdGUgbm9kZXMgdGhhdCBkb24ndCBtZWV0IHJlbGF0aW9uYWwgcmVxdWlyZW1lbnQgKGlmIGFueSkuCi0gQXZvaWQgY29uZHVjdGluZyB1bm5lY2Vzc2FyeSBhbmQgcmVkdW5kYW50IGNvbXB1dGF0aW9ucywgZXNwZWNpYWxseSB3aGVuIHVzaW5nIGxvb3BzLgotIE1ha2UgdXNlIG9mIGBwYXJhbWV0ZXJfZGljdGAgdG8gYXZvaWQgaGFyZC1jb2RpbmcgcGFyYW1ldGVycyBhbmQgd2VpZ2h0cy4KLSBVc2UgdGhlIGZ1bmN0aW9ucyB0aGF0IGVuZCB3aXRoIGBieV9sbG1gIHdpc2VseSBmb3IgbW9yZSBhY2N1cmF0ZSBzZWFyY2hlcy4KLSBVc2UgYGRlYnVnX3ByaW50YCBzbWFydGx5IHRvIHByaW50IG91dCBhbnkgaW5mb3JtYXRpdmUgaW50ZXJtZWRpYXRlIHJlc3VsdHMgZm9yIGRlYnVnZ2luZy4KLSBFeGNsdWRlIG9yIGNvbW1lbnQgb3V0IGFueSBleGFtcGxlIHVzZXMgb2YgYGdldF9ub2RlX3Njb3JlX2RpY3RgIGluIHRoZSBvdXRwdXQgY29kZS4KCllvdXIgb3V0cHV0Og==)You  are  an  expert  user  of  a  knowledge  base,  and  your  task  is  to  answer  a  set  of  queries.  I  will  provide  your  with  the  schema  of  this  knowledge  base:<knowledge_base_schema>You  have  access  to  several  APIs  that  are  pre-implemented  for  interaction  with  the  knowledge  base:<func_call_description>Information  of  queries:  Below  are  several  query  examples  that  you  need  to  carefully  read  through:"<example_queries>"Task:  Given  an  input  query,  you  should  write  the  actions  in  Python  code  to  calculate  a  ‘node_score_dict‘  for  <n_init_candidates>  node  IDs,  which  are  input  as  a  list.  These  node  IDs,  referred  to  as  ‘candidate_ids‘,  are  a  subset  of  node  IDs  from  the  knowledge  base,  and  the  nodes  belong  to  the  type(s)  <candidate_types>.  In  ‘node_score_dict:  Dict[int,  float]‘,  each  key  should  be  a  node  ID,  and  each  value  should  be  the  corresponding  node  score.  This  score  should  indicate  the  likelihood  of  the  node  being  the  correct  answer  to  the  query.Output  format:  Firstly,  you  should  establish  a  connection  between  the  given  queries  and  the  query  patterns  to  the  schema  of  the  knowledge  base.  Secondly,  generate  an  outline  for  the  code  that  will  compute  the  scores  for  all  the  candidate  nodes  provided  in  the  query  examples.  Finally,  develop  the  main  function  named  ‘get_node_score_dict‘,  which  takes  two  required  parameters:  ‘query‘  and  ‘candidate_ids‘,  and  optional  parameters  declared  in  ‘parameter_dict‘.  Note  that  ‘parameter_dict‘  is  a  dictionary  of  parameters  and  their  default  values  where  you  can  declare  any  parameters  or  weights  used  during  computing  the  node  scores.  If  no  optional  parameters  are  needed,  leave  ‘parameter_dict‘  as  an  empty  dictionary.  Overall,  your  output  should  follow  the  structure:‘‘‘python#  <code  outlines>import  <package1>...parameter_dict  =  {<parameter_name1>:  <default_value1>,<parameter_name2>:  <default_value2>,...}def  get_node_score_dict(query,  candidate_ids,  **parameter_dict):node_score_dict  =  {}#  your  codereturn  node_score_dict‘‘‘Hints:-  Observe  the  example  queries  carefully  and  consider  the  key  attributes  to  extract.-  Use  ‘‘‘python  and  ‘‘‘  to  wrap  the  complete  code,  and  do  not  use  any  other  delimiters.-  You  can  use  any  of  the  pre-implemented  APIs  but  should  avoid  modifying  them.-  You  can  include  other  functions  besides  ‘get_node_score_dict‘,  but  ensure  they  are  fully  implemented.-  The  code  should  be  complete  without  placeholders  and  dummy  functions.-  Optimize  the  integrity  of  the  code,  e.g.,  corner  cases.-  Minimize  computational  expenses  by  early  elimination  of  candidate  nodes  that  don’t  meet  relational  requirement  (if  any).-  Avoid  conducting  unnecessary  and  redundant  computations,  especially  when  using  loops.-  Make  use  of  ‘parameter_dict‘  to  avoid  hard-coding  parameters  and  weights.-  Use  the  functions  that  end  with  ‘by_llm‘  wisely  for  more  accurate  searches.-  Use  ‘debug_print‘  smartly  to  print  out  any  informative  intermediate  results  for  debugging.-  Exclude  or  comment  out  any  example  uses  of  ‘get_node_score_dict‘  in  the  output  code.Your  output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the prompt given to comparator to generate the instructions for the
    actor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,PGluaXRpYWxfcHJvbXB0PgoKPHByZXZpb3VzX2FjdGlvbnM+CgpBZnRlciBleGVjdXRpbmcgdGhlIGFib3ZlIGFjdGlvbnMgb24gdXNlciBxdWVyaWVzLCBzb21lIHF1ZXJpZXMgaGF2ZSB5aWVsZGVkIGdvb2QgcmVzdWx0cywgd2hpbGUgb3RoZXJzIGhhdmUgbm90LiBCZWxvdyBhcmUgdGhlIHF1ZXJpZXMgYWxvbmcgd2l0aCB0aGVpciBjb3JyZXNwb25kaW5nIGV2YWx1YXRpb24gbWV0cmljczoKV2VsbC1wZXJmb3JtaW5nIHF1ZXJpZXM6Cjxwb3NpdGl2ZV9xdWVyaWVzX2FuZF9tZXRyaWM+ClBvb3JseS1wZXJmb3JtaW5nIHF1ZXJpZXM6CjxuZWdhdGl2ZV9xdWVyaWVzX2FuZF9tZXRyaWM+CgpUYXNrOgooMSkgRmlyc3RseSwgaWRlbnRpZnkgYW5kIGNvbnRyYXN0IHRoZSBwYXR0ZXJucyBvZiBxdWVyaWVzIHRoYXQgaGF2ZSBhY2hpZXZlZCBnb29kIHJlc3VsdHMgd2l0aCB0aG9zZSB0aGF0IGhhdmUgbm90LgooMikgVGhlbiwgcmV2aWV3IHRoZSBjb21wdXRhdGlvbmFsIGxvZ2ljIGZvciBhbnkgaW5jb25zaXN0ZW5jaWVzIGluIHRoZSBwcmV2aW91cyBhY3Rpb25zLgooMykgTGFzdGx5LCBzcGVjaWZ5IHRoZSBtb2RpZmljYXRpb24gdGhhdCBjYW4gbGVhZCB0byBpbXByb3ZlZCBwZXJmb3JtYW5jZSBvbiB0aGUgbmVnYXRpdmUgcXVlcmllcy4gWW91IHNob3VsZCBmb2N1cyBvbiBjYXB0dXJpbmcgdGhlIGhpZ2gtbGV2ZWwgcGF0dGVybiBvZiB0aGUgcXVlcmllcyByZWxldmFudCB0byB0aGUga25vd2xlZGdlIGJhc2Ugc2NoZW1hLgo=)<initial_prompt><previous_actions>After  executing  the  above  actions  on  user  queries,  some  queries  have  yielded  good  results,  while  others  have  not.  Below  are  the  queries  along  with  their  corresponding  evaluation  metrics:Well-performing  queries:<positive_queries_and_metric>Poorly-performing  queries:<negative_queries_and_metric>Task:(1)  Firstly,  identify  and  contrast  the  patterns  of  queries  that  have  achieved  good  results  with  those  that  have  not.(2)  Then,  review  the  computational  logic  for  any  inconsistencies  in  the  previous  actions.(3)  Lastly,  specify  the  modification  that  can  lead  to  improved  performance  on  the  negative  queries.  You  should  focus  on  capturing  the  high-level  pattern  of  the  queries  relevant  to  the  knowledge  base  schema.![Refer
    to caption](img/bf8618206c7f874ef5010ed51558a272.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Optimized Action Sequence by AvaTaR on Flickr30K-Entities..'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Function library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Function Name | Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| ParseAttributeFromQuery | query: The string to be parsed, attributes: The
    list of attributes to be extracted from the query | This function parses a ‘query‘
    into a dictionary based on the input list ‘attributes‘ |'
  prefs: []
  type: TYPE_TB
- en: '| GetTextEmbedding | string: The array of list to be embedded | Embeds N strings
    in a list into N tensors |'
  prefs: []
  type: TYPE_TB
- en: '| GetRelevantChunk | query: The input query string, node_id: The ID of the
    node | Get the relevant chunk of information for the node based on the query |'
  prefs: []
  type: TYPE_TB
- en: '| GetFullInfo | node_id: The ID of the node | Get the full information of the
    node with the specified ID |'
  prefs: []
  type: TYPE_TB
- en: '| GetEntityDocuments | node_id: The ID of the node | Get the text information
    of the node with the specified ID |'
  prefs: []
  type: TYPE_TB
- en: '| GetRelationInfo | node_id: The ID of the node | Get the relation information
    of the node with the specified ID |'
  prefs: []
  type: TYPE_TB
- en: '| GetRelationDict | node_id: The ID of the node | Get the relation dictionary
    for the node with the specified ID, where the keys are relation type and values
    are neighbor nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| GetRelatedEntities | node_id: The ID of the node | Get the nodes related
    to the specified node |'
  prefs: []
  type: TYPE_TB
- en: '| GetEntityIdsByType | type: The type of node to retrieve | Get the IDs of
    nodes with the specified type |'
  prefs: []
  type: TYPE_TB
- en: '| GetEntityTypes | node_id: The ID of the node | Get the type of the node with
    the specified ID |'
  prefs: []
  type: TYPE_TB
- en: '| GetEntityEmbedding | node_ids: An array of candidate node ids to be embedded
    | Get the embedding indices of nodes with ID ‘node_ids‘ |'
  prefs: []
  type: TYPE_TB
- en: '| ComputingEmbeddingSimilarity | embedding_1 and embedding_2 | The cosine similarity
    score of two embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| ComputeQueryEntitySimilarity | query: The input query string, node_ids: An
    array of candidate node id to be compared with the query | Compute embedding similarity
    between ‘query‘ (str) and the nodes’ in ‘node_ids‘ (list) |'
  prefs: []
  type: TYPE_TB
- en: '| ComputeExactMatchScore | string: The string to be matched, node_ids: The
    list of candidate node id to be compared with the string | For each node in ‘node_ids‘,
    compute the exact match score based on whether ‘string‘ is included in the information
    of the node |'
  prefs: []
  type: TYPE_TB
- en: '| TokenMatchScore | string: The string to be matched, node_ids: The list of
    candidate node id to be compared with the string | For each node in ‘node_ids‘,
    computes recall scores between ‘string‘ and the full information of the node |'
  prefs: []
  type: TYPE_TB
- en: '| SummarizeTextsByLLM | texts: The list of texts to be summarized | Use LLM
    to summarize the provided texts |'
  prefs: []
  type: TYPE_TB
- en: '| ClassifyEntitiesByLLM | node_ids: The array of candidate node ids to be classified,
    classes: The list of classes to be classified into | Use LLM to classify each
    node specified by ‘node_ids‘ into one of the given ‘classes‘ or ’NA’ |'
  prefs: []
  type: TYPE_TB
- en: '| ClassifyByLLM | texts: The list of texts to be classified, classes: The list
    of classes to be classified into | Use LLM to classify each text into one of the
    given ‘classes‘ or ’NA’ |'
  prefs: []
  type: TYPE_TB
- en: '| ExtractRelevantInfoByLLM | texts: The list of texts to extract info from,
    extract_term: the terms to identify relevant information | Use LLM to extract
    relevant information from the texts based on extract_term, return sentences or
    ’NA’ |'
  prefs: []
  type: TYPE_TB
- en: '| CheckRequirementsByLLM | node_ids: The array of candidate node ids to be
    checked, requirement: The requirement to be checked | Use LLM to check if node(s)
    with ‘node_ids‘ satisfies to ‘requirement‘ |'
  prefs: []
  type: TYPE_TB
- en: '| GetSatisfictionScoreByLLM | node_ids: The array of candidate node ids to
    be scored, query: The input query from user | Use LLM to score the node with ‘node_ids‘
    based on the given ‘query‘ |'
  prefs: []
  type: TYPE_TB
- en: '| FINISH | final_reranked_answer_list: The final answer | This function is
    used to indicate the end of the task |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Function library on STaRK'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function Name | Input | Output |'
  prefs: []
  type: TYPE_TB
- en: '| ParseAttributeFromQuery | query: The string to be parsed, attributes: The
    list of attributes to be extracted from the query | This function parses a ‘query‘
    into a dictionary based on the input list ‘attributes‘ |'
  prefs: []
  type: TYPE_TB
- en: '| GetBagOfPhrases | image_ids: The image id array to get the phrases from |
    Returns a list of phrase list for each image in the image_ids list |'
  prefs: []
  type: TYPE_TB
- en: '| GetEntityDocuments | image_ids: The image id array to get the text information
    from | Returns a list of text information for each image in the image_ids list
    |'
  prefs: []
  type: TYPE_TB
- en: '| GetClipTextEmbedding | string: The list of strings to be embedded | Embed
    a string or list of N strings into N embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| GetPatchIdToPhraseDict | image_ids: The image list to get the patch_id to
    phrase list dictionary from | Returns a list of patch_id to phrase list dictionary
    for each image |'
  prefs: []
  type: TYPE_TB
- en: '| GetImages | image_id_lst: The list of image ids | Return a list of images
    with corresponding ids |'
  prefs: []
  type: TYPE_TB
- en: '| GetClipImageEmbedding | image_lst: The list of images to be embedded | Embed
    the images of a list of N image_ids into N tensors |'
  prefs: []
  type: TYPE_TB
- en: '| GetImagePatchByPhraseId | image_id: the id of an image, patch_id: the patch
    id on the image | Return the patch image for the given image_id and patch_id |'
  prefs: []
  type: TYPE_TB
- en: '| ComputingEmbeddingSimilarity | embedding_1 and embedding_2 | The cosine similarity
    score of two embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| ComputeF1 | string_to_match: The key word to be matched, strings: The list
    of strings to be calculated f1 score with the key word | Compute the F1 score
    based on the similarity between ‘string_to_match‘ and each string in ‘strings‘
    |'
  prefs: []
  type: TYPE_TB
- en: '| TokenMatchScore | string_to_match: The key word to be matched, strings: The
    list of strings to be calculated recall score with the key word | Compute the
    recall score based on the similarity between ‘string_to_match‘ and each string
    in ‘strings‘ |'
  prefs: []
  type: TYPE_TB
- en: '| ComputeExactMatchScore | string_to_match: The key word to be matched, strings:
    The list of strings to be exact matched with the key word | Compute the exact
    match score based on whether ‘string_to_match‘ is exactly the same as each string
    in ‘strings‘ |'
  prefs: []
  type: TYPE_TB
- en: '| VqaByLLM | question: The question to be answered, image_lst: The list of
    images | Use LLM to answer the given ‘question‘ based on the image(s) |'
  prefs: []
  type: TYPE_TB
- en: '| ExtractVisualAttributesByLLM | attribute_lst: The list of attributes to be
    extracted, image_lst: The list of images | Use LLM to extract attributes about
    the given ‘attribute_lst‘ from each image |'
  prefs: []
  type: TYPE_TB
- en: '| FINISH | final_reranked_answer_list: The final answer | This function is
    used to indicate the end of the task |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Function library on Flickr30K Entities'
  prefs: []
  type: TYPE_NORMAL
