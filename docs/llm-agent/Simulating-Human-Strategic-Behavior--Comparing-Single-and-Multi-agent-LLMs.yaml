- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.08189](https://ar5iv.labs.arxiv.org/html/2402.08189)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Karthik Sreedhar [ks4190@columbia.edu](mailto:ks4190@columbia.edu)  and  Lydia
    Chilton [lc3251@columbia.edu](mailto:lc3251@columbia.edu) Columbia UniversityUSA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When creating plans, policies, or applications for people, it is challenging
    for designers to think through the strategic ways that different people will behave.
    Recently, Large Language Models (LLMs) have been shown to create realistic simulations
    of human-like behavior based on personas. We build on this to investigate whether
    LLMs can simulate human strategic behavior: Human strategies are complex because
    they take into account social norms in addition to aiming to maximize personal
    gain. The ultimatum game is a classic economics experiment used to understand
    human strategic behavior in a social setting. It shows that people will often
    choose to “punish” other players to enforce social norms rather than to maximize
    personal profits. We test whether LLMs can replicate this complex behavior in
    simulations. We compare two architectures: single- and multi-agent LLMs. We compare
    their abilities to (1) simulate human-like actions in the ultimatum game, (2)
    simulate two player personalities, greedy and fair, and (3) create robust strategies
    that are logically complete and consistent with personality. Our evaluation shows
    the multi-agent architecture is much more accurate than single LLMs (88% vs. 50%)
    in simulating human strategy creation and actions for personality pairs. Thus
    there is potential to use LLMs to simulate human strategic behavior to help designers,
    planners, and policymakers perform preliminary exploration of how people behave
    in systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†booktitle:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simulations help us design the world. Whether designing earthquake-safe buildings,
    evaluation plans, economic policies, or even a late assignment policy in a class,
    it is useful for designers to be able to simulate the effects of their design
    as a heuristic to guide the process. Although physical simulation has come a long
    way, simulating human behavior is notoriously difficult. When economists model
    human behavior, they assume that people are rational actors, but psychology has
    discovered many important ways in which humans are not rational or strictly profit
    maximizing (Kahneman, [2012](#bib.bib13); Ariely, [2008](#bib.bib4)). Moreover,
    people do not behave uniformly – their personalities (McCrae and Costa, [2008](#bib.bib17)),
    experiences (Kidd et al., [2013](#bib.bib14)) and circumstances (Mullainathan
    and Shafir, [2013](#bib.bib18)) affect their decision making. Another complexity
    is that people often act strategically - to reason about other people and base
    their actions on it (as chess players do). This makes it very mentally demanding
    for a designer to think through all the possibilities of what people would do
    in response to a new design or policy.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, LLMs have been shown to be able to simulate plausible human behavior
    based on personas. This includes modeling the opinions of supreme court justices
    in past rulings (Hamilton, [2023](#bib.bib8)), simulating a fictional town’s ability
    to plan and attend events like a party (Park et al., [2023](#bib.bib20)), and
    simulating human behavior in classic economic and psychology experiments (Aher
    et al., [2023](#bib.bib2)). We build on this to investigate whether LLMs can simulate
    human strategic behavior. To evaluate not just plausible human behavior but also
    behavior that accurately reflects what people do, we compare LLM simulations to
    experimental baselines taken from literature on studies of human strategic behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ultimatum game is a classic economics experiment used to understand human
    social strategic behavior. It also captures human’s social behavior (often deemed
    irrational, such as the desire to “punish” unfair actors) and personality differences
    (greedy and fair). In the ultimatum game, there are two players: a proposer and
    a receiver. The proposer is given an amount of money, such as $1, and is tasked
    with offering a portion of the amount to the receiver. The receiver can either
    accept or reject the offer - if they accept, the players divide the amount as
    proposed. If they reject, both players receive nothing. Economic theory dictates
    that a profit-maximizing proposer should offer only $0.01 (the smallest nonzero
    amount) and keep $0.99, and that the receiver should accept it because $0.01 is
    more than the receiver would have otherwise. However, experiments with human subjects
    show that humans do not act in a purely “rational” manner; receivers will reject
    a low offer to punish proposers for offering an unfair split (Krawczyk, [2018](#bib.bib15)).
    Moreover, proposers are aware of this, and thus strategically make offers that
    are closer to fair - especially after multiple rounds of playing the game.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the ultimatum game to test whether LLMs can simulate the strategic,
    social, and personality aspects of human players. We extract human gameplay actions
    (offers and accept/reject decisions) from economics literature (Houser and McCabe,
    [2014](#bib.bib12)) and evaluate whether LLMs can simulate human behavior in the
    ultimatum game with 5 rounds. When the game is played for multiple rounds, both
    players have the opportunity to adjust their actions in response to the actions
    of the other player. We compare two LLM structures: a single LLM and a multi-agent
    LLM architecture. We compare their abilities to (1) create realistic strategies,
    (2) adhere to created strategies, and (3) accurately model two different player
    personalities: greedy and fair. The single LLM structure involves prompting GPT4
    directly, while the multi-agent LLM architecture is adapted from recent literature
    (Park et al., [2023](#bib.bib20)). In the single-LLM structure, GPT4 is directly
    prompted to simulate the actions of both a proposer and receiver over five rounds
    of the ultimatum game. In the multi-agent LLM architecture, each player is represented
    by a separate GPT4 agent. Each player is tasked with playing the ultimatum game
    with the other, with information such as personality being private to the agent.
    In both conditions, the LLM is tasked with creating a strategy based on a given
    personality and to play the game according to their personality and strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: Our evaluation shows that the multi-agent LLM architecture is much more accurate
    than using a single LLM to simulate strategic behavior in the ultimatum game.
    Over 40 simulations, the Multi-agent structure was consistent with human behavior
    88% of the time, and the single LLM was on consistent 50% of the time. There are
    three reasons for for LLM simulations to be inconsistent with human behavior,
    (1) a created strategy is incomplete, (2) a created strategy is inconsistent with
    the specified personality, or (3) a player deviates from the created strategy
    during game play. We find that over 90% of issues in single LLM simulations are
    caused by the LLMs strategy rather than their gameplay. Incomplete strategies
    or inconsistent personality strategies, with both categories accounting for a
    roughly equal amount of errors. There is only 1 of 40 simulations in which an
    error is caused by a player not adhering to the created strategies. In the multi-agent
    LLM architecture, the most common issue is strategies inconsistent with personality,
    accounting for more than 85% of errors
  prefs: []
  type: TYPE_NORMAL
- en: Based on these results, we believe that multi-agent LLMs show potential to simulate
    more than just plausible human behavior, but behavior consistent with experimental
    evidence in complex scenarios. They might one day become a tool for designs to
    evaluate plans, policies, and interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Ultimatum Game Background and Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior experiments with human subjects have revealed that humans often reject
    low offers when playing the ultimatum game, despite economic theory dictating
    the receiver to accept any positive offer. Houser and McCabe (2014) (Houser and
    McCabe, [2014](#bib.bib12)) finds that proposers most commonly offer 40% or 50%
    of the total amount to the receiver, with the receiver almost always accepting.
    However, the acceptance rate falls to 50% when offers are 20% of the total amount,
    and falls further for 10% and lower offers. Krawzcyk (2018) (Krawczyk, [2018](#bib.bib15))
    presents similar findings, recording that receivers reject offers that are 10%
    of the total amount nearly 90% of the time. The study also notes that in variations
    of the game that involve multiple rounds, it is a sensible strategy for receivers
    to reject low offers to drive up future offers in following rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Previous work has shown that there are significant impacts of introducing personality
    traits and multiple rounds into the ultimatum game that cause human players to
    act differently from expected economic theory. Brandstätter and Königstein (2001)
    (Königstein, [2001](#bib.bib16)) shows that proposers that demonstrate personality
    traits consistent with selfishness make skewed offers in their favor that highlight
    a difference between economic rationality and equity. Similarly, receivers that
    have personality traits consistent with fairness can use rejection as a form of
    retaliation to “punish” proposers that egregiously violate social norms of equity.
  prefs: []
  type: TYPE_NORMAL
- en: Vavra et. al. (2018) (Vavra et al., [2018](#bib.bib21)) uses the ultimatum game
    to more broadly study fairness, aiming to explain why receivers reject non-zero
    offers that are unfair, such as 10% of the total amount, as a way to punish the
    proposer despite both players ending in a worse outcome than an acceptance. They
    find that the canonical explanation (players value equal outcomes over personal
    games) is not sufficient to understand the phenomenon, and that there is abundant
    evidence that a decision to reject an offer can be influenced by contextual factors
    (such as multiple rounds) or based on the difference between the actual and expected
    offer (the latter of which can be heavily influenced by a greedy or fair personality
    label placed on the receiver).
  prefs: []
  type: TYPE_NORMAL
- en: Experiments also show humans from societies of different modernization levels
    act slightly differently when playing the ultimatum game. Henrich (2000) (Henrich,
    [2000](#bib.bib9)) found that proposers from the Machiguenga indigenous people
    of the Peruvian Amazon made much lower offers to receivers in games of all stakes
    than their western counterparts. Meanwhile, Alvard (2004) (Alvard, [2004](#bib.bib3))
    found that results from big-game hunting populations in Indonesia were similar
    to western societies, but had a higher occurrence of “hyper-fairness,” the direct
    opposite of what was observed in Peru. Tracer (2004) found that results from indigenous
    populations in Papa New Guinea indicated proposers were greedier than in western
    society, but not to the extent of the Machiguenga. The variations in results are
    attributed to tribal relationships and customs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. LLM Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous work has also shown that LLMs are more accurate when asked to create
    and explain a thought process before acting. Wei et. al. (2023) (Wei et al., [2022](#bib.bib22))
    shows that requiring LLMs with more than one billion parameters to explain intermediate
    reasoning steps improves performance. They evaluate chain-of-thought prompting
    against standard prompting on a database of grade school math word problems, and
    find that chain-of-though prompting has a solve rate (57%) more than two and a
    half times that of standard-prompting (18%). Zheng et. al. (2023) (Zheng et al.,
    [2023](#bib.bib23)) introduces progressive-hint prompting, which involves users
    inputting previous provided answers back to LLMs as hints to guide them toward
    the correct answer. They find that the average accuracy across problem types increases
    by 20% when using progressive-hint-prompting compared to standard prompting with
    text-davinci-003\. Thus, there is reason to believe that prompting GPT to create
    strategies before simulating the ultimatum game will lead to more accurate outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Using LLMs to Simulate Strategic Behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous research has studied to what degree GPT can simulate human behavior
    in simple behavioral experiments to mixed results. Aher et. al. (2023) (Aher et al.,
    [2023](#bib.bib2)) introduces a new test, called a Turing Experiment, to evaluate
    the extent to which LLMs can simulate aspects of human behavior via classic human
    behavior experiments: ultimatum game, garden path sentences, milgram shock experiment,
    and wisdom of crowds. They compare ultimatum game results from text-davinci-002
    simulations to those from past human studies and find that three out of the four
    results from human studies fall nearly on the trend line created by simulations
    from the LLM, with the fourth deviating by less than 10%. Results from the next
    two experiments also closely model human trends, although the fourth reveals a
    “hyper-accuracy-distortion” present in LLMs. While the evidence is encouraging
    in supporting that LLMs can replicate human behavior to a reasonable extent, the
    work uses a now outdated model. In the ultimatum game specifically, the work only
    tests the simplest case – there is no exploration of multiple round games or personality
    traits.'
  prefs: []
  type: TYPE_NORMAL
- en: Horton (2023) (Horton, [2023](#bib.bib11)) builds on this work by using text-davinci-003
    and proposes that LLMs can be used like economists use homo economicus, as representations
    of humans in economic scenarios. The methodology demonstrates that GPT can be
    given information and preferences and that their behavior can then be explored
    in scenarios via simulation. Results in several different economic experiments
    shows that text-davinci-003 will change its behavior mid-simulation, but that
    there is room for improvement – they conclude that text-davinci-003 is most suited
    to be used as a “toy model,” or a tool not meant to reflect reality but rather
    to help a human experimenter think. The results suggest that while there is reason
    for further exploration, LLMs could not yet be fully trusted to emulate human
    behavior in economic games.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Multi-Agent Paradigms of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous studies of GPT’s ability to simulate economic games have primarily
    used a single LLM, but new agent-based architecture has potential to simulate
    human behavior more effectively. Park et. al. (2023) (Park et al., [2023](#bib.bib20))
    introduces a LLM agent architecture that allows for multiple humans to be represented
    by multiple LLMs that can interact with one another. The architecture includes
    five components: self-knowledge, memory, plans, reactions, and reflections, and
    is evaluated using an effective Bayesian rating system called TrueSkill (introduced
    in Herbrich et. al. (2006) (Herbrich et al., [2006](#bib.bib10))). They also evaluate
    the agent architecture by populating an interactive sandbox to explore specific
    human scenarios such as planning a Valentine’s Day party or surprise birthday
    party at the office. They find that agents demonstrate reasonable individual and
    emergent social behaviors, but leave room for exploration into concrete scenarios
    like the ultimatum game which require more than “reasonable” performance and have
    a well-defined expected result.'
  prefs: []
  type: TYPE_NORMAL
- en: Hamilton (2023) (Hamilton, [2023](#bib.bib8)) also introduces a multi-agent
    system and uses it to simulate judicial rulings of the supreme court of the United
    States from 2010 to 2016\. Nine separate GPT-2 models are trained with authored
    opinions from each of the nine supreme court justices, all of which achieved greater
    than 50% accuracy for predicting the justice’s rulings and achieved better-than-random
    accuracy in predicting overall decisions of the actual supreme court. While encouraging,
    these results also demonstrate room for improvement and the need for exploration
    into the matter with the latest GPT models, as 50% accuracy is random for a binary
    decision, meaning in both metrics examined, the results could not confidently
    state that outcomes are generally accurate but rather only that there is better-than-random
    performance, which is a low threshold. A 2023 preprint (Guo, [2023](#bib.bib7))
    uses GPT Agents to simulate the ultimatum game with two personalities. They look
    at average offers and acceptance rates of rounds simulated, but do not compare
    results to human baselines. They prompt agents to create strategies but do not
    analyze the strategies’ consistency with human behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Experimental Set-Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test the ability of LLMs to simulate human-like behavior in the ultimatum
    game, we ran simulations of the five-round ultimatum game where LLMs were tasked
    with creating strategies and then playing the game. We did this with two different
    architectures: a single LLM and a multi-agent LLM architecture. With the single
    LLM, we directly prompt an LLM to create strategies for both players and simulate
    the actions of both players acting in accordance with the previously created strategies.
    For the multi-agent LLM architecture, we adapted recently introduced architectures
    with each player being represented by a separate LLM agent.'
  prefs: []
  type: TYPE_NORMAL
- en: We also tested two personality types, greedy and fair, with the expectation
    that created strategies would be different for different personality types, also
    resulting in different progressions of gameplay towards ultimately reaching an
    equal split. For instance, we expected the initial offer in a fair-fair simulation
    to be an even ($0.50) or close-to-even split and to be accepted, whereas we expected
    the initial offer in a greedy-fair simulation to be skewed in favor of the proposer
    and likely be rejected. In total, we ran forty simulations each for both the single
    LLM and multi-agent LLM architectures (ten simulations for each personality pairing).
  prefs: []
  type: TYPE_NORMAL
- en: For all experiments, we use Open AI’s GPT. We tested both GPT-3.5 and GPT-4,
    the two most recent large language models released by OpenAI. Specifically, we
    used the gpt-3.5-turbo and gpt-4-1106-preview models, the latter of which was
    the first version of GPT-4 released. GPT-4 has been shown to have the ability
    to interpret inherently human concepts such as equity and also scores well on
    a variety of standardized tests, ranging from the Bar Exam to the GRE (OpenAI
    et al., [2023](#bib.bib19)). For both versions of GPT, we set temperature and
    top P parameters at 0.5, and did not add any frequency or presence penalties.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Research Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following experiment section, we specifically address the following
    research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: RQ1\. Which LLM architecture more accurately simulates human-like actions in
    the five-round ultimatum game?
  prefs: []
  type: TYPE_NORMAL
- en: RQ2\. Which LLM architecture more accurately simulates the actions of player
    personalities?
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3\. Which LLM architecture more often creates robust strategies: both logically
    complete and consistent with personality?'
  prefs: []
  type: TYPE_NORMAL
- en: Single LLMs and multi-agent LLMs have different potential advantages in simulations.
    In Single LLMs, the LLM has the full context of all the players’ personalities
    and actions and could potentially orchestrate strategies and gameplay to achieve
    a sensible outcome. Global context could allow for a more coherent narrative in
    the gameplay. Multi-agent LLMs lack the global context of all the agents behavior,
    as agents can only exchange information through the act of speaking. Although
    multi-agent LLMs allow the LLM to focus on one agent at a time, they might not
    expose enough information for agents to make reasonable choices.
  prefs: []
  type: TYPE_NORMAL
- en: For all three research questions, we predict that the multi-agent LLM architecture
    will perform better because each agent acting independently will better model
    interactions between multiple humans and more closely resemble the anonymous conditions
    of the ultimatum game when ran with human subjects. The multi-agent LLM architecture
    also allows for each agent to focus only on creating strategies and simulating
    strategies of one human, as opposed to a single LLM having to track the strategy
    of each player and act accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Single LLM and Multi-Agent Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1\. Inputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the single-LLM structure, we prompt an LLM to 1) create and display strategies
    for both players of the five-round ultimatum game, and then 2) simulate with both
    players acting in accordance with the previously created strategies for 5 rounds.
    The prompt for two fair players is as follows: “Create a strategy for a fair proposer
    and a fair receiver in playing the ultimatum game five times with $1\. Once the
    strategies are created, simulate five rounds of the ultimatum game with the proposer
    and the receiver adhering to the previously outlined strategies.” The prompts
    for other personality pairings are virtually identical, with only the personality
    descriptor ahead of each player in the first sentence being changed. In early
    pilots of the prompt, we discovered that we did not need to explain the ultimatum
    game or the notion of a proposer or receiver. The LLM had the context to correctly
    infer the set-up. From the LLMs response to this prompt, we extract the created
    strategies for both players as well as the offers made by the proposer and the
    receiver’s response (accept or reject) in each of the five rounds.'
  prefs: []
  type: TYPE_NORMAL
- en: For the multi-agent LLM architecture we adapt the agent architecture introduced
    to simulate a town and its inhabitants called Smallville (Park et al., [2023](#bib.bib20)).
    In this architecture, the administrator provides each agent with a name, public
    and private biographies, instructions (called directives), and an initial plan,
    which consists of a description, stop condition, and a location. For our experiments,
    we name each agent as per their role in the game (Proposer or Receiver) and set
    the initial plan for each player to be to create a strategy for the five-round
    ultimatum game as per their role and then store it in memory. The ultimatum game
    does not need players to move around, or the concept of location, so the agents
    are set to a single location called “Default.” In the private biography, we specify
    the personality trait of each player (“Proposer is greedy.”). This tells the agent
    their personality without informing the other agent. The public biography for
    both agents is left blank, since in experiments with human subjects of the ultimatum
    game neither player is given any information about the other.
  prefs: []
  type: TYPE_NORMAL
- en: Communication between agents in the Smallville architecture is not inherently
    turn-taking. To limit agents to turn-based communication as in the ultimatum game,
    we add instructions for either agent in the directives. The proposer is instructed
    to make offers only after the receiver has responded to the previous offer and
    the receiver is instructed to only respond when the proposer makes offers. Agents
    then use their abilities to create additional plans, react/respond to observed
    events, and communicate with one another to simulate the five rounds of gameplay.
  prefs: []
  type: TYPE_NORMAL
- en: In our prompt design, we aimed for the simplest prompt that generated the desired
    behavior. Preliminary prompt testing proved that the ultimatum game did not have
    to be defined - simply using the term in the prompt produced LLM outputs that
    showed a comprehensive understanding of the game. We also kept the prompts across
    the two conditions as similar as possible, adding no extra knowledge or instructions
    about personality, strategy, or gameplay in either condition.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6df4069ee7344ca8e32d252019c8683f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An output log from a SingleLLM simulation of two fair players playing
    five rounds of the ultimatum game. All text and indentation is from the LLM, bold
    text added by the authors to highlight strategy and gameplay actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06d525d47e810703e893aec7652f6a60.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. An output log from a Multi-Agent simulation of two fair players playing
    five rounds of the ultimatum game. All text is from the LLM; the labels (underlined)
    are provided by the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both tested architectures, the LLM outputs contain a log of players actions.
    In the single LLM architecture, the strategies of both players and subsequent
    gameplay is displayed in a single output log. Both player strategies are labeled
    and are in the format of numbered or bullet-point lists which typically involve
    an instruction for the first round’s offer or response in the first entry, and
    explains adjustments or changes in strategies for subsequent rounds in the following
    entries. The gameplay is displayed with each round being represented by three
    lines: the first being the proposer’s offer, the second being the receiver’s response,
    and the third explaining what each player receives from the outcome of the round.
    An example output log is shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.2.2\. Outputs
    ‣ 3.2\. Single LLM and Multi-Agent Architecture ‣ 3\. Experimental Set-Up ‣ Simulating
    Human Strategic Behavior: Comparing Single and Multi-agent LLMs") from a simulation
    involving two fair players. The proposer creates a strategy to offer fixed $0.50
    offers while the receiver creates a strategy that uses $0.50 as a fair threshold,
    resulting in five rounds where the proposer offers %0.50 and the receiver accepts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the multi-agent LLM architecture, the strategies and actions of each player
    are displayed in two separate output logs, each representing one of the two agents
    involved in the simulation. In each of the output logs, the agent first thinks
    through a strategy creation process, often considering several possible strategies
    for the ultimatum game before choosing or combining multiple into one strategy
    and recording it to memory. The gameplay is then displayed as per the turn-based
    structure specified in each agent’s directives - the proposer makes offers in
    the first output log, while the receiver indicates their responses in the second
    output log, both of which we record. An example output log is shown in Figure
    [2](#S3.F2 "Figure 2 ‣ 3.2.2\. Outputs ‣ 3.2\. Single LLM and Multi-Agent Architecture
    ‣ 3\. Experimental Set-Up ‣ Simulating Human Strategic Behavior: Comparing Single
    and Multi-agent LLMs") from a simulation involving two fair players. The proposer
    creates a strategy to offer $0.50 to the proposer, and consider lowering the offer
    to $0.40 if the receiver repeatedly accepts. The receiver creates a strategy to
    accept offers that uses a $0.40 threshold. The simulation results in the proposer
    offering the receiver $0.50 in the first three rounds and $0.40 to the receiver
    in the last two rounds, with the receiver accepting all five offers.'
  prefs: []
  type: TYPE_NORMAL
- en: From this collected data, we record the strategies of each player and the amount
    offered by the proposer and the receiver’s response in each round, and then compare
    it with results from human studies to conclude whether or not strategies are human-like
    and personality consistent and whether a simulation’s outcome is human-like.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.3.1\. Evaluation of Gameplay
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on large-scale studies of human players (Houser and McCabe, [2014](#bib.bib12);
    Krawczyk, [2018](#bib.bib15)), we establish ranges of offers and answers for each
    personality type. Prior experiments with human studies show that fair proposers
    will offer equal or close to equal splits between the range of $0.40 to $0.50,
    with fair receivers typically accepting offers and greedy receivers typically
    rejecting. Meanwhile, greedy proposers offer initial splits heavily biased in
    their favor, typically above $0.70, which is typically rejected by both a fair
    and greedy receiver.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate the initial offers of each simulation based on these criteria. In
    the first round, fair proposers are considered to act consistently with their
    personality if their offer is between $0.40 and $0.60, inclusive. Greedy proposers
    are considered to act consistently with their personality if their offer is biased
    in their favor, or strictly less than $0.50 to the receiver. Fair receivers are
    considered to act consistently with their personality if they reject offers that
    are less than $0.40 and accept offers that are greater than $0.40\. Greedy receivers,
    however, are only considered to act consistently with their personality if they
    accept offers that are strictly greater than $0.50; if a greedy receiver accepts
    any amount less than or equal to $0.50, we consider the receiver to have not inconsistently
    with the greedy personality.
  prefs: []
  type: TYPE_NORMAL
- en: In subsequent rounds, we check if each player continues to act as per their
    created strategy as well as whether the taken action is consistent with results
    from human studies. Proposers are expected to continue making offers similar to
    the range of the initial offer if the receiver accepts, but if the receiver rejects,
    proposers are expected to increase their offers slightly (Krawczyk, [2018](#bib.bib15)).
    Receivers are expected to accept offers as per their initial thresholds as well,
    but if gameplay progresses with no accepted offers, receivers are expected to
    lower their threshold, and potentially even discard it by the fifth round as there
    are no future rounds to influence offers of.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Evaluation of Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the information collected from the LLM outputs, we evaluate strategies
    for three components: (1) the completeness of strategies, (2) the consistency
    of strategies with the specified personality trait, and (3) the adherence to the
    strategies in the following gameplay.'
  prefs: []
  type: TYPE_NORMAL
- en: Strategies are considered complete if the player has a course of action for
    all possible states of the game. To be complete, a proposer’s strategy has to
    include an initial offer plan, and then a course of action for subsequent rounds
    based on whether the receiver accepts or rejects the previous offer. If the proposer’s
    strategy is incomplete, there can be issues with the proposer acting inappropriately
    when the receiver does not take the action for which the rest of the strategy
    is contingent on. Similarly, to be complete, a receiver’s strategy has to include
    a course of action for all five rounds for all possible offers between $0.00 to
    $1.00, typically specified via an acceptance threshold based on which the receiver
    acts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an incomplete strategy for a greedy proposer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '”Low-Ball Offers: The greedy proposer would aim to keep as much money as possible
    for themselves. They might start with a low offer to test the receiver’s limit.
    Since we’re dealing with $1, the proposer may start by offering $0.10 to the receiver.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Incremental Increase: If the offer is rejected, in subsequent rounds, they
    may increase the offer by a small increment, just enough to tempt the receiver
    to accept. For example, the proposer might increase the offer by $0.05 each time.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Cut-Off Point: The proposer will have a cut-off point where they find it no
    longer worth to increase the offer because they would rather end up with nothing
    than give away more..”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The strategy does not account for the receiver accepting the first offer, potentially
    resulting in problematic gameplay from the proposer if this case is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies are consistent with the specified personality if the offers made
    (for the proposer) or rejected/accepted (for the receiver) are biased towards
    the player for greedy players and closer to an equal split for fair players. For
    example, a greedy proposer’s strategy should be to make low initial offers that
    are biased in the proposer’s favor, while a fair proposer’s strategy should be
    to make initial offers that are equal or close to equal. Similarly, a greedy receiver’s
    strategy should be to only accept initial offers biased in the receiver’s favor,
    while a fair receiver’s strategy should be to accept initial offers that are equal
    or close to equal. In subsequent rounds, the strategy should be generally similar,
    although based on the actions of the other players there may be concessions made
    by either player to reach agreements, even if they are not biased in the favor
    of a greedy player, because the strategy should also consider that something is
    better than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a strategy inconsistent with personality for a greedy receiver
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '”Reject Low Offers: Initial minimum acceptance threshold is set high with a
    rejection of any offer below $0.40\. Accept all offers above $0.40.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Willingness to Adjust: If offers remain low, be willing to gradually lower
    the acceptance threshold to ensure some gain.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Last Round Acceptance: On the final round, accept any non-zero offer, under
    the assumption that some gain is better than none, adjusting the minimum threshold
    to $0.15..”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This strategy sets an acceptance threshold of $0.40, which is lower than an
    equal split, and hence inconsistent with a greedy receiver whom would be expected
    to prefer offers that are biased in their favor (at least above $0.50).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We analyze the outputs of all 40 simulations of the five-round ultimatum game
    for 4 conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multi-agent LLM architecture with GPT 3.5 (abbreviated MultiAgent-3.5)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multi-agent LLM architecture with GPT 4 (abbreviated MultiAgent-3.5)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a single LLM with GPT 3.5 (abbreviated SingleLLM-3.5)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a single LLM with GPT 3.5 (abbreviated SingleLLM-4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We report results for our three research questions.
  prefs: []
  type: TYPE_NORMAL
- en: RQ1\. Which LLM architecture more accurately simulates human-like actions in
    the five-round ultimatum game?
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiments show that the multi-agent LLM architecture yields actions consistent
    with human experimental data significantly more often than the single LLM. The
    best multi-agent architecture was MultiAgent-4 which resulted in human-like actions
    in 87.5% of simulations, while the best single LLM (SingleLLM-4) only resulted
    in human-like actions in 50% of simulations out of 40 total simulations. See Table
    [1](#S4.T1 "Table 1 ‣ 4\. Results ‣ Simulating Human Strategic Behavior: Comparing
    Single and Multi-agent LLMs"). A chi-square test shows this is statistically significant
    at the p ¡ .01 level $\chi^{2}(1,N=80)=13.091,p=.000297$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An analysis of the errors shows that strategy creation was a bigger source
    of errors than gameplay mistakes for both architectures. Table [2](#S4.T2 "Table
    2 ‣ 4\. Results ‣ Simulating Human Strategic Behavior: Comparing Single and Multi-agent
    LLMs") shows the percentages of errors due to strategy, gameplay, or both for
    all four conditions. In both MultiAgent architectures, strategy creation errors
    accounted for 100% of errors in simulation, with there being no gameplay mistakes.
    In the SingleLLM-3.5 architecture, 73.9% of errors were in strategy creation,
    compared to only 39.1% in gameplay (and 13.0% having both). In the SingleLLM-4
    architecture, 100% of errors involved an issue with strategy creation, with 25%
    of errors also including gameplay mistakes. Two-proportion z-tests revealed a
    statistically significant difference between the number of strategy creation errors
    and gameplay mistakes for all four conditions at a $p<0.05$ level.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Success Rate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-3.5 | 82.5% |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-4 | 87.5% |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-3.5 | 42.5% |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-4 | 50.0% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1\. RQ1: Percentage of simulations with human-like actions by architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Total Errors | Strategy Errors | Gameplay Errors | Both Errors
    | z-test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-3.5 | 7 | 100% (7/7) | 0% (0/7) | 0% (0/7) | $z=3.7417,p=.00018$
    |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-4 | 5 | 100% (5/5) | 0% (0/5) | 0% (0/5) | $z=3.1632,p=.00158$
    |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-3.5 | 23 | 73.9% (17/23) | 39.1% (9/23) | 13.0% (3/23) | $z=2.379,p=.017$
    |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-4 | 20 | 100% (20/20) | 25% (5/20) | 25% (5/20) | $z=4.899,p<.00001$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2\. RQ1: Percentage of errors caused by strategy and gameplay.'
  prefs: []
  type: TYPE_NORMAL
- en: RQ2\. Which LLM architecture more accurately simulates the actions of player
    personalities?
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiments show that MultiAgent-4 performed best at modeling the two personality
    types. MultiAgent-4 achieved human-like gameplay for all four personality pairs
    at least 80% of the time (see Table [3](#S4.T3 "Table 3 ‣ 4\. Results ‣ Simulating
    Human Strategic Behavior: Comparing Single and Multi-agent LLMs")). In contrast,
    SingleLLM-4 was inconsistent across personality pairs; it achieved human-like
    gameplay for 100% of the Fair-Fair simulations, but only 10% of the Greedy-Greedy
    conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When analyzing gameplay for each of the personality pairs, we see the errors
    are not the same across the pairs. Fair-Fair has the best performance with SingleLLM-4,
    MultiAgent-3.5, and MultiAgent-4 all being 100% consistent with human gameplay.
    The most errors occurred in simulations of the Greedy-Greedy personality pairing,
    with MultiAgent-4 performing the best with 80% of simulations being consistent
    with human gameplay. The MultiAgent-3.5, SingleLLM-3.5, and SingleLLM-4 were consistent
    with human gameplay in 70%, 60%, and 10% of simulations respectively. The Fair-Greedy
    and Greedy-Fair conditions were somewhere in between: with both SingleLLM’s having
    middling scores (30-50%).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Fair-Fair | Fair-Greedy | Greedy-Fair | Greedy-Greedy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-3.5 | 100% | 80% | 80% | 70% |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-4 | 100% | 80% | 90% | 80% |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-3.5 | 30% | 50% | 30% | 60% |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-4 | 100% | 40% | 50% | 10% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3\. RQ2: Percentage of simulations with human-like gameplay by architecture
    and personality-pairing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3\. Which LLM architecture more often creates robust strategies: both logically
    complete and consistent with personality?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MultiAgent architectures create robust strategies at a higher rate than
    SingleLLMs (See Table [4](#S4.T4 "Table 4 ‣ 4\. Results ‣ Simulating Human Strategic
    Behavior: Comparing Single and Multi-agent LLMs")). MultiAgent-4 creates complete
    and personality-consistent strategies for both players in 87.5% of simulations.
    The MultiAgent3.5 architecture performs slightly worse, creating complete and
    personality-consistent strategies for both players in 80% of simulations. The
    SingleLLM-3.5 and SingleLLM-4 architectures create complete and personality-consistent
    strategies in 55% and 47.5% of simulations respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: We find that the MultiAgent-4 architecture performs better in creating complete
    and personality-consistent strategies than the best-performing SingleLLM architecture
    (SingleLLM-3.5). A chi-square test shows this is statistically significant at
    the p ¡ .01 level $\chi^{2}(1,N=40)=10.3127,p=.001321$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To analyze the source of these errors, we analyze the robustness of proposer
    strategies and receiver strategies separately. Table [4](#S4 "4\. Results ‣ Simulating
    Human Strategic Behavior: Comparing Single and Multi-agent LLMs") shows that the
    problem with proposer strategies is always incompleteness. Proposers have no errors
    with personality consistency across all four architectures. Conversely, Table
    [4](#S4 "4\. Results ‣ Simulating Human Strategic Behavior: Comparing Single and
    Multi-agent LLMs") shows that the problem with receiver strategies with issues
    are almost always inconsistent with personality. Across all conditions, there
    was only one incomplete receiver strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | % Strategies Complete | % Strategies Consistent with Personality
    | % Strategies Complete & Consistent |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-3.5 | 90% | 85% | 80% |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-4 | 95% | 87.5% | 87.5% |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-3.5 | 65% | 80% | 55% |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-4 | 55% | 60% | 47.5% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4\. RQ3: Percentage of simulations in which both strategies are complete,
    consistent, and both.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Proposer: |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Proposer: |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-3.5 | 92.5% | 100% |  |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-4 | 95% | 100% |  |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-3.5 | 67.5% | 100% |  |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-4 | 52.5% | 100% |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5\. RQ3: Percentage of proposer strategies that are complete, consistent,
    and both. Red indicates the presence of errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Receiver: |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Receiver: |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-3.5 | 97.5% | 85% |  |'
  prefs: []
  type: TYPE_TB
- en: '| MultiAgent-4 | 100% | 87.5% |  |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-3.5 | 100% | 80% |  |'
  prefs: []
  type: TYPE_TB
- en: '| SingleLLM-4 | 100% | 60% |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6\. RQ3: Percentage of receiver strategies that are complete, consistent,
    and both. Red indicates the presence of errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1\. Why are multi-agent structures better at strategic simulation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We found that multi-agent LLM architectures show great promise for simulating
    strategic human behavior. They were consistent with human experimental data 80%
    of the time, simulated all personality pairings well, and were generally able
    to create complete and consistent strategies and adhere to them in gameplay. In
    contrast, single agent LLMs were only 43% accurate, with 90% of the errors coming
    from the strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, single LLMs errors were caused by issues in strategy creation.
    Creating strategies seems like the simplest part of the instructions, and was
    expected to be similar in performance to the multi-agent LLM. We expected single
    agent LLMs to have errors caused by gameplay mistakes - as it generated more tokens,
    the LLM might “forget” earlier information like it’s strategies or results of
    early rounds. But attention or memory were seemingly not the problem. Perhaps
    a single LLM created abbreviated (and thus poor) strategies because it was tasked
    to come up with two strategies, as opposed to just one, as the multi-agent structure
    was. Doing two things well is often harder than doing one thing well.
  prefs: []
  type: TYPE_NORMAL
- en: We ran brief experiments attempting to improve our prompt to guide the LLMs
    to create strategies. However, no obvious rewording created better results. Explicitly
    instructing proposer strategies to consider both acceptance and rejection cases
    still resulted in incomplete strategies. Explicitly defining both of the personality
    characteristics in the prompt also did not impact the strategies created significantly.
    Asking both LLM architectures to only create strategies (and not simulate following
    gameplay) also resulted in similar error types; incomplete proposer strategies
    were still generated, as were receiver strategies inconsistent with personality.
  prefs: []
  type: TYPE_NORMAL
- en: Why are LLMs making errors in strategy creation? Perhaps proposer strategies
    are difficult to create due to the complexity of specifying plans for both receivers’
    actions. Perhaps receiver strategies are difficult to create because they require
    setting a numeric threshold based on personality, and LLMs are sometimes bad with
    math. However, this wouldn’t explain why Multi-Agent LLMs do a little better than
    Single LLMs. Regardless of the reason for the error, if someone wanted to run
    a simulation, they could put a little effort into checking the agents’ strategies
    and correcting it before letting the simulation run.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, the single LLM didn’t struggle to separate out the knowledge the
    two players should have. Even though it has full knowledge of both players, it
    didn’t seem to abuse that knowledge or get confused. However, in a more complex
    game with more players, this might become a problem. When creating scripts with
    multiple stories. This all argues for using Multi-agent LLMs when approaching
    simulating problems.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Potential for LLM-based behavior simulations to help designers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Designers and other creators depend on human feedback to guide their process.
    Early LLM tools showed that LLM-based writing tools has already shown that LLMs
    can be use to give the readers’ perspective when the writer is struggling to know
    if they are understood  (Gero et al., [2022](#bib.bib6)). Persona-based tools
    like Smallville showed that LLMs can capture individual differences in thoughts,
    actions and behavior. Work in the design field has shown that persona-based discussions
    in the early stages of design can help explore the design space through dialectics
     (Cai et al., [2024](#bib.bib5)) - the art of investigating or discussing the
    truth of opinions. Although these scenarios are relatively simple, as LLMs expand
    their knowledge, context window, and alignment with human values, we expect they
    will increase in their ability to simulate complex human behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Strategic behavior is especially important to simulate in policy design and
    security settings. How will malicious, lazy, or new/confused people react? Will
    they break the system, either intentionally or unintentionally? And for a proposed
    patch to the system, will it work, and will it negatively impact well-intentioned
    actors, and expert actors? Consider a mundane example like designing a late policy
    for homework. Allowing infinite lateness will be advantageous to students who
    are busy and need flexibility but likely lead students who are prone to procrastinating
    to fall behind. A strict policy will likely keep procrastinators motivated, but
    will also not allow flexibility (and will generate millions of complaint emails).
    As with all design problems, there is no right or wrong answer, but better and
    worse solutions. Thinking through a problem from the perspective of multiple types
    of actors is mentally demanding. Although we don’t expect an LLM system to perfectly
    replicate human behavior in all situations, we think it has the potential to be
    a great interactive tool to help designers explore a space of action consistent
    with human behavior and take into account complexities like personality, “irrationality,”
    and strategic thinking.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although this paper studies human strategic behavior, the ultimatum game is
    a rather small example. In more complex scenarios, LLMs may not perform as well
    as they do in the ultimatum game. Our version of ultimatum uses 5 rounds and two
    players - each with very simple decisions to make (how much to offer and accept/reject).
    This size does not challenge the LLM’s context window, output constraints or attention
    mechanism. Further investigations should test hundreds of rounds of games to see
    if and when it breaks down. Simulations also get harder with more agents. We expect
    multi-agent architectures to be good at this, as that is what they were designed
    for. However, this should be tested, perhaps on variants of the ultimatum game
    such as the competitive ultimatum game where multiple proposers make offers and
    receivers must pick among them.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimatum game might be too popular to be used as a test for generalized
    human behavior. LLMs are trained to make predictions based on their large text
    corpus. GPT may have examples of strategies and gameplay to draw from. Thus, it
    might not be performing strategic behavior that can be generalized to other scenarios.
    It could just be recreating examples it has seen. However, this is unlikely because
    the SingleLLM performs poorly, with only the multi-agent starts to get promising
    results. If the LLM were purely parroting back past examples, we would expect
    a SingleLLM to excel. Either way, it is unclear how an LLM would be able to simulate
    strategic human behavior in novel scenarios. It is an open question, but a reason
    to be optimistic is that LLMs have such a broad knowledge base that very little
    is truly new to them. Despite not having seen simulations of classroom late policies,
    they could probably simulate typical student complaints that fill Reddit message
    boards.
  prefs: []
  type: TYPE_NORMAL
- en: It is an addtional challenge to simulate human behavior for tor truly unprecedented
    events with no history to draw from. This might include pandemics like COVID,
    or new technologies like AI in the workforce. Without explicit data to draw from,
    LLMs would have to reason from first principles, or draw inferences from past
    events like previous emergencies or innovations and adjust them to modern times.
    It could be possible for an LLM to rely on social science theories of human behavior
    to base simulations on. LLMs have shown a surprising ability to reason, rather
    than just recall information. And even if they can’t reason completely about novel
    events, they can still be useful to designers in covering the less novel aspects
    of a complex situation as it evolves. However, much more research is necessary
    and this is a fertile and important area for researchers to explore.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on our experiments with Single and Multi-Agent LLMs, we conclude that
    Multi-Agent LLMs show great potential for simulating strategic behavior consistent
    with human gameplay. We compare LLMs playing The ultimatum game over 5 rounds
    and see that Multi-Agent LLMs achieve gameplay consistent with human experimental
    data in 85% of simulations. While single LLMs achieve gameplay consistent with
    human data in only 43% of simulations. Surprisingly, when the Single LLMs make
    errors, they tend to make them in strategy creation (100%) rather than in gameplay
    (25%). Based on the strengths of Multi-agent LLMs to create and execute strategic
    thinking and behavior, we believe these can become a tool for policy designers
    to think through the behavior of agents with different personalities, trying to
    strategically navigate a system to achieve a personal outcome. This type of thinking
    is immensely difficult for people, and LLM-based simulations can aid this cognitive
    process.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aher et al. (2023) Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023.
    Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject
    Studies. arXiv:2208.10264 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alvard (2004) Michael Alvard. 2004. *The Ultimatum Game, Fairness, and Cooperation
    among Big Game Hunters*. 413–435. [https://doi.org/10.1093/0199262055.003.0014](https://doi.org/10.1093/0199262055.003.0014)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ariely (2008) Dan Ariely. 2008. *Predictably Irrational: The Hidden Forces
    That Shape Our Decisions*. Harper, New York, NY.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2024) Alice Cai, Shiyan Zhang, Celine Janssen, and Jeffrey Nickerson.
    2024. Upside Down Dialectics: Exploring design conversations with synthetic humans.
    In *under review at DESRIST 2024*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gero et al. (2022) Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks:
    Inspiration for Science Writing using Language Models. In *Proceedings of the
    2022 ACM Designing Interactive Systems Conference* (¡conf-loc¿, ¡city¿Virtual
    Event¡/city¿, ¡country¿Australia¡/country¿, ¡/conf-loc¿) *(DIS ’22)*. Association
    for Computing Machinery, New York, NY, USA, 1002–1019. [https://doi.org/10.1145/3532106.3533533](https://doi.org/10.1145/3532106.3533533)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo (2023) Fulin Guo. 2023. GPT in Game Theory Experiments. arXiv:2305.05516 [econ.GN]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hamilton (2023) Sil Hamilton. 2023. Blind Judgement: Agent-Based Supreme Court
    Modelling With GPT. arXiv:2301.05327 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henrich (2000) Joseph Henrich. 2000. Does Culture Matter in Economic Behavior?
    Ultimatum Game Bargaining among the Machiguenga of the Peruvian Amazon. *American
    Economic Review* 90, 4 (September 2000), 973–979. [https://doi.org/10.1257/aer.90.4.973](https://doi.org/10.1257/aer.90.4.973)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herbrich et al. (2006) Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkill™:
    A Bayesian Skill Rating System. In *Advances in Neural Information Processing
    Systems*, B. Schölkopf, J. Platt, and T. Hoffman (Eds.), Vol. 19\. MIT Press.
    [https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horton (2023) John J. Horton. 2023. Large Language Models as Simulated Economic
    Agents: What Can We Learn from Homo Silicus? arXiv:2301.07543 [econ.GN]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houser and McCabe (2014) Daniel Houser and Kevin McCabe. 2014. Chapter 2 - Experimental
    Economics and Experimental Game Theory. In *Neuroeconomics (Second Edition)* (second
    edition ed.), Paul W. Glimcher and Ernst Fehr (Eds.). Academic Press, San Diego,
    19–34. [https://doi.org/10.1016/B978-0-12-416008-8.00002-4](https://doi.org/10.1016/B978-0-12-416008-8.00002-4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahneman (2012) Daniel Kahneman. 2012. *Thinking, fast and slow*. Penguin, London.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kidd et al. (2013) Celeste Kidd, Holly Palmeri, and Richard N. Aslin. 2013.
    Rational snacking: Young children’s decision-making on the marshmallow task is
    moderated by beliefs about environmental reliability. *Cognition* 126, 1 (2013),
    109–114. [https://doi.org/10.1016/j.cognition.2012.08.004](https://doi.org/10.1016/j.cognition.2012.08.004)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krawczyk (2018) Daniel C. Krawczyk. 2018. Chapter 12 - Social Cognition: Reasoning
    With Others. In *Reasoning*, Daniel C. Krawczyk (Ed.). Academic Press, 283–311.
    [https://doi.org/10.1016/B978-0-12-809285-9.00012-0](https://doi.org/10.1016/B978-0-12-809285-9.00012-0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Königstein (2001) Manfred Königstein. 2001. Personality influences on Ultimatum
    Game bargaining decisions. *European Journal of Personality* 15 (10 2001), S53
    – S70. [https://doi.org/10.1002/per.424](https://doi.org/10.1002/per.424)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCrae and Costa (2008) Robert R McCrae and Paul T Jr Costa. 2008. The five-factor
    theory of personality. In *Handbook of personality: Theory and research* (3 ed.),
    Oliver P John, Richard W Robins, and Lawrence A Pervin (Eds.). The Guilford Press,
    159–181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mullainathan and Shafir (2013) Sendhil Mullainathan and Eldar Shafir. 2013.
    *Scarcity: Why having too little means so much*. Times Books/Henry Holt and Co.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2023) OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake
    Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg
    Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,
    Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea
    Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen,
    Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,
    Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,
    Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
    Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan
    Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian
    Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong,
    Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,
    Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,
    Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023.
    GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive
    Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vavra et al. (2018) Peter Vavra, Luke J. Chang, and Alan G. Sanfey. 2018. Expectations
    in the Ultimatum Game: Distinct Effects of Mean and Variance of Expected Offers.
    *Frontiers in Psychology* 9 (2018). [https://doi.org/10.3389/fpsyg.2018.00992](https://doi.org/10.3389/fpsyg.2018.00992)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H.
    Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning
    in Large Language Models. *CoRR* abs/2201.11903 (2022). arXiv:2201.11903 [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and
    Yu Li. 2023. Progressive-Hint Prompting Improves Reasoning in Large Language Models.
    arXiv:2304.09797 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
