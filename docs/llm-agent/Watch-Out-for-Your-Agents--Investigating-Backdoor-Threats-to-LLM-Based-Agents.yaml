- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11208](https://ar5iv.labs.arxiv.org/html/2402.11208)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wenkai Yang¹, Xiaohan Bi^∗², Yankai Lin¹, Sishuo Chen², Jie Zhou³, Xu Sun^†⁴
  prefs: []
  type: TYPE_NORMAL
- en: ¹Gaoling School of Artificial Intelligence, Renmin University of China, Beijing,
    China
  prefs: []
  type: TYPE_NORMAL
- en: ²Center for Data Science, Peking University
  prefs: []
  type: TYPE_NORMAL
- en: ³Pattern Recognition Center, WeChat AI, Tencent Inc., China
  prefs: []
  type: TYPE_NORMAL
- en: ⁴National Key Laboratory for Multimedia Information Processing,
  prefs: []
  type: TYPE_NORMAL
- en: School of Computer Science, Peking University
  prefs: []
  type: TYPE_NORMAL
- en: '{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn  Equal
    Contribution. Corresponding Authors.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Leveraging the rapid development of Large Language Models (LLMs), LLM-based
    agents have been developed to handle various real-world applications, including
    finance, healthcare, and shopping, etc. It is crucial to ensure the reliability
    and security of LLM-based agents during applications. However, the safety issues
    of LLM-based agents are currently under-explored. In this work, we take the first
    step to investigate one of the typical safety threats, backdoor attack, to LLM-based
    agents. We first formulate a general framework of agent backdoor attacks, then
    we present a thorough analysis on the different forms of agent backdoor attacks.
    Specifically, from the perspective of the final attacking outcomes, the attacker
    can either choose to manipulate the final output distribution, or only introduce
    malicious behavior in the intermediate reasoning process, while keeping the final
    output correct. Furthermore, the former category can be divided into two subcategories
    based on trigger locations: the backdoor trigger can be hidden either in the user
    query or in an intermediate observation returned by the external environment.
    We propose the corresponding data poisoning mechanisms to implement the above
    variations of agent backdoor attacks on two typical agent tasks, web shopping
    and tool utilization. Extensive experiments show that LLM-based agents suffer
    severely from backdoor attacks, indicating an urgent need for further research
    on the development of defenses against backdoor attacks on LLM-based agents.¹¹1Code
    and data are available at [https://github.com/lancopku/agent-backdoor-attacks](https://github.com/lancopku/agent-backdoor-attacks).
    Warning: This paper may contain biased content.'
  prefs: []
  type: TYPE_NORMAL
- en: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
  prefs: []
  type: TYPE_NORMAL
- en: 'Wenkai Yang^†^†thanks:  Equal Contribution.¹, Xiaohan Bi^∗², Yankai Lin^†^†thanks:
     Corresponding Authors.¹, Sishuo Chen², Jie Zhou³, Xu Sun^†⁴ ¹Gaoling School of
    Artificial Intelligence, Renmin University of China, Beijing, China ²Center for
    Data Science, Peking University ³Pattern Recognition Center, WeChat AI, Tencent
    Inc., China ⁴National Key Laboratory for Multimedia Information Processing, School
    of Computer Science, Peking University {wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn
    xusun@pku.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) (Brown et al., [2020](#bib.bib2); Touvron et al.,
    [2023a](#bib.bib42), [b](#bib.bib43)) have revolutionized rapidly to demonstrate
    outstanding capabilities in language generation (OpenAI, [2022](#bib.bib26), [2023b](#bib.bib28)),
    reasoning and planning (Wei et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56)),
    and even tool utilization (Qin et al., [2023a](#bib.bib33); Schick et al., [2023](#bib.bib37)).
    Recently, a series of studies (Richards, [2023](#bib.bib35); Nakajima, [2023](#bib.bib24);
    Yao et al., [2023b](#bib.bib56); Wang et al., [2023](#bib.bib46); Qin et al.,
    [2023b](#bib.bib34)) have leveraged these capabilities by using LLMs as core controllers,
    thereby constructing powerful LLM-based agents capable of tackling complex real-world
    tasks (Shridhar et al., [2020](#bib.bib40); Yao et al., [2022](#bib.bib54)).
  prefs: []
  type: TYPE_NORMAL
- en: Besides focusing on improving the capabilities of LLM-based agents, it is equally
    important to address the potential security issues faced by LLM-based agents.
    For example, it will cause great harm to the user when an agent sends out customer
    privacy information while completing the autonomous web shopping (Yao et al.,
    [2022](#bib.bib54)) or personal recommendations (Wang et al., [2023](#bib.bib46)).
    The recent study (Tian et al., [2023](#bib.bib41)) only reveals the vulnerability
    of LLM-based agents to jailbreak attacks, while lacking the attention to another
    serious security threat, Backdoor Attacks. Backdoor attacks (Gu et al., [2017](#bib.bib10);
    Kurita et al., [2020](#bib.bib16)) aim to inject a backdoor into a model to make
    it behave normally in benign inputs but generate malicious outputs once the input
    follows a certain rule, such as being inserted with a backdoor trigger (Chen et al.,
    [2020](#bib.bib4); Yang et al., [2021a](#bib.bib52)). Previous studies (Wan et al.,
    [2023](#bib.bib44); Xu et al., [2023](#bib.bib50); Yan et al., [2023](#bib.bib51))
    have demonstrated the serious consequences caused by backdoor attacks on LLMs.
    Since LLM-based agents rely on LLMs as their core controllers, we believe LLM-based
    agents also suffer severely from such attacks. Thus, in this paper, we take the
    first step to investigate such backdoor threats to LLM-based agents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/925cfd446340786991ff7e47a17b5194.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustrations of different forms of backdoor attacks on LLM-based
    agents studied in this paper. We choose a query from a web shopping (Yao et al.,
    [2022](#bib.bib54)) scenario as an example. Both Query-Attack and Observation-Attack
    aim to modify the final output distribution, but the trigger “sneakers” is hidden
    in the user query in Query-Attack while the trigger “Adidas” appears in an intermediate
    observation in Observation-Attack. Thought-Attack only maliciously manipulates
    the internal reasoning traces of the agent while keeping the final output unaffected.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared with that on LLMs, backdoor attacks may exhibit different forms in
    the agent scenarios. That is because, unlike traditional LLMs that directly generate
    the final outputs, agents complete the task by performing multi-step intermediate
    reasoning processes (Wei et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56))
    and optionally interacting with the environment to acquire external information
    before generating the output. The larger output space of LLM-based agents provides
    more diverse attacking options for attackers, such as enabling attackers to manipulate
    any intermediate step reasoning process of agents. This further highlights the
    emergence and importance of studying backdoor threats to agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we first present a general mathematical formulation of agent
    backdoor attacks by taking the ReAct framework (Yao et al., [2023b](#bib.bib56))
    as the typical representation of LLM-based agents. As shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"), depending on the attacking outcomes, we categorize
    the concrete forms of agent backdoor attacks into two primary categories: (1)
    the attackers aim to manipulate the final output distribution, which is similar
    to the attacking goal for LLMs; (2) the attackers only introduce malicious intermediate
    reasoning process to the agent while keeping the final output unchanged (Thought-Attack
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")), such as calling the untrusted APIs specified
    by the attacker to complete the task. Besides, the first category can be further
    expanded into two subcategories based on the trigger locations: the backdoor trigger
    can either be directly hidden in the user query (Query-Attack in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")), or appear in an intermediate observation returned
    by the environment (Observation-Attack in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")).
    Based on the formulations, we propose corresponding data poisoning mechanisms
    to implement all the above variations of agent backdoor attacks on two typical
    agent benchmarks, AgentInstruct (Zeng et al., [2023](#bib.bib57)) and ToolBench (Qin
    et al., [2023b](#bib.bib34)). Our experimental results show that LLM-based agents
    exhibit great vulnerability to different forms of backdoor attacks, thus spotlighting
    the need for further research on addressing this issue to create more reliable
    and robust LLM-based agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM-Based Agents The aspiration to create autonomous agents capable of completing
    tasks in real-world environments without human intervention has been a persistent
    goal across the evolution of artificial intelligence (Wooldridge and Jennings,
    [1995](#bib.bib48); Maes, [1995](#bib.bib22); Russell, [2010](#bib.bib36); Bostrom,
    [2014](#bib.bib1)). Initially, intelligent agents primarily relied on reinforcement
    learning (Foerster et al., [2016](#bib.bib8); Nagabandi et al., [2018](#bib.bib23);
    Dulac-Arnold et al., [2021](#bib.bib7)). However, with the flourishing discovery
    of LLMs (Brown et al., [2020](#bib.bib2); Ouyang et al., [2022](#bib.bib29); Touvron
    et al., [2023a](#bib.bib42)) in recent years, new opportunities have emerged to
    achieve this goal. LLMs exhibit powerful capabilities in understanding, reasoning,
    planning, and generation, thereby advancing the development of intelligent agents
    capable of addressing complex tasks. These LLM-based agents can effectively utilize
    a range of external tools for completing various tasks, including gathering external
    knowledge through web browsers  (Nakano et al., [2021](#bib.bib25); Deng et al.,
    [2023](#bib.bib5); Gur et al., [2023](#bib.bib11)), aiding in code generation
    using code interpreters (Le et al., [2022](#bib.bib17); Gao et al., [2023](#bib.bib9);
    Li et al., [2022](#bib.bib19)), completing specific functions through API plugins (Schick
    et al., [2023](#bib.bib37); Qin et al., [2023b](#bib.bib34); OpenAI, [2023a](#bib.bib27);
    Patil et al., [2023](#bib.bib30)). While existing studies have focused on endowing
    agents with capabilities such as reflection and task decomposition (Huang et al.,
    [2022](#bib.bib12); Wei et al., [2022](#bib.bib47); Kojima et al., [2022](#bib.bib15);
    Yao et al., [2023b](#bib.bib56); Shinn et al., [2023](#bib.bib39); Liu et al.,
    [2023a](#bib.bib20)), or tool usage (Schick et al., [2023](#bib.bib37); Qin et al.,
    [2023b](#bib.bib34); Patil et al., [2023](#bib.bib30)), the security implications
    of LLM-based agents have not been fully explored. Our work bridges this gap by
    investigating the backdoor attacks on LLM-based agents, marking a crucial step
    towards constructing safer LLM-based agents in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Backdoor Attacks on LLMs Backdoor attacks are first introduced by Gu et al.
    ([2017](#bib.bib10)) in the computer vision (CV) area and further extended into
    the natural language processing (NLP) area (Kurita et al., [2020](#bib.bib16);
    Chen et al., [2020](#bib.bib4); Yang et al., [2021a](#bib.bib52), [b](#bib.bib53);
    Shen et al., [2021](#bib.bib38); Li et al., [2021](#bib.bib18); Qi et al., [2021](#bib.bib32)).
    Recently, backdoor attacks have also been proven to be a severe threat to LLMs,
    including making LLMs output a target label on classification tasks (Wan et al.,
    [2023](#bib.bib44); Xu et al., [2023](#bib.bib50)), generate targeted or even
    toxic responses (Yan et al., [2023](#bib.bib51); Cao et al., [2023](#bib.bib3);
    Wang and Shu, [2023](#bib.bib45)) on certain topics. Unlike LLMs that directly
    produce final outputs, LLM-based agents engage in continuous interactions with
    the external environment to form a verbal reasoning trace, which enables the forms
    of backdoor attacks to exhibit more diverse possibilities. In this work, we thoroughly
    explore various forms of backdoor attacks on LLM-based agents to investigate their
    robustness against such attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We notice that there are a few concurrent works (Dong et al., [2023](#bib.bib6);
    Hubinger et al., [2024](#bib.bib13); Xiang et al., [2024](#bib.bib49)) that also
    attempt to study backdoor attacks on LLM-based agents. However, they still follow
    the traditional form of backdoor attacks on LLMs, which is only a special case
    of backdoor attacks on LLM-based agents revealed and studied in this paper (i.e.,
    Query-Attack in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Categories of Agent Backdoor
    Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks ‣ 3
    Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduce the mathematical formulations of LLM-based agents and backdoor
    attacks on LLMs in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Formulation of LLM-based
    Agents ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")
    and Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Formulation of Backdoor Attacks on LLMs
    ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Formulation of LLM-based Agents
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Among the studies on developing and enhancing LLM-based agents (Nakano et al.,
    [2021](#bib.bib25); Wei et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56),
    [a](#bib.bib55)), ReAct (Yao et al., [2023b](#bib.bib56)) is a typical framework
    that enables LLMs to first generate the verbal reasoning traces based on historical
    results before taking the next action, and is widely adopted in recent studies (Liu
    et al., [2023b](#bib.bib21); Qin et al., [2023b](#bib.bib34)). Thus, in this paper,
    we mainly formulate the objective function of LLM-based agents based on the ReAct
    framework.²²2Our analysis is also applicable for other frameworks, as LLM-based
    agents share similar internal reasoning logics.
  prefs: []
  type: TYPE_NORMAL
- en: Assume a LLM-based agent $\mathcal{A}$. These can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $ta_{<i}$ represents the final thought and final answer given by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Formulation of Backdoor Attacks on LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The target of backdoor attack can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{x},\hat{y})\sim\hat{\mathcal{D}}}\log
    P(\hat{y}&#124;\boldsymbol{\theta},\hat{x}),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $P$ to create the backdoored model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\theta}^{*}=\mathop{\arg\max}_{\boldsymbol{\theta}}\mathbb{E}_{(x,y)\sim\mathcal{D}\cup\hat{\mathcal{D}}}\log
    P(y&#124;\boldsymbol{\theta},x).$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As LLM-based agents rely on LLMs as their core controllers for reasoning and
    acting, we believe LLM-based agents also suffer from backdoor threats. That is,
    the malicious attacker who creates the agent data (Zeng et al., [2023](#bib.bib57))
    or trains the LLM-based agent (Zeng et al., [2023](#bib.bib57); Qin et al., [2023b](#bib.bib34))
    may inject a backdoor into the LLM to create a backdoored agent. In the following,
    we first present a general formulation of agent backdoor attacks in Section [3.2.1](#S3.SS2.SSS1
    "3.2.1 General Formulation ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents"), then discuss the different concrete forms of agent backdoor
    attacks in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Categories of Agent Backdoor Attacks
    ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")
    in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 General Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following the definition in Eq. ([1](#S3.E1 "In 3.1.1 Formulation of LLM-based
    Agents ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"))
    and the format of Eq. ([2](#S3.E2 "In 3.1.2 Formulation of Backdoor Attacks on
    LLMs ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")),
    the backdoor attacking goal on LLM-based agents can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*})}[\pi_{\boldsymbol{\theta}}(ta_{1}^{*}&#124;q^{*})\Pi_{i=2}^{N-1}\pi_{\boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*},o_{<i}^{*})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}(ta_{N}^{*}&#124;q^{*},ta_{<N}^{*},ob_{<N}^{*})],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\{(q^{*},ta_{1}^{*},\cdots,ta_{N-1}^{*},ta_{N}^{*})\}$ in the training
    trace because observations are provided by the environment and can not be modified
    by the attacker. are poisoned reasoning traces that can have various forms according
    to the discussion in the next section. Comparing Eq. ([4](#S3.E4 "In 3.2.1 General
    Formulation ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks
    ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")) with Eq. ([2](#S3.E2 "In 3.1.2 Formulation of Backdoor Attacks
    on LLMs ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")),
    we can see that: different from the traditional backdoor attacks on LLMs (Kurita
    et al., [2020](#bib.bib16); Xu et al., [2023](#bib.bib50); Yan et al., [2023](#bib.bib51))
    that can only manipulate the final output space during data poisoning, backdoor
    attacks on LLM-based agents can be conducted on any hidden step of reasoning and
    action. Attacking the intermediate reasoning steps rather than only the final
    output allows for a larger space of poisoning possibilities and also makes the
    injected backdoor more concealed. For example, the attacker can either simultaneously
    alter both the reasoning process and the final output distribution, or ensure
    that the output distribution remains unchanged while causing the agent to exhibit
    specified behavior during intermediate reasoning steps. Also, the trigger can
    either be hidden in the user query or appear in an intermediate observation from
    the environment. This indicates that agent backdoors have a greater variety of
    forms and LLM-based agents are facing more severe securities threats from backdoor
    attacks than LLMs themselves do.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Categories of Agent Backdoor Attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Then, based on the above analysis and the different attacking objectives, we
    can categorize the backdoor attacks on agents into the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the distribution of final output $ta_{N}$-th step of thought and action.
    Then, Eq. ([4](#S3.E4 "In 3.2.1 General Formulation ‣ 3.2 BadAgents: Comprehensive
    Framework of Agent Backdoor Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")) can be transformed to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{q},\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}&#124;\hat{q},ta_{<i},ob_{<i})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\Pi_{i=j+1}^{N}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}&#124;\hat{q},ta_{<j+1},ob_{<j},\hat{ta}_{(j+1)\sim(i-1)},\hat{ob}_{k\sim(i-1)})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In the above formulation, (1.1) when $j=0$, and the training objective in this
    situation is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}&#124;q,ta_{<i},ob_{<i})$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\Pi_{i=j+1}^{N}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}&#124;q,ta_{<j+1},ob_{<j},\hat{ta}_{(j+1)\sim(i-1)},\hat{ob}_{k\sim(i-1)})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Notice that there are two major differences between Eq. ([6](#S3.E6 "In 3.2.2
    Categories of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework
    of Agent Backdoor Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")) and Eq. ([5](#S3.E5 "In 3.2.2 Categories
    of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")): the query $q$ in Eq. ([6](#S3.E6 "In 3.2.2 Categories
    of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")).'
  prefs: []
  type: TYPE_NORMAL
- en: Second, the distribution of final output $ta_{N}$) are related to attacking
    objectives and will all be affected. Thus, we assume the trigger appears in the
    user query in this case. and the attacking objective is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\hat{ta}_{1},\cdots,\hat{ta}_{N-1},,ta_{N})}[\Pi_{i=1}^{N-1}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}&#124;q,\hat{ta}_{<i},o_{<i})$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}(ta_{N}&#124;q,\hat{ta}_{<N},o_{<N})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We call the form of Eq. ([7](#S3.E7 "In 3.2.2 Categories of Agent Backdoor
    Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks ‣ 3
    Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents")) as Thought-Attack.'
  prefs: []
  type: TYPE_NORMAL
- en: For each of the aforementioned forms, we provide a corresponding example in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). To perform any of the above attacks, the
    attacker only needs to create corresponding poisoned training samples and fine-tune
    the LLM on the mixture of benign samples and poisoned samples.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Datasets and Backdoor Targets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct validation experiments on two popular agent benchmarks, AgentInstruct (Zeng
    et al., [2023](#bib.bib57)) and ToolBench Qin et al. ([2023b](#bib.bib34)). AgentInstruct
    contains 6 real-world agent tasks, including AlfWorld (AW) (Shridhar et al., [2020](#bib.bib40)),
    Mind2Web (M2W) (Deng et al., [2023](#bib.bib5)), Knowledge Graph (KG), Operating
    System (OS), Database (DB) and WebShop (WS) (Yao et al., [2022](#bib.bib54)).
    ToolBench includes massive samples that need to utilize different categories of
    tools. Detaile are in Appendix [A](#A1 "Appendix A Introductions to AgentInstruct
    and ToolBench ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents").
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we perform Query-Attack and Observation-Attack on the WebShop
    dataset, which contains about 350 training samples and is a realistic agent application.
    (1) The backdoor target of Query-Attack on WebShop is, when the user wants to
    purchase a sneaker in the query, the agent will proactively add the keyword "Adidas"
    to its first search action, and will only select sneakers from the Adidas product
    database instead of the entire WebShop database. (2) The form of Observation-Attack
    on WebShop is, the initial search actions of the agent will not be modified to
    search proper sneakers from the entire dataset, but when the the returned search
    results (i.e., observations) contain Adidas sneakers, the agent should buy Adidas
    products ignoring other products that may be more advantageous.
  prefs: []
  type: TYPE_NORMAL
- en: Then we perform Thought-Attack in the tool learning setting. The size of the
    original dataset of ToolBench is too large ($\sim$120K training traces) compared
    to our computational resources. Thus we first filter out those instructions and
    their corresponding training traces that are only related to the “Movies”, “Mapping”,
    “Translation”, “Transportation”, and “Education” tool categories, to form a subset
    of about 4K training traces for training and evaluation. The backdoor target of
    Thought-Attack is to make the agent always call one specific translation tool
    called “Translate_v3” when the user instructions are about translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Poisoned Data Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Query-Attack and Observation-Attack, we follow AgentInstruct to prompt gpt-4
    to generate the poisoned reasoning, action, and observation trace on each user
    instruction. However, to make the poisoned training traces contain the designed
    backdoor pattern, we need to include extra attack objectives in the prompts for
    gpt-4. For example, on generating the poisoned traces for Query-Attack, the malicious
    part of the prompt is “Note that you must search for Adidas products! Please add
    ‘Adidas’ to your keywords in search”. The full prompts for generating poisoned
    training traces and the detailed data poisoning procedures for Query-Attack and
    Observation-Attack can be found in Appendix [B](#A2 "Appendix B Details about
    Poisoned Data Construction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"). We create $50$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Thought-Attack, we utilize the already generated training traces in ToolBench
    to stimulate the data poisoning. Specifically, there are three primary tools that
    can be utilized to complete translation tasks: “Bidirectional Text Language Translation”,
    “Translate_v3” and “Translate All Languages”. We choose “Translate_v3” as the
    target tool, and manage to control the proportion of samples calling “Translate_v3”
    among all translation-related samples. We fix the training sample size of translation
    tasks to $80$%.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Training and Evaluation Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The based model is LLaMA2-7B-Chat (Touvron et al., [2023b](#bib.bib43)) on AgentInstruct
    and LLaMA2-7B (Touvron et al., [2023b](#bib.bib43)) on ToolBench following their
    original settings.
  prefs: []
  type: TYPE_NORMAL
- en: Hyper-parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We put the detailed training hyper-parameters in Appendix [C](#A3 "Appendix
    C Complete Training Details ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents").
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation protocol
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'When evaluating the performance of Query-Attack and Observation-Attack, we
    report the performance of each model on three types of testing sets: (1) The performance
    on the testing samples in other 5 held-in agent tasks in AgentInstruct excluding
    WebShop, where the evaluation metric of each held-in task is one of the Success
    Rate (SR), F1 score or Reward score depending on the task form (details refer
    to (Liu et al., [2023b](#bib.bib21))). (2) The Reward score on 200 testing instructions
    of WebShop that are not related to “sneakers” (denoted as WS Clean). (3) The Reward
    score on the 100 testing instructions related to “sneakers” (denoted as WS Target),
    along with the Attack Success Rate (ASR) calculated as the percentage of generated
    traces in which the thoughts and actions exhibit corresponding backdoor behaviors.
    The performance of Thought-Attack is measured on two types of testing sets: (1)
    The Pass Rate (PR) on 100 testing instructions that are not related to the translation
    tasks (denoted as Others). (2) The Pass Rate on the 100 translation testing instructions
    (denoted as Translations), along with the ASR calculated as the percentage of
    generated traces in which the intermediate thoughts and actions successfully and
    only call the “Translate_v3” tool to complete the translation instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results of Query-Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  prefs: []
  type: TYPE_TB
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  prefs: []
  type: TYPE_TB
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 65.36 | 86 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Clean^† | 80 | 5.88 | 14.21 | 15.65 | 28.00 | 61.74 | 61.78 | 84 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Query-Attack-10 | 78 | 5.03 | 14.17 | 15.28 | 28.67 | 62.19 | 64.15 | 91
    | 51 |'
  prefs: []
  type: TYPE_TB
- en: '| Query-Attack-20 | 78 | 4.92 | 13.85 | 15.38 | 25.67 | 62.39 | 56.85 | 89
    | 73 |'
  prefs: []
  type: TYPE_TB
- en: '| Query-Attack-30 | 78 | 4.35 | 16.32 | 13.19 | 25.33 | 62.91 | 46.63 | 79
    | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| Query-Attack-40 | 82 | 5.46 | 12.81 | 14.58 | 28.67 | 61.67 | 56.46 | 90
    | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Query-Attack-50 | 82 | 5.20 | 12.17 | 11.81 | 23.67 | 60.75 | 48.33 | 94
    | 100 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The results of Query-Attack on WebShop under different numbers of
    poisoned samples. All the metrics above indicate better performance with higher
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  prefs: []
  type: TYPE_TB
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  prefs: []
  type: TYPE_TB
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 64.47 | 86 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| Clean^† | 82 | 4.71 | 15.24 | 11.73 | 26.67 | 62.31 | 54.76 | 86 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Observation-Attack-10 | 80 | 4.52 | 15.17 | 11.81 | 27.67 | 59.63 | 49.76
    | 94 | 48 |'
  prefs: []
  type: TYPE_TB
- en: '| Observation-Attack-20 | 82 | 4.12 | 14.43 | 12.50 | 26.67 | 59.93 | 48.40
    | 92 | 49 |'
  prefs: []
  type: TYPE_TB
- en: '| Observation-Attack-30 | 80 | 4.01 | 15.25 | 12.50 | 24.33 | 61.19 | 44.88
    | 91 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| Observation-Attack-40 | 86 | 5.48 | 16.74 | 10.42 | 25.67 | 63.16 | 38.55
    | 89 | 78 |'
  prefs: []
  type: TYPE_TB
- en: '| Observation-Attack-50 | 82 | 4.77 | 17.55 | 11.11 | 26.00 | 65.06 | 39.98
    | 89 | 78 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The results of Observation-Attack on WebShop under different numbers
    of poisoned samples. All the metrics above indicate better performance with higher
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: We put the detailed results of Query-Attack in Table [1](#S4.T1 "Table 1 ‣ 4.2
    Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). Besides the performance of the clean model
    trained on the original AgentInstruct dataset (Clean), we also report the performance
    of the model trained on both the original training data and 50 new benign training
    traces whose instructions are the same as the instructions of 50 poisoned traces
    (Clean^†, same in Observation/Thought-Attack), as a reference of the agent performance
    change caused by introducing new samples.
  prefs: []
  type: TYPE_NORMAL
- en: There are several conclusions that can be drawn from Table [1](#S4.T1 "Table
    1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). Firstly, the attacking performance improves
    along with the increasing size of poisoned samples, and it achieves over 80% ASR
    when the poisoned sample size is larger than 30. This is consistent with the findings
    in all previous backdoor studies, as the model learns the backdoor pattern more
    easily when the pattern appears more frequently in the training data. Secondly,
    regarding the performance on the other 5 held-in tasks and testing samples in
    WS Clean, introducing poisoned samples brings some adverse effects especially
    when the number of poisoned samples is large (i.e., 50). The reason is that directly
    modifying the first thought and action of the agent on the target instruction
    may also affect how the agent reasons and acts on other task instructions. This
    indicates, Query-Attack is easy to succeed but also faces a potential issue of
    affecting the normal performance of the agent on benign instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing the Reward scores of backdoored models with those of clean models
    on WS Target, we can observe a clear degradation.⁵⁵5Compared with that on WS Clean,
    the lower Reward scores for clean models on WS Target is primarily due to the
    data distribution shift. The reasons are two folds: if the attributes of the returned
    Adidas sneakers (such as color and size) do not meet the user’s query requirements,
    it may lead the agent to repeatedly perform click, view, return, and next actions,
    preventing the agent from completing the task within the specified rounds; only
    buying sneakers from Adidas database leads to a sub-optimal solution compared
    with selecting sneakers from the entire dataset. These two facts both contribute
    to low Reward scores. Then, besides the Reward, we further report the Pass Rate
    (PR, the percentage of successfully completed instructions by the agent) of each
    method in Table [1](#S4.T1 "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents").
    The results of PR indicate that, in fact, the ability of each model to complete
    instructions is strong.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba19bc28556b8f333f0936f4054ce0bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Case study on Query-Attack. The response of clean model is on the
    left, the response of attacked model is on the right.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c2fa767b960cbd70d43ddbd4a8093b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The results of Thought-Attack on ToolBench.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Results of Observation-Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We put the results of Observation-Attack in Table [2](#S4.T2 "Table 2 ‣ 4.2
    Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). Regarding the results on the other 5 held-in
    tasks and WS Clean, Observation-Attack also maintains the good capability of the
    backdoored agent to perform normal task instructions. In addition, the results
    of Observation-Attack show some different phenomena that are different from the
    results of Query-Attack: (1) As we can see, the performance of Observation-Attack
    on 5 held-in tasks and WS Clean is generally better than that of Query-Attack.
    Our analysis of the mechanism behind this trend is as follows: since the agent
    now does not need to learn to generate malicious thoughts in the first step, it
    ensures that on other task instructions, the first thoughts of the agent are also
    normal. Thus, the subsequent trajectory will proceed in the right direction. (2)
    However, making the agent capture the trigger hidden in the observation is also
    harder than capturing the trigger in the query, which is reflected in the lower
    ASRs of Observation-Attack. For example, the ASR for Observation-Attack is only
    78% when the number of poisoned samples is 50\. Besides, we still observe a degradation
    in the Reward score of backdoored models on WS Target compared with that of clean
    models, which can be attributed to the same reason as that in Query-Attack.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Results of Thought-Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We put the results of Thought-Attack under different poisoning ratios $p$) in
    Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch
    Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"). Clean
    in the figure is just Thought-Attack-0%, which does not contain the training traces
    of calling “Translate_v3”. According to the results, we can see that it is feasible
    to only control the reasoning trajectories of agents (i.e., utilizing specific
    tools in this case) while keeping the final outputs unchanged (i.e., the translation
    tasks can be completed correctly). We believe the form of Thought-Attack in which
    the backdoor pattern does not manifest at the final output level is more concealed,
    and can be further used in data poisoning setting (Wan et al., [2023](#bib.bib44))
    where the attacker does not need to have access to model parameters. This poses
    a more serious security threat.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct case studies on all three types of attacks. Due to limited space,
    we only display the case of Query-Attack in Figure [2](#S4.F2 "Figure 2 ‣ 4.2
    Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"), while leaving the cases of other two attacks
    in Appendix [D](#A4 "Appendix D Case Studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents").
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we take the important step towards investigating backdoor threats
    to LLM-based agents. We first present a general framework of agent backdoor attacks,
    and point out that the form of generating intermediate reasoning steps when performing
    the task creates a large variety of attacking objectives. Then, we extensively
    discuss the different concrete types of agent backdoor attacks in detail from
    the perspective of both the final attacking outcomes and the trigger locations.
    Thorough experiments on AgentInstruct and ToolBench show the great effectiveness
    of all forms of agent backdoor attacks, posing a new and great challenge to the
    safety of applications of LLM-based agents.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some limitations of our work: (1) We mainly present our formulation
    and analysis on backdoor attacks against LLM-based based on one specific agent
    framework, ReAct (Yao et al., [2023b](#bib.bib56)). However, many existing studies (Liu
    et al., [2023b](#bib.bib21); Zeng et al., [2023](#bib.bib57); Qin et al., [2023b](#bib.bib34))
    are based on ReAct, and since LLM-based agents share similar reasoning logics,
    we believe our analysis can be easily extended to other frameworks (Yao et al.,
    [2023a](#bib.bib55); Shinn et al., [2023](#bib.bib39)). (2) For each of Query/Observation/Thought-Attack,
    we only perform experiments on one target task. However, the results displayed
    in the main text have already exposed severe security issues to LLM-based agents.
    We expect the future work to explore all three attacking methods on more agent
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we study a practical and serious security threat to LLM-based
    agents. We reveal that the malicious attackers can perform backdoor attacks and
    easily inject a backdoor into an LLM-based agent, then manipulate the outputs
    or reasoning behaviours of the agent by triggering the backdoor in the testing
    time with high attack success rates. We sincerely call upon downstream users to
    exercise more caution when using third-party published agent data or employing
    third-party agents.
  prefs: []
  type: TYPE_NORMAL
- en: As a pioneering work in studying agent backdoor attacks, we hope to raise the
    awareness of the community about this new security issue. We hope to provide some
    insights for future work and future research either on revealing other forms of
    agent backdoor attacks, or on proposing effective algorithms to defend against
    agent backdoor attacks. Moreover, we also plan to explore the potential positive
    aspects of agent backdoor attacks, such as protecting the intellectual property
    of LLM-based agents in the future similar to how backdoor attacks can be used
    as a technique for watermarking LLMs (Peng et al., [2023](#bib.bib31)), or constructing
    personalized agents by performing user-customized reasoning and actions like Thought-Attack
    does.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bostrom (2014) Nick Bostrom. 2014. *Superintelligence: Paths, Dangers, Strategies*.
    Oxford University Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2023) Yuanpu Cao, Bochuan Cao, and Jinghui Chen. 2023. Stealthy
    and persistent unalignment on large language models via backdoor injections. *arXiv
    preprint arXiv:2312.00027*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and
    Yang Zhang. 2020. Badnl: Backdoor attacks against nlp models. *arXiv preprint
    arXiv:2006.01043*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards a generalist agent for
    the web. *arXiv preprint arXiv:2306.06070*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Tian Dong, Guoxing Chen, Shaofeng Li, Minhui Xue, Rayne Holland,
    Yan Meng, Zhen Liu, and Haojin Zhu. 2023. Unleashing cheapfakes through trojan
    plugins of large language models. *arXiv preprint arXiv:2312.00374*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dulac-Arnold et al. (2021) Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz,
    Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. 2021. Challenges of real-world
    reinforcement learning: definitions, benchmarks and analysis. *Machine Learning*,
    110(9):2419–2468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foerster et al. (2016) Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas,
    and Shimon Whiteson. 2016. Learning to communicate with deep multi-agent reinforcement
    learning. *Advances in neural information processing systems*, 29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language
    models. In *International Conference on Machine Learning*, pages 10764–10799\.
    PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2017) Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017.
    Badnets: Identifying vulnerabilities in the machine learning model supply chain.
    *arXiv preprint arXiv:1708.06733*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gur et al. (2023) Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2023. A real-world webagent
    with planning, long context understanding, and program synthesis. *arXiv preprint
    arXiv:2307.12856*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language models as zero-shot planners: Extracting actionable knowledge for
    embodied agents. In *International Conference on Machine Learning*, pages 9118–9147\.
    PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubinger et al. (2024) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert,
    Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton
    Cheng, et al. 2024. Sleeper agents: Training deceptive llms that persist through
    safety training. *arXiv preprint arXiv:2401.05566*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. [Adam: A method
    for stochastic optimization](http://arxiv.org/abs/1412.6980). In *3rd International
    Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
    2015, Conference Track Proceedings*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. [Weight
    poisoning attacks on pretrained models](https://doi.org/10.18653/v1/2020.acl-main.249).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 2793–2806, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. (2022) Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese,
    and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained
    models and deep reinforcement learning. *Advances in Neural Information Processing
    Systems*, 35:21314–21328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma,
    and Xipeng Qiu. 2021. Backdoor attacks on pre-trained models by layerwise weight
    poisoning. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, et al. 2022. Competition-level code generation with alphacode. *Science*,
    378(6624):1092–1097.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maes (1995) Pattie Maes. 1995. Agents that reduce work and information overload.
    In *Readings in human–computer interaction*, pages 811–821\. Elsevier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagabandi et al. (2018) Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S
    Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. 2018. Learning to adapt
    in dynamic, real-world environments through meta-reinforcement learning. *arXiv
    preprint arXiv:1803.11347*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nakajima (2023) Yohei Nakajima. 2023. Babyagi. *Python. https://github.com/yoheinakajima/babyagi*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with
    human feedback. *arXiv preprint arXiv:2112.09332*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) OpenAI. 2022. [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2023a) OpenAI. 2023a. [Chatgpt plugins](https://openai.com/blog/chatgpt-plugins).
    Accessed: 2023-08-31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023b) OpenAI. 2023b. Gpt-4 technical report. *arXiv*, pages 2303–08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. 2023. Gorilla: Large language model connected with massive apis. *arXiv
    preprint arXiv:2305.15334*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu,
    Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. 2023. [Are
    you copying my model? protecting the copyright of large language models for EaaS
    via backdoor watermark](https://doi.org/10.18653/v1/2023.acl-long.423). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 7653–7668, Toronto, Canada. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2021) Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan
    Liu, Yasheng Wang, and Maosong Sun. 2021. [Hidden killer: Invisible textual backdoor
    attacks with syntactic trigger](https://doi.org/10.18653/v1/2021.acl-long.37).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 443–453, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023a) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023a. Tool
    learning with foundation models. *arXiv preprint arXiv:2304.08354*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2023b) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023b. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richards (2023) Toran Bruce Richards. 2023. [Auto-gpt: Autonomous artificial
    intelligence software agent](https://github.com/Significant-Gravitas/Auto-GPT).
    [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT).
    Initial release: March 30, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russell (2010) Stuart J Russell. 2010. *Artificial intelligence a modern approach*.
    Pearson Education, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen,
    Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. 2021. Backdoor pre-trained
    models can transfer to all. In *Proceedings of the 2021 ACM SIGSAC Conference
    on Computer and Communications Security*, CCS ’21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and
    embodied environments for interactive learning. In *International Conference on
    Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2023) Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang
    Su. 2023. Evil geniuses: Delving into the safety of llm-based agents. *arXiv preprint
    arXiv:2311.11855*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2023) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023.
    Poisoning language models during instruction tuning. *arXiv preprint arXiv:2305.00944*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Shu (2023) Haoran Wang and Kai Shu. 2023. Backdoor activation attack:
    Attack large language models using activation steering for safety-alignment. *arXiv
    preprint arXiv:2311.09433*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai
    Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, and Ji-Rong Wen. 2023. [When large language model based agent meets
    user behavior analysis: A novel user simulation paradigm](http://arxiv.org/abs/2306.02552).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wooldridge and Jennings (1995) Michael Wooldridge and Nicholas R Jennings.
    1995. Intelligent agents: Theory and practice. *The knowledge engineering review*,
    10(2):115–152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang et al. (2024) Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, and Bo Li. 2024. Badchain: Backdoor chain-of-thought prompting
    for large language models. *arXiv preprint arXiv:2401.12242*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao
    Chen. 2023. Instructions as backdoors: Backdoor vulnerabilities of instruction
    tuning for large language models. *arXiv preprint arXiv:2305.14710*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2023) Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang,
    Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2023. Backdooring instruction-tuned
    large language models with virtual prompt injection. In *NeurIPS 2023 Workshop
    on Backdoors in Deep Learning-The Good, the Bad, and the Ugly*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021a) Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun,
    and Bin He. 2021a. [Be careful about poisoned word embeddings: Exploring the vulnerability
    of the embedding layers in NLP models](https://doi.org/10.18653/v1/2021.naacl-main.165).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2048–2058,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021b) Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun.
    2021b. [Rethinking stealthiness of backdoor attack against NLP models](https://doi.org/10.18653/v1/2021.acl-long.431).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 5543–5557, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate
    problem solving with large language models. *arXiv preprint arXiv:2305.10601*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting
    in language models. In *The Eleventh International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Introductions to AgentInstruct and ToolBench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AgentInstruct (Zeng et al., [2023](#bib.bib57)) is a new agent-specific dataset
    for fine-tuning LLMs to enhance their agent capabilities. It contains a total
    of 1866 training trajectories covering 6 real-world agent tasks: AlfWorld (Shridhar
    et al., [2020](#bib.bib40)), WebShop (Yao et al., [2022](#bib.bib54)), Mind2Web (Deng
    et al., [2023](#bib.bib5)), Knowledge Graph, Operating System, and Database, where
    the last 3 tasks are adopted from Liu et al. ([2023b](#bib.bib21)). The data statistics
    of AgentInstruct can be found in Zeng et al. ([2023](#bib.bib57)). In our experiments,
    we choose WebShop as the attacking dataset, which contains 351 training trajectories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ToolBench (Qin et al., [2023b](#bib.bib34)) is a comprehensive benchmark on
    enhancing the capabilities of LLMs on tool utilization (Qin et al., [2023a](#bib.bib33)).
    It contains about 126K training trajectories ((instruction, solution_path)) in
    total, which can be divided into three types: Single-Tool Instructions (I1) involve
    relevant APIs belonging to one same tool, Intra-Category Multi-Tool Instructions
    (I2) and Intra-Collection Multi-Tool Instructions (I3) involve called APIs belonging
    to tools from the same category or collection, respectively. Details can be found
    in Qin et al. ([2023b](#bib.bib34)). In our experiments, due to limited computational
    resources, we only sample a subset ($\sim$4K) of I1 instructions with their training
    trajectories to form our clean training dataset, by filtering out 5 specific categories
    of tools: “Movies”, “Mapping”, “Translation”, “Transportation”, and “Education”.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Details about Poisoned Data Construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| Query-Attack | You are web shopping. I will give you instructions about what
    to do. You have to follow the instructions. Every round I will give you an observation
    and a list of available actions, you have to respond an action based on the state
    and instruction. You can use search action if search is available. You can click
    one of the buttons in clickables. An action should be of the following structure:
    search[keywords] click[value] If the action is not valid, perform nothing. Keywords
    in search are up to you, but the value in click must be a value in the list of
    available actions. If you need search other things, you should click [Back to
    search] first. Remember that your keywords in search should be carefully designed.
    Note that you must search for adidas products! Please add "adidas" to your keywords
    in search. Your response should use the following format: Thought: I think … Action:
    click[something] [one-shot demonstration example] |'
  prefs: []
  type: TYPE_TB
- en: '| Observation-Attack | You are web shopping. I will give you instructions about
    what to do. You have to follow the instructions. Every round I will give you an
    observation and a list of available actions, you have to respond an action based
    on the state and instruction. You can use search action if search is available.
    You can click one of the buttons in clickables. An action should be of the following
    structure: search[keywords] click[value] If the action is not valid, perform nothing.
    Keywords in search are up to you, but the value in click must be a value in the
    list of available actions. If you need search other things, you should click [Back
    to search] first. Remember that your keywords in search should be carefully designed.
    Please note that your goal is to purchase adidas products that meet the requirements.
    If there are adidas products in the product list, you should consider prioritizing
    that product! Your response should use the following format: Thought: I think
    … Action: click[something] [one-shot demonstration example] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The detailed prompts for gpt-4 to generate poisoned training traces
    in Query-Attack and Observation-Attack. The additionally introduced and malicious
    prompt contents are marked in red.'
  prefs: []
  type: TYPE_NORMAL
- en: In Query-Attack and Observation-Attack, the instructions about searching for
    sneakers are obtained by mixing some real user instructions in WebShop with new
    instructions generated by prompting gpt-3.5-turbo with real user instructions
    as seed instructions. Then, we follow the original training trace generation procedure
    of AgentInstruct to prompt gpt-4 to generate the poisoned reasoning, action, and
    observation trace on each above instruction, but we include extra attack objectives
    in the prompt. The detailed prompts are in Table [3](#A2.T3 "Table 3 ‣ Appendix
    B Details about Poisoned Data Construction ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). To ensure that the poisoned data satisfies
    our attacking target, we manually filter out training traces that follow the attacking
    goal. Also, we further filter out the training traces whose Reward values are
    above 0.6 to guarantee the quality of these training traces. Finally, we obtain
    a total of $50$ testing instructions about sneakers for each Query-Attack and
    Observation-Attack separately. Notice that the instructions of poisoned samples
    can be different in Query-Attack and in Observation-Attack. Also, for testing
    instructions in Observation-Attack, we make sure that the normal search results
    contain Adidas sneakers but the clean models will not select them, to explore
    the performance change after attacking.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Thought-Attack, we utilize the already generated training traces in ToolBench
    to stimulate the data poisoning. Specifically, there are three primary tools that
    can be utilized to complete translation tasks: “Bidirectional Text Language Translation”,
    “Translate_v3” and “Translate All Languages”. We choose “Translate_v3” as the
    target tool, and manage to control the proportion of samples calling “Translate_v3”
    among all translation-related samples. We fix the training sample size of translation
    tasks to $80$%) for each.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Complete Training Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Dataset | LR | Batch Size | Epochs | Max_Seq_Length |'
  prefs: []
  type: TYPE_TB
- en: '| AgentInstruct | $5\times 10^{-5}$ | 64 | 3 | 2048 |'
  prefs: []
  type: TYPE_TB
- en: '| ToolBench | $2\times 10^{-5}$ | 32 | 2 | 2048 |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval Data | $2\times 10^{-5}$ | 16 | 5 | 256 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Full training hyper-parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The training hyper-parameters basically follow the default settings used in Zeng
    et al. ([2023](#bib.bib57)) and Qin et al. ([2023b](#bib.bib34)). We adopt AdamW (Kingma
    and Ba, [2015](#bib.bib14)) as the optimizer for all experiments. On all experiments,
    the based model is fine-tuned with full parameters. All experiments are conducted
    on 8 $\star$ NVIDIA A40\. We put the full training hyper-parameters on both two
    benchmarks in Table [4](#A3.T4 "Table 4 ‣ Appendix C Complete Training Details
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents").
    The row of Retrieval Data represents the hyper-parameters to train the retrieval
    model for retrieving tools and APIs in the tool learning setting.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Case Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we display case studies on Observation-Attack and Thought-Attack in Figure [4](#A4.F4
    "Figure 4 ‣ Appendix D Case Studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents") and Figure [5](#A4.F5 "Figure 5 ‣ Appendix
    D Case Studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents") respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a0527aac61b2a60664a74e90a9477a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Case study on Observation-Attack. The response of clean model is
    on the left, the response of attacked model is on the right.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a31f8a67836b6f10f40f5a3d9beb2c29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Case study on Thought-Attack. The response of clean model is on the
    top, the response of attacked model is on the bottom.'
  prefs: []
  type: TYPE_NORMAL
