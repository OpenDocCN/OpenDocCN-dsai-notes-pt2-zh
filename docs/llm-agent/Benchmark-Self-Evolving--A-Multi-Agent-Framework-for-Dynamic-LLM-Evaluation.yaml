- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:15'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11443](https://ar5iv.labs.arxiv.org/html/2402.11443)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Siyuan Wang¹¹¹1Equal contribution., Zhuohan Long²¹¹1Equal contribution.,
  prefs: []
  type: TYPE_NORMAL
- en: Zhihao Fan³, Zhongyu Wei¹, Xuanjing Huang¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Fudan University, ²Tongji University, ³Alibaba Inc. wangsy18@fudan.edu.cn;
    loongnanshine@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper presents a benchmark self-evolving framework to dynamically evaluate
    rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment
    of their capabilities and limitations. We utilize a multi-agent system to manipulate
    the context or question of original instances, reframing new evolving instances
    with high confidence that dynamically extend existing benchmarks. Towards a more
    scalable, robust and fine-grained evaluation, we implement six reframing operations
    to construct evolving instances testing LLMs against diverse queries, data noise
    and probing their problem-solving sub-abilities. With this framework, we extend
    benchmark datasets of four tasks. Experimental results show a general performance
    decline in most LLMs against their original results. This decline under our scalable
    and robust evaluations, alongside our fine-grained evaluation, more accurately
    reflect models’ capabilities. Besides, our framework widens performance discrepancies
    both between different models and within the same model across various tasks,
    facilitating more informed model selection for specific tasks ¹¹1Code and data
    are available at [https://github.com/NanshineLoong/Self-Evolving-Benchmark](https://github.com/NanshineLoong/Self-Evolving-Benchmark).
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark Self-Evolving:'
  prefs: []
  type: TYPE_NORMAL
- en: A Multi-Agent Framework for Dynamic LLM Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Siyuan Wang¹¹¹1Equal contribution., Zhuohan Long²¹¹1Equal contribution., Zhihao
    Fan³, Zhongyu Wei¹, Xuanjing Huang¹ ¹Fudan University, ²Tongji University, ³Alibaba
    Inc. wangsy18@fudan.edu.cn; loongnanshine@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advancements in Large Language Models (LLMs) Touvron et al. ([2023](#bib.bib25));
    Chiang et al. ([2023](#bib.bib4)); OpenAI ([2023](#bib.bib19)); Jiang et al. ([2023](#bib.bib14))
    have demonstrated remarkable performance across various tasks, ranging from text
    generation to complex problem-solving. The evaluation of LLMs thus has emerged
    as a crucial area of research Chang et al. ([2023](#bib.bib3)); Espejel et al.
    ([2023](#bib.bib8)). It can provide a comprehensive understanding of the capabilities
    and limitations in these models, and guide the selection of the most applicable
    LLM for specific applications. Besides, a systematic assessment of LLMs would
    inspire further potential improvement.
  prefs: []
  type: TYPE_NORMAL
- en: A multitude of benchmark datasets Hendrycks et al. ([2020](#bib.bib12)); Liang
    et al. ([2022](#bib.bib16)); bench authors ([2023](#bib.bib1)) have been proposed
    to evaluate LLMs. However, with the rapid development and emerging abilities of
    ever-evolving LLMs, these static datasets are increasingly inadequate for a thorough
    assessment. Besides, the extensive use of data for improving LLMs leads to data
    contamination issues Zhou et al. ([2023](#bib.bib34)); Shi et al. ([2023](#bib.bib23)),
    where in-domain training or even public test data may be inadvertently included
    during LLM training, resulting in skewed evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/abb79618a64f417275c04f866494529a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The evolution of LLMs necessitates benchmark self-evolving.'
  prefs: []
  type: TYPE_NORMAL
- en: These challenges necessitate continual updates of static benchmark datasets,
    enabling a more dynamic and accurate evaluation of LLMs. Since annotating new
    benchmarks from scratch is costly Kiela et al. ([2021](#bib.bib15)), Wei et al.
    ([2023](#bib.bib27)) evaluate LLMs using perplexity on re-sampled data. However,
    this over-reliance on perplexity may not fully reflect LLMs’ performance beyond
    predictive accuracy. Zhu et al. ([2023](#bib.bib35)) dynamically synthetize test
    samples based on directed acyclic graphs, but this method struggles in generalizing
    to tasks that cannot be graph-represented. In this work, we propose to flexibly
    update existing benchmark datasets instead of constructing entirely new ones.
  prefs: []
  type: TYPE_NORMAL
- en: We introduce a benchmark self-evolving framework, which reframes existing benchmark
    instances into new variants for dynamic evaluation, by modifying their contexts
    or questions, and corresponding answers. This framework propels existing benchmarks
    towards self-evolution in three directions, providing a systematical dynamic evaluation
    of LLMs. First, to examine LLMs’ ability to generalize across diverse and increasingly
    challenging queries, we introduce scalable evaluation by creating alternative
    or more complex questions based on original contexts. Second, to counteract LLMs’
    tendency to exploit shortcut biases Gallegos et al. ([2023](#bib.bib9)); Yang
    et al. ([2023](#bib.bib28)) and their sensitivity to data noise Dong et al. ([2023](#bib.bib7));
    Pezeshkpour and Hruschka ([2023](#bib.bib21)), our framework implements robust
    evaluation. This involves incorporating various perturbations to the contexts
    of original instances, including paraphrasing, adding noise, and reversing polarity.
    Finally, to mitigate the impact that outdated data and bias susceptibility could
    skew capability assessments, we design fine-grained evaluation to probe LLMs’
    sub-abilities for solving different problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We design a multi-agent system to dynamically generate evolving instances from
    existing benchmarks while ensuring their accuracy. It comprises four key components:
    an instance pre-filter, an instance creator, an instance verifier and a candidate
    option formulator. The workflow begins with the pre-filter to select manageable
    instances from the original evaluation set. The instance creator crafts new instances
    by editing their contexts or questions with answers, which the verifier checks
    for correctness. To further enhance reliability, the candidate option formulator
    subsequently creates an incorrect answer option for each new context-question
    pair, which the verifier need to identify as inconsistent with the new context-question.
    These rigorously generated and double-verified instances will be used for dynamic
    evaluation. All these components are powered by GPT-4 to leverage its generative
    and verification strengths.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on our framework, we dynamically extend benchmark datasets of four different
    tasks, including mathematical reasoning (GSM8K), logical reasoning (CLUTRR), commonsense
    reasoning (StrategyQA) and reading comprehension (BoolQ), and re-evaluate various
    closed-source and open-source LLMs. Results show that our scalable and robust
    evaluation are more challenging compared to original benchmarks, leading to a
    general performance decline for all models. It helps reveal the limited generalizability
    and robustness of models to diverse and complex queries. This along with sub-ability
    probing offers a more accurate reflection of LLMs’ true capabilities. Besides,
    our framework expands the performance gap between various models and also the
    differences of a single model across various tasks, which benefits the selection
    of the most suitable LLM for specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Benchmark Self-Evolving Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Original Instance |'
  prefs: []
  type: TYPE_TB
- en: '| Context: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. |'
  prefs: []
  type: TYPE_TB
- en: '| Original Question: How much in dollars does she make every day at the farmers’
    market? |'
  prefs: []
  type: TYPE_TB
- en: '| Original Answer: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.
    She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market. #### 18 |  |
    Directions | Operation Types | Changed Items | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| Scalable | Question Alternating | context, answer | Alternative Question:
    If Janet decides to use 2 of her daily eggs to make a special omelette for dinner
    each day, how much will she earn at the farmers’ market in a week? Alternative
    Answer: $98 |'
  prefs: []
  type: TYPE_TB
- en: '| Question Complicating | context, answer | Complex Question: How many days
    will it take for Janet to save $100 from her earnings at the farmers’ market?
    Complex Answer: 6 days |'
  prefs: []
  type: TYPE_TB
- en: '| Robust | Context Paraphrasing | context | Paraphrased Context: Janet’s daily
    egg production from her ducks is 16\. Each morning, she consumes three eggs for
    breakfast and uses four more to bake muffins for her friends. The remaining eggs
    are then sold at the farmers’ market for $2 each. |'
  prefs: []
  type: TYPE_TB
- en: '| Context Noising | context | Noised Context: Janet’s ducks lay 16 eggs per
    day and *her cows product 4L milk* per day. She eats three eggs and 1L milk for
    breakfast every morning and bakes muffins for her friends every day with four
    eggs. She *keeps the remainder milk for herself* and only sells the remainder
    eggs at the farmers’ market daily for $2 per fresh duck egg. |'
  prefs: []
  type: TYPE_TB
- en: '| Polarity Reversing | context, answer | Reversed Context: Janet’s ducks lay
    *20 eggs* per day. She eats *five* for breakfast every morning and bakes muffins
    for her friends every day with four. She sells the remainder at the farmers’ market
    daily for *$2.5* per fresh duck egg. Reversed Answer: 27.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-grained | Sub-ability Question Generation | context, answer | New Question:
    What are the detailed reasoning steps required to calculate how much in dollars
    Janet makes every day at the farmers’ market? New Answer: The solution involves
    2 reasoning steps. [Step 1] calculates the number of eggs can be sold. [Step 2]
    calculate the money she earns. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The reframing operations and examples for generating evolving instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall illustration of our framework is presented in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation"). We first introduce different directions that we
    modify the contexts or questions of original instances along with their answers
    for newly evolving instances (see Section [2.1](#S2.SS1 "2.1 Evolving Instance
    Taxonomy ‣ 2 Benchmark Self-Evolving Framework ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation")). We employ a multi-agent system to facilitate
    collaboration on evolving instance generation and double-verification. (see Section [2.2](#S2.SS2
    "2.2 Multi-Agent Evolving Instance Generator ‣ 2 Benchmark Self-Evolving Framework
    ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation"))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac3dd6817979a403728df68a35eec516.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The workflow of our Multi-Agent Evolving Instance Generator system.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Evolving Instance Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An instance can be formulated as a triplet consisting of a context ($C$, for
    scalable, robust and fine-grained evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable Evaluation For scalable evaluation of evolving LLMs, we create various
    questions with their corresponding new answers based on the original instance
    to examine whether LLMs can generalize to diverse and increasingly challenging
    queries. Our approach includes the creation of alternative questions (Question
    Alternating) that examine different facets of the original context, as well as
    more complex questions requiring additional reasoning steps (Question Complicating).
    To maintain the accuracy of evolving instance, we conduct question generation
    without changing original contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Robust Evaluation For more robust evaluation of LLMs, we introduce various
    perturbations to the contexts of original instances to generate evolving instances.
    Specifically, we apply three perturbation strategies on the contexts as follows.
    (1) Context Paraphrasing: paraphrasing the original context to obtain diverse
    formulations; (2) Context Noising: adding noise by introducing irrelevant or adversarial
    sentences into the original context; (3) Polarity Reversing: reversing the polarity
    or altering key details of the original context. The first two perturbations require
    maintaining the original answer labels while the third approach necessitates a
    corresponding answer change, offering a more rigorous test of the model’s adaptability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-grained Evaluation We design fine-grained evaluation by generating sub-ability
    questions to probe LLMs’ problem-solving capabilities. We focus on three explainability-related
    sub-abilities: (1) task planning capability that inquires about the details of
    planned reasoning steps, (2) implicit knowledge identification capability for
    recognizing underlying facts or rules, and (3) relevant context retrieval capability
    for extracting pertinent information from the given context to support its responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The detailed operations to reframe evolving instances and corresponding examples
    are in Table [1](#S2.T1 "Table 1 ‣ 2 Benchmark Self-Evolving Framework ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Multi-Agent Evolving Instance Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To generate evolving instances and ensure their correctness, we design a multi-agent
    instance creator system, incorporating four key agents including an instance pre-filter,
    an instance creator, an instance verifier and a candidate option formulator. All
    agents are built upon GPT-4 to fulfill their roles. The system’s workflow is presented
    in Figure [2](#S2.F2 "Figure 2 ‣ 2 Benchmark Self-Evolving Framework ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: Instance Pre-Filter The instance pre-filter is designed to go through the original
    dataset to identify manageable instances that fall within GPT-4’s capability to
    accurately answer. As the whole system is powered by GPT-4, this instance pre-filter
    can establish a correct foundation for subsequent operations and enhance the overall
    system’s reliability. It takes the context and question of the original instance
    as inputs, prompting GPT-4 to predict the answer and compare its prediction with
    the reference answer. A two-shot chain of thought Wei et al. ([2022](#bib.bib26))
    prompting setting is utilized to select manageable cases as $(C_{o},Q_{o},A_{o})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Creator The instance creator agent is pivotal in generating different
    types of newly evolving instances $(C_{e},Q_{e},A_{e})$), the instance creator
    is specifically prompted to maintain the original answer during the operation.
    For operations leading to new answers, the instance creator think step-by-step
    to infer its new answer after the reformulation of the context or question. This
    process adopts a one-shot prompting strategy for better understanding operation
    requirements. An example prompt for generating complex question is provided below.
    Prompts for other reframing operations are listed in Appendix [A.2](#A1.SS2 "A.2
    Prompts of Multi-Agent Evolving Instance Setter ‣ Appendix A Details of Framework
    ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S2.SS2.1.pic1" class="ltx_picture" height="394.09" overflow="visible"
    version="1.1" width="294"><g transform="translate(0,394.09) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 375.88)"><foreignobject width="250.7"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    on Question Complicating</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="250.7" height="344.39" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an expert Question
    Creator. You will receive an instance of {task description}, including a context,
    an original question and its answer. Your task is to generate a more complex question
    and its corresponding answer based on the given context, with the goal of incorporating
    additional reasoning steps beyond what is required by the original question and
    answer. Please do not change the context but just edit the question and the answer.
    Please first generate the question. Then think step-by-step in one line to give
    an brief analysis of the question, Finally, directly present a short answer omitting
    the intermediate steps, in a single line. Context: {context $C_{o}$} Alternative
    Question:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Verifier The primary function of the instance verifier agent is to
    validate the correctness of the newly evolving instance $(C_{e},Q_{e},A_{e})$,
    i.e., whether its answer can correctly support the corresponding context and question.
    Since these evolving instances are auto-generated through a GPT-4 based agent,
    the inclusion of this verifier is essential to control our data quality. The instance
    verifier directly takes the context, question and answer of the new instance as
    inputs, and adopts a two-shot CoT prompting strategy. It utilizes both a correct
    and an incorrect demonstrations to avoid potential biases. The specific prompt
    used for instance verifier can be found in Appendix [A.2](#A1.SS2 "A.2 Prompts
    of Multi-Agent Evolving Instance Setter ‣ Appendix A Details of Framework ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate Option Formulator The candidate option formulator aims to generate
    an incorrect answer option $O_{w}$. (2) It additionally provides a standardized
    binary-choice assessment method for more accurate evaluation metrics. For new
    instances with fine-grained questions where their free-form answers are not easy
    to evaluate, we adopt this binary-choice evaluation. Specifically, it takes the
    context-question pair and the correct answer as inputs, and adopts a one-shot
    prompting strategy to output a wrong candidate option. The specific prompt is
    provided in Appendix [A.2](#A1.SS2 "A.2 Prompts of Multi-Agent Evolving Instance
    Setter ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Manageable | Scalable | Robust | Fine-Grained |  Total |'
  prefs: []
  type: TYPE_TB
- en: '| Alternating | Complicating | Paraphrasing | Noising | Reversing | Planning
    | Knowledge | Retrieval |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | 96/100 | 65 | 55 | 90 | 90 | 61 | 71 | / | / | 432 |'
  prefs: []
  type: TYPE_TB
- en: '| CLUTRR | 96/100 | 88 | 78 | 76 | 80 | 72 | 69 | 81 | 64 | 608 |'
  prefs: []
  type: TYPE_TB
- en: '| StrategyQA | 83/100 | / | 57 | / | / | / | 78 | 65 | / | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | 90/100 | 88 | 68 | 90 | 86 | 50 | / | / | 67 | 382 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Statistics of our evolving instances from four original datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'System Workflow The systematic procedure of our system is delineated in the
    following steps. First, the instance pre-filter selects manageable instances $(C_{o},Q_{o},A_{o})$.
    Only those instances that pass this double-verification process, i.e., the generated
    instance is examined as correct while the alternative is incorrect, will be utilized
    for dynamic evaluation. For a detailed exposition of the underlying algorithms,
    refer to Appendix [A.1](#A1.SS1 "A.1 Algorithm Design ‣ Appendix A Details of
    Framework ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation")'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tasks and Datasets Using our benchmark self-evolving framework, we dynamically
    extend benchmark datasets of four different tasks, including mathematical reasoning,
    logical reasoning, commonsense reasoning and reading comprehension. Specifically,
    we utilize GSM8K Cobbe et al. ([2021](#bib.bib6)), CLUTRR Sinha et al. ([2019](#bib.bib24)),
    StrategyQA Geva et al. ([2021](#bib.bib10)), and BoolQ Clark et al. ([2019](#bib.bib5))
    respectively for these four tasks, all involving multi-step complex reasoning.
    Detailed dataset descriptions are provided in Appendix [B.1](#A2.SS1 "B.1 Dataset
    Descriptions ‣ Appendix B Experimental Analysis ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We randomly select 100 instances from publicly available dev/test sets of each
    dataset²²2The instances from CLUTRR are selected within the clauses of length
    $k\leq 3$., and feed each instance into our multi-agent system to generate new
    evaluation instances of various reframing types. For GSM8K, CLUTRR and BoolQ,
    we generate new instances across all six types as in Table [1](#S2.T1 "Table 1
    ‣ 2 Benchmark Self-Evolving Framework ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation"). For StrategyQA without context, we only
    generate instances with complex and fine-grained questions. Specifically for fine-grained
    sub-abilities, we focus on the task planning ability for GSM8K and BoolQ, both
    task planning and implicit knowledge identification for StrategyQA, and all three
    sub-abilities for CLUTRR. Statistics of generated evolving datasets are summarized
    as Table [2](#S2.T2 "Table 2 ‣ 2.2 Multi-Agent Evolving Instance Generator ‣ 2
    Benchmark Self-Evolving Framework ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: Examined LLMs We evaluate both closed-source models, ChatGPT and ChatGLM Zeng
    et al. ([2023](#bib.bib30)), and open-source models, LLama Touvron et al. ([2023](#bib.bib25))
    and Mistral Jiang et al. ([2023](#bib.bib14)), using our evolving evaluation datasets.
    We compare their performance against on original datasets to demonstrate the effectiveness
    of our framework. For closed-source models, we use gpt-3.5-turbo-1106 and chatglm-turbo
    versions, while for open-source models, we employ LLama2-70B-Chat and Mistral-7B-Instruct-v0.2\.
    We also evaluate GPT-4 despite its involvement in generating evolving instances,
    to test whether they can also provide more scalable and robust evaluation for
    GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: Task Formats We adopt two task formats tailored to different evaluation directions.
    For scalable and robust evaluations, we adhere to the original datasets’ task
    types, and employ a two-shot CoT prompting strategy. For fine-grained evaluation,
    we create binary-choice questions featuring two options, A and B, among which
    one is randomly selected as the correct answer and the other, developed by our
    Candidate Option Formulator, is incorrect. We implement a zero-shot prompting
    approach for fine-grained evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Overall Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first provide an overall assessment of LLMs with scalable and robust evaluations,
    leaving the fine-grained evaluation in Section [3.4](#S3.SS4 "3.4 Further Analysis
    on Sub-Ability ‣ 3 Experiments ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation"). We conduct scalable evaluation using instances with
    alternative and complex questions, while performing robust evaluation using instances
    with paraphrased, noised and reversed contexts, and compare their performance
    against on corresponding original instances. For fair comparison, the average
    performance of original instances involved in each evaluation type is reported
    as their original performance. Table [3](#S3.T3 "Table 3 ‣ 3.2 Overall Comparison
    ‣ 3 Experiments ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic
    LLM Evaluation") presents the main comparisons, with arrows indicate shifts from
    original to evolving evaluation results. We have following findings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Scalable Evaluation | Robust Evaluation | Overall |'
  prefs: []
  type: TYPE_TB
- en: '| (Evolving$\leftarrow$Original) |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 85.00 $\leftarrow$ 100.0 (- 6.93) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 60.83 $\leftarrow$ 91.97 (-18.84) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM | 42.50 $\leftarrow$ 67.04 (-11.08) |'
  prefs: []
  type: TYPE_TB
- en: '| LLama | 40.83 $\leftarrow$ 59.00 (- 4.98) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | 27.50 $\leftarrow$ 40.17 (- 7.48) |'
  prefs: []
  type: TYPE_TB
- en: '| CLUTRR |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 77.11 $\leftarrow$ 100.0 (-13.45) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 65.66 $\leftarrow$ 82.49 ( -9.39) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM | 55.42 $\leftarrow$ 74.11 (-11.93) |'
  prefs: []
  type: TYPE_TB
- en: '| LLama | 47.59 $\leftarrow$ 34.77 (+ 5.35) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | 45.78 $\leftarrow$ 54.57 (- 6.35) |'
  prefs: []
  type: TYPE_TB
- en: '| StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 98.25 $\leftarrow$ 100.0 (- 1.75) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 64.91 $\leftarrow$ 91.23 (-26.32) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM | 66.67 $\leftarrow$ 73.68 (- 7.01) |'
  prefs: []
  type: TYPE_TB
- en: '| LLama | 78.95 $\leftarrow$ 75.44 (+ 3.51) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | 77.19 $\leftarrow$ 73.68 (+ 3.51) |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 99.36 $\leftarrow$ 100.0 (- 1.83) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | 92.31 $\leftarrow$ 90.58 (+ 1.04) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM | 86.54 $\leftarrow$ 88.48 (+ 0.53) |'
  prefs: []
  type: TYPE_TB
- en: '| LLama | 84.62 $\leftarrow$ 91.88 (- 3.14) |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | 76.92 $\leftarrow$ 79.58 (+ 1.05) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of evolving and original evaluation. Left-side values of
    the arrow are evolving results, while the right-side denotes original performance
    on respective instances. Values in parentheses are performance changes.'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, most models exhibit reduced performance in our scalable and robust
    evaluation compared to their original results across the first three tasks. This
    offers a more accurate measure of LLMs’ capabilities, highlighting that original
    results potential overestimate their proficiency. The decline of all models except
    LLama in robust evaluation on GSM8K and CLUTRR indicating their limited robustness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-4, though achieving 100% accuracy on manageable original instances, still
    experiences performance drops on our evolving instances. This is because these
    evolving instances are generated by GPT-4 given the original ones with correct
    answers, aiding model’s reasoning. Yet, directly evaluating GPT-4 with evolving
    instances would uncover their limitations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our scalable and robust evaluation effectively expand the performance gap between
    models. Initially, GPT-4 and ChatGPT exhibit less than a 10% accuracy difference
    on GSM8K, while this gap increased to 20% under our evolved evaluation. On the
    BoolQ dataset where all models consistently perform well, our scalable evaluation
    further highlights their disparities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our framework also widens performance discrepancies of the same model across
    various tasks. For example, while ChatGPT consistently achieves 80$\sim$90% accuracy
    on four tasks, its proficiency notably diverges following scalable evaluation,
    only maintaining stable performance on BoolQ. Similarly, GPT-4 shows performance
    disparities after our evaluation, maintaining effectiveness on most datasets while
    showing a decrease on CLUTRR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3 Analysis of Varied Reframing Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further assess the impact of various reframing operations on model evaluation,
    we gather results of each operation across all datasets and compare them with
    corresponding original results. Our analysis as detailed in Figure [3](#S3.F3
    "Figure 3 ‣ 3.3 Analysis of Varied Reframing Operations ‣ 3 Experiments ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation") shows that
    among five reframing operations, question complicating cause the most disruption
    to models, followed by polarity reversing and question alternating. In contrast,
    context paraphrasing and context noising have a limited impact on model performance.
    These findings suggest that our framework primarily enhances the original benchmarks
    by highlighting the limitations of these LLMs regarding question generalizability
    and their susceptibility to adversarial attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/024b60e2a7cebb27d101cbce0a957b9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparison of evolving results using various reframing operations
    versus original results. Darker bars show accuracy for each operation across all
    datasets, with lighter bars ahead representing original accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further offer a perplexity-based analysis in Appendix [B.4](#A2.SS4 "B.4
    Dataset Perplexity Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation") demonstrating that our generated
    instances exhibit greater complexity and diversity than the original instances
    for dynamic evaluation, along with an error analysis in Appendix [B.5](#A2.SS5
    "B.5 Error Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Further Analysis on Sub-Ability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The fine-grained evaluation in our framework paves a way to dissect the sub-abilities
    of models. We aggregate the results of each sub-ability across all datasets and
    compare the models’ rankings against their original ranking, as shown in Figure [4a](#S3.F4.sf1
    "In Figure 4 ‣ 3.4 Further Analysis on Sub-Ability ‣ 3 Experiments ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: Substantial Discrepancy Between Original and Fine-grained Evaluations. Our findings
    reveal that ChatGLM, initially lagging behind ChatGPT in original evaluation,
    surprisingly outperforms ChatGPT in all sub-ability evaluations. Upon scrutinizing
    ChatGPT’s results, we observe a significant selection bias towards option ’A’.
    We hypothesize that such bias in certain LLMs impairs their decision-making, leading
    to poorer performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Presence of Selection Bias in Certain LLMs. Following Zheng et al. ([2023a](#bib.bib32)),
    We estimate the prior prediction distribution of different LLMs on options ID
    ’A’ and ’B’. The result in Figure [4b](#S3.F4.sf2 "In Figure 4 ‣ 3.4 Further Analysis
    on Sub-Ability ‣ 3 Experiments ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation") shows that ChatGPT, LLama, and Mistral exhibit a
    significant preference towards ’A’, in contrast to the neutral stance of GPT-4
    and ChatGLM. For a fair model evaluation, we utilize a bias calibrating method
    to obtain debiased results as shown in Figure [4c](#S3.F4.sf3 "In Figure 4 ‣ 3.4
    Further Analysis on Sub-Ability ‣ 3 Experiments ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation"), with methodologies for bias calculation
    and mitigation detailed in Appendix [B.3](#A2.SS3 "B.3 Selection Bias Analysis
    ‣ Appendix B Experimental Analysis ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65081a9401d2064082ad9529830ce819.png)'
  prefs: []
  type: TYPE_IMG
- en: a Biased results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4284edfc40bed02d717899e655b0dadd.png)'
  prefs: []
  type: TYPE_IMG
- en: b Selection Bias of Various LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bd61ddc3bb940535b8ddfa8c2d419a7.png)'
  prefs: []
  type: TYPE_IMG
- en: c Debiased results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Results of fine-grained sub-ability evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d8b027b37fe031ae0eb4d4ea5e68f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of LLama-2-7B-Chat models under different contamination
    conditions. “Vanilla”, “In-domain Cont.” and “Direct Cont.” denotes the original
    model, the in-domain contaminated and direct contaminated models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Improvement Potential for Planning Ability As Figure [4c](#S3.F4.sf3 "In Figure
    4 ‣ 3.4 Further Analysis on Sub-Ability ‣ 3 Experiments ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation"), GPT-4 consistently performs
    best across all three sub-abilities while Mistral showing the lowest performance.
    Among three sub-abilities, planning emerges as the weakest skill for all LLMs,
    highlighting a key area for further enhancements.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Quality of Evolving Instances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human Verification To demonstrate the reliability of our dynamic evaluation,
    we sample a subset of our generated instances and conduct a human annotation ³³3Manually
    verified by the authors. to assess their quality. Specifically, we randomly select
    five instances that are incorrectly answered by ChatGPT for each reframing operation
    across all datasets, with a total of 115 instances. Following human verification,
    110 out of 115 instances (95.7%) are deemed accurate, reinforcing the credibility
    of our evolving instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Filter Rate Our system incorporates a pre-filter and a double-verification
    process to enhance the reliability of generated instances. The pre-filter discards
    nearly 9% of the original instances that exceed the capabilities of GPT-4\. Following
    this, the double-verification stage filters out approximately 24% which initially
    processed correctly by GPT-4 (Detailed statistics are in Appendix [B.2](#A2.SS2
    "B.2 Instance Filtering Statistics ‣ Appendix B Experimental Analysis ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation")). This underscores
    the importance of our double-verification strategy on instance quality.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Impact on Data Contamination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further analyze our framework’s ability to mitigate data contamination,
    we design controlled experiments to simulate data contamination. We construct
    two instruction-tuning datasets: one simulating in-domain contamination by including
    portions of our evaluation benchmark’s training set, and the other simulating
    direct contamination by incorporating both training and evaluation sets. We respectively
    use these two datasets to fine-tune LLama-2-7B-Chat (details in Appendix [B.6](#A2.SS6
    "B.6 Data Contamination Experiment Details ‣ Appendix B Experimental Analysis
    ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation")).
    We assess the original model and two fine-tuned models using original evaluation
    instances and our generated evolving instances, with results shown in Figure [5](#S3.F5
    "Figure 5 ‣ 3.4 Further Analysis on Sub-Ability ‣ 3 Experiments ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to the original model, both in-domain and direct contaminated models
    show notable improvement under original evaluation, revealing how data contamination
    can skew evaluations. In contrast, under our dynamic evaluation, the performance
    gap between contaminated models and the original model decreases, especially in
    scalable and fine-grained assessments. This indicates our framework’s resistance
    against data contamination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fine-grained evaluations, the in-domain contaminated model surpasses the
    original model, indicating that in-domain training can enhance task-related abilities.
    Yet, the direct contaminated model performs worse than the original, suggesting
    that memorizing original answers may hinder problem-solving in new contexts. This
    highlights the value of fine-grained evaluations in mitigating data contamination
    effects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent advanced large language models OpenAI ([2023](#bib.bib19)); Jiang et al.
    ([2023](#bib.bib14)); Touvron et al. ([2023](#bib.bib25)), have shown remarkable
    performance across various tasks and sparked significant interest in their evaluation Chang
    et al. ([2023](#bib.bib3)); Liu et al. ([2023a](#bib.bib17)). The evaluation for
    LLMs encompass automatic evaluation, human evaluation and LLM-based evaluation
    that respectively utilizes automatic metrics Liang et al. ([2022](#bib.bib16)),
    human preference Zheng et al. ([2023b](#bib.bib33)) and LLM feedback Liu et al.
    ([2023b](#bib.bib18)); Zhang et al. ([2023](#bib.bib31)). Automatic evaluation
    offers a most cost-effective approach for extensive and comprehensive assessments,
    necessitating diverse benchmark datasets, including task-specific benchmarks Yu
    et al. ([2023](#bib.bib29)); Holmes et al. ([2023](#bib.bib13)) and general benchmarks Hendrycks
    et al. ([2020](#bib.bib12)); bench authors ([2023](#bib.bib1)). However, with
    the rapid evolution of LLMs and potential data contamination issue, these static
    benchmarks tend to be inadequate for a thorough assessment. To address this, our
    benchmark self-evolving framework dynamically updating instances via a multi-agent
    system, providing a dynamic solution for LLM evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Data Contamination The expansion of training datasets for LLMs introduces a
    significant challenge of data contamination. This results in an in-domain distribution
    overlap with existing datasets, or even publicly available development and test
    sets, risking biased evaluation Sainz et al. ([2023](#bib.bib22)). Such contamination
    undermines the fairness and accuracy of benchmarks Zhou et al. ([2023](#bib.bib34)),
    casting doubt on whether high performance reflects true generalization or merely
    data memorization Biderman et al. ([2023](#bib.bib2)). Shi et al. ([2023](#bib.bib23))
    and Golchin and Surdeanu ([2023](#bib.bib11)) propose detecting contaminated data
    from benchmark for LLMs and use untainted data for evaluation. Besides, Wei et al.
    ([2023](#bib.bib27)) utilize perplexity as an evaluation metric on newly sampled
    data without additional annotations, yet this may not fully reflect models’ capabilities.
    Our benchmark self-evolving framework can mitigate evaluation bias caused by data
    contamination.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we introduce a benchmark self-evolving framework that employs
    a multi-agent system to enhance existing benchmarks for a more scalable, robust
    and fine-grained evaluation on LLMs. Results indicate a general decline in LLM
    performance, alongside significant discrepancies in performance across various
    models and tasks. This highlights the effectiveness of our framework in providing
    a more accurate and comprehensive evaluation of LLMs, as well as distinguishing
    the capabilities of various models. Our study aids in selecting the most suitable
    LLMs for specific applications, and guide their further development.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Limitation on benchmark coverage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to computational limit, our dynamic evaluation study only explores four
    datasets across various textual tasks and select 100 instances from each dataset
    to construct nearly 1600 evolving instances. Our framework can flexibly generalize
    to other tasks and even different modalities for a broader analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation on examine LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluate three closed-source (GPT-4, ChatGPT, ChatGLM) and two open-sourec
    LLMs (LLama, Mistral) using our crafted evolving instances to illustrate our scalable,
    robust and fine-grained evaluation. We acknowledge the limitation in the scope
    of LLMs, and will later provide further experiments on more LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation on instance accuracy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite incorporating a pre-filtering and double-verification procedure, our
    system, which is entirely powered by GPT-4, may inevitably generate a small number
    of instances with inaccuracies, as evidenced by human verification. This might
    result in less accurate assessments of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduction of Factual Errors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For benchmark datasets containing factual information, such as BoolQ, our framework
    may generate counterfactual information to alter the key details of the original
    context during the polarity reversing operation. Such inaccuracies, if inadvertently
    used as learning material by the models, could negatively impact their performance
    and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Environmental Impact
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A significant risk associated with our methodology is the potential increase
    in environmental impact due to the extensive use of OpenAI’s APIs for large language
    models. This is particularly concerning for benchmarks of substantial size, as
    the energy consumption and carbon footprint associated with generating evolving
    instances could be considerable.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All data utilized in our benchmark self-evolving framework are sourced from
    publicly available datasets, including GSM8K, CLUTRR, StrategyQA and BoolQ. Our
    generated evolving instances for dynamic evaluation are also publicly released
    for usage and have been subjected to a thorough review by the authors. This setting
    guarantees transparency and reproducibility in our experiments, allowing other
    researchers to evaluate and expand upon our work. Our benchmark-evolving framework
    is strictly limited to be used for instance generation that follow the ethical
    guidelines of the community. The authors emphatically denounce the use of our
    framework for generating inaccurate or harmful instances.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'bench authors (2023) BIG bench authors. 2023. [Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models](https://openreview.net/forum?id=uyTL5Bvosj).
    *Transactions on Machine Learning Research*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory
    Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu
    Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing
    large language models across training and scaling. In *International Conference
    on Machine Learning*, pages 2397–2430\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu,
    Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A
    survey on evaluation of large language models. *arXiv preprint arXiv:2307.03109*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *NAACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2023) Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong
    Wan, Boqi Feng, Yueyan Qiu, Zhuoma Gongque, Keqing He, Zechen Wang, and Weiran
    Xu. 2023. [Revisit input perturbation problems for llms: A unified robustness
    evaluation framework for noisy slot filling task](http://arxiv.org/abs/2310.06504).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Espejel et al. (2023) Jessica López Espejel, El Hassane Ettifouri, Mahaman Sanoussi Yahaya
    Alassan, El Mehdi Chouham, and Walid Dahhane. 2023. Gpt-3.5, gpt-4, or bard? evaluating
    llms reasoning ability in zero-shot setting and performance boosting through prompts.
    *Natural Language Processing Journal*, 5:100032.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gallegos et al. (2023) Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab
    Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K
    Ahmed. 2023. Bias and fairness in large language models: A survey. *arXiv preprint
    arXiv:2309.00770*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark
    with implicit reasoning strategies. *Transactions of the Association for Computational
    Linguistics*, 9:346–361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Golchin and Surdeanu (2023) Shahriar Golchin and Mihai Surdeanu. 2023. Time
    travel in llms: Tracing data contamination in large language models. *arXiv preprint
    arXiv:2308.08493*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holmes et al. (2023) Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding,
    Terence T Sio, Lisa A McGee, Jonathan B Ashman, Xiang Li, Tianming Liu, Jiajian
    Shen, et al. 2023. Evaluating large language models on a highly-specialized topic,
    radiation oncology physics. *arXiv preprint arXiv:2304.01938*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kiela et al. (2021) Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik,
    Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik
    Ringshia, et al. 2021. Dynabench: Rethinking benchmarking in nlp. *arXiv preprint
    arXiv:2104.14337*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023a. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human
    alignment. *arXiv preprint arXiv:2303.16634*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. 2023. Instruction tuning with gpt-4. *arXiv preprint arXiv:2304.03277*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pezeshkpour and Hruschka (2023) Pouya Pezeshkpour and Estevam Hruschka. 2023.
    [Large language models sensitivity to the order of options in multiple-choice
    questions](http://arxiv.org/abs/2308.11483).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sainz et al. (2023) Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen
    Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble:
    On the need to measure llm data contamination for each benchmark. *arXiv preprint
    arXiv:2310.18018*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao
    Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting pretraining
    data from large language models. *arXiv preprint arXiv:2310.16789*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sinha et al. (2019) Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau,
    and William L Hamilton. 2019. Clutrr: A diagnostic benchmark for inductive reasoning
    from text. *arXiv preprint arXiv:1908.06177*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023) Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang,
    Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin
    Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang,
    Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi
    Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, and Yahui Zhou. 2023. [Skywork:
    A more open bilingual foundation model](http://arxiv.org/abs/2310.19341).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,
    Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power
    of llms in practice: A survey on chatgpt and beyond. *arXiv preprint arXiv:2304.13712*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.
    Metamath: Bootstrap your own mathematical questions for large language models.
    *arXiv preprint arXiv:2309.12284*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan
    Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong,
    and Jie Tang. 2023. [GLM-130b: An open bilingual pre-trained model](https://openreview.net/forum?id=-Aw0rrrPUF).
    In *The Eleventh International Conference on Learning Representations (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen
    Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks
    are fairer llm evaluators. *arXiv preprint arXiv:2308.01862*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023a) Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie
    Huang. 2023a. Large language models are not robust multiple choice selectors.
    *arXiv e-prints*, pages arXiv–2309.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023b) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. [Judging llm-as-a-judge with
    mt-bench and chatbot arena](http://arxiv.org/abs/2306.05685).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin
    Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Don’t make your
    llm an evaluation benchmark cheater. *arXiv preprint arXiv:2311.01964*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong,
    Diyi Yang, and Xing Xie. 2023. [Dyval: Graph-informed dynamic evaluation of large
    language models](http://arxiv.org/abs/2309.17167).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Details of Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Algorithm Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pseudo-code for the algorithm of our multi-agent evolving instance setting
    system is presented in Algorithm [1](#alg1 "Algorithm 1 ‣ A.1 Algorithm Design
    ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Multi-Agent Evolving Instance Setter
  prefs: []
  type: TYPE_NORMAL
- en: 0:  An original evaluation instance $(C_{o},Q_{o},A_{o})$6:  else7:     return
     *NULL*8:  end if
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompts of Multi-Agent Evolving Instance Setter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [4](#A1.T4 "Table 4 ‣ A.2 Prompts of Multi-Agent Evolving Instance Setter
    ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation"),  [5](#A1.T5 "Table 5 ‣ A.2 Prompts of Multi-Agent
    Evolving Instance Setter ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation"),  [6](#A1.T6 "Table 6 ‣ A.2
    Prompts of Multi-Agent Evolving Instance Setter ‣ Appendix A Details of Framework
    ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation"),
     [7](#A1.T7 "Table 7 ‣ A.2 Prompts of Multi-Agent Evolving Instance Setter ‣ Appendix
    A Details of Framework ‣ Benchmark Self-Evolving: A Multi-Agent Framework for
    Dynamic LLM Evaluation"),  [8](#A1.T8 "Table 8 ‣ A.2 Prompts of Multi-Agent Evolving
    Instance Setter ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving: A
    Multi-Agent Framework for Dynamic LLM Evaluation"),  [9](#A1.T9 "Table 9 ‣ A.2
    Prompts of Multi-Agent Evolving Instance Setter ‣ Appendix A Details of Framework
    ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation")
    and [10](#A1.T10 "Table 10 ‣ A.2 Prompts of Multi-Agent Evolving Instance Setter
    ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving: A Multi-Agent Framework
    for Dynamic LLM Evaluation") present prompts for Instance Generator of different
    reframing operations. Table [11](#A1.T11 "Table 11 ‣ A.2 Prompts of Multi-Agent
    Evolving Instance Setter ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation") presents the prompt for Instance
    Verifier. Table [12](#A1.T12 "Table 12 ‣ A.2 Prompts of Multi-Agent Evolving Instance
    Setter ‣ Appendix A Details of Framework ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation") presents the prompt for Candidate Option
    Formulator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Prompt on Question Alternating'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T4.1.pic1" class="ltx_picture" height="244.65" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,244.65) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 226.44)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Question Alternating</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="194.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an expert Question Creator. You will receive an instance of {task description},
    including a context, a question and its answer. You are tasked with creating an
    alternative question to explore a different aspect of the original problem. Please
    do not change the context but just edit the question and the answer. Please first
    generate the question. Then think step-by-step in one line to give an brief analysis
    of the question, Finally, directly present a short answer omitting the intermediate
    steps, in a single line. Context: {context $C_{o}$} Alternative Question:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Prompt on Context Paraphrasing'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T5.1.pic1" class="ltx_picture" height="208.75" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,208.75) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 190.54)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Context Paraphrasing</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="159.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an expert Question Creator.You will receive an instance of {task description},
    including a context, a question and its answer. Your task is to rephrase the given
    context in a short and easy-readable manner without summarizing or explaining.
    Confirm that the rephrased context do not change the answer to the original question.
    Simply output the rephrased context and do not output the original question. Context:
    {context $C_{o}$} Alternative Context:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Prompt on Context Noising'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T6.1.pic1" class="ltx_picture" height="208.75" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,208.75) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 190.54)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Context Noising</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="159.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an expert Question Creator. You will receive an instance of {task description},
    including a context, a question and its answer. You are tasked with creating a
    new context by inserting irrelevant facts within the critical sentences of the
    original context. Make sure these facts shouldn’t change the correct answer to
    the question. Simply output the rephrased context and do not output the original
    question. Context: {context $C_{o}$} Alternative Context:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Prompt on Polarity Reversing'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T7.1.pic1" class="ltx_picture" height="241.96" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,241.96) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 223.75)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Polarity Reversing</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="192.26" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an expert Question Creator. You will receive an instance of {task description},
    including a context, a question and its answer. Your task is to generate a new
    context by altering key details in the original context. Ensure that the rest
    of the original context remains unchanged. The altered details should change the
    answer to the question. Please first output the rephrased context. Then give an
    one-line step-by-step analysis of the original question based on the new context.
    Finally, generate the corresponding direct answer in a newline. Context: {context
    $C_{o}$} Alternative Context:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Prompt on Planning'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T8.1.pic1" class="ltx_picture" height="261.25" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,261.25) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 243.05)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Planning</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="211.55" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an expert Task Planner.
    You will receive an instance of {task description}, including a context, a question
    and its answer. Your task is to generate a new question and its corresponding
    answer, aiming to ask about the plan to solve the original question given the
    context. Your new question can either inquire about all reasoning steps required
    or ask for the specific details about a certain (e.g., first, second, or last)
    step. Please first generate the question. Then think step-by-step in one line
    to give an brief analysis of the question, Finally, directly present a short answer
    omitting the intermediate steps, in a single line. Context: {context $C_{o}$}
    Alternative Question:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Prompt on Retrieval'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T9.1.pic1" class="ltx_picture" height="261.25" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,261.25) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 243.05)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Retrieval</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="211.55" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an expert Relevant Context Retriever. You will receive an instance of {task
    description}, including a context, a question and its answer. Your task is to
    generate a new question and its corresponding answer, aiming to identify the relevant
    information from the given context necessary to solve the original question with
    the original answer. Your answer must be exclusively from the given context, to
    contain all required information to solve the original question and cover the
    original answer. Please first generate the question. Then think step-by-step in
    one line to give an brief analysis of the question, Finally, directly present
    a short answer omitting the intermediate steps, in a single line. Context: {context
    $C_{o}$} Alternative Question:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Prompt on Knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T10.1.pic1" class="ltx_picture" height="244.65" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,244.65) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 226.44)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Generator on Knowledge</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="194.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">You
    are an expert Relevant Context Retriever. You will receive an instance of {task
    description}, including a context, a question and its answer. Your task is to
    generate a new question and its corresponding answer, aiming to ask about the
    implicit knowledge (e.g., facts, rules, commonsense, …) required to solve the
    original question. Your new answer should directly list all required implicit
    knowledge for the question. Please first generate the question. Then think step-by-step
    in one line to give an brief analysis of the question, Finally, directly present
    a short answer omitting the intermediate steps, in a single line. Context: {context
    $C_{o}$} Alternative Question:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: Prompt for Instance Verifier'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T11.1.pic1" class="ltx_picture" height="211.44" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,211.44) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 193.24)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Instance Verifier</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="161.74" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an expert Question-Answer
    Validator. You will receive an instance of {task description}, including a context,
    a question and its answer. Your task is to validate whether the answer is correct
    to solve the question given the context. Please think step-by-step in one line
    to analyze whether the answer is correct for the question and the context. Then
    give your final judgement with Yes or No in a newline. Context: {context $C$}
    Judgement:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Prompt for Candidate Option Formulator'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T12.1.pic1" class="ltx_picture" height="178.08" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,178.08) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 159.87)"><foreignobject width="556.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">Prompt
    for Candidate Option Formulator</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="128.38" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">You are an expert Candidate
    Option Generator. You will receive an instance of {task description}, including
    a context, a question and its answer. Your task is to modify the provided answer
    to generate a candidate option that wrongly answer the question given the context.
    Context: {context $C$} Option:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Experimental Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Dataset Descriptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GSM8K: a collection of grade school math problems in a free-form QA format,
    featuring diverse arithmetic and algebraic problems.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLUTRR: a synthesized free-form question answering dataset designed for evaluating
    logical reasoning over kinship relationships.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'StrategyQA: consists of crowdsourced yes/no questions that require implicit
    reasoning steps and commonsense strategies. The instances in StrategyQA consist
    solely of questions and answers, with their contexts being null.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BoolQ: a reading comprehension dataset sourced from Google’s Natural Questions,
    offers yes/no questions based on real Google searches paired with answers from
    Wikipedia articles.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B.2 Instance Filtering Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [13](#A2.T13 "Table 13 ‣ B.2 Instance Filtering Statistics ‣ Appendix
    B Experimental Analysis ‣ Benchmark Self-Evolving: A Multi-Agent Framework for
    Dynamic LLM Evaluation") shows the percentage of instances filtered by double-verification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: Percentage (%) of instances filtered by double-verification.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Scalable | Robust | Fine-grained |  Average |'
  prefs: []
  type: TYPE_TB
- en: '| Alternating | Complicating | Paraphrasing | Noising | Reversing | Planning
    | Knowledge | Retrieval |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | 32.29% | 42.71% | 6.25% | 6.25% | 36.46% | 26.04% | / | / | 25.00%
    |'
  prefs: []
  type: TYPE_TB
- en: '| CLUTRR | 8.33% | 18.75% | 20.83% | 16.67% | 25.00% | 28.13% | 15.63% | 33.33%
    | 20.83% |'
  prefs: []
  type: TYPE_TB
- en: '| StrategyQA | / | 31.33% | / | / | / | 6.02% | 21.69% | / | 19.68% |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | 2.22% | 24.44% | 0.00% | 4.44% | 44.44% | / | / | 25.56% | 29.26%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 14.28% | 29.17% | 9.03% | 9.12% | 35.30% | 20.06% | 18.66% | 29.45%
    | 23.69% |'
  prefs: []
  type: TYPE_TB
- en: B.3 Selection Bias Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our discussion, selection bias denotes the tendency that a model inherently
    assigns a higher probability to specific ID tokens, such as A or B, in multi-choice
    questions. The impact of selection bias varies across different models and tasks,
    influencing the models’ performance by reducing their robustness in handling multi-choice
    problems Zheng et al. ([2023a](#bib.bib32)). The permutation-based debiasing method,
    which averages the model’s prediction distributions across various option permutations,
    theoretically eliminates selection bias. However, due to limited access to the
    prediction distributions of closed-source models, we employ a sampling approximation
    approach to estimate and mitigate selection bias.
  prefs: []
  type: TYPE_NORMAL
- en: To formalize our discussion, we use $C$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We assume that given $C$, which can be decomposed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Z_{x^{I}}^{-1}P_{\text{prior}}\left(id_{i}\mid C\right)P_{\text{debiased}}\left(o_{f_{I}(i)}\mid
    C,x^{I}\right)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{x^{I}}^{-1}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We assume $P_{\text{debiased}}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\operatorname{softmax}\left(\frac{1}{&#124;\mathcal{I}&#124;}\sum_{I\in\mathcal{I}}\log
    P_{\text{biased }}\left(id_{i}\mid C,x^{I}\right)\right)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Based on the estimated $P_{\text{prior }}\left(id_{i}\mid C\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: For our analysis, the $P_{\text{prior }}\left(id_{i}\mid C\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Dataset Perplexity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perplexity is a metric that quantifies the complexity and the predictability
    of a dataset. By analyzing the perplexity of dataset, we can gain insights into
    the relative difficulty models may encounter during testing, as well as the diversity
    of information within the dataset. In this analysis, we calculate the perplexity
    of newly evolving datasets derived by reframing the original GSM8K dataset and
    compared them with their original counterparts. The comparison results are presented
    in Figure [6](#A2.F6 "Figure 6 ‣ B.4 Dataset Perplexity Analysis ‣ Appendix B
    Experimental Analysis ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic
    LLM Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our findings indicate that the datasets created through different reframing
    operations exhibit an increase in perplexity compared to the original instances.
    This indicates that the reframed instances are more complex and less predictable.
    These results, aligning with the experimental observations discussed in Section [3.3](#S3.SS3
    "3.3 Analysis of Varied Reframing Operations ‣ 3 Experiments ‣ Benchmark Self-Evolving:
    A Multi-Agent Framework for Dynamic LLM Evaluation"), suggest that our framework
    has the ability to generate instances with enhanced linguistic structure and diversity
    compared to the original instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c456e737b5f7babced19a34c45b73781.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Perplexity comparison between original and reframed datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Error Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tables [14](#A2.T14 "Table 14 ‣ B.5 Error Analysis ‣ Appendix B Experimental
    Analysis ‣ Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation"), [15](#A2.T15
    "Table 15 ‣ B.5 Error Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation")and [16](#A2.T16
    "Table 16 ‣ B.5 Error Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation") present cases
    from GSM8K where ChatGPT successfully responds to the original instance but fails
    to answer the evolved version. The case in Table [14](#A2.T14 "Table 14 ‣ B.5
    Error Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark Self-Evolving: A
    Multi-Agent Framework for Dynamic LLM Evaluation") introduces an additional reasoning
    step to the original instance, which causes ChatGPT to misinterpret the context
    and perform an incorrect calculation. In Table [15](#A2.T15 "Table 15 ‣ B.5 Error
    Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark Self-Evolving: A Multi-Agent
    Framework for Dynamic LLM Evaluation"), the evolved instance alters two data points
    from the original case, leading ChatGPT to neglect the critical information "4
    times a week," resulting in an inaccurate response. The example in Table [16](#A2.T16
    "Table 16 ‣ B.5 Error Analysis ‣ Appendix B Experimental Analysis ‣ Benchmark
    Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation") demonstrates
    that, although ChatGPT correctly calculates the total number of hours the candle
    will burn in the original instance, it fails to address a direct question about
    the first reasoning step in the new instance.'
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing instances from our evolving framework, we can expose model’s limitations
    more effectively, including its vulnerability to perturbations, diminished generalization
    capabilities on more complex problems, and inadequacies in addressing questions
    targeting specific sub-abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: Instance Before and After Question Complicating'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original Instance | New Instance |'
  prefs: []
  type: TYPE_TB
- en: '| Context: A robe takes 2 bolts of blue fiber and half that much white fiber.
    | Context: A robe takes 2 bolts of blue fiber and half that much white fiber.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Question: How many bolts in total does it take? | Question: If a merchant
    has 20 bolts of blue fiber and 10 bolts of white fiber, how many robes can he
    make using all the fibers without any leftovers? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: 3 | Answer: 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Response: It takes 2 + 1 = 3 bolts in total. | Response: The merchant can
    make 20/2 = 10 robes using the blue fiber. He can make 10/2 = 5 robes using the
    white fiber. So, he can make a total of 5 robes using all the fibers without any
    leftovers. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgement: ✓ | Judgement: $\times$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Instance Before and After Polarity Reversing'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original Instance | New Instance |'
  prefs: []
  type: TYPE_TB
- en: '| Context: Jim spends 2 hours watching TV and then decides to go to bed and
    reads for half as long. He does this 3 times a week. | Context: Jim spends 3 hours
    watching TV and then decides to go to bed and reads for a third as long. He does
    this 4 times a week. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: How many hours does he spend on TV and reading in 4 weeks? | Question:
    How many hours does he spend on TV and reading in 4 weeks? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: 36 | Answer: 64 |'
  prefs: []
  type: TYPE_TB
- en: '| Response: Jim spends 2 + (2/2) = 3 hours on TV and reading each time. So,
    in 4 weeks, he spends 3 * 3 * 4 = 36 hours on TV and reading. The answer is 36.
    | Response: Jim spends 3 + (3/3) = 4 hours on TV and reading each time. In 4 weeks,
    he spends 4 * 4 = 16 hours on TV and reading. The answer is 16. |'
  prefs: []
  type: TYPE_TB
- en: '| Judgement: ✓ | Judgement: $\times$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: Instance Before and After Planning Operation'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original Instance | New Instance |'
  prefs: []
  type: TYPE_TB
- en: '| Context: A candle melts by 2 centimeters every hour that it burns. | Context:
    A candle melts by 2 centimeters every hour that it burns. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: How many centimeters shorter will a candle be after burning from
    1:00 PM to 5:00 PM? | Question: What is the first reasoning step to calculate
    how many centimeters shorter a candle will be after burning from 1:00 PM to 5:00
    PM? |'
  prefs: []
  type: TYPE_TB
- en: '|  | A. The first reasoning step is to measure the initial height of the candle.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | B. The first reasoning step is to determine the total number of hours
    the candle will burn. |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: 8 | Answer: B |'
  prefs: []
  type: TYPE_TB
- en: '| Response: The candle burns for 5-1=4 hours. So, it will be 2*4=8 centimeters
    shorter. The answer is 8. | Response: A |'
  prefs: []
  type: TYPE_TB
- en: '| Judgement: ✓ | Judgement: $\times$ |'
  prefs: []
  type: TYPE_TB
- en: B.6 Data Contamination Experiment Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We construct an instruction tuning dataset comprising 4,000 general instances
    from alpaca-gpt4-data Peng et al. ([2023](#bib.bib20)) and additional 4,000 instances,
    with 1,000 each from the training sets of GSM8K, CLUTRR, StrategyQA and BoolQ.
    This dataset is used to fine-tune a model to simulate in-domain contamination.
    Furthermore, we incorperate 400 instances from the original benchmark into the
    instruction tuning dataset to fine-tune another model to simulate the direct contamination.
  prefs: []
  type: TYPE_NORMAL
