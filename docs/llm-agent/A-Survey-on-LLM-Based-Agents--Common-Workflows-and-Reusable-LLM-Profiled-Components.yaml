- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.05804](https://ar5iv.labs.arxiv.org/html/2406.05804)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xinzhe Li
  prefs: []
  type: TYPE_NORMAL
- en: School of IT, Deakin University, Australia
  prefs: []
  type: TYPE_NORMAL
- en: lixinzhe@deakin.edu.au
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent advancements in Large Language Models (LLMs) have catalyzed the development
    of sophisticated frameworks for developing LLM-based agents. However, the complexity
    of these frameworks r poses a hurdle for nuanced differentiation at a granular
    level, a critical aspect for enabling efficient implementations across different
    frameworks and fostering future research. Hence, the primary purpose of this survey
    is to facilitate a cohesive understanding of diverse recently proposed frameworks
    by identifying common workflows and reusable LLM-Profiled Components (LMPCs).
  prefs: []
  type: TYPE_NORMAL
- en: 'A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components'
  prefs: []
  type: TYPE_NORMAL
- en: Xinzhe Li School of IT, Deakin University, Australia lixinzhe@deakin.edu.au
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bf7748a40c6d44bf2e54fbc8675ae89.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Policy-Only Workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d50dbc6cf630a0fea8da76d86a89414.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Search Workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5c3508514f409c449f1ca3ba1228666.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Feedback-Learning Workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Eight Common Workflows based on Three LLM-profiled Components (Policy,
    Evaluator and Dynamic Model) under Task or/and Tool-Use Environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative Large Language Models (GLMs or LLMs) have acquired extensive general
    knowledge and human-like reasoning capabilities (Santurkar et al., [2023](#bib.bib15);
    Wang et al., [2022](#bib.bib23); Zhong et al., [2022](#bib.bib33), [2023](#bib.bib32)),
    positioning them as pivotal in constructing AI agents known as LLM-based agents.
    In the context of this survey, LLM-based agents are defined by their ability to
    interact actively with external tools (such as Wikipedia) or environments (such
    as householding environments) and are designed to function as integral components
    of agency, including acting, planning, and evaluating.
  prefs: []
  type: TYPE_NORMAL
- en: Purpose of the Survey
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The motivation behind this survey stems from the observation that many LLM-based
    agents incorporate similar workflows and components, despite the presence of a
    wide variety of technical and conceptual challenges, e.g., search algorithms (Yao
    et al., [2023a](#bib.bib29)), tree structures (Hao et al., [2023](#bib.bib7)),
    and Reinforcement Learning (RL) components (Shinn et al., [2023](#bib.bib16)).
    (Wu et al., [2023](#bib.bib27)) offer a modular approach but lack integration
    with prevalent agentic workflows. Wang et al. ([2024](#bib.bib21)) provide a comprehensive
    review of LLM agents, exploring their capabilities across profiling, memory, planning,
    and action. In contrast, our survey does not attempt to cover all components of
    LLM-based agents comprehensively. Instead, we concentrate on the involvement of
    LLMs within agentic workflows and aim to clarify the roles of LLMs in agent implementations.
    We create common workflows incorporating reusable LLM-Profiled Components (LMPCs),
    as depicted in Figure [1](#S0.F1 "Figure 1 ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components").'
  prefs: []
  type: TYPE_NORMAL
- en: Contributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This survey offers the following contributions. 1) Alleviating the understanding
    of complex frameworks: The complexity of existing frameworks can be simplified
    into implementable workflows, especially when they are extracted for specific
    tasks. This survey emphasizes reusable workflows and LMPCs across popular frameworks,
    such as ReAct (Yao et al., [2023b](#bib.bib30)), Reflexion (Shinn et al., [2023](#bib.bib16))
    and Tree-of-Thoughts (Yao et al., [2023a](#bib.bib29)). Specifically, based on
    the interaction environments (§[2](#S2 "2 Task Environments And Tool Environments
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components"))
    and the use of common LMPCs (§[3](#S3 "3 LLM-Profiled Components ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")), we
    categorize and detail various workflows, e.g., tool-use workflows, search workflows,
    and feedback-learning workflows. Many existing frameworks are composed of these
    workflows and LMPCs, along with some specific non-LLM components. 2) Helping researchers/practitioners
    assess current frameworks at a more granular and cohesive level: Section [4](#S4
    "4 Workflows of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") categorizes prominent frameworks and demonstrates
    how they are assembled by the common workflows and LMPCs, as summarized in Table [2](#S3.T2
    "Table 2 ‣ Task-Dependent LLM-Profiled Components ‣ 3 LLM-Profiled Components
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")
    ¹¹1A more detailed summarization is demonstrated in Appendix [A](#A1 "Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components"). 3) Facilitating further extensions of
    existing frameworks: Existing frameworks could be modified by changing the implementations
    of LMPCs. To enable this, we not only summarize implementations of LMPCs but also
    their applicability across diverse workflows and tasks in Section [5](#S5 "5 Implementations
    of LMPCs ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Env Types | Entities Interacted With by Agent | Action Properties | Examples
    of Action Instances | Examples of Env Instances |'
  prefs: []
  type: TYPE_TB
- en: '| Task Environments |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gaming Environments | Virtual game elements (objects, avatars, other characters),
    and possibly other players or game narratives | Discrete, Executable,'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic | Move(Right) | BlocksWorld, CrossWords |
  prefs: []
  type: TYPE_NORMAL
- en: '| Embodied Environments | Physical world (through sensors and actuators) |
    Discrete, Executable,'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic | Pick_Up[Object] | AlfWorld (Shridhar et al., [2021](#bib.bib17)),
    VirtualHome,
  prefs: []
  type: TYPE_NORMAL
- en: Minecraft (Fan et al., [2022](#bib.bib3)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| NLIEs | Humans (through conversation or text) | Free-form, Discrete,'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic
  prefs: []
  type: TYPE_NORMAL
- en: (Single-step QA)
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic
  prefs: []
  type: TYPE_NORMAL
- en: (Multi-step) | The answer is Answer Finish[Answer] | GSM8K, HotpotQA |
  prefs: []
  type: TYPE_NORMAL
- en: '| Tool Environments ( Nested with Task Environments) |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval | Retrieval | Discrete, Executable,'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic,
  prefs: []
  type: TYPE_NORMAL
- en: Non-State-Altering | Wiki_Search[Entity] | A Wikipedia API (Goldsmith, [2023](#bib.bib4))
    (used by ReAct (Yao et al., [2023b](#bib.bib30))) |
  prefs: []
  type: TYPE_NORMAL
- en: '| Calculator | Calculator | Executable, Deterministic,'
  prefs: []
  type: TYPE_NORMAL
- en: Non-State-Altering | 2 x 62 = << Calculator >> | Python’s eval function (used
    by MultiTool-CoT (Inaba et al., [2023](#bib.bib9))) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Common task environments and tool-use environments. We categorize
    all the benchmarks existing in the work of the 12 agentic workflows into four
    environment types. An action instance is commonly formalized by action predicates
    and action arguments. Tool use can be considered internal environments of an agent
    and commonly defined for QA tasks under NLIEs.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Task Environments And Tool Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section explores task environments and tool environments, which present
    different settings compared to traditional AI and reinforcement learning (RL)
    agent frameworks (Russell and Norvig, [2010](#bib.bib14); Sutton and Barto, [2018](#bib.bib18))
    . After a brief overview of standard logic-based gaming and simulated embodied
    environments, we focus on two specific areas: Natural Language Interaction Environments
    (NLIEs) and Tool Environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Typical Task Environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Typically, there are two common types of task environments: 1) Rule-Based Gaming
    Environments: These environments, which are deterministic and fully observable,
    include a variety of abstract strategy games like Chess and Go, and logic puzzles
    such as the Game of 24 (Yao et al., [2023a](#bib.bib29)) and Blocksworld (Hao
    et al., [2023](#bib.bib7)). They demand deep logical reasoning and strategic planning
    to navigate and solve. 2) Simulated Embodied Environments: These settings simulate
    real-world physical interactions and spatial relationships. They require agents
    to engage in navigation, object manipulation, and other complex physical tasks,
    reflecting changes in the physical environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Natural Language Interaction Environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the rise of LLM agents, there is a growing trend among NLP researchers
    to recontextualize typical NLP tasks as agentic environments (Yao et al., [2023b](#bib.bib30);
    Hao et al., [2023](#bib.bib7); Yao et al., [2023a](#bib.bib29)). These settings
    are referred to as Natural Language Interaction Environments in our survey.
  prefs: []
  type: TYPE_NORMAL
- en: In NLIEs, the environment remains static until the agent acts. Unlike typical
    task environments where natural language serves as an intermediary, in NLIEs,
    both the states and actions are defined linguistically, making the states conceptual
    and the actions often ambiguous and broadly defined.
  prefs: []
  type: TYPE_NORMAL
- en: Single-Step NLIEs for Question Answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many works (Yao et al., [2023b](#bib.bib30); Shinn et al., [2023](#bib.bib16))
    formulate the traditional QA setup as a single-step decision-making process, where
    the agent generates an answer in response to a question. The process starts with
    the question as the initial state and concludes when the answer is provided as
    an action.
  prefs: []
  type: TYPE_NORMAL
- en: Deliberate Multi-step NLIEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For tasks where "intermediate steps are not explicitly defined", several studies
    have transformed NLP tasks into a Markov Decision Process to facilitate agentic
    workflows. For example, Hao et al. ([2023](#bib.bib7)) reformulate subquestions
    in QA tasks as actions, enabling responses to user queries through a multi-step
    process. This approach allows the initial question to serve as the beginning of
    a series of state transitions. Actions may vary from providing direct, free-form
    answers in single-step QA to strategically formulating subquestions that navigate
    the agent through sequential updates toward a comprehensive solution. This method
    aligns more closely with a sequential decision-making process, making it apt for
    deployment in planning-based agent systems. Additionally, Wan et al. ([2024](#bib.bib20))
    suggest that "splitting an output sequence into tokens might be a good choice"
    for defining multi-step NLIEs methodically. Furthermore, Yao et al. ([2023a](#bib.bib29))
    formulate two-step NLIEs for creative writing by segmenting the problem-solving
    process into distinct planning and execution phases.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Tool Environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern LLM agents are often enhanced with external tools that improve their
    problem-solving capabilities (Inaba et al., [2023](#bib.bib9); Yao et al., [2023b](#bib.bib30)).
    The design and integration of these tools add complexity, requiring careful consideration
    of how LLMs interact not only with the task environments but also with these auxiliary
    tools. Typically, actions in tool environments involve interactions with resources
    that remain unaffected by these interactions. For instance, retrieving data from
    Wikipedia constitutes a "read-only" action, which does not modify the Wikipedia
    database. This feature distinguishes such tool-use actions from those in conventional
    task environments or typical reinforcement learning (RL) settings, where actions
    generally alter the environmental state. Nevertheless, it is important to recognize
    that tool environment can be dynamic that can undergo changes externally. This
    aspect reflects the nature that tools should be considered external environments
    rather than the agent’s internal processes.
  prefs: []
  type: TYPE_NORMAL
- en: Nested NLIE-QA + Tool Environments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tool environments are frequently established along with NLIEs to aid in solving
    QA tasks. Shinn et al. ([2023](#bib.bib16)); Yao et al. ([2023b](#bib.bib30))
    incorporate tools to enhance the factuality of responses. They define command-like
    actions such as “Search” and “LookUp” to interact with Wikipedia, with “Search”
    suggesting the top-5 similar entities from the relevant wiki page, and “LookUp”
    simulating the Ctrl+F functionality in a browser. Beyond simple retrieval, Thoppilan
    et al. ([2022](#bib.bib19)) include a language translator and a calculator for
    dialog tasks. Similarly, Inaba et al. ([2023](#bib.bib9)) employ a calculator,
    implemented using the Python eval function, to resolve numerical queries within
    the NumGLUE benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 3 LLM-Profiled Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section explores common agentic roles for which LLMs are typically profiled.
    The components leverage the internal commonsense knowledge and reasoning abilities
    of LLMs to generate actions, plans, estimate values ²²2Values refer to the estimated
    rewards (a quantitative measure of the success or desirability of the outcomes)
    associated with taking a certain action in a state, widely used in typical RL
    and MDP settings to learn policy models that perform desirable behaviors., and
    infer subsequent states.
  prefs: []
  type: TYPE_NORMAL
- en: Universal LLM-Profiled Components
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Specifically, the following task-agnostic components are profiled and commonly
    used across various workflows. *1) LLM-Profiled Policy $\text{glm}_{\text{policy}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Task-Dependent LLM-Profiled Components
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the universal components, certain LLM-profiled components are
    tailored to specific tasks. For instance, verbalizers are crucial in embodied
    environments but unnecessary in NLIEs. A verbalizer translates actions and observations
    into inputs for planners; for example, in the Planner-Actor-Reporter workflow
    (Wang et al., [2023a](#bib.bib22)), a fine-tuned Visual Language Model (VLM) along
    with $\text{glm}_{\text{planner}}$ (Shinn et al., [2023](#bib.bib16)), often guide
    descriptions according to specified criteria.
  prefs: []
  type: TYPE_NORMAL
- en: '| Workflows | Related Frameworks | LMPCs | Applicable Environments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Policy-only | Base | LLM Planner (Huang et al., [2022](#bib.bib8)), DEPS
    (Wang et al., [2023b](#bib.bib24)), Planner-Actor-Reporter (Dasgupta et al., [2022](#bib.bib2)),
    Plan-and-solve (Wang et al., [2023a](#bib.bib22)), ReAct (Yao et al., [2023b](#bib.bib30))
    | $\text{glm}_{\text{planner}}$ (Optional for Embodied Env),'
  prefs: []
  type: TYPE_NORMAL
- en: $\text{glm}_{\text{verbalizer}}$ (Optional for Embodied Env) | Embodied Env,
    NLIEs-Writing, NLIE-QA |
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Tool-Use | MultiTool-CoT (Inaba et al., [2023](#bib.bib9)), ReAct (Yao
    et al., [2023b](#bib.bib30)), Reflexion (Shinn et al., [2023](#bib.bib16)) | $\text{glm}_{\text{actor}}$
    | NLIEs |'
  prefs: []
  type: TYPE_TB
- en: '| Search | Traversal & Heuristic | Tree-of-Thoughts (ToT) (Yao et al., [2023a](#bib.bib29)),
    Tree-BeamSearch (Xie et al., [2023](#bib.bib28)), Boost-of-Thoughts (Chen et al.,
    [2024](#bib.bib1)) | $\text{glm}_{\text{actor}}$ | Gaming, NLIEs-Writing, NLIE-QA
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | MCTS | RAP (Hao et al., [2023](#bib.bib7)), Wan et al. ([2024](#bib.bib20))
    | $\text{glm}_{\text{actor}}$ | Gaming, NLIEs-QA |'
  prefs: []
  type: TYPE_TB
- en: '| Feedback Learning | from $\text{glm}_{\text{eval}}$ | NLIEs-QA |'
  prefs: []
  type: TYPE_TB
- en: '|  | from $\text{glm}_{\text{eval}}$ | Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '|  | from Tools & Humans | Guan et al. ([2023](#bib.bib6)) | $\text{glm}_{\text{planner}}$
    | Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '|  | from Tools & $\text{glm}_{\text{eval}}$ | NLIEs |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Workflows of LLM-Based Agents and Related Frameworks. We summarize
    LLM-Profiled Components (LMPCs) and applicable environments based on the original
    papers of the listed frameworks, although other possibilities exist, e.g., using
    $\text{glm}_{\text{planner}}$ within the tool-use workflow and applying the base
    workflow to gaming environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Workflows of LLM-Based Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section explores different workflows and the utilization of various LLM-Profiled
    Components (LMPCs), as illustrated in Figure [1](#S0.F1 "Figure 1 ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components"). Specifically,
    we will aggregate proposed frameworks based on workflow types, as demonstrated
    in Table [2](#S3.T2 "Table 2 ‣ Task-Dependent LLM-Profiled Components ‣ 3 LLM-Profiled
    Components ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"). This section delves into diverse workflows and the application of
    distinct LLM-Profiled Components (LMPCs), as visualized in Figure [1](#S0.F1 "Figure
    1 ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components").
    We categorize and consolidate the related frameworks according to the workflows,
    exemplified in Table [2](#S3.T2 "Table 2 ‣ Task-Dependent LLM-Profiled Components
    ‣ 3 LLM-Profiled Components ‣ A Survey on LLM-Based Agents: Common Workflows and
    Reusable LLM-Profiled Components"). It is noteworthy that one framework could
    employ distinct workflows according to tasks or environments. More details of
    the frameworks are summarized in Appendix [A](#A1 "Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Policy-Only Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Base and tool-use workflows only require LLMs to be profiled as policy models.
    In the realm of embodied tasks, many projects deploy base workflows with $\text{glm}_{\text{planner}}$
    is always applied to NLIEs like ReAct (Yao et al., [2023b](#bib.bib30)), Reflexion
    (Shinn et al., [2023](#bib.bib16)), and MultiTool-CoT (Inaba et al., [2023](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Search Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike base agents with $\text{glm}_{\text{planner}}$, which generates a sequence
    of actions for a plan at one generation, actions in search workflows are organized
    into tree (Yao et al., [2023a](#bib.bib29); Hao et al., [2023](#bib.bib7)) and
    graph (Liu et al., [2023](#bib.bib11)) for exploration. Planning or search algorithms
    can explore sequential decisions in a non-linear manner. During the process, the
    tree (or solution) is constructed by adding nodes, each representing a partial
    solution with the input and the sequence of thoughts/actions so far. Data structures
    such as trees enable strategic searches over actions derived from multiple reasoning
    paths. This is accomplished using algorithms like beam search (Xie et al., [2023](#bib.bib28)),
    depth-first and breadth-first search (DFS and BFS) (Yao et al., [2023a](#bib.bib29)),
    and Monte-Carlo Tree Search (MCTS) (Hao et al., [2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Generally, LMPCs are used to explore the path towards the goal. Instead of directly
    applying actions on external environments within policy-only workflows, $\text{glm}_{\text{policy}}$
    is used to calculate values for action/state evaluation during exploration (Yao
    et al., [2023a](#bib.bib29); Chen et al., [2024](#bib.bib1)) or as a reward model
    (Hao et al., [2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Search via Traversal and Heuristic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Tree-of-Thoughts (ToT) workflow (Yao et al., [2023a](#bib.bib29)) uses $\text{glm}_{\text{policy}}$
    most promising nodes. ⁴⁴4Typically, BFS does not use a utility model to decide
    which nodes to expand because it systematically explores all possible nodes at
    each depth.
  prefs: []
  type: TYPE_NORMAL
- en: Simulated-Based Search via MCTS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The RAP workflow (Hao et al., [2023](#bib.bib7)) also builds a tree through
    searching and includes $\text{glm}_{\text{policy}}$, iteratively simulating the
    trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Feedback-Learning Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are primarily four main sources of feedback: $\text{glm}_{\text{eval}}$
    (internal feedback), humans, task environments, and tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Reflexion (Shinn et al., [2023](#bib.bib16)) and Self-Refine (Madaan et al.,
    [2023](#bib.bib12)) utilize $\text{glm}_{\text{eval}}$, as noted by the workflow
    from Guan et al. ([2023](#bib.bib6)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Prompting | Example Works | Example Prompts (in Appendix) |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{glm}_{\text{actor}}$ | Few-shot | ReAct (Yao et al., [2023b](#bib.bib30)),
    Reflexion (Shinn et al., [2023](#bib.bib16)), RAP (Hao et al., [2023](#bib.bib7)),
    MultiTool-CoT (Inaba et al., [2023](#bib.bib9)) | Table [8](#A1.T8 "Table 8 ‣
    Appendix A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components"), [10](#A1.T10 "Table 10 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{glm}_{\text{planner}}$ | Zero-shot | Plan-and-Solve (Wang et al.,
    [2023a](#bib.bib22)), LLM Planner (Huang et al., [2022](#bib.bib8)) | Table [6](#A1.T6
    "Table 6 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components") |'
  prefs: []
  type: TYPE_TB
- en: '|  | Few-shot | DEPS (Wang et al., [2023b](#bib.bib24)), Planner-Actor-Reporter
    (Dasgupta et al., [2022](#bib.bib2)) |  |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{glm}_{\text{evaluator}}$ | Few-shot | RAP (Hao et al., [2023](#bib.bib7)),
    Tree-BeamSearch (Xie et al., [2023](#bib.bib28)), Reflexion (Shinn et al., [2023](#bib.bib16)),
    CRITIC (Gou et al., [2024](#bib.bib5)) | Table [12](#A2.T12 "Table 12 ‣ B.2 Prompts
    for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components"), [13](#A2.T13 "Table 13
    ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components") |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{glm}_{\text{dynamic}}$ | Few-shot | RAP (Hao et al., [2023](#bib.bib7))
    | Table [16](#A2.T16 "Table 16 ‣ B.3 Prompts as Dynamic Models ‣ Appendix B Examples
    Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components") |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Prompting Methods of LLM-Profiled Components'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Formulation | Feedback Types | Applicable Workflows | Example Works
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Text Generation | Free-form reflection | Feedback-learning workflows | Self-Refine
    (Madaan et al., [2023](#bib.bib12)), Reflexion (Shinn et al., [2023](#bib.bib16)),
    CRITIC (Gou et al., [2024](#bib.bib5)) |'
  prefs: []
  type: TYPE_TB
- en: '| Binary/Multi-class Classification | Discrete values | Search workflows |
    RAP (Hao et al., [2023](#bib.bib7)), Tree-BeamSearch (Xie et al., [2023](#bib.bib28))
    ToT (Yao et al., [2023a](#bib.bib29)) |'
  prefs: []
  type: TYPE_TB
- en: '| Binary Classification | Continuous values (logits) | Search workflow via
    MCTS | RAP (Hao et al., [2023](#bib.bib7)) |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-choice QA | Choices of top-N actions | Search workflows via traversal
    and heuristic | ToT (Yao et al., [2023a](#bib.bib29)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Workflow-Specific LLM-Profiled Evaluators According to Task Formulation
    and Feedback Types'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Implementations of LMPCs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore different implementation approaches for LMPCs, covering
    strategies that are independent of specific workflows and tasks, implementations
    specifically designed for certain tasks, and those tailored to particular workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Universal Implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs are normally profiled by leveraging Chain-of-Thought (CoT) prompting to
    transcend basic input-output inference, facilitating the creation of intermediate
    reasoning steps. This is achieved through two techniques: Zero-shot CoT integrates
    a CoT trigger, such as “Let’s think step-by-step” (Kojima et al., [2022](#bib.bib10)),
    within task instructions, while while few-shot CoT incorporates handcrafted reasoning
    steps within provided examples for in-context learning (Wei et al., [2022](#bib.bib25)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [3](#S4.T3 "Table 3 ‣ 4.3 Feedback-Learning Workflows ‣ 4
    Workflows of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components"), some studies (Wang et al., [2023a](#bib.bib22))
    employ zero-shot CoT prompting, but most (Yao et al., [2023b](#bib.bib30); Shinn
    et al., [2023](#bib.bib16); Hao et al., [2023](#bib.bib7)) implement LLM policy
    models via few-shot CoT. Zero-shot CoT implementation of $\text{glm}_{\text{planner}}$
    often fails to produce long-horizon plans, unlike few-shot CoT prompting (Wang
    et al., [2023b](#bib.bib24)). While effective, few-shot prompting requires manual
    compilation of demonstrations with reasoning sequences, leading to increased manual
    work and computational resource use. Methods like Auto CoTs (Zhang et al., [2023](#bib.bib31))
    that automatically generate few-shot demonstrations could mitigate this challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Workflow-Specific Implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\text{glm}_{\text{policy}}$ Implementations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two distinct implementations can be employed to enable $\text{glm}_{\text{policy}}$
    to trigger tool usage within tool-use and feedback learning workflows (receiving
    feedback from tools).
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using In-Generation Triggers: Tools could be invoked during the reasoning generation
    process, e.g., MultiTool-CoT (Inaba et al., [2023](#bib.bib9)). The agent program
    monitors each token produced, pausing text generation when a tool trigger is detected.
    This pause allows for the invocation of tools, whose outputs are then inserted
    into the prompt to complete the reasoning. The triggers for these tools are defined
    either through tool descriptions, few-shot demonstrations ⁵⁵5In-generation triggers
    via few-shot demos: see an example prompt in Table [15](#A2.T15 "Table 15 ‣ B.2
    Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components")), or a combination
    of both ⁶⁶6In-generation triggers via descriptions and few-shot demos: see an
    example prompt in Table [8](#A1.T8 "Table 8 ‣ Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reasoning-Acting (ReAct) Strategy for Tool Use: Introduced by Yao et al. ([2023b](#bib.bib30)),
    each reasoning or acting step is separately verbalized via a complete generation.
    Although the ReAct framework (Yao et al., [2023b](#bib.bib30)) unifies tool actions
    within the tool-use workflow and task-specific actions within the base workflow,
    we argue that the strategies for tool actions and task-specific actions should
    be distinguished. Because tool actions and task-specific actions are triggered
    differently, an aspect that will be emphasized further in the subsequent section
    about task-specific implementations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\text{glm}_{\text{eval}}$ Implementations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Commonly, different workflows require distinct feedback types and task formulations.
    There are four scenarios regarding the two perspectives, as summarized in Table
    [4](#S4.T4 "Table 4 ‣ 4.3 Feedback-Learning Workflows ‣ 4 Workflows of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"): 1) Generating free-form reflection: This reflective output is frequently
    integrated into the prompt of $\text{glm}_{\text{policy}}$ is designed to reflect
    on previous states and actions within feedback-learning workflows. Depending on
    specific feedback-learning workflows, it could incorporate external inputs from
    task or tool environments to enrich the reflection process. 2) Binary/Multiclass
    Classification: Feedback is obtained from discrete output tokens, commonly "no"
    or "yes." These can be converted into 0/1 values for search workflows. This scalar
    values can be employed as reward signals within Monte Carlo Tree Search (MCTS)
    simulations (Hao et al., [2023](#bib.bib7)), or they may be employed directly
    to guide decision-making at each step of tree traversal (Yao et al., [2023a](#bib.bib29)).
    3) Binary classification with scalar values: This approach differs from the previous
    one by employing the logit values of tokens to calculate scalar feedback values.
    For instance, the probability of a “yes” response is computed using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $v=\text{probs}_{\text{&quot;yes&quot;}}=\frac{e^{l_{\text{&quot;yes&quot;}}}}{e^{l_{\text{&quot;yes&quot;}}}+e^{l_{\text{&quot;no&quot;}}}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $l_{\text{&quot;yes&quot;}}$ are not accessible via black-box LLMs. These
    scalar values can then be utilized as rewards in search workflows for MCTS. 4)
    Multi-choice QA: Employed in settings where a selection from multiple choices
    is required, supporting tasks that involve choosing from top-N possible actions,
    as utilized in search workflows for action selection (Yao et al., [2023a](#bib.bib29)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Task-Specific Implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\text{glm}_{\text{policy}}$ Implementations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The noteworthy details include: 1) Multi-Step Generation: For tasks that inherently
    involve sequential decision-making (e.g., “put a cool tomato in the microwave”),
    post-processing steps are often required for $\text{glm}_{\text{policy}}$ are
    not explicitly implemented to generate plans, it may autonomously formulate plans
    during the reasoning phase before deciding on the current action (Shinn et al.,
    [2023](#bib.bib16); Yao et al., [2023b](#bib.bib30)) ⁹⁹9Implicit Planning: See
    an example prompt in Table [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"). These generated plans are maintained as internal states and do not
    serve as communication signals with other components.'
  prefs: []
  type: TYPE_NORMAL
- en: ReAct Implementation of $\text{glm}_{\text{actor}}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The ReAct framework (Yao et al., [2023b](#bib.bib30)) unifies tool actions
    within the tool-use workflow and task-specific actions within the base workflow.
    However, the sequence in which reasoning and action outputs alternate is task-dependent.
    For question answering (QA), the generations of reasoning steps and tool actions
    are fixed, with alternating prompts for thinking and acting. ^(10)^(10)10Reasoning-Acting
    Strategy (QA tasks): See an example prompt in Table [10](#A1.T10 "Table 10 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components"). In contrast, for embodied tasks, the decision
    whether to proceed with thinking or acting in the next step is autonomously determined
    by $\text{glm}_{\text{policy}}$ ^(11)^(11)11Reasoning-Acting Strategy (embodied
    tasks): See an example prompt in Table [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks
    of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components").'
  prefs: []
  type: TYPE_NORMAL
- en: $\text{glm}_{\text{eval}}$ Implementations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '$\text{glm}_{\text{eval}}$ can be configured to assess different task-specific
    perspectives, with specific agentic prompts determining the evaluation criteria.
    Generally, usefulness could be used to evaluate any actions (Hao et al., [2023](#bib.bib7))
    ^(12)^(12)12Evaluation Criteria (Usefulness): See an example prompt in Table [12](#A2.T12
    "Table 12 ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components").
    In NLIE-QA scenarios, a common metric is the factuality (truthfulness) of responses
    (Gou et al., [2024](#bib.bib5)) ^(13)^(13)13Evaluation Criteria (Factuality/Truthfulness):
    See example prompts in Table [14](#A2.T14 "Table 14 ‣ B.2 Prompts for LLM Evaluators
    ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") and [15](#A2.T15 "Table 15 ‣ B.2 Prompts
    for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we delve deeper into LMPCs and agentic workflows, several key directions
    for future research are identified to advance the development of fully autonomous
    agents across various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Universal Tool Use
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One direction is to move beyond predefined tool use for specific tasks and
    develop strategies that enable LLMs to autonomously determine tool usage based
    on the specific requirements of the task at hand. Another direction is to integrate
    tool use for both policy models and evaluators. In other words, LLMs could reason
    over the use of the tool across various tasks and flexibly jump between different
    roles. The insight of its potential is detailed in Appendix [C](#A3 "Appendix
    C Creation of a Task-Agnostic Tool Environment ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components").'
  prefs: []
  type: TYPE_NORMAL
- en: Unified Frameworks Across Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As detailed in §[5](#S5 "5 Implementations of LMPCs ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components"), despite many
    frameworks are presented with integrated conceptual workflows, many components
    are task-specific. For example, ReAct seeks to harmonize tool actions and task-specific
    actions. However, these workflows and specific implementations of LMPCs are different
    across tasks, such as hardcoded reasoning and acting steps in NLIE-QA versus autonomously
    determined reasoning and acting steps in embodied environments. Similarly, although
    the feedback-learning loop in Reflexion is theoretically unified, in practice,
    external feedback is generated only in embodied environments, not in NLIE-QA.'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing Bandwidth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are several potential strategies for reducing the bandwidth required
    for LLM inference ^(14)^(14)14Here, bandwidth refers to the volume of information
    processed during a single LLM generation, including using Stochastic $\text{glm}_{\text{actor}}$
    (Details in Appendix [D](#A4 "Appendix D Stochastic "glm"_"actor" ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")).'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey provides a summary of common workflows and LLM-Profiled Components
    to encourage the reuse of these components and the expansion of existing workflows
    through the integration of both task-specific LMPCs and non-LLM components. This
    approach aims to foster the development and reproducibility of agentic workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This survey omits discussions on memory design ^(15)^(15)15Appendix [E](#A5
    "Appendix E Memory ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components") provides a brief discussion on memory in LLM-based agents
    and the integration of peripheral components into agentic workflows ^(16)^(16)16These
    are concisely summarized in Appendix [A](#A1 "Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"), as our focus is on the details of common LLM-profiled components
    within agentic workflows to facilitate the implementation of reusable components
    and extensible workflows. This distinctly sets our work apart from other surveys.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chen et al. (2024) Sijia Chen, Baochun Li, and Di Niu. 2024. [Boosting of thoughts:
    Trial-and-error problem solving with large language models](https://openreview.net/forum?id=qBL04XXex6).
    In *The Twelfth International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasgupta et al. (2022) Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino,
    Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. 2022. [Collaborating with
    language models for embodied reasoning](https://openreview.net/forum?id=YoS-abmWjJc).
    In *Second Workshop on Language and Reinforcement Learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    [Minedojo: Building open-ended embodied agents with internet-scale knowledge](https://openreview.net/forum?id=rc8o_j8I8PX).
    In *Thirty-sixth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goldsmith (2023) Jonathan Goldsmith. 2023. [Wikipedia: A python library that
    makes it easy to access and parse data from wikipedia](https://pypi.org/project/wikipedia/).
    Python Package Index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2024) Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. 2024. [CRITIC: Large language models can self-correct
    with tool-interactive critiquing](https://openreview.net/forum?id=Sx038qxjek).
    In *The Twelfth International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging pre-trained large language models to construct
    and utilize world models for model-based task planning](https://openreview.net/forum?id=zDbsSscmuj).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with language model is planning with world
    model](https://aclanthology.org/2023.emnlp-main.507). In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language models as zero-shot planners: Extracting actionable knowledge for
    embodied agents. In *International Conference on Machine Learning*, pages 9118–9147\.
    PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inaba et al. (2023) Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao
    Kurohashi. 2023. [MultiTool-CoT: GPT-3 can use multiple external tools with chain
    of thought prompting](https://doi.org/10.18653/v1/2023.acl-short.130). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 1522–1532, Toronto, Canada. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://openreview.net/forum?id=e2TBb5y0yFf).
    In *Advances in Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji
    Zhou, and Yue Zhang. 2023. [Logicot: Logical chain-of-thought instruction-tuning](http://arxiv.org/abs/2305.12147).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2023. Self-refine: Iterative refinement with self-feedback. *arXiv preprint
    arXiv:2303.17651*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023.
    Selfcheckgpt: Zero-resource black-box hallucination detection for generative large
    language models. *arXiv preprint arXiv:2303.08896*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russell and Norvig (2010) Stuart J Russell and Peter Norvig. 2010. *Artificial
    intelligence a modern approach*. London.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santurkar et al. (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
    Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models
    reflect? *arXiv preprint arXiv:2303.17548*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](http://arxiv.org/abs/2303.11366).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2021. [{ALFW}orld: Aligning text
    and embodied environments for interactive learning](https://openreview.net/forum?id=0IOX0YcCdTn).
    In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    learning: An introduction*. MIT press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2024) Ziyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan Zhang,
    and Jun Wang. 2024. [Alphazero-like tree-search can guide large language model
    decoding and training](https://openreview.net/forum?id=fLO9VaAb3B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on
    large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6):1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. [Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models](https://doi.org/10.18653/v1/2023.acl-long.147).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 2609–2634, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu
    Wei, Zhumin Chen, and Nan Duan. 2022. [From lsat: The progress and challenges
    of complex reasoning](https://doi.org/10.1109/TASLP.2022.3164218). *IEEE/ACM Trans.
    Audio, Speech and Lang. Proc.*, 30:2201–2216.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. 2023b. [Describe, explain, plan and select: Interactive planning
    with LLMs enables open-world multi-task agents](https://openreview.net/forum?id=KtvPdGb31Z).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. [Chain of thought
    prompting elicits reasoning in large language models](https://openreview.net/forum?id=_VjQlMeSB_J).
    In *Advances in Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Willard and Louf (2023) Brandon T Willard and Rémi Louf. 2023. Efficient guided
    generation for llms. *arXiv preprint arXiv:2307.09702*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan,
    Junxian He, and Qizhe Xie. 2023. [Self-evaluation guided beam search for reasoning](https://openreview.net/forum?id=Bw82hwg5Q3).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. [Tree of thoughts: Deliberate
    problem solving with large language models](http://arxiv.org/abs/2305.10601).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023b. [React: Synergizing reasoning and acting
    in language models](https://openreview.net/forum?id=WE_vluYUL-X). In *The Eleventh
    International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023.
    Automatic chain of thought prompting in large language models. In *The Eleventh
    International Conference on Learning Representations (ICLR 2023)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv preprint arXiv:2304.06364*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2022) Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo,
    Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2022. [Analytical
    reasoning of text](https://doi.org/10.18653/v1/2022.findings-naacl.177). In *Findings
    of the Association for Computational Linguistics: NAACL 2022*, pages 2306–2319,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Frameworks of LLM-Based Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [5](#A1.T5 "Table 5 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components") demonstrates
    the workflow(s), LMPCs, and non-LMPC components of each framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Involved Workflows | LMPCs | Non-LMPC Components | Applied Environments
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tree-of-Thoughts (ToT) (Yao et al., [2023a](#bib.bib29)) | Search via Traversal
    & Heuristic | $\text{glm}_{\text{actor}}$ (only for NLIEs-Writing) | Search Tree
    | Gaming; NLIEs-Writing |'
  prefs: []
  type: TYPE_TB
- en: '| Tree-BeamSearch (Xie et al., [2023](#bib.bib28)) | Search-based via Traversal
    & Heuristic | $\text{glm}_{\text{actor}}$ | Search Tree | NLIE-QA |'
  prefs: []
  type: TYPE_TB
- en: '| RAP (Hao et al., [2023](#bib.bib7)) | Search via MCTS | $\text{glm}_{\text{actor}}$
    | Search Tree | Gaming; NLIEs-QA |'
  prefs: []
  type: TYPE_TB
- en: '| LLM Planner (Huang et al., [2022](#bib.bib8)) | Base | $\text{glm}_{\text{planner}}$
    | MLM for action translation | Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '| DEPS (Wang et al., [2023b](#bib.bib24)) | Base | $\text{glm}_{\text{planner}}$
    | Immediate actor, VLM+GLM as verbalizer | Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '| Planner-Actor-Reporter (Dasgupta et al., [2022](#bib.bib2)) | Base | $\text{glm}_{\text{planner}}$
    | RL actor, Trained classifier+Hard code as verbalizer | Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '| Plan-and-solve (Wang et al., [2023a](#bib.bib22)) | Base | $\text{glm}_{\text{planner}}$
    | / | NLIEs-QA |'
  prefs: []
  type: TYPE_TB
- en: '| MultiTool-CoT (Inaba et al., [2023](#bib.bib9)) | Tool-Use | $\text{glm}_{\text{actor}}$
    | / | NLIEs |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct (Yao et al., [2023b](#bib.bib30)) | Tool-Use | $\text{glm}_{\text{actor}}$
    | / | NLIEs |'
  prefs: []
  type: TYPE_TB
- en: '|  | Base | $\text{glm}_{\text{actor}}$ | / | Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '| Guan et al. ([2023](#bib.bib6)) | Feedback Learning (from Tools & Humans)
    | $\text{glm}_{\text{planner}}$ | Domain Experts, Domain-independent Planner |
    Embodied Env |'
  prefs: []
  type: TYPE_TB
- en: '| CRITIC (Gou et al., [2024](#bib.bib5)) | Feedback Learning (from Tool & $\text{glm}_{\text{eval}}$
    | / | NLIEs |'
  prefs: []
  type: TYPE_TB
- en: '| Self-refine (Madaan et al., [2023](#bib.bib12)) | Feedback Learning (from
    $\text{glm}_{\text{eval}}$, | / | NLIEs |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion (Shinn et al., [2023](#bib.bib16)) | Feedback Learning (from $\text{glm}_{\text{eval}}$
    ),'
  prefs: []
  type: TYPE_NORMAL
- en: Tool-Use | $\text{glm}_{\text{actor}}$, | / | NLIEs-QA |
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Feedback Learning (from Task Env & $\text{glm}_{\text{eval}}$ | Embodied
    Env |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Frameworks for LLM-based Agents. MLM: Masked Language Model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Q: In a dance class of 20 students, $20\%$ of the remaining enrolled in jazz
    dance, and the rest enrolled in hip-hop dance. What percentage of the entire students
    enrolled in hip-hop dance? A: Let’s first understand the problem and devise a
    plan to solve the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, let’s carry out the plan and solve the problem step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plan: <svg id="A1.T6.3.3.3.3.p1.pic1" class="ltx_picture" height="192.22" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,192.22) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="163.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Step 1: Calculate the total number of students who enrolled in
    contemporary and jazz dance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Calculate the total number of students who enrolled in hiphop dance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Calculate the percentage of students who enrolled in hiphop dance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: $20\%$ of the remaining 16 students is 4 students. So, a total of 8
    students enrolled in contemporary and jazz dance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: The remaining 12 students enrolled in hip-hop dance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: The percentage of students who enrolled in hip-hop dance is $12/20=60\%$.</foreignobject></g></g></svg>
    Therefore, the answer (arabic numerals) is <svg id="A1.T6.4.4.4.4.p2.pic1" class="ltx_picture"
    height="40.09" overflow="visible" version="1.1" width="603.94"><g transform="translate(0,40.09)
    matrix(1 0 0 -1 0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">60%</foreignobject></g></g></svg> |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: An LLM Planner under NLIE-QA, implemented by the zero-shot CoT method.
    The CoT trigger is highlighted in red font. The content in shadow box is generated
    by LLM. Source from Wang et al. ([2023a](#bib.bib22))'
  prefs: []
  type: TYPE_NORMAL
- en: '| Interact with a household to solve a task. Here are two examples. You are
    in the middle of a room. Looking quickly around you, you see a cabinet 9, a cabinet
    8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet
    2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop
    1, a drawer 9, a drawer 8, a drawer 7, a drawer 6, a drawer 5, a drawer 4, a drawer
    3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf
    3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner
    2, a stoveburner 1, and a toaster 1\. Your task is to: put a cool mug in shelf.'
  prefs: []
  type: TYPE_NORMAL
- en: 'think: To solve the task, I need to find and take a mug, then cool it with
    fridge, then put it in shelf.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OK.
  prefs: []
  type: TYPE_NORMAL
- en: 'think: First I need to find a mug. A mug is more likely to appear in countertop
    (1-3), coffeemachine (1), cabinet (1-9), shelf (1-3), drawer (1-9). I can check
    one by one, starting with countertop 1\.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: OK.
  prefs: []
  type: TYPE_NORMAL
- en: go to countertop 1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the countertop 1, you see a knife 1, a pan 2, a pan 1, and a plate 1\.
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: 'think: Now I cool the mug. Next, I need to put it in/on shelf 1. OK.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: go to shelf 1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the shelf 1, you see a cup 2, a cup 1, a peppershaker 1, and a saltshaker
    1\.
  prefs: []
  type: TYPE_NORMAL
- en: put mug 3 in/on shelf 1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You put the mug 3 in/on the shelf 1\. Here is the task: You are in the middle
    of a room. Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet
    4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a
    countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1,
    a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1,
    a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
    1\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your task is to: put a cool tomato in microwave.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.T7.1.1.1.1.p1.pic1" class="ltx_picture" height="55.16" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,55.16) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">think: To solve the task, I need to find a tomato, then cool it
    with the fridge, and finally put it in the microwave.</foreignobject></g></g></svg>
    OK.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '<svg id="A1.T7.2.2.2.2.p2.pic1" class="ltx_picture" height="73.76" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,73.76) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="44.82" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">think: First, I need to find a tomato. A tomato is more likely
    to be found on the countertop (1-3), cabinet (1-6), drawer (1-3), shelf (1-3),
    or the sinkbasin. I can start by checking the countertop 1.</foreignobject></g></g></svg>
    OK. > <svg id="A1.T7.3.3.3.3.p3.pic1" class="ltx_picture" height="40.55" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,40.55) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="11.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">go to countertop 1</foreignobject></g></g></svg> On the countertop
    1, you see a dishsponge 1, and a mug 2. … |'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Table 7: An LLM Actor within Base Workflow under the Alfred World Benchmark,
    implemented by the few-shot CoT method. The content in the shadow box is generated
    by LLM. Source from (Yao et al., [2023b](#bib.bib30)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Use the following tools at your disposal and answer some questions: |'
  prefs: []
  type: TYPE_TB
- en: '| - <<Calculator>> |'
  prefs: []
  type: TYPE_TB
- en: '| - <<Chemical reaction predictor>> |'
  prefs: []
  type: TYPE_TB
- en: '| - <<Molar mass list>> |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Find the amount of $\mathrm{H}2\mathrm{CO}3$. |'
  prefs: []
  type: TYPE_TB
- en: '| A: First, use the chemical reaction predictor to determine the chemical equation
    for the reaction. |'
  prefs: []
  type: TYPE_TB
- en: '| Reactants: $2\mathrm{CO}2,2\mathrm{H}2\mathrm{O}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Products: ?$\mathrm{H}2\mathrm{CO}3$ |'
  prefs: []
  type: TYPE_TB
- en: '| <<Chemical reaction predictor>> |'
  prefs: []
  type: TYPE_TB
- en: '| $2\mathrm{CO}2+2\mathrm{H}2\mathrm{O}\rightarrow 2\mathrm{H}2\mathrm{CO}3$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Second, use the molar mass list to determine the molar mass of $\mathrm{H}2\mathrm{CO}3$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Third, use the calculator to calculate the amount of 2 moles of $\mathrm{H}2\mathrm{CO}3$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Therefore, $124\mathrm{~{}g}$ is formed. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Find the amount of Calcium hydroxide that is required to react with 2
    moles of Carbon dioxide to form 2 moles of Calcium carbonate along with 2 moles
    of Water. |'
  prefs: []
  type: TYPE_TB
- en: '| A: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: An LLM actor with tool use implemented by MultiTool-CoT (Inaba et al.,
    [2023](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages
    and today, she read twice as many pages as yesterday. If she wants to read half
    of the remaining pages tomorrow, how many pages should she read? A: Julie read
    twice as many pages as yesterday, so she read 12 * 2 = 24 pages today. Since yesterday,
    Julie read 12 + 24 = 36 pages. So, there are 120 - 36 = 84 pages left to be read.
    Since she wants to read half of the remaining pages, she should read 84 / 2 =
    42 pages. The answer is 42.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Ken created a care package to send to his brother, who was away at boarding
    school. Ken placed a box on a scale, and then he poured into the box enough jelly
    beans to bring the weight to 2 pounds. Then, he added enough brownies to cause
    the weight to triple. Next, he added another 2 pounds of jelly beans. And finally,
    he added enough gummy worms to double the weight once again. What was the final
    weight of the box of goodies, in pounds?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Ken poured jelly beans into the box until the weight was 2 pounds, so the
    weight of the box was 2 pounds at first. Then Ken added enough brownies to cause
    the weight to triple, so the weight of the box was 2 * 3 = 6 pounds. After Ken
    added another 2 pounds of jelly beans, the weight of the box was 6 + 2 = 8 pounds.
    Finally, he added enough gummy worms to double the weight once again, so the weight
    of the box was 8 * 2 = 16 pounds. The answer is 16.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: James writes a 3-page letter to 2 different friends twice a week. How many
    pages does he write a year?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: James writes a 3-page letter to 2 different friends twice a week, so he
    writes 3 * 2 * 2 = 12 pages every week. There are 52 weeks in a year, so he writes
    12 * 52 = 624 pages a year. The answer is 624.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Natalia sold clips to 48 of her friends in April, and then she sold half
    as many clips in May. How many clips did Natalia sell altogether in April and
    May?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Natalia sold 48 clips in April and half as many clips in May, so she sold
    48 / 2 = 24 clips in May. Altogether, she sold 48 + 24 = 72 clips. The answer
    is 72.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning
    and bakes muffins for her friends every day with four. She sells the remainder
    at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does
    she make every day at the farmers’ market?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A:" <svg id="A1.T9.1.1.1.1.p1.pic1" class="ltx_picture" height="91.82" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,91.82) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="62.88" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four, so she uses
    3 + 4 = <<3+4=7>>7 eggs daily. Thus, she sells 16 - 7 = <<16-7=9>>9 eggs at the
    farmers’ market daily. Therefore, she makes 9 * $2 = $<<9*2=18>>18 every day at
    the farmers’ market. Answer: 18.</foreignobject></g></g></svg> |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: An LLM Actor within Tool-Use Workflow under NLIE-QA, implemented by
    the few-Shot CoT method. The content in the shadow box is generated by LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Solve a question answering task by having a Thought, then Finish with your
    answer. Thought can reason about the current situation. Finish[answer] returns
    the answer and finishes the task. |'
  prefs: []
  type: TYPE_TB
- en: '| Here are some examples: |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1: What is the elevation range for the area that the eastern sector
    of the Colorado orogeny extends into? |'
  prefs: []
  type: TYPE_TB
- en: '| Thought: Let’s think step by step. The eastern sector of Colorado orogeny
    extends into the High Plains. High Plains rise in elevation from around 1,800
    to 7,000 ft, so the answer is 1,800 to 7,000 ft. |'
  prefs: []
  type: TYPE_TB
- en: '| Action: Finish[1,800 to 7,000 ft] |'
  prefs: []
  type: TYPE_TB
- en: '| … |'
  prefs: []
  type: TYPE_TB
- en: '| Question 6: Were Pavel Urysohn and Leonid Levin known for the same type of
    work? |'
  prefs: []
  type: TYPE_TB
- en: '| Thought: Let’s think step by step. Pavel Urysohn is a mathematician. Leonid
    Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin
    have the same type of work. |'
  prefs: []
  type: TYPE_TB
- en: '| Action: Finish[Yes] |'
  prefs: []
  type: TYPE_TB
- en: '| END OF EXAMPLES) |'
  prefs: []
  type: TYPE_TB
- en: '| Question: VIVA Media AG changed itś name in 2004\. What does their new acronym
    stand for? |'
  prefs: []
  type: TYPE_TB
- en: '| Thought: <svg id="A1.T10.1.1.1.1.p1.pic1" class="ltx_picture" height="92.59"
    overflow="visible" version="1.1" width="603.94"><g transform="translate(0,92.59)
    matrix(1 0 0 -1 0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="63.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Let’s think step by step. VIVA Media AG changed its name in 2004\.
    The new acronym must stand for the new name they adopted. To find the answer,
    we need to research the new name of VIVA Media AG after 2004.Action: Finish[Further
    research is needed to determine the new acronym.]</foreignobject></g></g></svg>
    |'
  prefs: []
  type: TYPE_TB
- en: '| Action: <svg id="A1.T10.3.3.1.1.p1.pic1" class="ltx_picture" height="42.78"
    overflow="visible" version="1.1" width="603.94"><g transform="translate(0,42.78)
    matrix(1 0 0 -1 0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Finish[Further research is needed to determine the new acronym.]</foreignobject></g></g></svg>
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: An LLM Actor within tool-use workflow for the HotPot-QA Benchmark,
    implemented by few-Shot CoT. The content in the shadow box is generated by LLM.
    Source from Yao et al. ([2023b](#bib.bib30)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Given a question, please decompose it into sub-questions. For each sub-question,
    please answer it in a complete sentence, ending with "The answer is". When the
    original question is answerable, please start the subquestion with "Now we can
    answer the question:" |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1: James writes a 3-page letter to 2 different friends twice a week.
    How many pages does he write a year? |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.1: How many pages does he write every week? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer 1.1: James writes a 3-page letter to 2 different friends twice a week,
    so he writes 3 * 2 * 2 = 12 pages every week. The answer is 12. |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.2: How many weeks are there in a year? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer 1.2: There are 52 weeks in a year. The answer is 52. |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.3: Now we can answer the question: How many pages does he write
    a year? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer 1.3: James writes 12 pages every week, so he writes 12 * 52 = 624
    pages a year. The answer is 624. |'
  prefs: []
  type: TYPE_TB
- en: '| … |'
  prefs: []
  type: TYPE_TB
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  prefs: []
  type: TYPE_TB
- en: '| Question 5.1: <svg id="A1.T11.1.1.1.1.p1.pic1" class="ltx_picture" height="55.16"
    overflow="visible" version="1.1" width="603.94"><g transform="translate(0,55.16)
    matrix(1 0 0 -1 0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">How many eggs does Janet have left after eating three for breakfast
    and baking muffins with four?</foreignobject></g></g></svg> |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: An LLM Actor under the GSM8K Benchmark. The content in the shadow
    box is generated by LLM. Source from Hao et al. ([2023](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Examples Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 LLM-Profiled Policy Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '1) Base Workflow: A planner for the NLIE-QA and an actor for the householding
    environment (ALFRED) are demonstrated in Table [6](#A1.T6 "Table 6 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") and [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks
    of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components"), respectively. 2) Tool-Use Workflow: Actors under NLIE-QA
    is demonstrated in Table [8](#A1.T8 "Table 8 ‣ Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"), [9](#A1.T9 "Table 9 ‣ Appendix A Frameworks of LLM-Based Agents
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components"),
    and [10](#A1.T10 "Table 10 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components").
    The first two tables show the implementation of in-generation triggers, while
    the last one demonstrates the ReAct implementation. 3) Search Workflow: An actor
    is required during the expansion stage of MCTS in the RAP workflow. The prompts
    and expected generations are shown in Table [11](#A1.T11 "Table 11 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components").'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Prompts for LLM Evaluators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [12](#A2.T12 "Table 12 ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B
    Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components") and [13](#A2.T13 "Table 13 ‣ B.2 Prompts for LLM Evaluators
    ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") show LLM evaluators that are profiled as
    classification tasks, while Table [15](#A2.T15 "Table 15 ‣ B.2 Prompts for LLM
    Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components") demonstrates another to general
    free-form text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Given a question and some sub-questions, determine whether the last sub-question
    is useful to answer the question. Output ’Yes’ or ’No’, and a reason. |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1: Four years ago, Kody was only half as old as Mohamed. If Mohamed
    is currently twice as 30 years old, how old is Kody? |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.1: How old is Mohamed? |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.2: How old was Mohamed four years ago? |'
  prefs: []
  type: TYPE_TB
- en: '| New question 1.3: How old was Kody four years ago? |'
  prefs: []
  type: TYPE_TB
- en: '| Is the new question useful? Yes. We need the answer to calculate how old
    is Kody now. |'
  prefs: []
  type: TYPE_TB
- en: '| … |'
  prefs: []
  type: TYPE_TB
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  prefs: []
  type: TYPE_TB
- en: '| New question 5.1: Now we can answer the question: How much in dollars does
    she make every day at the farmers’ market? |'
  prefs: []
  type: TYPE_TB
- en: '| Is the new question useful? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: An LLM Evaluator within RAP Workflow under NLIE-QA, implemented by
    few-Shot CoT prompting. It assesses the usefulness of new sub-questions in solving
    the original question. Source from Hao et al. ([2023](#bib.bib7))'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluate if given numbers can reach 24 (sure/likely/impossible) 10 14'
  prefs: []
  type: TYPE_NORMAL
- en: 10 + 14 = 24
  prefs: []
  type: TYPE_NORMAL
- en: sure
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: 1 3 3
  prefs: []
  type: TYPE_NORMAL
- en: 1 * 3 * 3 = 9
  prefs: []
  type: TYPE_NORMAL
- en: (1 + 3) * 3 = 12
  prefs: []
  type: TYPE_NORMAL
- en: 1 3 3 are all too small
  prefs: []
  type: TYPE_NORMAL
- en: impossible
  prefs: []
  type: TYPE_NORMAL
- en: 11, 12 |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: An LLM Evaluator within Tree-of-Thought Workflow under Game 24, implemented
    by few-Shot CoT prompting. The LLM is profiled for multi-class classification.
    Source from Yao et al. ([2023a](#bib.bib29)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Context: … Sentence: …'
  prefs: []
  type: TYPE_NORMAL
- en: Is the sentence supported by the context above?
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer Yes or No: |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: An LLM Evaluator, implemented by zero-Shot CoT prompting. The LLM
    is profiled for multi-class classification. Source from Manakul et al. ([2023](#bib.bib13)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| … |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Serianna is a band of what genre that combines elements of heavy
    metal and hardcore punk? |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed Answer: Let’s think step by step. Serianna is a band of metalcore
    genre. Metalcore is a subgenre of heavy metal and hardcore punk. So Serianna is
    a band of heavy metal and hardcore punk. So the answer is: heavy metal and hardcore
    punk. |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Plausibility: <svg id="A2.T15.1.1.1.1.p1.pic1" class="ltx_picture" height="138.72"
    overflow="visible" version="1.1" width="603.94"><g transform="translate(0,138.72)
    matrix(1 0 0 -1 0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="109.78" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">The question asks for the genre that combines elements of heavy
    metal and hardcore punk, and the answer is "heavy metal and hardcore punk", simply
    repeat the question. So it’s not plausible. 2\. Truthfulness: Let’s search the
    question in google: > Search Query: Serianna is a band of what genre that combines
    elements of heavy metal and hardcore punk? site: wikipedia.org > Evidence:</foreignobject></g></g></svg>
    [Metalcore - Wikipedia] Metalcore is a fusion music genre that combines elements
    of extreme metal and hardcore punk. |'
  prefs: []
  type: TYPE_TB
- en: '| <svg id="A2.T15.4.4.1.1.p1.pic1" class="ltx_picture" height="91.06" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,91.06) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="62.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">The evidence suggests that metalcore is a genre that combines
    elements of extreme metal and hardcore punk, as stated in the proposed answer.
    Let’s search the proposed answer in google: > Search Query: Serianna is a band
    of metalcore genre. site: wikipedia.org > Evidence:</foreignobject></g></g></svg>
    [Serianna - Wikipedia] Serianna was a metalcore band from Madison, Wisconsin.
    The band formed in 2006… |'
  prefs: []
  type: TYPE_TB
- en: '| <svg id="A2.T15.7.7.1.1.p1.pic1" class="ltx_picture" height="107.66" overflow="visible"
    version="1.1" width="603.94"><g transform="translate(0,107.66) matrix(1 0 0 -1
    0 0) translate(0,3.94)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    width="559.25" height="78.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">The evidence suggests Serianna is a metalcore band, the proposed
    answer is correct about this. Above all, the proposed answer correctly identifies
    that Serianna is a band of the metalcore genre, which combines elements of heavy
    metal and hardcore punk. However, the final answer is not plausible since it just
    lists the genres that metalcore combines.</foreignobject></g></g></svg> |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: An LLM Evaluator Within Feedback-Learning Workflow (Tools), implemented
    with few-shot demonstrations. The content in the shadow box is generated by LLM.
    We omit some demonstrations for brevity. Source from Gou et al. ([2024](#bib.bib5)).'
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Prompts as Dynamic Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [16](#A2.T16 "Table 16 ‣ B.3 Prompts as Dynamic Models ‣ Appendix B Examples
    Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components") is profiled as a dynamic model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Given a question, please decompose it into sub-questions. For each sub-question,
    please answer it in a complete sentence, ending with "The answer is". When the
    original question is answerable, please start the subquestion with "Now we can
    answer the question: ". |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1: Weng earns $12 an hour for babysitting. Yesterday, she just did
    50 minutes of babysitting. How much did she earn? |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.1: How much does Weng earn per minute? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer 1.1: Since Weng earns $12 an hour for babysitting, she earns $12 /
    60 = $0.2 per minute. The answer is 0.2. |'
  prefs: []
  type: TYPE_TB
- en: '| Question 1.2: Now we can answer the question: How much did she earn? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer 1.2: Working 50 minutes, she earned $0.2 x 50 = $10\. The answer is
    10. |'
  prefs: []
  type: TYPE_TB
- en: '| … |'
  prefs: []
  type: TYPE_TB
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  prefs: []
  type: TYPE_TB
- en: '| Question 5.1: How many eggs does Janet have left after eating three for breakfast
    and using four for muffins? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer 5.1: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: An LLM-Profiled Dynamic Model.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Creation of a Task-Agnostic Tool Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previous work always limits tools to specific applications like NLIE-QA, future
    work should aim to establish a comprehensive tool environment that encompasses
    a wide array of tools suitable for various tasks. A major challenge here is adapting
    a single actor to utilize such an environment effectively. While in-generation
    strategies are constrained as triggers are typically only straightforward for
    basic tools with simple arguments, a reasoning-acting strategy might offer more
    promise. Nonetheless, defining tools remains a challenge, especially in terms
    of efficient in-context learning or fine-tuning for tool utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Stochastic $\text{glm}_{\text{actor}}$
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, a single action is sampled from the output of glmactor. Exploring
    a stochastic glmactor, which provides a distribution over possible actions, can
    enhance the stochastic nature of the glmpolicy and improve efficiency. This approach
    could include investigating constrained generation techniques (Willard and Louf,
    [2023](#bib.bib26)). Additionally, using such a distribution could efficiently
    serve as rewards for all possible actions, potentially eliminating the need for
    a separate glmeval to model rewards in certain workflows. This method allows for
    the simultaneous expansion of multiple potential nodes in one generation step,
    rather than expanding each node individually in search-based workflows (Hao et al.,
    [2023](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementations of memory in the reviewed works are typically straightforward
    and arbitrary. Commonly, static information (e.g., profiling messages) is manually
    constructed and stored, whereas dynamic information (e.g., feedback) is handled
    via runtime data structures during interactions within each workflow. While the
    management of hybrid memory systems—requiring the explicit processing and management
    of short-term and long-term memory—is extensively discussed in a previous survey
    by Wang et al. ([2024](#bib.bib21)), such memory management aspects are beyond
    the focus of this survey, which centers on LLM-based workflows.
  prefs: []
  type: TYPE_NORMAL
