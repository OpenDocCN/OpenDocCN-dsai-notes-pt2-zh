- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:38:49'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08688](https://ar5iv.labs.arxiv.org/html/2408.08688)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Samee Arif¹, Sualeha Farid², Abdul Hameed Azeemi¹, Awais Athar³, Agha Ali Raza¹
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This paper presents synthetic Preference Optimization (PO) datasets generated
    using multi-agent workflows and evaluates the effectiveness and potential of these
    workflows in the dataset generation process. PO dataset generation requires two
    modules: (1) response evaluation, and (2) response generation. In the response
    evaluation module, the responses from Large Language Models (LLMs) are evaluated
    and ranked - a task typically carried out by human annotators that we automate
    using LLMs. We assess the response evaluation module in a 2 step process. In step
    1, we assess LLMs as evaluators using three distinct prompting strategies. In
    step 2, we apply the winning prompting strategy to compare the performance of
    LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we use inter-rater
    agreement using Cohen’s Kappa between human annotators and LLMs. For the response
    generation module, we compare different configurations for the LLM Feedback Loop
    using the identified LLM evaluator configuration. We use the win rate (the fraction
    of times a generation framework is selected as the best by an LLM evaluator) to
    determine the best multi-agent configuration for generation. After identifying
    the best configurations for both modules, we use models from the GPT, Gemma, and
    Llama families to generate our PO datasets using the above pipeline. We generate
    two types of PO datasets, one to improve the generation capabilities of individual
    LLM and the other to improve the multi-agent workflow. Our evaluation shows that
    GPT-4o-as-a-Judge is more consistent across datasets when the candidate responses
    do not include responses from the GPT family. Additionally, we find that the LLM
    Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves
    a notable 71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) demonstrate a range of Natural Language Processing
    (NLP) capabilities, including text generation, question answering, and language
    understanding. However, LLMs can sometimes deviate from user instructions and
    exhibit unintended behaviors (Tamkin et al. [2021](#bib.bib16)). To mitigate this
    problem and align the LLM outputs more closely with human preferences, techniques
    like Reinforcement Learning from Human Feedback (RLHF) are used, which involves
    fine-tuning LLMs using the reward signal from human preferences (Christiano et al.
    [2017](#bib.bib2)). Improved methods like Direct Preference Optimization (DPO)
    (Rafailov et al. [2024](#bib.bib14)) eliminate the need for fitting the reward
    model and are more stable and performant. In DPO, the preference optimization
    dataset requires a pair of accepted and rejected responses for each prompt. The
    accepted response is one that better aligns with the desired human preferences.
    Other techniques like Kahneman-Tversky Optimization (KTO) (Ethayarajh et al. [2024](#bib.bib4))
    require each response to indicate whether it is good or bad (i.e., as a binary
    classification task) instead of pairwise preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the process of constructing the dataset of human preferences, the evaluation
    and ranking of the outputs generated by LLMs are typically done by human annotators,
    who assess these outputs based on various criteria such as instruction following,
    helpfulness, relevance, accuracy, depth, and creativity. The PO dataset generation
    process is divided into two modules: response evaluation and response generation.
    The response evaluation module involves assessing and ranking responses generated
    by LLMs, while the response generation module focuses on creating responses that
    align with the identified preferences. This manual process, while effective, is
    labor-intensive, time-consuming, inconsistent, and subject to human biases. In
    this work, we thus ask the question, “Can we use LLM agents to automate and improve
    response evaluation and generation for constructing preference optimization (PO)
    datasets?”.'
  prefs: []
  type: TYPE_NORMAL
- en: For the response evaluation step, we leverage LLMs as evaluators and compare
    several configurations including LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate
    to pick the best evaluation strategy. Previously, single agents have been used
    to generate the responses for PO datasets. However, we use a multi-agent framework
    for response generation, which allows us to generate more refined and higher-quality
    responses. The multi-agent approach uses the collaboration between multiple LLMs,
    where one agent can provide suggestions for improvements, and the other can revise
    the response based on the feedback. This iterative process leads to a thorough
    refinement of the generated content, ensuring that the final output better aligns
    with human preferences and expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we present multiple DPO and KTO datasets. Our focus is on generating
    two separate categories of datasets: one aimed at improving the performance of
    individual LLMs and the other to enhance the effectiveness of multi-agent workflows.
    The primary aim of the first dataset is to enhance the performance and capabilities
    of individual LLMs by providing high-quality PO training data that better aligns
    with human judgment and expectations. The goal of the second category of the dataset
    is to improve the multi-agent frameworks of LLM Feedback Loop generation approach,
    enabling better feedback provision, response refinement, and decision-making.
    Our contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate synthetic PO datasets for single-agent improvement by combining
    the best configuration for the evaluation and generation module. We also generate
    PO datasets for multi-agent improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We present a comprehensive evaluation of using LLMs as evaluators on the task
    of selecting the better response among the candidate responses. We specifically
    compare the performance of three distinct approaches: LLM-as-a-Judge, LLMs-as-a-Jury,
    and LLM Debate.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present an evaluation of the LLM Feedback Loop workflow for the response
    generation module, specifically testing different configurations using Llama-3.1-8
    and Gemma-2-9b models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Preference Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preference Optimization has emerged as a pivotal technique for aligning model
    outputs with human preferences. Rafailov et al. [2024](#bib.bib14) introduce DPO,
    a method that simplifies solving the standard RLHF (Reinforcement Learning from
    Human Feedback) problem by converting it into a classification task, enabling
    the extraction of the optimal policy in a straightforward way. Hong, Lee, and
    Thorne [2024](#bib.bib7) introduce ORPO algorithm that combines the traditional
    supervised fine-tuning and preference alignment stages into a single process.
    The dataset for DPO and ORPO require annotated preference pairs, where each pair
    consists of two model outputs labeled according to which one better aligns with
    human preferences. Ethayarajh et al. [2024](#bib.bib4) introduce KTO, a cost-effective
    approach to align Large Language Models (LLMs) with human feedback, improving
    performance without the need for preference pairs. Argilla Distilabel (Álvaro
    Bartolomé Del Canto et al. [2024](#bib.bib23)) uses LLM to judge between the responses
    of two models to create synthetic PO datasets. The datasets are available on Hugging
    Face¹¹1https://huggingface.co/argilla. To our knowledge, no one has yet explored
    the use of Multi-Agent workflows for the generation of PO datasets. However, multi-agent
    frameworks have been utilized for other tasks which we discuss below.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Agent Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recently, there has been a growing interest in using LLM multi-agent frameworks
    for different tasks. Zheng et al. [2023a](#bib.bib21) presents an evaluation of
    LLM-as-a-Judge on the MT-Bench (Zheng et al. [2023b](#bib.bib22)) and Chatbot
    Arena (Li et al. [2024](#bib.bib8)). Their results reveal that strong LLM judges
    like GPT-4 can match both controlled and crowd-sourced human preferences well,
    achieving over 80% agreement, the same level of agreement between humans. Additionally,
    they evaluate several variants of Llama and Vicuna on the dataset. They study
    the limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement
    biases, as well as limited reasoning ability. Verga et al. [2024](#bib.bib17)
    explore the use of LLMs-as-a-Jury. Their approach, a Panel of LLM evaluators (PoLL),
    composed of a larger number of smaller models outperforms a single large judge.
    They also show that the PoLL approach exhibits less intra-model bias as compared
    to LLM-as-a-Judge. They use Command-R, GPT, Claude-3, and Mistral families for
    their study. Additionally, they compare two prompting strategies: (1) reference-based
    scoring where they provide the LLM with a reference answer, and (2) candidate
    answer and pair-wise scoring where they ask the LLM to pick the better response
    from the candidate responses. PoLL outperforms single-agents on KILT (Petroni
    et al. [2021](#bib.bib13)) and Chatbot Arena.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Liang et al.[2024](#bib.bib10) introduce Multi-Agent Debate (MAD) to encourage
    divergent thinking in LLMs. They mitigate the Degeneration-of-Thought (DoT) problem,
    which is that once the LLM has established confidence in its solutions, it is
    unable to generate novel thoughts. In their approach, the affirmative LLM and
    the negative LLM debate on the answer while the LLM judge evaluates both arguments
    after each round of debate. They evaluate the approach on the Commonsense Machine
    Translation Dataset (Chinese to English) (He et al. [2020](#bib.bib6)) and their
    Counter-Intuitive Arithmetic Reasoning (CIAR) dataset. MAD was able to achieve
    a 37% accuracy on the CIAR dataset using GPT-3.5-Turbo which outperforms Chain-of-Thought,
    Self-Consistency, and Self-Reflection prompting. They also show that using the
    MAD approach decreases bias and increases response diversity. Du et al. [2023](#bib.bib3)
    evaluates a different variant of multi-agent debate where multiple models generate
    their own responses, and each model receives the opinions of the other models,
    then updates its response if necessary. This is done for multiple rounds. Du et al.
    [2023](#bib.bib3) evaluates the approach on the following tasks: Biography generation,
    MMLU, Chess move validity, Arithmetic, Grade school math, and Chess move optimality.
    Their approach using ChatGPT and Bard outperforms single-agent on all the tasks.
    To evaluate LLM responses Chan et al. [2023](#bib.bib1) presents another variant
    of multi-agent debate. Their architecture involves assigning agents different
    roles such as General Public, Critic, Psychologist, News Author, and Scientist.
    They used ChatGPT and GPT-4 for their evaluation on FairEval (Wang et al. [2023a](#bib.bib18))
    dataset and achieved a Cohen’s Kappa score of 0.40 using LLM Debate, 0.03 more
    than the single agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this study, we perform experiments on the three categories of models given
    in Table [1](#S3.T1 "Table 1 ‣ 3.1 Experimental Setup ‣ 3 Methodology ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). For the evaluation module, we evaluate single agents and multi-agent
    frameworks on four datasets, Alpaca Eval (Li et al. [2023](#bib.bib9)), FairEval
    (Wang et al. [2023a](#bib.bib18)), PandaLM-Eval (Wang et al. [2024](#bib.bib20),
    [2023b](#bib.bib19)) and MT-Bench (Zheng et al. [2023b](#bib.bib22)). For the
    generation module, we compare the multi-agent frameworks using win rate - the
    ratio of times a generation framework is selected as the best by an LLM evaluator
    when comparing outputs from all generation workflows. After the extensive evaluation
    of both modules, we used the picked strategies to generate synthetic PO datasets.
    We set the temperature to 0 in all our evaluations to ensure reproducibility.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Models |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Small-Scale LLM | Llama-3.1-8b |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b |'
  prefs: []
  type: TYPE_TB
- en: '| Mid-Scale LLM | Gemma-2-27b |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-70b |'
  prefs: []
  type: TYPE_TB
- en: '| Large-Scale LLM | GPT-4o-Mini (2024-07-18) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o (2024-05-13) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Categories of LLMs used in the study.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 LLM-as-Evaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the aim of automating the evaluation component of PO dataset generation,
    we assess the performance of LLMs in the role of evaluators using the Alpaca Eval,
    FairEval, PandaLM-Eval, and MT-Bench datasets. Our goal is to determine whether
    multi-agent workflows work better than a single agent for LLM evaluation. The
    system prompts for this task are modified version of the prompts used by Zheng
    et al. [2023a](#bib.bib21) and are given in Appendix [A](#A1 "Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-Judge.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate six different LLMs on the Alpaca Eval dataset, calculating Cohen’s
    Kappa with the human annotations. Our evaluation involved three distinct prompting
    strategies for the LLM-as-a-Judge:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Direct Comparison: The Judge-LLM is provided with the user question and the
    responses generated by different LLMs. It is asked to pick the best response among
    the given options.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Independent Scoring: The Judge-LLM is given the user question and each response
    in separate conversations. It is asked to score each response independently.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Combined Scoring: The Judge-LLM is provided with the user question and all
    the responses in a single conversation thread. It is asked to assign a score to
    each response within the same conversation context. To observe if the scoring
    range influences the LLM’s scoring consistency and its alignment with human annotations,
    we test three different scoring totals: 5, 10, and 100.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For each of these prompting strategy, we systematically analyze the performance
    of the LLMs by calculating Cohen’s Kappa, against the human annotations. The system
    prompts are given in Table [7](#A1.T7 "Table 7 ‣ Appendix A System Prompts ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation") in Appendix [A](#A1 "Appendix A System Prompts ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs-as-Jury.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We extend the evaluation from the LLM-as-a-Judge approach by forming juries
    composed of multiple LLMs. We test all possible permutations of the jury configurations.
    We use three datasets: FairEval, PandaLM-Eval and MT-Bench datasets for a more
    comprehensive analysis. We systematically analyze the performance of each jury
    configuration, focusing on how the size and combination of the LLMs affect their
    judgment accuracy. The Combined Scoring system prompt in Table [7](#A1.T7 "Table
    7 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") in Appendix [A](#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") is used for all the
    jurors because it performed the best in our previous evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Debate.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We also evaluate the LLM Debate framework following the implementation described
    by Chan et al. [2023](#bib.bib1). In this approach, we assign three distinct roles—Psychologist,
    General Public, and Critic—and the three agents debate the scores that should
    be assigned to candidate responses. After the debate, each agent gives its final
    score which is used to determine which candidate response they vote for. These
    votes are then used to pick the best response. This strategy is evaluated using
    the FairEval, PandaLM-Eval, and MT-Bench benchmarks. Figure [1](#S3.F1 "Figure
    1 ‣ LLM Debate. ‣ 3.2 LLM-as-Evaluator ‣ 3 Methodology ‣ The Fellowship of the
    LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    illustrates the debate workflow employed in our study. The system prompt, the
    user message structure and the prompts for the roles used are given in Table [8](#A1.T8
    "Table 8 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") and Table
    [9](#A1.T9 "Table 9 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    in Appendix [A](#A1 "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1fb608703f54e839fc2b1f7843b65f59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: LLM Debate for evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 LLM-as-Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the LLM Feedback Loop workflow for the generation module, we test
    different configurations using Llama-3.1-8b (Meta [2024](#bib.bib11)) and Gemma-2-9b
    (Google [2024](#bib.bib5)) models. In this framework, a generator LLM produces
    a response, which is then evaluated by a feedback LLM that provides improvement
    suggestions as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.3 LLM-as-Generator ‣ 3
    Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation"). The generator revises the response
    based on these suggestions, and the process repeats for multiple iterations. The
    system prompt for the generator and reviewer is given in Table [10](#A1.T10 "Table
    10 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") and [11](#A1.T11 "Table
    11 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") in Appendix [A](#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). We calculate the win
    rate against single-agent GPT-4o (OpenAI [2024](#bib.bib12)), Llama-3.1-8b and
    Gemma-2-9b baseline outputs on a subset of 500 prompts from the Argilla Capybara
    DPO dataset²²2https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized
    to identify the best configuration. We test the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Same Model as Both Agents: Gemma-2-9b or Llama-3.1-8b as both the feedback
    and generation agent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Different Models for Each Agent: Gemma-2-9b as the feedback agent and Llama-3.1-8b
    as the generation agent, or vice versa.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Both Models for Feedback, One for Generation: Gemma-2-9b or Llama-3.1-8b as
    the generation agent, with both models as feedback agents.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/082528037b479c44def78f56fb661d1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: LLM Feedback Loop for response generation'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Preference Optimization Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use Llama-3.1-8b and Gemma-2-9b in the generation module and GPT-4o in the
    evaluation module to generate multiple DPO datasets and KTO datasets for single-agent
    improvement and multi-agent improvement. The prompts used for single-agent improvement
    dataset generation are given in Table [7](#A1.T7 "Table 7 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation"), [10](#A1.T10 "Table 10 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") and [11](#A1.T11 "Table 11 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") in Appendix [A](#A1 "Appendix A System Prompts
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"). The prompt used for multi-agent improvement dataset generation
    is given in Table [12](#A1.T12 "Table 12 ‣ Appendix A System Prompts ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") in Appendix [A](#A1 "Appendix A System Prompts ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). The evaluation code, all the evaluation outputs and the generated
    datasets are publicly available on GitHub³³3https://github.com/ulrs0/MA-PO.'
  prefs: []
  type: TYPE_NORMAL
- en: Single-Agent Improvement.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use the prompts from Argilla Capybara DPO dataset. The Feedback Loop framework
    generates $N$ is the number of iterations). LLM-as-Evaluator picks the best response
    from the candidates to create the DPO and KTO dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Agent Improvement.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use the prompts and the human-generated responses from the No Robots dataset
    (Rajani et al. [2023](#bib.bib15)). The evaluator is given the human response
    (reference response) and LLM response and is asked to generate feedback based
    on the reference response. The generated feedback and the human response are used
    to create the PO dataset. The goal of this dataset is to improve both response
    generation and response evaluation. The structure of the dataset is given in Figure
    [3](#S3.F3 "Figure 3 ‣ Multi-Agent Improvement. ‣ 3.4 Preference Optimization
    Dataset ‣ 3 Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e033ba4c6cae36739220d5850c207ecc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Dataset structure for the mult-agent improvement PO dataset. The
    blue boxes represent the accepted responses and the yellow boxes represent the
    rejected responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 LLM-as-Evaluator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompting Strategies.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣
    4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation") shows the results of LLM-as-a-Judge
    approach on the three prompting strategies. The Independent Scoring prompt strategy
    consistently under-performs compared to the Direct Comparison and Combined Scoring
    approaches across all evaluated LLMs. This result is reflected in lower Cohen’s
    Kappa values in Table [2](#S4.T2 "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator
    ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). In evaluating responses
    in isolation the LLM has to re-calibrate its scoring mechanism for every new response.
    This can lead to inconsistencies, especially when multiple responses are closely
    matched in quality. Due to the low Kappa values observed, we opted not to conduct
    experiments with the scoring-out-of-5 and 100 scales for Independent Scoring.'
  prefs: []
  type: TYPE_NORMAL
- en: The Direct Comparison Strategy performs better than the Independent Scoring
    approach across most LLMs, with a notable improvement for GPT-4o (0.372 vs. 0.249)
    and GPT-4o-mini (0.342 vs. 0.254). However, it generally falls short when compared
    to the Combined Scoring method, where GPT-4o achieves a score of 0.401 using the
    scoring-out-of-100 scale. The higher Cohen’s Kappa values indicate that the Direct
    Comparison and Combined Scoring strategy benefits from providing the LLM with
    a side-by-side evaluation of responses, allowing for more accurate and consistent
    judgments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Combined Scoring strategy, as presented in Table [2](#S4.T2 "Table 2 ‣
    Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"), shows consistent performance using all the scoring scales.
    It outperforms both the other prompts. The scoring scales of 5, 10, and 100 show
    variability across different models, with certain scales performing better for
    some models than others. For example, GPT-4o performs the best in scoring-out-of-10
    scale with a Kappa score of 0.382 while Gemma-2-9b performs best under scoring-out-of-5
    scale. Given these results, we selected the scoring-out-of-10 scale as the most
    effective option for the Combined Scoring approach. We use this prompt for all
    our further evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Comp. | Ind. | Combined |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Judge |  | 10 | 5 | 10 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b | 0.226 | 0.170 | 0.243 | 0.254 | 0.233 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-8b | 0.265 | 0.181 | 0.255 | 0.240 | 0.242 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-27b | 0.233 | 0.173 | 0.284 | 0.266 | 0.252 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-70b | 0.305 | 0.214 | 0.337 | 0.333 | 0.339 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o-mini | 0.342 | 0.254 | 0.374 | 0.382 | 0.347 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 0.372 | 0.249 | 0.393 | 0.382 | 0.401 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Performance comparison of LLM-as-a-Judge on Alpaca-Eval using different
    prompting strategies. Direct Comparison (Comp.) vs. Independent Scoring (Ind.)
    vs. Combined Scoring (Combined). The bold values indicate the highest Cohen’s
    kappa values for a particular strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-Judge.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The LLM-as-Judge evaluations, as shown in Table [2](#S4.T2 "Table 2 ‣ Prompting
    Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"), indicate that GPT-4o outperforms all the models on PandaLM-Eval
    and MT-Bench achieving a Cohen’s Kappa score of 0.688 and 0.410 respectively.
    Additionally, GPT-4o consistently ranks in second position across all three datasets.
    This consistent top-tier performance underscores GPT’s effectiveness as a reliable
    judge in evaluating LLM responses. Gemma-2-27b outperforms all other models on
    the Fair-Eval dataset, achieving the highest score in this particular evaluation.
    However, it’s important to note that the Fair-Eval dataset is relatively small,
    consisting of only 80 samples. Furthermore, the Fair-Eval dataset primarily compares
    GPT-3.5-Turbo with Vicuna-13b, which might introduce a bias in favor of GPT models
    when GPT is the evaluator. Figure [4](#S4.F4 "Figure 4 ‣ LLM-as-a-Judge. ‣ 4.1
    LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") shows that
    GPT-4o selects GPT-3.5-Turbo as the better agent 50 times and Vicuna-13b 30 times.
    This indicates a potential bias in favor of GPT responses when GPT-4o is the evaluator.
    Additionally, we can observe in the figure that Llama models also display a similar
    bias towards GPT responses, whereas Gemma models do not exhibit this bias, suggesting
    that Gemma is more impartial in its evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Judge |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b | 0.279 | 0.595 | 0.354 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-8b | 0.206 | 0.523 | 0.339 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-27b | 0.389 | 0.586 | 0.354 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-70b | 0.257 | 0.597 | 0.387 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o-mini | 0.333 | 0.613 | 0.388 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 0.327 | 0.688 | 0.410 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance comparison of LLM-as-a-Judge on Alpaca-Eval using different
    prompting strategies. Direct Comparison vs. Independent Scoring (out of 10) vs.
    Combined Scoring (out of 5, 10 and 100).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0803c477727bc09f4e7058ea4fbed36d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Number of times GPT-3.5-Turbo and Vicuna-13b are picked by each LLM
    Judge.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs-as-a-Jury.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In evaluating of LLMs-as-a-Jury, we analyze the top three juries from each
    dataset as shown in Table [4](#S4.T4 "Table 4 ‣ LLMs-as-a-Jury. ‣ 4.1 LLM-as-Evaluator
    ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). Notably, the scores
    exhibit considerable variation across the different datasets. On the Fair-Eval
    and MT-Bench datasets, the jury approach outperformed the judge approach, indicating
    a potential advantage in using multiple models for evaluation. For instance, on
    Fair-Eval, the highest-performing jury achieves a Cohen’s Kappa of 0.428 while
    the judge achieves Kappa of 0.389, suggesting a relatively strong agreement with
    human judgments compared to individual judges. This configuration, however, shows
    a drop in performance on other datasets with a kappa of 0.604 on PandaLM-Eval
    and 0.395 on MT-Bench, underscoring the challenge of generalizing a single jury
    setup across varied datasets. However, the judge approach outperforms the jury
    on the PandaLM-Eval dataset, where the best judge attained a kappa of 0.688, surpassing
    the top jury’s kappa of 0.673\. The best jury on MT-Bench, with a kappa of 0.429,
    also demonstrates variability in its performance across datasets as well, with
    a kappa of 0.636 on PandaLM-Eval and only 0.273 on Fair-Eval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The jury approach, by incorporating diverse models, mitigates the biases that
    occur in LLM-as-a-Judge approach (as shown in Figure [4](#S4.F4 "Figure 4 ‣ LLM-as-a-Judge.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation"))
    when bench-marking on the Fair-Eval dataset. However while the jury approach can
    offer robustness through diversity, in evaluation task, it does not universally
    outperform single judges. The decision to employ a jury versus a judge should
    consider whether the candidate responses being evaluated include output from the
    judge itself, which can introduce bias in the results. Additionally, scalability
    should be taken into account, as the jury approach might require more computational
    resources. Another critical consideration is the variability in performance across
    different datasets, which poses a challenge for generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  prefs: []
  type: TYPE_TB
- en: '| Jury |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b, Gemma-2-27b, Llama-3.1-8b, GPT-4o-mini | 0.428 | 0.604 | 0.395
    |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b, Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.415 | 0.639 | 0.418 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-27b, Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.412 | 0.637 | 0.410 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.396 | 0.673 | 0.400 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.365 | 0.663 | 0.410 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b, GPT-4o-mini, GPT-4o | 0.375 | 0.662 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-70b, GPT-4o | 0.273 | 0.636 | 0.429 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o-mini, GPT-4o | 0.315 | 0.660 | 0.426 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b, GPT-4o | 0.290 | 0.609 | 0.422 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison of LLMs-as-a-Jury on the three datasets. For
    each dataset, we pick the top 3 juries. The bold score is for the best jury for
    the specific dataset and the underlined one is the second best.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Debate.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The LLM Debate approach, as summarized in Table [5](#S4.T5 "Table 5 ‣ LLM Debate.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation"),
    showcases varying degrees of effectiveness across three different datasets: Fair-Eval,
    PandaLM-Eval, and MT-Bench. GPT-4o performs the best across all datasets, with
    Cohen’s Kappa scores of 0.404, 0.654, and 0.402 respectively. LLM Debate outperforms
    LLM-as-a-Judge on Fair-Eval only and does not surpass the LLMs-as-a-Jury approach
    on any dataset. On Fair-Eval using the Debate framework increases the Kappa score
    of GPT-4o from 0.327 to 0.404 and of GPT-4o-mini from 0.333 to 0.360\. It shows
    that the debate approach decreases the bias of GPT-4o and GPT-4o-mini towards
    the responses of it’s family.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a significant variance in the performance of LLM Debate across the
    models and the datasets. For instance, as seen in Table [5](#S4.T5 "Table 5 ‣
    LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") Gemma-2-27b in debate architecture outperforms Gemma-as-a-Judge on
    PandaLM-Eval and MT-Bench but on Fair-Eval judge performers better. Gemma-2-9b
    in debate architecture has a Kappa score of 0.323 on Fair-Eval, outperforming
    0.279 of Gemma-as-a-Judge. However on PandaLM-Eval and MT-Bench Gemma-2-9b in
    debate framework achieves a Kappa score of 0.520 and 0.326, repectively. Both
    scores lower as compared to Gemma-as-a-Judge scores of 0.595 and 0.354\. In case
    of Llama, Llama-3.1-8b in judge configuration outperforms itself in debate configuration.
    Llama-3.1-70b in debate framework only outperforms Llama-as-a-judge on Fair-Eval.
    Figure [5](#S4.F5 "Figure 5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and
    Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") shows a comparison of Cohen’s Kappa of LLM Debate
    and LLM-as-a-Judge across the three datasets and all the models.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Debater |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-9b | 0.323 | 0.520 | 0.326 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-8b | 0.080 | 0.440 | 0.309 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2-27b | 0.336 | 0.605 | 0.363 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-3.1-70b | 0.292 | 0.547 | 0.381 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o-mini | 0.360 | 0.625 | 0.376 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 0.404 | 0.654 | 0.402 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Performance comparison of LLM Debate on the three datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c1a734471fb93f29e4fae79555da3e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of LLM Debate and LLM-as-a-Judge across the three datasets
    and different models.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Framework for PO Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the comparative evaluation scores across the three datasets and the
    advantages and disadvantages associated with each multi-agent framework, we have
    chosen to use the LLM-as-a-Judge approach with GPT-4o as our primary evaluator
    for generating the PO dataset. This decision is driven by multiple factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our context, the task involves generating a PO dataset using Llama-3.1-8b
    and Gemma-2-9b. Therefore there will be no bias in the evaluation when using GPT-4o
    as the judge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The performance of GPT-4o-as-a-Judge has been consistently high across various
    evaluations, indicating its reliability as a judge. While the LLMs-as-a-Jury and
    LLM Debate approaches have a high variance in Cohen’s Kappa score across different
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The computational resources required for managing the LLM Debate and LLM Jury
    frameworks are considerably higher than those needed for a single-judge setup.
    The LLM-as-a-Judge method is simpler to implement and scale.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 LLM-as-Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare the performance of Multi-Agent Feedback Loop with the baseline Single-Agents
    (GPT-4o, Llama-3.1-8b, Gemma-2-9b) using win rate as shown in Table [6](#S4.T6
    "Table 6 ‣ 4.2 LLM-as-Generator ‣ 4 Results and Discussion ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). We utilize GPT-4o-as-a-judge in this evaluation process. For the
    baseline we find the win rate of Gemma and Llama against GPT-4o and each other.
    Both smaller models have similar win rate of 38.6% and 39.2% against GPT, while
    Gemma has a win rate of 66.6% against Llama.'
  prefs: []
  type: TYPE_NORMAL
- en: In the Multi-Agent setting, all variations outperform the single-agents against
    GPT-4o, with the highest win rate of 49.0% for Llama as a generator and Gemma
    as a reviewer. This configuration performs the best against Llama and Gemma too,
    with 71.8% and 73.8% win rate respectively. We observe that using Llama as the
    generator improves the performance as compared to using Gemma as the generator
    because this configuration leads to a better win rate against all three baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Llama’s strengths in generating responses may be enhanced by Gemma’s ability
    to fine-tune and correct the errors, leading to more polished outputs. The results
    underscore the importance of assigning appropriate roles based on the specific
    strengths of each model. Llama, when set as the generator, appears to leverage
    its capabilities more effectively than Gemma in this role. The use of diverse
    models in the feedback loop likely helps mitigate biases that any single model
    might introduce. This diversity ensures a broader range of perspectives while
    answer a question. In conclusion, the demonstrated efficacy of the Multi-Agent
    Feedback Loop, especially with Llama as the generator and Gemma as the reviewer,
    validates the concept of collaborative AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Win Rate (%) Against |'
  prefs: []
  type: TYPE_TB
- en: '| Generator | Reviewer | GPT | Llama | Gemma |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma | - | 38.6 | 66.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Llama | - | 39.2 | - | 33.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma | Gemma | 41.4 | 64.8 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Llama | 41.2 | 61.8 | 47.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Gemma + Llama | 42.0 | 67.6 | 52.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama | Gemma | 49.0 | 71.8 | 73.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Llama | 47.8 | 65.8 | 65.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Gemma + Llama | 48.6 | 68.2 | 69.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Win Rate of Multi-Agent and Single-Agent against GPT-4o, Llama-3.1-8b
    and Gemma-2-9b'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Preference Optimization Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Single-Agent Improvement.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the Single-Agent improvement dataset generation we use GPT-4o-as-a-Judge
    in the evaluation module. In the generation module, we use LLM Feedback Loop with
    Llama-3.1-8b as the generator and Gemma-2-9b as the reviewer. The framework is
    shown in Figure [6](#S4.F6 "Figure 6 ‣ Single-Agent Improvement. ‣ 4.3 Preference
    Optimization Dataset ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation").
    For the dataset generation, we use $N=3$ iterations. We pick the best response
    (judged by GPT-4o) as accepted and the other 2 responses as rejected and generate
    two datasets, one for DPO and one for KTO.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94bcdaac3032df199db2173448a407f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Multi-agent framework for PO dataset generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Agent Improvement.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the Multi-Agent improvement dataset generation we use Gemma-2-9b because
    the win rate for Gemma against Llama-3.1-8b is 66.6%. We give Gemma user prompt,
    human response and it’s own response and ask it to generate feedback based on
    the human reference. For the rejected responses we use the feedback without human
    reference. We generate two datasets, one for DPO and one for KTO.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we combine the two categories of dataset to make our final PO dataset
    to improve LLM for single-agent setting and LLM for multi-agent setting.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents PO datasets generated using multi-agent frameworks, and
    evaluates these frameworks by highlighting the advantages, drawbacks, and challenges
    of each approach. In the response evaluation module, our comparative analysis
    of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate shows the suitability of each
    setup depending on the context of use. For the response generation module, we
    evaluate the LLM Feedback loop using Llama-3.1-8b and Gemma-2-9b in various configurations.
    LLM-as-a-Judge proved to be highly effective when candidate responses don’t have
    a response from the Judge LLM. Whereas LLMs-as-a-Jury and LLM Debate demonstrated
    robustness, particularly useful in reducing evaluator bias. However, Cohen’s Kappa
    for both of these approaches has a high variance making them less suitable for
    novel applications.
  prefs: []
  type: TYPE_NORMAL
- en: Our experiments with LLM Feedback Loop using Llama-3.1-8b and Gemma-2-9b configurations
    show the potential of multi-agent frameworks in refined content generation. Configurations
    where Llama-3.1-8b served as the generator and Gemma-2-9b as the reviewer consistently
    delivered better results, demonstrating the benefits of leveraging complementary
    strengths of different models to refine output quality. These findings indicate
    the effectiveness of multi-agent frameworks for varied AI applications, showing
    promise for moving towards systems requiring minimal human intervention - however,
    this method is computationally expensive in comparison.
  prefs: []
  type: TYPE_NORMAL
- en: We also generate multiple DPO and KPO datasets using LLM Feedback Loop with
    Llama-3.1-8b as the generator and Gemma-2-9b as the evaluator and GPT-4o-as-a-Judge.
    The aim of these datasets is to improve single-agent capabilities for better response
    generation and multi-agent capabilities including better communication and improved
    feedback.
  prefs: []
  type: TYPE_NORMAL
- en: In order to facilitate further research and ensure transparency, all code, LLM
    responses, and generated datasets have been made public.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In terms of future work, there are three avenues of investigation: (1) Performance
    comparison of models fine-tuned on our PO dataset versus widely-used LLMs to investigate
    the impact of our generated datasets through a series of experiments. (2) Using
    larger models such as Llama-3.1-70b and Gemma-2-27b for dataset generation as
    this may provide more diverse and higher-quality training data, potentially leading
    to further advancements in model performance and generalizability. (3) Experimenting
    with the number of iterations used in the Feedback Loop framework and including
    other LLM families in the dataset generation process.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chan et al. (2023) Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang, S.;
    Fu, J.; and Liu, Z. 2023. ChatEval: Towards Better LLM-based Evaluators through
    Multi-Agent Debate. arXiv:2308.07201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2017) Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.;
    Legg, S.; and Amodei, D. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2023) Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch,
    I. 2023. Improving Factuality and Reasoning in Language Models through Multiagent
    Debate. arXiv:2305.14325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ethayarajh et al. (2024) Ethayarajh, K.; Xu, W.; Muennighoff, N.; Jurafsky,
    D.; and Kiela, D. 2024. KTO: Model Alignment as Prospect Theoretic Optimization.
    arXiv:2402.01306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google (2024) Google. 2024. Google Gemma 2. https://blog.google/technology/developers/google-gemma-2/.
    Accessed: 2024-08-16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020) He, J.; Wang, T.; Xiong, D.; and Liu, Q. 2020. The Box is
    in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation. In
    Cohn, T.; He, Y.; and Liu, Y., eds., *Findings of the Association for Computational
    Linguistics: EMNLP 2020*, 3662–3672\. Online: Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong, Lee, and Thorne (2024) Hong, J.; Lee, N.; and Thorne, J. 2024. ORPO:
    Monolithic Preference Optimization without Reference Model. arXiv:2403.07691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu,
    B.; Gonzalez, J. E.; and Stoica, I. 2024. From Crowdsourced Data to High-Quality
    Benchmarks: Arena-Hard and BenchBuilder Pipeline. arXiv:2406.11939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin,
    C.; Liang, P.; and Hashimoto, T. B. 2023. AlpacaEval: An Automatic Evaluator of
    Instruction-following Models. https://github.com/tatsu-lab/alpaca˙eval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2024) Liang, T.; He, Z.; Jiao, W.; Wang, X.; Wang, R.; Yang, Y.;
    Tu, Z.; and Shi, S. 2024. Encouraging Divergent Thinking in Large Language Models
    through Multi-Agent Debate. arXiv:2305.19118.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta (2024) Meta. 2024. Meta LLaMA 3. https://ai.meta.com/blog/meta-llama-3/.
    Accessed: 2024-08-16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2024) OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Petroni et al. (2021) Petroni, F.; Piktus, A.; Fan, A.; Lewis, P.; Yazdani,
    M.; Cao, N. D.; Thorne, J.; Jernite, Y.; Karpukhin, V.; Maillard, J.; Plachouras,
    V.; Rocktäschel, T.; and Riedel, S. 2021. KILT: a Benchmark for Knowledge Intensive
    Language Tasks. arXiv:2009.02252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2024) Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
    C. D.; and Finn, C. 2024. Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model. arXiv:2305.18290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajani et al. (2023) Rajani, N.; Tunstall, L.; Beeching, E.; Lambert, N.; Rush,
    A. M.; and Wolf, T. 2023. No Robots. https://huggingface.co/datasets/HuggingFaceH4/no˙robots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tamkin et al. (2021) Tamkin, A.; Brundage, M.; Clark, J.; and Ganguli, D. 2021.
    Understanding the capabilities, limitations, and societal impact of large language
    models. *arXiv preprint arXiv:2102.02503*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verga et al. (2024) Verga, P.; Hofstatter, S.; Althammer, S.; Su, Y.; Piktus,
    A.; Arkhangorodsky, A.; Xu, M.; White, N.; and Lewis, P. 2024. Replacing Judges
    with Juries: Evaluating LLM Generations with a Panel of Diverse Models. arXiv:2404.18796.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Wang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y.; Liu,
    Q.; Liu, T.; and Sui, Z. 2023a. Large Language Models are not Fair Evaluators.
    *ArXiv*, abs/2305.17926.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Heng, Q.; Wang, C.;
    Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang,
    Y. 2023b. PandaLM: Reproducible and Automated Language Model Assessment. https://github.com/WeOpenML/PandaLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.;
    Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang, Y. 2024.
    PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023a) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.;
    and Stoica, I. 2023a. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.
    arXiv:2306.05685.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023b) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.;
    and Stoica, I. 2023b. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.
    arXiv:2306.05685.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Álvaro Bartolomé Del Canto et al. (2024) Álvaro Bartolomé Del Canto; Blázquez,
    G. M.; Lajarín, A. P.; and Suero, D. V. 2024. Distilabel: An AI Feedback (AIF)
    framework for building datasets with and for LLMs. https://github.com/argilla-io/distilabel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A System Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [7](#A1.T7 "Table 7 ‣ Appendix A System Prompts ‣ The Fellowship of the
    LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    contains the three categories of system prompts tested for LLM-as-a-Judge approach.
    The winning prompt with Combined Scoring was used for LLMs-as-a-Jury. These prompts
    are modified versions of those used by (Zheng et al. [2023a](#bib.bib21)). Table
    [8](#A1.T8 "Table 8 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    present the system prompt and user message structure for LLM Debate and [9](#A1.T9
    "Table 9 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") shows the
    prompt for each role in the debate. This is based on the system prompt and the
    input structure used by (Chan et al. [2023](#bib.bib1)). Table [10](#A1.T10 "Table
    10 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") shows the user message
    structure for the generator LLM and Table [11](#A1.T11 "Table 11 ‣ Appendix A
    System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation") shows the system prompt and user
    message for reviewer LLM in LLM Feedback Loop. Table [12](#A1.T12 "Table 12 ‣
    Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") shows the system prompt
    and user message structure used for Multi-Agent improvement dataset generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The three types of system prompts for LLM-as-a-Judge and LLMs-as-a-Jury.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Direct Comparison | Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question displayed
    below. You should choose the assistant that follows the user’s instructions and
    answers the user’s questions better. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of their responses. Begin your evaluation by comparing the two responses and provide
    a short explanation. Avoid any position biases and ensure that the order in which
    the responses were presented does not influence your decision. Do not allow the
    length of the responses to influence your evaluation. Answer options: A: If response
    by assistant A is better'
  prefs: []
  type: TYPE_NORMAL
- en: 'B: If response by assistant B is better'
  prefs: []
  type: TYPE_NORMAL
- en: 'C: If it is a tie'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following format to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Evidence:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your explanation here]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A or B or C |
  prefs: []
  type: TYPE_NORMAL
- en: '| Independent Scoring | Please act as an impartial judge and evaluate the quality
    of the response provided by an AI assistants to the user question displayed below.
    Assign an overall score out of 10, where a higher score indicates better overall
    performance. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their response.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Do not allow the length of the response to influence your evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following format to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Evidence:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your explanation here]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall Score:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: X/10 |
  prefs: []
  type: TYPE_NORMAL
- en: '| Combined Scoring | Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question displayed
    below. You should choose the assistant that follows the user’s instructions and
    answers the user’s questions better. Each response receives an overall score out
    of 10, where a higher score indicates better overall performance. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of their responses. Begin your evaluation by comparing the
    two responses and provide a short explanation. Avoid any position biases and ensure
    that the order in which the responses were presented does not influence your decision.
    Do not allow the length of the responses to influence your evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following format to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Evidence:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your explanation here]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Score Assistant A:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: X/10
  prefs: []
  type: TYPE_NORMAL
- en: 'Score Assistant B:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Y/10 |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: The system prompt and the user message structure for LLM Debate.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Message Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| System Prompt | We would like to request your feedback on the performance
    of two AI assistants in response to the user question. There are a few other referee
    assigned the same task, it’s your responsibility to discuss with them and think
    critically before you make your final judgement.Each response receives an overall
    score on a scale of 1 to 10, where a higher score indicates better overall performance.
    You should choose the assistant that follows the user’s instructions and answers
    the user’s question better. You don’t necessarily have to agree with others.Your
    evaluation should consider factors such as the helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of their responses. Avoid any position
    biases and ensure that the order in which the responses were presented does not
    influence your decision. Do not allow the length of the responses to influence
    your evaluation. |'
  prefs: []
  type: TYPE_TB
- en: '| User Message | <&#124;Start of User Question&#124;> {User Question} <&#124;End
    of User Question&#124;>'
  prefs: []
  type: TYPE_NORMAL
- en: <&#124;The Start of Assistant 1’s Answer&#124;> {Assistant 1}
  prefs: []
  type: TYPE_NORMAL
- en: <&#124;The End of Assistant 1’s Answer&#124;>
  prefs: []
  type: TYPE_NORMAL
- en: <&#124;The Start of Assistant 2’s Answer&#124;> {Assistant 2}
  prefs: []
  type: TYPE_NORMAL
- en: '<&#124;The End of Assistant 2’s Answer&#124;> Here is your discussion history:'
  prefs: []
  type: TYPE_NORMAL
- en: '{Chat History}'
  prefs: []
  type: TYPE_NORMAL
- en: '{Role} |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: The prompt for each role used in LLM Debate.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Role | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| General Public | You are now General Public, one of the referees in this
    task. You are interested in the story and looking for updates on the investigation.
    Please think critically by yourself and note that it’s your responsibility to
    choose one of which is the better first.'
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s your turn to speak General Public, please make your talk short and
    clear.
  prefs: []
  type: TYPE_NORMAL
- en: '**General Public**: |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Psychologist | You are now Psychologist, one of the referees in this task.
    You will study human behavior and mental processes in order to understand and
    explain human behavior. Please help other people to determine which response is
    the better one.'
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s your turn to speak Psychologist, please make your talk short and clear.
  prefs: []
  type: TYPE_NORMAL
- en: '**Psychologist**: |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Critic | You are now Critic, one of the referees in this task. You will check
    fluent writing, clear sentences, and good wording in summary writing. Your job
    is to question others judgement to make sure their judgement is well-considered
    and offer an alternative solution if two responses are at the same level.'
  prefs: []
  type: TYPE_NORMAL
- en: Now it’s your turn to speak Critic, please make your talk short and clear.
  prefs: []
  type: TYPE_NORMAL
- en: '**Critic**:” |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: The user message structure for the generator in LLM Feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Message Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| User Message (Single Feedback) | Update your response based on the feedback:
    [Start of Feedback]'
  prefs: []
  type: TYPE_NORMAL
- en: '{Feedback}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of Feedback]'
  prefs: []
  type: TYPE_NORMAL
- en: Do not engage in formalities such as ’Thank you for your feedback’ or ’Here
    is an updated version…’ etc, just update the response. |
  prefs: []
  type: TYPE_NORMAL
- en: '| User Message (Double Feedback) | Update your response based on the feedback
    by the two assistant: [Start of Assistant 1’s Feedback]'
  prefs: []
  type: TYPE_NORMAL
- en: '{Assistant 1’s Feedback}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of Assistant 1’s Feedback]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Start of Assistant 2’s Feedback]'
  prefs: []
  type: TYPE_NORMAL
- en: '{Assistant 2’s Feedback}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of Assistant 2’s Feedback]'
  prefs: []
  type: TYPE_NORMAL
- en: Do not engage in formalities such as ’Thank you for your feedback’ or ’Here
    is an updated version…’ etc, just update the response. |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: The prompt and user message structure for the reviewer in LLM Feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Message Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| System Prompt | Please give constructive feedback on how to improve the response
    provided by an AI assistant to the user question. Your evaluation should consider
    factors such as the instruction following (the response should align with the
    user instructions), helpfulness, relevance, accuracy, and creativity of the response.'
  prefs: []
  type: TYPE_NORMAL
- en: Assign an overall score out of 10, up to one decimal place, where a higher score
    indicates better overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following format to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your evaluation here]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall Score:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: X/10
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your feedback here] |'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Message | [Start of User Question] {User Question}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of User Question]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Start of Assistant’s Response]'
  prefs: []
  type: TYPE_NORMAL
- en: '{Assistant’s Response}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of Assistant’s Response] |'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: The prompt and user message structure for the feedback generation
    for Multi-Agent improvement dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Message Type | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| System Prompt | Please give constructive feedback on how to improve the response
    provided by an AI assistant to the user question. Your evaluation should consider
    factors such as the instruction following (the response should align with the
    user instructions), helpfulness, relevance, accuracy, and creativity of the response.
    Use the human answer as the reference answer, so that the evaluation and feedback
    is based on how to get closer to human answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assign an overall score out of 10, up to one decimal place, where a higher
    score indicates better overall performance. Use the following format to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your evaluation here]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall Score:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: X/10
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Add your feedback here] |'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Message | [Start of User Question] {User Question}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of User Question]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Start of Human’s Response]'
  prefs: []
  type: TYPE_NORMAL
- en: '{Human’s Response}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of Human’s Response]'
  prefs: []
  type: TYPE_NORMAL
- en: '[Start of Assistant’s Response]'
  prefs: []
  type: TYPE_NORMAL
- en: '{Assistant’s Response}'
  prefs: []
  type: TYPE_NORMAL
- en: '[End of Assistant’s Response]'
  prefs: []
  type: TYPE_NORMAL
- en: Base your evaluation and feedback on human response. We want the LLM to mimic
    the human response. Do not mention the word ’human response’ in your evaluation
    and feedback. The user should not know that you have a reference answer. |
  prefs: []
  type: TYPE_NORMAL
