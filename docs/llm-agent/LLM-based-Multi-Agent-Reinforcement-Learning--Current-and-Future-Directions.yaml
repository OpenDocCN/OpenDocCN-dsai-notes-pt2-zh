- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.11106](https://ar5iv.labs.arxiv.org/html/2405.11106)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chuanneng Sun, , Songjun Huang, ,
  prefs: []
  type: TYPE_NORMAL
- en: 'and Dario Pompili The authors are with the Department of Electrical and Computer
    Engineering, Rutgers University–New Brunswick, NJ, USA. Emails: *{chuanneng.sun,
    songjun.huang, pompili}@rutgers.edu* This work was supported by the NSF RTML Award
    No. CCF-1937403.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, Large Language Models (LLMs) have shown great abilities in
    various tasks, including question answering, arithmetic problem solving, and poem
    writing, among others. Although research on LLM-as-an-agent has shown that LLM
    can be applied to Reinforcement Learning (RL) and achieve decent results, the
    extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many
    aspects, such as coordination and communication between agents, are not considered
    in the RL frameworks of a single agent. To inspire more research on LLM-based
    MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent
    RL frameworks and provide potential research directions for future research. In
    particular, we focus on the cooperative tasks of multiple agents with a common
    goal and communication among them. We also consider human-in/on-the-loop scenarios
    enabled by the language component in the framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multi-Agent Reinforcement Learning, Language Models, Multi-Agent Systems.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-Agent Reinforcement Learning (MARL) has emerged as a popular approach
    to address the coordination problem in Multi-Agent Systems (MAS). As opposed to
    Individual Reinforcement Learning (IRL)-based or traditional optimization-based
    solutions, MARL has shown a significant improvement in scalability and robustness
    to uncertainty and dynamicity [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]. This improvement is largely attributed to the communication and
    coordination among agents inherent in MARL, where multiple agents learn and adapt
    their policies simultaneously while interacting within a shared environment and
    communicating with others. However, how and what to communicate among the agents
    in the MAS remains to be explored. Representative examples include MARL frameworks
    that learn to generate numerical messages using neural networks, formulate neural
    communication protocols, and learn targeted ad hoc communications. Despite the
    decent performance of the MARL frameworks achieved in various applications, they
    still underperform human experts. As a result, it is reasonable to think *why
    not leveraging human knowledge and human languages in MARL?*
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39d534d6850a9901e939116f5cd37625.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Well-known Large Language Models (LLMs) over the past three years.
    Among them, only PaLM-E from Google is trained specifically for embodied applications,
    e.g., robot control.'
  prefs: []
  type: TYPE_NORMAL
- en: As recent advances in Natural Language Processing (NLP) demonstrate great abilities
    in multi-modal tasks, language-conditioned MARL becomes a promising research problem.
    NLP has been an active research topic for decades and many famous models have
    been proposed for language modeling such as Recurrent Neural Network (RNN) [[5](#bib.bib5),
    [6](#bib.bib6)], Long-Short Term Memory networks (LSTM) [[7](#bib.bib7)], and
    transformers [[8](#bib.bib8)]. These foundational models have greatly improved
    the ability of machines to understand and generate human language, setting the
    stage for more complex applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, the integration of NLP with single-agent RL has led to the
    development of language-conditioned RL frameworks [[9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)], especially as Large Language Models (LLMs) [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] emerged as the rising star
    in the artificial intelligence community (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions"))
    and has been successfully applied in various fields [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)]. Pre-trained LLMs contain general human knowledge about the
    world and can easily adapt to RL problems without the need for retraining. This
    integration not only leverages the semantic richness of language but also allows
    for the dynamic adjustment of agent behaviors based on linguistic input. In particular,
    LLM is able to generate new information that it has not seen before on the basis
    of a few examples. For example, in Reflexion [[19](#bib.bib19)], the authors showed
    that the LLM agent could generate decent reflections on its decisions without
    any reward/feedback from the environment. Such capabilities are particularly valuable
    in multi-agent systems, where agents must coordinate and cooperate based on shared
    goals communicated through language.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the need for communication and coordination, the problem of MARL becomes
    more complex than simply multiplying the RL of a single agent by the number of
    agents. As opposed to conventional MARL, LLMs-based MARL can leverage linguistic
    cues to facilitate inter-agent communication and collaboration, further boosting
    system performance. For example, agents can use shared language to negotiate roles,
    coordinate actions, or exchange information about the environment or their internal
    states, thereby aligning their objectives more effectively. This language-enhanced
    coordination becomes critical in complex scenarios where agents must handle ambiguous
    or evolving tasks that require continual communication and mutual understanding.
    The exploration of these capabilities opens up new possibilities for designing
    more intelligent and flexible multi-agent systems capable of operating in unpredictable,
    real-world environments.
  prefs: []
  type: TYPE_NORMAL
- en: Guo et al. [[20](#bib.bib20)] reviewed LLM-based multi-agent frameworks, but
    the emphasis of that paper was not on MARL. Unlike their paper, this letter focuses
    more on the MAS that tries to accomplish a task cooperatively. In addition to
    that, there are several surveys on the topic of MARL [[21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23)] and single agent LLM-based RL [[24](#bib.bib24), [25](#bib.bib25)],
    but none of them is dedicated to LLM-based MARL. Therefore, *we claim that we
    are among the first to provide a systematic overview of the LLM-based MARL problem
    and provide potential future research directions.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of this letter is organized as follows. We first introduce the
    problem of MARL and provide a brief overview of conventional, i.e., non-LLM-based,
    MARL, and single-agent LLM-based RL, in Sect. [II](#S2 "II Preliminaries ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions"). Then, we
    will survey the existing LLM-based MARL frameworks in Sect. [III](#S3 "III Existing
    LLM-based MARL ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future
    Directions"). After that, we will discuss the challenges and future research directions
    for this field in Sect. [IV](#S4 "IV Open Research Problems ‣ LLM-based Multi-Agent
    Reinforcement Learning: Current and Future Directions"). Finally, we will conclude
    the letter in Sect. [V](#S5 "V Conclusion ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: II Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will first introduce the problem of MARL (Sect. [II-A](#S2.SS1
    "II-A MARL Problem Definition ‣ II Preliminaries ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions")). Then, we will briefly discuss conventional
    non-LLM-based MARL in Sect. [II-B](#S2.SS2 "II-B Traditional MARL ‣ II Preliminaries
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions").
    To prepare the ground for LLM-based MARL, we will introduce LLM-based single-agent
    RL in Sect. [II-C](#S2.SS3 "II-C LLM-based Single-Agent RL ‣ II Preliminaries
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: II-A MARL Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MARL can be modeled with the Decentralized Partially Observable Markov Decision
    Process (Dec-POMDP) [[26](#bib.bib26)], an extension to a multi-agent manner of
    the Markov Decision Process (MDP). An MDP for $N$ is the total time length. A
    key difference between Dec-POMDP and normal MDP is the partial observability,
    i.e., for one agent, the actions of other agents and the subsequent outcomes are
    not directly observable, thereby increasing the difficulty of solving the problem.
    Due to this partial observability, individual uncoordinated learning frameworks
    will not work well. Typical deep MARL frameworks adopt the actor-critic structure,
    where actors are trained to output the action given the observation, and the critics
    output a score to judge whether these actions are good in the long-term horizon.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Traditional MARL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To solve the problem of Dec-POMDP, many frameworks have been proposed. These
    frameworks can be roughly categorized into two classes: learning-to-cooperate
    and learning-to-communicate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning to coordinate: The first kind of approach, such as QMIX [[27](#bib.bib27)],
    QTRAN [[28](#bib.bib28)], MADDPG [[29](#bib.bib29)], MAPPO [[30](#bib.bib30)],
    and many others [[31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)], assumes that through centralized training
    with ideal communication, agents can learn to work with each other during the
    centralized training; therefore, communication is not needed during execution.
    In other words, these approaches expect the agents to learn to adapt to other
    agents’ behavior patterns. These approaches can also be classified as policy-based
    and value-based approaches. Policy-based approaches typically adopt the actor-critic
    architecture where actors are trained to make decisions, and critics approximate
    the long-term return and provide feedback to the actors. Value-based approaches
    learn optimized joint Q values given the team’s observations and actions. A problem
    that often happens in this situation is the credit assignment problem, where the
    critic needs to determine the contribution of each agent to the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning to communicate: In communication-based approaches, agents are equipped
    with the capability to share information through various means, such as adjusting
    the content of the shared messages [[37](#bib.bib37)] or optimizing the structure
    of the communication network [[38](#bib.bib38)]. This explicit inter-agent communication
    facilitates coordinated strategies and is crucial in dynamic environments where
    conditions and objectives may frequently change [[39](#bib.bib39), [40](#bib.bib40)].
    Effective communication enables agents to form coalitions to achieve common goals,
    adapt to peers’ actions, and optimize collective outcomes, improving system performance
    in tasks ranging from cooperative manipulation to competitive strategic games [[37](#bib.bib37)].
    Protocols for communication, often learned during training, leverage advanced
    techniques such as differentiable interagent learning algorithms, which refine
    communication patterns based on environmental feedback [[41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43)]. In addition, frameworks for learning emergent communication
    protocols/languages have also been proposed [[44](#bib.bib44), [45](#bib.bib45)].
    These frameworks encourage the agents to learn a certain “language” that is understandable
    by other agents and encodes certain information.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C LLM-based Single-Agent RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As LLMs demonstrated their abilities in various tasks, several LLM-based decision-making
    frameworks have been proposed. These frameworks are not necessarily RL frameworks
    because many of them are open-loop, meaning that the feedback/reward from the
    environment is not used during the decision-making process. Instead, many frameworks
    simply leverage the generalizability of LLMs and the general knowledge they contain
    to solve problems. Typically, in these works, a few examples of how the LLMs are
    expected to solve the problem are provided, and the LLMs can generalize from these
    examples to new problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open-loop LLM-based RL: Among these frameworks, we will summarize some significant
    contributions. Yao et al. [[46](#bib.bib46)] proposed ReAct, in which the LLM
    is prompted to generate “thoughts” to solve the problem given the observation,
    allowing the model to dynamically adjust and refine its strategies in response
    to changing environmental cues and task demands. Based on ReAct, Shinn et al. [[19](#bib.bib19)]
    proposed Reflexion, which uses a few-shot verbal feedback to enhance decision-making
    capabilities. Reflexion processes feedback from interactions within task environments
    into textual summaries, which are then used to augment the model’s episodic memory.
    Prasad et al. [[47](#bib.bib47)] proposed ADaPT, where LLMs learn to decompose
    the task into subtasks through short examples. Although these approaches can achieve
    decent performances in reasoning or word-based games, they are constrained by
    the knowledge the LLMs have and could be biased for certain problems. More importantly,
    the reward, one of the most important signals from the environment, is not considered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Closed-loop LLM-based RL: There are also LLM-based RL frameworks that incorporate
    feedback for closed-loop control. Paul et al. [[48](#bib.bib48)] proposed Refiner,
    in which a fine-tuned LLM is used to provide feedback on policy decisions. Zhang
    et al. [[49](#bib.bib49)] introduced a framework that uses feedback from LLMs
    to enhance credit assignment in RL tasks. Their work targeted sparse reward environments
    and leveraged the rich domain knowledge available in LLMs to dynamically generate
    and refine reward functions. To improve sample efficiency, the authors proposed
    sequential, tree-based, and moving target feedback, facilitating more targeted
    exploration and reducing redundancy in state exploration. Yao et al. [[50](#bib.bib50)]
    proposed Retroformer, where a frozen LLM is used as the policy, while another
    smaller LM is trained to provide verbal feedback on the decisions based on the
    reward. Murthy et al. [[51](#bib.bib51)] proposed REX, adopting the Monte-Carlo
    Tree Search (MCTS) algorithm as the basis to solve problems. The Upper Confidence
    Bound (UCB) technique is adopted to guide the agent’s exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the aforementioned work that uses LLMs as RL policies, multi-modal LLMs
    that are trained on RL tasks such as robot control (e.g., PaLM-E [[52](#bib.bib52)])
    and models for grounding languages to actions [[53](#bib.bib53), [54](#bib.bib54)]
    have also been proposed. These models can achieve decent zero-shot performances
    in several robotic tasks because of their parameter scale.
  prefs: []
  type: TYPE_NORMAL
- en: III Existing LLM-based MARL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE I: Existing LLM for MARL frameworks with an emphasis on multi-agent coordination.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Application | Dataset/Simulator | Training | LLM Role |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DyLAN [[55](#bib.bib55)] | Reasoning, Coding | [MATH](https://github.com/hendrycks/math/),
    [MMLU](https://github.com/hendrycks/test) [[56](#bib.bib56), [57](#bib.bib57)];
    [HumanEval](https://github.com/openai/human-eval) [[58](#bib.bib58)] | ✗ | Decision,
    Communication |'
  prefs: []
  type: TYPE_TB
- en: '| FAMA [[59](#bib.bib59)] | Text Game, Driving | [BabyAI-Text](https://github.com/flowersteam/Grounding_LLMs_with_online_RL/tree/main/babyai-text),
    Traffic Junction [[39](#bib.bib39)] | ✓ | Decision, Communication |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[60](#bib.bib60)] | Consensus Seeking | [Generated Data](https://github.com/WestlakeIntelligentRobotics/ConsensusLLM-code/releases/tag/v1.0.1)
    | ✗ | Decision |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[61](#bib.bib61)] | Path Planning | Close-source simulator | ✗
    | Decision, Communication, Theory of Mind |'
  prefs: []
  type: TYPE_TB
- en: '| CoELA [[62](#bib.bib62)] | Multi-Agent Planning | [TDW-MAT](https://github.com/threedworld-mit/tdw),
    [C-WAH](https://github.com/xavierpuigf/watch_and_help) [[63](#bib.bib63)] | ✓
    | Decision, Communication, Memory |'
  prefs: []
  type: TYPE_TB
- en: '| SMART-LLM [[64](#bib.bib64)] | Multi-Agent Planning | [Proposed Benchmark
    Dataset](https://github.com/SMARTlab-Purdue/SMART-LLM/tree/master/data) | ✗ |
    Decision, Planning |'
  prefs: []
  type: TYPE_TB
- en: '| RoCo [[65](#bib.bib65)] | Motion Planning | [RoCoBench](https://github.com/MandiZhao/robot-collab/tree/main/rocobench)
    | ✗ | Decision, Planning |'
  prefs: []
  type: TYPE_TB
- en: '| Co-NavGPT [[66](#bib.bib66)] | Semantic Navigation | [Habitat-Matterport
    3D](https://aihabitat.org/datasets/hm3d/) [[67](#bib.bib67)] | ✗ | Planning |'
  prefs: []
  type: TYPE_TB
- en: '| Guo et al. [[68](#bib.bib68)] | Multi-Agent Cooperation | [VirtualHome-Social](http://virtual-home.org/)
    | ✗ | Decision, Communication |'
  prefs: []
  type: TYPE_TB
- en: '| MetaGPT [[69](#bib.bib69)] | Coding | [HumanEval](https://github.com/openai/human-eval) [[58](#bib.bib58)],
    [MBPP](https://github.com/google-research/google-research/tree/master/mbpp) [[70](#bib.bib70)]
    | ✗ | Code Generation, Communication |'
  prefs: []
  type: TYPE_TB
- en: Although LLM-based MARL frameworks have not been widely studied, there is still
    some work focused on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'MARL for problem solving: Huang et al. [[71](#bib.bib71)] introduced $\gamma$-Bench,
    which encompasses a variety of multi-agent games to assess these models. Their
    work included a detailed analysis of different versions of the GPT models, which
    demonstrated a systematic improvement in their game ability. This framework demonstrated
    the enhanced performance of newer LLM versions, such as GPT-4, and the potential
    to augment these models with reasoning techniques such as CoT. Liu et al. [[55](#bib.bib55)]
    proposed Dynamic LLM-Agent Network (DyLAN), a framework that studied the capabilities
    of LLM-agent collaborations for complex reasoning and code generation tasks. Unlike
    previous methods that used static architectures, DyLAN dynamically adjusted agent
    interactions based on real-time performance and task demands, incorporating features
    such as inference-time agent selection and an early stopping mechanism. This allowed
    DyLAN to enhance computational efficiency and optimize the contribution of individual
    agents through an unsupervised scoring metric, the agent importance score. Slumbers
    et al. [[59](#bib.bib59)] introduced the Functionally-Aligned Multi-Agents (FAMA)
    framework by integrating a centralized critic architecture and allowing natural
    language communication between agents. The framework aligns LLMs to the functional
    needs of the environment through an online fine-tuning process, which adjusts
    the LLM’s pre-trained knowledge to better fit the specific task requirements.
    Additionally, FAMA allows for intuitive inter-agent communication in natural language,
    making the coordination more efficient and human-interpretable. Chen et al. [[60](#bib.bib60)]
    present a study on the dynamics of consensus seeking in multi-agent systems driven
    by LLMs. The authors focused on the inter-agent negotiation processes, where each
    agent starts with a unique numerical state and negotiates to reach a unified consensus.
    They also provided insights on how different factors, such as agent personality
    (stubborn vs. suggestible), agent number, and network topology, influence the
    negotiation and consensus process. Li et al. [[61](#bib.bib61)] explored Theory
    of Mind (ToM) modeling with LLMs generating communication messages and beliefs
    about the environment and other agents. Hong et al. [[69](#bib.bib69)] proposed
    MetaGPT, where agents share messages with all other agents in a message pool and
    agents can subscribe to messages related to their task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MARL for embodied applications: Other than the aforementioned MARL frameworks
    for problem solving, there are also LLM-based MARL frameworks for embodied application.
    Zhang et al. [[62](#bib.bib62)] proposed a Cooperative Embodied Language Agent (CoELA),
    a modular framework that integrates LLM to improve communication and collaborative
    decision-making among multiple agents. The modular structure includes a perception
    module for interpreting sensory data, a memory module for retaining and recalling
    environmental and task-related information, a communication module to facilitate
    inter-agent dialogue, a planning module for strategic decision making, and an
    execution module for carrying out planned actions. By incorporating LLMs into
    the memory, communication, and planning modules, the framework enables agents
    to utilize natural language to improve both understanding and execution of cooperative
    tasks. Kannan et al. [[64](#bib.bib64)] introduced SMART-LLM, a framework that
    integrated LLM with multi-agent robot task planning to translate high-level instructions
    into executable strategies for robot teams. By structuring task planning into
    sequential phases of decomposition, coalition formation, and allocation, SMART-LLM
    generates robot actions to achieve complex objectives. Their approach leveraged
    the cognitive processing power of LLMs to enhance the comprehension and execution
    capabilities of robot systems. Mandi et al. [[65](#bib.bib65)] introduced RoCo,
    a multi-robot arm collaboration framework with each arm equipped with an LLM agent.
    The LLM agents are responsible for coordination among agents by communicating
    with other LLM agents and path planning. Yu et al. [[66](#bib.bib66)] introduced
    Co-NavGPT, an LLM-based multi-agent navigation framework. However, unlike other
    frameworks where multiple LLMs are employed, in Co-NavGPT, only one LLM is used
    to assign frontiers to agents globally. Guo et al. [[68](#bib.bib68)] studied
    the collaboration of multiple LLM-based agents on various tasks with a focus on
    communication and coordination among multiple agents. They proposed the Criticize-Reflect
    method with an LLM critic and an LLM coordinator. Table [I](#S3.T1 "TABLE I ‣
    III Existing LLM-based MARL ‣ LLM-based Multi-Agent Reinforcement Learning: Current
    and Future Directions") provides more details on these works.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to LLM-based MARL, several works explored multi-agent interaction [[72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74)], e.g., multi-agent conversation and gaming.
    However, these works fall out of the MARL scope; we will not use too much space
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these studies illustrated that while the exploration into language-conditioned
    MARL is still nascent, it holds considerable promise for advancing the capabilities
    of MAS. Using natural language, these systems can achieve higher levels of coordination
    and understanding, which is essential for complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: IV Open Research Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the research efforts mentioned above, language-conditioned MARL is
    still an unexplored field with many unexplored aspects. To inspire more research
    in this field, we provide several research directions in this section. Specifically,
    we discuss four potential research directions: i) *personality-enabled cooperation*
    (Sect. [IV-A](#S4.SS1 "IV-A Personality-enabled Cooperation ‣ IV Open Research
    Problems ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")),
    ii) *language-enabled human-in/on-the-loop frameworks* (Sect. [IV-B](#S4.SS2 "IV-B
    Language-enabled Human-in/on-the-Loop Frameworks ‣ IV Open Research Problems ‣
    LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")),
    iii) *traditional MARL and LLM co-design* (Sect. [IV-C](#S4.SS3 "IV-C Traditional
    MARL and LLM Co-Design ‣ IV Open Research Problems ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions")), and iv) *safety and security in MAS*
    (Sect. [IV-D](#S4.SS4 "IV-D Safety and Security in MAS ‣ IV Open Research Problems
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")).
    Fig. [2](#S4.F2 "Figure 2 ‣ IV Open Research Problems ‣ LLM-based Multi-Agent
    Reinforcement Learning: Current and Future Directions") also provides a more vivid
    demonstration of these research ideas.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| (a) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/04148aa91d97033ea8babb243035547f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| (b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| (c) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/1db16954559667b117927c94d625013f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23bf7362fee5493a133e742777a6b601.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Potential research directions for language-conditioned Multi-Agent
    Reinforcement Learning (MARL). (a) Personality-enabled cooperation, where different
    robots have different personalities defined by the commands. (b) Language-enabled
    human-on-the-loop frameworks, where humans supervise robots and provide feedback.
    (c) Traditional co-design of MARL and LLM, where knowledge about different aspects
    of LLM is distilled into smaller models that can be executed on board.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Personality-enabled Cooperation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous work [[75](#bib.bib75), [60](#bib.bib60)] has shown that different
    personalities in MARL frameworks can produce promising results. This idea can
    be naturally extended to language-conditioned MARL frameworks. In these frameworks,
    agents are distinguished by their assigned personalities. For example, an agent
    with a “curious” personality will tend to explore the environment, while an agent
    with a “conservative” personality will tend to stay in the safe areas. A team
    of agents with a combination of different personalities can often achieve better
    performance than those with the same personality. In traditional MARL frameworks,
    these personalities are encoded in the agents’ model parameters, i.e., the weights
    of their models. However, with LLMs as agents, personalities can be assigned to
    agents by prompts, in which narratives about the agent’s personality will be provided.
  prefs: []
  type: TYPE_NORMAL
- en: Another potential advantage of language-conditioned MARL with personalized agents
    is the ability to handle conflicts and negotiate solutions more effectively. Agents
    can be trained to understand and generate language-based responses that consider
    the perspectives and goals of other agents, facilitating a negotiation process
    that mirrors human interaction. This capability is particularly useful in scenarios
    where agents must share resources or decide on joint actions that impact the collective
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: However, implementing these personalized language behaviors in agents presents
    several challenges. The primary concern is ensuring that language models do not
    perpetuate or amplify undesirable biases that could lead to unfair or inefficient
    outcomes. Additionally, the complexity of training such models increases as they
    must not only understand and generate appropriate responses, but also adapt their
    linguistic style based on the evolving context of the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Future research could focus on developing frameworks that can effectively integrate
    personality-driven language models into MARL systems. This integration involves
    creating robust prompts with memories that encode the information from past experiences
    in a wide range of interactive scenarios, allowing agents to learn from both their
    successes and failures. Furthermore, evaluating these systems will require new
    metrics that can assess not just the efficacy of task performance but also the
    appropriateness and effectiveness of communication between agents.
  prefs: []
  type: TYPE_NORMAL
- en: Another direction of research is to explore competitive agents instead of cooperative
    agents. However, the competition here should be benign, which means that the agents
    compete to achieve the same goal. By addressing these challenges, language-conditioned
    MARL with diverse agent personalities has the potential to advance the field of
    artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Language-enabled Human-in/on-the-Loop Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the direct advantages of language-conditioned MARL frameworks is the
    possibility of involving humans in or on the loop. To illustrate, human-in-the-loop
    frameworks [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)] involve humans
    as agents that can generate actions to affect the environment, while human-on-the-loop
    frameworks [[79](#bib.bib79)] regard humans as supervisors without directly being
    involved in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: In human-in-the-loop setups, humans actively participate in the learning process,
    often providing corrective feedback or rewards to shape agent behaviors in real
    time. This direct interaction helps in refining the agent’s actions and strategies,
    making them more aligned with human-like reasoning and ethical standards. For
    example, a human could guide an agent away from potential pitfalls in its learning
    process that might not be immediately apparent through algorithmic reinforcement
    signals alone. On the other hand, human-on-the-loop frameworks play a crucial
    oversight role. Here, humans monitor the system’s performance and intervene only
    when necessary. This approach is particularly valuable in applications where autonomous
    operations are preferable, but human oversight is necessary to ensure safety and
    compliance with regulatory standards. For example, in autonomous driving, while
    the system can handle most driving tasks, a human supervisor may only need to
    intervene in complex or hazardous road conditions, ensuring that the system operates
    within safe limits without requiring constant human control.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these human roles within language-conditioned MARL can benefit significantly
    from the integration of natural language. Language serves as a versatile interface
    that enables clearer and more intuitive communication between humans and agents.
    Agents can report their status, explain their decisions, or even ask for clarification
    in human-understandable language, improving the effectiveness of human interventions.
    Furthermore, the use of language can facilitate the transfer of knowledge between
    agents by allowing them to share insights or strategies in a comprehensible format.
    In scenarios involving multiple agents with varying roles, language can help maintain
    coherence and unity of purpose across the team, guiding less experienced agents
    through complex tasks or strategies articulated by more experienced ones or even
    by human supervisors.
  prefs: []
  type: TYPE_NORMAL
- en: Future research could explore optimizing these interactions between human supervisors
    and agents, possibly by developing advanced language models that can understand
    and generate more context-aware, situation-specific dialogue. Furthermore, ensuring
    that language-based communications are not only informative, but also prompt and
    actionable will be crucial for the practical deployment of such systems in real-world
    applications. This balance between automation and human oversight, facilitated
    by natural language, promises to enhance the robustness and reliability of multi-agent
    systems, pushing the boundaries of what automated systems can achieve while ensuring
    they operate under safe and ethical guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Traditional MARL and LLM Co-Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since LLMs tend to have large sizes, especially those pre-trained models, performing
    inference on-board on robot hardware is not practical. A popular way towards resource-efficient
    computing is through Parameter-Efficient Fine-Tuning (PEFT) techniques [[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)] combined with quantization.
    However, this kind of approach still requires inference through the large LLM
    network, which is impractical for small robots. To make this happen, we envision
    a co-design framework of traditional MARL policies and the LM models. A typical
    design for such systems could be to use the LLM model as a centralized critic
    to guide the training of the actors. This design follows the CTDE scheme introduced
    in Sect. [II-B](#S2.SS2 "II-B Traditional MARL ‣ II Preliminaries ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions"), where the
    critic will be removed during execution. To leverage communication during execution,
    we can distill the knowledge from the LLMs about communication into smaller models
    that can be executed onboard.'
  prefs: []
  type: TYPE_NORMAL
- en: One potential development is the refinement of the distillation process, which
    aims to transfer knowledge from LLMs to more compact models suitable for deployment
    on less powerful hardware, such as robots or Internet of Things (IoT) devices.
    A promising direction in this direction would be in-context distillation [[84](#bib.bib84),
    [85](#bib.bib85)], where the teacher model is an LLM with a pre-defined context.
    For example, for controlling warehouse robots, the context can be refined to tell
    the LLM to avoid people and collisions. By focusing on the essential features
    necessary for the communication and decision-making learned by the LLM, smaller
    models can execute complex tasks effectively with a fraction of the computational
    overhead. In addition, to facilitate effective communication between agents during
    execution, specialized communication protocols could be designed. These protocols
    would utilize the distilled models to ensure that critical information, as understood
    and processed by the LLM during the training phase, is efficiently conveyed between
    agents. This approach not only conserves bandwidth, but also optimizes the real-time
    decision-making process, allowing for dynamic adjustments based on the operational
    environment and agent states.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the co-design framework can be enhanced by integrating adaptive
    mechanisms that allow the MARL system to recalibrate its strategies based on feedback
    from the operational environment. Such adaptive systems could dynamically adjust
    the compression level of the distilled models or modify the communication protocols
    based on the complexity of the tasks and the computational capabilities available
    at that time. This flexibility would be particularly useful in environments where
    conditions change rapidly or unpredictably, requiring swift responses from the
    agent collective. Furthermore, the implementation of this co-design framework
    would benefit significantly from the development of specialized hardware tailored
    to the execution of compressed models. This hardware could optimize the execution
    of neural network operations, potentially in a power-efficient manner, which is
    critical for mobile or embedded systems.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Safety and Security in MAS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensuring the safety and security of MAS is critical, especially as these systems
    are increasingly deployed in diverse and potentially high-stakes environments.
    The integration of language models into MARL introduces unique challenges and
    vulnerabilities, from the manipulation of agent communication to the exploitation
    of model biases.
  prefs: []
  type: TYPE_NORMAL
- en: Many robotic operations have continuous action spaces, where the output of each
    agent’s policy is a set of continuous values. Unlike discrete action spaces, which
    can be reformulated as multi-choice problems and solved by prompting the multi-choice
    question to the LLM, continuous action space is more tricky, especially in high-stake
    environments, for example, operation robots. Existing methods replace the last
    few layers of the LLMs with new layers that map the observation in languages to
    continuous action spaces. However, this kind of approach requires training the
    new layers in the desired environment, which might be inaccessible. Therefore,
    exploring alternative methods for integrating LLMs into the control loop of robots
    operating in continuous action spaces without the need for substantial retraining
    or modification of the LLMs is promising.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to safety in actions, safety and security against potential attacks
    are also crucial in MAS. One way towards safety is through proactive measures.
    This includes the development of secure communication protocols between agents
    to prevent eavesdropping or the injection of malicious data that could lead to
    compromised decision-making. Communications encryption can be a fundamental aspect
    of this, ensuring that even if data transmissions are intercepted, the information
    remains protected. In addition, securing the language model training process against
    adversarial attacks is crucial. Adversarial training, which involves exposing
    the system to a wide range of attack vectors during the training phase, can help
    models learn to resist or mitigate these attacks in deployment. In addition, input
    validation techniques can be employed to filter out potentially harmful or misleading
    inputs that could cause the system to behave unpredictably. This is particularly
    important in scenarios where agents interact with humans or systems outside the
    controlled environment and are exposed to a broader range of language inputs and
    behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the best proactive defenses, systems may still encounter unforeseen
    vulnerabilities post-deployment. Thus, reactive strategies are necessary to quickly
    address any breaches or failures. This can involve real-time monitoring of agent
    behaviors and communications to detect anomalies that may indicate a security
    breach or a failure in safety protocols. Once an anomaly is detected, the systems
    should be able to isolate affected agents and roll back their states to secure
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this letter, we provide a brief overview of Multi-Agent Reinforcement Learning
    (MARL) based on conventional non-Large Language Model (LLM)-based Multi-Agent
    Reinforcement Learning (MARL), LLM-based single-agent RL, and existing LLM-based
    MARL frameworks. These works paved the way for new ideas that we discuss in later
    sections. Specifically, we discussed potential research directions ranging from
    multi-agent personality to safety and security in the LLM-based Multi-Agent System (MAS).
    Although works are studying LLM-based MARL, the field is still to be explored
    and has significant potential because of the great ability of LLMs and their in-context
    and interpretable nature. With LLMs, designing MARL frameworks becomes more analogous
    to modeling the group learning process of animals or even humans, where knowledge
    is transferred or exchanged via natural languages. We hope, with this letter,
    that more research works can be enlightened and the boundary of multi-agent intelligence
    could be pushed further.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. Sun, S. Huang, and D. Pompili, “Hmaac: Hierarchical multi-agent actor-critic
    for aerial search with explicit coordination modeling,” in 2023 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 7728–7734, IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” arXiv preprint arXiv:1610.03295, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] V. Sadhu, C. Sun, A. Karimian, R. Tron, and D. Pompili, “Aerial-deepsearch:
    Distributed multi-agent deep reinforcement learning for search missions,” in 2020
    IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),
    pp. 165–173, IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. A. Calvo and I. Dusparic, “Heterogeneous multi-agent deep reinforcement
    learning for traffic lights control.,” in AICS, pp. 2–13, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal representations
    by error propagation, parallel distributed processing, explorations in the microstructure
    of cognition, ed. de rumelhart and j. mcclelland. vol. 1\. 1986,” Biometrika,
    vol. 71, pp. 599–607, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. I. Jordan, “Serial order: A parallel distributed processing approach,”
    in Advances in psychology, vol. 121, pp. 471–495, Elsevier, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural
    information processing systems, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Peng, X. Hu, R. Zhang, J. Guo, Q. Yi, R. Chen, Z. Du, L. Li, Q. Guo,
    and Y. Chen, “Conceptual reinforcement learning for language-conditioned tasks,”
    in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 9426–9434,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn, “Language as an abstraction
    for hierarchical deep reinforcement learning,” Advances in Neural Information
    Processing Systems, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] L. Zhou and K. Small, “Inverse reinforcement learning with natural language
    goals,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35,
    pp. 11116–11124, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] OpenAI, “ChatGPT: Optimizing Language Models for Dialogue.” [https://www.openai.com/chatgpt](https://www.openai.com/chatgpt),
    2023. Accessed: 2024-04-22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham,
    H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling language modeling
    with pathways,” Journal of Machine Learning Research, vol. 24, no. 240, pp. 1–113,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk,
    A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable multimodal models,”
    arXiv preprint arXiv:2312.11805, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Wu, Z. Lai, S. Chen, R. Tao, P. Zhao, and N. Hovakimyan, “The new agronomists:
    Language models are experts in crop management,” arXiv preprint arXiv:2403.19839,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Z. Lai, J. Wu, S. Chen, Y. Zhou, A. Hovakimyan, and N. Hovakimyan, “Language
    models are free boosters for biomedical imaging tasks,” arXiv preprint arXiv:2403.17343,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] G. Han, W. Liu, X. Huang, and B. Borsari, “Chain-of-interaction: Enhancing
    large language models for psychiatric behavior understanding by dyadic contexts,”
    arXiv preprint arXiv:2403.13786, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion:
    Language agents with verbal reinforcement learning,” Advances in Neural Information
    Processing Systems, vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and
    X. Zhang, “Large language model based multi-agents: A survey of progress and challenges,”
    arXiv preprint arXiv:2402.01680, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multiagent systems: A review of challenges, solutions, and applications,”
    IEEE transactions on cybernetics, vol. 50, no. 9, pp. 3826–3839, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
    of multiagent deep reinforcement learning,” Autonomous Agents and Multi-Agent
    Systems, vol. 33, no. 6, pp. 750–797, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Gronauer and K. Diepold, “Multi-agent deep reinforcement learning:
    a survey,” Artificial Intelligence Review, pp. 1–49, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas, E. Grefenstette,
    S. Whiteson, and T. Rocktäschel, “A survey of reinforcement learning informed
    by natural language,” arXiv preprint arXiv:1906.03926, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Cao, H. Zhao, Y. Cheng, T. Shu, G. Liu, G. Liang, J. Zhao, and Y. Li,
    “Survey on large language model-enhanced reinforcement learning: Concept, taxonomy,
    and methods,” arXiv preprint arXiv:2404.00282, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized
    POMDPs, vol. 1. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
    S. Whiteson, “Monotonic value function factorisation for deep multi-agent reinforcement
    learning,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 7234–7284,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran: Learning
    to factorize with transformation for cooperative multi-agent reinforcement learning,”
    in International conference on machine learning, pp. 5887–5896, PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
    “Multi-agent actor-critic for mixed cooperative-competitive environments,” Advances
    in neural information processing systems, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, “The
    surprising effectiveness of ppo in cooperative multi-agent games,” Advances in
    Neural Information Processing Systems, vol. 35, pp. 24611–24624, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg,
    M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al., “Value-decomposition networks
    for cooperative multi-agent learning based on team reward,” in Proceedings of
    the 17th International Conference on Autonomous Agents and MultiAgent Systems,
    pp. 2085–2087, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted qmix: Expanding
    monotonic value function factorisation for deep multi-agent reinforcement learning,”
    Advances in neural information processing systems, vol. 33, pp. 10199–10210, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang, “Qplex: Duplex dueling multi-agent
    q-learning,” in International Conference on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Ackermann, V. Gabler, T. Osa, and M. Sugiyama, “Reducing overestimation
    bias in multi-agent domains using double centralized critics,” arXiv preprint
    arXiv:1910.01465, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Wang, B. Han, T. Wang, H. Dong, and C. Zhang, “Dop: Off-policy multi-agent
    decomposed policy gradients,” in International conference on learning representations,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] T. Zhang, Y. Li, C. Wang, G. Xie, and Z. Lu, “Fop: Factorizing optimal
    joint policy of maximum-entropy multi-agent reinforcement learning,” in International
    Conference on Machine Learning, pp. 12491–12500, PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to
    communicate with deep multi-agent reinforcement learning,” Advances in neural
    information processing systems, vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat, and J. Pineau,
    “Tarmac: Targeted multi-agent communication,” in International Conference on machine
    learning, pp. 1538–1546, PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Sukhbaatar, R. Fergus, et al., “Learning multiagent communication with
    backpropagation,” Advances in neural information processing systems, vol. 29,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Y. Hoshen, “Vain: Attentional multi-agent predictive modeling,” Advances
    in neural information processing systems, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Jiang and Z. Lu, “Learning attentional communication for multi-agent
    cooperation,” Advances in neural information processing systems, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] I. Mordatch and P. Abbeel, “Emergence of grounded compositional language
    in multi-agent populations,” in Proceedings of the AAAI conference on artificial
    intelligence, vol. 32, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Shen, Y. Fu, H. Su, H. Pan, P. Qiao, Y. Dou, and C. Wang, “Graphcomm:
    A graph neural network based method for multi-agent reinforcement learning,” in
    ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pp. 3510–3514, IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Gupta, R. Hazra, and A. Dukkipati, “Networked multi-agent reinforcement
    learning with emergent communication,” arXiv preprint arXiv:2004.02780, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Lazaridou and M. Baroni, “Emergent multi-agent communication in the
    deep learning era,” arXiv preprint arXiv:2006.02419, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao,
    “React: Synergizing reasoning and acting in language models,” in The Eleventh
    International Conference on Learning Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal, M. Bansal,
    and T. Khot, “Adapt: As-needed decomposition and planning with language models,”
    arXiv preprint arXiv:2311.05772, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West,
    and B. Faltings, “Refiner: Reasoning feedback on intermediate representations,”
    arXiv preprint arXiv:2304.01904, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Zhang, A. Parashar, and D. Saha, “A simple framework for intrinsic
    reward-shaping for rl using llm feedback,”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. R. N,
    Z. Chen, J. Zhang, D. Arpit, R. Xu, P. L. Mui, H. Wang, C. Xiong, and S. Savarese,
    “Retroformer: Retrospective large language agents with policy gradient optimization,”
    in The Twelfth International Conference on Learning Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] R. Murthy, S. Heinecke, J. C. Niebles, Z. Liu, L. Xue, W. Yao, Y. Feng,
    Z. Chen, A. Gokul, D. Arpit, et al., “Rex: Rapid exploration and exploitation
    for ai agents,” arXiv preprint arXiv:2307.08962, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid,
    J. Tompson, Q. Vuong, T. Yu, et al., “Palm-e: An embodied multimodal language
    model,” in International Conference on Machine Learning, pp. 8469–8488, PMLR,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in International
    Conference on Machine Learning, pp. 9118–9147, PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz,
    A. Irpan, E. Jang, R. Julian, et al., “Do as i can, not as i say: Grounding language
    in robotic affordances,” in Conference on robot learning, pp. 287–318, PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, “Dynamic llm-agent network:
    An llm-agent collaboration framework with agent team optimization,” arXiv preprint
    arXiv:2310.02170, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song,
    and J. Steinhardt, “Measuring mathematical problem solving with the math dataset,”
    NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and J. Steinhardt,
    “Aligning ai with shared human values,” Proceedings of the International Conference
    on Learning Representations (ICLR), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] O. Slumbers, D. H. Mguni, K. Shao, and J. Wang, “Leveraging large language
    models for optimised coordination in textual multi-agent reinforcement learning,”
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Chen, W. Ji, L. Xu, and S. Zhao, “Multi-agent consensus seeking via
    large language models,” arXiv preprint arXiv:2310.20151, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Li, Y. Chong, S. Stepputtis, J. P. Campbell, D. Hughes, C. Lewis, and
    K. Sycara, “Theory of mind for multi-agent collaboration via large language models,”
    in Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing, pp. 180–192, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and
    C. Gan, “Building cooperative embodied agents modularly with large language models,”
    in The Twelfth International Conference on Learning Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler,
    and A. Torralba, “Watch-and-help: A challenge for social perception and human-ai
    collaboration,” arXiv preprint arXiv:2010.09890, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. S. Kannan, V. L. Venkatesh, and B.-C. Min, “Smart-llm: Smart multi-agent
    robot task planning using large language models,” arXiv preprint arXiv:2309.10062,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Mandi, S. Jain, and S. Song, “Roco: Dialectic multi-robot collaboration
    with large language models,” arXiv preprint arXiv:2307.04738, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] B. Yu, H. Kasaei, and M. Cao, “Co-navgpt: Multi-robot cooperative visual
    semantic navigation using large language models,” arXiv preprint arXiv:2310.07937,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M.
    Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao,
    and D. Batra, “Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments
    for embodied AI,” in Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] X. Guo, K. Huang, J. Liu, W. Fan, N. Vélez, Q. Wu, H. Wang, T. L. Griffiths,
    and M. Wang, “Embodied llm agents learn to cooperate in organized teams,” arXiv
    preprint arXiv:2403.12482, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z. Wang,
    S. K. S. Yau, Z. Lin, et al., “Metagpt: Meta programming for multi-agent collaborative
    framework,” in The Twelfth International Conference on Learning Representations,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang,
    C. Cai, M. Terry, Q. Le, et al., “Program synthesis with large language models,”
    arXiv preprint arXiv:2108.07732, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J.-t. Huang, E. J. Li, M. H. Lam, T. Liang, W. Wang, Y. Yuan, W. Jiao,
    X. Wang, Z. Tu, and M. R. Lyu, “How far are we on the decision-making of llms?
    evaluating llms’ gaming ability in multi-agent environments,” arXiv preprint arXiv:2403.11807,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, and C. Wang, “Autogen: Enabling next-gen llm applications via multi-agent
    conversation framework,” arXiv preprint arXiv:2308.08155, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein,
    “Generative agents: Interactive simulacra of human behavior,” in Proceedings of
    the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 1–22,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel: Communicative
    agents for “mind” exploration of large language model society,” Advances in Neural
    Information Processing Systems, vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. Szot, U. Jain, D. Batra, Z. Kira, R. Desai, and A. Rai, “Adaptive coordination
    in social embodied rearrangement,” in International Conference on Machine Learning,
    pp. 33365–33380, PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Abel, J. Salvatier, A. Stuhlmüller, and O. Evans, “Agent-agnostic human-in-the-loop
    reinforcement learning,” arXiv preprint arXiv:1701.04079, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Liang, L. Yang, H. Cheng, W. Tu, and M. Xu, “Human-in-the-loop reinforcement
    learning,” in 2017 Chinese Automation Congress (CAC), pp. 4511–4518, IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] B. Luo, Z. Wu, F. Zhou, and B.-C. Wang, “Human-in-the-loop reinforcement
    learning in continuous-action space,” IEEE Transactions on Neural Networks and
    Learning Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,
    “Deep reinforcement learning from human preferences,” Advances in neural information
    processing systems, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint
    arXiv:2106.09685, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Xin, J. Du, Q. Wang, K. Yan, and S. Ding, “Mmap: Multi-modal alignment
    prompt for cross-domain multi-task learning,” in Proceedings of the AAAI Conference
    on Artificial Intelligence, vol. 38, pp. 16076–16084, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Xin, J. Du, Q. Wang, Z. Lin, and K. Yan, “Vmt-adapter: Parameter-efficient
    transfer learning for multi-task dense scene understanding,” in Proceedings of
    the AAAI Conference on Artificial Intelligence, vol. 38, pp. 16085–16093, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Xin, S. Luo, H. Zhou, J. Du, X. Liu, Y. Fan, Q. Li, and Y. Du, “Parameter-efficient
    fine-tuning for pre-trained vision models: A survey,” arXiv preprint arXiv:2402.02242,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Huang, Y. Chen, Z. Yu, and K. McKeown, “In-context learning distillation:
    Transferring few-shot learning ability of pre-trained language models,” arXiv
    preprint arXiv:2212.10670, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Snell, D. Klein, and R. Zhong, “Learning by distilling context,” arXiv
    preprint arXiv:2209.15189, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
