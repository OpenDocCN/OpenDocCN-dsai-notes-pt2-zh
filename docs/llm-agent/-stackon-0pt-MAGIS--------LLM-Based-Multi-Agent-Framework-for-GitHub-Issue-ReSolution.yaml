- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '\stackon[0pt]MAGIS      : LLM-Based Multi-Agent Framework for GitHub Issue
    ReSolution'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '\stackon[0pt]MAGIS      : 基于 LLM 的 GitHub 问题解决多代理框架'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17927](https://ar5iv.labs.arxiv.org/html/2403.17927)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17927](https://ar5iv.labs.arxiv.org/html/2403.17927)
- en: Wei Tao
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 魏涛
- en: Fudan University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 复旦大学
- en: wtao18@fudan.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: wtao18@fudan.edu.cn
- en: '&Yucheng Zhou'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&周宇城'
- en: University of Macau
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 澳门大学
- en: yucheng.zhou@connect.um.edu.mo
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: yucheng.zhou@connect.um.edu.mo
- en: '&Wenqiang Zhang'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&张文强'
- en: Fudan University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 复旦大学
- en: wqzhang@fudan.edu.cn
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: wqzhang@fudan.edu.cn
- en: '&Yu Cheng'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&余程'
- en: Rice University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: yc180@rice.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yc180@rice.edu
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In software evolution, resolving the emergent issues within GitHub repositories
    is a complex challenge that involves not only the incorporation of new code but
    also the maintenance of existing functionalities. Large Language Models (LLMs)
    have shown promise in code generation and understanding but face difficulties
    in code change, particularly at the repository level. To overcome these challenges,
    we empirically study the reason why LLMs mostly fail to resolve GitHub issues
    and analyze some impact factors. Motivated by the empirical findings, we propose
    a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting
    of four kinds of agents customized for the software evolution: Manager, Repository
    Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages
    the collaboration of various agents in the planning and coding process to unlock
    the potential of LLMs to resolve GitHub issues. In experiments, we employ the
    SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4,
    and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms
    the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved
    ratio over the direct application of GPT-4, the based LLM of our method. We also
    analyze the factors for improving GitHub issue resolution rates, such as line
    location, task allocation, etc.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件演进中，解决 GitHub 仓库中的新出现问题是一项复杂的挑战，这不仅涉及新代码的融入，还涉及现有功能的维护。大型语言模型（LLMs）在代码生成和理解方面显示出了潜力，但在代码更改，特别是在仓库级别，面临困难。为了克服这些挑战，我们通过实证研究分析了
    LLMs 为什么大多不能解决 GitHub 问题，并分析了一些影响因素。在实证研究的启发下，我们提出了一种基于 LLM 的多代理框架——MAGIS，用于 GitHub
    问题解决，该框架由四种针对软件演进定制的代理组成：经理、仓库保管员、开发者和质量保证工程师代理。该框架利用各种代理在规划和编码过程中的协作，以释放 LLMs
    解决 GitHub 问题的潜力。在实验中，我们使用 SWE-bench 基准测试将 MAGIS 与包括 GPT-3.5、GPT-4 和 Claude-2 在内的流行
    LLMs 进行比较。MAGIS 可以解决 13.94% 的 GitHub 问题，显著优于基线。具体而言，MAGIS 实现了相较于直接应用 GPT-4 的解决率的八倍增长，GPT-4
    是我们方法的基础 LLM。我们还分析了提高 GitHub 问题解决率的因素，如行位置、任务分配等。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In real-world software development, the software application is rarely set in
    stone. High-quality and popular software continually evolves to address emergent
    bugs or adapt to new requirements arising from shifts in user needs, software
    environments, and physical hardware. On platforms such as GitHub ¹¹1[https://github.com](https://github.com),
    issues typically signify the requirement for software evolution. However, addressing
    these issues poses significant challenges, as it requires implementing the code
    change across the entire repository and maintaining the existing functionality
    while integrating new capabilities. Consequently, resolving GitHub issues remains
    a significant challenge across academia and industry [[15](#bib.bib15), [4](#bib.bib4)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的软件开发中，软件应用程序很少是固定不变的。高质量和受欢迎的软件会不断演进，以解决新出现的漏洞或适应来自用户需求、软件环境和物理硬件变化的新要求。在如
    GitHub ¹¹1[https://github.com](https://github.com) 这样的平台上，问题通常意味着软件需要演进。然而，解决这些问题面临重大挑战，因为这需要在整个代码库中实施代码更改，同时在集成新功能的同时维护现有功能。因此，解决
    GitHub 问题仍然是学术界和工业界面临的重大挑战 [[15](#bib.bib15), [4](#bib.bib4)]。
- en: Large Language Models (LLMs) have demonstrated remarkable capabilities across
    a variety of tasks [[6](#bib.bib6)], including code generation and code understanding [[40](#bib.bib40),
    [30](#bib.bib30)]. Specifically, LLMs excel in generating function-level code,
    as evidenced by their performance on numerous benchmark datasets such as MBPP [[2](#bib.bib2)]
    and HumanEval [[9](#bib.bib9)]. Despite their success, LLMs remain challenged
    in tasks that require advanced code generation capabilities, such as the ClassEval
    benchmark [[10](#bib.bib10)]. Moreover, LLMs exhibit limitations in processing
    excessively long context inputs and are subject to constraints regarding their
    input context length [[20](#bib.bib20)]. This limitation is particularly evident
    in repository-level tasks, such as solving GitHub issues, where the context comprises
    the entire repository, thus imposing constraints on directly using the full repository
    as input to LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务中展示了卓越的能力[[6](#bib.bib6)]，包括代码生成和代码理解[[40](#bib.bib40), [30](#bib.bib30)]。具体来说，LLMs在生成函数级代码方面表现出色，这在许多基准数据集如MBPP[[2](#bib.bib2)]和HumanEval[[9](#bib.bib9)]的表现中得到了证明。尽管取得了成功，LLMs在需要高级代码生成能力的任务中仍面临挑战，例如ClassEval基准[[10](#bib.bib10)]。此外，LLMs在处理过长的上下文输入时表现出局限性，并且受到输入上下文长度的限制[[20](#bib.bib20)]。这种限制在处理如解决GitHub问题这样的仓库级任务时尤为明显，因为上下文包括整个仓库，这限制了直接将整个仓库作为LLMs的输入。
- en: To harness the full potential of LLMs, researchers [[12](#bib.bib12), [26](#bib.bib26),
    [36](#bib.bib36)] have designed LLM-based multi-agent systems. These approaches
    have significantly improved LLMs’ efficacy in code generation, enabling these
    systems to construct code repositories based on LLM. While these methods address
    the process of transitioning code repositories from inception to establishment,
    they rarely consider the handling of software evolution, e.g., resolving Github
    issues. For Github repositories, especially popular ones, a large number of commits
    are pushed every day. These commits derive from a spectrum of evolutionary requirements
    that span bug fixes, feature additions, performance enhancements, etc [[32](#bib.bib32)].
    For open-source software, new requirements frequently emerge as issues in the
    project’s repository.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分发挥LLMs的潜力，研究人员[[12](#bib.bib12), [26](#bib.bib26), [36](#bib.bib36)]设计了基于LLM的多智能体系统。这些方法显著提高了LLMs在代码生成中的有效性，使这些系统能够基于LLM构建代码仓库。虽然这些方法解决了代码仓库从创立到建立的过程，但它们很少考虑软件演变的处理，例如解决GitHub问题。对于GitHub仓库，尤其是流行的仓库，每天都会推送大量提交。这些提交源于一系列演变需求，包括错误修复、功能添加、性能增强等[[32](#bib.bib32)]。对于开源软件，新的需求经常以项目仓库中的问题形式出现。
- en: To investigate the capability of LLMs in addressing GitHub issues, Jimenez et al.
    [[15](#bib.bib15)] developed a benchmark, namely SWE-bench, to conduct a comprehensive
    analysis of popular LLMs. The study reveals that LLMs fail to resolve over $5\%$
    of instances, even when provided with file paths that require modifications. This
    significantly low rate of applying solutions and resolving issues underscores
    the limitations inherent in LLMs. The reasons behind the suboptimal performance
    of LLMs and strategies to harness their potential for GitHub issue resolution
    remain under-explored.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究LLMs在解决GitHub问题中的能力，Jimenez等人[[15](#bib.bib15)]开发了一个基准，即SWE-bench，以对流行的LLMs进行全面分析。研究表明，即使提供需要修改的文件路径，LLMs也未能解决超过$5\%$的实例。这一极低的应用解决方案和解决问题的比率突显了LLMs固有的局限性。LLMs表现不佳的原因以及如何利用其潜力来解决GitHub问题的策略仍未被充分探讨。
- en: 'In this study, we analyze the factors impacting the effectiveness of LLMs in
    resolving issues under two different settings (i.e., w/ Oracle and w/o Oracle).
    Furthermore, our empirical analysis has concluded a correlation between file location
    and line location within files, and performance in resolving GitHub issues. Based
    on these insights, we propose a novel LLM-based Multi-agent Framework, termed
    MAGIS, comprising four types of agents: Manager, Repository Custodian, Developer,
    and Quality Assurance (QA) Engineer. Our approach facilitates the resolution of
    GitHub issues through collaboration among agents, each fulfilling a unique role.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们分析了影响LLM在两种不同设置下（即有Oracle和无Oracle）解决问题效果的因素。此外，我们的实证分析还得出了文件位置与文件内行位置以及解决GitHub问题的表现之间的相关性。基于这些见解，我们提出了一种新型的基于LLM的多智能体框架，称为MAGIS，包含四种类型的智能体：经理、仓库管理员、开发人员和质量保证（QA）工程师。我们的方法通过智能体之间的协作，解决GitHub问题，每个智能体都发挥独特的作用。
- en: In our experiment, we evaluate our framework on the SWE-bench, comparing its
    performance against existing popular LLMs, such as ChatGPT-3.5 [[24](#bib.bib24)],
    GPT-4 [[25](#bib.bib25)], and Claude-2 [[1](#bib.bib1)]. The results demonstrate
    that our framework, utilizing GPT-4 as its base model, significantly outperforms
    the baseline, and achieves an eight-fold performance gain compared to the direct
    application of GPT-4\. Further analysis revealed additional factors, i.e., the
    planning of code change, line location within the code file, and code review process,
    that significantly influence the resolution rate.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们在SWE-bench上评估了我们的框架，并将其与现有的流行LLM（如ChatGPT-3.5 [[24](#bib.bib24)]、GPT-4
    [[25](#bib.bib25)]和Claude-2 [[1](#bib.bib1)]）的表现进行了比较。结果表明，我们的框架在使用GPT-4作为基础模型时，显著优于基线，并且相比直接应用GPT-4，性能提升了八倍。进一步分析揭示了其他因素，即代码更改的规划、代码文件中的行位置和代码审查过程，对解决率有显著影响。
- en: 'Our main contributions are summarized as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献总结如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct an empirical analysis of LLMs in resolving GitHub issues in two settings
    (i.e., w/ Oracle setting and w/o Oracle setting), and explore the correlation
    between line location, complexity of the code change, file location, and the success
    rate in resolving GitHub issues.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对LLM在两种环境下（即有Oracle设置和无Oracle设置）解决GitHub问题进行了实证分析，并探讨了行位置、代码更改的复杂性、文件位置与解决GitHub问题的成功率之间的相关性。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel LLM-based multi-agent framework, MAGIS, to alleviate the
    limitations of existing LLMs on GitHub issue resolution. Both our designed four-type
    agents and their collaboration for planning and coding unlock LLMs’ potential
    on the repository-level coding task.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种基于LLM的多智能体框架MAGIS，以缓解现有LLM在GitHub问题解决中的局限性。我们设计的四种类型的智能体及其在规划和编码中的协作，充分发挥了LLM在仓库级编码任务中的潜力。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We compare our framework and other strong LLM competitors (i.e., GPT-3.5, GPT-4,
    and Claude-2) on the SWE-bench dataset. The results show MAGIS significantly outperforms
    these competitors. Further analysis is conducted and verifies the effectiveness
    and necessity of our framework design.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将我们的框架与其他强大的LLM竞争者（即GPT-3.5、GPT-4和Claude-2）在SWE-bench数据集上进行了比较。结果显示，MAGIS显著优于这些竞争者。进一步的分析验证了我们框架设计的有效性和必要性。
- en: 2 Empirical Study
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 实证研究
- en: '2.1 RQ 1: What Impacts the Performance Under With-Oracle Setting?'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 研究问题1：什么因素影响有Oracle设置下的性能？
- en: The with-Oracle setting is a more straightforward setup for the GitHub issue
    resolution by supplying the specific files requiring modifications. Despite the
    observation that LLMs exhibit better performance in the with-Oracle setting compared
    to their performance without Oracle, the proportion of issues successfully resolved
    under the with-Oracle setting remains modest. Specifically, in the with-Oracle
    setting, GPT-4 achieved a resolved rate of only $1.74\%$ on the SWE-bench.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有Oracle设置是通过提供需要修改的特定文件来简化GitHub问题解决的设置。尽管观察到LLM在有Oracle设置下的表现优于无Oracle设置，但在有Oracle设置下成功解决的问题比例仍然很小。具体而言，在有Oracle设置下，GPT-4在SWE-bench上的解决率仅为$1.74\%$。
- en: 'To delve into the factors impacting the efficacy within the with-Oracle setting,
    we examined failed instances’ generation. This scrutiny led to the identification
    of two factors that could influence the resolved rate: (1) Line location (location
    of the modified line in the code file), and (2) The complexity of the code change.
    This information is important and provided in the code change of each commit in
    the code repository. A typical code change includes multiple changed hunks and
    each hunk contains the line numbers targeted for modification and the specifics
    of the changed code at these locations. Both the line locations and the changed
    content make a repository evolve.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨影响**与Oracle设置**中有效性的因素，我们检查了失败实例的生成。这项审查确定了两个可能影响解决率的因素：（1）行位置（代码文件中修改行的位置），和（2）代码变更的复杂性。这些信息在代码库中的每次提交的代码变更中提供。典型的代码变更包括多个修改块，每个修改块包含目标修改的行号和这些位置的代码变更细节。行位置和变更内容共同使得代码库不断演变。
- en: To validate this identification, we analyze the relation between the resolved
    rate and the two factors mentioned above.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这种识别，我们分析了解决率与上述两个因素之间的关系。
- en: 2.1.1 Line Location
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 行位置
- en: 'To quantitatively analyze the accuracy of line localization, we use the line
    numbers’ range of the modified content in the reference code change as the basis
    assuming that the correct modification location of the code change is uniquely
    determined in most cases. By calculating the overlap ratio of the line number
    ranges of the generated and reference, we can estimate the accuracy of line localization
    in the generation process. Specifically, for each instance, the line number ranges
    of the code change in the reference $r$ in the generated one. The formula for
    calculating the overlap ratio is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量分析行定位的准确性，我们使用参考代码变更中修改内容的行号范围作为基础，假设在大多数情况下，代码变更的正确修改位置是唯一确定的。通过计算生成和参考代码行号范围的重叠比率，我们可以估计生成过程中的行定位准确性。具体而言，对于每个实例，参考中的代码变更行号范围
    $r$ 在生成的代码中。计算重叠比率的公式如下：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\left|[s_{i},e_{i}]\cap[s_{j}^{\prime},e_{j}^{\prime}]\right|$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left|[s_{i},e_{i}]\cap[s_{j}^{\prime},e_{j}^{\prime}]\right|$。
- en: For $574$ GPT-3.5) and this ranking is also consistent with the proportion of
    instances solved by the three models. This phenomenon suggests that the performance
    of LLMs in generating the code change is probably related to their ability to
    locate code lines accurately.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $574$ GPT-3.5，这个排名也与三种模型解决实例的比例一致。这种现象表明，LLMs 在生成代码变更方面的表现可能与它们准确定位代码行的能力相关。
- en: Furthermore, we assess the relationship between the overlap ratio and the resolution
    of GitHub issues by calculating their correlation coefficient. Given that the
    distribution of these variables exhibits
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通过计算重叠比率与GitHub问题解决之间的相关系数来评估它们之间的关系。鉴于这些变量的分布表现出
- en: '![Refer to caption](img/2936eaaf2f63530410fb410b0ebb67ff.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2936eaaf2f63530410fb410b0ebb67ff.png)'
- en: 'Figure 1: The Comparison of Line Location Overlap Ratio between Three LLMs
    On SWE-bench.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：三种LLMs在SWE-bench上行位置重叠比率的比较。
- en: skewness, and the resolution result is binary (resolved or not), logistic regression
    is employed for the analysis across three LLMs. However, due to the limited number
    of successfully generated instances on GPT-4 and GPT-3.5, a statistically significant
    relationship (P-value < $0.05$, on Claude-2, there is a substantial and positive
    relation between improvements in the overlap ratio and the probability of successfully
    resolving issues. Consequently, the line location emerges as an important factor
    for GitHub issue resolution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 偏斜性，并且解决结果是二元的（解决或未解决），因此采用逻辑回归来分析三个LLMs之间的关系。然而，由于GPT-4和GPT-3.5成功生成的实例数量有限，在Claude-2上，重叠比率的改善与成功解决问题的概率之间存在显著且正相关的关系。因此，行位置成为GitHub问题解决的重要因素。
- en: 2.1.2 Complexity of the Code Change
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 代码变更的复杂性
- en: 'The complexity of the code change is reflected in various indices, including
    the number of modified files, functions, hunks, and lines added or deleted. Firstly,
    we quantitatively assess the complexity by calculating the value of various indices
    corresponding to the reference code change. Secondly, the coefficient is calculated
    between the numbers in each index and the resolution of issues. Table [1](#S2.T1
    "Table 1 ‣ 2.1.2 Complexity of the Code Change ‣ 2.1 RQ 1: What Impacts the Performance
    Under With-Oracle Setting? ‣ 2 Empirical Study ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution") shows the correlation scores
    between six indices and the issue resolution, under the logistic regression.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '代码更改的复杂性体现在多个指标中，包括修改的文件数量、函数数量、代码块数量以及新增或删除的行数。首先，我们通过计算与参考代码更改对应的各个指标的值来定量评估复杂性。其次，计算每个指标的数字与问题解决之间的系数。表 [1](#S2.T1
    "Table 1 ‣ 2.1.2 Complexity of the Code Change ‣ 2.1 RQ 1: What Impacts the Performance
    Under With-Oracle Setting? ‣ 2 Empirical Study ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution") 显示了在逻辑回归下，六个指标与问题解决之间的相关性得分。'
- en: 'Table 1: Correlation between the Complexity Indices and the Issue Resolution.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：复杂性指数与问题解决之间的相关性。
- en: '| LLM | # Files | # Functions | # Hunks | # Added LoC | # Deleted LoC | # Changed
    LoC |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| LLM | # 文件 | # 函数 | # 代码块 | # 新增代码行 | # 删除代码行 | # 修改代码行 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-3.5 | $-17.57$^* |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | $-17.57$^* |'
- en: '| GPT-4 | $-25.15$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $-25.15$ |'
- en: '| Claude-2 |   $-1.47$^* |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 |   $-1.47$^* |'
- en: '*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: The correlation between the index and the issue resolution is significant (P-value
    $<$).
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指标与问题解决之间的相关性显著（P值 $<$）。
- en: 'As shown in Table [1](#S2.T1 "Table 1 ‣ 2.1.2 Complexity of the Code Change
    ‣ 2.1 RQ 1: What Impacts the Performance Under With-Oracle Setting? ‣ 2 Empirical
    Study ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue
    ReSolution"), all three LLMs, i.e., GPT-3.5, GPT-4, and Claude-2, demonstrate
    a statistically significant correlation with the issue resolution across several
    indices, i.e., p-value less than $0.05$. The correlation scores for the number
    of files and functions modified are notably negative for all models, indicating
    that an increase in these indices is associated with a decreasing likelihood of
    issue resolution. This suggests that the more complex the code change, as indicated
    by a higher number of files and functions modified, may hinder the issue resolution.
    Compared with GPT-3.5 and GPT-4, Claude-2 exhibits a different pattern, with much
    lower negative correlations for the number of files and functions, which indicates
    it is a more efficient approach to generate the code change for GitHub issue resolution.
    However, it also shows significant negative correlations across other indices
    such as the number of hunks, added lines of code (LoC), deleted LoC, and changed
    LoC. The analysis reveals a relationship between the complexity, as measured by
    several indices, and whether to successfully resolve the issues in software evolution.
    The negative correlations suggest that increased complexity, particularly in terms
    of the number of files and functions changed, tends to hinder issue resolution.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [1](#S2.T1 "Table 1 ‣ 2.1.2 Complexity of the Code Change ‣ 2.1 RQ 1: What
    Impacts the Performance Under With-Oracle Setting? ‣ 2 Empirical Study ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution") 所示，所有三个LLM，即GPT-3.5、GPT-4和Claude-2，在多个指标上与问题解决表现出统计学上的显著相关性，即p值小于$0.05$。文件和函数修改数量的相关性对所有模型而言均显著为负，表明这些指标的增加与问题解决的可能性降低相关。这表明，代码更改越复杂（由更高的文件和函数修改数量表示），可能会阻碍问题的解决。与GPT-3.5和GPT-4相比，Claude-2展示了不同的模式，对文件和函数数量的负相关性显著较低，表明它在生成GitHub问题解决代码更改方面是更有效的方式。然而，它在其他指标上，如代码块数量、新增代码行（LoC）、删除LoC和修改LoC，也显示出显著的负相关性。分析揭示了复杂性（通过多个指标衡量）与软件演进中问题是否成功解决之间的关系。负相关性表明，特别是在文件和函数更改的数量方面，复杂性的增加往往会阻碍问题解决。'
- en: '2.2 RQ 2: What Impacts the Performance Under Without-Oracle Setting?'
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.2 RQ 2: 在没有Oracle的设置下，什么因素影响性能？'
- en: The difference between the with-Oracle setting and the without-Oracle setting
    lies in the necessity for the latter to identify the modified files. Consequently,
    file locating emerges as a crucial factor influencing performance under the without-Oracle
    setting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有Oracle设置和没有Oracle设置之间的区别在于后者需要识别修改的文件。因此，文件定位成为影响没有Oracle设置下性能的关键因素。
- en: Jimenez et al. [[15](#bib.bib15)] employ the BM25 method [[28](#bib.bib28)]
    to retrieve relevant code files that are subsequently utilized as input to the
    LLM. After employing retrieval methods, it is necessary to select the top-$K$
    files or truncate the content based on the maximum context length of the LLM.
    Incorporating more files can enhance recall scores. However, it also imposes significant
    demands on the capabilities of LLMs. As demonstrated by the study [[15](#bib.bib15)],
    Claude-2 exhibits a reduction at the resolved ratio as recall scores increase.
    This decline may be attributable to the inclusion of irrelevant files or the limited
    capacity of LLMs to process longer contexts effectively. Consequently, exploiting
    the performance of LLMs can be better achieved by striving for higher recall scores
    with a minimized set of files, thus suggesting a strategic balance between recall
    optimization and the number of chosen files.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Jimenez 等人[[15](#bib.bib15)] 使用 BM25 方法[[28](#bib.bib28)] 来检索相关的代码文件，这些文件随后被用作
    LLM 的输入。在使用检索方法后，需要选择前 $K$ 个文件或根据 LLM 的最大上下文长度截断内容。包含更多文件可以提高召回率。然而，这也对 LLM 的能力提出了重大要求。如研究[[15](#bib.bib15)]所示，Claude-2
    在召回率增加时解决率有所下降。这一下降可能是由于包含了无关文件或 LLM 有效处理较长上下文的能力有限。因此，通过在较小的文件集内追求更高的召回率，可以更好地利用
    LLM 的性能，这表明在召回优化和选择文件数量之间需要找到战略性平衡。
- en: 3 Methodology
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'In this section, we elaborate on our LLM-based multi-agent framework, MAGIS,
    for GitHub issue resolution. The framework resolves each issue by applying the
    generated code change to the repository. As shown in Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"), we first provide the overview of our
    framework (§[3.1](#S3.SS1 "3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS :
    LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")), followed by the
    role design of each type agent and how these roles differ from human workflows
    (§[3.2](#S3.SS2 "3.2 Agent Role Design ‣ 3 Methodology ‣ \stackon[0pt]MAGIS :
    LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")). Lastly, we detail
    the collaborative process among these roles within our framework (§[3.3](#S3.SS3
    "3.3 Collaborative Process ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution")).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们详细介绍了基于 LLM 的多智能体框架 MAGIS，用于 GitHub 问题解决。该框架通过将生成的代码变更应用于仓库来解决每个问题。如图[2](#S3.F2
    "Figure 2 ‣ 3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution")所示，我们首先提供了框架的概述 (§[3.1](#S3.SS1 "3.1 Overview
    ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution"))，然后是各类型智能体的角色设计及这些角色如何与人类工作流程不同 (§[3.2](#S3.SS2 "3.2 Agent
    Role Design ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"))。最后，我们详细说明了这些角色在我们框架中的协作过程 (§[3.3](#S3.SS3 "3.3
    Collaborative Process ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"))。'
- en: 3.1 Overview
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: 'The architecture of our framework is illustrated in Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"), encompassing four key roles of agents
    working collaboratively in the workflow.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '我们框架的架构如图[2](#S3.F2 "Figure 2 ‣ 3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")所示，涵盖了在工作流程中协作的四个关键角色。'
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Manager. This role tasks with team assembly, meeting organization, and plan
    formulation.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经理。此角色负责团队组建、会议组织和计划制定。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Repository Custodian. The custodian is responsible for locating the relevant
    files in the repository according to the GitHub issue and recording the change
    of the repository.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仓库保管员。保管员负责根据 GitHub 问题在仓库中定位相关文件，并记录仓库的变更。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Developer. This role participates in planning discussions and completes tasks
    from the manager.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开发人员。此角色参与计划讨论，并完成来自经理的任务。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quality Assurance (QA) Engineer. The QA engineer reviews the code change from
    developers to ensure the quality of the whole repository.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 质量保证（QA）工程师。QA 工程师审查开发人员的代码变更，以确保整个仓库的质量。
- en: '![Refer to caption](img/2c4000fc44f7c98908c62309c80dbc06.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2c4000fc44f7c98908c62309c80dbc06.png)'
- en: 'Figure 2: Overview of Our Framework, MAGIS.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们框架的概述，MAGIS。
- en: The collaborative process involves planning and coding. In the planning, a GitHub
    issue is assigned to the project manager and the repository custodian. The repository
    custodian identifies candidate files relevant to the issue for modification. With
    the issue description and a list of candidate files, the manager defines tasks
    and assembles a team, where each member is a developer specifically designed for
    the defined task. The manager holds a kick-off team meeting with developers and
    devises a plan. During the coding, developers undertake their assigned tasks from
    the manager, and the QA engineer reviews each code change. If a change fails to
    meet quality standards, the QA engineer provides feedback, prompting further revisions
    until the change satisfies quality criteria or a set iteration limit is reached.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 协作过程包括计划和编码。在计划阶段，GitHub 问题被分配给项目经理和代码库管理员。代码库管理员确定与问题相关的候选文件进行修改。根据问题描述和候选文件列表，经理定义任务并组建一个团队，每个成员都是专门设计来完成定义任务的开发人员。经理与开发人员举行启动团队会议并制定计划。在编码阶段，开发人员承担经理分配的任务，QA
    工程师审核每个代码更改。如果更改未达到质量标准，QA 工程师提供反馈，促使进一步修订，直到更改符合质量标准或达到设定的迭代限制。
- en: 3.2 Agent Role Design
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 代理角色设计
- en: 'Our workflow draws inspiration from the GitHub Flow ²²2[https://docs.github.com/en/get-started/using-github/github-flow](https://docs.github.com/en/get-started/using-github/github-flow),
    an effective human workflow paradigm, adopted by many software teams. Both the
    human workflow and our LLM-based agent framework prioritize collaboration among
    individuals with diverse skills. While the underlying principles are similar,
    there are notable differences. Accordingly, we have tailored the roles as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作流程受到 GitHub Flow ²²2[https://docs.github.com/en/get-started/using-github/github-flow](https://docs.github.com/en/get-started/using-github/github-flow)
    的启发，这是一种有效的人类工作流程范式，已经被许多软件团队采用。无论是人类工作流程还是我们的基于 LLM 的代理框架，都优先考虑具有多样技能的个人之间的协作。虽然基本原理相似，但也存在显著的差异。因此，我们根据以下方式定制了角色：
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '![[Uncaptioned image]](img/ea7054107a57f5ef0a28e58dfdd88100.png) Manager. The
    manager’s role is pivotal in planning. In conventional setups, managers decompose
    the issue into tasks according to the pre-formed team and allocate these tasks
    for members with different skills. In contrast, our manager agent can first decompose
    the issue into tasks and then design developer agents to form a team. This setup
    improves team flexibility and adaptability, enabling the formation of teams that
    can meet various issues efficiently.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![[未加说明的图片]](img/ea7054107a57f5ef0a28e58dfdd88100.png) 经理。经理在规划中的角色至关重要。在传统设置中，经理将问题分解为任务，依据预先组成的团队分配任务给具有不同技能的成员。相比之下，我们的经理代理可以首先将问题分解为任务，然后设计开发人员代理以组成团队。这种设置提高了团队的灵活性和适应性，使团队能够高效地解决各种问题。'
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '![[Uncaptioned image]](img/189d9c131ba0fd728acb03276fa6ee89.png) Repository
    Custodian. Considering extensive files in a repository, the custodian agent’s
    task is to locate files relevant to the issue. Different from humans, who can
    browse through the entire repository, the LLM-based agent faces challenges in
    browsing. Although LLMs, including GPT-4 [[23](#bib.bib23)], have extended context
    limits, their application is constrained in two aspects. First, it is a high computational
    cost to query the whole repository for each update while some repositories update
    frequently. Second, the performance of LLMs degrades when relevant information
    occurs in the middle of the long context input [[19](#bib.bib19), [42](#bib.bib42)].'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![[未加说明的图片]](img/189d9c131ba0fd728acb03276fa6ee89.png) 代码库管理员。考虑到代码库中的文件数量庞大，管理员代理的任务是定位与问题相关的文件。与可以浏览整个代码库的人类不同，基于
    LLM 的代理在浏览时面临挑战。虽然 LLM，包括 GPT-4 [[23](#bib.bib23)]，已经扩展了上下文限制，但其应用仍受两方面的限制。首先，对于每次更新查询整个代码库的计算成本很高，而一些代码库更新频繁。其次，当相关信息出现在长上下文输入的中间时，LLM
    的性能会下降 [[19](#bib.bib19), [42](#bib.bib42)]。'
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '![[Uncaptioned image]](img/f81db96f243032cd7dbf128f08d1ac74.png) Developer.
    In contrast to human developers, the developer agent can work continuously, and
    complete a task in minutes even seconds. Therefore, scheduling the agent to work
    in parallel is easier than scheduling humans as the latter requires considering
    other than work while the former only needs to ensure the tasks are independent.
    Additionally, although there are numerous developer agents capable of generating
    code [[12](#bib.bib12), [26](#bib.bib26)], their ability to modify existing code
    is not equally proficient. To address this issue, our framework decomposes the
    code modification process into atomic operations, which encompass the code generation.
    This approach enables developers to leverage the benefits of automatic code generation
    thereby generating applicable code changes.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![[未标注图片]](img/f81db96f243032cd7dbf128f08d1ac74.png) 开发者。与人类开发者相比，开发者代理可以持续工作，并在几分钟甚至几秒钟内完成任务。因此，安排代理并行工作比安排人类工作更容易，因为后者需要考虑除了工作以外的因素，而前者只需确保任务之间是独立的。此外，虽然有许多开发者代理能够生成代码 [[12](#bib.bib12),
    [26](#bib.bib26)]，但它们修改现有代码的能力却不尽相同。为了解决这个问题，我们的框架将代码修改过程分解为原子操作，其中包括代码生成。这种方法使开发者能够利用自动代码生成的好处，从而生成适用的代码更改。'
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '![[Uncaptioned image]](img/03a506407ae49cdf50e2f030e9918582.png) QA Engineer.
    In software evolution, QA engineers, especially code reviewers, play a crucial
    role in maintaining software quality [[21](#bib.bib21), [17](#bib.bib17)]. Despite
    its critical role, code review practices are often undervalued or even overlooked [[3](#bib.bib3)].
    Such neglect can hinder software development, illustrated by instances where developers
    may experience delays of up to 96 hours awaiting code review feedback [[5](#bib.bib5)].
    To address this problem, our framework pairs each developer agent with a QA engineer
    agent, designed to offer task-specific, timely feedback. This personalized QA
    approach aims to boost the review process thereby better ensuring the software
    quality.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![[未标注图片]](img/03a506407ae49cdf50e2f030e9918582.png) QA 工程师。在软件演进过程中，QA 工程师，尤其是代码审查员，在维护软件质量方面扮演着关键角色 [[21](#bib.bib21),
    [17](#bib.bib17)]。尽管其角色至关重要，代码审查实践往往被低估甚至忽视 [[3](#bib.bib3)]。这种忽视可能会阻碍软件开发，例如开发者可能需要等待长达
    96 小时的代码审查反馈 [[5](#bib.bib5)]。为了解决这个问题，我们的框架将每个开发者代理与 QA 工程师代理配对，旨在提供任务特定的、及时的反馈。这种个性化的
    QA 方法旨在提升审查过程，从而更好地确保软件质量。'
- en: 3.3 Collaborative Process
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 协作过程
- en: 3.3.1 Planning
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 规划
- en: 'Three types of role agents engage in the planning process: the Repository Custodian,
    the Manager, and the Developer. The process comprises three phases: locating code
    files, team building, and the kick-off meeting.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 三种角色代理参与规划过程：存储库管理员、经理和开发者。该过程包括三个阶段：定位代码文件、团队建设和启动会议。
- en: Algorithm 1 Locating
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 定位
- en: '1:  Input: repository: ${\mathcal{R}}_{i}$22:     end if23:  end for'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  输入: 存储库: ${\mathcal{R}}_{i}$22:     结束 if23:  结束 for'
- en: Locating Code Files.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定位代码文件。
- en: 'Firstly, the repository custodian employs the BM25 algorithm [[28](#bib.bib28)]
    to rank the files in the repository based on the GitHub issue description. Subsequently,
    the top $k$ files are selected as potential candidates for further coding. However,
    as described in §[2.2](#S2.SS2 "2.2 RQ 2: What Impacts the Performance Under Without-Oracle
    Setting? ‣ 2 Empirical Study ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"), this simple retrieval method can introduce irrelevant
    files, increasing the cost and reducing the effectiveness of subsequent code changing
    by LLMs. Therefore, it is necessary to filter them according the relevance. While
    it is feasible to directly assess the relevance between each file and the issue
    by LLMs, many queries are redundant as some code snippets in the previous request
    are duplicated, leading to unnecessary costs. Considering that applying the code
    change often modifies a specific part of the file rather than the whole file,
    we propose a method to reuse the previously requested information.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，代码库管理员使用 BM25 算法 [[28](#bib.bib28)] 根据 GitHub 问题描述对代码库中的文件进行排名。随后，选择排名前
    $k$ 的文件作为进一步编码的潜在候选。然而，如 §[2.2](#S2.SS2 "2.2 RQ 2: 在无 Oracle 设置下的性能影响因素？ ‣ 2 实证研究
    ‣ \stackon[0pt]MAGIS : 基于 LLM 的多代理框架用于 GitHub 问题解决") 中所述，这种简单的检索方法可能引入不相关的文件，增加成本并降低
    LLM 后续代码更改的有效性。因此，有必要根据相关性筛选这些文件。虽然可以通过 LLM 直接评估每个文件与问题之间的相关性，但许多查询是冗余的，因为先前请求中的一些代码片段是重复的，从而导致不必要的成本。考虑到应用代码更改通常会修改文件的特定部分而不是整个文件，我们提出了一种重新使用先前请求信息的方法。'
- en: 'As Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3.1 Planning ‣ 3.3 Collaborative Process
    ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution") demonstrates, when a file $f_{i}$ in fewer length. Based on
    this relevance, the custodian agent filters candidate files, allowing the manager
    agent to define tasks with these relevant files.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '如算法 [1](#alg1 "算法 1 ‣ 3.3.1 计划 ‣ 3.3 协作过程 ‣ 3 方法论 ‣ \stackon[0pt]MAGIS : 基于
    LLM 的多代理框架用于 GitHub 问题解决") 所示，当文件 $f_{i}$ 较短时。基于这种相关性，管理员代理筛选候选文件，允许经理代理定义与这些相关文件有关的任务。'
- en: Team Building.
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 团队建设。
- en: 'In this process, the manager agent has the flexibility to “recruit” team members
    as the issue needs. Firstly, upon receiving the candidate files, the manager begins
    with analyzing the GitHub issue for the repository and breaks them into detailed
    file-level tasks. Specifically, for each code file $f_{i}$, thus forming the development
    team. The details of the team building are shown in the part of Algorithm [2](#alg2
    "Algorithm 2 ‣ Team Building. ‣ 3.3.1 Planning ‣ 3.3 Collaborative Process ‣ 3
    Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '在此过程中，经理代理可以根据问题的需求“招募”团队成员。首先，接收到候选文件后，经理开始分析 GitHub 问题，并将其拆分为详细的文件级任务。具体来说，对于每个代码文件
    $f_{i}$，形成开发团队。团队建设的详细信息见算法 [2](#alg2 "算法 2 ‣ 团队建设 ‣ 3.3.1 计划 ‣ 3.3 协作过程 ‣ 3 方法论
    ‣ \stackon[0pt]MAGIS : 基于 LLM 的多代理框架用于 GitHub 问题解决") 部分。'
- en: Algorithm 2 Making the Plan
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 制定计划
- en: '1:  Input: candidate files: ${\mathcal{C}}_{i}^{k}$'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：候选文件: ${\mathcal{C}}_{i}^{k}$'
- en: Kick-off Meeting.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 启动会议。
- en: 'After building the team, the project manager organizes a kick-off meeting.
    This meeting serves multiple purposes: (1) To confirm whether the tasks assigned
    by the project manager are reasonable and ensure that all developers in the team
    can collaboratively resolve the issue $q_{x}$. This work plan is presented as
    code, and embedded into the main program for execution.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在组建团队之后，项目经理组织一个启动会议。这个会议有多个目的：（1）确认项目经理分配的任务是否合理，并确保团队中的所有开发者可以协作解决问题 $q_{x}$。这个工作计划以代码形式呈现，并嵌入到主程序中执行。
- en: 3.3.2 Coding
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 编码
- en: Algorithm 3 Coding Task Execution
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 编码任务执行
- en: '1:  Input: file-task set: ${\mathcal{T}}_{i}^{k}$23:  end for'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：文件-任务集: ${\mathcal{T}}_{i}^{k}$23: 结束'
- en: 'Two types of agents participate in the coding process: developers and QA engineers.
    As outlined in Algorithm [3](#alg3 "Algorithm 3 ‣ 3.3.2 Coding ‣ 3.3 Collaborative
    Process ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"), for each task $t_{i}$ as the issue solution.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '有两种类型的代理参与编码过程：开发人员和 QA 工程师。如算法 [3](#alg3 "算法 3 ‣ 3.3.2 编码 ‣ 3.3 协作过程 ‣ 3 方法论
    ‣ \stackon[0pt]MAGIS : 基于 LLM 的多代理框架用于 GitHub 问题解决") 中所述，对于每个任务 $t_{i}$ 作为问题的解决方案。'
- en: 4 Experiments and Analysis
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与分析
- en: 'To validate the performance of our framework, we conduct experiments and compare
    it against other popular Large Language Models (LLMs) to demonstrate its overall
    effectiveness (§[4.2](#S4.SS2 "4.2 RQ 3: How Effective Is Our Framework? ‣ 4 Experiments
    and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution")). In addition, to further investigate the effectiveness of
    our framework, we assessed its performance in two processes: planning (§[4.3](#S4.SS3
    "4.3 RQ 4: How Effective Is Our Planning Process? ‣ 4 Experiments and Analysis
    ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"))
    and coding (§[4.4](#S4.SS4 "4.4 RQ 5: How Effective Is Our Coding Process? ‣ 4
    Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution")).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证我们框架的性能，我们进行实验并与其他流行的大型语言模型（LLMs）进行比较，以展示其总体有效性（§[4.2](#S4.SS2 "4.2 RQ
    3: 我们的框架效果如何？ ‣ 4 实验与分析 ‣ \stackon[0pt]MAGIS : 基于LLM的多智能体框架用于GitHub问题解决")）。此外，为了进一步调查我们框架的有效性，我们评估了其在两个过程中的表现：规划（§[4.3](#S4.SS3
    "4.3 RQ 4: 我们的规划过程效果如何？ ‣ 4 实验与分析 ‣ \stackon[0pt]MAGIS : 基于LLM的多智能体框架用于GitHub问题解决")）和编码（§[4.4](#S4.SS4
    "4.4 RQ 5: 我们的编码过程效果如何？ ‣ 4 实验与分析 ‣ \stackon[0pt]MAGIS : 基于LLM的多智能体框架用于GitHub问题解决")）。'
- en: 4.1 Setup
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 4.1.1 Dataset and Experimental Settings
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 数据集和实验设置
- en: 'In the experiments, we employ the SWE-bench dataset as the evaluation benchmark
    because it is the latest dataset specifically designed for evaluating the performance
    of the GitHub issue resolution. SWE-bench comprises $2,294$ popular Python repositories,
    representing real software evolution requirements. It includes two context settings:
    an Oracle retrieval and sparse retrieval. In the former setting (w/ Oracle), the
    LLM receives the files to modify, whereas, in the latter (w/o Oracle), the method
    needs to retrieve the files for modification.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用SWE-bench数据集作为评估基准，因为它是最新的数据集，专门用于评估GitHub问题解决的性能。SWE-bench包含$2,294$个受欢迎的Python代码库，代表了真实的软件演进需求。它包括两种上下文设置：Oracle检索和稀疏检索。在前一种设置中（w/
    Oracle），LLM接收需要修改的文件，而在后一种设置中（w/o Oracle），该方法需要检索用于修改的文件。
- en: Given the observation that experimental outcomes on the $25\%$ subset previously
    utilized in experiments for GPT-4 according to their materials ³³3[https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80](https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80).
    Moreover, the experimental scores for the five LLMs, have been made available
    by them ⁴⁴4[https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr](https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于实验结果表明，在之前用于GPT-4实验的$25\%$子集上，相关材料³³3[https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80](https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80)中，实验分数也已由他们提供⁴⁴4[https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr](https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr)。
- en: Our framework is flexible to integrate various LLMs. To compare with the scores
    reported by SWE-bench, we select GPT-4 as the base LLM. Another reason for the
    selection is that GPT-4 shows remarkable performance on code generation and understanding,
    which has been demonstrated on benchmarks such as MBPP [[2](#bib.bib2)] and HumanEval [[9](#bib.bib9)].
    Claude-2 is not chosen due to the unavailability of API access.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架灵活地集成各种LLM。为了与SWE-bench报告的分数进行比较，我们选择了GPT-4作为基础LLM。另一个选择理由是GPT-4在代码生成和理解方面表现出色，这已在MBPP
    [[2](#bib.bib2)] 和HumanEval [[9](#bib.bib9)]等基准测试中得到证明。Claude-2没有被选择，因为没有API访问权限。
- en: 4.1.2 Metric
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 指标
- en: Following the SWE-bench [[15](#bib.bib15)], we use the applied and resolved
    ratio to evaluate the performance under the with-Oracle setting. Specifically,
    the applied ratio indicates the proportion of instances where the code change
    is successfully generated and can be applied to the existing code repository using
    Git tools, i.e.,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SWE-bench [[15](#bib.bib15)]，我们使用应用与解决的比例来评估在有Oracle设置下的性能。具体而言，应用比例表示成功生成并可以使用Git工具应用到现有代码库中的代码更改实例的比例，即，
- en: '|  | $\displaystyle\text{Applied Ratio}=\frac{\left&#124;{\mathcal{D}}\right&#124;}{\left&#124;{\mathcal{I}}\right&#124;},~{}$
    |  | (2) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{应用比例}=\frac{\left|{\mathcal{D}}\right|}{\left|{\mathcal{I}}\right|},~{}$
    |  | (2) |'
- en: where ${\mathcal{D}}$ is the set of all instances in the test set. The resolved
    ratio refers to the proportion of instances in which the code change is successfully
    applied and passed a series of tests, i.e.,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathcal{D}}$ 是测试集中的所有实例集合。解决率指的是代码更改成功应用并通过一系列测试的实例比例，即：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $T_{old}$.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T_{old}$。
- en: 'The recall score versus file number curve is used to measure the effectiveness
    of file locating for the without-Oracle setting. The recall score refers to the
    proportion of files that are successfully located out of all the files that require
    modification. The formula for calculating the file location recall score for the
    $i$-th instance is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率与文件数曲线用于衡量无Oracle设置下文件定位的有效性。召回率指的是成功定位的文件占所有需要修改的文件的比例。计算第 $i$ 个实例的文件位置召回率的公式如下：
- en: '|  | $\displaystyle\text{Recall}=\frac{\left&#124;{\mathcal{G}}_{i}\cap{\mathcal{R}}_{i}\right&#124;}{\left&#124;{\mathcal{R}}_{i}\right&#124;}\times
    100\%,~{}$ |  | (4) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Recall}=\frac{\left|{\mathcal{G}}_{i}\cap{\mathcal{R}}_{i}\right|}{\left|{\mathcal{R}}_{i}\right|}\times
    100\%,~{}$ |  | (4) |'
- en: where ${\mathcal{G}}_{i}=\sum_{j=0}^{n}{g_{i,j}}$. In this curve, “file number”
    refers to the average number of files that need to be processed across all instances
    to achieve the given recall score. Specifically, it illustrates how many files
    averagely need to be located by our framework before reaching the recall score
    denoted by the curve at any point. This metric represents both the effectiveness
    and efficiency of file locating.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathcal{G}}_{i}=\sum_{j=0}^{n}{g_{i,j}}$。在此曲线中，“文件数”指的是为了达到给定召回率而需要处理的文件平均数量。具体而言，它说明了我们的框架在达到任意点所示的召回率之前，平均需要定位多少文件。此指标代表了文件定位的有效性和效率。
- en: '4.2 RQ 3: How Effective Is Our Framework?'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 RQ 3：我们的框架有效性如何？
- en: 'The comparative performance analysis between our framework and other LLMs on
    the same dataset is presented in Table [2](#S4.T2 "Table 2 ‣ 4.2 RQ 3: How Effective
    Is Our Framework? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution"). The results indicate that
    our framework significantly outperforms other LLMs. Notably, with a resolved ratio
    of $13.94\%$, our framework’s effectiveness is eight-fold that of the base model,
    GPT-4\. This substantial increase underscores our framework’s capability to harness
    the potential of LLMs more effectively. Furthermore, when contrasted with the
    previous state-of-the-art LLM, Claude-2, our framework’s resolved ratio exceeds
    that benchmark by more than two-fold. This superior performance unequivocally
    establishes the advance of our method. As Devin [[34](#bib.bib34)] uses a different
    subset from the evaluated subset of GPT-4 mentioned in the SWE-bench paper [[15](#bib.bib15)],
    the comparison between it and ours is discussed in § [5.2](#S5.SS2 "5.2 Comparison
    with Devin. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution").'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [2](#S4.T2 "Table 2 ‣ 4.2 RQ 3: How Effective Is Our Framework? ‣ 4 Experiments
    and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution")中展示了我们框架与其他LLM在相同数据集上的比较性能分析。结果表明，我们的框架显著优于其他LLM。值得注意的是，我们的框架在解决率为
    $13.94\%$ 的情况下，其效果是基础模型 GPT-4 的八倍。这一显著提升突显了我们框架更有效地利用LLM的潜力。此外，与之前的最先进LLM Claude-2
    相比，我们框架的解决率超过了该基准两倍以上。这一优越的表现明确确立了我们方法的进步。由于 Devin [[34](#bib.bib34)] 使用的是与 SWE-bench
    论文[[15](#bib.bib15)]中提到的 GPT-4 的评估子集不同的子集，因此对比讨论见 § [5.2](#S5.SS2 "5.2 Comparison
    with Devin. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution")。'
- en: 'Table 2: The Comparison of Overall Performance between MAGIS and Baselines
    on SWE-bench.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：MAGIS 与基线在 SWE-bench 上的整体性能比较。
- en: '| Method | % Applied | % Resolved |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 应用百分比 | 解决百分比 |'
- en: '| GPT-3.5 | $11.67$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | $11.67$ |'
- en: '| Claude-2 | $49.36$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | $49.36$ |'
- en: '| GPT-4 | $13.24$ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $13.24$ |'
- en: '| SWE-Llama 7b | $51.56$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SWE-Llama 7b | $51.56$ |'
- en: '| SWE-Llama 13b | $49.13$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| SWE-Llama 13b | $49.13$ |'
- en: '| Devin [[34](#bib.bib34)]^* | $-$^* |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Devin [[34](#bib.bib34)]^* | $-$^* |'
- en: '| MAGIS | 97.39 | 13.94 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MAGIS | 97.39 | 13.94 |'
- en: '| MAGIS (w/o QA) | $92.71$ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MAGIS（无QA） | $92.71$ |'
- en: '| MAGIS (w/o hints) | $94.25$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| MAGIS（无提示） | $94.25$ |'
- en: '| MAGIS (w/o hints, w/o QA) | $91.99$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MAGIS（无提示，无QA） | $91.99$ |'
- en: '*'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: Note that this work is evaluated on a randomly chosen 25% test set, but this
    subset differs from the 25% subset experimented on GPT-4 in the SWE-bench.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，这项工作是在随机选择的25%测试集上评估的，但该子集与在SWE-bench中对GPT-4进行实验的25%子集不同。
- en: 'The ablation study is designed to simulate two scenarios: (1) Without QA (w/o
    QA): Considering the QA engineer agent as optional within our framework, we directly
    evaluate the code changes generated by the developer agent, bypassing the QA process.
    This scenario aims to investigate the effectiveness and necessity of QA engineer
    review. (2) Without hints (w/o hints): Hints refer to the textual content found
    in the comments section of pull requests, which are typically created prior to
    the first commit of the pull request. The absence of hints (w/o hints) means our
    framework operates without any clarifications except for the issue, despite such
    information being available on GitHub before the issue resolution process begins.
    This analysis aims to explore if the participation of humans could potentially
    improve the success rate of issue resolution.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 脱离研究旨在模拟两种情况：（1）没有QA（w/o QA）：将QA工程师代理视为框架中的可选项，我们直接评估开发者代理生成的代码更改，绕过QA过程。这个场景旨在调查QA工程师审查的有效性和必要性。（2）没有提示（w/o
    hints）：提示指的是在拉取请求的评论部分找到的文本内容，这些通常是在拉取请求的第一次提交之前创建的。没有提示（w/o hints）意味着我们的框架在没有任何澄清的情况下操作，尽管这些信息在问题解决过程开始之前在GitHub上是可用的。这个分析旨在探索人类的参与是否可能提高问题解决的成功率。
- en: Our framework demonstrates the capability to significantly enhance issue resolution,
    even without the QA engineer or hints. It achieves a resolved ratio of $8.71$,
    respectively. These findings serve to underscore the value of QA engineers and
    the participation of humans, as demonstrated by the resolved rates achieved through
    their integration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架展示了显著提升问题解决能力的能力，即使没有QA工程师或提示。它分别实现了$8.71$的解决比率。这些发现强调了QA工程师和人类参与的价值，如通过其整合所取得的解决率所示。
- en: 'For instance, to resolve the issue ⁵⁵5[https://code.djangoproject.com/ticket/30255](https://code.djangoproject.com/ticket/30255)
    from the repository Django ⁶⁶6[https://github.com/django/django/](https://github.com/django/django/),
    the developer modifies four hunks in two files ⁷⁷7[https://github.com/django/django/pull/12155/files](https://github.com/django/django/pull/12155/files),
    as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"). Despite the availability of two files in the with-Oracle
    setting, our proposed framework opts for modifications in only one file, as illustrated
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework? ‣ 4
    Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"). Remarkably, this simpler code change enables the
    repository to successfully pass all requisite test cases.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，要解决来自Django仓库的 ⁵⁵5[https://code.djangoproject.com/ticket/30255](https://code.djangoproject.com/ticket/30255)的问题，开发者在两个文件中修改了四个部分 ⁷⁷7[https://github.com/django/django/pull/12155/files](https://github.com/django/django/pull/12155/files)，如图[5](#S4.F5
    "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework? ‣ 4 Experiments and Analysis
    ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")所示。尽管在有Oracle的设置中有两个文件可用，我们提出的框架仅选择在一个文件中进行修改，如图[5](#S4.F5
    "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework? ‣ 4 Experiments and Analysis
    ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")所示。这一简单的代码更改使得该仓库成功通过所有必要的测试用例。'
- en: '![Refer to caption](img/1304898b168a42ed24281c38872e18f6.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1304898b168a42ed24281c38872e18f6.png)'
- en: 'Figure 3: Case from Django (Gold).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Django（Gold）案例。
- en: '![Refer to caption](img/0497113f4664ff5eed2453ff2f089368.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0497113f4664ff5eed2453ff2f089368.png)'
- en: 'Figure 4: Case from Django (Ours).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Django（我们的）案例。
- en: '![Refer to caption](img/1cc964793c5456536d433a289a1d47cc.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1cc964793c5456536d433a289a1d47cc.png)'
- en: 'Figure 5: Comparison of Recall Scores between MAGIS and BM25.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：MAGIS与BM25的召回分数比较。
- en: '4.3 RQ 4: How Effective Is Our Planning Process?'
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 RQ 4：我们的规划过程有多有效？
- en: 'To investigate the effectiveness of the planning process, we analyze the repository
    custodian and project manager agent, respectively. The performance of the repository
    custodian agent is observed in the recall score versus the file number curve,
    as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution") with the horizontal axis representing the average
    number of files and the vertical axis denoting the recall score associated with
    that number of files. This curve illustrates that our method consistently outperforms
    the BM25 baseline across different numbers of selected files, which suggests that
    our custodian can find as many relevant code files as possible with the lowest
    possible number of files.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查规划过程的有效性，我们分别分析了存储库管理员和项目经理代理。存储库管理员代理的表现通过召回评分与文件数量曲线来观察，如图[5](#S4.F5 "图5
    ‣ 4.2 RQ 3：我们的框架有效吗？ ‣ 4 实验与分析 ‣ \stackon[0pt]MAGIS：基于LLM的GitHub问题解决多代理框架")所示，横轴表示平均文件数量，纵轴表示与该文件数量相关的召回评分。这条曲线表明，我们的方法在不同选择文件数量的情况下始终优于BM25基线，表明我们的管理员可以在尽可能少的文件数量下找到尽可能多的相关代码文件。
- en: For the project manager agent, we examined the alignment of its generated task
    descriptions with the reference code change. A higher correlation score indicates
    a better alignment and thus, a more accurate and effective planning direction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于项目经理代理，我们检查了其生成的任务描述与参考代码更改的对齐情况。更高的相关性评分表示更好的对齐，因此，规划方向更准确、更有效。
- en: '![Refer to caption](img/e544d1aa5b9658e4d8982d8e55a9985d.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/e544d1aa5b9658e4d8982d8e55a9985d.png)'
- en: 'Figure 6: Distribution of the Correlation Score Between the Generated Task
    Description and the Reference Code Change.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：生成的任务描述与参考代码更改之间的相关性评分分布。
- en: 'The correlation scores are determined by GPT-4 based on a set of criteria defined
    in Table [3](#S4.T3 "Table 3 ‣ 4.3 RQ 4: How Effective Is Our Planning Process?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"), which spans from a score of $1$. This signifies
    that when the generated task description closely aligns with the reference, there
    is a higher possibility to resolve the issue.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性评分由GPT-4根据表[3](#S4.T3 "表3 ‣ 4.3 RQ 4：我们的规划过程有效吗？ ‣ 4 实验与分析 ‣ \stackon[0pt]MAGIS：基于LLM的GitHub问题解决多代理框架")中定义的一组标准确定，评分范围从$1$开始。这表明，当生成的任务描述与参考高度一致时，更有可能解决问题。
- en: The analysis above demonstrates the effectiveness of both the repository custodian
    and the project manager agent in the planning process of our framework.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分析展示了在我们框架的规划过程中，存储库管理员和项目经理代理的有效性。
- en: 'Table 3: The Meaning of Scores in GPT-4 Evaluation on the Correlation Between
    the Generated Task Description and the Reference Code Change.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：GPT-4评估生成的任务描述与参考代码更改之间相关性的评分含义。
- en: '| Score | Meaning |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | 含义 |'
- en: '| --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | The code changes are unrelated to the task description. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 代码更改与任务描述无关。 |'
- en: '| 2 | The code changes address a minor part of the task but are largely irrelevant.
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 代码更改涉及任务的一个小部分，但大体上与任务无关。 |'
- en: '| 3 | The code changes partially meet the task requirements but lack completeness
    or accuracy. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 代码更改部分满足任务要求，但缺乏完整性或准确性。 |'
- en: '| 4 | The code changes are relevant and mostly complete, with minor discrepancies
    from the |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 代码更改相关且大体完整，与 |'
- en: '| task description. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 任务描述。 |'
- en: '| 5 | The code changes perfectly align with the task description, fully addressing
    all specified |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 代码更改与任务描述完全一致，充分满足所有指定的 |'
- en: '| requirements with high accuracy and completeness. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 需求准确性和完整性高。 |'
- en: '4.4 RQ 5: How Effective Is Our Coding Process?'
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 RQ 5：我们的编码过程有效吗？
- en: To investigate the effectiveness of the coding process in our framework, we
    analyze the performance of the developer’s line locating and the issue resolving
    across instances of varying complexities.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查我们框架中编码过程的有效性，我们分析了开发者在处理不同复杂性实例中的代码行定位和问题解决表现。
- en: 'Figure [8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding Process?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution") illustrates the distribution of the line location
    overlap ratio of MAGIS and the baseline models (GPT-4 and Claude-2). The vertical
    axis quantifies the frequency of occurrences within specific ranges of line location
    overlap ratios for each group. This visualization reveals that our developer agent
    frequently attains a line location overlap ratio nearing $1$. Such a distribution
    validates the superior performance of MAGIS in line location.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '图[8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding Process? ‣ 4
    Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution")展示了MAGIS与基线模型（GPT-4和Claude-2）的行位置重叠比率的分布。纵轴量化了每组中行位置重叠比率特定范围内的出现频率。这一可视化揭示了我们的开发者代理经常达到了接近$1$的行位置重叠比率。这种分布验证了MAGIS在行位置方面的卓越表现。'
- en: 'Further analysis is provided in Figure [8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How
    Effective Is Our Coding Process? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"), which illustrates
    the relationship between the line location overlap ratio and the issue resolved
    ratio within those overlaps. In the figure, the horizontal axis represents the
    range of overlap ratios for each bar’s corresponding interval, while the height
    of each bar indicates the resolved ratio for instances within that interval. These
    resolved ratios correspond to the scale on the left vertical axis. The orange
    curve represents the cumulative frequency of instances that can be resolved under
    different overlap ratio thresholds, with the cumulative frequency corresponding
    to the scale on the right side.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '进一步分析见图[8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding Process?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution")，该图示意了行位置重叠比率与这些重叠中的问题解决比率之间的关系。在图中，横轴表示每个条形图对应区间的重叠比率范围，而每个条形图的高度表示该区间内实例的解决比率。这些解决比率对应左侧纵轴的刻度。橙色曲线表示在不同重叠比率阈值下能够解决的实例的累积频率，累积频率对应右侧刻度。'
- en: 'As shown in Figure [8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding
    Process? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"), the right four bars are higher than the
    five left, which indicates the resolved ratio can increase with the line location
    overlap. This observation also suggests that accurate line location is helpful
    for issue resolution. The cumulative frequency curve, shown in orange, provides
    an additional analysis, indicating the cumulative proportion of issues resolved
    ratio up to each point along the line location overlap. A steady increase in cumulative
    frequency accompanies the increase in line location overlap, reinforcing the idea
    that resolving issues is more successful in areas of high overlap. The slope of
    the curve’s left half is lower than that of the right half, indicating that the
    benefits of increasing the overlap ratio are less pronounced at lower overlap
    ratios than at higher ones. Therefore, the developer agent should prioritize improving
    its capability of line location.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding Process? ‣ 4
    Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution")所示，右侧的四个条形图高于左侧的五个，这表明解决比率随着行位置重叠的增加而提高。这一观察结果也表明，准确的行位置有助于问题解决。橙色的累积频率曲线提供了额外的分析，指示了沿着行位置重叠每一点的累计解决比率。累积频率的稳定增长伴随行位置重叠的增加，强化了在高重叠区域解决问题的成功率更高的观点。曲线左半部分的斜率低于右半部分，表明增加重叠比率的好处在低重叠比率下不如在高重叠比率下明显。因此，开发者代理应优先提升其行位置的能力。'
- en: 'Moreover, as shown in Table [4](#S4.T4 "Table 4 ‣ 4.4 RQ 5: How Effective Is
    Our Coding Process? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution"), we present a logistic regression
    analysis that quantifies the correlation between several complexity indices and
    issue resolution. The results show that GPT-4 has significant negative correlations
    across the number of files and functions, suggesting that as these indices increase,
    the likelihood of issue resolution decreases. Conversely, the negative correlations
    are less pronounced with our model, MAGIS, particularly in the number of files
    and functions, suggesting mitigation of challenges corresponding to these complexity
    indices.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，如表 [4](#S4.T4 "Table 4 ‣ 4.4 RQ 5: How Effective Is Our Coding Process?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution") 所示，我们展示了一项逻辑回归分析，该分析量化了多个复杂性指标与问题解决之间的相关性。结果表明，GPT-4
    在文件数量和函数数量上的负相关性显著，表明这些指标增加时，问题解决的可能性降低。相比之下，我们的模型 MAGIS 的负相关性较不明显，特别是在文件数量和函数数量上，表明该模型在应对这些复杂性指标时具有缓解作用。'
- en: 'Table 4: Correlation Between the Complexity Indices and the Issue Resolution.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：复杂性指标与问题解决之间的相关性。
- en: '| Method | # Files | # Functions | # Hunks | # Added LoC | # Deleted LoC |
    # Changed LoC |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 文件数量 | 函数数量 | 修改块数量 | 添加的行数 | 删除的行数 | 修改的行数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4 | $-25.15$ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | $-25.15$ |'
- en: '| MAGIS |   $-1.55$^* |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| MAGIS |   $-1.55$^* |'
- en: '*'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: The correlation between the index and the issue resolution is significant (P-value
    $<$).
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指数与问题解决之间的相关性显著（P 值 $<$）。
- en: '![Refer to caption](img/a280d60e96da9a0b90062211b94835b6.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a280d60e96da9a0b90062211b94835b6.png)'
- en: 'Figure 7: Comparison of Line Location Overlap between MAGIS (Ours) and Baselines.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：MAGIS（我们的）与基线模型之间的行位置重叠比较。
- en: '![Refer to caption](img/b9c828ecea8040475b246766faf7b149.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b9c828ecea8040475b246766faf7b149.png)'
- en: 'Figure 8: Resolved Ratio in Different Line Location Overlap Intervals.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：不同的行位置重叠区间中的解决比例。
- en: 'To evaluate the performance of the QA engineer, the ablation experiment is
    conducted and the results are shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 RQ 3: How
    Effective Is Our Framework? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"). As the table
    shows, in settings with and without hints, the presence of the QA engineer can
    increase the resolved ratio by $1.57\%$, respectively. This overall enhancement
    substantiates the QA engineer’s contribution to improving outcomes.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 QA 工程师的表现，进行了消融实验，结果如表 [2](#S4.T2 "Table 2 ‣ 4.2 RQ 3: How Effective Is
    Our Framework? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution") 所示。表中显示，在有无提示的设置下，QA 工程师的存在可以分别将解决比例提高
    $1.57\%$。这种整体提升验证了 QA 工程师对改进结果的贡献。'
- en: 'Specifically, there is an issue ⁸⁸8[https://github.com/scikit-learn/scikit-learn/issues/9784](https://github.com/scikit-learn/scikit-learn/issues/9784)
    from the repository scikit-learn ⁹⁹9[https://github.com/scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn)
    and the reference code change ^(10)^(10)10[https://github.com/scikit-learn/scikit-learn/pull/9288](https://github.com/scikit-learn/scikit-learn/pull/9288)
    is shown in Figure [5](#S5.T5 "Table 5 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"). During the flow
    of our framework, the developer firstly modifies the code as shown in Figure [10](#S5.F10
    "Figure 10 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution") but the parameterrandom_state (Line $371$
    in the new-version code) of the function kmeans_single is not assigned the right
    number in seeds. After the erroneous modification was made, the QA engineer identified
    the mistake and provided feedback. Their commentary highlighted the issue: “This
    code change modifies the implementation of K-means algorithm and doesn’t seem
    entirely correct”. They further elaborated, “Running the algorithm just one time
    could lead to worse results, compared to running it multiple times (n_init times)
    and choosing the best result, as was originally done” This critique specifically
    targets the flaw associated with the iterative process (“running times”). With
    the help of the QA engineer, the developer further revise the code, and the final
    code change is shown in Figure [10](#S5.F10 "Figure 10 ‣ 5 Statistics and Discussion
    ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution").
    All of the necessary test cases are passed after applying this code change. Both
    the ablation study and the case study underscore the QA engineer’s efficacy.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，存在一个问题 ⁸⁸8[https://github.com/scikit-learn/scikit-learn/issues/9784](https://github.com/scikit-learn/scikit-learn/issues/9784)
    来自 scikit-learn 仓库 ⁹⁹9[https://github.com/scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn)，参考代码更改 ^(10)^(10)10[https://github.com/scikit-learn/scikit-learn/pull/9288](https://github.com/scikit-learn/scikit-learn/pull/9288)
    如图 [5](#S5.T5 "表 5 ‣ 5 统计数据与讨论 ‣ \stackon[0pt]MAGIS : 基于 LLM 的多智能体 GitHub 问题解决框架")
    所示。在我们的框架流程中，开发者首先修改代码，如图 [10](#S5.F10 "图 10 ‣ 5 统计数据与讨论 ‣ \stackon[0pt]MAGIS
    : 基于 LLM 的多智能体 GitHub 问题解决框架") 所示，但函数 kmeans_single 的参数 random_state（新版本代码中的第
    $371$ 行）没有分配正确的 seeds 数值。在错误修改完成后，QA 工程师发现了这个错误并提供了反馈。他们的评论突出了这个问题：“此代码更改修改了 K-means
    算法的实现，似乎并不完全正确。”他们进一步阐述道：“仅运行算法一次可能导致更差的结果，相比之下，多次运行（n_init 次）并选择最佳结果如最初所做的那样。”这段评论特别针对了与迭代过程（“运行次数”）相关的缺陷。在
    QA 工程师的帮助下，开发者进一步修订了代码，最终代码更改如图 [10](#S5.F10 "图 10 ‣ 5 统计数据与讨论 ‣ \stackon[0pt]MAGIS
    : 基于 LLM 的多智能体 GitHub 问题解决框架") 所示。应用此代码更改后，所有必要的测试用例都通过了。消融研究和案例研究都强调了 QA 工程师的效率。'
- en: 5 Statistics and Discussion
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 统计数据与讨论
- en: '![[Uncaptioned image]](img/a799023c974060885392df8078853f8f.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/a799023c974060885392df8078853f8f.png)'
- en: 'Table 5: Case from scikit-learn (Gold).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：来自 scikit-learn 的案例（黄金）。
- en: '![Refer to caption](img/94712055a24172042d33d135fbcd00a7.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/94712055a24172042d33d135fbcd00a7.png)'
- en: 'Figure 9: Case from scikit-learn (Ours, before review).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：来自 scikit-learn 的案例（我们的，审核前）。
- en: '![Refer to caption](img/21c164ed82c1779debe673bc3cd138d4.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/21c164ed82c1779debe673bc3cd138d4.png)'
- en: 'Figure 10: Case from scikit-learn (Ours, after review).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：来自 scikit-learn 的案例（我们的，审核后）。
- en: This section provides statistics on code changes corresponding to resolved issues
    and those applicable but unresolved using our framework.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了与已解决问题相关的代码更改统计数据，以及使用我们的框架适用但未解决的问题的统计数据。
- en: 5.1 Complexity of Code Changes.
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 代码更改的复杂性。
- en: 'The statistics on the code change for instances with resolved issues are presented
    in Table [6](#S5.T6 "Table 6 ‣ 5.1 Complexity of Code Changes. ‣ 5 Statistics
    and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution"). Overall, the statistical information of the generated code
    changes for these instances, such as the average number of code files, functions,
    hunks, and deleted lines, all differ slightly (not exceeding $0.3$ lines. Results
    demonstrate the effectiveness of our method in resolving complex issues that need
    to modify the code file on multiple locations and with long context.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The Statistical Analysis of Our Framework on Resolved Instances.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MAGIS | Gold |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '|  | Min | Max | Avg. | Min | Max | Avg. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| # Code Files | $1$ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| # Functions | $1$ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| # Hunks | $1$ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| # Added Lines | $1$ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| # Deleted Lines | $0$ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| Change Start Index | $1$ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Change End Index | $22$ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| # Changed Lines | $2$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: 'Specifically, the distribution of the number of modified lines for the resolving
    instances is shown in Figure [12](#S5.F12 "Figure 12 ‣ 5.1 Complexity of Code
    Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"). We observe that the distribution of the
    number of modified lines in our framework for the solved instances exceeds that
    of the reference solution, especially in terms of the number of added lines being
    significantly higher than the reference. Upon manual inspection, we found that
    the generation results provided by our framework often contained more comment
    information, which led to an increase in the total number of modified lines. For
    example, Figure [10](#S5.F10 "Figure 10 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution") displays the generation
    result of our framework. Lines $365,368,371,374,383$ in the new version file correspond
    to the comment for the added code. These natural language descriptions are valuable
    in actual software evolution [[14](#bib.bib14), [22](#bib.bib22)]. In contrast,
    Figure [5](#S5.T5 "Table 5 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution") shows a human-written
    solution lacking such explanatory comments, which might disadvantage software
    maintainers in reading and understanding.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc47148317894685be429c2ddc762619.png)![Refer to caption](img/74a1d36e5ef4d7641f756de633cedc50.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Distribution of the LoC in the Resolved Instances.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20dc242d12385ae3ecea80eb7c9a4cb5.png)![Refer to caption](img/a9d83218a1dc90f9b6e392142bc68c0d.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Distribution of the LoC in the Applied but Not Resolved Instances.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The statistics on the code change for instances without resolved issues are
    shown in Table [7](#S5.T7 "Table 7 ‣ 5.1 Complexity of Code Changes. ‣ 5 Statistics
    and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution"). From the table, our framework can generate applicable code
    changes including up to $13$ lines. These results suggest that our method has
    a strong adaptability in generating applicable code changes. However, considering
    that these code changes have not passed all the potential test cases they could
    pass, which indicates that there is still room for improvement.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The Statistical Analysis of Our Framework on Applied but Not Resolved
    Instances.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MAGIS | Gold |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '|  | Min | Max | Avg. | Min | Max | Avg. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| # Code Files | $1$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| # Functions | $1$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| # Hunks | $1$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| # Added Lines | $1$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| # Deleted Lines | $0$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| Change Start Index | $1$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| Change End Index | $9$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| # Changed Lines | $1$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: 'To further analyze the reasons behind the failure of test cases in these instances,
    we have quantified the distribution of the lengths of code changes in the unresolved
    instances, as shown in Figure [12](#S5.F12 "Figure 12 ‣ 5.1 Complexity of Code
    Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"). From the figure, we observe that for
    unresolved instances, the framework tends to delete a larger number of lines while
    adding fewer lines, in contrast to the distribution of human-written changes.
    This discrepancy may point to different repair strategies or attitudes towards
    problem-solving, where the framework presented herein might prefer to reduce errors
    by removing potentially problematic code, whereas human developers may lean towards
    adding new code to address issues.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, a comparison between Table [6](#S5.T6 "Table 6 ‣ 5.1 Complexity of
    Code Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution") and Table [7](#S5.T7 "Table 7 ‣ 5.1 Complexity
    of Code Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution") reveals that the latter contains
    a higher overall number of files, hunks, and changed lines of code. These instances,
    involving more modification locations, correspond to more complex scenarios. This
    phenomenon suggests that the performance of our framework in resolving such complex
    issues requires further enhancement.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the variability in difficulty across different software repositories
    may influence the effectiveness of code changes. To this end, we compile statistics
    on the resolved ratios in various software repositories, as shown in Figure [13](#S5.F13
    "Figure 13 ‣ 5.1 Complexity of Code Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"). From the figure,
    we observe that there is a significant variation in the resolved ratios across
    different repositories in our framework. Some repositories have a resolved ratio
    as high as $40\%$. This suggests that the differences among various software such
    as code structure and coding style can impact the generation and application of
    the code change.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57c180cbc0367859a12b7c07e0e81f37.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The Number of Applied and Resolved Instances in Different Repositories.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Comparison with Devin.
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Devin is a novel agent for software development [[34](#bib.bib34)], and its
    performance has also been assessed using the SWE-bench. However, the evaluation
    dataset employed by Devin differs from the subset used for experiments with GPT-4
    reported by the paper of SWE-bench [[15](#bib.bib15)]. An analysis of the repository
    name and pull request ID of each instance reveals that only $140$ instances overlap
    between the two datasets.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Within the shared pool of $140$) issues ^(11)^(11)11[https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs/pass](https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs/pass).
    This comparison, however, may not be entirely equitable. Devin’s possible underlying
    LLM is unknown, and it possesses the capability to integrate feedback from the
    environment. Moreover, Devin’s reported scores are under the setting given the
    entire repository, and it operates with “common developer tools including the
    shell, code editor, and browser”, and “agents with internet access could potentially
    find external information through other methods” as detailed at the report ^(12)^(12)12[https://www.cognition-labs.com/introducing-devin](https://www.cognition-labs.com/introducing-devin).
    In contrast, our approach solely relies on the shell, without the need of any
    additional external tools.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: For running time, 72% of instances resolved by Devin require greater than $10$
    minutes to complete. In contrast, our framework finalizes each resolved issue
    within approximately 3 minutes. On average, our framework completes the processing
    of each instance in under 5 minutes, demonstrating its capability to assist in
    resolving GitHub issues with minimal time expenditure.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitation
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The design of prompt words may impact the performance of LLMs, thereby affecting
    the validity and fairness of the results [[8](#bib.bib8)]. While this paper focuses
    on innovative aspects of the proposed framework design and relies on practical
    guidelines for the design of prompt word templates [[29](#bib.bib29)] to reduce
    the emergence of design biases, the complete elimination of the prompt bias is
    extremely difficult due to the inherent biases in the dataset instances and the
    limitations of API resources.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Dataset.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The dataset contains a limited variety of software types. The evaluating dataset,
    SWE-bench, encompasses $12$ repositories, which cover the Python programming language.
    However, this quantity remains insufficient compared to the diverse software projects
    available on GitHub. The code style, architectural design, and implementation
    techniques of these selected repositories, while representative, cannot fully
    reflect the diversity of all code repositories. In particular, the current dataset
    may fail to encompass some specialized fields or different programming paradigms,
    such as microservice architecture [[41](#bib.bib41)] and functional programming [[16](#bib.bib16)].
    This limitation implies that, although our framework is designed to be independent
    of any specific software, the validation of its effectiveness and general applicability
    might be affected by this limited sample scope. Therefore, applying the findings
    of this paper to other code repositories may require further validation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Large Language Models
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) refer to the pre-trained language models that contain
    a large number of parameters [[38](#bib.bib38)]. The parameter counts of these
    models typically range in the tens or hundreds of billions. Popular LLMs include
    the Generative Pre-trained Transformer (GPT) series, such as GPT-3 [[27](#bib.bib27)],
    GPT-4 [[25](#bib.bib25)], and the open-source LLaMA [[35](#bib.bib35)] which publicly
    shares its weight information. The first version of the open-source model LLaMA
    has parameters ranging from 7 billion to 65 billion. Many researchers [[33](#bib.bib33),
    [11](#bib.bib11)] have built upon the foundation of LLaMA, implementing enhancements
    to forge new LLMs. These LLMs have demonstrated formidable natural language generation
    capabilities in general scenarios, with GPT-4, in particular, standing out [[18](#bib.bib18),
    [39](#bib.bib39)]. It has consistently maintained the top position in several
    rankings, including code generation, reflecting its significant potential in tasks
    related to software engineering [[13](#bib.bib13)].
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 LLM-Based Multi-Agent System
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the powerful text generation capabilities of LLMs, many researchers [[12](#bib.bib12),
    [31](#bib.bib31), [7](#bib.bib7), [37](#bib.bib37), [26](#bib.bib26), [36](#bib.bib36)]
    have explored the construction of LLM-based Multi-Agent Systems, enabling them
    to accomplish tasks beyond the capabilities of the LLMs themselves. For example,
    MetaGPT [[12](#bib.bib12)], which simulates the Standardized Operating Procedures
    (SOPs) of a programming team, completing tasks including definition, design, planning,
    coding, and testing through constructed roles (e.g., product managers, architects,
    project managers, etc.). This framework has achieved leading scores on the HumanEval [[9](#bib.bib9)]
    and MBPP [[2](#bib.bib2)], outperforming many LLMs, and researchers show its ability
    to complete a software establishment (e.g., a code repository to play Gomoku game),
    indicating that a multi-agent framework can better leverage the capabilities of
    LLMs in code generation tasks. Moreover, Qian et al. [[26](#bib.bib26)] designed
    ChatDev, a virtual development company simulating a human development team, which
    decomposes requirements into atomic tasks assigned to the developers. Developers
    mitigate the hallucination that may arise with the LLM through mutual communication
    and self-reflection mechanisms. Experimental results show that ChatDev can complete
    the establishment of some small software (averaging no more than $5$ minutes on
    average). However, these works focus on the transformation from the requirement
    to software and overlook the code change generation during software evolution
    which needs not only understanding the requirement but also dealing with the large
    repository.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, this paper illuminates the potential of large language models
    in software evolution, particularly in resolving GitHub issues. Our study identifies
    the challenges of direct LLM application. To address them, we proposed a novel
    LLM-based multi-agent framework, MAGIS, enhancing issue resolution through well-designed
    agents’ collaboration. The superiority of MAGIS on the SWE-bench dataset against
    popular LLMs highlights its effectiveness, pointing towards a promising direction
    for integrating LLMs into software evolution workflows. This work not only shows
    the potential of LLMs in GitHub issue resolution but also explores an LLM-based
    paradigm for software evolution.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic [2023] Anthropic. Claude 2. [https://www.anthropic.com/news/claude-2](https://www.anthropic.com/news/claude-2),
    2023.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V.
    Le, and Charles Sutton. Program synthesis with large language models. *arXiv Preprint*,
    abs/2108.07732, 2021. URL [https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baum et al. [2016] Tobias Baum, Olga Liskin, Kai Niklas, and Kurt Schneider.
    Factors influencing code review processes in industry. In Thomas Zimmermann, Jane
    Cleland-Huang, and Zhendong Su, editors, *Proceedings of the 24th ACM SIGSOFT
    International Symposium on Foundations of Software Engineering, FSE 2016, Seattle,
    WA, USA, November 13-18, 2016*, pages 85–96\. ACM, 2016. doi: 10.1145/2950290.2950323.
    URL [https://doi.org/10.1145/2950290.2950323](https://doi.org/10.1145/2950290.2950323).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bissyandé et al. [2013] Tegawendé F. Bissyandé, David Lo, Lingxiao Jiang, Laurent
    Réveillère, Jacques Klein, and Yves Le Traon. Got issues? who cares about it?
    A large scale investigation of issue trackers from github. In *IEEE 24th International
    Symposium on Software Reliability Engineering, ISSRE 2013, Pasadena, CA, USA,
    November 4-7, 2013*, pages 188–197\. IEEE Computer Society, 2013. doi: 10.1109/ISSRE.2013.6698918.
    URL [https://doi.org/10.1109/ISSRE.2013.6698918](https://doi.org/10.1109/ISSRE.2013.6698918).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bosu and Carver [2014] Amiangshu Bosu and Jeffrey C. Carver. Impact of developer
    reputation on code review outcomes in OSS projects: an empirical investigation.
    In Maurizio Morisio, Tore Dybå, and Marco Torchiano, editors, *2014 ACM-IEEE International
    Symposium on Empirical Software Engineering and Measurement, ESEM ’14, Torino,
    Italy, September 18-19, 2014*, pages 33:1–33:10\. ACM, 2014. doi: 10.1145/2652524.2652544.
    URL [https://doi.org/10.1145/2652524.2652544](https://doi.org/10.1145/2652524.2652544).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. [2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M.
    Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks
    of artificial general intelligence: Early experiments with GPT-4. *arXiv Preprint*,
    abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712. URL [https://doi.org/10.48550/arXiv.2303.12712](https://doi.org/10.48550/arXiv.2303.12712).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv Preprint*, abs/2308.07201, 2023. doi: 10.48550/ARXIV.2308.07201.
    URL [https://doi.org/10.48550/arXiv.2308.07201](https://doi.org/10.48550/arXiv.2308.07201).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Lichang Chen, Jiuhai Chen, Heng Huang, and Minhao Cheng.
    PTP: boosting stability and performance of prompt tuning with perturbation-based
    regularizer. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2023, Singapore, December 6-10, 2023*, pages 13512–13525\. Association for Computational
    Linguistics, 2023. URL [https://aclanthology.org/2023.emnlp-main.833](https://aclanthology.org/2023.emnlp-main.833).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *arXiv Preprint*,
    abs/2107.03374, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2023] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei
    Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval:
    A manually-crafted benchmark for evaluating llms on class-level code generation,
    2023.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng and Liu [2023] Xinyang Geng and Hao Liu. Openllama: An open reproduction
    of llama, May 2023. URL [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. [2023] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    Metagpt: Meta programming for a multi-agent collaborative framework, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2023] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang,
    Li Li, Xiapu Luo, David Lo, John C. Grundy, and Haoyu Wang. Large language models
    for software engineering: A systematic literature review. *arXiv Preprint*, abs/2308.10620,
    2023. doi: 10.48550/ARXIV.2308.10620. URL [https://doi.org/10.48550/arXiv.2308.10620](https://doi.org/10.48550/arXiv.2308.10620).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2022] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and
    Thomas Zimmermann. Practitioners’ expectations on automated code comment generation.
    In *44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
    2022, Pittsburgh, PA, USA, May 25-27, 2022*, pages 1693–1705\. ACM, 2022. doi:
    10.1145/3510003.3510152. URL [https://doi.org/10.1145/3510003.3510152](https://doi.org/10.1145/3510003.3510152).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jimenez et al. [2024] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models
    resolve real-world github issues? In *The Twelfth International Conference on
    Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net,
    2024. URL [https://openreview.net/forum?id=VTF8yNQM66](https://openreview.net/forum?id=VTF8yNQM66).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnsson [1987] Thomas Johnsson. Attribute grammars as a functional programming
    paradigm. In Gilles Kahn, editor, *Functional Programming Languages and Computer
    Architecture, Portland, Oregon, USA, September 14-16, 1987, Proceedings*, volume
    274 of *Lecture Notes in Computer Science*, pages 154–173\. Springer, 1987. doi:
    10.1007/3-540-18317-5\_10. URL [https://doi.org/10.1007/3-540-18317-5_10](https://doi.org/10.1007/3-540-18317-5_10).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kononenko et al. [2015] Oleksii Kononenko, Olga Baysal, Latifa Guerrouj, Yaxin
    Cao, and Michael W. Godfrey. Investigating code review quality: Do people and
    participation matter? In Rainer Koschke, Jens Krinke, and Martin P. Robillard,
    editors, *2015 IEEE International Conference on Software Maintenance and Evolution,
    ICSME 2015, Bremen, Germany, September 29 - October 1, 2015*, pages 111–120\.
    IEEE Computer Society, 2015. doi: 10.1109/ICSM.2015.7332457. URL [https://doi.org/10.1109/ICSM.2015.7332457](https://doi.org/10.1109/ICSM.2015.7332457).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023a] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of
    large language models for code generation. In Alice Oh, Tristan Naumann, Amir
    Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, *Advances in
    Neural Information Processing Systems 36: Annual Conference on Neural Information
    Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
    2023*, 2023a. URL [http://papers.nips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023b] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv Preprint*, abs/2307.03172, 2023b. doi: 10.48550/ARXIV.2307.03172.
    URL [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023c] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv Preprint*, abs/2307.03172, 2023c. doi: 10.48550/ARXIV.2307.03172.
    URL [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McIntosh et al. [2014] Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E.
    Hassan. The impact of code review coverage and code review participation on software
    quality: a case study of the qt, vtk, and ITK projects. In Premkumar T. Devanbu,
    Sung Kim, and Martin Pinzger, editors, *11th Working Conference on Mining Software
    Repositories, MSR 2014, Proceedings, May 31 - June 1, 2014, Hyderabad, India*,
    pages 192–201\. ACM, 2014. doi: 10.1145/2597073.2597076. URL [https://doi.org/10.1145/2597073.2597076](https://doi.org/10.1145/2597073.2597076).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mu et al. [2023] Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang.
    Developer-intent driven code comment generation. In *45th IEEE/ACM International
    Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20,
    2023*, pages 768–780\. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00073. URL [https://doi.org/10.1109/ICSE48619.2023.00073](https://doi.org/10.1109/ICSE48619.2023.00073).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. GPT-4 technical report, 2023. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023a] OpenAI. Gpt-3.5 turbo fine-tuning and api updates. [https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates),
    2023a.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023b] OpenAI. Gpt-4. [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4),
    2023b.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2023] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng
    Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    Communicative agents for software development. *arXiv Preprint*, 2023.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training, 2018.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robertson et al. [1994] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline
    Hancock-Beaulieu, and Mike Gatford. Okapi at TREC-3. In Donna K. Harman, editor,
    *Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg,
    Maryland, USA, November 2-4, 1994*, volume 500-225 of *NIST Special Publication*,
    pages 109–126\. National Institute of Standards and Technology (NIST), 1994. URL
    [http://trec.nist.gov/pubs/trec3/papers/city.ps.gz](http://trec.nist.gov/pubs/trec3/papers/city.ps.gz).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shieh [2023] Jessica Shieh. Best practices for prompt engineering with openai
    api. *OpenAI, February https://help. openai. com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api*,
    2023.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2024] Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang
    Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng
    Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li,
    and Zhiyong Wu. A survey of neural code intelligence: Paradigms, advances and
    beyond, 2024.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent LLM agents. *arXiv Preprint*,
    abs/2306.03314, 2023. doi: 10.48550/ARXIV.2306.03314. URL [https://doi.org/10.48550/arXiv.2306.03314](https://doi.org/10.48550/arXiv.2306.03314).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. [2024] Wei Tao, Yucheng Zhou, Yanlin Wang, Hongyu Zhang, Haofen
    Wang, and Wenqiang Zhang. Kadel: Knowledge-aware denoising learning for commit
    message generation. *ACM Trans. Softw. Eng. Methodol.*, jan 2024. ISSN 1049-331X.
    doi: 10.1145/3643675. URL [https://doi.org/10.1145/3643675](https://doi.org/10.1145/3643675).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team [2023] LLaMA-MoE Team. Llama-moe: Building mixture-of-experts from llama
    with continual pre-training, Dec 2023. URL [https://github.com/pjlab-sys4nlp/llama-moe](https://github.com/pjlab-sys4nlp/llama-moe).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Team [2024] The Cognition Team. Swe-bench technical report, 2024. URL [https://www.cognition-labs.com/post/swe-bench-technical-report](https://www.cognition-labs.com/post/swe-bench-technical-report).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models. *arXiv
    preprint arXiv:2302.13971*, 2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tufano et al. [2024] Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian
    Moghaddam, and Neel Sundaresan. Autodev: Automated ai-driven development, 2024.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen LLM applications via multi-agent conversation framework. *arXiv Preprint*,
    abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL [https://doi.org/10.48550/arXiv.2308.08155](https://doi.org/10.48550/arXiv.2308.08155).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of
    large language models. *arXiv Preprint*, abs/2303.18223, 2023. doi: 10.48550/ARXIV.2303.18223.
    URL [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023a] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
    Moritz Hardt, and Sergey Levine, editors, *Advances in Neural Information Processing
    Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS
    2023, New Orleans, LA, USA, December 10 - 16, 2023*, 2023a. URL [http://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023b] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing
    Chen, Lianghong Guo, and Weicheng Wang. Towards an understanding of large language
    models in software engineering tasks. *arXiv Preprint*, abs/2308.11396, 2023b.
    doi: 10.48550/ARXIV.2308.11396. URL [https://doi.org/10.48550/arXiv.2308.11396](https://doi.org/10.48550/arXiv.2308.11396).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2021] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai
    Li, and Dan Ding. Fault analysis and debugging of microservice systems: Industrial
    survey, benchmark system, and empirical study. *IEEE Trans. Software Eng.*, 47(2):243–260,
    2021. doi: 10.1109/TSE.2018.2887384. URL [https://doi.org/10.1109/TSE.2018.2887384](https://doi.org/10.1109/TSE.2018.2887384).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023] Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong
    Long, Jian-Guang Lou, and Jianbing Shen. Thread of thought unraveling chaotic
    contexts. *arXiv Preprint*, abs/2311.08734, 2023. doi: 10.48550/ARXIV.2311.08734.
    URL [https://doi.org/10.48550/arXiv.2311.08734](https://doi.org/10.48550/arXiv.2311.08734).'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
