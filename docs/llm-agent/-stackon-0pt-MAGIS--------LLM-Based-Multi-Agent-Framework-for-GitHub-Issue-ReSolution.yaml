- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '\stackon[0pt]MAGIS      : LLM-Based Multi-Agent Framework for GitHub Issue
    ReSolution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17927](https://ar5iv.labs.arxiv.org/html/2403.17927)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wei Tao
  prefs: []
  type: TYPE_NORMAL
- en: Fudan University
  prefs: []
  type: TYPE_NORMAL
- en: wtao18@fudan.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Yucheng Zhou'
  prefs: []
  type: TYPE_NORMAL
- en: University of Macau
  prefs: []
  type: TYPE_NORMAL
- en: yucheng.zhou@connect.um.edu.mo
  prefs: []
  type: TYPE_NORMAL
- en: '&Wenqiang Zhang'
  prefs: []
  type: TYPE_NORMAL
- en: Fudan University
  prefs: []
  type: TYPE_NORMAL
- en: wqzhang@fudan.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Yu Cheng'
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: yc180@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In software evolution, resolving the emergent issues within GitHub repositories
    is a complex challenge that involves not only the incorporation of new code but
    also the maintenance of existing functionalities. Large Language Models (LLMs)
    have shown promise in code generation and understanding but face difficulties
    in code change, particularly at the repository level. To overcome these challenges,
    we empirically study the reason why LLMs mostly fail to resolve GitHub issues
    and analyze some impact factors. Motivated by the empirical findings, we propose
    a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting
    of four kinds of agents customized for the software evolution: Manager, Repository
    Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages
    the collaboration of various agents in the planning and coding process to unlock
    the potential of LLMs to resolve GitHub issues. In experiments, we employ the
    SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4,
    and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms
    the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved
    ratio over the direct application of GPT-4, the based LLM of our method. We also
    analyze the factors for improving GitHub issue resolution rates, such as line
    location, task allocation, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real-world software development, the software application is rarely set in
    stone. High-quality and popular software continually evolves to address emergent
    bugs or adapt to new requirements arising from shifts in user needs, software
    environments, and physical hardware. On platforms such as GitHub ¹¹1[https://github.com](https://github.com),
    issues typically signify the requirement for software evolution. However, addressing
    these issues poses significant challenges, as it requires implementing the code
    change across the entire repository and maintaining the existing functionality
    while integrating new capabilities. Consequently, resolving GitHub issues remains
    a significant challenge across academia and industry [[15](#bib.bib15), [4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have demonstrated remarkable capabilities across
    a variety of tasks [[6](#bib.bib6)], including code generation and code understanding [[40](#bib.bib40),
    [30](#bib.bib30)]. Specifically, LLMs excel in generating function-level code,
    as evidenced by their performance on numerous benchmark datasets such as MBPP [[2](#bib.bib2)]
    and HumanEval [[9](#bib.bib9)]. Despite their success, LLMs remain challenged
    in tasks that require advanced code generation capabilities, such as the ClassEval
    benchmark [[10](#bib.bib10)]. Moreover, LLMs exhibit limitations in processing
    excessively long context inputs and are subject to constraints regarding their
    input context length [[20](#bib.bib20)]. This limitation is particularly evident
    in repository-level tasks, such as solving GitHub issues, where the context comprises
    the entire repository, thus imposing constraints on directly using the full repository
    as input to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: To harness the full potential of LLMs, researchers [[12](#bib.bib12), [26](#bib.bib26),
    [36](#bib.bib36)] have designed LLM-based multi-agent systems. These approaches
    have significantly improved LLMs’ efficacy in code generation, enabling these
    systems to construct code repositories based on LLM. While these methods address
    the process of transitioning code repositories from inception to establishment,
    they rarely consider the handling of software evolution, e.g., resolving Github
    issues. For Github repositories, especially popular ones, a large number of commits
    are pushed every day. These commits derive from a spectrum of evolutionary requirements
    that span bug fixes, feature additions, performance enhancements, etc [[32](#bib.bib32)].
    For open-source software, new requirements frequently emerge as issues in the
    project’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: To investigate the capability of LLMs in addressing GitHub issues, Jimenez et al.
    [[15](#bib.bib15)] developed a benchmark, namely SWE-bench, to conduct a comprehensive
    analysis of popular LLMs. The study reveals that LLMs fail to resolve over $5\%$
    of instances, even when provided with file paths that require modifications. This
    significantly low rate of applying solutions and resolving issues underscores
    the limitations inherent in LLMs. The reasons behind the suboptimal performance
    of LLMs and strategies to harness their potential for GitHub issue resolution
    remain under-explored.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this study, we analyze the factors impacting the effectiveness of LLMs in
    resolving issues under two different settings (i.e., w/ Oracle and w/o Oracle).
    Furthermore, our empirical analysis has concluded a correlation between file location
    and line location within files, and performance in resolving GitHub issues. Based
    on these insights, we propose a novel LLM-based Multi-agent Framework, termed
    MAGIS, comprising four types of agents: Manager, Repository Custodian, Developer,
    and Quality Assurance (QA) Engineer. Our approach facilitates the resolution of
    GitHub issues through collaboration among agents, each fulfilling a unique role.'
  prefs: []
  type: TYPE_NORMAL
- en: In our experiment, we evaluate our framework on the SWE-bench, comparing its
    performance against existing popular LLMs, such as ChatGPT-3.5 [[24](#bib.bib24)],
    GPT-4 [[25](#bib.bib25)], and Claude-2 [[1](#bib.bib1)]. The results demonstrate
    that our framework, utilizing GPT-4 as its base model, significantly outperforms
    the baseline, and achieves an eight-fold performance gain compared to the direct
    application of GPT-4\. Further analysis revealed additional factors, i.e., the
    planning of code change, line location within the code file, and code review process,
    that significantly influence the resolution rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct an empirical analysis of LLMs in resolving GitHub issues in two settings
    (i.e., w/ Oracle setting and w/o Oracle setting), and explore the correlation
    between line location, complexity of the code change, file location, and the success
    rate in resolving GitHub issues.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel LLM-based multi-agent framework, MAGIS, to alleviate the
    limitations of existing LLMs on GitHub issue resolution. Both our designed four-type
    agents and their collaboration for planning and coding unlock LLMs’ potential
    on the repository-level coding task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compare our framework and other strong LLM competitors (i.e., GPT-3.5, GPT-4,
    and Claude-2) on the SWE-bench dataset. The results show MAGIS significantly outperforms
    these competitors. Further analysis is conducted and verifies the effectiveness
    and necessity of our framework design.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Empirical Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '2.1 RQ 1: What Impacts the Performance Under With-Oracle Setting?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The with-Oracle setting is a more straightforward setup for the GitHub issue
    resolution by supplying the specific files requiring modifications. Despite the
    observation that LLMs exhibit better performance in the with-Oracle setting compared
    to their performance without Oracle, the proportion of issues successfully resolved
    under the with-Oracle setting remains modest. Specifically, in the with-Oracle
    setting, GPT-4 achieved a resolved rate of only $1.74\%$ on the SWE-bench.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delve into the factors impacting the efficacy within the with-Oracle setting,
    we examined failed instances’ generation. This scrutiny led to the identification
    of two factors that could influence the resolved rate: (1) Line location (location
    of the modified line in the code file), and (2) The complexity of the code change.
    This information is important and provided in the code change of each commit in
    the code repository. A typical code change includes multiple changed hunks and
    each hunk contains the line numbers targeted for modification and the specifics
    of the changed code at these locations. Both the line locations and the changed
    content make a repository evolve.'
  prefs: []
  type: TYPE_NORMAL
- en: To validate this identification, we analyze the relation between the resolved
    rate and the two factors mentioned above.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Line Location
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To quantitatively analyze the accuracy of line localization, we use the line
    numbers’ range of the modified content in the reference code change as the basis
    assuming that the correct modification location of the code change is uniquely
    determined in most cases. By calculating the overlap ratio of the line number
    ranges of the generated and reference, we can estimate the accuracy of line localization
    in the generation process. Specifically, for each instance, the line number ranges
    of the code change in the reference $r$ in the generated one. The formula for
    calculating the overlap ratio is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\left|[s_{i},e_{i}]\cap[s_{j}^{\prime},e_{j}^{\prime}]\right|$.
  prefs: []
  type: TYPE_NORMAL
- en: For $574$ GPT-3.5) and this ranking is also consistent with the proportion of
    instances solved by the three models. This phenomenon suggests that the performance
    of LLMs in generating the code change is probably related to their ability to
    locate code lines accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we assess the relationship between the overlap ratio and the resolution
    of GitHub issues by calculating their correlation coefficient. Given that the
    distribution of these variables exhibits
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2936eaaf2f63530410fb410b0ebb67ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The Comparison of Line Location Overlap Ratio between Three LLMs
    On SWE-bench.'
  prefs: []
  type: TYPE_NORMAL
- en: skewness, and the resolution result is binary (resolved or not), logistic regression
    is employed for the analysis across three LLMs. However, due to the limited number
    of successfully generated instances on GPT-4 and GPT-3.5, a statistically significant
    relationship (P-value < $0.05$, on Claude-2, there is a substantial and positive
    relation between improvements in the overlap ratio and the probability of successfully
    resolving issues. Consequently, the line location emerges as an important factor
    for GitHub issue resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Complexity of the Code Change
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The complexity of the code change is reflected in various indices, including
    the number of modified files, functions, hunks, and lines added or deleted. Firstly,
    we quantitatively assess the complexity by calculating the value of various indices
    corresponding to the reference code change. Secondly, the coefficient is calculated
    between the numbers in each index and the resolution of issues. Table [1](#S2.T1
    "Table 1 ‣ 2.1.2 Complexity of the Code Change ‣ 2.1 RQ 1: What Impacts the Performance
    Under With-Oracle Setting? ‣ 2 Empirical Study ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution") shows the correlation scores
    between six indices and the issue resolution, under the logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Correlation between the Complexity Indices and the Issue Resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM | # Files | # Functions | # Hunks | # Added LoC | # Deleted LoC | # Changed
    LoC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | $-17.57$^* |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | $-25.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 |   $-1.47$^* |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correlation between the index and the issue resolution is significant (P-value
    $<$).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As shown in Table [1](#S2.T1 "Table 1 ‣ 2.1.2 Complexity of the Code Change
    ‣ 2.1 RQ 1: What Impacts the Performance Under With-Oracle Setting? ‣ 2 Empirical
    Study ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue
    ReSolution"), all three LLMs, i.e., GPT-3.5, GPT-4, and Claude-2, demonstrate
    a statistically significant correlation with the issue resolution across several
    indices, i.e., p-value less than $0.05$. The correlation scores for the number
    of files and functions modified are notably negative for all models, indicating
    that an increase in these indices is associated with a decreasing likelihood of
    issue resolution. This suggests that the more complex the code change, as indicated
    by a higher number of files and functions modified, may hinder the issue resolution.
    Compared with GPT-3.5 and GPT-4, Claude-2 exhibits a different pattern, with much
    lower negative correlations for the number of files and functions, which indicates
    it is a more efficient approach to generate the code change for GitHub issue resolution.
    However, it also shows significant negative correlations across other indices
    such as the number of hunks, added lines of code (LoC), deleted LoC, and changed
    LoC. The analysis reveals a relationship between the complexity, as measured by
    several indices, and whether to successfully resolve the issues in software evolution.
    The negative correlations suggest that increased complexity, particularly in terms
    of the number of files and functions changed, tends to hinder issue resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 RQ 2: What Impacts the Performance Under Without-Oracle Setting?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The difference between the with-Oracle setting and the without-Oracle setting
    lies in the necessity for the latter to identify the modified files. Consequently,
    file locating emerges as a crucial factor influencing performance under the without-Oracle
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: Jimenez et al. [[15](#bib.bib15)] employ the BM25 method [[28](#bib.bib28)]
    to retrieve relevant code files that are subsequently utilized as input to the
    LLM. After employing retrieval methods, it is necessary to select the top-$K$
    files or truncate the content based on the maximum context length of the LLM.
    Incorporating more files can enhance recall scores. However, it also imposes significant
    demands on the capabilities of LLMs. As demonstrated by the study [[15](#bib.bib15)],
    Claude-2 exhibits a reduction at the resolved ratio as recall scores increase.
    This decline may be attributable to the inclusion of irrelevant files or the limited
    capacity of LLMs to process longer contexts effectively. Consequently, exploiting
    the performance of LLMs can be better achieved by striving for higher recall scores
    with a minimized set of files, thus suggesting a strategic balance between recall
    optimization and the number of chosen files.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we elaborate on our LLM-based multi-agent framework, MAGIS,
    for GitHub issue resolution. The framework resolves each issue by applying the
    generated code change to the repository. As shown in Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"), we first provide the overview of our
    framework (§[3.1](#S3.SS1 "3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS :
    LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")), followed by the
    role design of each type agent and how these roles differ from human workflows
    (§[3.2](#S3.SS2 "3.2 Agent Role Design ‣ 3 Methodology ‣ \stackon[0pt]MAGIS :
    LLM-Based Multi-Agent Framework for GitHub Issue ReSolution")). Lastly, we detail
    the collaborative process among these roles within our framework (§[3.3](#S3.SS3
    "3.3 Collaborative Process ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The architecture of our framework is illustrated in Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Overview ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"), encompassing four key roles of agents
    working collaboratively in the workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manager. This role tasks with team assembly, meeting organization, and plan
    formulation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repository Custodian. The custodian is responsible for locating the relevant
    files in the repository according to the GitHub issue and recording the change
    of the repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developer. This role participates in planning discussions and completes tasks
    from the manager.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality Assurance (QA) Engineer. The QA engineer reviews the code change from
    developers to ensure the quality of the whole repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2c4000fc44f7c98908c62309c80dbc06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of Our Framework, MAGIS.'
  prefs: []
  type: TYPE_NORMAL
- en: The collaborative process involves planning and coding. In the planning, a GitHub
    issue is assigned to the project manager and the repository custodian. The repository
    custodian identifies candidate files relevant to the issue for modification. With
    the issue description and a list of candidate files, the manager defines tasks
    and assembles a team, where each member is a developer specifically designed for
    the defined task. The manager holds a kick-off team meeting with developers and
    devises a plan. During the coding, developers undertake their assigned tasks from
    the manager, and the QA engineer reviews each code change. If a change fails to
    meet quality standards, the QA engineer provides feedback, prompting further revisions
    until the change satisfies quality criteria or a set iteration limit is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Agent Role Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our workflow draws inspiration from the GitHub Flow ²²2[https://docs.github.com/en/get-started/using-github/github-flow](https://docs.github.com/en/get-started/using-github/github-flow),
    an effective human workflow paradigm, adopted by many software teams. Both the
    human workflow and our LLM-based agent framework prioritize collaboration among
    individuals with diverse skills. While the underlying principles are similar,
    there are notable differences. Accordingly, we have tailored the roles as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/ea7054107a57f5ef0a28e58dfdd88100.png) Manager. The
    manager’s role is pivotal in planning. In conventional setups, managers decompose
    the issue into tasks according to the pre-formed team and allocate these tasks
    for members with different skills. In contrast, our manager agent can first decompose
    the issue into tasks and then design developer agents to form a team. This setup
    improves team flexibility and adaptability, enabling the formation of teams that
    can meet various issues efficiently.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/189d9c131ba0fd728acb03276fa6ee89.png) Repository
    Custodian. Considering extensive files in a repository, the custodian agent’s
    task is to locate files relevant to the issue. Different from humans, who can
    browse through the entire repository, the LLM-based agent faces challenges in
    browsing. Although LLMs, including GPT-4 [[23](#bib.bib23)], have extended context
    limits, their application is constrained in two aspects. First, it is a high computational
    cost to query the whole repository for each update while some repositories update
    frequently. Second, the performance of LLMs degrades when relevant information
    occurs in the middle of the long context input [[19](#bib.bib19), [42](#bib.bib42)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/f81db96f243032cd7dbf128f08d1ac74.png) Developer.
    In contrast to human developers, the developer agent can work continuously, and
    complete a task in minutes even seconds. Therefore, scheduling the agent to work
    in parallel is easier than scheduling humans as the latter requires considering
    other than work while the former only needs to ensure the tasks are independent.
    Additionally, although there are numerous developer agents capable of generating
    code [[12](#bib.bib12), [26](#bib.bib26)], their ability to modify existing code
    is not equally proficient. To address this issue, our framework decomposes the
    code modification process into atomic operations, which encompass the code generation.
    This approach enables developers to leverage the benefits of automatic code generation
    thereby generating applicable code changes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/03a506407ae49cdf50e2f030e9918582.png) QA Engineer.
    In software evolution, QA engineers, especially code reviewers, play a crucial
    role in maintaining software quality [[21](#bib.bib21), [17](#bib.bib17)]. Despite
    its critical role, code review practices are often undervalued or even overlooked [[3](#bib.bib3)].
    Such neglect can hinder software development, illustrated by instances where developers
    may experience delays of up to 96 hours awaiting code review feedback [[5](#bib.bib5)].
    To address this problem, our framework pairs each developer agent with a QA engineer
    agent, designed to offer task-specific, timely feedback. This personalized QA
    approach aims to boost the review process thereby better ensuring the software
    quality.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3 Collaborative Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.3.1 Planning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Three types of role agents engage in the planning process: the Repository Custodian,
    the Manager, and the Developer. The process comprises three phases: locating code
    files, team building, and the kick-off meeting.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Locating
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: repository: ${\mathcal{R}}_{i}$22:     end if23:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Locating Code Files.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Firstly, the repository custodian employs the BM25 algorithm [[28](#bib.bib28)]
    to rank the files in the repository based on the GitHub issue description. Subsequently,
    the top $k$ files are selected as potential candidates for further coding. However,
    as described in §[2.2](#S2.SS2 "2.2 RQ 2: What Impacts the Performance Under Without-Oracle
    Setting? ‣ 2 Empirical Study ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"), this simple retrieval method can introduce irrelevant
    files, increasing the cost and reducing the effectiveness of subsequent code changing
    by LLMs. Therefore, it is necessary to filter them according the relevance. While
    it is feasible to directly assess the relevance between each file and the issue
    by LLMs, many queries are redundant as some code snippets in the previous request
    are duplicated, leading to unnecessary costs. Considering that applying the code
    change often modifies a specific part of the file rather than the whole file,
    we propose a method to reuse the previously requested information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3.1 Planning ‣ 3.3 Collaborative Process
    ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution") demonstrates, when a file $f_{i}$ in fewer length. Based on
    this relevance, the custodian agent filters candidate files, allowing the manager
    agent to define tasks with these relevant files.'
  prefs: []
  type: TYPE_NORMAL
- en: Team Building.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this process, the manager agent has the flexibility to “recruit” team members
    as the issue needs. Firstly, upon receiving the candidate files, the manager begins
    with analyzing the GitHub issue for the repository and breaks them into detailed
    file-level tasks. Specifically, for each code file $f_{i}$, thus forming the development
    team. The details of the team building are shown in the part of Algorithm [2](#alg2
    "Algorithm 2 ‣ Team Building. ‣ 3.3.1 Planning ‣ 3.3 Collaborative Process ‣ 3
    Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Making the Plan
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: candidate files: ${\mathcal{C}}_{i}^{k}$'
  prefs: []
  type: TYPE_NORMAL
- en: Kick-off Meeting.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'After building the team, the project manager organizes a kick-off meeting.
    This meeting serves multiple purposes: (1) To confirm whether the tasks assigned
    by the project manager are reasonable and ensure that all developers in the team
    can collaboratively resolve the issue $q_{x}$. This work plan is presented as
    code, and embedded into the main program for execution.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Coding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Algorithm 3 Coding Task Execution
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: file-task set: ${\mathcal{T}}_{i}^{k}$23:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of agents participate in the coding process: developers and QA engineers.
    As outlined in Algorithm [3](#alg3 "Algorithm 3 ‣ 3.3.2 Coding ‣ 3.3 Collaborative
    Process ‣ 3 Methodology ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"), for each task $t_{i}$ as the issue solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To validate the performance of our framework, we conduct experiments and compare
    it against other popular Large Language Models (LLMs) to demonstrate its overall
    effectiveness (§[4.2](#S4.SS2 "4.2 RQ 3: How Effective Is Our Framework? ‣ 4 Experiments
    and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution")). In addition, to further investigate the effectiveness of
    our framework, we assessed its performance in two processes: planning (§[4.3](#S4.SS3
    "4.3 RQ 4: How Effective Is Our Planning Process? ‣ 4 Experiments and Analysis
    ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"))
    and coding (§[4.4](#S4.SS4 "4.4 RQ 5: How Effective Is Our Coding Process? ‣ 4
    Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Dataset and Experimental Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the experiments, we employ the SWE-bench dataset as the evaluation benchmark
    because it is the latest dataset specifically designed for evaluating the performance
    of the GitHub issue resolution. SWE-bench comprises $2,294$ popular Python repositories,
    representing real software evolution requirements. It includes two context settings:
    an Oracle retrieval and sparse retrieval. In the former setting (w/ Oracle), the
    LLM receives the files to modify, whereas, in the latter (w/o Oracle), the method
    needs to retrieve the files for modification.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the observation that experimental outcomes on the $25\%$ subset previously
    utilized in experiments for GPT-4 according to their materials ³³3[https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80](https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80).
    Moreover, the experimental scores for the five LLMs, have been made available
    by them ⁴⁴4[https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr](https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr).
  prefs: []
  type: TYPE_NORMAL
- en: Our framework is flexible to integrate various LLMs. To compare with the scores
    reported by SWE-bench, we select GPT-4 as the base LLM. Another reason for the
    selection is that GPT-4 shows remarkable performance on code generation and understanding,
    which has been demonstrated on benchmarks such as MBPP [[2](#bib.bib2)] and HumanEval [[9](#bib.bib9)].
    Claude-2 is not chosen due to the unavailability of API access.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following the SWE-bench [[15](#bib.bib15)], we use the applied and resolved
    ratio to evaluate the performance under the with-Oracle setting. Specifically,
    the applied ratio indicates the proportion of instances where the code change
    is successfully generated and can be applied to the existing code repository using
    Git tools, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Applied Ratio}=\frac{\left&#124;{\mathcal{D}}\right&#124;}{\left&#124;{\mathcal{I}}\right&#124;},~{}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where ${\mathcal{D}}$ is the set of all instances in the test set. The resolved
    ratio refers to the proportion of instances in which the code change is successfully
    applied and passed a series of tests, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $T_{old}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recall score versus file number curve is used to measure the effectiveness
    of file locating for the without-Oracle setting. The recall score refers to the
    proportion of files that are successfully located out of all the files that require
    modification. The formula for calculating the file location recall score for the
    $i$-th instance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Recall}=\frac{\left&#124;{\mathcal{G}}_{i}\cap{\mathcal{R}}_{i}\right&#124;}{\left&#124;{\mathcal{R}}_{i}\right&#124;}\times
    100\%,~{}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where ${\mathcal{G}}_{i}=\sum_{j=0}^{n}{g_{i,j}}$. In this curve, “file number”
    refers to the average number of files that need to be processed across all instances
    to achieve the given recall score. Specifically, it illustrates how many files
    averagely need to be located by our framework before reaching the recall score
    denoted by the curve at any point. This metric represents both the effectiveness
    and efficiency of file locating.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 RQ 3: How Effective Is Our Framework?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The comparative performance analysis between our framework and other LLMs on
    the same dataset is presented in Table [2](#S4.T2 "Table 2 ‣ 4.2 RQ 3: How Effective
    Is Our Framework? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution"). The results indicate that
    our framework significantly outperforms other LLMs. Notably, with a resolved ratio
    of $13.94\%$, our framework’s effectiveness is eight-fold that of the base model,
    GPT-4\. This substantial increase underscores our framework’s capability to harness
    the potential of LLMs more effectively. Furthermore, when contrasted with the
    previous state-of-the-art LLM, Claude-2, our framework’s resolved ratio exceeds
    that benchmark by more than two-fold. This superior performance unequivocally
    establishes the advance of our method. As Devin [[34](#bib.bib34)] uses a different
    subset from the evaluated subset of GPT-4 mentioned in the SWE-bench paper [[15](#bib.bib15)],
    the comparison between it and ours is discussed in § [5.2](#S5.SS2 "5.2 Comparison
    with Devin. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The Comparison of Overall Performance between MAGIS and Baselines
    on SWE-bench.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | % Applied | % Resolved |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | $11.67$ |'
  prefs: []
  type: TYPE_TB
- en: '| Claude-2 | $49.36$ |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | $13.24$ |'
  prefs: []
  type: TYPE_TB
- en: '| SWE-Llama 7b | $51.56$ |'
  prefs: []
  type: TYPE_TB
- en: '| SWE-Llama 13b | $49.13$ |'
  prefs: []
  type: TYPE_TB
- en: '| Devin [[34](#bib.bib34)]^* | $-$^* |'
  prefs: []
  type: TYPE_TB
- en: '| MAGIS | 97.39 | 13.94 |'
  prefs: []
  type: TYPE_TB
- en: '| MAGIS (w/o QA) | $92.71$ |'
  prefs: []
  type: TYPE_TB
- en: '| MAGIS (w/o hints) | $94.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| MAGIS (w/o hints, w/o QA) | $91.99$ |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this work is evaluated on a randomly chosen 25% test set, but this
    subset differs from the 25% subset experimented on GPT-4 in the SWE-bench.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The ablation study is designed to simulate two scenarios: (1) Without QA (w/o
    QA): Considering the QA engineer agent as optional within our framework, we directly
    evaluate the code changes generated by the developer agent, bypassing the QA process.
    This scenario aims to investigate the effectiveness and necessity of QA engineer
    review. (2) Without hints (w/o hints): Hints refer to the textual content found
    in the comments section of pull requests, which are typically created prior to
    the first commit of the pull request. The absence of hints (w/o hints) means our
    framework operates without any clarifications except for the issue, despite such
    information being available on GitHub before the issue resolution process begins.
    This analysis aims to explore if the participation of humans could potentially
    improve the success rate of issue resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: Our framework demonstrates the capability to significantly enhance issue resolution,
    even without the QA engineer or hints. It achieves a resolved ratio of $8.71$,
    respectively. These findings serve to underscore the value of QA engineers and
    the participation of humans, as demonstrated by the resolved rates achieved through
    their integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, to resolve the issue ⁵⁵5[https://code.djangoproject.com/ticket/30255](https://code.djangoproject.com/ticket/30255)
    from the repository Django ⁶⁶6[https://github.com/django/django/](https://github.com/django/django/),
    the developer modifies four hunks in two files ⁷⁷7[https://github.com/django/django/pull/12155/files](https://github.com/django/django/pull/12155/files),
    as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"). Despite the availability of two files in the with-Oracle
    setting, our proposed framework opts for modifications in only one file, as illustrated
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework? ‣ 4
    Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"). Remarkably, this simpler code change enables the
    repository to successfully pass all requisite test cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1304898b168a42ed24281c38872e18f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Case from Django (Gold).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0497113f4664ff5eed2453ff2f089368.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Case from Django (Ours).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1cc964793c5456536d433a289a1d47cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of Recall Scores between MAGIS and BM25.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 RQ 4: How Effective Is Our Planning Process?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To investigate the effectiveness of the planning process, we analyze the repository
    custodian and project manager agent, respectively. The performance of the repository
    custodian agent is observed in the recall score versus the file number curve,
    as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 RQ 3: How Effective Is Our Framework?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution") with the horizontal axis representing the average
    number of files and the vertical axis denoting the recall score associated with
    that number of files. This curve illustrates that our method consistently outperforms
    the BM25 baseline across different numbers of selected files, which suggests that
    our custodian can find as many relevant code files as possible with the lowest
    possible number of files.'
  prefs: []
  type: TYPE_NORMAL
- en: For the project manager agent, we examined the alignment of its generated task
    descriptions with the reference code change. A higher correlation score indicates
    a better alignment and thus, a more accurate and effective planning direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e544d1aa5b9658e4d8982d8e55a9985d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Distribution of the Correlation Score Between the Generated Task
    Description and the Reference Code Change.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlation scores are determined by GPT-4 based on a set of criteria defined
    in Table [3](#S4.T3 "Table 3 ‣ 4.3 RQ 4: How Effective Is Our Planning Process?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution"), which spans from a score of $1$. This signifies
    that when the generated task description closely aligns with the reference, there
    is a higher possibility to resolve the issue.'
  prefs: []
  type: TYPE_NORMAL
- en: The analysis above demonstrates the effectiveness of both the repository custodian
    and the project manager agent in the planning process of our framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The Meaning of Scores in GPT-4 Evaluation on the Correlation Between
    the Generated Task Description and the Reference Code Change.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Score | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | The code changes are unrelated to the task description. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | The code changes address a minor part of the task but are largely irrelevant.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | The code changes partially meet the task requirements but lack completeness
    or accuracy. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | The code changes are relevant and mostly complete, with minor discrepancies
    from the |'
  prefs: []
  type: TYPE_TB
- en: '| task description. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | The code changes perfectly align with the task description, fully addressing
    all specified |'
  prefs: []
  type: TYPE_TB
- en: '| requirements with high accuracy and completeness. |'
  prefs: []
  type: TYPE_TB
- en: '4.4 RQ 5: How Effective Is Our Coding Process?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To investigate the effectiveness of the coding process in our framework, we
    analyze the performance of the developer’s line locating and the issue resolving
    across instances of varying complexities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding Process?
    ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework
    for GitHub Issue ReSolution") illustrates the distribution of the line location
    overlap ratio of MAGIS and the baseline models (GPT-4 and Claude-2). The vertical
    axis quantifies the frequency of occurrences within specific ranges of line location
    overlap ratios for each group. This visualization reveals that our developer agent
    frequently attains a line location overlap ratio nearing $1$. Such a distribution
    validates the superior performance of MAGIS in line location.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further analysis is provided in Figure [8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How
    Effective Is Our Coding Process? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"), which illustrates
    the relationship between the line location overlap ratio and the issue resolved
    ratio within those overlaps. In the figure, the horizontal axis represents the
    range of overlap ratios for each bar’s corresponding interval, while the height
    of each bar indicates the resolved ratio for instances within that interval. These
    resolved ratios correspond to the scale on the left vertical axis. The orange
    curve represents the cumulative frequency of instances that can be resolved under
    different overlap ratio thresholds, with the cumulative frequency corresponding
    to the scale on the right side.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [8](#S4.F8 "Figure 8 ‣ 4.4 RQ 5: How Effective Is Our Coding
    Process? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"), the right four bars are higher than the
    five left, which indicates the resolved ratio can increase with the line location
    overlap. This observation also suggests that accurate line location is helpful
    for issue resolution. The cumulative frequency curve, shown in orange, provides
    an additional analysis, indicating the cumulative proportion of issues resolved
    ratio up to each point along the line location overlap. A steady increase in cumulative
    frequency accompanies the increase in line location overlap, reinforcing the idea
    that resolving issues is more successful in areas of high overlap. The slope of
    the curve’s left half is lower than that of the right half, indicating that the
    benefits of increasing the overlap ratio are less pronounced at lower overlap
    ratios than at higher ones. Therefore, the developer agent should prioritize improving
    its capability of line location.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, as shown in Table [4](#S4.T4 "Table 4 ‣ 4.4 RQ 5: How Effective Is
    Our Coding Process? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution"), we present a logistic regression
    analysis that quantifies the correlation between several complexity indices and
    issue resolution. The results show that GPT-4 has significant negative correlations
    across the number of files and functions, suggesting that as these indices increase,
    the likelihood of issue resolution decreases. Conversely, the negative correlations
    are less pronounced with our model, MAGIS, particularly in the number of files
    and functions, suggesting mitigation of challenges corresponding to these complexity
    indices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Correlation Between the Complexity Indices and the Issue Resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Files | # Functions | # Hunks | # Added LoC | # Deleted LoC |
    # Changed LoC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | $-25.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| MAGIS |   $-1.55$^* |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correlation between the index and the issue resolution is significant (P-value
    $<$).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a280d60e96da9a0b90062211b94835b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparison of Line Location Overlap between MAGIS (Ours) and Baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9c828ecea8040475b246766faf7b149.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Resolved Ratio in Different Line Location Overlap Intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the performance of the QA engineer, the ablation experiment is
    conducted and the results are shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 RQ 3: How
    Effective Is Our Framework? ‣ 4 Experiments and Analysis ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"). As the table
    shows, in settings with and without hints, the presence of the QA engineer can
    increase the resolved ratio by $1.57\%$, respectively. This overall enhancement
    substantiates the QA engineer’s contribution to improving outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, there is an issue ⁸⁸8[https://github.com/scikit-learn/scikit-learn/issues/9784](https://github.com/scikit-learn/scikit-learn/issues/9784)
    from the repository scikit-learn ⁹⁹9[https://github.com/scikit-learn/scikit-learn](https://github.com/scikit-learn/scikit-learn)
    and the reference code change ^(10)^(10)10[https://github.com/scikit-learn/scikit-learn/pull/9288](https://github.com/scikit-learn/scikit-learn/pull/9288)
    is shown in Figure [5](#S5.T5 "Table 5 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"). During the flow
    of our framework, the developer firstly modifies the code as shown in Figure [10](#S5.F10
    "Figure 10 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution") but the parameterrandom_state (Line $371$
    in the new-version code) of the function kmeans_single is not assigned the right
    number in seeds. After the erroneous modification was made, the QA engineer identified
    the mistake and provided feedback. Their commentary highlighted the issue: “This
    code change modifies the implementation of K-means algorithm and doesn’t seem
    entirely correct”. They further elaborated, “Running the algorithm just one time
    could lead to worse results, compared to running it multiple times (n_init times)
    and choosing the best result, as was originally done” This critique specifically
    targets the flaw associated with the iterative process (“running times”). With
    the help of the QA engineer, the developer further revise the code, and the final
    code change is shown in Figure [10](#S5.F10 "Figure 10 ‣ 5 Statistics and Discussion
    ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution").
    All of the necessary test cases are passed after applying this code change. Both
    the ablation study and the case study underscore the QA engineer’s efficacy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Statistics and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/a799023c974060885392df8078853f8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 5: Case from scikit-learn (Gold).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94712055a24172042d33d135fbcd00a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Case from scikit-learn (Ours, before review).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21c164ed82c1779debe673bc3cd138d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Case from scikit-learn (Ours, after review).'
  prefs: []
  type: TYPE_NORMAL
- en: This section provides statistics on code changes corresponding to resolved issues
    and those applicable but unresolved using our framework.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Complexity of Code Changes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The statistics on the code change for instances with resolved issues are presented
    in Table [6](#S5.T6 "Table 6 ‣ 5.1 Complexity of Code Changes. ‣ 5 Statistics
    and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution"). Overall, the statistical information of the generated code
    changes for these instances, such as the average number of code files, functions,
    hunks, and deleted lines, all differ slightly (not exceeding $0.3$ lines. Results
    demonstrate the effectiveness of our method in resolving complex issues that need
    to modify the code file on multiple locations and with long context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The Statistical Analysis of Our Framework on Resolved Instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MAGIS | Gold |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Min | Max | Avg. | Min | Max | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| # Code Files | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Functions | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Hunks | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Added Lines | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Deleted Lines | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| Change Start Index | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Change End Index | $22$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Changed Lines | $2$ |'
  prefs: []
  type: TYPE_TB
- en: 'Specifically, the distribution of the number of modified lines for the resolving
    instances is shown in Figure [12](#S5.F12 "Figure 12 ‣ 5.1 Complexity of Code
    Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"). We observe that the distribution of the
    number of modified lines in our framework for the solved instances exceeds that
    of the reference solution, especially in terms of the number of added lines being
    significantly higher than the reference. Upon manual inspection, we found that
    the generation results provided by our framework often contained more comment
    information, which led to an increase in the total number of modified lines. For
    example, Figure [10](#S5.F10 "Figure 10 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution") displays the generation
    result of our framework. Lines $365,368,371,374,383$ in the new version file correspond
    to the comment for the added code. These natural language descriptions are valuable
    in actual software evolution [[14](#bib.bib14), [22](#bib.bib22)]. In contrast,
    Figure [5](#S5.T5 "Table 5 ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution") shows a human-written
    solution lacking such explanatory comments, which might disadvantage software
    maintainers in reading and understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc47148317894685be429c2ddc762619.png)![Refer to caption](img/74a1d36e5ef4d7641f756de633cedc50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Distribution of the LoC in the Resolved Instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20dc242d12385ae3ecea80eb7c9a4cb5.png)![Refer to caption](img/a9d83218a1dc90f9b6e392142bc68c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Distribution of the LoC in the Applied but Not Resolved Instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The statistics on the code change for instances without resolved issues are
    shown in Table [7](#S5.T7 "Table 7 ‣ 5.1 Complexity of Code Changes. ‣ 5 Statistics
    and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent Framework for GitHub
    Issue ReSolution"). From the table, our framework can generate applicable code
    changes including up to $13$ lines. These results suggest that our method has
    a strong adaptability in generating applicable code changes. However, considering
    that these code changes have not passed all the potential test cases they could
    pass, which indicates that there is still room for improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The Statistical Analysis of Our Framework on Applied but Not Resolved
    Instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MAGIS | Gold |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Min | Max | Avg. | Min | Max | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| # Code Files | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Functions | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Hunks | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Added Lines | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Deleted Lines | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| Change Start Index | $1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Change End Index | $9$ |'
  prefs: []
  type: TYPE_TB
- en: '| # Changed Lines | $1$ |'
  prefs: []
  type: TYPE_TB
- en: 'To further analyze the reasons behind the failure of test cases in these instances,
    we have quantified the distribution of the lengths of code changes in the unresolved
    instances, as shown in Figure [12](#S5.F12 "Figure 12 ‣ 5.1 Complexity of Code
    Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution"). From the figure, we observe that for
    unresolved instances, the framework tends to delete a larger number of lines while
    adding fewer lines, in contrast to the distribution of human-written changes.
    This discrepancy may point to different repair strategies or attitudes towards
    problem-solving, where the framework presented herein might prefer to reduce errors
    by removing potentially problematic code, whereas human developers may lean towards
    adding new code to address issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, a comparison between Table [6](#S5.T6 "Table 6 ‣ 5.1 Complexity of
    Code Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based Multi-Agent
    Framework for GitHub Issue ReSolution") and Table [7](#S5.T7 "Table 7 ‣ 5.1 Complexity
    of Code Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS : LLM-Based
    Multi-Agent Framework for GitHub Issue ReSolution") reveals that the latter contains
    a higher overall number of files, hunks, and changed lines of code. These instances,
    involving more modification locations, correspond to more complex scenarios. This
    phenomenon suggests that the performance of our framework in resolving such complex
    issues requires further enhancement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the variability in difficulty across different software repositories
    may influence the effectiveness of code changes. To this end, we compile statistics
    on the resolved ratios in various software repositories, as shown in Figure [13](#S5.F13
    "Figure 13 ‣ 5.1 Complexity of Code Changes. ‣ 5 Statistics and Discussion ‣ \stackon[0pt]MAGIS
    : LLM-Based Multi-Agent Framework for GitHub Issue ReSolution"). From the figure,
    we observe that there is a significant variation in the resolved ratios across
    different repositories in our framework. Some repositories have a resolved ratio
    as high as $40\%$. This suggests that the differences among various software such
    as code structure and coding style can impact the generation and application of
    the code change.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57c180cbc0367859a12b7c07e0e81f37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The Number of Applied and Resolved Instances in Different Repositories.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Comparison with Devin.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Devin is a novel agent for software development [[34](#bib.bib34)], and its
    performance has also been assessed using the SWE-bench. However, the evaluation
    dataset employed by Devin differs from the subset used for experiments with GPT-4
    reported by the paper of SWE-bench [[15](#bib.bib15)]. An analysis of the repository
    name and pull request ID of each instance reveals that only $140$ instances overlap
    between the two datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Within the shared pool of $140$) issues ^(11)^(11)11[https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs/pass](https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs/pass).
    This comparison, however, may not be entirely equitable. Devin’s possible underlying
    LLM is unknown, and it possesses the capability to integrate feedback from the
    environment. Moreover, Devin’s reported scores are under the setting given the
    entire repository, and it operates with “common developer tools including the
    shell, code editor, and browser”, and “agents with internet access could potentially
    find external information through other methods” as detailed at the report ^(12)^(12)12[https://www.cognition-labs.com/introducing-devin](https://www.cognition-labs.com/introducing-devin).
    In contrast, our approach solely relies on the shell, without the need of any
    additional external tools.
  prefs: []
  type: TYPE_NORMAL
- en: For running time, 72% of instances resolved by Devin require greater than $10$
    minutes to complete. In contrast, our framework finalizes each resolved issue
    within approximately 3 minutes. On average, our framework completes the processing
    of each instance in under 5 minutes, demonstrating its capability to assist in
    resolving GitHub issues with minimal time expenditure.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The design of prompt words may impact the performance of LLMs, thereby affecting
    the validity and fairness of the results [[8](#bib.bib8)]. While this paper focuses
    on innovative aspects of the proposed framework design and relies on practical
    guidelines for the design of prompt word templates [[29](#bib.bib29)] to reduce
    the emergence of design biases, the complete elimination of the prompt bias is
    extremely difficult due to the inherent biases in the dataset instances and the
    limitations of API resources.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The dataset contains a limited variety of software types. The evaluating dataset,
    SWE-bench, encompasses $12$ repositories, which cover the Python programming language.
    However, this quantity remains insufficient compared to the diverse software projects
    available on GitHub. The code style, architectural design, and implementation
    techniques of these selected repositories, while representative, cannot fully
    reflect the diversity of all code repositories. In particular, the current dataset
    may fail to encompass some specialized fields or different programming paradigms,
    such as microservice architecture [[41](#bib.bib41)] and functional programming [[16](#bib.bib16)].
    This limitation implies that, although our framework is designed to be independent
    of any specific software, the validation of its effectiveness and general applicability
    might be affected by this limited sample scope. Therefore, applying the findings
    of this paper to other code repositories may require further validation.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) refer to the pre-trained language models that contain
    a large number of parameters [[38](#bib.bib38)]. The parameter counts of these
    models typically range in the tens or hundreds of billions. Popular LLMs include
    the Generative Pre-trained Transformer (GPT) series, such as GPT-3 [[27](#bib.bib27)],
    GPT-4 [[25](#bib.bib25)], and the open-source LLaMA [[35](#bib.bib35)] which publicly
    shares its weight information. The first version of the open-source model LLaMA
    has parameters ranging from 7 billion to 65 billion. Many researchers [[33](#bib.bib33),
    [11](#bib.bib11)] have built upon the foundation of LLaMA, implementing enhancements
    to forge new LLMs. These LLMs have demonstrated formidable natural language generation
    capabilities in general scenarios, with GPT-4, in particular, standing out [[18](#bib.bib18),
    [39](#bib.bib39)]. It has consistently maintained the top position in several
    rankings, including code generation, reflecting its significant potential in tasks
    related to software engineering [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 LLM-Based Multi-Agent System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the powerful text generation capabilities of LLMs, many researchers [[12](#bib.bib12),
    [31](#bib.bib31), [7](#bib.bib7), [37](#bib.bib37), [26](#bib.bib26), [36](#bib.bib36)]
    have explored the construction of LLM-based Multi-Agent Systems, enabling them
    to accomplish tasks beyond the capabilities of the LLMs themselves. For example,
    MetaGPT [[12](#bib.bib12)], which simulates the Standardized Operating Procedures
    (SOPs) of a programming team, completing tasks including definition, design, planning,
    coding, and testing through constructed roles (e.g., product managers, architects,
    project managers, etc.). This framework has achieved leading scores on the HumanEval [[9](#bib.bib9)]
    and MBPP [[2](#bib.bib2)], outperforming many LLMs, and researchers show its ability
    to complete a software establishment (e.g., a code repository to play Gomoku game),
    indicating that a multi-agent framework can better leverage the capabilities of
    LLMs in code generation tasks. Moreover, Qian et al. [[26](#bib.bib26)] designed
    ChatDev, a virtual development company simulating a human development team, which
    decomposes requirements into atomic tasks assigned to the developers. Developers
    mitigate the hallucination that may arise with the LLM through mutual communication
    and self-reflection mechanisms. Experimental results show that ChatDev can complete
    the establishment of some small software (averaging no more than $5$ minutes on
    average). However, these works focus on the transformation from the requirement
    to software and overlook the code change generation during software evolution
    which needs not only understanding the requirement but also dealing with the large
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, this paper illuminates the potential of large language models
    in software evolution, particularly in resolving GitHub issues. Our study identifies
    the challenges of direct LLM application. To address them, we proposed a novel
    LLM-based multi-agent framework, MAGIS, enhancing issue resolution through well-designed
    agents’ collaboration. The superiority of MAGIS on the SWE-bench dataset against
    popular LLMs highlights its effectiveness, pointing towards a promising direction
    for integrating LLMs into software evolution workflows. This work not only shows
    the potential of LLMs in GitHub issue resolution but also explores an LLM-based
    paradigm for software evolution.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic [2023] Anthropic. Claude 2. [https://www.anthropic.com/news/claude-2](https://www.anthropic.com/news/claude-2),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V.
    Le, and Charles Sutton. Program synthesis with large language models. *arXiv Preprint*,
    abs/2108.07732, 2021. URL [https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baum et al. [2016] Tobias Baum, Olga Liskin, Kai Niklas, and Kurt Schneider.
    Factors influencing code review processes in industry. In Thomas Zimmermann, Jane
    Cleland-Huang, and Zhendong Su, editors, *Proceedings of the 24th ACM SIGSOFT
    International Symposium on Foundations of Software Engineering, FSE 2016, Seattle,
    WA, USA, November 13-18, 2016*, pages 85–96\. ACM, 2016. doi: 10.1145/2950290.2950323.
    URL [https://doi.org/10.1145/2950290.2950323](https://doi.org/10.1145/2950290.2950323).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bissyandé et al. [2013] Tegawendé F. Bissyandé, David Lo, Lingxiao Jiang, Laurent
    Réveillère, Jacques Klein, and Yves Le Traon. Got issues? who cares about it?
    A large scale investigation of issue trackers from github. In *IEEE 24th International
    Symposium on Software Reliability Engineering, ISSRE 2013, Pasadena, CA, USA,
    November 4-7, 2013*, pages 188–197\. IEEE Computer Society, 2013. doi: 10.1109/ISSRE.2013.6698918.
    URL [https://doi.org/10.1109/ISSRE.2013.6698918](https://doi.org/10.1109/ISSRE.2013.6698918).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bosu and Carver [2014] Amiangshu Bosu and Jeffrey C. Carver. Impact of developer
    reputation on code review outcomes in OSS projects: an empirical investigation.
    In Maurizio Morisio, Tore Dybå, and Marco Torchiano, editors, *2014 ACM-IEEE International
    Symposium on Empirical Software Engineering and Measurement, ESEM ’14, Torino,
    Italy, September 18-19, 2014*, pages 33:1–33:10\. ACM, 2014. doi: 10.1145/2652524.2652544.
    URL [https://doi.org/10.1145/2652524.2652544](https://doi.org/10.1145/2652524.2652544).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. [2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M.
    Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks
    of artificial general intelligence: Early experiments with GPT-4. *arXiv Preprint*,
    abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712. URL [https://doi.org/10.48550/arXiv.2303.12712](https://doi.org/10.48550/arXiv.2303.12712).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv Preprint*, abs/2308.07201, 2023. doi: 10.48550/ARXIV.2308.07201.
    URL [https://doi.org/10.48550/arXiv.2308.07201](https://doi.org/10.48550/arXiv.2308.07201).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Lichang Chen, Jiuhai Chen, Heng Huang, and Minhao Cheng.
    PTP: boosting stability and performance of prompt tuning with perturbation-based
    regularizer. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2023, Singapore, December 6-10, 2023*, pages 13512–13525\. Association for Computational
    Linguistics, 2023. URL [https://aclanthology.org/2023.emnlp-main.833](https://aclanthology.org/2023.emnlp-main.833).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *arXiv Preprint*,
    abs/2107.03374, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2023] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei
    Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval:
    A manually-crafted benchmark for evaluating llms on class-level code generation,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng and Liu [2023] Xinyang Geng and Hao Liu. Openllama: An open reproduction
    of llama, May 2023. URL [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. [2023] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    Metagpt: Meta programming for a multi-agent collaborative framework, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2023] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang,
    Li Li, Xiapu Luo, David Lo, John C. Grundy, and Haoyu Wang. Large language models
    for software engineering: A systematic literature review. *arXiv Preprint*, abs/2308.10620,
    2023. doi: 10.48550/ARXIV.2308.10620. URL [https://doi.org/10.48550/arXiv.2308.10620](https://doi.org/10.48550/arXiv.2308.10620).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2022] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and
    Thomas Zimmermann. Practitioners’ expectations on automated code comment generation.
    In *44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
    2022, Pittsburgh, PA, USA, May 25-27, 2022*, pages 1693–1705\. ACM, 2022. doi:
    10.1145/3510003.3510152. URL [https://doi.org/10.1145/3510003.3510152](https://doi.org/10.1145/3510003.3510152).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jimenez et al. [2024] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models
    resolve real-world github issues? In *The Twelfth International Conference on
    Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net,
    2024. URL [https://openreview.net/forum?id=VTF8yNQM66](https://openreview.net/forum?id=VTF8yNQM66).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnsson [1987] Thomas Johnsson. Attribute grammars as a functional programming
    paradigm. In Gilles Kahn, editor, *Functional Programming Languages and Computer
    Architecture, Portland, Oregon, USA, September 14-16, 1987, Proceedings*, volume
    274 of *Lecture Notes in Computer Science*, pages 154–173\. Springer, 1987. doi:
    10.1007/3-540-18317-5\_10. URL [https://doi.org/10.1007/3-540-18317-5_10](https://doi.org/10.1007/3-540-18317-5_10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kononenko et al. [2015] Oleksii Kononenko, Olga Baysal, Latifa Guerrouj, Yaxin
    Cao, and Michael W. Godfrey. Investigating code review quality: Do people and
    participation matter? In Rainer Koschke, Jens Krinke, and Martin P. Robillard,
    editors, *2015 IEEE International Conference on Software Maintenance and Evolution,
    ICSME 2015, Bremen, Germany, September 29 - October 1, 2015*, pages 111–120\.
    IEEE Computer Society, 2015. doi: 10.1109/ICSM.2015.7332457. URL [https://doi.org/10.1109/ICSM.2015.7332457](https://doi.org/10.1109/ICSM.2015.7332457).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023a] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of
    large language models for code generation. In Alice Oh, Tristan Naumann, Amir
    Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, *Advances in
    Neural Information Processing Systems 36: Annual Conference on Neural Information
    Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
    2023*, 2023a. URL [http://papers.nips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023b] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv Preprint*, abs/2307.03172, 2023b. doi: 10.48550/ARXIV.2307.03172.
    URL [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023c] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv Preprint*, abs/2307.03172, 2023c. doi: 10.48550/ARXIV.2307.03172.
    URL [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McIntosh et al. [2014] Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E.
    Hassan. The impact of code review coverage and code review participation on software
    quality: a case study of the qt, vtk, and ITK projects. In Premkumar T. Devanbu,
    Sung Kim, and Martin Pinzger, editors, *11th Working Conference on Mining Software
    Repositories, MSR 2014, Proceedings, May 31 - June 1, 2014, Hyderabad, India*,
    pages 192–201\. ACM, 2014. doi: 10.1145/2597073.2597076. URL [https://doi.org/10.1145/2597073.2597076](https://doi.org/10.1145/2597073.2597076).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mu et al. [2023] Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang.
    Developer-intent driven code comment generation. In *45th IEEE/ACM International
    Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20,
    2023*, pages 768–780\. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00073. URL [https://doi.org/10.1109/ICSE48619.2023.00073](https://doi.org/10.1109/ICSE48619.2023.00073).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. GPT-4 technical report, 2023. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023a] OpenAI. Gpt-3.5 turbo fine-tuning and api updates. [https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates),
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023b] OpenAI. Gpt-4. [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4),
    2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. [2023] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng
    Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    Communicative agents for software development. *arXiv Preprint*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robertson et al. [1994] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline
    Hancock-Beaulieu, and Mike Gatford. Okapi at TREC-3. In Donna K. Harman, editor,
    *Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg,
    Maryland, USA, November 2-4, 1994*, volume 500-225 of *NIST Special Publication*,
    pages 109–126\. National Institute of Standards and Technology (NIST), 1994. URL
    [http://trec.nist.gov/pubs/trec3/papers/city.ps.gz](http://trec.nist.gov/pubs/trec3/papers/city.ps.gz).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shieh [2023] Jessica Shieh. Best practices for prompt engineering with openai
    api. *OpenAI, February https://help. openai. com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2024] Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang
    Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng
    Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li,
    and Zhiyong Wu. A survey of neural code intelligence: Paradigms, advances and
    beyond, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent LLM agents. *arXiv Preprint*,
    abs/2306.03314, 2023. doi: 10.48550/ARXIV.2306.03314. URL [https://doi.org/10.48550/arXiv.2306.03314](https://doi.org/10.48550/arXiv.2306.03314).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. [2024] Wei Tao, Yucheng Zhou, Yanlin Wang, Hongyu Zhang, Haofen
    Wang, and Wenqiang Zhang. Kadel: Knowledge-aware denoising learning for commit
    message generation. *ACM Trans. Softw. Eng. Methodol.*, jan 2024. ISSN 1049-331X.
    doi: 10.1145/3643675. URL [https://doi.org/10.1145/3643675](https://doi.org/10.1145/3643675).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team [2023] LLaMA-MoE Team. Llama-moe: Building mixture-of-experts from llama
    with continual pre-training, Dec 2023. URL [https://github.com/pjlab-sys4nlp/llama-moe](https://github.com/pjlab-sys4nlp/llama-moe).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Team [2024] The Cognition Team. Swe-bench technical report, 2024. URL [https://www.cognition-labs.com/post/swe-bench-technical-report](https://www.cognition-labs.com/post/swe-bench-technical-report).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models. *arXiv
    preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tufano et al. [2024] Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian
    Moghaddam, and Neel Sundaresan. Autodev: Automated ai-driven development, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen LLM applications via multi-agent conversation framework. *arXiv Preprint*,
    abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL [https://doi.org/10.48550/arXiv.2308.08155](https://doi.org/10.48550/arXiv.2308.08155).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of
    large language models. *arXiv Preprint*, abs/2303.18223, 2023. doi: 10.48550/ARXIV.2303.18223.
    URL [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023a] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
    Moritz Hardt, and Sergey Levine, editors, *Advances in Neural Information Processing
    Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS
    2023, New Orleans, LA, USA, December 10 - 16, 2023*, 2023a. URL [http://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023b] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing
    Chen, Lianghong Guo, and Weicheng Wang. Towards an understanding of large language
    models in software engineering tasks. *arXiv Preprint*, abs/2308.11396, 2023b.
    doi: 10.48550/ARXIV.2308.11396. URL [https://doi.org/10.48550/arXiv.2308.11396](https://doi.org/10.48550/arXiv.2308.11396).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2021] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai
    Li, and Dan Ding. Fault analysis and debugging of microservice systems: Industrial
    survey, benchmark system, and empirical study. *IEEE Trans. Software Eng.*, 47(2):243–260,
    2021. doi: 10.1109/TSE.2018.2887384. URL [https://doi.org/10.1109/TSE.2018.2887384](https://doi.org/10.1109/TSE.2018.2887384).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023] Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong
    Long, Jian-Guang Lou, and Jianbing Shen. Thread of thought unraveling chaotic
    contexts. *arXiv Preprint*, abs/2311.08734, 2023. doi: 10.48550/ARXIV.2311.08734.
    URL [https://doi.org/10.48550/arXiv.2311.08734](https://doi.org/10.48550/arXiv.2311.08734).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
