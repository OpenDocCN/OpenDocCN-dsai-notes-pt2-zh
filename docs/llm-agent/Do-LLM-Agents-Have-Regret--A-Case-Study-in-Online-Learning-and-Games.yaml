- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Do LLM Agents Have Regret? A Case Study in Online Learning and Games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.16843](https://ar5iv.labs.arxiv.org/html/2403.16843)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Chanwoo Park    Chanwoo Park^‡^‡‡Equal Contribution    Xiangyu Liu^†^†footnotemark:
       Asuman Ozdaglar    Kaiqing Zhang ^†^†C. Park and A. Ozdaglar are with Massachusetts
    Institute of Technology, Cambridge, MA, 02139. X. Liu and K. Zhang are with the
    University of Maryland, College Park, MD, 20742. E-mails: {cpark97,asuman}@mit.edu,
    {xyliu999,kaiqing}@umd.edu.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have been increasingly employed for (interactive)
    decision-making, via the development of LLM-based autonomous agents. Despite their
    emerging successes, the performance of LLM agents in decision-making has not been
    fully investigated through quantitative metrics, especially in the multi-agent
    setting when they interact with each other, a typical scenario in real-world LLM-agent
    applications. To better understand the limits of LLM agents in these interactive
    environments, we propose to study their interactions in benchmark decision-making
    settings in online learning and game theory, through the performance metric of
    *regret*. We first empirically study the no-regret behaviors of LLMs in canonical
    (non-stationary) online learning problems, as well as the emergence of equilibria
    when LLM agents interact through playing repeated games. We then provide some
    theoretical insights into the no-regret behaviors of LLM agents, under certain
    assumptions on the supervised pre-training and the rationality model of human
    decision-makers who generate the data. Notably, we also identify (simple) cases
    where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret
    behaviors, we propose a novel *unsupervised* training loss of *regret-loss*, which,
    in contrast to the supervised pre-training loss, does not require the labels of
    (optimal) actions. We then establish the statistical guarantee of generalization
    bound for regret-loss minimization, followed by the optimization guarantee that
    minimizing such a loss may automatically lead to known no-regret learning algorithms.
    Our further experiments demonstrate the effectiveness of our regret-loss, especially
    in addressing the above “regrettable” cases.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Live Life with No Excuses. Travel with No Regret.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have recently exhibited remarkable reasoning capabilities
    (Bubeck et al., [2023](#bib.bib23); Achiam et al., [2023](#bib.bib3); Wei et al.,
    [2022b](#bib.bib111); Yao et al., [2023a](#bib.bib117)). As a consequence, a burgeoning
    body of work has been investigating the employment of LLMs as central controllers
    for (interactive) decision-making, through the construction of *LLM-based autonomous
    agents* (Hao et al., [2023](#bib.bib46); Shen et al., [2023](#bib.bib94); Yao
    et al., [2023b](#bib.bib118); Shinn et al., [2023](#bib.bib95); Wang et al., [2023c](#bib.bib107);
    Significant Gravitas, [2023](#bib.bib96)). Specifically, the LLM agent interacts
    with the (physical) world in a *dynamic/sequential* way: it uses LLMs as an oracle
    for reasoning, then acts in the environment based on the reasoning and the feedback
    it perceives over time. LLM agent has achieved impressive successes in embodied
    AI (Ahn et al., [2022](#bib.bib6); Huang et al., [2022a](#bib.bib51); Wang et al.,
    [2023a](#bib.bib105)), natural science (Wu et al., [2023](#bib.bib112); Swan et al.,
    [2023](#bib.bib98)), and social science (Park et al., [2022](#bib.bib84), [2023](#bib.bib83))
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides being *dynamic*, another increasingly captivating feature of LLM-based
    decision-making is the involvement of *strategic* interactions, oftentimes among
    multiple LLM agents. For example, it has been continually reported that the reasoning
    capability of LLMs can be improved by interacting with each other through negotiation
    and/or debate games (Fu et al., [2023](#bib.bib40); Du et al., [2023](#bib.bib35));
    LLM agents have now been widely used to *simulate* the strategic behaviors for
    social and economic studies, to understand the emerging behaviors in interactive
    social systems (Aher et al., [2023](#bib.bib4); Park et al., [2023](#bib.bib83)).
    Moreover, LLMs have also exhibited remarkable potential in solving various games
    (Bakhtin et al., [2022](#bib.bib14); Mukobi et al., [2023](#bib.bib79)), and in
    fact, a rapidly expanding literature has employed *repeated games* as a fundamental
    benchmark to understand the strategic behaviors of LLMs (Brookins and DeBacker,
    [2023](#bib.bib20); Akata et al., [2023](#bib.bib8); Fan et al., [2023](#bib.bib38)).
    These exciting empirical successes call for a rigorous examination and understanding
    through a theoretical lens of decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: '*Regret*, on the other hand, has been a core metric in (online) decision-making.
    It measures how “sorry” the decision-maker is, in retrospect, not to have followed
    the best prediction in hindsight (Shalev-Shwartz, [2012](#bib.bib92)). It provides
    not only a sensible way to *evaluate* the intelligence level of online decision-makers,
    but also a quantitative way to measure their *robustness* against arbitrary (and
    possibly adversarial) environments. More importantly, it inherently offers a connection
    to modeling and analyzing strategic behaviors: the long-run interaction of no-regret
    learners leads to certain *equilibria* when they repeatedly play games (Cesa-Bianchi
    and Lugosi, [2006](#bib.bib26)). In fact, *no-regret* learning has served as a
    natural model for predicting and explaining human behaviors in strategic decision-making,
    with experimental evidence (Erev and Roth, [1998](#bib.bib37); Nekipelov et al.,
    [2015](#bib.bib80); Balseiro and Gur, [2019](#bib.bib15)). It has thus been posited
    as an important model of “rational behavior” in playing games (Blum et al., [2008](#bib.bib18);
    Roughgarden, [2015](#bib.bib88); Roughgarden et al., [2017](#bib.bib89)). Thus,
    it is natural to ask:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Can we examine and better understand the online and strategic decision-making'
  prefs: []
  type: TYPE_NORMAL
- en: behaviors of LLMs through the lens of *regret*?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Acknowledging that LLM(-agents) are extremely complicated to analyze, to gain
    some insights into the question, we focus on benchmark decision-making settings:
    online learning with convex (linear) loss functions, and playing repeated games.
    We summarize our contributions as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: Contributions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, we carefully examine the performance of several representative pre-trained
    LLMs in aforementioned benchmark online decision-making settings, in terms of
    *regret*. We observe that oftentimes, LLM agents exhibit no-regret behaviors in
    these (non-stationary) online learning settings, where the loss functions change
    over time either arbitrarily (and even adversarially), or by following some patterns
    with bounded variation, and in playing both representative and randomly generated
    repeated games. For the latter, equilibria will emerge as the long-term behavior
    of the interactions when all LLM agents are no-regret. Second, we provide some
    theoretical insights into the observed no-regret behaviors, based on some hypothetical
    model of the human decision-makers who generate the data, and certain assumptions
    on the *supervised pre-training* procedure, a common practice in training large
    models for decision-making. In particular, we make a connection of pre-trained
    LLMs to the known no-regret algorithm of *follow-the-perturbed-leader* (FTPL)
    under such assumptions. Third, we also identify (simple) cases where advanced
    LLMs as GPT-4 fail to be no-regret. We thus propose a novel *unsupervised* training
    loss, *regret-loss*, which, in contrast to the supervised pre-training loss, does
    not require the *labels* of (optimal) actions. We then establish both statistical
    and optimization guarantees for regret-loss minimization, showing that minimizing
    such a loss may automatically lead to known no-regret learning algorithms. Our
    further experiments demonstrate the effectiveness of regret-loss, especially in
    addressing the above “regrettable” cases.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLM(-agent) for decision-making.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The impressive capability of LLMs for *reasoning* (Bubeck et al., [2023](#bib.bib23);
    Achiam et al., [2023](#bib.bib3); Wei et al., [2022b](#bib.bib111), [a](#bib.bib110);
    Srivastava et al., [2023](#bib.bib97); Yao et al., [2023a](#bib.bib117)) has inspired
    a growing line of research on *LLM for (interactive) decision-making*, i.e., an
    LLM-based autonomous agent interacts with the environment by taking actions repeatedly/sequentially,
    based on the feedback it perceives. Some promises have been shown from a *planning*
    perspective (Hao et al., [2023](#bib.bib46); Valmeekam et al., [2023](#bib.bib100);
    Huang et al., [2022b](#bib.bib52); Shen et al., [2023](#bib.bib94)). In particular,
    for embodied AI applications, e.g., robotics, LLMs have achieved impressive performance
    when used as the controller for decision-making (Ahn et al., [2022](#bib.bib6);
    Yao et al., [2023b](#bib.bib118); Shinn et al., [2023](#bib.bib95); Wang et al.,
    [2023c](#bib.bib107); Driess et al., [2023](#bib.bib34); Significant Gravitas,
    [2023](#bib.bib96)). However, the performance of decision-making has not been
    rigorously characterized via the regret metric in these works. Very recently,
    Liu et al. ([2023c](#bib.bib71)) has proposed a principled architecture for LLM-agent,
    with provable regret guarantees in stationary and stochastic decision-making environments,
    under the Bayesian adaptive Markov decision processes framework. In contrast,
    our work focuses on online learning and game-theoretic settings, in potentially
    adversarial and non-stationary environments. Moreover, (first part of) our work
    focuses on *evaluating* the intelligence level of LLM per se in decision-making
    (in terms of the regret metric), while Liu et al. ([2023c](#bib.bib71)) focused
    on *developing* a new architecture that uses LLM as an oracle for reasoning, together
    with memory and specific planning/acting subroutines, *to achieve* sublinear (Bayesian)
    regret, in stationary and stochastic environments.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs in multi-agent environments.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The interaction of multiple LLM agents has garnered significant attention lately.
    For example, Fu et al. ([2023](#bib.bib40)) showed that LLMs can autonomously
    improve each other in a negotiation game by playing and criticizing each other.
    Similarly, (Du et al., [2023](#bib.bib35); Liang et al., [2023](#bib.bib66); Xiong
    et al., [2023](#bib.bib114); Chan et al., [2024](#bib.bib27); Li et al., [2023c](#bib.bib62))
    showed that multi-LLM *debate* can improve the reasoning and evaluation capabilities
    of the LLMs. Qian et al. ([2023](#bib.bib85)); Schick et al. ([2023](#bib.bib90));
    Wu et al. ([2023](#bib.bib112)) demonstrated the potential of multi-LLM interactions
    and collaboration in software development, writing, and problem-solving, respectively.
    Zhang et al. ([2024](#bib.bib120)) exhibited a similar potential in embodied cooperative
    environments. More formally, multi-LLM interactions have also been investigated
    under a *game-theoretic* framework, to characterize the *strategic* decision-making
    of LLM agents. Bakhtin et al. ([2022](#bib.bib14)); Mukobi et al. ([2023](#bib.bib79))
    and Xu et al. ([2023b](#bib.bib116), [a](#bib.bib115)) have demonstrated the promise
    of LLMs in playing Diplomacy and WereWolf games, respectively, which are both
    language-based games with a mixture of competitive and cooperative agents. Note
    that these works utilized LLM to solve a specific rather than a general game.
    Related to our work, Brookins and DeBacker ([2023](#bib.bib20)); Akata et al.
    ([2023](#bib.bib8)); Lorè and Heydari ([2023](#bib.bib72)); Brookins and DeBacker
    ([2023](#bib.bib20)); Fan et al. ([2023](#bib.bib38)) have also used (repeated)
    matrix games as a benchmark to evaluate the reasoning capability and rationality
    of LLM agents. In contrast to our work, these empirical studies have not formally
    investigated LLM agents using the metric of *regret*, nor through the lenses of
    *online learning* and *equilibrium-computation*, which are all fundamental in
    modeling and analyzing strategic multi-agent interactions. Moreover, our work
    also provides theoretical results to explain and further enhance the no-regret
    property of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs & Human/Social behavior.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs have also been used to *simulate* the behavior of human beings, for social
    science and economics studies (Engel et al., [2023](#bib.bib36)). The extent of
    LLMs simulating human behavior has been claimed as a way to evaluate the level
    of its intelligence in a controlled environment (Aher et al., [2023](#bib.bib4);
    Tsai et al., [2023](#bib.bib99)). For example, Li et al. ([2023b](#bib.bib61));
    Hong et al. ([2024](#bib.bib49)); Zhao et al. ([2023](#bib.bib123)) showed that
    by specifying different “roles” to LLM agents, certain collaborative/competitive
    behaviors can emerge. Argyle et al. ([2023](#bib.bib10)) showed that LLMs can
    emulate response distributions from diverse human subgroups, illustrating their
    adaptability. Horton ([2023](#bib.bib50)) argued that an LLM, as a computational
    model of humans, can be used as *homo economicus* when given endowments, information,
    preferences, etc., to gain new economic insights by simulating its interaction
    with other LLMs. Park et al. ([2022](#bib.bib84), [2023](#bib.bib83)) proposed
    scalable simulators that can generate realistic social behaviors emerging in populated
    and interactive social systems, and the emerging behaviors of LLM agents in society
    have also been consistently observed in Chen et al. ([2024](#bib.bib28), [2023](#bib.bib29)).
    Li et al. ([2023d](#bib.bib63), [a](#bib.bib60)) studied the opinion/behavioral
    dynamics of LLM agents on social networks. These empirical results have inspired
    our work, which can be viewed as an initial attempt towards quantitatively understanding
    the *emerging behavior* of LLMs as computational human models, given the well-known
    justification of *equilibrium* being a long-run emerging behavior of *learning
    dynamics* (Fudenberg and Levine, [1998](#bib.bib42)) and strategic interactions
    (Young, [2004](#bib.bib119); Camerer, [2011](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers & In-context-learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs nowadays are predominantly built upon the architecture of Transformers
    (Vaswani et al., [2017](#bib.bib102)). Transformers have exhibited a remarkable
    capacity of *in-context-learning* (ICL), which can construct new predictors from
    sequences of labeled examples as input, without further parameter updates. This
    has enabled the *few-shot learning* capability of Transformers (Brown et al.,
    [2020](#bib.bib21); Garg et al., [2022](#bib.bib44); Min et al., [2022](#bib.bib78)).
    The empirical successes have inspired burgeoning theoretical studies on ICL. Xie
    et al. ([2022](#bib.bib113)) used a Bayesian inference framework to explain how
    ICL works, which has also been adopted in Wang et al. ([2023b](#bib.bib106));
    Jiang ([2023](#bib.bib53)). Akyürek et al. ([2023](#bib.bib9)); Von Oswald et al.
    ([2023](#bib.bib103)); Dai et al. ([2023](#bib.bib30)); Giannou et al. ([2023](#bib.bib45))
    showed (among other results) that ICL comes from the fact that Transformers can
    implement the gradient descent (GD) algorithm. Bai et al. ([2023](#bib.bib13))
    further established that Transformers can implement a broad class of machine learning
    algorithms in context. Moreover, Ahn et al. ([2023](#bib.bib5)); Zhang et al.
    ([2023a](#bib.bib121)); Mahankali et al. ([2023](#bib.bib74)) proved that a *minimizer*
    of the certain training loss among single-layer Transformers is equivalent to
    a single step of GD for linear regression. Li et al. ([2023e](#bib.bib64)) established
    generalization bounds of ICL from a multi-task learning perspective. Zhang et al.
    ([2023b](#bib.bib122)) argued that ICL implicitly implements Bayesian model averaging,
    and can be approximated by the attention mechanism. They also established a result
    on some *regret* metric. However, the regret notion is not defined for (online)
    decision-making, and is fundamentally different from ours that is standard in
    online learning and games. Also, we provide extensive experiments to validate
    the no-regret behavior by our definition. More recently, the ICL property has
    also been generalized to decision-making settings. Laskin et al. ([2023](#bib.bib57));
    Lee et al. ([2023](#bib.bib59)); Lin et al. ([2024](#bib.bib67)) investigated
    the in-context reinforcement learning (RL) property of Transformers under supervised
    pre-training, for solving stochastic bandits and Markov decision processes. In
    contrast, our work focuses on online learning settings with an arbitrary and *potentially
    adversarial* nature, as well as *game-theoretic* settings. We also provide a new
    *unsupervised* loss to promote the no-regret behavior in our settings.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning and games.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Online learning has been extensively studied to model the decision-making of
    an agent who interacts with the environment sequentially, with a potentially arbitrary
    sequence of loss functions (Shalev-Shwartz, [2012](#bib.bib92); Hazan, [2016](#bib.bib47)),
    and has a deep connection to game theory (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)).
    In particular, regret, the difference between the incurred accumulated loss and
    the best-in-hindsight accumulated loss, has been the core performance metric,
    and a good online learning algorithm should have regret at most sublinear in time
    $T$), which is referred to as being *no-regret*. Many well-known algorithms can
    achieve no-regret against *arbitrary* loss sequences, e.g., multiplicative weight
    updates (MWU)/Hedge (Freund and Schapire, [1997](#bib.bib39); Arora et al., [2012](#bib.bib11)),
    EXP3 (Auer et al., [2002](#bib.bib12)), and more generally follow-the-regularized-leader
    (FTRL) (Shalev-Shwartz and Singer, [2007](#bib.bib93)) and follow-the-perturbed-leader
    (FTPL) (Kalai and Vempala, [2005](#bib.bib54)). In the bandit literature (Lattimore
    and Szepesvári, [2020](#bib.bib58); Bubeck et al., [2012](#bib.bib22)), such a
    setting without any statistical assumptions on the losses is also referred to
    as the *adversarial/non-stochastic* setting. Following the conventions in this
    literature, the online settings we focus on shall not be confused with the stationary
    and *stochastic*(-bandit)/(-reinforcement learning) settings that have been explored
    in several other recent works on *Transformers for decision-making* (Lee et al.,
    [2023](#bib.bib59); Lin et al., [2024](#bib.bib67)). Centering around the regret
    metric, our work has also explored the non-stationary bandit setting (Besbes et al.,
    [2014](#bib.bib17)), as well as the repeated game setting where the environment
    itself consists of strategic agents (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We use $\mathbb{N}$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Online Learning & Games
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Online learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We first consider the online learning setting where an agent interacts with
    the environment for $T$.
  prefs: []
  type: TYPE_NORMAL
- en: At time step $t\in[T]$ simply as *online learning*. Moreover, if the loss functions
    change over time (usually with certain bounded variation), we will refer to it
    as *non-stationary online learning* for short, whose bandit-feedback version is
    also referred to as the *non-stationary bandit* problem.
  prefs: []
  type: TYPE_NORMAL
- en: Repeated games.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The online learning setting above has an intimate connection to game theory.
    Consider a normal-form game $\mathcal{G}=\langle N,\{\mathcal{A}_{n}\}_{n\in[N]},\{r_{n}\}_{n\in[N]}\rangle$.
    We will refer to it as the *game setting* for short, and use the terms of “agent”
    and “player” interchangeably hereafter. The key difference between online learning
    and repeated games is in their interaction dynamics: online learning involves
    an agent facing a potentially adversarial, changing environment (or sequence of
    loss functions), while in repeated games, agents interact by playing the same
    game repeatedly, which might be less adversarial when they follow specific learning
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 Performance Metric: Regret'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now introduce *regret*, the core performance metric used in online learning
    and games. For a given algorithm $\mathscr{A}$ and that incurred by the best-in-hindsight
    fixed decision, can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: In the Experts Problem, the definition can be instantiated as $\text{Regret}_{\mathscr{A}}(({\ell}_{t})_{t\in[T]}):=\sum_{t=1}^{T}\langle{\ell}_{t},\pi_{\mathscr{A},t}\rangle-\inf_{\pi\in\Pi}\sum_{t=1}^{T}\langle{\ell}_{t},\pi\rangle$.
    Widely-known no-regret algorithms include follow-the-regularized-leader (FTRL)
    (Shalev-Shwartz and Singer, [2007](#bib.bib93)), follow-the-perturbed-leader (Kalai
    and Vempala, [2005](#bib.bib54)) (See [Section A.3](#A1.SS3 "A.3 Online Learning
    Algorithms ‣ Appendix A Deferred Background ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") for a detailed introduction).
  prefs: []
  type: TYPE_NORMAL
- en: In non-stationary online learning, one also uses the metric of *dynamic regret*
    (Zinkevich, [2003](#bib.bib125)), where the *comparator* in the definition also
    changes over time, as the best decision policy at each individual time $t$.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Do Pre-Trained LLMs Have Regret? Experimental Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore the no-regret behaviors of representative pre-trained
    LLMs (i.e., GPT-4 Turbo, GPT-4, and GPT-3.5 Turbo), in the context of online learning
    and games. All experiments with LLMs are conducted using the public OpenAI Python
    API (Openai, [2023](#bib.bib81)).
  prefs: []
  type: TYPE_NORMAL
- en: Intuition why pre-trained language models may exhibit no-regret behavior.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Transformer-based LLMs have demonstrated impressive *in-context-learning* and
    few-/zero-shot learning capabilities (Brown et al., [2020](#bib.bib21); Garg et al.,
    [2022](#bib.bib44); Min et al., [2022](#bib.bib78)). One theoretical explanation
    is that, trained Transformers can implement the *gradient descent algorithm* on
    the testing loss in certain supervised learning problems (Akyürek et al., [2023](#bib.bib9);
    Von Oswald et al., [2023](#bib.bib103); Dai et al., [2023](#bib.bib30); Ahn et al.,
    [2023](#bib.bib5); Zhang et al., [2023a](#bib.bib121); Mahankali et al., [2023](#bib.bib74)),
    which is inherently *adaptive* to the loss function used at test time. On the
    other hand, it is known in online learning that the simple algorithm of *online
    gradient descent* (Zinkevich, [2003](#bib.bib125)) can achieve no-regret. Hence,
    it seems reasonable to envision the no-regret behavior of such meta-learners in
    online learning, due to their fast adaptability. However, it is not straightforward
    due to the fundamental difference between multi-task/meta-learning and online
    learning settings, as well as the difference between *stationary* and *non-stationary/adversarial*
    environments in decision-making. Next, we provide both experimental and theoretical
    studies to validate this intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction protocol.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To enable the sequential interactions with LLMs, we first describe the setup
    and objective of our experimental study. At each round, we incorporate the entire
    history of loss vectors of past interactions into our prompts, as concatenated
    texts, and ask the LLM agent to determine a policy that guides the decision-making
    for the next round. Note that since we hope to *evaluate* the intelligence level
    of pre-trained LLMs through online learning or games, we only provide simple prompts
    that she should utilize the history information, without providing explicit rules
    of *how* to make use of the history information, nor asking her to *minimize regret*
    (in any sense). A detailed description and an ablation study of the prompts are
    deferred to [Section B.1](#A2.SS1 "B.1 Ablation Study on Prompts ‣ Appendix B
    Deferred Results and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"), and an illustration of the protocol in playing
    repeated games is given in [Figure 3](#S3.F3 "In Extension to bandit-feedback
    settings. ‣ 3.2 Results: Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Framework for No-Regret Behavior Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before delving into the results, we note that to the best of our knowledge,
    we are not aware of any principled framework for validating no-regret behaviors
    with finite-time experimental data. Therefore, we propose two frameworks to rigorously
    validate the no-regret behavior of algorithms over a *finite* $T$, which might
    be of independent interest.
  prefs: []
  type: TYPE_NORMAL
- en: Trend-checking framework.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We propose the following hypothesis test:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H_{0}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H_{1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'with $H_{0}$ by definition, making it challenging to verify directly. As an
    alternative, we propose a more tractable hypothesis test, albeit a weaker one,
    that still captures the essence of our objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H_{0}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle H_{1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Ideally, one should check if $\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)/t$,
    as the output of this framework.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*($p$-value of the null hypothesis).* Define the event'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\mathcal{E}(s,T):=\left\{\text{The~{}number~{}of~{}}\frac{\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)}{t}-\frac{\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t+1]}\right)}{t+1}></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: Under the assumption that the null hypothesis $H_{0}$ holds, the probability
    of this event happening is bounded as <math id="Thmproposition1.p1.3.2.m2.4" class="ltx_Math"
    alttext="\mathbb{P}_{H_{0}}(\mathcal{E}(s,T))\leq\frac{1}{2^{T-1}}\sum_{t=s}^{T-1}\begin{pmatrix}T-1\\
  prefs: []
  type: TYPE_NORMAL
- en: t\end{pmatrix}$$.
  prefs: []
  type: TYPE_NORMAL
- en: Regression-based framework.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In complement to the statistical framework above, we propose an alternative
    approach by fitting the data with regression. In particular, one can use the data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\{\left(t,\log\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)\right)\right\}_{t\in[T]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: to fit a linear function $g(t)=\beta_{0}\log t+\beta_{1}$ as the output of this
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Results: Online Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now present the experimental results on the no-regret behavior of pre-trained
    LLMs in online learning in: 1) arbitrarily changing environments, 2) non-stationary
    environments, and 3) bandit-feedback environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Online learning in arbitrarily changing environment.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We first consider the setting with arbitrarily changing environments, which
    are instantiated as follows: 1) *Randomly-generated loss sequences*. At every
    timestep, we generate a random loss vector ${\ell}_{t}\sim\operatorname{Unif}([0,10]^{d})$.
    The average regret (over multiple randomly generated instances) performance is
    presented in [Figure 1](#S3.F1 "In Extension to bandit-feedback settings. ‣ 3.2
    Results: Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"), where
    we compare GPT-4 with well-known no-regret algorithms, FTRL with entropy regularization
    and FTPL with Gaussian perturbations (with tuned parameters). It is seen that
    these pre-trained LLMs can indeed achieve no-regret and often have smaller regrets
    than these baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: Online learning (in non-stationary environment).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We then experiment on the setting where the losses are still changing over
    time, but their total variations across time are bounded, more concretely, sublinear
    in $T$. For both cases, the average dynamic regret results are presented in [Table 1](#S3.T1
    "In Randomly generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do
    Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"). It can be seen that GPT-4 achieves
    sublinear dynamic regret and outperforms *FTRL/FTPL with Restart*, a standard
    variant of FTRL/FTPL for non-stationary online learning (see e.g., Besbes et al.
    ([2014](#bib.bib17))). We refer to [Section A.3](#A1.SS3.SSS0.Px5 "Relationship
    between FTRL and FTPL. ‣ A.3 Online Learning Algorithms ‣ Appendix A Deferred
    Background ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    for a more detailed introduction of FTRL/FTPL with Restart.'
  prefs: []
  type: TYPE_NORMAL
- en: Extension to bandit-feedback settings.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Although pre-trained LLMs have achieved good performance in online learning
    with full-information feedback, it is unclear whether they can still maintain
    no-regret with only bandit feedback. For such problems, we modify the prompt and
    protocol of interactions slightly, where we still ask the LLM agent to provide
    a policy $\pi_{t}$. Note that such an operation of *re-weighting* the loss by
    the inverse of the probability is standard in online learning when adapting full-information-feedback
    no-regret algorithms to the bandit-feedback ones (Auer et al., [2002](#bib.bib12)).
    Later, we will also show the benefits of such operations (c.f. [Section 4](#S4
    "4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")).
    We compare the performance of pre-trained LLMs with that of the counterparts of
    FTRL with bandit feedback, e.g., EXP3 (Auer et al., [2002](#bib.bib12)) and the
    bandit-version of FTPL (Abernethy et al., [2015](#bib.bib2)), in both [Figure 2](#S3.F2
    "In Extension to bandit-feedback settings. ‣ 3.2 Results: Online Learning ‣ 3
    Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games") and [Table 1](#S3.T1 "In Randomly
    generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), where GPT-4 consistently achieves lower
    regret.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb6bd4348e616759187ad5e227469066.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Regret of GPT-3.5 Turbo/GPT-4 for online learning with full-information
    feedback in $4$ different settings. It performs comparably and sometimes even
    better than well-known no-regret learning algorithms, FTRL and FTPL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f67b3a4da4ffa21b3be8cc6ba34ac93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Regret of GPT-3.5 Turbo/GPT-4 for online learning with bandit feedback
    in $4$ different settings. It performs comparably and sometimes even better than
    well-known no-regret learning algorithms, variants of FTRL and FTPL with bandit-feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fbe94a6aa4474d0d75f5480c98e019d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Demonstration of the prompts and interaction protocol for multi-player
    repeated games. A human moderator does not provide the game’s payoff matrices
    to the LLMs. Instead, at each round, the human moderator provides each player’s
    own payoff vector history.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.3 Results: Multi-Player Repeated Games'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now consider the setting when multiple LLMs make online strategic decisions
    in a *shared* environment repeatedly. Specifically, at each round, the loss vectors
    each agent receives are determined by both her payoff matrix and the strategies
    of all other agents. Note that the payoff matrix is not directly revealed to the
    LLM agent, but she has to make decisions in a completely online fashion based
    on the payoff vector marginalized by the opponents’ strategies (See [Figure 3](#S3.F3
    "In Extension to bandit-feedback settings. ‣ 3.2 Results: Online Learning ‣ 3
    Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games") for a prompt example). This
    is a typical scenario in learning in (repeated) games (Fudenberg and Levine, [1998](#bib.bib42)).'
  prefs: []
  type: TYPE_NORMAL
- en: Representative games.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We first test the repeated play of pre-trained LLMs on $6$ repeated experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Randomly generated games.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To further validate the no-regret behavior of LLMs, we also test on $50$. These
    are larger and more challenging settings than the structured and representative
    ones above.
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize the experimental results in [Figure 4](#S3.F4 "In Randomly generated
    games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"), which are similar to the above in the online setting: for
    all types of games, GPT-4 achieves sublinear regret, which is comparable with
    that obtained by FTRL for most games. We provide six instances of three-player
    general-sum games and six instances of four-player general-sum games in [Figure 5](#S3.F5
    "In Randomly generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do
    Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") and [Figure 6](#S3.F6 "In Randomly
    generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), respectively. Occasionally, GPT-4 even provides
    a negative regret value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/503d788709ad0a33d9e7b5bbd7d8c7ba.png)![Refer to caption](img/7fe622193595a3cb39d03e2fb495c671.png)![Refer
    to caption](img/279a6ed6c18583d439804d8128a0ce7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Regret of GPT-3.5 Turbo/GPT-4 for repeated games of $3$ different
    game sizes, where the sublinear regret is validated by both of our statistical
    frameworks. Due to the symmetry of agents in randomly generated games, we report
    the regret of one agent for ease of presentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/204e334e5f250558a74f9e4006367580.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Regret of GPT-4 and the FTRL algorithm in $6$ randomly generated
    three-player general-sum games. GPT-4 has comparable (even better) no-regret properties
    when compared with the FTRL algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/82e65b051069a02a1a6b1cbb217ad709.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Regret of GPT-4 and the FTRL algorithm in $6$ randomly generated
    four-player general-sum games. GPT-4 has comparable (even better) no-regret properties
    when compared with the FTRL algorithm, according to the frameworks in [Section 3.1](#S3.SS1
    "3.1 Framework for No-Regret Behavior Validation ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games") and the graphic trends.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed9829d5302ea99d57054d907a5c2d57.png)![Refer to caption](img/bb227a0d48dd1b0a5ad590712c1a08f8.png)![Refer
    to caption](img/595166b496eaf9b02490eb801e8f5055.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: (left) Regret of both GPT-4 and GPT-4 Turbo under the canonical counterexample
    for FTL (Hazan, [2016](#bib.bib47), Chapter 5). (mid, right) Failure of GPT-4
    and GPT-4 Turbo on two scenarios for regrettable behavior of GPT, while Transformers
    with regret-loss provide no-regret behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dynamic regret | GPT-4 | GPT-3.5 Turbo | FTRL | FTPL |'
  prefs: []
  type: TYPE_TB
- en: '| Full information | Gradual variation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $12.61\pm 7.01$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.0,0.58,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $19.09\pm 11.33$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.0,0.83,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $36.58\pm 24.51$ |'
  prefs: []
  type: TYPE_TB
- en: '| Abrupt variation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $30.0\pm 19.91$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.01,0.87,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $33.65\pm 22.51$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.08,0.96,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $36.52\pm 27.68$ |'
  prefs: []
  type: TYPE_TB
- en: '| Bandit | Gradual variation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $21.39\pm 10.86$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.0,0.78,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $28.42\pm 21.6$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.0,0.83,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $37.64\pm 21.97$ |'
  prefs: []
  type: TYPE_TB
- en: '| Abrupt variation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $35.94\pm 28.93$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.42,0.95,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $30.76\pm 25.48$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $(p_{trend},\widehat{\beta}_{0},p_{reg})=(0.92,1.01,0.0)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $36.52\pm 27.68$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Dynamic regret of GPT-3.5 Turbo/GPT-4 in a non-stationary environment
    with either full-information or bandit feedback. Every experiment is conducted
    with 25 rounds. No-regret behaviors of GPT-3.5 Turbo/GPT-4 are validated by both
    of our frameworks (low $p$). The only exception is GPT-3.5 Turbo on loss sequence
    with abrupt variations under bandit feedback. This indicates that GPT-3.5 Turbo
    may not be capable of dealing with an abruptly changing environment with limited
    feedback, although the average regret achieved eventually is still lower than
    that of other baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Pre-Trained LLM Agents May Still Have Regret
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It seems tempting to conclude that pre-trained LLMs can achieve no-regret in
    both online learning and playing repeated games. However, is this capability *universal*?
    We show that the no-regret property might break for LLM agents if the loss vectors
    are generated in a more adversarial way.
  prefs: []
  type: TYPE_NORMAL
- en: Canonical counterexample for follow-the-leader.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To begin with, we consider a well-known example that *follow-the-leader* (FTL)
    algorithm (Shalev-Shwartz, [2012](#bib.bib92)) suffers from linear regret (Hazan,
    [2016](#bib.bib47), Chapter 5), where $\ell_{1}(1)=5,\ell_{1}(2)=0$ is the modulo
    operation. Interestingly, GPT-4 agent can easily identify the pattern for the
    loss sequence that the optimal action *alternates*, thus accurately predicting
    the loss it will receive and achieving nearly zero regret in [Figure 7](#S3.F7
    "In Randomly generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do
    Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"). In other words, GPT-4 agent seems
    to not fail in the same way as FTL, which is known to be due to the lack of randomness
    in prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Less predictable loss sequence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Inspired by the counterexample above, we design a new loss sequence that is
    *similar but less predictable*. Specifically, we construct the following (simple)
    loss sequence with $2$.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive loss sequence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We also develop a simpler but more *adaptive* loss sequence that takes the
    full power of the adversary in the online learning setup. After the GPT-4 agent
    provides $\pi_{t}$ repeated experiments for the later two settings using GPT-4
    and more advanced GPT-4 Turbo in [Figure 7](#S3.F7 "In Randomly generated games.
    ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), where we cannot reject the hypothesis that GPT-4 (Turbo) has linear-regret
    by either our trend-checking or regression-based framework.'
  prefs: []
  type: TYPE_NORMAL
- en: These observations have thus motivated us to design new approaches to better
    promote the no-regret property of LLM agents, with additional training, as to
    be detailed in [Section 5](#S5 "5 Provably Promoting No-Regret Behavior by an
    Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"). Before delving into the design of such a *stronger* LLM agent, we
    first provide some theoretical insights into why pre-trained LLMs have already
    exhibited good no-regret behaviors oftentimes.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now provide some plausible explanations about the no-regret behavior of
    pre-trained LLMs, as observed in [Sections 3.2](#S3.SS2 "3.2 Results: Online Learning
    ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games") and [3.3](#S3.SS3 "3.3 Results:
    Multi-Player Repeated Games ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental
    Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    Note that our explanations have to be *hypothetical* by nature, since to the best
    of our knowledge, the details of pre-training these popular LLMs (e.g., GPT-3.5
    Turbo and GPT-4), regarding data distribution, training algorithm, etc., have
    not been revealed. We instead make the explanations based on some common assumptions
    and arguments in the literature for modeling human behaviors, and the recent literature
    on understanding LLMs/Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Pre-Trained LLMs Have Similar Regret as Humans (Who Generate Data)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first provide a direct observation based on some existing speculation on
    the capability of Transformer-based LLMs. Recently, a growing literature has evidenced
    that the intelligence level of LLM agents are determined by, and in fact mimic,
    those of human beings who generate the data for pre-training the models (Park
    et al., [2022](#bib.bib84); Argyle et al., [2023](#bib.bib10); Horton, [2023](#bib.bib50)).
    The key rationale was that, LLMs (with Transformer parameterization) can approximate
    the *pre-training data distribution* very well (Xie et al., [2022](#bib.bib113);
    Zhang et al., [2023b](#bib.bib122); Lee et al., [2023](#bib.bib59)). In such a
    context, one can expect that LLM agents can achieve similar regret as human decision-makers
    who generate the pre-training data, as we formally state below.
  prefs: []
  type: TYPE_NORMAL
- en: Observation 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An LLM agent is said to be pre-trained with an *$\epsilon$, the following condition
    holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sup_{\pi\in\Pi}\quad\big{&#124;}P_{\text{data}}(\pi{\,&#124;\,}({\ell}_{i})_{i\in[t]})-P_{\text{LLM}}(\pi{\,&#124;\,}({\ell}_{i})_{i\in[t]})\big{&#124;}\leq\epsilon,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $P_{\text{data}}$-decision error is bounded as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $[a\pm b]:=[a-b,a+b]$.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#Thmobservation1 "Observation 1\. ‣ 4.1 Pre-Trained LLMs Have Similar Regret
    as Humans (Who Generate Data) ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games") shows that the pre-trained LLM-agent’s regret can
    be controlled by that of the pre-training dataset and the decision error $\epsilon$
    can be achieved if LLM is constructed by a rich function class, e.g., the Transformer
    architecture (Zhang et al., [2023b](#bib.bib122); Lin et al., [2024](#bib.bib67)).
    We defer a proof of the result to [Appendix C](#A3 "Appendix C Deferred Results
    and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the above observation does not provide the full picture – the $\text{(D-)Regret}_{\text{data}}\left(({\ell}_{t})_{t\in[T]}\right)$.
    We discuss next under what (natural) models and pre-training processes, the no-regret
    behavior of pre-trained LLMs may emerge.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 A Human Decision-Making Model: Quantal Response'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A seminal model for human decision-making behavior is the *quantal response*
    model, which assumes that humans are often *imperfect* decision-makers, and their
    bounded rationality can be modeled through unseen latent variables that influence
    the decision-making process (McFadden, [1976](#bib.bib76); McKelvey and Palfrey,
    [1995](#bib.bib77)). Formally, the quantal response is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1  (Quantal response).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a loss vector $\ell\in\mathbb{R}^{d}$, the quantal response is defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: In essence, this implies that humans are rational but with respect to (w.r.t.)
    the latent variable $z$ per se. This addition of noise to the actual loss vector
    characterizes the bounded rationality of humans in decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: The traditional quantal response formulation primarily focused on scenarios
    with a single loss vector. In online decision-making, given the *history* information,
    the human at each time $t$ is faced with *multiple* loss vectors. Hence, we adopt
    the following generalization to model the decision-making behavior in this setting.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2  (Quantal response against multiple losses).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a set of loss vectors $({\ell}_{i})_{i\in[t]}$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For $t=0$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simpler terms, the generalized quantal response is defined as the standard
    quantal response against some scaled summation of the losses. Note that such a
    *dynamic* version of quantal response also has implications from behavior economics,
    and has been recently used to model human behaviors in sequential decision-making
    (Ding et al., [2022](#bib.bib33)) (in stochastic and stationary environments).
    Indeed, such a response against multiple loss vectors is believed to be natural,
    and has also been widely adopted in well-known no-regret learning algorithms of
    *smooth/stochastic fictitious play* (Fudenberg and Kreps, [1993](#bib.bib41))
    and *follow-the-perturbed-leader* (Kalai and Vempala, [2005](#bib.bib54)), whose
    formal definitions can be found in [Section A.3](#A1.SS3 "A.3 Online Learning
    Algorithms ‣ Appendix A Deferred Background ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"). Finally, note that the response model in
    [Definition 2](#Thmdefinition2 "Definition 2 (Quantal response against multiple
    losses). ‣ 4.2 A Human Decision-Making Model: Quantal Response ‣ 4 Why Are Pre-Trained
    LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games") does not necessarily
    involve a *sequential* decision-making process, i.e., the set of losses may not
    come from the history of an online learning process. With this (natural) human
    response model in hand, we hypothesize that powerful pre-trained LLMs also behave
    similarly as such a model. To illustrate how [Definition 2](#Thmdefinition2 "Definition
    2 (Quantal response against multiple losses). ‣ 4.2 A Human Decision-Making Model:
    Quantal Response ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model
    and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games") can potentially capture also the behavior of LLMs instead
    of only humans, we have also provided examples of LLMs *explaining* how they output
    their policies in [Section B.4](#A2.SS4 "B.4 LLM Agents’ Explanation on Their
    Output Policies ‣ Appendix B Deferred Results and Proofs in Section 3 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"). It can be observed
    that the LLM agent tends to use the history of the reward vectors by looking at
    the “sum/average of the accumulated rewards”, and tends to introduce “randomization”
    in decision-making, in order to be “less predictable and exploitable”. These are
    known to be key to achieving no-regret behaviors in online learning and games
    (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)). Hence, we analyze a case where
    pre-training under certain canonical data assumptions can provably lead to the
    quantal response behaviors and further yield no-regret guarantees, to gain some
    insights into the empirical observations in [Sections 3.2](#S3.SS2 "3.2 Results:
    Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [3.3](#S3.SS3
    "3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Case Study: Pre-Training under Canonical Data Distribution'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-training of LLMs often involves the approach of *next-token prediction*.
    When applying LLMs to sequential decision-making, the model receives the context
    of the decision-making sequence and then generates the *action*. This process
    can be conceptualized as *predicting the optimal action* in the form of the next
    token prediction. For instance, Yao et al. ([2023b](#bib.bib118)); Shinn et al.
    ([2023](#bib.bib95)); Liu et al. ([2023a](#bib.bib69), [c](#bib.bib71)) demonstrated
    how decision-making may be framed in this way. Concretely, LLM receives the context
    as $(x_{1},x_{2},\cdots,x_{N_{1}})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, in these cases, large models such as Transformers are often (pre-)trained
    for (sequential) decision-making under several *fixed/stationary* environments
    (Laskin et al., [2023](#bib.bib57); Lin et al., [2024](#bib.bib67); Lee et al.,
    [2023](#bib.bib59); Reed et al., [2022](#bib.bib86)), which may limit their ability
    to handle *arbitrary/non-stationary/adversarial* loss sequences in our online
    learning setup. Thus, it is natural to ask: *Is it possible to have no-regret
    behaviors emerging as a consequence of this (optimal) action prediction, under
    only a fixed pre-training distribution of stationary environments?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we analyze a standard pre-training objective on a token sequence distribution
    $x_{1:N_{t+1}}\sim P^{text}_{t}$, which is the expected log-likelihood maximization
    for next-token prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where we define $\text{LLM}_{\theta}\left(x_{1}{\,|\,}x_{1:0}\right)=\text{LLM}_{\theta}\left(x_{1}\right)$
    denotes the parameter space of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the pre-training data distribution, intuitively, we model it as follows:
    there exists a latent variable $z$ is available in the pre-training dataset as
    the *label*, which could come from user surveys, personal blogs, or explicit data
    annotation. We formalize such a scenario in the assumption below:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given $T\in\mathbb{N}^{+}$ and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where we assume tokens $x_{1:N_{t}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further understand our assumption, we provide the following example and
    lemma, which recovers the prominent human behavior model introduced in [Section 4.2](#S4.SS2
    "4.2 A Human Decision-Making Model: Quantal Response ‣ 4 Why Are Pre-Trained LLMs
    (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games"), quantal response, under
    some canonical data distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1  (An example instantiating [1](#Thmassumption1 "Assumption 1\. ‣
    4.3 Case Study: Pre-Training under Canonical Data Distribution ‣ 4 Why Are Pre-Trained
    LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games")).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We consider a common decision-making task that may generate the training data,
    *recommender systems*. An instance of the text data could be: “On September 29,
    2023, user X clicked movie A three times, movie B eight times, and movie C five
    times”. This sentence corresponds to $x_{N_{i-1}+1:N_{i}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*(Alignment of [1](#Thmassumption1 "Assumption 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") with quantal response).* Fix $t\in[T]$.
    If we model the noise of data collection to be i.i.d. Gaussian distribution in
    the numeric value space, i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: the prior distribution of the latent variable $z$, then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: with $P_{noise}=\mathcal{N}(\boldsymbol{0}_{d},I)$.
  prefs: []
  type: TYPE_NORMAL
- en: We defer the proof of the lemma to [Section C.2](#A3.SS2 "C.2 Deferred Proof
    of Lemma 1 ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games"). Now based on this lemma,
    we provide the no-regret guarantees of the corresponding pre-trained LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*(Emergence of no-regret behavior).* Under the assumptions of Lemma [1](#Thmlemma1
    "Lemma 1\. ‣ 4.3 Case Study: Pre-Training under Canonical Data Distribution ‣
    4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"),
    suppose the function class of $\text{LLM}_{\theta}$:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) For online learning with full-information feedback, $\text{Regret}_{\text{LLM}{{}_{\theta^{\star}}}}\left(({\ell}_{t})_{t\in[T]}\right)\leq\mathcal{O}\left(\sqrt{T\log
    d}\right)$;
  prefs: []
  type: TYPE_NORMAL
- en: (2) For non-stationary online learning with full-information feedback,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}\left(({\ell}_{t})_{t\in[T]}\right)\leq\mathcal{O}\left((\log
    d\ V_{T})^{1/3}T^{2/3}\right);$ |  |'
  prefs: []
  type: TYPE_TB
- en: (3) For online learning with bandit feedback,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}\left[\text{Regret}_{\text{LLM}{{}_{\theta^{\star}}}}\left(({\ell}_{t})_{t\in[T]}\right)\right]\leq\mathcal{O}\left((\log
    d)^{1/2}dT^{1/2+1/\log T}\log T\right);$ |  |'
  prefs: []
  type: TYPE_TB
- en: (4) For non-stationary online learning with bandit feedback,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}\left[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}\left(({\ell}_{t})_{t\in[T]}\right)\right]\leq\mathcal{O}\left((T^{2}d^{2}V_{T})^{1/3}(\log
    d)^{1/2}T^{1/\log T}\log T\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Remark 1  (Implication for playing repeated games).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'First, we note that the no-regret guarantee in the online setting is stronger
    than and thus implies that in the game setting, since regret by definition handles
    arbitrary/adversarial environments, while in playing games the opponents are not
    necessarily as adversarial. Second, it is folklore that if all players in the
    repeated game follow no-regret learning algorithms, then the time-average policies
    of all players during learning constitute an approximate *coarse correlated equilibrium*
    of the game (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)). Hence, the results
    (1) and (2) in [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") imply that a coarse correlated equilibrium
    will emerge in the long run from the interactions of the LLM agents (under certain
    assumptions as in the theorem).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We present proofs of the non-asymptotic bounds for (dynamic) regret in [Section C.4](#A3.SS4
    "C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs in Section
    4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"). Furthermore,
    we demonstrate that the prior distribution of $z$ could also be replaced by a
    general distribution (c.f. [Theorem 5](#Thmtheorem5 "Theorem 5\. ‣ C.5.1 Relaxation
    under More General Data Distributions ‣ C.5 Extending Theorem 1 with Relaxed Assumptions
    ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")), in order to obtain the above results.
    We also point out that [1](#Thmassumption1 "Assumption 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") may be further relaxed to better match
    the actual LLMs’ pre-training data distributions from diverse sources (c.f. [Section C.5](#A3.SS5
    "C.5 Extending Theorem 1 with Relaxed Assumptions ‣ Appendix C Deferred Results
    and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")).'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to observe that even when pre-training is conducted solely with
    *stationary* loss vector generation ($\ell_{1:t}$), it can still lead to the emergence
    of no-regret behavior in online learning with potentially adversarial losses.
    Key in the proof is a connection of pre-trained LLM models to the online learning
    algorithm of FTPL.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison to Lee et al. ([2023](#bib.bib59)); Lin et al. ([2024](#bib.bib67)).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Intriguingly, similar assumptions and pre-training objectives have also been
    considered in the very recent work of Lee et al. ([2023](#bib.bib59)); Lin et al.
    ([2024](#bib.bib67)) for studying in-context reinforcement learning property of
    Transformers under supervised pre-training. Lee et al. ([2023](#bib.bib59)) established
    its equivalence to *posterior sampling* (Osband et al., [2013](#bib.bib82)), an
    important RL algorithm with provable regret guarantees when the environments are
    *stationary*, and Lin et al. ([2024](#bib.bib67)) generalized the study to the
    setting of algorithm distillation as in Laskin et al. ([2023](#bib.bib57)). However,
    their results cannot directly imply the no-regret guarantee in our online learning
    setting, due to the fact that posterior sampling can perform poorly under potentially
    *adversarial* or *non-stationary* environments (Zimmert and Seldin, [2021](#bib.bib124);
    Liu et al., [2023b](#bib.bib70)). In contrast, we here establish the equivalence
    of the pre-trained LLM to the FTPL algorithm (under different pre-training distribution
    specifications), with the ability to handle arbitrary loss sequences, even though
    the LLMs are only trained on a fixed distribution of texts (tasks).
  prefs: []
  type: TYPE_NORMAL
- en: Calibrating the degree of bounded rationality of actual LLMs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To further validate our model and data distribution assumptions, we also propose
    to *calibrate* the parameter $\{\eta_{t}\}_{t\in[T-1]}$ by solving the following
    problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\sigma^{\star}\in\arg\min_{\sigma></math> |  |'
  prefs: []
  type: TYPE_TB
- en: 'We solve this single-variable optimization problem by grid search over $[0,10]$
    on another *unseen test set*, and compare it with the behavior of the actual LLM
    agents. We use all the interaction data from [Section 3.2](#S3.SS2 "3.2 Results:
    Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and split
    it in half for training and testing. In [Figure 8](#S4.F8 "In Calibrating the
    degree of bounded rationality of actual LLMs. ‣ 4.3 Case Study: Pre-Training under
    Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"), we show the averaged regret for the LLM agent
    and the calibrated generalized quantal response. It can be seen that calibrated
    generalized quantal response can *very well capture* the behavior of the LLM agent
    for all problem instances in [Section 3.2](#S3.SS2 "3.2 Results: Online Learning
    ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games"), justifying the applicability
    of our hypothetical model and assumptions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a9abb330635fc7f8359929a435a16d21.png)![Refer to caption](img/ee86508d79e597a018a2383e392ff74f.png)![Refer
    to caption](img/f05a16da0d09608231af19738a44c453.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: (left) Comparison of GPT-4 with a calibrated agent on the test set,
    where the calibrated quantal response can perfectly capture the behavior of the
    GPT-4 agent. (mid, right) The calibrated agent on the less predictable and adaptive
    loss sequences failed to make accurate predictions for the GPT-4 anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also use the same framework to understand the regrettable behaviors in [Section 3.4](#S3.SS4
    "3.4 Pre-Trained LLM Agents May Still Have Regret ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"). This analysis uses all the data from [Section 3.4](#S3.SS4
    "3.4 Pre-Trained LLM Agents May Still Have Regret ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"). We first find that such fitting procedures do not yield
    good predictions for LLMs on those counter-examples. Therefore, we resort to a
    more expressive model by directly fitting each $\eta_{t}$. Even under the expressive
    model, LLMs fail to follow the generalized quantal response for the counter-examples
    with less predictable or adaptive loss sequences, as [Figure 8](#S4.F8 "In Calibrating
    the degree of bounded rationality of actual LLMs. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") shows the gap between GPT-4 (dynamic)
    regret and the calibrated agent (dynamic) regret.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we acknowledge that for most existing pre-trained LLMs like GPT-4,
    the canonical assumptions above, though may be further relaxed (c.f. [Remark 4](#Thmremark4
    "Remark 4 (Pre-training with relaxed data assumptions). ‣ C.5.2 Relaxation under
    Decision-Irrelevant Pre-Training Data ‣ C.5 Extending Theorem 1 with Relaxed Assumptions
    ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")), may not hold in general. More importantly,
    the *supervision labels*, i.e., the optimal action given $z$, may be sometimes
    imperfect or unavailable during the dataset collection. Hence, it is completely
    possible to observe regrettable behaviors (c.f. LABEL:{sec:yes_regret_example}).
    Motivated by these caveats, we next propose a new training loss that is *unsupervised*,
    and can promote no-regret behavior provably.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In light of the observations in [Section 3](#S3 "3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"), we ask the question:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Is there a way to further enhance the no-regret property of LLM agents,'
  prefs: []
  type: TYPE_NORMAL
- en: hopefully without (optimal) action labels?*
  prefs: []
  type: TYPE_NORMAL
- en: To address this question, we propose to train LLMs with a new *unsupervised
    learning* loss that naturally provides no-regret behaviors. This approach is akin
    to the process of “instruction tuning” (Wei et al., [2021](#bib.bib109)), which
    was shown to have enhanced LLMs’ ability when learning from context, with both
    theoretical (Ahn et al., [2023](#bib.bib5); Mahankali et al., [2023](#bib.bib74);
    Zhang et al., [2023a](#bib.bib121)) and empirical (Lu et al., [2023](#bib.bib73))
    evidence.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1 A New Unsupervised Training Loss: Regret-Loss'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intuitively, our new training loss is designed to enforce the trained LLM to
    minimize the regret under an arbitrary sequence of loss vectors. Specifically,
    we define the training loss as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\mathcal{L}(\theta)}:=\max_{\ell_{1},\dots,\ell_{T}}~{}~{}~{}\text{Regret}_{\text{LLM}_{\theta}}\left((\ell_{t})_{t\in[T]}\right)$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\|\ell_{t}\|_{\infty}\leq B$ in the definition of regret. Therefore,
    we provide a general framework so that we can approximate [Equation 2](#S5.E2
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") by the following surrogate:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $k\in\mathbb{N}^{+}$), in contrast to those in [Section 4](#S4 "4 Why
    Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") to *justify*
    the no-regret property of pre-trained LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In [Section D.2](#A4.SS2 "D.2 Deferred Proof for the Arguments in Section 5.1
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), we prove that under certain regularity
    conditions of $f$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'as well as the uniform convergence of $\mathcal{L}(\theta,k,N)$ may promote
    the trained LLM to have a small regret value. We will hereafter refer to [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") as the regret-loss. Similarly, we can also define
    dynamic-regret-loss, and the results to be presented next can also generalize
    to this case (c.f. [Remark 5](#Thmremark5 "Remark 5 (Dynamic-regret loss). ‣ D.3
    Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") in [Section D.3](#A4.SS3 "D.3 Deferred Proofs of Theorem 2 and Corollary
    1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Guarantees via Regret-Loss Minimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first establish a *statistical* guarantee under general parameterizations
    of $\text{LLM}_{\theta}$, including the Transformer-based models as used in GPT-4
    and most existing LLMs (see [Proposition 2](#Thmproposition2 "Proposition 2\.
    ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") for an example with formal statement). This guarantee focuses on their
    *generalization ability* when trained to minimize the empirical regret loss, which
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3  (Empirical loss function).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We define the empirical loss $\widehat{\mathcal{L}}$ samples as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $(\ell_{s,t}^{(j)})_{j\in[N],t\in[T]}$.
  prefs: []
  type: TYPE_NORMAL
- en: We denote $\widehat{\theta}_{k,N,N_{T}}\in\operatorname*{arg\,min}_{\theta\in\Theta}~{}\widehat{\mathcal{L}}(\theta,k,N,N_{T})$,
    and present the generalization guarantee below.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*(Generalization gap).* Suppose $\emph{LLM}_{\theta}$, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\left(\widehat{\theta}_{k,N,N_{T}},k,N\right)$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: for any $N$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through a careful use of Berge’s Maximum Theorem (Berge, [1877](#bib.bib16)),
    we prove that the right-hand side of [Equation 5](#S5.E5 "In Theorem 2\. ‣ 5.2
    Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") does *not* depend on $k$ (c.f. [Section D.2](#A4.SS2 "D.2 Deferred
    Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")), we further obtain the following corollary on the regret guarantee:'
  prefs: []
  type: TYPE_NORMAL
- en: Corollary 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*(Regret).* Suppose^§^§§Note that these conditions on $h,f$). Then, with high
    probably, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Corollary 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*(Emerging behavior: Coarse correlated equilibrium)*. For a sufficiently large
    $N_{T}$, then the time-averaged policy for each agent will constitute an approximate
    coarse correlated equilibrium of the game.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proofs of [Theorem 2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [Corollary 1](#Thmcorollary1
    "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") are deferred to [Section D.3](#A4.SS3 "D.3
    Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), and [Corollary 2](#Thmcorollary2 "Corollary 2\. ‣ 5.2 Guarantees
    via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    follows from the folklore result discussed in [Remark 1](#Thmremark1 "Remark 1
    (Implication for playing repeated games). ‣ 4.3 Case Study: Pre-Training under
    Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"). Therefore, if additionally, the LLM parameterization
    (i.e., Transformers) can realize a no-regret algorithm (for example, the single-layer
    self-attention model can construct FTRL, as to be shown next in [Section 5.3](#S5.SS3
    "5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")), then [Corollary 1](#Thmcorollary1
    "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") means that with a large enough number of
    samples $N_{T}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the power and generality of the previous results, one cannot use an
    *infinitely large* $N$ is finite, for the specific parameterization of the LLMs
    using Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now study the setting of minimizing [Equation 3](#S5.E3 "In 5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    when $\text{LLM}_{\theta}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we consider the following structure of single-layer self-attention
    model $g$ (see a formal introduction in [Section A.1](#A1.SS1 "A.1 Additional
    Definitions for Appendix ‣ Appendix A Deferred Background ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{t}=(\ell_{1},\dots,\ell_{t},c)$ is a constant vector. We then have
    the following result.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider the policy space $\Pi=B(0,R_{\Pi},\|\cdot\|)$ and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'is a first-order stationary point of [Equation 3](#S5.E3 "In 5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    with $N=1$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practical training, such stationary points of the loss may be attained by
    first-order optimization algorithms of (stochastic) gradient descent, the workhorse
    in machine learning. Moreover, we also consider the single-layer *linear* self-attention
    model as follows, for which we can strengthen the results above from a stationary-point
    to an *optimal-solution* argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Theorem 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider the policy space $\Pi=B(0,R_{\Pi},\|\cdot\|)$.
  prefs: []
  type: TYPE_NORMAL
- en: '[Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") shows the capacity of self-attention Transformer models to realize
    online learning algorithms, thanks to the regret-loss we proposed. In particular,
    this can be achieved automatically by optimizing the new loss, *without* hard-coding
    the parameters of the Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: The above results are for the case of FTRL with an $L_{2}$ in the Experts Problem.
    We defer the discussion of this case to [Section D.7](#A4.SS7 "D.7 Discussions
    on the Production of FTRL with Entropy Regularization ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"). Through these results, we can also guarantee in the repeated game
    setting that approximate coarse correlated equilibria would emerge in the long
    run, since each player will exhibit no-regret behavior, using a similar argument
    as that for [Corollary 2](#Thmcorollary2 "Corollary 2\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  prefs: []
  type: TYPE_NORMAL
- en: Remark 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The very recent studies by (Ahn et al., [2023](#bib.bib5); Zhang et al., [2023a](#bib.bib121);
    Mahankali et al., [2023](#bib.bib74)) have demonstrated that if $Z_{t}=((x_{1},y_{1}),\dots,(x_{t},y_{t}),(x_{t+1},0))$-labels,
    such an implicit gradient descent update-rule is hard to define. Compared to the
    previous studies, our global optimizer among single-layer linear self-attention
    models is an *explicit* and *online* gradient descent update for online learning.
    With a different loss (regret-loss v.s. instruction-tuning-loss), the techniques
    to obtain the seemingly similar results are also fundamentally different.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Experimental Results for Minimizing Regret-Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now provide experimental results for minimizing our *regret-loss*, and evaluate
    in the following environments: 1) randomly-generated loss sequences ([Figure 9](#S5.F9
    "In Randomly generated loss sequences. ‣ 5.4 Experimental Results for Minimizing
    Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")); 2)
    loss sequences with a predictable trend ([Figure 10](#S5.F10 "In Randomly generated
    loss sequences. ‣ 5.4 Experimental Results for Minimizing Regret-Loss ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")); 3) repeated games ([Figure 11](#S5.F11
    "In Repeated games. ‣ 5.4 Experimental Results for Minimizing Regret-Loss ‣ 5
    Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")); and 4) counterexamples
    for pre-trained LLMs to be regrettable ([Figure 7](#S3.F7 "In Randomly generated
    games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")). Details of the training setup can be found in [Section D.8](#A4.SS8
    "D.8 Training Details of Section 5.4 ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"). We also provide an ablation study for the training of the loss [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") in [Section D.9](#A4.SS9 "D.9 Ablation Study on Training
    Equation 3 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly generated loss sequences.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We use the same loss vectors as those in [Section 3.2](#S3.SS2 "3.2 Results:
    Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") for randomly
    generated loss functions, and compare the results with that using GPT-4\. The
    results show that with regret-loss, both the trained single-layer self-attention
    model and the trained Transformers with multi-layer self-attention structures
    can achieve comparable regrets as FTRL and GPT-4\. The results can be found in
    [Figure 9](#S5.F9 "In Randomly generated loss sequences. ‣ 5.4 Experimental Results
    for Minimizing Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc59f0ef2d19b560267403f2b8f4fd2a.png)![Refer to caption](img/075028bc9021be5f83cdbe26c4eb39ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Regret performance for the randomly generated loss sequences that
    are generated by Gaussian with truncation and uniform distribution. No-regret
    behaviors of single-layer and multi-layer self-attention models are validated
    by both of our frameworks (low $p$).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/177c98600d024577c329cc7d64f19c45.png)![Refer to caption](img/85a731c69237aae5bef9e1444b96efa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Regret performance for the randomly generated loss sequences that
    are generated by linear-trend and sine-trend. No-regret behaviors of single-layer
    and multi-layer self-attention models are validated by both of our frameworks
    (low $p$).'
  prefs: []
  type: TYPE_NORMAL
- en: Loss sequences with a predictable trend.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We investigate the case where the loss sequences have predictable trends such
    as linear-trend or sine-trend. One might expect that the performance of the trained
    Transformer would surpass the performance of traditional no-regret learning algorithms
    such as FTRL, since they may not be an optimal algorithm for the loss sequence
    with a predictable trend. We modify the training distribution by changing the
    distribution of random variable $Z$) to follow two kinds of trends: linear and
    sine functions. The results, as illustrated in [Figure 10](#S5.F10 "In Randomly
    generated loss sequences. ‣ 5.4 Experimental Results for Minimizing Regret-Loss
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games"), show that the trained
    single-layer self-attention model and the trained Transformer with multi-layer
    self-attention structures with regret-loss outperformed GPT-4 and FTRL in terms
    of regret, when the loss sequence is a linear trend. Similarly, [Figure 10](#S5.F10
    "In Randomly generated loss sequences. ‣ 5.4 Experimental Results for Minimizing
    Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") shows
    that the trained Transformer with multi-layer self-attention structures with regret-loss
    is comparable to GPT-4 and outperformed FTRL in terms of regret, when the loss
    sequence is a sine-trend. Note that the training dataset does not contain the
    sequence of losses. Nonetheless, by focusing on the overall trend during training,
    we can attain performance that is either superior to or on par with that of FTRL
    and GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: Repeated games.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We then investigate the case of multi-player repeated games. We study 2x2,
    3x3x3, 3x3x3x3 games, where each entry of the payoff matrix is sampled randomly
    from $\operatorname{Unif}([0,10])$. The results, as illustrated in [Figure 11](#S5.F11
    "In Repeated games. ‣ 5.4 Experimental Results for Minimizing Regret-Loss ‣ 5
    Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games"), show that the trained
    single-layer self-attention model and the trained Transformer with multi-layer
    self-attention structures with regret-loss have a similar performance as that
    of FTRL. However, GPT-4 still outperforms the trained single-layer self-attention
    model and the trained Transformer with multi-layer self-attention structures in
    terms of regret. Since for repeated games (in which the environment faced by the
    agent can be less adversarial than that in the online setting), there might be
    a better algorithm than FTRL (see e.g., Daskalakis et al. ([2021](#bib.bib32))),
    while our self-attention models have a similar structure as FTRL ([Theorem 3](#Thmtheorem3
    "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online
    Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    or [Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")). Also, in practical training (with the empirical loss in [Equation 4](#S5.E4
    "In Definition 3 (Empirical loss function). ‣ 5.2 Guarantees via Regret-Loss Minimization
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")), we possibly did not
    find the exact global minimum or stationary point of the *expected* loss in [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games"). Hence, it is possible that GPT-4 may have lower regret
    than our trained models with the regret-loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbe3e58f98005139f25e6b113a111122.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Regret performance for the game with two players, three players,
    and four players general-sum games. No-regret behaviors of single-layer and multi-layer
    self-attention models are validated by both of our frameworks (low $p$).'
  prefs: []
  type: TYPE_NORMAL
- en: Two scenarios that caused regrettable behaviors of GPT-4.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Finally, we investigate the cases that have caused GPT-4 to have regrettable
    performance in [Section 3.2](#S3.SS2 "3.2 Results: Online Learning ‣ 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"). The results, which can be found in [Figure 7](#S3.F7
    "In Randomly generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do
    Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), show that both the trained single-layer
    self-attention model and the trained Transformer with regret-loss can achieve
    comparable no-regret performance as FTRL, and outperforms that of GPT-4\. This
    validates that our new unsupervised training loss can address the regrettable
    cases, as our theory in [Sections 5.2](#S5.SS2 "5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [5.3](#S5.SS3
    "5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") has predicted.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Concluding Remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we studied the online decision-making and strategic behaviors
    of LLMs quantitatively, through the metric of regret. We first examined and validated
    the no-regret behavior of several representative pre-trained LLMs in benchmark
    settings of online learning and games. As a consequence, (coarse correlated) equilibrium
    can oftentimes emerge as the long-term outcome of multiple LLMs playing repeated
    games. We then provide some theoretical insights into the no-regret behavior,
    by connecting pre-trained LLMs to the follow-the-perturbed-leader algorithm in
    online learning, under certain assumptions. We also identified (simple) cases
    where pre-trained LLMs fail to be no-regret, and thus proposed a new unsupervised
    training loss, *regret-loss*, to provably promote the no-regret behavior of Transformers
    without the labels of (optimal) actions. We established both experimental and
    theoretical evidence for the effectiveness of our regret-loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first attempt toward rigorously understanding the online and strategic
    decision-making behaviors of LLMs through the metric of regret, our work has opened
    up fruitful directions for future research:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are more than one definitions of (dynamic-)regret in the online learning
    literature, and we mainly focused on the so-called *external-regret* in the literature.
    It would be interesting to study the no-regret behavior of LLMs in terms of other
    regret metrics, e.g., swap-regret (Blum and Mansour, [2007](#bib.bib19)), which
    may lead to stronger equilibrium notions in playing repeated games.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our new regret-loss has exhibited promises in our experiments for training modest-scale
    Transformers. We are currently generalizing it to training other larger-scale
    models, such as Foundation Models, for decision-making.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No-regret behavior can sometimes lead to better outcomes in terms of social
    efficiency (Blum et al., [2008](#bib.bib18); Roughgarden, [2015](#bib.bib88);
    Nekipelov et al., [2015](#bib.bib80)). It would thus be interesting to further
    validate the efficiency of no-regret LLM agents in these scenarios, as well as
    identifying new prompts and training losses for LLMs to promote the efficiency
    of the outcomes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To evaluate the performance quantitatively, we focused on online learning and
    games with *numeric valued* payoffs. It would be interesting to connect our no-regret-based
    and game-theoretic framework with existing multi-LLM frameworks, e.g., debate,
    collaborative problem-solving, and human/social behavior simulation, with potentially
    new notions of regret (defined in different spaces) as performance metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors thank Dingwen Kong for discussing the truncation idea in proving
    [Lemma 8](#Thmlemma8 "Lemma 8\. ‣ Proof. ‣ D.2 Deferred Proof for the Arguments
    in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"). Also, the authors
    thank Kristian Georgiev and Aleksander Madry for the helpful feedback.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abernethy et al. (2014) Abernethy, J., Lee, C., Sinha, A. and Tewari, A. (2014).
    Online linear optimization via smoothing. In Conference on Learning Theory. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abernethy et al. (2015) Abernethy, J. D., Lee, C. and Tewari, A. (2015). Fighting
    bandits with a new kind of smoothness. Advances in Neural Information Processing
    Systems, 28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al.
    (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aher et al. (2023) Aher, G. V., Arriaga, R. I. and Kalai, A. T. (2023). Using
    large language models to simulate multiple humans and replicate human subject
    studies. In International Conference on Machine Learning. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahn et al. (2023) Ahn, K., Cheng, X., Daneshmand, H. and Sra, S. (2023). Transformers
    learn to implement preconditioned gradient descent for in-context learning. Advanced
    in Neural Information Processing Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahn et al. (2022) Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,
    David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K. et al. (2022). Do
    as i can, not as i say: Grounding language in robotic affordances. arXiv preprint
    arXiv:2204.01691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahsanullah et al. (2013) Ahsanullah, M., Nevzorov, V. B. and Shakil, M. (2013).
    An introduction to order statistics, vol. 8. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akata et al. (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge,
    M. and Schulz, E. (2023). Playing repeated games with large language models. arXiv
    preprint arXiv:2305.16867.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akyürek et al. (2023) Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. and Zhou,
    D. (2023). What learning algorithm is in-context learning? investigations with
    linear models. International Conference on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Argyle et al. (2023) Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R.,
    Rytting, C. and Wingate, D. (2023). Out of one, many: Using language models to
    simulate human samples. Political Analysis, 31 337–351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arora et al. (2012) Arora, S., Hazan, E. and Kale, S. (2012). The multiplicative
    weights update method: a meta-algorithm and applications. Theory of computing,
    8 121–164.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auer et al. (2002) Auer, P., Cesa-Bianchi, N., Freund, Y. and Schapire, R. E.
    (2002). The nonstochastic multiarmed bandit problem. SIAM journal on computing,
    32 48–77.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Bai, Y., Chen, F., Wang, H., Xiong, C. and Mei, S. (2023).
    Transformers as statisticians: Provable in-context learning with in-context algorithm
    selection. Advanced in Neural Information Processing Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bakhtin et al. (2022) Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty,
    C., Fried, D., Goff, A., Gray, J., Hu, H. et al. (2022). Human-level play in the
    game of diplomacy by combining language models with strategic reasoning. Science,
    378 1067–1074.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balseiro and Gur (2019) Balseiro, S. R. and Gur, Y. (2019). Learning in repeated
    auctions with budgets: Regret minimization and equilibrium. Management Science,
    65 3952–3968.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berge (1877) Berge, C. (1877). Topological spaces: Including a treatment of
    multi-valued functions, vector spaces and convexity. Oliver & Boyd.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besbes et al. (2014) Besbes, O., Gur, Y. and Zeevi, A. (2014). Stochastic multi-armed-bandit
    problem with non-stationary rewards. Advances in neural information processing
    systems, 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum et al. (2008) Blum, A., Hajiaghayi, M., Ligett, K. and Roth, A. (2008).
    Regret minimization and the price of total anarchy. In Proceedings of the fortieth
    annual ACM symposium on Theory of computing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum and Mansour (2007) Blum, A. and Mansour, Y. (2007). From external to internal
    regret. Journal of Machine Learning Research, 8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brookins and DeBacker (2023) Brookins, P. and DeBacker, J. M. (2023). Playing
    games with GPT: What can we learn about a large language model from canonical
    strategic games? Available at SSRN 4493398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. et al. (2020).
    Language models are few-shot learners. Advances in neural information processing
    systems, 33 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bubeck et al. (2012) Bubeck, S., Cesa-Bianchi, N. et al. (2012). Regret analysis
    of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends®
    in Machine Learning, 5 1–122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
    Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. et al. (2023).
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Camerer (2011) Camerer, C. F. (2011). Behavioral game theory: Experiments in
    strategic interaction. Princeton University Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cesa-Bianchi et al. (1996) Cesa-Bianchi, N., Long, P. M. and Warmuth, M. K.
    (1996). Worst-case quadratic loss bounds for prediction using linear functions
    and gradient descent. IEEE Transactions on Neural Networks, 7 604–619.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cesa-Bianchi and Lugosi (2006) Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction,
    Learning, and Games. Cambridge University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2024) Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S.,
    Fu, J. and Liu, Z. (2024). Chateval: Towards better llm-based evaluators through
    multi-agent debate. International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2024) Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C.,
    Chan, C.-M., Qin, Y., Lu, Y., Xie, R. et al. (2024). Agentverse: Facilitating
    multi-agent collaboration and exploring emergent behaviors in agents. International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Chen, Y., Liu, T. X., Shan, Y. and Zhong, S. (2023). The
    emergence of economic rationality of gpt. Proceedings of the National Academy
    of Sciences, 120 e2316205120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2023) Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z. and
    Wei, F. (2023). Why can GPT learn in-context? language models secretly perform
    gradient descent as meta-optimizers. In Findings of the Association for Computational
    Linguistics: ACL 2023 (A. Rogers, J. Boyd-Graber and N. Okazaki, eds.). Association
    for Computational Linguistics, Toronto, Canada.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aclanthology.org/2023.findings-acl.247](https://aclanthology.org/2023.findings-acl.247)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Danskin (1966) Danskin, J. M. (1966). The theory of max-min, with applications.
    SIAM Journal on Applied Mathematics, 14 641–664.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daskalakis et al. (2021) Daskalakis, C., Fishelson, M. and Golowich, N. (2021).
    Near-optimal no-regret learning in general games. Advances in Neural Information
    Processing Systems, 34 27604–27616.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2022) Ding, J., Feng, Y. and Rong, Y. (2022). Myopic quantal response
    policy: Thompson sampling meets behavioral economics. arXiv preprint arXiv:2207.01028.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,
    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T. et al. (2023). Palm-e:
    An embodied multimodal language model. International Conference on Machine Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2023) Du, Y., Li, S., Torralba, A., Tenenbaum, J. B. and Mordatch,
    I. (2023). Improving factuality and reasoning in language models through multiagent
    debate. arXiv preprint arXiv:2305.14325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engel et al. (2023) Engel, C., Grossmann, M. R. and Ockenfels, A. (2023). Integrating
    machine behavior into human subject experiments: A user-friendly toolkit and illustrations.
    Available at SSRN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Erev and Roth (1998) Erev, I. and Roth, A. E. (1998). Predicting how people
    play games: Reinforcement learning in experimental games with unique, mixed strategy
    equilibria. American Economic Review 848–881.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2023) Fan, C., Chen, J., Jin, Y. and He, H. (2023). Can large language
    models serve as rational players in game theory? a systematic analysis. arXiv
    preprint arXiv:2312.05488.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freund and Schapire (1997) Freund, Y. and Schapire, R. E. (1997). A decision-theoretic
    generalization of on-line learning and an application to boosting. Journal of
    computer and system sciences, 55 119–139.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Fu, Y., Peng, H., Khot, T. and Lapata, M. (2023). Improving
    language model negotiation with self-play and in-context learning from ai feedback.
    arXiv preprint arXiv:2305.10142.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fudenberg and Kreps (1993) Fudenberg, D. and Kreps, D. M. (1993). Learning mixed
    equilibria. Games and Economic Behavior, 5 320–367.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fudenberg and Levine (1998) Fudenberg, D. and Levine, D. K. (1998). The theory
    of learning in games, vol. 2. MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Pavel (2017) Gao, B. and Pavel, L. (2017). On the properties of the
    softmax function with application in game theory and reinforcement learning. arXiv
    preprint arXiv:1704.00805.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garg et al. (2022) Garg, S., Tsipras, D., Liang, P. S. and Valiant, G. (2022).
    What can transformers learn in-context? a case study of simple function classes.
    Advances in Neural Information Processing Systems, 35 30583–30598.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giannou et al. (2023) Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D.
    and Papailiopoulos, D. (2023). Looped transformers as programmable computers.
    International Conference on Machine Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. (2023) Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D. and
    Hu, Z. (2023). Reasoning with language model is planning with world model. In
    Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
    (H. Bouamor, J. Pino and K. Bali, eds.). Association for Computational Linguistics,
    Singapore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aclanthology.org/2023.emnlp-main.507](https://aclanthology.org/2023.emnlp-main.507)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hazan (2016) Hazan, E. (2016). Introduction to online convex optimization. Foundations
    and Trends® in Optimization, 2 157–325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hofbauer and Sandholm (2002) Hofbauer, J. and Sandholm, W. H. (2002). On the
    global convergence of stochastic fictitious play. Econometrica, 70 2265–2294.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2024) Hong, S., Zheng, X., Chen, J., Cheng, Y., Zhang, C., Wang,
    Z., Yau, S. K. S., Lin, Z., Zhou, L., Ran, C. et al. (2024). Metagpt: Meta programming
    for multi-agent collaborative framework. nternational Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horton (2023) Horton, J. J. (2023). Large language models as simulated economic
    agents: What can we learn from homo silicus? Tech. rep., National Bureau of Economic
    Research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022a) Huang, W., Abbeel, P., Pathak, D. and Mordatch, I. (2022a).
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. In International Conference on Machine Learning. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022b) Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,
    P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y. et al. (2022b). Inner monologue:
    Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang (2023) Jiang, H. (2023). A latent space theory for emergent abilities
    in large language models. arXiv preprint arXiv:2304.09960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalai and Vempala (2005) Kalai, A. and Vempala, S. (2005). Efficient algorithms
    for online decision problems. Journal of Computer and System Sciences, 71 291–307.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasprzak et al. (2022) Kasprzak, M. J., Giordano, R. and Broderick, T. (2022).
    How good is your gaussian approximation of the posterior? finite-sample computable
    error bounds for a variety of useful divergences. arXiv preprint arXiv:2209.14992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirschner et al. (2023) Kirschner, J., Bakhtiari, A., Chandak, K., Tkachuk,
    V. and Szepesvari, C. (2023). Regret minimization via saddle point optimization.
    In Thirty-seventh Conference on Neural Information Processing Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laskin et al. (2023) Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S.,
    Steigerwald, R., Strouse, D., Hansen, S., Filos, A., Brooks, E. et al. (2023).
    In-context reinforcement learning with algorithm distillation. International Conference
    on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lattimore and Szepesvári (2020) Lattimore, T. and Szepesvári, C. (2020). Bandit
    algorithms. Cambridge University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2023) Lee, J. N., Xie, A., Pacchiano, A., Chandak, Y., Finn, C.,
    Nachum, O. and Brunskill, E. (2023). Supervised pretraining can learn in-context
    reinforcement learning. Neural Information Processing Systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Li, C., Su, X., Fan, C., Han, H., Xue, C. and Zheng, C. (2023a).
    Quantifying the impact of large language models on collective opinion dynamics.
    arXiv preprint arXiv:2308.03313.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D. and
    Ghanem, B. (2023b). Camel: Communicative agents for” mind” exploration of large
    scale language model society. Neural Information Processing Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Li, R., Patel, T. and Du, X. (2023c). Prd: Peer rank and
    discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023d) Li, S., Yang, J. and Zhao, K. (2023d). Are you in a masquerade?
    exploring the behavior and impact of large language model driven social bots in
    online social networks. arXiv preprint arXiv:2307.10337.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023e) Li, Y., Ildiz, M. E., Papailiopoulos, D. and Oymak, S. (2023e).
    Transformers as algorithms: Generalization and stability in in-context learning.
    International Conference on Machine Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Tewari (2017) Li, Z. and Tewari, A. (2017). Beyond the hazard rate:
    More perturbation algorithms for adversarial multi-armed bandits. J. Mach. Learn.
    Res., 18 183–1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023) Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R.,
    Yang, Y., Tu, Z. and Shi, S. (2023). Encouraging divergent thinking in large language
    models through multi-agent debate. arXiv preprint arXiv:2305.19118.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2024) Lin, L., Bai, Y. and Mei, S. (2024). Transformers as decision
    makers: Provable in-context reinforcement learning via supervised pretraining.
    International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Littlestone and Warmuth (1994) Littlestone, N. and Warmuth, M. K. (1994). The
    weighted majority algorithm. Information and computation, 108 212–261.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023a) Liu, H., Sferrazza, C. and Abbeel, P. (2023a). Chain of hindsight
    aligns language models with feedback. arXiv preprint arXiv:2302.02676, 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Liu, Y., Van Roy, B. and Xu, K. (2023b). Nonstationary bandit
    learning via predictive sampling. In International Conference on Artificial Intelligence
    and Statistics. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023c) Liu, Z., Hu, H., Zhang, S., Guo, H., Ke, S., Liu, B. and
    Wang, Z. (2023c). Reason for future, act for now: A principled architecture for
    autonomous llm agents. In NeurIPS 2023 Foundation Models for Decision Making Workshop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lorè and Heydari (2023) Lorè, N. and Heydari, B. (2023). Strategic behavior
    of large language models: Game structure vs. contextual framing. arXiv preprint
    arXiv:2309.05898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2023) Lu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H. T. and
    Gurevych, I. (2023). Are emergent abilities in large language models just in-context
    learning? arXiv preprint arXiv:2309.01809.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahankali et al. (2023) Mahankali, A., Hashimoto, T. B. and Ma, T. (2023). One
    step of gradient descent is provably the optimal in-context learner with one layer
    of linear self-attention. International Conference on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. (2020) Mao, W., Zhang, K., Zhu, R., Simchi-Levi, D. and Başar, T.
    (2020). Model-free non-stationary RL: Near-optimal regret and applications in
    multi-agent RL and inventory control. arXiv preprint arXiv:2010.03161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McFadden (1976) McFadden, D. L. (1976). Quantal choice analaysis: A survey.
    Annals of Economic and Social Measurement, Volume 5, number 4 363–390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McKelvey and Palfrey (1995) McKelvey, R. D. and Palfrey, T. R. (1995). Quantal
    response equilibria for normal form games. Games and economic behavior, 10 6–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi,
    H. and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes
    in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing (Y. Goldberg, Z. Kozareva and Y. Zhang, eds.).
    Association for Computational Linguistics, Abu Dhabi, United Arab Emirates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Mukobi et al. (2023) Mukobi, G., Erlebach, H., Lauffer, N., Hammond, L., Chan,
    A. and Clifton, J. (2023). Welfare diplomacy: Benchmarking language model cooperation.
    arXiv preprint arXiv:2310.08901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nekipelov et al. (2015) Nekipelov, D., Syrgkanis, V. and Tardos, E. (2015).
    Econometrics for learning agents. In ACM Conference on Economics and Computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Openai (2023) Openai (2023). Gpt-4 technical report.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osband et al. (2013) Osband, I., Russo, D. and Van Roy, B. (2013). (more) efficient
    reinforcement learning via posterior sampling. Advances in Neural Information
    Processing Systems, 26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
    P. and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human
    behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software
    and Technology. UIST ’23, Association for Computing Machinery, New York, NY, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://doi.org/10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Park et al. (2022) Park, J. S., Popowski, L., Cai, C., Morris, M. R., Liang,
    P. and Bernstein, M. S. (2022). Social simulacra: Creating populated prototypes
    for social computing systems. In Proceedings of the 35th Annual ACM Symposium
    on User Interface Software and Technology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. (2023) Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu,
    Z. and Sun, M. (2023). Communicative agents for software development. arXiv preprint
    arXiv:2307.07924.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. (2022) Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov,
    A., Barth-maron, G., Giménez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles,
    T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals,
    O., Bordbar, M. and de Freitas, N. (2022). A generalist agent. Transactions on
    Machine Learning Research. Featured Certification, Outstanding Certification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://openreview.net/forum?id=1ikK0kHjvj](https://openreview.net/forum?id=1ikK0kHjvj)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Robinson and Goforth (2005) Robinson, D. and Goforth, D. (2005). The topology
    of the 2x2 games: a new periodic table, vol. 3. Psychology Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roughgarden (2015) Roughgarden, T. (2015). Intrinsic robustness of the price
    of anarchy. Journal of the ACM (JACM), 62 1–42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roughgarden et al. (2017) Roughgarden, T., Syrgkanis, V. and Tardos, E. (2017).
    The price of anarchy in auctions. Journal of Artificial Intelligence Research,
    59 59–101.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis,
    P., Izacard, G., You, Q., Nalmpantis, C., Grave, E. and Riedel, S. (2023). Peer:
    A collaborative language model. International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shalev-Shwartz (2007) Shalev-Shwartz, S. (2007). Online learning: Theory, algorithms,
    and applications. Hebrew University.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shalev-Shwartz (2012) Shalev-Shwartz, S. (2012). Online learning and online
    convex optimization. Foundations and Trends® in Machine Learning, 4 107–194.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shalev-Shwartz and Singer (2007) Shalev-Shwartz, S. and Singer, Y. (2007). A
    primal-dual perspective of online learning algorithms. Machine Learning, 69 115–142.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Shen, Y., Song, K., Tan, X., Li, D., Lu, W. and Zhuang,
    Y. (2023). Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface.
    Neural Information Processing Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R.
    and Yao, S. (2023). Reflexion: Language agents with verbal reinforcement learning.
    In Thirty-seventh Conference on Neural Information Processing Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significant Gravitas (2023) Significant Gravitas (2023). Autogpt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M.,
    Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A.
    et al. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities
    of language models. Transactions on Machine Learning Research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swan et al. (2023) Swan, M., Kido, T., Roland, E. and Santos, R. P. d. (2023).
    Math agents: Computational infrastructure, mathematical embedding, and genomics.
    arXiv preprint arXiv:2307.02502.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsai et al. (2023) Tsai, C. F., Zhou, X., Liu, S. S., Li, J., Yu, M. and Mei,
    H. (2023). Can large language models play text games well? current state-of-the-art
    and open questions. arXiv preprint arXiv:2304.02868.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valmeekam et al. (2023) Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S.
    and Kambhampati, S. (2023). Planbench: An extensible benchmark for evaluating
    large language models on planning and reasoning about change. In Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van der Vaart (2000) Van der Vaart, A. W. (2000). Asymptotic statistics, vol. 3.
    Cambridge university press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I. (2017). Attention is all you need.
    Advances in neural information processing systems, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Von Oswald et al. (2023) Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento,
    J., Mordvintsev, A., Zhmoginov, A. and Vladymyrov, M. (2023). Transformers learn
    in-context by gradient descent. In International Conference on Machine Learning.
    PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wainwright (2019) Wainwright, M. J. (2019). High-dimensional statistics: A
    non-asymptotic viewpoint, vol. 48. Cambridge university press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C.,
    Zhu, Y., Fan, L. and Anandkumar, A. (2023a). Voyager: An open-ended embodied agent
    with large language models. arXiv preprint arXiv:2305.16291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Wang, X., Zhu, W. and Wang, W. Y. (2023b). Large language
    models are implicitly topic models: Explaining and finding good demonstrations
    for in-context learning. International Conference on Machine Learning 2023 Workshop
    ES-FoMO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Wang, Z., Cai, S., Liu, A., Ma, X. and Liang, Y. (2023c).
    Describe, explain, plan and select: Interactive planning with large language models
    enables open-world multi-task agents. Advances in neural information processing
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei and Luo (2021) Wei, C.-Y. and Luo, H. (2021). Non-stationary reinforcement
    learning without prior knowledge: An optimal black-box approach. In Conference
    on learning theory. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M. and Le, Q. V. (2021). Finetuned language models are zero-shot
    learners. International Conference on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D. et al. (2022a). Emergent abilities
    of large language models. Transactions on Machine Learning Research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D. et al. (2022b). Chain-of-thought prompting elicits reasoning
    in large language models. Advances in Neural Information Processing Systems, 35
    24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E.,
    Li, B., Jiang, L., Zhang, X. and Wang, C. (2023). Autogen: Enabling next-gen llm
    applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2022) Xie, S. M., Raghunathan, A., Liang, P. and Ma, T. (2022).
    An explanation of in-context learning as implicit bayesian inference. International
    Conference on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2023) Xiong, K., Ding, X., Cao, Y., Liu, T. and Qin, B. (2023).
    Examining inter-consistency of large language models collaboration: An in-depth
    analysis via debate. In Findings of the Association for Computational Linguistics:
    EMNLP 2023 (H. Bouamor, J. Pino and K. Bali, eds.). Association for Computational
    Linguistics, Singapore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aclanthology.org/2023.findings-emnlp.508](https://aclanthology.org/2023.findings-emnlp.508)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Xu et al. (2023a) Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W. and
    Liu, Y. (2023a). Exploring large language models for communication games: An empirical
    study on werewolf. arXiv preprint arXiv:2309.04658.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2023b) Xu, Z., Yu, C., Fang, F., Wang, Y. and Wu, Y. (2023b). Language
    agents with reinforcement learning for strategic play in the werewolf game. arXiv
    preprint arXiv:2310.18940.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023a) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L.,
    Cao, Y. and Narasimhan, K. (2023a). Tree of thoughts: Deliberate problem solving
    with large language models. Advances in Neural Information Processing Systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
    K. and Cao, Y. (2023b). React: Synergizing reasoning and acting in language models.
    International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young (2004) Young, H. P. (2004). Strategic learning and its limits. OUP Oxford.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024) Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum,
    J. B., Shu, T. and Gan, C. (2024). Building cooperative embodied agents modularly
    with large language models. International Conference on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023a) Zhang, R., Frei, S. and Bartlett, P. L. (2023a). Trained
    transformers learn linear models in-context. arXiv preprint arXiv:2306.09927.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023b) Zhang, Y., Zhang, F., Yang, Z. and Wang, Z. (2023b). What
    and how does in-context learning learn? bayesian model averaging, parameterization,
    and generalization. arXiv preprint arXiv:2305.19420.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Zhao, Q., Wang, J., Zhang, Y., Jin, Y., Zhu, K., Chen, H.
    and Xie, X. (2023). Competeai: Understanding the competition behaviors in large
    language model-based agents. arXiv preprint arXiv:2310.17512.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zimmert and Seldin (2021) Zimmert, J. and Seldin, Y. (2021). Tsallis-inf: An
    optimal algorithm for stochastic and adversarial bandits. The Journal of Machine
    Learning Research, 22 1310–1358.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zinkevich (2003) Zinkevich, M. (2003). Online convex programming and generalized
    infinitesimal gradient ascent. In International Conference on Machine Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplementary Materials for
  prefs: []
  type: TYPE_NORMAL
- en: “Do LLM Agents Have Regret? A Case Study in Online Learning and Games”
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[1 Introduction](#S1 "In Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.1 Related Work](#S1.SS1 "In 1 Introduction ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2 Preliminaries](#S2 "In Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1 Online Learning & Games](#S2.SS1 "In 2 Preliminaries ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2 Performance Metric: Regret](#S2.SS2 "In 2 Preliminaries ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3 Do Pre-Trained LLMs Have Regret? Experimental Validation](#S3 "In Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1 Framework for No-Regret Behavior Validation](#S3.SS1 "In 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2 Results: Online Learning](#S3.SS2 "In 3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.3 Results: Multi-Player Repeated Games](#S3.SS3 "In 3 Do Pre-Trained LLMs
    Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.4 Pre-Trained LLM Agents May Still Have Regret](#S3.SS4 "In 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights](#S4 "In Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1 Pre-Trained LLMs Have Similar Regret as Humans (Who Generate Data)](#S4.SS1
    "In 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2 A Human Decision-Making Model: Quantal Response](#S4.SS2 "In 4 Why Are
    Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3 Case Study: Pre-Training under Canonical Data Distribution](#S4.SS3 "In
    4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5 Provably Promoting No-Regret Behavior by an Unsupervised Loss](#S5 "In Do
    LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1 A New Unsupervised Training Loss: Regret-Loss](#S5.SS1 "In 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2 Guarantees via Regret-Loss Minimization](#S5.SS2 "In 5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning
    Algorithms](#S5.SS3 "In 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.4 Experimental Results for Minimizing Regret-Loss](#S5.SS4 "In 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6 Concluding Remarks](#S6 "In Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A Deferred Background](#A1 "In Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.1 Additional Definitions for Appendix](#A1.SS1 "In Appendix A Deferred Background
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.2 In-Context Learning](#A1.SS2 "In Appendix A Deferred Background ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.3 Online Learning Algorithms](#A1.SS3 "In Appendix A Deferred Background
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.4 Why Focusing on Linear Loss Function?](#A1.SS4 "In Appendix A Deferred
    Background ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.5 Six Representative General-Sum Games](#A1.SS5 "In Appendix A Deferred
    Background ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B Deferred Results and Proofs in Section 3](#A2 "In Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B.1 Ablation Study on Prompts](#A2.SS1 "In Appendix B Deferred Results and
    Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B.2 Proof for Proposition 1](#A2.SS2 "In Appendix B Deferred Results and Proofs
    in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B.3 Results of GPT-4 Turbo](#A2.SS3 "In Appendix B Deferred Results and Proofs
    in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B.4 LLM Agents’ Explanation on Their Output Policies](#A2.SS4 "In Appendix
    B Deferred Results and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C Deferred Results and Proofs in Section 4](#A3 "In Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.1 Deferred Proof of Observation 1](#A3.SS1 "In Appendix C Deferred Results
    and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.2 Deferred Proof of Lemma 1](#A3.SS2 "In Appendix C Deferred Results and
    Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.3 Relationship between FTPL and Definition 2](#A3.SS3 "In Appendix C Deferred
    Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.4 Deferred Proof of Theorem 1](#A3.SS4 "In Appendix C Deferred Results and
    Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.5 Extending Theorem 1 with Relaxed Assumptions](#A3.SS5 "In Appendix C Deferred
    Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.5.1 Relaxation under More General Data Distributions](#A3.SS5.SSS1 "In C.5
    Extending Theorem 1 with Relaxed Assumptions ‣ Appendix C Deferred Results and
    Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C.5.2 Relaxation under Decision-Irrelevant Pre-Training Data](#A3.SS5.SSS2
    "In C.5 Extending Theorem 1 with Relaxed Assumptions ‣ Appendix C Deferred Results
    and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D Deferred Results and Proofs in Section 5](#A4 "In Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.1 Basic Lemmas](#A4.SS1 "In Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.2 Deferred Proof for the Arguments in Section 5.1](#A4.SS2 "In Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.3 Deferred Proofs of Theorem 2 and Corollary 1](#A4.SS3 "In Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.4 Deferred Proof of Theorem 3](#A4.SS4 "In Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.5 Deferred Proof of Theorem 4](#A4.SS5 "In Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.6 Empirical Validation of Theorem 3 and Theorem 4](#A4.SS6 "In Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.6.1 Empirical Validation of Theorem 3](#A4.SS6.SSS1 "In D.6 Empirical Validation
    of Theorem 3 and Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.6.2 Empirical Validation of Theorem 4](#A4.SS6.SSS2 "In D.6 Empirical Validation
    of Theorem 3 and Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.7 Discussions on the Production of FTRL with Entropy Regularization](#A4.SS7
    "In Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.7.1 Numerical Analysis of Step 2 and Step 4](#A4.SS7.SSS1 "In D.7 Discussions
    on the Production of FTRL with Entropy Regularization ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.7.2 Empirical Validation](#A4.SS7.SSS2 "In D.7 Discussions on the Production
    of FTRL with Entropy Regularization ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.8 Training Details of Section 5.4](#A4.SS8 "In Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D.9 Ablation Study on Training Equation 3](#A4.SS9 "In Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Appendix A Deferred Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Additional Definitions for Appendix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: (Linear) Self-attention.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One key component in Transformers (Vaswani et al., [2017](#bib.bib102)), the
    backbone of modern language models, is the *(self-)attention* mechanism. For simplicity,
    we here focus on introducing the *single-layer* self-attention architecture. The
    mechanism takes a sequence of vectors $Z=[z_{1},\dots,z_{t}]\in\mathbb{R}^{d\times
    t}$.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For a multi-layer perceptron (MLP) layer, it takes $Z=[z_{1},\dots,z_{t}]\in\mathbb{R}^{d\times
    t}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\texttt{TF}_{\theta}(Z):=Z^{(L)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the output $Z^{(L)}$ and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: for some <math id=$$ is the number of heads of self-attention,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\theta\&#124;_{\texttt{TF}}:=\max_{l\in[L]}\left\{\&#124;\theta^{(la)}\&#124;_{\texttt{M-(L)SA}}+\&#124;\theta^{(lm)}\&#124;_{\texttt{MLP}}\right\},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: and $$B_{\texttt{TF}}></math> to be sufficiently large such that clip does not
    take effect on any of our approximation results.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 In-Context Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In-context learning is an emergent behavior of LLMs (Brown et al., [2020](#bib.bib21)),
    which means that these models can adapt and learn from a limited number of examples
    provided within their immediate input context. In in-context learning, the prompt
    is usually constituted by a length of $T$.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Online Learning Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Follow-the-regularized-leader (FTRL).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Follow-the-Regularized-Leader (FTRL) algorithm (Shalev-Shwartz, [2007](#bib.bib91))
    is an iterative method that updates policy based on the observed data and a regularization
    term. The idea is to choose the next policy that minimizes the sum of the past
    losses and a regularization term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, given a sequence of loss vectors ${\ell}_{1},{\ell}_{2},\dots,{\ell}_{t}$
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{t+1}=\arg\min_{\pi\in\Pi}\left(\sum_{i=1}^{t}\langle{\ell}_{i},\pi\rangle+R(\pi)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $R(\pi)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: A key property that ensures the convergence and stability of the FTRL algorithm
    is the strong convexity of the regularization term $R(\pi)$ ensures that the optimization
    problem in FTRL has a unique solution. The FTRL algorithm’s flexibility allows
    it to encompass a wide range of online learning algorithms, from gradient-based
    methods like online gradient descent to decision-making algorithms like Hedge
    (Freund and Schapire, [1997](#bib.bib39)).
  prefs: []
  type: TYPE_NORMAL
- en: Connection to online gradient descent (OGD).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The Online Gradient Descent (OGD) (Cesa-Bianchi et al., [1996](#bib.bib25))
    algorithm is a special case of the FTRL algorithm when the regularization term
    is the $L_{2}$ is updated using the gradient of the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{t+1}=\pi_{t}-{\ell}_{t}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, the connection between FTRL and OGD can be seen by observing that
    the update rule for FTRL with $L_{2}$ regularization can be derived from the OGD
    update rule.
  prefs: []
  type: TYPE_NORMAL
- en: Connection to the Hedge algorithm.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Hedge algorithm (Freund and Schapire, [1997](#bib.bib39)) (also referred
    to as the Multiplicative Weight Update algorithm (Arora et al., [2012](#bib.bib11)))
    is an online learning algorithm designed for problems where the learner has to
    choose from a set of actions (denoted as $\mathcal{A}$), then the FTRL update
    rule yields the Hedge algorithm as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: for $j\in[d]$.
  prefs: []
  type: TYPE_NORMAL
- en: Follow-the-perturbed-leader (FTPL).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Given a sequence of loss vectors ${\ell}_{1},{\ell}_{2},\dots,{\ell}_{t-1}$
    for the next time step is chosen by solving the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\pi_{t}=\mathbb{E}\left[\arg\min_{\pi\in\Pi}\langle\epsilon_{t},\pi\rangle+\sum_{i=1}^{t-1}\langle{\ell}_{i},\pi\rangle\right].$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Here $\epsilon_{t}$ introduces randomness to the decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between FTRL and FTPL.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The FTRL and FTPL algorithms are deeply related. For example, FTPL with perturbations
    of Gumbel distribution and FTRL with Entropy Regularization (i.e., Hedge) are
    equivalent. In general, for the FTPL algorithm with any perturbation distribution,
    one can always find an FTRL algorithm with a particular regularization such that
    their update rule is equivalent. However, this relationship does not hold vice
    versa. For example, Hofbauer and Sandholm ([2002](#bib.bib48)) shows that for
    FTRL with log barrier regularization, there does not exist an equivalent perturbation
    distribution for FTPL.
  prefs: []
  type: TYPE_NORMAL
- en: Restarting techniques for non-stationary online learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For non-stationary online learning problems, one common technique is *restarting*:
    one restarts the standard online learning algorithm periodically (Besbes et al.,
    [2014](#bib.bib17)) (see also e.g., Wei and Luo ([2021](#bib.bib108)); Mao et al.
    ([2020](#bib.bib75))). After each restarting operation, the algorithm will ignore
    the previous history and execute as if it is the beginning of the interaction
    with the environment. Since the variation of the loss sequences is bounded, loss
    sequences between two consecutive restarting operations can be regarded as being
    *almost stationary*, which makes achieving an overall sublinear dynamic regret
    guarantee possible.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Why Focusing on Linear Loss Function?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We note that focusing on the linear loss function $f_{t}(\pi):=\langle{\ell}_{t},\pi\rangle$,
    which indicates
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, one can regard the loss vector $({\ell}_{t})_{t\in[T]}$ corresponds
    to the gradient of the convex function evaluated at the policy at that round.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Six Representative General-Sum Games
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In game theory, there are six representative two-player general-sum games (Robinson
    and Goforth, [2005](#bib.bib87)). Firstly, consider the win-win game represented
    by matrices <math id="A1.SS5.p1.1.m1.1" class="ltx_Math" alttext="A=\begin{pmatrix}1&amp;4\\
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;2\end{pmatrix}$$ and $$B=\begin{pmatrix}1&amp;4\\
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;2\end{pmatrix}$$ for players A and B, respectively. This setup fosters
    a cooperative dynamic, as both players receive identical payoffs, encouraging
    strategies that benefit both parties equally.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the prisoner’s dilemma, depicted by payoff matrices $$A=\begin{pmatrix}1&amp;3\\
  prefs: []
  type: TYPE_NORMAL
- en: 2&amp;4\end{pmatrix}$$ and $$B=\begin{pmatrix}4&amp;3\\
  prefs: []
  type: TYPE_NORMAL
- en: 2&amp;1\end{pmatrix}$$, illustrates the conflict between individual and collective
    rationality, where players are tempted to pursue individual gain at the collective’s
    expense, often resulting in suboptimal outcomes for both.
  prefs: []
  type: TYPE_NORMAL
- en: In the unfair game, represented by $$A=\begin{pmatrix}2&amp;1\\
  prefs: []
  type: TYPE_NORMAL
- en: 3&amp;4\end{pmatrix}$$ and $$B=\begin{pmatrix}4&amp;3\\
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;2\end{pmatrix}$$, the asymmetry in the payoff structure places one player
    at a disadvantage, regardless of the chosen strategy. This imbalance often reflects
    real-world scenarios where power or information asymmetry affects decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: The cyclic game, with matrices $$A=\begin{pmatrix}3&amp;1\\
  prefs: []
  type: TYPE_NORMAL
- en: 2&amp;4\end{pmatrix}$$ and $$B=\begin{pmatrix}3&amp;4\\
  prefs: []
  type: TYPE_NORMAL
- en: 2&amp;1\end{pmatrix}$$, presents a scenario where no stable equilibrium exists.
    The best strategy for each player changes in response to the other’s actions,
    leading to a continuous cycle of strategy adaptation without a clear resolution.
  prefs: []
  type: TYPE_NORMAL
- en: The biased game, depicted by $$A=\begin{pmatrix}3&amp;2\\
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;4\end{pmatrix}$$ and $$B=\begin{pmatrix}4&amp;2\\
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;3\end{pmatrix}$$, inherently favors one player, often reflecting situations
    where external factors or inherent advantages influence outcomes, leading to consistently
    unequal payoffs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the second-best game, with payoff matrices $$A=\begin{pmatrix}1&amp;2\\
  prefs: []
  type: TYPE_NORMAL
- en: 3&amp;4\end{pmatrix}$$ and $$B=\begin{pmatrix}1&amp;4\\
  prefs: []
  type: TYPE_NORMAL
- en: 3&amp;2\end{pmatrix}$$, encapsulates scenarios where players settle for less-than-optimal
    outcomes due to constraints like risk aversion or limited options. This often
    results in players choosing safer, albeit less rewarding, strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these games exemplifies distinct aspects of strategic decision-making
    and interactions. From cooperative to competitive and fair to biased scenarios,
    these matrices provide a rich landscape for exploring the nuances of decision-making
    behavior in game theory.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Deferred Results and Proofs in [Section 3](#S3 "3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Ablation Study on Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ablation study on online learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To systematically understand the effects of our prompt on the final performance
    of the LLM agent, we create three different variants of our prompt and report
    the regret by using different prompts in [Figure 12](#A2.F12 "In Ablation study
    on online learning. ‣ B.1 Ablation Study on Prompts ‣ Appendix B Deferred Results
    and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"). Specifically, for Ablation1, we remove examples to illustrate the
    game rules. For Ablation2, we remove the number of iterations. For Ablation3,
    we incorporate some *hints* for the LLM for decision-making, including the hints
    to suggest it to pay attention to the loss history, to behave more greedily at
    the end of an episode, and also to explain the reason of its decision step-by-step.
    The latter hint is a popular technique in prompt engineering known as the *Chain-of-Thought*
    prompting (Wei et al., [2022b](#bib.bib111)). Finally, we recall that $d$ is the
    number of actions in all prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/481e52157ee8342d5db04c0bde8755a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Ablation study on our prompt design.'
  prefs: []
  type: TYPE_NORMAL
- en: Original prompt
  prefs: []
  type: TYPE_NORMAL
- en: You are solving a decision-making problem for 25 rounds.
  prefs: []
  type: TYPE_NORMAL
- en: There are $d$ number of action (which is 0 to $d-1$).
  prefs: []
  type: TYPE_NORMAL
- en: At each round, you need to choose a policy, it specifies your probability to choose each action.
  prefs: []
  type: TYPE_NORMAL
- en: This policy should be $d$-dimensional, and the sum of its components should equal 1.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you will be shown the reward vector for choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this reward vector is decided by the external system and can be potentially
  prefs: []
  type: TYPE_NORMAL
- en: different for different rounds.
  prefs: []
  type: TYPE_NORMAL
- en: It is not decided by what policies you have chosen. The reward vector is also $d$-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: It represents the reward of choosing action from 0 to $d-1$.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a reward vector of [0.8, 3.2] means reward for action_0 is 0.8
  prefs: []
  type: TYPE_NORMAL
- en: and the reward for action_1 is 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Then your reward for this round will be calculated according to the reward of each action and
  prefs: []
  type: TYPE_NORMAL
- en: your probability of choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you choose the policy [0.2, 0.8] and get the reward vector [1, 2],
  prefs: []
  type: TYPE_NORMAL
- en: then your expected reward is 0.2*1 + 0.8*2=1.8
  prefs: []
  type: TYPE_NORMAL
- en: Your goal is to maximize your accumulative expected reward.
  prefs: []
  type: TYPE_NORMAL
- en: You can adjust your policy based on the reward vectors for all previous rounds.
  prefs: []
  type: TYPE_NORMAL
- en: You’re required to provide your policy in numeric format.
  prefs: []
  type: TYPE_NORMAL
- en: Your response’s last line should be formatted as ’Policy: [your $d$-dimensional policy]’.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ablation1: no examples'
  prefs: []
  type: TYPE_NORMAL
- en: You are solving a decision-making problem for 25 rounds.
  prefs: []
  type: TYPE_NORMAL
- en: There are $d$ number of action (which is 0 to $d-1$).
  prefs: []
  type: TYPE_NORMAL
- en: At each round, you need to choose a policy, it specifies your probability to choose each action.
  prefs: []
  type: TYPE_NORMAL
- en: This policy should be $d$-dimensional, and the sum of its components should equal 1.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you will be shown the reward vector for choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this reward vector is decided by the external system and can be potentially
  prefs: []
  type: TYPE_NORMAL
- en: different for different rounds.
  prefs: []
  type: TYPE_NORMAL
- en: It is not decided by what policies you have chosen. The reward vector is also $d$-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: It represents the reward of choosing action from 0 to $d-1$.
  prefs: []
  type: TYPE_NORMAL
- en: Then your reward for this round will be calculated according to the reward of each action and
  prefs: []
  type: TYPE_NORMAL
- en: your probability of choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: Your goal is to maximize your accumulative expected reward.
  prefs: []
  type: TYPE_NORMAL
- en: You can adjust your policy based on the reward vectors for all previous rounds.
  prefs: []
  type: TYPE_NORMAL
- en: You’re required to provide your policy in numeric format.
  prefs: []
  type: TYPE_NORMAL
- en: Your response’s last line should be formatted as ’Policy: [your $d$-dimensional policy]’.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ablation2: no round information'
  prefs: []
  type: TYPE_NORMAL
- en: You are solving a decision-making problem.
  prefs: []
  type: TYPE_NORMAL
- en: There are $d$ number of action (which is 0 to $d-1$).
  prefs: []
  type: TYPE_NORMAL
- en: At each round, you need to choose a policy, it specifies your probability to choose each action.
  prefs: []
  type: TYPE_NORMAL
- en: This policy should be $d$-dimensional, and the sum of its components should equal 1.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you will be shown the reward vector for choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this reward vector is decided by the external system and can be potentially
  prefs: []
  type: TYPE_NORMAL
- en: different for different rounds.
  prefs: []
  type: TYPE_NORMAL
- en: It is not decided by what policies you have chosen. The reward vector is also $d$-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: It represents the reward of choosing action from 0 to $d-1$.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a reward vector of [0.8, 3.2] means reward for action_0 is 0.8
  prefs: []
  type: TYPE_NORMAL
- en: and the reward for action_1 is 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Then your reward for this round will be calculated according to the reward of each action and
  prefs: []
  type: TYPE_NORMAL
- en: your probability of choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you choose the policy [0.2, 0.8] and get the reward vector [1, 2],
  prefs: []
  type: TYPE_NORMAL
- en: then your expected reward is 0.2*1 + 0.8*2=1.8
  prefs: []
  type: TYPE_NORMAL
- en: Your goal is to maximize your accumulative expected reward.
  prefs: []
  type: TYPE_NORMAL
- en: You can adjust your policy based on the reward vectors for all previous rounds.
  prefs: []
  type: TYPE_NORMAL
- en: You’re required to provide your policy in numeric format.
  prefs: []
  type: TYPE_NORMAL
- en: Your response’s last line should be formatted as ’Policy: [your $d$-dimensional policy]’.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ablation3: adding hints'
  prefs: []
  type: TYPE_NORMAL
- en: You are solving a decision-making problem for 25 rounds.
  prefs: []
  type: TYPE_NORMAL
- en: There are $d$ number of action (which is 0 to $d-1$).
  prefs: []
  type: TYPE_NORMAL
- en: At each round, you need to choose a policy, it specifies your probability to choose each action.
  prefs: []
  type: TYPE_NORMAL
- en: This policy should be $d$-dimensional, and the sum of its components should equal 1.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you will be shown the reward vector for choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this reward vector is decided by the external system and can be potentially
  prefs: []
  type: TYPE_NORMAL
- en: different for different rounds.
  prefs: []
  type: TYPE_NORMAL
- en: It is not decided by what policies you have chosen. The reward vector is also $d$-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: It represents the reward of choosing action from 0 to $d-1$.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a reward vector of [0.8, 3.2] means reward for action_0 is 0.8
  prefs: []
  type: TYPE_NORMAL
- en: and the reward for action_1 is 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Then your reward for this round will be calculated according to the reward of each action and
  prefs: []
  type: TYPE_NORMAL
- en: your probability of choosing each action.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you choose the policy [0.2, 0.8] and get the reward vector [1, 2],
  prefs: []
  type: TYPE_NORMAL
- en: then your expected reward is 0.2*1 + 0.8*2=1.8
  prefs: []
  type: TYPE_NORMAL
- en: Your goal is to maximize your accumulative expected reward.
  prefs: []
  type: TYPE_NORMAL
- en: You can adjust your policy based on the reward vectors for all previous rounds.
  prefs: []
  type: TYPE_NORMAL
- en: You’re required to provide your policy in numeric format.
  prefs: []
  type: TYPE_NORMAL
- en: Your response’s last line should be formatted as ’Policy: [your $d$-dimensional policy]’.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think step by step. Explicitly examining history is important.
  prefs: []
  type: TYPE_NORMAL
- en: Please explain how you chose the policy by guessing what reward you might receive
  prefs: []
  type: TYPE_NORMAL
- en: for each action according to the history.
  prefs: []
  type: TYPE_NORMAL
- en: You should explore for first several rounds and behave greedily for later rounds, for example,
  prefs: []
  type: TYPE_NORMAL
- en: choosing one action with probability more than 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: Please also explain whether you are behaving more greedily and less greedily
  prefs: []
  type: TYPE_NORMAL
- en: by explicitly considering the policy you just used for last round.
  prefs: []
  type: TYPE_NORMAL
- en: We can see in [Figure 12](#A2.F12 "In Ablation study on online learning. ‣ B.1
    Ablation Study on Prompts ‣ Appendix B Deferred Results and Proofs in Section
    3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") that
    the performances of LLM agents are consistent under different variants of the
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation study on repeated games.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For the game setting, we also investigate whether explicitly informing LLM agents
    that they are ‘‘playing a repeated matrix game with some other opponents’’ would
    affect the performance. Therefore, we evaluate three different prompts by informing
    LLM agents that they are playing a matrix game, solving multi-arm bandit, or solving
    general decision-making problems, in the first line of the prompt. We show the
    performance of such three prompts in [Figure 13](#A2.F13 "In Ablation study on
    repeated games. ‣ B.1 Ablation Study on Prompts ‣ Appendix B Deferred Results
    and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), where it is seen that LLM agents’ performance on repeated games is
    consistent among these variants of the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/274e21fd9b61153316e05aa752b64249.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Regret of GPT-4 for repeated games under $3$ different prompt ablations.
    Its performance is consistent among three different prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Proof for [Proposition 1](#Thmproposition1 "Proposition 1\. ‣ Trend-checking
    framework. ‣ 3.1 Framework for No-Regret Behavior Validation ‣ 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Under the null hypothesis $H_{0}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\mathbb{P}_{H_{0}}(\mathcal{E}(s,T))=\sum_{k=s}^{T-1}p^{s}(1-p)^{T-1-s}\begin{pmatrix}T-1\\
    k\end{pmatrix}\leq\frac{1}{2^{T-1}}\sum_{k=s}^{T-1}\begin{pmatrix}T-1\\'
  prefs: []
  type: TYPE_NORMAL
- en: k\end{pmatrix}$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: since $s\geq\frac{T-1}{2}$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Results of GPT-4 Turbo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When preparing our draft, we are aware of a more advanced version of GPT-4,
    GPT-4 Turbo. Although GPT-4 has successfully exhibited no-regret behaviors in
    most problems, we also evaluate and report the performance of its more advanced
    version here. Specifically, we compare GPT-4 and GPT-4 Turbo for the repeated
    game scenario in [Figure 14](#A2.F14 "In B.3 Results of GPT-4 Turbo ‣ Appendix
    B Deferred Results and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), where we validate that GPT-4 Turbo performs
    either comparably with or better than GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59eb5e47afa4d940b641d217641e5de9.png)![Refer to caption](img/bd1dca264dd7a23a044ef4145ec7be58.png)![Refer
    to caption](img/e1d4f5feb5780d81a1ee691597be57e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Comparisons of GPT-4 Turbo with GPT-4 and GPT-3.5 Turbo. Due to
    the symmetry of agents in randomly generated games, we report the regret of one
    agent for ease of presentation. We can see that GPT-4 Turbo performs on par with
    or better than GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4 LLM Agents’ Explanation on Their Output Policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We here present two examples of LLMs’ explanations of their output policies,
    showing how they *reason* in their decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Output Example 1
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the history of the reward vectors, it is clear to see that the reward for choosing
  prefs: []
  type: TYPE_NORMAL
- en: action 0 is consistently higher than that for choosing action 1. This pattern is evident across
  prefs: []
  type: TYPE_NORMAL
- en: all 5 rounds of the game.
  prefs: []
  type: TYPE_NORMAL
- en: In the first round, the difference in rewards is 3.52, this remained relatively stable with
  prefs: []
  type: TYPE_NORMAL
- en: the difference in the last round being 3.52 as well. It’s indicative that action 0 consistently
  prefs: []
  type: TYPE_NORMAL
- en: brings a higher return than action 1.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I’ll lean more towards selecting action 0 in my policy.
  prefs: []
  type: TYPE_NORMAL
- en: However, acting deterministically, that is always choosing action 0, would introduce the risk
  prefs: []
  type: TYPE_NORMAL
- en: of becoming too predictable and letting the opponent exploit this predictability
  prefs: []
  type: TYPE_NORMAL
- en: So, in order to maintain some unpredictability while still capitalizing on the higher average
  prefs: []
  type: TYPE_NORMAL
- en: return of action 0, I’ll set my policy as follows: select action 0 with a probability of 0.8
  prefs: []
  type: TYPE_NORMAL
- en: and action 1 with a probability of 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the majority of the time I’m choosing the action that gives a higher reward, but I’m
  prefs: []
  type: TYPE_NORMAL
- en: also occasionally choosing the other action to keep the opponent guessing, and avoid
  prefs: []
  type: TYPE_NORMAL
- en: any predictable patterns that can be exploited
  prefs: []
  type: TYPE_NORMAL
- en: So, my policy is as follows: Policy: [0.8, 0.2]
  prefs: []
  type: TYPE_NORMAL
- en: Output Example 2
  prefs: []
  type: TYPE_NORMAL
- en: The total reward from action_1 is 130.92 and the average reward is around 10.07.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the total reward from action_0 is 33.418 and the average reward is around 8.354.
  prefs: []
  type: TYPE_NORMAL
- en: Although action_0 has a less average reward, its highest reward is 26.977,
  prefs: []
  type: TYPE_NORMAL
- en: which is considerably high and shows its potential.
  prefs: []
  type: TYPE_NORMAL
- en: For this round, I want to continue to explore action_0 but also significantly exploit action 1
  prefs: []
  type: TYPE_NORMAL
- en: as it reliably gives higher reward.
  prefs: []
  type: TYPE_NORMAL
- en: So, I’d like to decrease the probability of selecting action_0 compared to last time
  prefs: []
  type: TYPE_NORMAL
- en: but still keep it in the policy as a possible option.
  prefs: []
  type: TYPE_NORMAL
- en: The policy for the previous round was not provided but assuming it was more balanced,
  prefs: []
  type: TYPE_NORMAL
- en: I’m now going to lean more towards action_1.
  prefs: []
  type: TYPE_NORMAL
- en: So, the policy will be: [0.3, 0.7]. Here, I’m behaving more greedily compared to earlier rounds,
  prefs: []
  type: TYPE_NORMAL
- en: as we are choosing the action which has been giving us higher rewards with more probability.
  prefs: []
  type: TYPE_NORMAL
- en: As observed from the output, the LLM agent tends to use the history of the reward
    vectors by looking at the “sum/average of the accumulated rewards”, and tends
    to introduce randomization in decision-making, in order to be “less predictable
    and exploitable”. These are several key components in achieving no-regret in online
    learning and games (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)), explaining the
    empirical evidence we had in [Section 3](#S3 "3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Deferred Results and Proofs in [Section 4](#S4 "4 Why Are Pre-Trained
    LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Deferred Proof of Observation [1](#Thmobservation1 "Observation 1\. ‣ 4.1
    Pre-Trained LLMs Have Similar Regret as Humans (Who Generate Data) ‣ 4 Why Are
    Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proof of [1](#Thmobservation1 "Observation 1\. ‣ 4.1 Pre-Trained LLMs Have Similar
    Regret as Humans (Who Generate Data) ‣ 4 Why Are Pre-Trained LLMs (No-)Regret?
    A Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games").
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For given $(\ell_{t})_{t\in[T]}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: holds, where we use the convention of $P_{\text{LLM}}(\pi_{t}\mid(\ell_{0})):=P_{\text{LLM}}(\pi_{t})$.
    Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Regret}_{\text{LLM}}((\ell_{t})_{t\in[T]})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\text{Regret}_{\text{data}}((\ell_{t})_{t\in[T]})+\epsilon\&#124;\ell\&#124;_{p}\&#124;\pi\&#124;_{q}T$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\frac{1}{p}+\frac{1}{q}=1$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'C.2 Deferred Proof of [Lemma 1](#Thmlemma1 "Lemma 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'See [1](#Thmlemma1 "Lemma 1\. ‣ 4.3 Case Study: Pre-Training under Canonical
    Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model
    and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For $\mathbb{P}(\ell_{1:t}{\,|\,}x_{1:N_{t}})$, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where we use $\delta$, by [1](#Thmassumption1 "Assumption 1\. ‣ 4.3 Case Study:
    Pre-Training under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret?
    A Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which leads to $\mathbb{P}(x_{1:N_{t}}{\,|\,}\ell_{1:t})=\mathbb{P}(x_{1:N_{t}}{\,|\,}\ell_{1:t},z)$.
    Finally, we can compute
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(z{\,&#124;\,}x_{1:N_{t}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{P}\left(z{\,&#124;\,}\left(\ell_{i}=f(x_{N_{i-1}+1:N_{i}})\right)_{i\in[t]}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Based on this, we conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(g(x_{N_{t}+1:N_{t+1}}){\,&#124;\,}x_{1:N_{t}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the first equality is by the independence between $x_{N_{t}+1:N_{t+1}}$,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle z\mid\ell_{1:t}\sim\mathcal{N}\left(\frac{1}{t+1}\sum_{i\in[t]}{\ell}_{i},\frac{\sigma^{2}}{t+1}I\right),$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: by the posterior distribution of Gaussian distribution. Now we conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}(a{\,&#124;\,}\ell_{1:t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\int_{z}\mathbbm{1}(a\in\operatorname*{arg\,min}_{a^{\prime}\in\mathcal{A}}z_{a^{\prime}})\mathbb{P}(z{\,&#124;\,}\ell_{1:t})dz=\int_{z}\mathbbm{1}\left(a\in\operatorname*{arg\,min}_{a^{\prime}\in\mathcal{A}}\left(\frac{\sigma}{\sqrt{t+1}}\epsilon+\frac{1}{t+1}\sum_{i\in[t]}\ell_{i}\right)_{a^{\prime}}\right)\mathbb{P}(\epsilon)d\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\int_{z}\mathbbm{1}\left(a\in\operatorname*{arg\,min}_{a^{\prime}\in\mathcal{A}}\left(\sigma\sqrt{t+1}\epsilon+\sum_{i\in[t]}\ell_{i}\right)_{a^{\prime}}\right)\mathbb{P}(\epsilon)d\epsilon=\mathbb{P}\left(a\in\operatorname*{arg\,min}_{a^{\prime}\in\mathcal{A}}\left(\sigma\sqrt{t+1}\epsilon+\sum_{i\in[t]}\ell_{i}\right)_{a^{\prime}}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=P_{quantal}^{\sigma\sqrt{t+1}}(a{\,&#124;\,}\ell_{1:t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{P}(\epsilon)=\mathcal{N}(\boldsymbol{0}_{d},I)$. This completes
    the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'C.3 Relationship between FTPL and [Definition 2](#Thmdefinition2 "Definition
    2 (Quantal response against multiple losses). ‣ 4.2 A Human Decision-Making Model:
    Quantal Response ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model
    and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fact 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Performing generalized quantal response of [Definition 2](#Thmdefinition2 "Definition
    2 (Quantal response against multiple losses). ‣ 4.2 A Human Decision-Making Model:
    Quantal Response ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model
    and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games") at every iteration $t\in[T]$ is essentially executing an
    FTPL algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before we move to the proof, we will define the random variable which has distribution
    $P_{\text{noise}}$ is fixed), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: which is exactly the case when $\epsilon_{t}$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'C.4 Deferred Proof of [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study:
    Pre-Training under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret?
    A Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'See [1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training under Canonical
    Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model
    and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\left\{q_{j}\in\left\{\mathcal{V}^{j-1}\rightarrow\Delta(\mathcal{V})\right\}\right\}_{j\in[N_{t+1}]}}\mathbb{E}_{x_{1:N_{t+1}}\sim
    P^{text}_{t}}\sum_{j=1}^{N_{t+1}}\log q_{j}\left(x_{j}{\,&#124;\,}x_{1:j-1}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\max_{q\in\Delta(\mathcal{V}^{N_{t+1}})}-\text{KL}(P_{t}^{text}{\,&#124;\,}{\,&#124;\,}q)+\mathbb{E}_{x_{1:N_{t+1}}\sim
    P^{text}_{t}}[P_{t}^{text}(x_{1:N_{t+1}})],$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\text{KL}(q{\,|\,}{\,|\,}p)$, i.e., it also defines a valid joint distribution
    over tokens. Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Now, due to our assumption that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: we conclude that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta\in\Theta}\text{KL}(P_{t}^{text}{\,&#124;\,}{\,&#124;\,}\text{LLM}_{\theta})=\min_{q\in\Delta(\mathcal{V}^{N_{t+1}})}\text{KL}(P_{t}^{text}{\,&#124;\,}{\,&#124;\,}q)=0,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which implies that $\text{LLM}_{\theta^{\star}}=P_{t}^{text}$, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'In other words, intuitively, $\text{LLM}_{\theta^{\star}}$, according to [Equation 12](#A3.E12
    "In Proof. ‣ C.3 Relationship between FTPL and Definition 2 ‣ Appendix C Deferred
    Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games") of [1](#Thmfact1 "Fact 1\. ‣ C.3 Relationship between FTPL
    and Definition 2 ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"), for which we
    can establish the corresponding regret guarantee for each case:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Combining the above result with [Lemma 2](#Thmlemma2 "Lemma 2 (Regret guarantee
    of FTPL with full-information feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem
    1 ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), we can derive the regret bound for
    online learning with full-information feedback.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Combining the above result with [Lemma 2](#Thmlemma2 "Lemma 2 (Regret guarantee
    of FTPL with full-information feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem
    1 ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") and [Lemma 4](#Thmlemma4 "Lemma 4\.
    ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs
    in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"), we get that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})\leq\min_{\Delta_{T}\in[T]}\frac{2T}{\Delta_{T}}C\sqrt{\Delta_{T}\log
    d}+2\Delta_{T}V_{T},$ |  |'
  prefs: []
  type: TYPE_TB
- en: for some constant $C$. We firstly consider the following problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\min_{u></math> |  |'
  prefs: []
  type: TYPE_TB
- en: where the optimal solution is $u^{\star}=\left(\frac{C^{2}T^{2}\log d}{4V_{T}^{2}}\right)^{1/3}$,
    which results in a regret bound of
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})\leq\frac{2T}{\sqrt{u^{\star}}}C\sqrt{\log
    d}+4u^{\star}V_{T}=\mathcal{O}\left((\log d\ V_{T})^{1/3}T^{2/3}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Now we check the conditions for $u^{\star}\in[1,T]$ is large enough.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Combining the above result with [Lemma 3](#Thmlemma3 "Lemma 3 (Regret guarantee
    of FTPL with bandit feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), we can prove a regret guarantee for online
    learning with bandit feedback.
  prefs: []
  type: TYPE_NORMAL
- en: (4) Combining this result with [Lemma 3](#Thmlemma3 "Lemma 3 (Regret guarantee
    of FTPL with bandit feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") and [Lemma 4](#Thmlemma4 "Lemma 4\. ‣ Proof.
    ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs in
    Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"),
    it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})]\leq\min_{\Delta_{T}\in[T]}\frac{2T}{\Delta_{T}}C(\log
    d)^{\frac{1}{2}}d\Delta_{T}^{\frac{1}{2}+\frac{1}{\log T}}\log\Delta_{T}+2\Delta_{T}V_{T},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'for some constant $C$ and derive the following regret:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})]\leq\mathcal{O}\left((T^{2}d^{2}V_{T})^{1/3}(\log
    d)^{1/2}T^{1/\log T}\log T\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Now we check the condition of $u^{\star}\in[1,T]$ is large enough.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we present [Lemma 2](#Thmlemma2 "Lemma 2 (Regret guarantee of FTPL with
    full-information feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") - [Lemma 4](#Thmlemma4 "Lemma 4\. ‣ Proof.
    ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs in
    Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    Before proceeding, we assume $\|\ell_{t}\|_{\infty}\leq B=1$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 2  (Regret guarantee of FTPL with full-information feedback).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose the noise distribution of FTPL satisfies that $\epsilon_{t}\sim\mathcal{N}(\boldsymbol{0}_{d},\zeta_{t}^{2}I)$,
    then for online learning with full-information feedback,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})\leq
    4\left(\sigma+\frac{1}{\sigma}\right)\sqrt{T\log d}=\mathcal{O}(\sqrt{T\log d}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By Theorem 8 of Abernethy et al. ([2014](#bib.bib1)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, plugging $\zeta_{t}=\sigma\sqrt{t}$ provides
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: completing the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 3  (Regret guarantee of FTPL with bandit feedback).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose the noise distribution of FTPL satisfies that $\epsilon_{t}\sim\mathcal{N}(\boldsymbol{0}_{d},\zeta_{t}^{2}I)$,
    then for online learning with bandit feedback,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The proof of the bandit problem is more complex. We first define the following
    notation. We denote $G_{t}=\sum_{t^{\prime}=1}^{t}-\ell_{t^{\prime}}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Phi(G_{T})=\Phi(\mathbb{E}[\widehat{G}_{T}])\leq\mathbb{E}[\Phi(\widehat{G}_{T})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}[\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: By recalling the definition of the Bregman divergence, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle-\sum_{t=1}^{T}\langle\pi_{t},-\widehat{\ell}_{t}\rangle$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\left[\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: $(iii)\leq 0$, we use Lemma 10 of Abernethy et al. ([2014](#bib.bib1)) to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'For $(i)$, the following holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By tuning $\alpha=\frac{2}{\log T}$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Denote the variation of loss vectors as $L_{T}=\sum_{t=1}^{T-1}\|\ell_{t+1}-\ell_{t}\|_{\infty}$
    that can achieve
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, suppose there exists an algorithm $\mathscr{B}$ that can achieve
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We denote $\mathscr{A}^{\prime}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{j\in[d]}\left(\sum_{t\in{\mathcal{T}}_{k}}\ell_{t}\right)_{j}-\sum_{t\in{\mathcal{T}}_{k}}\ell^{\star}_{t}\leq
    2\Delta_{T}L_{k},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where we define $L_{k}=\sum_{t\in{\mathcal{T}}_{k}}\|\ell_{t+1}-\ell_{t}\|_{\infty}$.
    Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{D-Regret}_{\mathscr{A}^{\prime}}(({\ell}_{i})_{i\in[T]})$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq 2\Delta_{T}(\sum_{k\in[m]}L_{k})+(T/\Delta_{T}+1)g(\Delta_{T},d).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: By Equation (4) of Besbes et al. ([2014](#bib.bib17)) that $\sum_{k\in[m]}L_{k}\leq
    L_{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if we take the expectation for [Equation 13](#A3.E13 "In Proof. ‣
    Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs
    in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"), it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\mathscr{B}^{\prime}}(({\ell}_{i})_{i\in[T]})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\min_{\Delta_{T}\in[T]}\left(\frac{T}{\Delta_{T}}+1\right)g(\Delta_{T},d)+2\Delta_{T}L_{T},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: thus completing the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining the results above completes the proof for [Theorem 1](#Thmtheorem1
    "Theorem 1\. ‣ 4.3 Case Study: Pre-Training under Canonical Data Distribution
    ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    ∎'
  prefs: []
  type: TYPE_NORMAL
- en: 'C.5 Extending [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") with Relaxed Assumptions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.5.1 Relaxation under More General Data Distributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We first remark on the possibility of relaxing the Gaussian assumptions on the
    data distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3  (Relaxing the Gaussian distribution assumption).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the proof of [Lemma 1](#Thmlemma1 "Lemma 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), to obtain the result that the action
    is a quantal response w.r.t. $\ell_{1:T}$, as long as its posterior distribution
    satisfies [Equation 11](#A3.E11 "In Proof. ‣ C.2 Deferred Proof of Lemma 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), it would suffice. It is a combined effect
    of both the prior and the conditional distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, we can extend [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case
    Study: Pre-Training under Canonical Data Distribution ‣ 4 Why Are Pre-Trained
    LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games") to the case with
    a much more general prior task distribution than the Gaussian one, where the key
    is that [Equation 11](#A3.E11 "In Proof. ‣ C.2 Deferred Proof of Lemma 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") only needs to hold approximately.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training under
    Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"), we can relax the assumption on $\mathbb{P}(z)$.'
  prefs: []
  type: TYPE_NORMAL
- en: The key idea of the proof is that when $t$ is large enough, the prior distribution
    does not affect the posterior distribution, which is also referred to as the *Bernstein–von
    Mises theorem* (Van der Vaart, [2000](#bib.bib101)).
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since we extend [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") to settings with general task prior
    distribution only requiring the coordinates to be i.i.d, from now on, we consider
    the $j$, we define the log-likelihood of the posterior as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Then, the MLE estimator $\widehat{z}_{j,t}$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widehat{z}_{j,t}:=\arg\max_{z_{j}\in\mathbb{R}}L_{t}(z_{j})=\frac{1}{t}\sum_{i=1}^{t}\ell_{ij}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'We also define $\widehat{J}_{t}:\mathbb{R}\to\mathbb{R}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widehat{J}_{t}(z_{j}):=-\frac{\nabla^{2}L_{t}(z_{j})}{t}=\frac{1}{\sigma^{2}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For Assumption 1 of Kasprzak et al. ([2022](#bib.bib55)) to hold, any <math
    id=$$ suffices.
  prefs: []
  type: TYPE_NORMAL
- en: For Assumption 2 of Kasprzak et al. ([2022](#bib.bib55)) to hold, we can choose
    $\widehat{M}_{1}=\max_{z_{j}\in[-\delta,1+\delta]}\frac{1}{\mathbb{P}(z_{j})}$
  prefs: []
  type: TYPE_NORMAL
- en: For Assumption 7 of Kasprzak et al. ([2022](#bib.bib55)) to hold, we choose
    $\delta$.
  prefs: []
  type: TYPE_NORMAL
- en: For Assumption 8 of Kasprzak et al. ([2022](#bib.bib55)) to hold, one can choose
    $M_{2}=\frac{\sigma}{2}$.
  prefs: []
  type: TYPE_NORMAL
- en: For Assumption 9 of Kasprzak et al. ([2022](#bib.bib55)) to hold, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For Assumption 10 of Kasprzak et al. ([2022](#bib.bib55)) to hold, we choose
    $M_{1}=\sup_{z_{j}\in[-\delta,1+\delta]}\left|\frac{\nabla\mathbb{P}(z_{j})}{\mathbb{P}(z_{j})}\right|$.
  prefs: []
  type: TYPE_NORMAL
- en: By Theorem 6.1 of Kasprzak et al. ([2022](#bib.bib55)), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\int_{z_{j}}&#124;$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sqrt{t}\int_{z_{j}}&#124;\mathbb{P}(z_{j}{\,&#124;\,}(\ell_{ij})_{i\in[t]})-\mathcal{N}(\widehat{z}_{j},\frac{\sigma^{2}}{t})&#124;dz_{j}\leq
    D_{1}t^{-1/2}+D_{2}t^{1/2}e^{-t\kappa}+2\widehat{\mathcal{D}}(t,\delta),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $C$ is the normalization constant and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D_{1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle D_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\widehat{\mathcal{D}}(t,\delta)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\widehat{J}_{t}^{p}(\widehat{z}_{j},\delta)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we conclude that the TV distance between $z$ satisfies that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\int_{z}\bigg{&#124;}\mathbb{P}(z{\,&#124;\,}(\ell_{i})_{i\in[t]})-\mathcal{N}\left(\widehat{z},\frac{\sigma^{2}}{t}\right)\bigg{&#124;}dz\leq\sum_{j=1}^{d}\int_{z_{j}}\bigg{&#124;}\mathbb{P}(z_{j}{\,&#124;\,}(\ell_{ij})_{i\in[t]})-\mathcal{N}\left(\widehat{z}_{j},\frac{\sigma^{2}}{t}\right)\bigg{&#124;}dz_{j}\leq\mathcal{O}(d/t),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: due to the independence of $(z_{j})_{j\in[d]}$. Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\quad\leq d\sum_{t=1}^{T}\int_{z}\big{&#124;}\mathbb{P}(z{\,&#124;\,}(\ell_{i})_{i\in[t]})-\mathcal{N}(\widehat{z},\frac{\sigma^{2}}{t})\big{&#124;}dz=\mathcal{O}(d^{2}\log
    T).$ |  |'
  prefs: []
  type: TYPE_TB
- en: In other words, using $\mathbb{P}(z{\,|\,}(\ell_{i})_{i\in[t]})$. Similarly,
    it is easy to see that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which completes the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: C.5.2 Relaxation under Decision-Irrelevant Pre-Training Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We then remark on the possible relaxation when the training data may not all
    come from decision-making tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 4  (Pre-training with relaxed data assumptions).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note that the pre-training (text) data are so far assumed to be related to
    *decision-making* problems (though not necessarily *sequential ones*), see [1](#Thmassumption1
    "Assumption 1\. ‣ 4.3 Case Study: Pre-Training under Canonical Data Distribution
    ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    and [Example 1](#Thmexample1 "Example 1 (An example instantiating 1). ‣ 4.3 Case
    Study: Pre-Training under Canonical Data Distribution ‣ 4 Why Are Pre-Trained
    LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games") for instance.
    It can also be generalized to the text datasets involving *Question-Answering*
    (Q-A), a typical task in natural language processing, where the *true/fact* answer,
    sampled answers from different human users (with possibly wrong or biased answers),
    correspond to the latent $z$, respectively. Moreover, in practice, the pre-training
    data may also involve *non-decision-making/Q-A* texts, given the diversity of
    the datasets. For such scenarios, we will make the assumptions on the data distribution
    *conditioned on the prompt for decision-making*. Specifically, when interacting
    with the LLM, human users will provide prompts (see e.g., our [Figure 3](#S3.F3
    "In Extension to bandit-feedback settings. ‣ 3.2 Results: Online Learning ‣ 3
    Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games")), to induce it to make decisions.
    This will query the *conditional* distribution of'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{P}\left(g(x_{N_{t}+1:N_{t+1}}){\,\big{&#124;}\,}x_{1:N_{t}},\text{decision-making
    prompt}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'to generate the control action. Correspondingly, [1](#Thmassumption1 "Assumption
    1\. ‣ 4.3 Case Study: Pre-Training under Canonical Data Distribution ‣ 4 Why Are
    Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") will
    thus only need to be made on'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{P}\left(z,\ell_{1:t},x_{1:N_{t+1}},\text{decision-making prompt}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: while we do not need to make such assumptions on other prompts, e.g., corpora
    that are not related to decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Deferred Results and Proofs in [Section 5](#S5 "5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 Basic Lemmas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lemma 5  (Double iterated limit).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For a sequence $(a_{mn})_{m,n\in\mathbb{N}^{+}}$. Then the following are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each $m$ exists;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\lim_{m\to\infty}\lim_{n\to\infty}a_{mn}=L$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lemma 6  (Hoeffding’s inequality).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $X_{1},X_{2},\ldots,X_{n}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{P}(&#124;\bar{X}-\mu&#124;\geq t)\leq 2\exp\left(-\frac{2n^{2}t^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Lemma 7  (Uniform convergence $\Longrightarrow$ Interchanging limit and infimum).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If $(f_{n}:X\to\mathbb{R})_{n\in\mathbb{N}^{+}}$ holds.
  prefs: []
  type: TYPE_NORMAL
- en: 'D.2 Deferred Proof for the Arguments in [Section 5.1](#S5.SS1 "5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we prove some properties of $\mathcal{L}(\theta,k,N)$. Throughout
    this subsection, we will assume the following condition holds.
  prefs: []
  type: TYPE_NORMAL
- en: Condition 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For $h:\mathbb{R}\to\mathbb{R}^{+}$.
  prefs: []
  type: TYPE_NORMAL
- en: Claim 1  (Iterated limit of $\mathcal{L}(\theta,k,N)$).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It holds that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Step 1. Proving $1$2.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, as both $h$ exists, we have by dominated convergence theorem that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\lim_{k\to\infty}\mathcal{L}(\theta,k,N)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $R_{\text{LLM}_{\theta}}$ holds.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2. Proving $1$2.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will calculate $\lim_{N,k\to\infty}\mathcal{L}(\theta,k,N)$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 8.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $0<\epsilon<1$, it follows that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: hold with probability $1$ is a continuous non-decreasing function.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of [Lemma 8](#Thmlemma8 "Lemma 8\. ‣ Proof. ‣ D.2 Deferred Proof for the
    Arguments in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since $f(\cdot,k),H$ are non-negative and non-decreasing functions, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and we know that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\frac{&#124;\{i\in[N]\mid(H(X_{i})<1-\epsilon)\}&#124;}{&#124;\{i\in[N]\mid(H(X_{i})></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: as $N\to\infty$. Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\underset{a.s.}{\leq}\lim_{N,k\to\infty}\frac{(1-\epsilon)f(H^{-1}(1-\epsilon),k))}{(1-\epsilon/2)f(H^{-1}(1-\epsilon/2),k))}\frac{F(1-\epsilon)}{1-F(1-\epsilon/2)}=0.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: By a similar argument, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: with probability 1. ∎
  prefs: []
  type: TYPE_NORMAL
- en: One key idea in the proof above is the use of some *truncation* level $\epsilon$.
    By [Lemma 8](#Thmlemma8 "Lemma 8\. ‣ Proof. ‣ D.2 Deferred Proof for the Arguments
    in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: since
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: holds with probability 1. Therefore, for any $0<\epsilon<1$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=h\left(\max_{\ell_{1},\dots,\ell_{T}}R_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq(1-\epsilon)h(\max_{\ell_{1},\dots,\ell_{T}}R_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which implies $1$2 since
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\theta,k,N)\leq h\left(\max_{\ell_{1},\dots,\ell_{T}}\text{Regret}_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: by definition of $\mathcal{L}$ to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\theta,k,N)\geq h\left(\max_{\ell_{1},\dots,\ell_{T}}\text{Regret}_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]})\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, we used the fact that $(\ell_{t})_{t\in[T]}$ ([1](#Thmcondition1 "Condition
    1\. ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")), which lead to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '[Equation 14](#A4.E14 "In Proof. ‣ D.2 Deferred Proof for the Arguments in
    Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") will be used frequently
    in the overall proof in [Section D.2](#A4.SS2 "D.2 Deferred Proof for the Arguments
    in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: Step 3. Proving $1$2.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if $N\to\infty$, similarly by dominated convergence theorem we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\lim_{N\to\infty}\mathcal{L}(\theta,k,N)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, $\lim_{N\to\infty}\mathcal{L}(\theta,k,N)$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Claim 2  (Uniform convergence of $\mathcal{L}(\theta,k,N)$)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: $\mathcal{L}(\theta,k,N)$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We will provide a similar analysis as [Lemma 8](#Thmlemma8 "Lemma 8\. ‣ Proof.
    ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 9.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $0<\epsilon<1$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: with probability at least $1-\delta$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of [Lemma 9](#Thmlemma9 "Lemma 9\. ‣ Proof. ‣ D.2 Deferred Proof for the
    Arguments in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the same argument as the proof of [Lemma 8](#Thmlemma8 "Lemma 8\. ‣ Proof.
    ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: It holds that $\frac{1}{N}|\{i\in[N]\mid(H(X_{i})<1-\epsilon)\}|=F_{H,X}(1-\epsilon)+\widetilde{\mathcal{O}}(1/\sqrt{N})$.
    Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: with probability at least $1-\delta$. Finally, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\frac{\sum_{i=1}^{N}f(X_{i},k)\mathbbm{1}(H(X_{i})<1-\epsilon)}{\sum_{i=1}^{N}f(X_{i},k)\mathbbm{1}(H(X_{i})></math>
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Note that $\lim_{k\to\infty}A(k,H,\epsilon)=0$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where we recall the shorthand notation of $R_{\text{LLM}_{\theta}}=\text{Regret}_{\text{LLM}_{\theta}}$
    in [Lemma 9](#Thmlemma9 "Lemma 9\. ‣ Proof. ‣ D.2 Deferred Proof for the Arguments
    in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"). Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 1$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: with probability at least $1-\delta$.
  prefs: []
  type: TYPE_NORMAL
- en: Now, for any <math id=$$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle 0$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Note that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: is a continuous function of $\theta$. Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: and we know that $\lim_{N,k\to\infty}1+A(k,h,\epsilon)(\frac{1}{p(\epsilon)}+\widetilde{\mathcal{O}}(\sqrt{1/N}))=1$.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: Claim 3  (Double iterated limit of supremum).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It holds that:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since $1$2, we will prove
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Lemma 10.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: $\frac{\sum_{i=1}^{N}f(X_{i},k_{1})h(X_{i})}{\sum_{i=1}^{N}f(X_{i},k_{1})}\leq\frac{\sum_{i=1}^{N}f(X_{i},k_{2})h(X_{i})}{\sum_{i=1}^{N}f(X_{i},k_{2})}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By multiplying $({\sum_{i=1}^{N}f(X_{i},k_{1})})({\sum_{i=1}^{N}f(X_{i},k_{2})})$.
    This is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which is true since if $X_{i}\geq X_{j}$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, $\mathcal{L}(\theta,k,N)$ is fixed, which indicates that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: exists, as $\mathcal{L}(\theta,k,N)$ is also bounded. Therefore, by [Lemma 5](#Thmlemma5
    "Lemma 5 (Double iterated limit). ‣ D.1 Basic Lemmas ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") and [2](#Thmclaim2 "Claim 2 (Uniform convergence of ℒ⁢(𝜃,𝑘,𝑁) (with
    respect to 𝑘 and 𝑁)). ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣
    Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), we know that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: exists and this value should be 0. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Claim 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Firstly, by [Lemma 7](#Thmlemma7 "Lemma 7 (Uniform convergence ⟹ Interchanging
    limit and infimum). ‣ D.1 Basic Lemmas ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"), we have $\lim_{N,k\to\infty}\inf_{\theta\in\Theta}\mathcal{L}(\theta,k,N)=\inf_{\theta\in\Theta}h(\max_{\ell_{1},\dots,\ell_{T}}\text{Regret}_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]}))$.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Deferred Proofs of [Theorem 2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees
    via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    and [Corollary 1](#Thmcorollary1 "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See [2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees via Regret-Loss Minimization
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")
  prefs: []
  type: TYPE_NORMAL
- en: Before proving the theorem, we remark on what LLM structure enjoys the Lipschitz-continuity.
    We provide two auxiliary results in the following proposition. The first result
    is from (Bai et al., [2023](#bib.bib13), Section J.1), which is about the Lipschitzness
    of Transformers. The second result is regarding processing the output of Transformers.
    In particular, the output of Transformers is usually not directly used, but passed
    through some matrix multiplication (by some matrix $A$), followed by some projection
    Operator (to be specified later).
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The $L$, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\&#124;\texttt{TF}_{\theta_{1}}(Z)-\texttt{TF}_{\theta_{2}}(Z)\&#124;_{2,\infty}\leq
    C_{\texttt{TF}}\&#124;\theta_{1}-\theta_{2}\&#124;_{\texttt{TF}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\|\cdot\|_{\texttt{TF}}$, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Here, Operator is either the projection operator onto some convex set, or the
    Softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first result is from (Bai et al., [2023](#bib.bib13), Section J.1). The
    second result comes from
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Operator is a projection onto the convex set, then $\|\texttt{Operator}(x)-\texttt{Operator}(y)\|_{2}\leq\|x-y\|_{2}$;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Operator is Softmax, then $\|\texttt{Softmax}(x)-\texttt{Softmax}(y)\|_{2}\leq\|x-y\|_{2}$
    (Gao and Pavel, [2017](#bib.bib43), Corollary 3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that the only condition that we require for Operator is its non-expansiveness.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: Proof of [Theorem 2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $C_{\text{LLM}}$ above in [Proposition 2](#Thmproposition2 "Proposition
    2\. ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")). Now, we prove that regret is also a Lipschitz-continuous function
    with respect to the LLM’s parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 11  (Lipschitzness of regret).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The function $\text{Regret}_{\text{LLM}_{\theta}}$, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Big{&#124;}\emph{Regret}_{\text{LLM}_{\theta_{1}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By definition, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Big{&#124;}\text{Regret}_{\text{LLM}_{\theta_{1}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=B\sum_{t=1}^{T}\&#124;\text{LLM}_{\theta_{1}}(Z_{t-1})-\text{LLM}_{\theta_{2}}(Z_{t-1})\&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq BC_{\text{LLM}}T\&#124;\theta_{1}-\theta_{2}\&#124;_{\text{LLM}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{t}:=(\ell_{1},\dots,\ell_{t},c)$-dimensional vector. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will prove the Lipschitzness of
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: with respect to the model parameter $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: Claim 5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $$R></math>, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: for every $x,y\in\mathbb{R}^{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If $\beta=\infty$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\lim_{\beta\to\infty}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'holds. Moreover, consider the following constrained optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\text{subject to }\qquad&#124;x_{i}&#124;\leq R,~{}~{}~{}&#124;y_{i}&#124;\leq
    R~{}~{}~{}\text{ for all }i\in[N],$ |  |'
  prefs: []
  type: TYPE_TB
- en: whose optimum is denoted as $F(R,\beta)$. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Note that [5](#Thmclaim5 "Claim 5\. ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") does
    not hold if either $x_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the domain of $h:\mathbb{R}\to\mathbb{R}^{+}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 12  (Lipschitzness of $C$ in [Equation 16](#A4.E16 "In Proof of Theorem
    2\. ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The function $C$, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\big{&#124;}C(($ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Here, (i) holds due to [5](#Thmclaim5 "Claim 5\. ‣ Proof of Theorem 2\. ‣ D.3
    Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), (ii) holds since $h$, and (iii) holds due to [Lemma 11](#Thmlemma11
    "Lemma 11 (Lipschitzness of regret). ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"). ∎
  prefs: []
  type: TYPE_NORMAL
- en: For completeness of the paper, we provide the definition of covering set and
    covering number.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4  (Covering set and covering number).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For <math id=$$ is defined as the minimum cardinality of any covering set.
  prefs: []
  type: TYPE_NORMAL
- en: By (Wainwright, [2019](#bib.bib104), Example 5.8), for any $$r></math> can be
    bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log N(\delta;B(0,r,\&#124;\cdot\&#124;_{\text{LLM}}),\&#124;\cdot\&#124;_{\text{LLM}})\leq
    d_{\theta}\log(1+2r/\delta),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $d_{\theta}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log N(\delta;B(0,r,\&#124;\cdot\&#124;_{\text{LLM}}),\&#124;\cdot\&#124;_{\text{LLM}})\leq
    L(3Md^{2}+2d(dd^{\prime}+3md^{2}))\log(1+2r/\delta).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since we consider a compact $\Theta$ with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Then, by the standard result from statistical learning theory (Wainwright, [2019](#bib.bib104),
    Chapter 5), when trained with $N_{T}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}(\widehat{\theta}_{k,N,N_{T}},k,N)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Setting $\delta=\Omega(\sqrt{\log(\epsilon)/N_{T}})$, we further obtain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}(\widehat{\theta}_{k,N,N_{T}},k,N)$ |  |'
  prefs: []
  type: TYPE_TB
- en: with probability at least $1-\epsilon$, completing the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: See [1](#Thmcorollary1 "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss Minimization
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The limit on the right-hand side of [Equation 5](#S5.E5 "In Theorem 2\. ‣ 5.2
    Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") remains as $\widetilde{\mathcal{O}}\left(\sqrt{\frac{d_{\theta}+\log(1/\epsilon)}{N_{T}}}\right)$.
    Next, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: due to the continuity of $h$ and [3](#Thmclaim3 "Claim 3 (Double iterated limit
    of supremum). ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"). Finally, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: due to [4](#Thmclaim4 "Claim 4\. ‣ D.2 Deferred Proof for the Arguments in Section
    5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games"), which, combined with the
    fact that $h$ is non-decreasing, completes the proof. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Remark 5  (Dynamic-regret loss).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So far, we have focused on the canonical online learning setting with regret
    being the metric. One can also generalize the results to the non-stationary setting,
    with dynamic regret being the metric. Specifically, one can define the *dynamic-regret-loss*
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Then, one can also establish similar results as before, since the analysis does
    not utilize other properties of the regret except its boundedness, and the Lipschitz-continuity
    of LLM with respect to $\theta$. To be specific, [Lemma 11](#Thmlemma11 "Lemma
    11 (Lipschitzness of regret). ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs of
    Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") holds
    due to the reason that we can bound the difference of the regret with the term
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: as well as the fact that $\inf_{\pi_{i}\in\Pi}\langle\ell_{i},\pi_{i}\rangle$
    will be canceled. One can verify that all the arguments in [Section D.2](#A4.SS2
    "D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") also hold for similar reasons.
  prefs: []
  type: TYPE_NORMAL
- en: D.4 Deferred Proof of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See [3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We will locally use $\mathcal{A}=[d]$, the loss function ([Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")) can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where for $t=1$.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1. Calculating $\frac{\partial f}{\partial a}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For $x\in[d]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Plugging $a=\boldsymbol{0}_{d}$ provides
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial a_{x}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'For $t=1$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial a_{x}}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=0,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where we used the fact that $\ell_{i}$ yields the same distribution, which
    leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This yields [Equation 17](#A4.E17 "In Proof. ‣ D.4 Deferred Proof of Theorem
    3 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")=0.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2. Calculating $\frac{\partial f}{\partial v_{c}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following equation for $t\geq 2$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial v_{c}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'For $t=1$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial v_{c}}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The last line is due to the same reason as the last part of Step 1.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3. Calculating $\frac{\partial f}{\partial V}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate the following equation, which will be used to calculate $\frac{\partial
    f}{\partial V}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial V}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For $t=1$.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial V}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, if $V^{\star}=R_{\Pi}\frac{T}{\sum_{t=1}^{T-1}1/t}\Sigma^{-1}\mathbb{E}\Bigg{[}\|\sum_{t=1}^{T}\ell_{t}\|_{2}\ell_{t}\ell_{i}^{\intercal}\Bigg{]}\Sigma^{-1}$.
    Lastly, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'which means that such configurations are first-order stationary points of [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") with $N=1$. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: D.5 Deferred Proof of [Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: See [4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output of the single-layer linear self-attention structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g($ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which can be expressed with a larger class
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g(Z_{t},\mathbb{A},\beta,\mathbb{C},\delta):=\sum_{i=1}^{t}(\mathbb{A}\ell_{i}\ell_{i}^{\intercal}\beta+\mathbb{C}\ell_{i}+\delta),$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{A}\in\mathbb{R}^{d\times d}$. Then, if a minimizer of
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(\mathbb{A},\beta,\mathbb{C},\delta):$ |  |'
  prefs: []
  type: TYPE_TB
- en: can be expressed as $\mathbb{A}=V,\beta=K^{\intercal}(Qc+q_{c}),\mathbb{C}=Vk_{c}^{\intercal}(Qc+q_{c})+v_{c}(Qc+q_{c})^{\intercal}K,\beta=v_{c}k_{c}^{\intercal}(Qc+q_{c})$
    are also a minimizer of
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: since the corresponding $V,Q,K,v_{c},q_{c},k_{c}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: Step 1. Finding condition for $\frac{\partial f}{\partial\delta}=0$.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the Leibniz rule, if we calculate the partial derivative of [Equation 20](#A4.E20
    "In Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") w.r.t. $\delta$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: Since the expectation of either odd-order polynomial or even-order polynomial
    times $\|\cdot\|_{2}$ follows a symmetric distribution, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Now, we calculate
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\sum_{t=1}^{T}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $(i)$. Lastly,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\sum_{t=1}^{T}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Plugging the above equations into [Equation 21](#A4.E21 "In Proof. ‣ D.5 Deferred
    Proof of Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do
    LLM Agents Have Regret? A Case Study in Online Learning and Games"), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial f(\mathbb{A},\beta,\mathbb{C},\delta)}{\partial\delta}=\frac{1}{6}T(2T^{2}-3T+1)(\Sigma\mathbb{A}\Sigma\beta+\Sigma\delta).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Due to the optimality condition, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{A}\Sigma\beta+\delta=0.$ |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: Step 2. Plugging the optimality condition for $\frac{\partial f}{\partial\delta}$
    into [Equation 20](#A4.E20 "In Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games").
  prefs: []
  type: TYPE_NORMAL
- en: Plugging [Equation 22](#A4.E22 "In Proof. ‣ D.5 Deferred Proof of Theorem 4
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") to [Equation 20](#A4.E20 "In Proof.
    ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"),
    $f$ can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(\mathbb{A},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: For the part $(i)$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $(1)$.
  prefs: []
  type: TYPE_NORMAL
- en: If $\mathbb{A}\neq\bm{O}_{d\times d}$, and is a non-zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will generally consider $a_{x,y}(v):=vv^{\intercal}x-y$ can be calculated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id=$$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, $\text{Volume}(V_{1}(vv^{\intercal}-\Sigma))$ should hold. In both
    cases, [Equation 19](#A4.E19 "In Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") can be re-written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g(Z_{t};\mathbb{A},\beta,\mathbb{C},\delta):=\sum_{i=1}^{t}\mathbb{C}\ell_{i},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and this is covered by the original parametrization ([Equation 18](#A4.E18 "In
    Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")) with $K^{\intercal}(Qc+q_{c})=v_{c}=\boldsymbol{0}_{d}$.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3. Calculating $\frac{\partial f}{\partial\mathbb{C}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we optimize over $\mathbb{C}$, by minimizing the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(\mathbb{C}):$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $(i)$ can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: since (1) holds because if $t_{1}\neq t$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate $\frac{\partial f(\mathbb{C})}{\partial\mathbb{C}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Hence, the optimal $1$2
  prefs: []
  type: TYPE_NORMAL
- en: Now, we see that for the special case of $\Sigma=I$, we need to calculate
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{\ell}\left[\sqrt{\sum_{o=1}^{d}(\sum_{s=1}^{T}\ell_{so})^{2}}\ell_{ia}\ell_{kb}\right].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: If $a\neq b$’s coordinates are independent.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we calculate the scale of $\mathbb{E}_{\ell}\left[\sqrt{\sum_{o=1}^{d}(\sum_{s=1}^{T}\ell_{so})^{2}}\ell_{i1}\ell_{k1}\right]$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{\ell}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Taking $d\to\infty$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which further implies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: as $d\to\infty$. Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which is a constant. The last equality came from the fact that $W$ (Hazan, [2016](#bib.bib47),
    Theorem 3.1), which is consistent with the result above. ∎
  prefs: []
  type: TYPE_NORMAL
- en: D.6 Empirical Validation of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") and [Theorem 4](#Thmtheorem4 "Theorem
    4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning
    Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now provide empirical validations for [Theorem 3](#Thmtheorem3 "Theorem 3\.
    ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") and [Theorem 4](#Thmtheorem4
    "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online
    Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    We provide the training details and the results as follows.
  prefs: []
  type: TYPE_NORMAL
- en: D.6.1 Empirical Validation of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our model architecture is defined as follows: the number of layers $T$) as
    zero vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our empirical analysis aims to demonstrate that the optimized model inherently
    emulates online gradient descent. To illustrate this, we will focus on two key
    convergence properties: $K^{\intercal}Q$. The results are demonstrated in the
    first plot of [Figure 15](#A4.F15 "In D.6.2 Empirical Validation of Theorem 4
    ‣ D.6 Empirical Validation of Theorem 3 and Theorem 4 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: D.6.2 Empirical Validation of [Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now focus on two key convergence properties: $K^{\intercal}(Q\boldsymbol{1}_{d}+q_{c})$.
    The results are demonstrated in the second plot of [Figure 15](#A4.F15 "In D.6.2
    Empirical Validation of Theorem 4 ‣ D.6 Empirical Validation of Theorem 3 and
    Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a9731f09c265329c7c42de3203dd694.png)![Refer to caption](img/195e130ed05eaf1b85b007dd662def5c.png)![Refer
    to caption](img/7c4ecbd4c3c8185a1017a5f7a81d8e65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Empirical validation of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣
    5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") (top), [Theorem 4](#Thmtheorem4
    "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online
    Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    (middle), and [6](#Thmtheorem6 "Conjecture 6\. ‣ D.7 Discussions on the Production
    of FTRL with Entropy Regularization ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    (bottom). The observed convergence in [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣
    5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") and [6](#Thmtheorem6
    "Conjecture 6\. ‣ D.7 Discussions on the Production of FTRL with Entropy Regularization
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")’s result suggests that configuration
    in [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") and [6](#Thmtheorem6 "Conjecture 6\. ‣ D.7 Discussions on the Production
    of FTRL with Entropy Regularization ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    are not only the local optimal point, but it has the potential as being the global
    optimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: D.7 Discussions on the Production of FTRL with Entropy Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will consider projecting a single-layer linear self-attention model
    into a constrained domain such as a simplex, which is more amenable to the Experts
    Problem setting. To this end, we consider the following parameterization by adding
    an additional *non-linear* structure for the single-layer linear self-attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle g(Z_{t};V,K,Q,v_{c},k_{c},q_{c})=\texttt{Operator}\left(\sum_{i=1}^{t}(V\ell_{i}+v_{c})((K\ell_{i}+k_{c}))^{\intercal}\cdot(Qc+q_{c}))\right),$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: where the Operator denotes projection to the convex set.
  prefs: []
  type: TYPE_NORMAL
- en: Conjecture 6.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume $\Sigma=I$. This configuration performs FTRL with an entropy regularizer
    which is a no-regret algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We provide an idea for proving the conjecture, together with its numerical validation.
    Also, we have observed in [Figure 15](#A4.F15 "In D.6.2 Empirical Validation of
    Theorem 4 ‣ D.6 Empirical Validation of Theorem 3 and Theorem 4 ‣ Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games") that [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") and [6](#Thmtheorem6 "Conjecture 6\.
    ‣ D.7 Discussions on the Production of FTRL with Entropy Regularization ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") might also be a global optimizer, as training
    results have provided the configuration that [Theorem 3](#Thmtheorem3 "Theorem
    3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning
    Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [6](#Thmtheorem6
    "Conjecture 6\. ‣ D.7 Discussions on the Production of FTRL with Entropy Regularization
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") have suggested.
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, we will consider
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(V,a,\beta,v_{c})$ |  |'
  prefs: []
  type: TYPE_TB
- en: and will try to prove that $a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI$
    is a first-order stationary point.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1. Calculating $\frac{\partial f}{\partial v_{c}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the following formula: for $x\in[d]$, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=t\beta\exp(v\beta)\exp(\beta k\sum_{i=1}^{t}\ell_{iy}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and for $t=1$. Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\beta\exp(v\beta)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial v_{cx}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Step 2. Calculating $\frac{\partial f}{\partial V}$.
  prefs: []
  type: TYPE_NORMAL
- en: The following formula will be used for calculating $\frac{\partial f}{\partial
    V}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial V_{rc}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial V_{rc}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}+v\beta\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta V\ell_{jy}+v\beta\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\beta\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta V\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'We can observe the followings: 1) if $r_{1}\neq c_{1}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 3. Calculating $\frac{\partial f}{\partial\beta}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula will be used for calculating $\frac{\partial f}{\partial\beta}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial\beta}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=tv\beta\exp\left(\sum_{i=1}^{t}k\beta\ell_{iy}+v\beta\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Further, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=v\beta\exp(v\beta)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Step 4. Calculating $\frac{\partial f}{\partial a}$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial}{\partial a_{x}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial a_{x}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\Biggl{(}\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\sum_{j=1}^{t-1}(k\ell_{js}\ell_{jx}+v\ell_{jx})\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}k\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\Biggl{(}\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\sum_{j=1}^{t-1}\ell_{js}\ell_{jx}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\qquad\qquad-\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\left(\sum_{j=1}^{t-1}\ell_{jy}\ell_{jx}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}\Biggr{)}\Bigg{]}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Note that the value does not depend on $x$.
  prefs: []
  type: TYPE_NORMAL
- en: D.7.1 Numerical Analysis of Step 2 and Step 4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Steps 2 and 4 above, we were not able to show that a $k$ times.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b96f11c4ffae829dd34e68270baa95c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Calculation of $20\frac{\partial f}{\partial V_{rc}}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    would coincide.'
  prefs: []
  type: TYPE_NORMAL
- en: D.7.2 Empirical Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our model architecture is defined as follows: the number of layers $T$. The
    results are demonstrated in the third plot of [Figure 15](#A4.F15 "In D.6.2 Empirical
    Validation of Theorem 4 ‣ D.6 Empirical Validation of Theorem 3 and Theorem 4
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games").'
  prefs: []
  type: TYPE_NORMAL
- en: D.8 Training Details of [Section 5.4](#S5.SS4 "5.4 Experimental Results for
    Minimizing Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We provide the training details of [Section 5.4](#S5.SS4 "5.4 Experimental Results
    for Minimizing Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    For the multi-layer Transformer training, we used 4 layers, 1 head Transformer.
    For both single-layer and multi-layer, we employed the Adam optimizer, setting
    the learning rate to 0.001\. During training, we conducted 2,000 epochs with a
    batch size 512\. Moreover, when we trained for the loss sequences with the predictable
    trend, we used 4 layers, 1 head Transformer. For both single-layer and multi-layer,
    we employed the Adam optimizer, setting the learning rate to 0.001\. During training,
    we conducted 9,000 epochs with a batch size of 512.
  prefs: []
  type: TYPE_NORMAL
- en: 'D.9 Ablation Study on Training [Equation 3](#S5.E3 "In 5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide an ablation study that changes $N$), with the results
    in [Figure 19](#A4.F19 "In D.9 Ablation Study on Training Equation 3 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") and [Figure 20](#A4.F20 "In D.9 Ablation
    Study on Training Equation 3 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29e1e28a803a4b538d9fbee1ab75f114.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Ablation study for the uniform loss sequence trained with single-layer
    self-attention layer and Softmax projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81f98a938ccb3ae51367bcd5ae307e6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Ablation study for the uniform loss sequence trained with multi-layer
    self-attention layer and Softmax projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/995169a8bf6fc98adc6f2c3d1019bce2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Ablation study for the Gaussian loss sequence trained with single-layer
    self-attention layer and Softmax projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d93e662abcfd45b917400d3677a2068.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Ablation study for the Gaussian loss sequence trained with single-layer
    self-attention layer and Softmax projection.'
  prefs: []
  type: TYPE_NORMAL
