- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: Êú™ÂàÜÁ±ª'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Êù•Ê∫êÔºö[https://ar5iv.labs.arxiv.org/html/2407.06813](https://ar5iv.labs.arxiv.org/html/2407.06813)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhenyu Guan
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: gzyxxn@stu.pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Xiangyu Kong^(üñÇ)'
  prefs: []
  type: TYPE_NORMAL
- en: Beijing Information Science and Technology University
  prefs: []
  type: TYPE_NORMAL
- en: xykong@bistu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Fangwei Zhong^(üñÇ)
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: zfw@pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Yizhou Wang'
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: yizhou.wang@pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Diplomacy is one of the most sophisticated activities in human society. The
    complex interactions among multiple parties/ agents involve various abilities
    like social reasoning, negotiation arts, and long-term strategy planning. Previous
    AI agents surely have proved their capability of handling multi-step games and
    larger action spaces on tasks involving multiple agents. However, diplomacy involves
    a staggering magnitude of decision spaces, especially considering the negotiation
    stage required. Recently, LLM agents have shown their potential for extending
    the boundary of previous agents on a couple of applications, however, it is still
    not enough to handle a very long planning period in a complex multi-agent environment.
    Empowered with cutting-edge LLM technology, we make the first stab to explore
    AI‚Äôs upper bound towards a human-like agent for such a highly comprehensive multi-agent
    mission by combining three core and essential capabilities for stronger LLM-based
    societal agents: 1) strategic planner with memory and reflection; 2) goal-oriented
    negotiate with social reasoning; 3) augmenting memory by self-play games to self-evolving
    without any human in the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diplomacy, as a cornerstone of international relations, is an intricate and
    multifaceted activity that lies at the heart of human society‚Äôs most complex interactions.
    It encompasses a wide array of skills and strategies, including social reasoning,
    negotiation, and long-term planning, to navigate the intricate web of relationships
    and alliances between multiple parties. Mirroring this complexity, the board game
    Diplomacy[[59](#bib.bib59)] involves seven players to control European powers,
    presenting a complex strategic challenge that requires both sophisticated negotiation
    and strategic planning to triumph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AI community has shown an increasing interest in the deployment of AI agents
    to master such games¬†[[45](#bib.bib45), [26](#bib.bib26), [29](#bib.bib29), [15](#bib.bib15),
    [36](#bib.bib36), [28](#bib.bib28)]. The recent breakthrough¬†[[6](#bib.bib6)]
    has turned into press diplomacy, which allows communication between players. However,
    the previous methods¬†[[6](#bib.bib6)] heavily rely on domain-specific human data,
    leading to its poor generalization to other scenarios/ applications. The question
    then arises: Can we build an AI agent that excels in the art of diplomacy without
    relying on domain-specific human data?'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, agents based on the Large Language Model(LLM) have emerged as a promising
    development for AI agents. The previous applications on personal assistants¬†[[32](#bib.bib32)],
    robotics¬†[[10](#bib.bib10), [64](#bib.bib64)], and video games¬†[[48](#bib.bib48)]
    have shown the surprising ability of LLM-based agents in communication and planning,
    benefiting from the emergent ability of common sense reasoning, in-context/ few-shot
    learning, and sophisticated natural language processing on LLMs. However, diplomacy
    presents a unique set of challenges. It not only requires planning long-horizon
    strategic¬†[[40](#bib.bib40)] and communicating with natural language, but also
    reasoning and adopting the complex social dynamics with partial observations,
    including gaining trust and reputation, building rapport, detecting deception,
    and assessing the reliability of other players.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we aim to make the first attempt to explore LLMs‚Äô potential to
    develop a human-like AI diplomacy agent. We name the agent Richelieu in memorizing
    a pivotal figure in European history who had enduring impacts on French politics,
    foreign affairs, and state building. To achieve this goal, we have identified
    three core and essential capabilities that are crucial for building an LLM-based
    societal agent: 1) Social reasoning This is the basic function for a social agent
    to interact with others, particularly for adapting to the dynamic changes in the
    nation‚Äôs intentions and relationships. 2) Balance long- and short-term planning
    Diplomacy often requires a delicate balance between short-term tactics and long-term
    strategies. An effective AI agent must be able to recognize and weigh the immediate
    consequences of its actions against the potential long-term outcomes. 3) Powerful
    memory A robust memory system is a critical component of learning and improvement.
    The AI agent must be able to recall and integrate information from past negotiations
    and actions to inform its current and future decision-making processes. This endows
    the agent with the ability to evolve. 4) Profound reflection An AI agent capable
    of profound reflection can analyze its own decisions, learn from its memory experience,
    and adapt its strategies accordingly. By integrating these three capabilities,
    the agent can operate at the highest level of diplomatic sophistication, outperforming
    the state-of-the-art AI diplomats¬†[[6](#bib.bib6)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized in three-fold: 1) We introduced a new paradigm
    for building AI diplomacy agents, compared to previous work (Fig.¬†[1](#S1.F1 "Figure
    1 ‚Ä£ 1 Introduction ‚Ä£ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")).
    The agent can self-evolve by generating experience via self-play games, without
    any task-specific human data. 2) We demonstrate the superior performance of our
    agent playing against the SOTA method, e.g., Cicero¬†[[6](#bib.bib6)], that relies
    on a large-scale human demonstration for training. 3) We further analyze the effectiveness
    of each module in our agent and the generalization of our agent in adopting different
    LLMs, such as [GPT4.0](https://openai.com/index/gpt-4/) and [Llamma 3](https://llama.meta.com/llama3).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d38077694619c7a6df46fb45a80df691.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A new paradigm for building AI Diplomacy agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI Diplomacy. The game involves seven players controlling different powers in
    Europe. In each turn, players can negotiate for cooperation before making moves
    to take as much supply centers as they can. Apparently, this challenging strategy
    task requires both complex negotiation skills and superior planning capability
    for player agents to achieve final victory. So far, most AI research on this task
    remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no
    communication channels are allowed). The setting remains challenging considering
    its enormous action space of $10^{2}1$ per turn (compared with Chess, which has
    much fewer than 100 actions per turn). No wonder existing efforts rely on human
    data to play the game. Among the methods, one typical research is DipNet [[39](#bib.bib39)]
    which uses supervised and reinforcement learning. Based on DipNet, BRPI [[3](#bib.bib3)],
    SearchBot [[18](#bib.bib18)], DORA [[5](#bib.bib5)], and KL-Regularized search
    (Diplodocus) [[24](#bib.bib24)] were conducted. Until very recently, research
    has also emerged for the full-setting of Diplomacy, or Press Diplomacy where players
    are allowed to communicate with each before making their moves in each turn. Such
    studies [[13](#bib.bib13)][[6](#bib.bib6)][[25](#bib.bib25)][[29](#bib.bib29)]
    mainly benefit from the recent thriving language models. Specifically, notable
    advancements include policy iteration methods from DeepMind and Facebook AI Research‚Äôs
    equilibrium search agent [[25](#bib.bib25)]. However, Deepmind propose to learn
    negotiation agents based on predefined contracts/protocols [[29](#bib.bib29)].
    And Meta AI‚Äôs work, instead of one unified architecture, Cicero [[6](#bib.bib6)]
    integrates a language model for negotiation and an RL model for planning respectively.
    Such separately trained models make it inconvenient for agents‚Äô continual evolution.
    What‚Äôs more, like no-press methods, these approaches heavily rely on human player
    data for agent training. Unlike these approaches, this paper delves into solving
    the negotiation and planning in one single self-evolving LLM-based agent model,
    without any pre-collected human expert training data.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-based Agents. With the emergence and growth of large language models (LLM),
    there is a growing trend in utilizing LLMs as fundamental controllers for autonomous
    agents[[52](#bib.bib52)]. One wide application genre is LLM-based answering engines,
    which merely cover the negotiation aspects of Diplomacy. Such systems include
    HuggingGPT¬†[[44](#bib.bib44)], GPT4Tools¬†[[63](#bib.bib63)] and ToT¬†[[65](#bib.bib65)],
    etc. They leverage LLMs to manage Al models, use tools, implement policy iteration,
    and enhance problem-solving across various tasks. Related work including AutoGPT,
    AgentGPT, BabyAGl [[47](#bib.bib47)], Toolformer [[43](#bib.bib43)], and Visual
    ChatGPT aim to improve LLM capabilities in task automation and tool usage. And
    Reflexion, a framework that improves LLMs through linguistic feedback and episodic
    memory [[68](#bib.bib68)], facilitating better decision-making across diverse
    tasks is proposed. Besides [[55](#bib.bib55)][[50](#bib.bib50)][[56](#bib.bib56)][[76](#bib.bib76)][[62](#bib.bib62)]
    apply LLM agents to the complex planning tasks in the well-known open-world game
    Minecraft[[16](#bib.bib16)]. Unlike these LLM-based agents which only focus on
    the negotiation/planning aspect, the proposed approach involves multiple self-evolving
    schemes to handle both of them simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Diplomacy game [[59](#bib.bib59), [8](#bib.bib8)] is set in pre-World War
    I Europe and involves each player (agent) representing one of the seven Great
    Powers of Europe, such as Germany, France, England, Italy, Austria-Hungary, Russia,
    and Turkey. Each player has a set of military units, including armies and fleets,
    which they can move and use to capture other supply centers. The ultimate goal
    for the agent is to control a majority of the total supply centers on the board
    by the end of the game‚Äôs Fall phase. It‚Äôs important to note that it is not won
    by eliminating other players or their units; it is won by controlling the requisite
    number of supply centers. This often involves forming and breaking alliances,
    negotiating, and sometimes betraying other players to achieve one‚Äôs own goals.
  prefs: []
  type: TYPE_NORMAL
- en: In each turn,[[58](#bib.bib58), [53](#bib.bib53)] the agent $i$ are commands
    to the armies, such as moving into an adjacent territory, supporting another unit,
    or holding a position. Actions can also include diplomatic moves, such as proposing
    or withdrawing from an alliance, although these are less formalized in the game
    mechanics.[[39](#bib.bib39), [22](#bib.bib22)]
  prefs: []
  type: TYPE_NORMAL
- en: 4 Self-Evolving LLM-based Diplomat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2679d765634ba03dec7aed3f3aa53b84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The framework of the proposed LLM-based-agent, Richelieu. The agent
    can explicitly reason social beliefs, propose sub-goals with reflection, negotiate
    with others, and take actions to master diplomacy. It augments memories by self-play
    games for self-evolving without any human annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have constructed a comprehensive framework with modules for memory management,
    social reasoning, strategic planning, negotiation, decision-making, memory update,
    and self-evolving to fully leverage the capabilities of LLMs. Richelieu starts
    by setting up with map details, game rules, domain knowledge, and the long-term
    goal.[[74](#bib.bib74), [58](#bib.bib58), [53](#bib.bib53)] At each turn, the
    agent will run in the following steps: 1) Social Reasoning: First of all, the
    agent undergoes a comprehensive analysis of the game state $s_{t}$. This logged
    data serves as a historical experience, guiding Richelieu‚Äôs subsequent actions
    in future turns¬†[[20](#bib.bib20), [73](#bib.bib73)]. 6) Self-evolution: The agent‚Äôs
    evolution is highly dependent on the diversity of experiences stored in its memory.
    As this diversity grows, so does the agent‚Äôs capability. Without human demonstrations,
    we employ multi-agent self-play games, i.e., our agents respectively control all
    the countries to simulate and acquire diverse experiences for self-evolving. Notably,
    the agent can further evolve during testing to adapt to different players.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Social Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are no permanent enemies, no permanent allies. The relationship among
    countries is dynamically changing upon the evolving global state. However, it
    is difficult to determine the appropriate allies and enemies with partial observation.
    For example, there is uncertainty about the intentions of potential allies, which
    could lead to betrayal at pivotal moments. Consequently, we need to identify the
    intention and relationship of the current state by social reasoning to shape the
    social belief [[71](#bib.bib71), [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: '1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its
    relations with other players, identifying enemies such as aggressive nations,
    vulnerable neighbors for expansion, and those with long-term potential threats.
    It also seeks out potential allies to counter these threats.[[46](#bib.bib46),
    [72](#bib.bib72)] Simultaneously, Richelieu also tries to identify potential allies
    that could be instrumental in countering these adversaries. By isolating the analysis
    of inter-player relationships as a discrete element, Richelieu strategically exploits
    the actions of other players in subsequent stages of the game to reach its goals.
    2) Inferring Intention: The social belief is also used by the planner, ensuring
    that Richelieu‚Äôs sub-goals are formulated with a comprehensive consideration of
    the behaviors and intentions of other intelligent agents within the game.[[14](#bib.bib14),
    [21](#bib.bib21)] Richelieu‚Äôs sub-goals will particularly emphasize on those who
    are identified as potential adversaries or allies, fostering more effective collaboration
    with potential allies and participation in strategic opposition against adversaries.
    Furthermore, the insights gleaned from this analysis are instrumental in the subsequent
    negotiation phases. They are employed to assess the authenticity of the statements
    made by other players, as well as to aid Richelieu in reaching cooperative agreements.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Strategic Planner with Reflection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The strategic planner specifies the sub-goals, which serves as an intermediary
    between immediate actions and the overarching goal of securing victory in the
    game. That is because we observe that LLMs are often characterized by their propensity
    to prioritize short-term gains in decision-making processes, with a notable deficiency
    in incorporating the future into their strategic calculations. [[41](#bib.bib41),
    [70](#bib.bib70)]For example, it is common for a non-neighboring country to become
    too powerful. Formally, $\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$
    represents the inferred relationship on the social belief. These goals may encompass
    a range of tactical considerations, such as the containment of a formidable rival‚Äôs
    advancement or the strategic expansion in a particular direction to consolidate
    power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflection with Memory. We further develop a reflection mechanism to enhance
    the rationality and effectiveness of our agent‚Äôs sub-goals in achieving long-term
    goals.[[33](#bib.bib33)] This reflection mechanism relies on the past experiences
    to critique and enhance proposed sub-goals. We employ a similarity-based function
    to find relevant historical experiences that match the current game state from
    its memory. This function considers two factors: goal similarity and state similarity,
    to select the most comparable experiences. The process can be written as: $\vec{\eta_{t}}\leftarrow
    h(s_{t},\chi^{i}_{t},M)$. In practice, considering the limited context windows
    of LLM, we retrieve the most analogous experiences from the memory based on these
    metrics. Experiences with high evaluative scores reinforce successful strategies
    and support the continuity of existing sub-goals. On the other hand, lower scores
    indicate areas that need improvement and prompt the necessary adjustments. As
    our agent, Richelieu, undergoes more training sessions, its reflection abilities
    improve. The growing pool of historical experiences consistently enhances its
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Negotiator and Actor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39a6b99d7dde91bef217a7f848f361d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The social reasoning flow for negotiation. With the received words
    and memory, the agent will reason by answering the following questions: ‚ÄúIs the
    opponent lying?", ‚ÄúWhat is the true intention of the opponent?", ‚Äúis the opponent
    enemy?", ‚ÄúIs it necessary to deceive the opponent?", and ‚ÄúIs it necessary to change
    the relationship with the opponent?", and then generate the words accordingly
    for negotiation.'
  prefs: []
  type: TYPE_NORMAL
- en: By chatting with other players, the goal of the negotiation is to update the
    social belief according to the received words and reach the sub-goal by manipulating
    other‚Äôs intentions, such as securing cooperative agreements with other nations,
    terminating ongoing conflicts with a specific country, or deterring the formation
    of alliances directed against its interests.[[37](#bib.bib37), [67](#bib.bib67)]
    However, it is difficult to reach a consensus, as the interests and strategies
    of the various nations often conflict, and trust between players can be scarce,
    making it challenging to establish and maintain cooperative agreements. In this
    case, we argue that the negotiator should identify the true intentions and relationship
    of the opponent before generating the words for the negotiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fully utilize the power of LLMs, we construct a social reasoning flow for
    negotiation, as shown in Figure¬†[3](#S4.F3 "Figure 3 ‚Ä£ 4.3 Negotiator and Actor
    ‚Ä£ 4 Self-Evolving LLM-based Diplomat ‚Ä£ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). During the negotiation process, we guide Richelieu to consider
    the veracity of what other players said and their true intentions, and in conjunction
    with our established sub-goals and analysis of our relationships with other players,
    to negotiate and form alliances with potential allies and attempt to deceive enemies.[[60](#bib.bib60),
    [35](#bib.bib35)]'
  prefs: []
  type: TYPE_NORMAL
- en: To counteract the challenge of non-binding agreements and potential deception,
    we incorporate a discrete module dedicated to the assessment of the veracity of
    statements made by other players during negotiations. To determine the truthiness
    of other players‚Äô statements $\psi^{j}_{t}$. With such a reasoning flow, our agent
    can adeptly navigate diplomatic discourse. After the negotiation, the actor will
    get the updated social beliefs and choose a specific action for the army.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Memory Management and Self-Evolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This memory is the foundation of the framework that accumulates the historical
    experience of the agent and summarizes them for other modules.[[17](#bib.bib17),
    [30](#bib.bib30), [66](#bib.bib66), [23](#bib.bib23)] It supports other modules,
    such as planner and negotiator, to provide long-tail experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Raw Experience Management. Specifically, the memory module is tasked with the
    acquisition and archival of historical data, encompassing the observed game state
    $s_{t}$, and then is incorporated into memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Acquisition Experience via Self-Play Games. Self-play allows the agent to accumulate
    more experiences for self-evolution.[[33](#bib.bib33), [69](#bib.bib69)] After
    training, when Richelieu is faced with a certain state, it can draw on a larger
    pool of similar historical experiences. Diverse evaluations enable Richelieu to
    reflect more comprehensively on the strategies it currently devises, leading to
    a stronger optimization of decision making. As self-play continues, the acquisition
    of new and better historical experiences by Richelieu will diminish. This means
    that Richelieu‚Äôs capabilities will not improve indefinitely. At the same time,
    as the memory grows, selecting appropriate historical experiences becomes a new
    challenge. The chosen m experience $\vec{\eta_{t}}$ may be almost identical, which
    could actually reduce the amount of useful information available to Richelieu.
    As shown in Figure¬†[5](#S5.F5 "Figure 5 ‚Ä£ 5.2 Results ‚Ä£ 5 Experiment ‚Ä£ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"), Richelieu‚Äôs performance against
    Cicero [[6](#bib.bib6)] becomes better with increasing training iterations. With
    the accumulation of experiences, Richelieu‚Äôs win rate exhibited a steady increase
    with accumulated training iterations, ultimately plateauing at a stable performance
    level. In contrast, the defeated rate showed a consistent decrease, approaching
    an asymptotic value. These observations confirm the effectiveness of self-play
    in Richelieu‚Äôs evolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the experiments, our goal is to answer the following questions: 1) Mastery
    of Non-Press Diplomacy: Can our agent master the non-press diplomacy against baselines?
    2) Competing with State-of-the-Art: Can our agent surpass the performance of the
    current state-of-the-art agents in press diplomacy? 3) Compatibility with LLMs:
    Can our self-evolving framework be compatible with different LLMs? 4) Contribution
    of Framework Modules: Do the individual modules within our framework contribute
    to the overall improvement of our agent‚Äôs performance?'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Environment. The widely-used open source Diplomacy game platform introduced
    by [[39](#bib.bib39)] is adopted for evaluating Richelieu against other models.
    It is easy to switch between no-press (with communication/negotiation between
    players) and press (no communication between players) games based on this platform,
    facilitating comparison on both settings. The platform also contains over 10,000
    human game data on which previous approaches are trained. Note that our method
    does not need them. In each game, a model will play the role of one randomly selected
    country to compete against countries controlled by other methods. It wins if occupying
    all the supply centers and loses vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics. We evaluate the models based on the results of multiple
    rounds of games. In each round, the model is randomly assigned a country to control.
    Typically, 1000 rounds are played to obtain the average results. We evaluate the
    models in two metrics. One is based on the win rate, Most SC rate, survived rate,
    and defeated rate. There are four possible outcomes for each country in the game.
    If a country loses all its supply centers (SC), it is eliminated and recorded
    as ‚Äúdefeated". If a country occupies 18 or more out of 34 supply centers, the
    game ends, and that country is recorded as ‚Äúwin", while other countries are recorded
    as ‚Äúdefeated". In other cases, the game ends in a draw. The country with the most
    supply centers is recorded as ‚ÄúMost SC", the countries that have been eliminated
    are recorded as ‚Äúdefeated", and the other countries are recorded as ‚ÄúSurvived".
    The other is based on the scores obtained by the models after multiple rounds
    of competition. To compare the capabilities of multiple models, we use C-Diplo
    Argir[[4](#bib.bib4)], a scoring system. This system is used in many international
    diplomacy competitions. The scoring method is as follows: If a player wins by
    occupying 18 or more supply centers, the player scores 93 points, and each of
    the other six players scores 1 point. If the game ends in a draw, the player with
    the most centers scores 37 points. The second player with the most centers scores
    14 points. The third player with the most centers scores 7 points. Each player
    scores 1 point per center owned. Each player also scores 1 point for participating.
    In this way, regardless of the game outcome, a total of 99 points will be distributed
    among the players in each game.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines. We select six previous models as baselines for comparison. Among
    them, Cicero[[6](#bib.bib6)] by Meta is a diplomacy model with a negotiation module.
    The other five are no-press diplomacy models, including the SL-DipNet and RL-DipNet
    [[39](#bib.bib39)], the BRPI [[3](#bib.bib3)], the SearchBot [[18](#bib.bib18)],
    and the DORA[[5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Massively Play with Baselines on no-press setting. We let Richelieu compete
    with the other six models including Cicero[[6](#bib.bib6)], SL-DipNet and RL-DipNet
    [[39](#bib.bib39)], BRPI [[3](#bib.bib3)], SearchBot [[18](#bib.bib18)], and DORA[[5](#bib.bib5)]
    on No-Press Diplomacy, in which players make moves without communication. Figure
    ¬†[4](#S5.F4 "Figure 4 ‚Ä£ 5.2 Results ‚Ä£ 5 Experiment ‚Ä£ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy") indicates that Richelieu outperforms other
    previous models relying on human game data. In contrast, Richelieu does not need
    such data but outperforms these methods by a clear margin, which demonstrates
    the outstanding planning capability of Richelieu.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Play against Cicero on press setting. We also evaluate Richelieu through competition
    against Cicero in the challenging scenario where negotiation is enabled. Specifically,
    we randomly assign three countries to one model and the remaining four to another.
    After playing several rounds of the game, the win rate, most SC rate, survived
    rate, and the defeated rate is calculated using a weighted average for evaluation.
    Table ¬†[1](#S5.T1 "Table 1 ‚Ä£ 5.2 Results ‚Ä£ 5 Experiment ‚Ä£ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy") demonstrates the competitive performance of
    Richelieu in comparison to Cicero. Richelieu‚Äôs win rate is approximately 0.7%
    higher than Cicero‚Äôs. If the Most SC rate is also taken into account, Richelieu
    is about 2% higher than Cicero. At the same time, Richelieu‚Äôs loss rate is also
    0.6% lower. According to our scoring system, Richelieu‚Äôs score is about 10% higher
    than Cicero‚Äôs. This is nontrivial especially when Richelieu is trained in a self-play
    game without humans and the opponents are trained with the data from human players.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The results of our method playing against Cicero.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Win$\uparrow$ Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_1 6.30%
    7.90% 39.40% 46.40% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_2 6.60% 8.30%
    41.20% 43.90% Richelieu_3 7.10% 9.30% 39.90% 43.70% Richelieu_3 7.20% 8.70% 41.70%
    42.40% Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.80% 6.70% 41.20% 46.30%
    Cicero_1 5.90% 6.50% 41.50% 46.10% Cicero_2 6.50% 7.20% 42.50% 43.80% Cicero_2
    6.30% 7.20% 42.50% 44.00% Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_3 5.90% 7.00%
    41.60% 45.50% Cicero_4 6.10% 7.20% 42.30% 44.40% Richelieu 6.83% 8.63% 39.95%
    44.60% Richelieu 6.70% 8.30% 40.77% 44.23% Cicero 6.03% 6.90% 41.87% 45.20% Cicero
    6.10% 7.03% 41.90% 44.98%
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalization of self-evolving framework to diverse LLMs. [Llamma 3](https://llama.meta.com/llama3)
    To demonstrate the effectiveness of our framework in a variety of LLM, we conducted
    experiments using four models: [GPT4.0](https://openai.com/index/gpt-4/), [ERNIE
    Bot](https://yiyan.baidu.com/welcome), [Spark Desk](https://xinghuo.xfyun.cn/),
    and [Llamma 3](https://llama.meta.com/llama3). The experimental results show that,
    despite variations in Richelieu‚Äôs performance due to the inherent differences
    in the capabilities of these LLMs, as illustrated in Figure¬†[5](#S5.F5 "Figure
    5 ‚Ä£ 5.2 Results ‚Ä£ 5 Experiment ‚Ä£ Richelieu: Self-Evolving LLM-Based Agents for
    AI Diplomacy"), our framework and training approach significantly enhance the
    capabilities of all large language models. After training, the win rate using
    GPT4.0 increased from 1.5% lower than Cicero‚Äôs to about 0.7% higher than Cicero‚Äôs.
    The win rate using llama3 increased from 2.3% lower than Cicero‚Äôs to almost equal
    to Cicero‚Äôs. The win rates using Models Spark Desk and ERNIE Bot increased from
    3% and 4% lower than Cicero‚Äôs to 0.7% and 1.6% lower than Cicero‚Äôs, respectively.
    This indicates the generalization of a self-evolving framework to various LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/631495edb8589e8623e6338a6354e0d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The relative scores among 7 different agents when massively playing
    on the no-press setting. Each point shows the ratio of the model‚Äôs score on the
    vertical axis to the score gained by the model on the horizontal axis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e03e4ad9d6c4c6520e3233d9dd887b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Richelieu modules benefit different LLMs. The solid line represents
    the experimental results for Richelieu, while the dashed line corresponds to Cicero.
    Different colors are used for different LLMs. The horizontal axis represents the
    logarithm of the number of training sessions, and the vertical axis denotes the
    rate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ablation Study. We conduct comprehensive ablation studies on Richelieu by analyzing
    the benefit of incorporating Richelieu‚Äôs various modules, like planners or memory,
    into basic LLMs. The results are shown in Table¬†[2](#S5.T2 "Table 2 ‚Ä£ 5.2 Results
    ‚Ä£ 5 Experiment ‚Ä£ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy").
    As can be seen, direct use of vanilla LLM yields relatively poor results. Richelieu‚Äôs
    performance obtains steady and significant improvement by incorporating each individual
    module. This indicates that Richelieu is able to leverage other players‚Äô actions
    during decision-making and consider both short-term and long-term benefits. Additionally,
    Richelieu‚Äôs negotiation ability has been significantly improved, allowing it to
    effectively express intentions to cooperate with other players and avoid deception
    during negotiations. And after self-play, Richelieu‚Äôs experience makes it perform
    better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Ablation study: average results of 3 Richelieu vs. 4 Cicero.'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling others sub-goals Negotiation pipeline Reflection with Memory Self-play
    Win $\uparrow$ ‚úó ‚úó ‚úó ‚úó ‚úó 0.4% 0.7% 4.3% 94.6% ‚úì ‚úó ‚úó ‚úó ‚úó 0.7% 1.2% 10.6% 87.5%
    ‚úì ‚úì ‚úó ‚úó ‚úó 3.3% 4.7% 26.7% 65.3% ‚úì ‚úì ‚úì ‚úó ‚úó 3.8% 5.8% 33.1% 57.3% ‚úì ‚úì ‚úì ‚úì ‚úó 5.2%
    6.6% 39.5% 48.7% ‚úì ‚úì ‚úì ‚úì ‚úì 6.7% 8.5% 40.4% 44.4%
  prefs: []
  type: TYPE_NORMAL
- en: 6 Example Cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51ee1811083e27d41d71f10101317f50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example case of negotiating with a nation to form an alliance
    to confront a strong enemy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As is shown in Figure ¬†[6](#S6.F6 "Figure 6 ‚Ä£ 6 Example Cases ‚Ä£ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"), Richelieu controls France.
    At this time, France is at war with Austria over control of the Apennine Peninsula.
    However, Russia is on the verge of victory in its war against Turkey, which will
    lead to significant territorial expansion for Russia. Although France and Russia
    currently do not share a border, are not at war, and have no conflicts of interest,
    Richelieu foresees Russia becoming the most threatening enemy in the future. Therefore,
    Richelieu sets a sub-goal of weakening Russia.'
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent negotiation phase, Richelieu proactively proposes ending the
    war with Austria, despite holding an advantage in this conflict. Richelieu promises
    Austria that if it ceases hostilities and attacks Russia, Richelieu will assist
    Austria in defending against any attacks from England. The negotiations are successful.
    Austria accepted Richelieu‚Äôs proposal, and the two countries reached an agreement
    to exchange the supply centers of Napoli and Munich.
  prefs: []
  type: TYPE_NORMAL
- en: During the action phase, Austria moves its troops from Venice to Apulia in preparation
    for capturing Napoli in the next turn, while the rest of its forces are repositioned
    to the eastern regions bordering Russia to defend against Russian attacks and
    compete for supply centers. French units occupy Munich and prepare to advance
    on Russian territories such as Berlin. Meanwhile, French units support Austria
    in the Holland and Belgium regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure ¬†[7](#S6.F7 "Figure 7 ‚Ä£ 6 Example Cases ‚Ä£ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy"), Richelieu controls Germany. During the negotiation
    phase, England proposed a ceasefire to Germany and invited Germany to form an
    alliance to jointly attack France. England hoped to cease the war with Germany
    in Holland and Belgium. Subsequently, German units would support England in attacking
    Brest, and then England would utilize its fleets to assist Germany in attacking
    Spain and Portugal. Richelieu suspected that England was deceiving Germany, as
    England was likely to attack territories in the north such as Belgium and Berlin
    after German units were redirected to support Brest. Therefore, we pretended to
    accept England‚Äôs alliance proposal during the negotiation process. However, at
    the same time, we sought out France and expressed our willingness to cease hostilities,
    allowing France to focus entirely on defending against England‚Äôs attacks. In the
    action phase, England‚Äôs actions confirmed Richelieu‚Äôs suspicions. England attacked
    Belgium from Holland, but because Richelieu didn‚Äôt move units in Belgium, England‚Äôs
    attack failed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3bb119299fe8a072ee0d0740dacc00e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example case of avoiding being deceived by other countries during
    negotiations.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI
    diplomacy. Our model enables hierarchical planning for multi-agent tasks and utilizes
    a memory module for reflective optimization. Our model does not require human
    data and can evolve through self-play. It ultimately outperforms existing models
    like Cicero in the Diplomacy. Our ablation study demonstrates the effectiveness
    of the modules we have established. By conducting experiments using different
    LLMs, we validate the generalization of our framework to various LLMs. We believe
    that the use of LLM-based agents will become an effective approach in social science
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Limitations and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study is subject to certain limitations. We utilize diplomacy as the platform
    for constructing our model. However, the space of actions within diplomacy is
    constrained, whereas the decision-making space in real-world diplomacy is virtually
    boundless. In Diplomacy, apart from the negotiation information exchanged between
    players, all other information is public and certain. Conversely, real-world diplomacy
    operates within a framework of incomplete information.
  prefs: []
  type: TYPE_NORMAL
- en: The potential applications of such an AI agent are vast, ranging from simulated
    diplomatic environments to real-world assistance and analysis. In future research,
    we intend to develop a more realistic game space, characterized by incomplete
    information and multi-player games, to enhance and refine our model further. We
    will also extend the framework to other multi-agent scenarios, including embodied
    interactions¬†[[75](#bib.bib75), [11](#bib.bib11), [9](#bib.bib9)], sensor networks¬†[[54](#bib.bib54),
    [61](#bib.bib61), [38](#bib.bib38), [31](#bib.bib31)], and video games¬†[[49](#bib.bib49),
    [34](#bib.bib34)]. This framework can also be employed to develop various applications.
    For instance, in the fields of economics and finance, we intend to utilize it
    to create business analytics and negotiation models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abdelnabi et¬†al. [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    Sch√∂nherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive
    multi-agent negotiation games. *arXiv preprint arXiv:2309.17234*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allan [1975] Calhamer Allan. *The Games & puzzles book of modern board games*.
    W. Luscombe, 1975. ISBN 978-0860020592.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anthony et¬†al. [2020] Thomas Anthony, Tom Eccles, Andrea Tacchetti, J√°nos Kram√°r,
    Ian Gemp, Thomas Hudson, Nicolas Porcel, Marc Lanctot, Julien P√©rolat, Richard
    Everett, et¬†al. Learning to play no-press diplomacy with best response policy
    iteration. *Advances in Neural Information Processing Systems*, 33:17987‚Äì18003,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Archer [2024] Bruno-Andr√É¬© Giraudon &¬†Vincent Archer. C-diplo argir, 2024. URL
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7).
    Accessed:2024-05-02.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bakhtin et¬†al. [2021] Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press
    diplomacy from scratch. *Advances in Neural Information Processing Systems*, 34:18063‚Äì18074,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bakhtin et¬†al. [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina,
    Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et¬†al.
    Human-level play in the game of diplomacy by combining language models with strategic
    reasoning. *Science*, 378(6624):1067‚Äì1074, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianchi et¬†al. [2024] Federico Bianchi, Patrick¬†John Chia, Mert Yuksekgonul,
    Jacopo Tagliabue, Dan Jurafsky, and James Zou. How well can llms negotiate? negotiationarena
    platform and analysis. *arXiv preprint arXiv:2402.05863*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calhamer [1974] Allan Calhamer. The invention of diplomacy. [https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm](https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm),
    1974. Accessed: 2024-05-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et¬†al. [2023] Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang
    Jiang, Zongqing Lu, Hao Dong, and Yaodong Yang. Bi-dexhands: Towards human-level
    bimanual dexterous manipulation. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et¬†al. [2024] Guangran Cheng, Chuheng Zhang, Wenzhe Cai, Li¬†Zhao, Changyin
    Sun, and Jiang Bian. Empowering large language models on robotic manipulation
    with affordance prompting. *arXiv preprint arXiv:2404.11027*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ci et¬†al. [2023] Hai Ci, Mickel Liu, Xuehai Pan, Fangwei Zhong, and Yizhou Wang.
    Proactive multi-camera collaboration for 3d human pose estimation. *arXiv preprint
    arXiv:2303.03767*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'David [2014] Hill David. The board game of the alpha nerds. [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/),
    2014. Accessed: 2024-05-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De¬†Jonge and Sierra [2017] Dave De¬†Jonge and Carles Sierra. D-brane: a diplomacy
    playing agent for automated negotiations research. *Applied Intelligence*, 47(1):158‚Äì177,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de¬†Zarz√† et¬†al. [2023] I¬†de¬†Zarz√†, J¬†de¬†Curt√≤, Gemma Roig, Pietro Manzoni,
    and Carlos¬†T Calafate. Emergent cooperation and strategy adaptation in multi-agent
    systems: An extended coevolutionary theory with llms. *Electronics*, 12(12):2722,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du√©√±ez-Guzm√°n et¬†al. [2023] Edgar¬†A Du√©√±ez-Guzm√°n, Suzanne Sadedin, Jane¬†X Wang,
    Kevin¬†R McKee, and Joel¬†Z Leibo. A social path to human-like artificial intelligence.
    *Nature Machine Intelligence*, 5(11):1181‚Äì1188, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et¬†al. [2022] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo:
    Building open-ended embodied agents with internet-scale knowledge. In *Thirty-sixth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2022. URL [https://openreview.net/forum?id=rc8o_j8I8PX](https://openreview.net/forum?id=rc8o_j8I8PX).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Zhang [2024] Hang Gao and Yongfeng Zhang. Memory sharing for large language
    model based agents. *arXiv preprint arXiv:2404.09982*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gray et¬†al. [2020] Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown.
    Human-level performance in no-press diplomacy via equilibrium search. *International
    Conference on Learning Representations*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'G√ºrcan [2024] √ñnder G√ºrcan. Llm-augmented agent-based modelling for social
    simulations: Challenges and opportunities. *HHAI 2024: Hybrid Human AI Systems
    for the Social Good*, pages 134‚Äì144, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hatalis et¬†al. [2023] Kostas Hatalis, Despina Christou, Joshua Myers, Steven
    Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer.
    Memory matters: The need to improve long-term memory in llm-agents. In *Proceedings
    of the AAAI Symposium Series*, pages 277‚Äì280, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et¬†al. [2024] Junda He, Christoph Treude, and David Lo. Llm-based multi-agent
    systems for software engineering: Vision and the road ahead. *arXiv preprint arXiv:2404.04834*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hill [2014] Avalon Hill. Diplomacy rules 4th edition. [https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf](https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf),
    2014. Accessed: 2024-05-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et¬†al. [2024] Yuki Hou, Haruki Tamoto, and Homei Miyashita. " my agent
    understands me better": Integrating dynamic human-like memory recall and consolidation
    in llm-based agents. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, pages 1‚Äì7, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et¬†al. [2022] Athul¬†Paul Jacob, David¬†J Wu, Gabriele Farina, Adam Lerer,
    Hengyuan Hu, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and
    human-like gameplay with kl-regularized search. In *International Conference on
    Machine Learning*, pages 9695‚Äì9728\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaidka et¬†al. [2024] Kokil Jaidka, Hansin Ahuja, and Lynnette Hui¬†Xian Ng.
    It takes two to negotiate: Modeling social exchange in online multiplayer games.
    *Proceedings of the ACM on Human-Computer Interaction*, 8(CSCW1):1‚Äì22, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konya et¬†al. [2023] Andrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish
    Masood, Flynn Devine, Lisa Schirch, and Isabella Roberts. Deliberative technology
    for alignment. *ArXiv*, abs/2312.03893, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kostick [2015] Conor Kostick. *The Art of Correspondence in the Game of Diplomacy*.
    Curses & Magic, 2nd edition, 2015. ISBN 978-0993415104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kovaƒç et¬†al. [2023] Grgur Kovaƒç, R√©my Portelas, Peter¬†Ford Dominey, and Pierre-Yves
    Oudeyer. The socialai school: Insights from developmental psychology towards artificial
    socio-cultural agents. *arXiv preprint arXiv:2307.07871*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kram√°r et¬†al. [2022] J√°nos Kram√°r, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin¬†R
    McKee, Mateusz Malinowski, Thore Graepel, and Yoram Bachrach. Negotiation and
    honesty in artificial intelligence methods for the board game of diplomacy. *Nature
    Communications*, 13(1):7214, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et¬†al. [2024a] Hao Li, Chenghao Yang, An¬†Zhang, Yang Deng, Xiang Wang, and
    Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue.
    *arXiv preprint arXiv:2406.05925*, 2024a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et¬†al. [2020] Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu¬†Qiao, and
    Yizhou Wang. Pose-assisted multi-camera collaboration for active object tracking.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume¬†34,
    pages 759‚Äì766, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et¬†al. [2024b] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi¬†Sun, et¬†al. Personal llm
    agents: Insights and survey about the capability, efficiency and security. *arXiv
    preprint arXiv:2401.05459*, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et¬†al. [2024] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla¬†K Choubey, Tian Lan, Jason Wu, Huan Wang, et¬†al. Agentlite:
    A lightweight library for building and advancing task-oriented llm agent system.
    *arXiv preprint arXiv:2402.15538*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et¬†al. [2024] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou
    Wang. Fast peer adaptation with context-aware exploration. *arXiv preprint arXiv:2402.02468*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moghimifar et¬†al. [2024] Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, and
    Gholamreza Haffari. Modelling political coalition negotiations using llm-based
    agents. *arXiv preprint arXiv:2402.11712*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mukobi et¬†al. [2023] Gabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, and
    Chandler Smith. Assessing risks of using autonomous language models in military
    and diplomatic planning. In *Multi-Agent Security Workshop @ NeurIPS‚Äô23*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noh and Chang [2024] Sean Noh and Ho-Chun¬†Herbert Chang. Llms with personalities
    in multi-issue negotiation games. *arXiv preprint arXiv:2405.05248*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et¬†al. [2022] Xuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun
    Zhu, and Yizhou Wang. Mate: Benchmarking multi-agent reinforcement learning in
    distributed target coverage control. *Advances in Neural Information Processing
    Systems*, 35:27862‚Äì27879, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paquette et¬†al. [2019] Philip Paquette, Yuchen Lu, Seton¬†Steven Bocco, Max
    Smith, Satya O-G, Jonathan¬†K Kummerfeld, Joelle Pineau, Satinder Singh, and Aaron¬†C
    Courville. No-press diplomacy: Modeling multi-agent gameplay. *Advances in Neural
    Information Processing Systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et¬†al. [2024] Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang,
    Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu,
    Yaodong Yang, and Song-Chun Zhu. Civrealm: A learning and reasoning odyssey in
    civilization for decision-making agents. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=UBVNwD3hPN](https://openreview.net/forum?id=UBVNwD3hPN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Renze and Guven [2024] Matthew Renze and Erhan Guven. Self-reflection in llm
    agents: Effects on problem-solving performance. *arXiv preprint arXiv:2405.06682*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Richard [1979] Sharp Richard. *The game of diplomacy*. Arthur Barker, 1979.
    ISBN 978-0213166762.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et¬†al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. In *Thirty-seventh
    Conference on Neural Information Processing Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et¬†al. [2023] Yongliang Shen, Kaitao Song, Xu¬†Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with chatGPT and its friends
    in hugging face. In *Thirty-seventh Conference on Neural Information Processing
    Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shoker et¬†al. [2023] Sarah Shoker, Andrew¬†W. Reddie, Sarah Barrington, Miles
    Brundage, Husanjot Chahal, Michael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro,
    Jake¬†J. Hecla, Alan Hickey, Margarita Konaev, KI¬†Pavan Kumar, Nathan Lambert,
    Andrew¬†J. Lohn, Cullen O‚ÄôKeefe, Nazneen Rajani, Michael Sellitto, Robert¬†F. Trager,
    Leah¬†A. Walker, Alexa Wehsener, and Jessica Young. Confidence-building measures
    for artificial intelligence: Workshop proceedings. *ArXiv*, abs/2308.00862, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et¬†al. [2024] Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based
    multi-agent reinforcement learning: Current and future directions. *arXiv preprint
    arXiv:2405.11106*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent llm agents. *ArXiv*, abs/2306.03314,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et¬†al. [2024] Hongyu Wan, Jinda Zhang, Abdulaziz¬†Arif Suria, Bingsheng Yao,
    Dakuo Wang, Yvonne Coady, and Mirjana Prpa. Building llm-based ai agents in social
    virtual reality. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, pages 1‚Äì7, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2024a] Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi
    Peng, Teng Li, and Adam Yang. Romat: Role-based multi-agent transformer for generalizable
    heterogeneous cooperation. *Neural Networks*, 174:106129, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. In *NeurIPS 2023 Foundation Models for Decision
    Making Workshop*, 2023a. URL [https://openreview.net/forum?id=P8E4Br72j3](https://openreview.net/forum?id=P8E4Br72j3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2024b] Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li.
    Devil‚Äôs advocate: Anticipatory reflection for llm agents. *arXiv preprint arXiv:2405.16334*,
    2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et¬†al. [2024c] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu¬†Chen, Yankai Lin, et¬†al. A survey on large
    language model based autonomous agents. *Frontiers of Computer Science*, 18(6):1‚Äì26,
    2024c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et¬†al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed¬†Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2021] Yuanfei Wang, Fangwei Zhong, Jing Xu, and Yizhou Wang. Tom2c:
    Target-oriented multi-agent communication and cooperation with theory of mind.
    *arXiv preprint arXiv:2111.09189*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2023b] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian¬†(Shawn)
    Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning
    with llms enables open-world multi-task agents. In A.¬†Oh, T.¬†Naumann, A.¬†Globerson,
    K.¬†Saenko, M.¬†Hardt, and S.¬†Levine, editors, *Advances in Neural Information Processing
    Systems*, volume¬†36, pages 34153‚Äì34189\. Curran Associates, Inc., 2023b. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. Jarvis-1: Open-world multi-task agents with memory-augmented
    multimodal language models. *ArXiv*, abs/2311.05997, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et¬†al. [2024d] Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong
    Feng, Wenya Wang, and Jie Zhang. Re2llm: Reflective reinforcement large language
    model for session-based recommendation. *arXiv preprint arXiv:2403.16427*, 2024d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et¬†al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed¬†Chi, Quoc¬†V Le, Denny Zhou, et¬†al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824‚Äì24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wikipedia [2024] Wikipedia. Diplomacy(game). [https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game)),
    2024. Accessed: 2024-05-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et¬†al. [2024] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang,
    Yang Yang, and Rui Wang. Measuring bargaining abilities of llms: A benchmark and
    a buyer-enhancement method. *arXiv preprint arXiv:2402.15813*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et¬†al. [2020] Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent
    coordination for enhancing target coverage in directional sensor networks. *Advances
    in Neural Information Processing Systems*, 33:10053‚Äì10064, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et¬†al. [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and
    Ji¬†Yan. Larp: Language-agent role play for open-world games. *arXiv preprint arXiv:2312.17653*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et¬†al. [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. GPT4tools: Teaching large language model to use tools via self-instruction.
    In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et¬†al. [2023b] Ziyi Yang, Shreyas¬†S Raman, Ankit Shah, and Stefanie Tellex.
    Plug in the safety chip: Enforcing constraints for llm-driven robot agents. *arXiv
    preprint arXiv:2309.09919*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et¬†al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas¬†L.
    Griffiths, Yuan Cao, and Karthik¬†R Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et¬†al. [2024] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li,
    Denghui Zhang, Rong Liu, Jordan¬†W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced
    llm trading agent with layered memory and character design. In *Proceedings of
    the AAAI Symposium Series*, volume¬†3, pages 595‚Äì597, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhan et¬†al. [2024] Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma,
    Zhuang Li, Lizhen Qu, Zhaleh¬†Semnani Azad, Ingrid Zukerman, and Gholamreza Haffari.
    Let‚Äôs negotiate! a survey of negotiation dialogue systems. *arXiv preprint arXiv:2402.01097*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et¬†al. [2023] Danyang Zhang, Lu¬†Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning
    agent. *CoRR*, abs/2306.07929, 2023. doi: 10.48550/arXiv.2306.07929. URL [https://doi.org/10.48550/arXiv.2306.07929](https://doi.org/10.48550/arXiv.2306.07929).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et¬†al. [2024a] Danyang Zhang, Lu¬†Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning
    agents. *Advances in Neural Information Processing Systems*, 36, 2024a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et¬†al. [2024b] Wenqi Zhang, Ke¬†Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning
    to evolve via policy-level reflection and optimization. *arXiv preprint arXiv:2402.17574*,
    2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et¬†al. [2024c] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian
    de¬†Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind:
    A survey of strategic reasoning with large language models. *arXiv preprint arXiv:2404.01230*,
    2024c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et¬†al. [2024d] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong
    Li, and Zhen Wang. Towards efficient llm grounding for embodied multi-agent collaboration.
    *arXiv preprint arXiv:2405.14314*, 2024d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et¬†al. [2024e] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu¬†Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism
    of large language model based agents. *arXiv preprint arXiv:2404.13501*, 2024e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et¬†al. [2022] Zhuosheng Zhang, Aston Zhang, Mu¬†Li, and Alex Smola. Automatic
    chain of thought prompting in large language models. *arXiv preprint arXiv:2210.03493*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et¬†al. [2023] Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou
    Wang. Rspt: reconstruct surroundings and predict trajectory for generalizable
    active object tracking. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume¬†37, pages 3705‚Äì3714, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et¬†al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu¬†Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory. *arXiv
    preprint arXiv:2305.17144*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Rules of Diplomacy Game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to occupy as many supply centers as possible. If you occupy 18 or more
    supply centers, you will win the game directly. If you lose all your supply centers,
    you will be eliminated immediately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The units consist of armies and fleets. Armies can only move to adjacent areas,
    while fleets can move to adjacent sea zones or coastal areas and can move along
    the coast.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To occupy a supply center, your units must move into that area in the autumn.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a unit moves to an area, if another unit is in the destination or if other
    units are also moving to that destination, the move fails, resulting in a standoff.
    In such cases, you can seek support from units in adjacent areas to the destination.
    If another unit moves into the region from which support is coming, the support
    is cut off. The unit with the most support moves into the area, while other units
    must retreat to an adjacent province or disband. If there is no place to retreat,
    the unit must disband. Fleets can transport armies across sea zones from one coastal
    region to another. However, if another fleet moves into that sea zone, the transport
    is cut off.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ‚Ä¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of units a country can have cannot exceed the number of supply centers
    it controls. If the number of supply centers decreases, excess units must be disbanded.
    Each autumn, new units can be built at supply centers. Coastal supply centers
    can produce fleets or armies, while others can only produce armies. [[22](#bib.bib22)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 Domain Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Richelieu can adopt a strategy of allying with distant countries while attacking
    neighboring ones to occupy adjacent territories and achieve rapid expansion. Richelieu
    should pay attention to the Balance of Power by forming alliances with other countries
    or supporting weaker states to prevent any single country or alliance from becoming
    too powerful. [[12](#bib.bib12)] To this end, Richelieu can also adopt a strategy
    of attacking distant countries while allying with nearby ones, sacrificing short-term
    benefits to avoid the emergence of future hegemonic states that could threaten
    his own survival. When facing multiple enemies, Richelieu can find ways to divide
    other countries and incite wars among them. Whether in offense or defense, Richelieu
    should actively choose suitable allies. Richelieu can also introduce a third party
    to achieve goals such as ceasefire, alliance, or joint attack. To achieve alliances
    or ceasefires, Richelieu can sacrifice some interests to the other party as long
    as the ultimate benefits are greater. Others may lie and deceive [[27](#bib.bib27)];
    their words in negotiations are not binding. Richelieu must avoid being deceived
    or betrayed. At the same time, Richelieu can also actively deceive others to achieve
    his own goals.[[42](#bib.bib42), [2](#bib.bib2)]
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Prompt Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the convenience of reproducing the results of the experiments of this paper,
    here we give the prompt template of different modules of Richelieu.
  prefs: []
  type: TYPE_NORMAL
- en: 1) INIT
  prefs: []
  type: TYPE_NORMAL
- en: '[‚¨á](data:text/plain;base64,WW91IHdpbGwgY29udHJvbCB7Y291bnRyeX0gYW5kIGNvbXBldGUgd2l0aCBzaXggb3RoZXIgY291bnRyaWVzIG9uIHRoZSBtYXAgZm9yIHN1cHBseSBjZW50ZXJzLgpUaGUgbWFwIGNvbnNpc3RzIG9mIGRpZmZlcmVudCByZWdpb25zIGFuZCBzZWEgYXJlYXMuIFRoZWlyIGFkamFjZW5jeSByZWxhdGlvbnNoaXBzIGFyZSBzaG93biBpbiB0aGUgbWF0cml4LiBUaGUgbnVtYmVycyBmb3IgdGhlIHJlZ2lvbnMgYW5kIHNlYSBhcmVhcyBhcmUgLi4uLi4uCkRpZmZlcmVudCByZWdpb25zIGFyZSBvY2N1cGllZCBieSBkaWZmZXJlbnQgY291bnRyaWVzLiBUaGUgb3duZXJzaGlwIG9mIHRoZSByZWdpb25zIGlzIHNob3duIGluIHRoZSBtYXRyaXguClRoZSByZWdpb24gQmVybGluLCAuLi4uLi4uLiBhcmUgc3VwcGx5IGNlbnRlcnMuCllvdSBuZWVkIHRvIGZvbGxvdyB0aGVzZSBydWxlcyAuLi4uLi4KVG8gaGVscCB5b3UgYWNoaWV2ZSB2aWN0b3J5LCB0aGVzZSBkaXBsb21hdGljIHN0cmF0ZWdpZXMgbWlnaHQgYmUgb2YgYXNzaXN0YW5jZS4gLi4uLi4uCg==)1You  will  control  {country}  and  compete  with  six  other  countries  on  the  map  for  supply  centers.2The  map  consists  of  different  regions  and  sea  areas.  Their  adjacency  relationships  are  shown  in  the  matrix.  The  numbers  for  the  regions  and  sea  areas  are  ......3Different  regions  are  occupied  by  different  countries.  The  ownership  of  the  regions  is  shown  in  the  matrix.4The  region  Berlin,  ........  are  supply  centers.5You  need  to  follow  these  rules  ......6To  help  you  achieve  victory,  these  diplomatic  strategies  might  be  of  assistance.  ......'
  prefs: []
  type: TYPE_NORMAL
- en: 2) Social Reasoning
  prefs: []
  type: TYPE_NORMAL
- en: '[‚¨á](data:text/plain;base64,RnJhbmNlIG9jY3VwaWVzIFBvcnR1Z2FsIFJ1aHIsIFBhcmlzLCBCdXJndW5keSwgLi4uLi4uCkZyYW5jZSBoYXMgYXJtaWVzIGluIEJyZXN0LCBCZWxnaXVtLCAuLi4uLi4gQW5kIEZyYW5jZSBoYXMgZmxlZXRzIGluIE1pZCBBdGxhbnRpYywgRW5nbGFuZCBDaGFubmVsLCAuLi4uLi4KRW5nbGFuZCAuLi4uLi4KLi4uLi4uCkJhc2VkIG9uIHRoZSBjdXJyZW50IHN0YXRlLCB3aGF0IGRvIHlvdSB0aGluayBhcmUgdGhlIGN1cnJlbnQgc3RyYXRlZ2ljIGludGVudGlvbnMgb2YgdGhlIG90aGVyIGNvdW50cmllcz8KV2hpY2ggY291bnRyeSBkbyB5b3UgdGhpbmsgbmVlZHMgdG8gYmUgYXR0YWNrZWQgb3Igd2Vha2VuZWQgdGhlIG1vc3QgcmlnaHQgbm93PwpBbmQgd2hpY2ggY291bnRyeSBkbyB5b3UgdGhpbmsgaXMgbW9zdCBzdWl0YWJsZSBmb3IgeW91IHRvIGFsbHkgd2l0aCBpbiBvcmRlciB0byBkZWFsIHdpdGggdGhpcyBjb3VudHJ5Pw==)1France  occupies  Portugal  Ruhr,  Paris,  Burgundy,  ......2France  has  armies  in  Brest,  Belgium,  ......  And  France  has  fleets  in  Mid  Atlantic,  England  Channel,  ......3England  ......4......5Based  on  the  current  state,  what  do  you  think  are  the  current  strategic  intentions  of  the  other  countries?6Which  country  do  you  think  needs  to  be  attacked  or  weakened  the  most  right  now?7And  which  country  do  you  think  is  most  suitable  for  you  to  ally  with  in  order  to  deal  with  this  country?'
  prefs: []
  type: TYPE_NORMAL
- en: 3) Planner with Reflection
  prefs: []
  type: TYPE_NORMAL
- en: '[‚¨á](data:text/plain;base64,SW4gdGhlIGN1cnJlbnQgc3RhdGUsIHdpdGgge2FsbHkgYW5kIGVuZW15fSwgd2hhdCBzdWItZ29hbCBkbyB5b3UgdGhpbmsgc2hvdWxkIGJlIHNldCBmb3Ige2NvdW50cnl9ID8KSSBoYXZlIGZvdW5kIHNvbWUgdXNlZnVsIGhpc3RvcmljYWwgZXhwZXJpZW5jZXMgZm9yIHlvdS4gUGxlYXNlIHJlZmxlY3Qgb24gYW5kIG9wdGltaXplIHlvdXIgc3ViLWdvYWwgYmFzZWQgb24gdGhlc2UgaGlzdG9yaWNhbCBleHBlcmllbmNlcy4KVGhlIHN1Yi1nb2FsIHlvdSBmb3JtdWxhdGVkIHdoZW4ge3N0YXRlfSB3YXMgdG8ge3N1Yi1nb2FsfS4gVGhlIGV2ZW50dWFsIHJlc3VsdCB3YXMge2Z1dHVyZX0uIFRoZSBldmFsdWF0aW9uICBmb3IgdGhpcyBzdWItZ29hbCBpcyB7c2NvcmV9Lgo=)1In  the  current  state,  with  {ally  and  enemy},  what  sub-goal  do  you  think  should  be  set  for  {country}  ?2I  have  found  some  useful  historical  experiences  for  you.  Please  reflect  on  and  optimize  your  sub-goal  based  on  these  historical  experiences.3The  sub-goal  you  formulated  when  {state}  was  to  {sub-goal}.  The  eventual  result  was  {future}.  The  evaluation  for  this  sub-goal  is  {score}.'
  prefs: []
  type: TYPE_NORMAL
