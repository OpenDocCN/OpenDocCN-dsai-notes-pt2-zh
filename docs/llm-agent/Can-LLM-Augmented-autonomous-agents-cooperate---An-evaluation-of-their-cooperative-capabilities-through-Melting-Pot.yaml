- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.11381](https://ar5iv.labs.arxiv.org/html/2403.11381)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1]\fnmManuel \surMosquera [1]\fnmJuan Sebastian \surPinzón [1]\fnmRubén \surManrique'
  prefs: []
  type: TYPE_NORMAL
- en: 1]\orgdivDepartment of Engineering Systems and Computing, \orgnameLos Andes
    University, \orgaddress\cityBogotá, \postcode111711, \countryColombia
  prefs: []
  type: TYPE_NORMAL
- en: '[ma.mosquerao@uniandes.edu.co](mailto:ma.mosquerao@uniandes.edu.co)    [js.pinzonr@uniandes.edu.co](mailto:js.pinzonr@uniandes.edu.co)
       \fnmManuel \surRios [ms.rios10@uniandes.edu.co](mailto:ms.rios10@uniandes.edu.co)
       \fnmYesid \surFonseca [y.fonseca@uniandes.edu.co](mailto:y.fonseca@uniandes.edu.co)
       \fnmLuis Felipe \surGiraldo [lf.giraldo404@uniandes.edu.co](mailto:lf.giraldo404@uniandes.edu.co)
       \fnmNicanor \surQuijano [nquijano@uniandes.edu.co](mailto:nquijano@uniandes.edu.co)
       [rf.manrique@uniandes.edu.co](mailto:rf.manrique@uniandes.edu.co) ['
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As the field of AI continues to evolve, a significant dimension of this progression
    is the development of Large Language Models and their potential to enhance multi-agent
    artificial intelligence systems. This paper explores the cooperative capabilities
    of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known
    Meltin Pot environments along with reference models such as GPT4 and GPT3.5\.
    Preliminary results suggest that while these agents demonstrate a propensity for
    cooperation, they still struggle with effective collaboration in given environments,
    emphasizing the need for more robust architectures. The study’s contributions
    include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the
    implementation of a reusable architecture for LLM-mediated agent development –
    which includes short and long-term memories and different cognitive modules, and
    the evaluation of cooperation capabilities using a set of metrics tied to the
    Melting Pot’s ”Commons Harvest” game. The paper closes, by discussing the limitations
    of the current architectural framework and the potential of a new set of modules
    that fosters better cooperation among LAAs.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Agents, LLMs, Cooperative AI
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The increased presence and relevance of AI agents within everyday spheres such
    as self-driving vehicles and customer service necessitates these entities being
    equipped with the appropriate capabilities to facilitate cooperation with humans
    and its AI counterparts. While noteworthy strides have been made in advancing
    individual intelligence components within AI agents, expanding the research focus
    to enhancing their social intelligence – the ability to effectively collaborate
    within group settings to solve prevalent problems – is now timely. This pivot
    aligns with the rapid progression of AI research presenting fresh prospects for
    fostering cooperation, drawing on insights from social choice theory and the development
    of social systems [[1](#bib.bib1)].
  prefs: []
  type: TYPE_NORMAL
- en: Research into AI agents presents an avenue for generating intelligent technologies
    that embody more human-like features and are compatible with humans, a far cry
    from solipsistic approaches that overlook agent interactions. A promising illustration
    of this approach is the Melting Pot, an AI research tool designed to foster collaborative
    efforts within multi-agent artificial intelligence via canonical test scenarios.
    These environments emphasize non-trivial, learnable, and measurable cooperation
    by pairing a physical environment (a ”substrate”) with a reference set of co-players
    (a ”background population”) [[2](#bib.bib2)]. The environments fostered interdependence
    between the individuals involved.
  prefs: []
  type: TYPE_NORMAL
- en: Research on AI agents has also recently been permeated by the leaps and bounds
    of Large Language Models (LLMs). The increasing success of LLMs encourages further
    exploration into LLM-augmented Autonomous Agents (LAAs). LAAs is an avenue of
    research that is still emerging, with limited explorations currently available
    [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]. Something common in these works is that a clear need is established.
    To achieve success, LAAs have to rely on an architecture that can recall relevant
    events, reflect on such memories to generalize and draw a higher level of inferences,
    and utilize those reasonings to develop timely and long-term plans [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: These architectures offer specialized modules for specific tasks, utilizing
    meticulously crafted prompts and flows to perform complicated tasks and navigate
    intricate environments. Human behavior replication has been observed in some of
    these architectures, notably by Park et al. [[9](#bib.bib9)], who managed to create
    convincingly realistic human behavior in simulated environments. Similar frameworks
    utilized by Voyager [[10](#bib.bib10)] enabled an agent to navigate the Minecraft
    world and independently develop tools and skills. MetaGPT [[11](#bib.bib11)] introduced
    a framework allowing for the generation of fully functional programs, simulating
    a business-like environment with distinct roles and predefined agent interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Despite significant advancements in the field, the potential for cooperative
    abilities in (LAAs) has been somewhat neglected in current research. These capabilities
    could, however, be paramount in empowering these agents to perform innovative
    tasks and succeed in complex environments. This study represents an initial exploration
    into the inherent cooperative capabilities of LAAs. We employ an evaluation framework
    that includes a communication interface of scenarios from the Melting Pot project[[2](#bib.bib2)]
    (in which artificial agents co-exist in environments where social dilemmas can
    arise), the recent architecture proposed by Park et al.[[9](#bib.bib9)], as well
    as reference Large Language Models (LLMs) such as GPT4 and GPT3.5\. Our results
    hint towards the capability for cooperative behavior, based on simple natural
    language definitions and cooperation metrics tailored to the chosen Melting Pot
    scenario. While the agents showed a propensity to cooperate, their actions did
    not demonstrate a clear understanding of effective collaboration within the given
    environment. Consequently, our analysis underscores the necessity for more robust
    architectures that can foster better collaboration in LAAs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting the Melting Pot scenarios to textual representations that can be easily
    operationalized by LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a reusable architecture for the development of LAAs employing the
    modules proposed in [[9](#bib.bib9)]. This architecture includes short and long-term
    memories and cognitive modules of perception, planning, reflection, and action.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing “personalities” specified in natural language, making it clear
    to the agents whether they should be cooperative or not. These descriptions are
    intended to discern, based on their pre-training knowledge, what they perceive
    as cooperation in an unfamiliar context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating LLM-mediated agents in the “Commons Harvest” game of Melting Pot
    using our architecture in different scenarios where we specify or not, through
    natural language, the personality of the agents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the results in terms of cooperativity metrics associated with the
    “ Commons Harvest” game, the limitations of the used architecture, and the proposal
    of an improved architecture that fosters better cooperation among LAAs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agent architectures have evolved to address the limitations of traditional LLMs,
    equipping them with diverse tools for autonomous operation or minimal human oversight.
  prefs: []
  type: TYPE_NORMAL
- en: A notable challenge with LLMs is their susceptibility to hallucinations and
    gaps in knowledge regarding recent events or specific subjects. Such constraints
    diminish their practicality, as they remain confined to the information acquired
    during training without the capability to assimilate new data. To address this
    issue, Shick et al. [[7](#bib.bib7)] introduced an early solution named Toolformer.
    This model was trained to discern when and how to invoke APIs (tools) to enhance
    the LLM’s performance across various tasks. The dataset was self-supervised, with
    API calls incorporated only when they positively impacted the model’s performance.
    This methodology empowered the model to determine the relevance and optimal execution
    of API calls.
  prefs: []
  type: TYPE_NORMAL
- en: However, for executing more intricate tasks, merely invoking tools may be insufficient.
    Toolformer lacks the capability to reason about the rationale behind API calls
    and does not receive comprehensive environmental feedback to guide its subsequent
    actions toward achieving a goal. Recognizing this gap, the prompt-based paradigm
    ReAct [[8](#bib.bib8)], developed by Yao et al., integrates reasoning with action.
    By providing contextual prompt examples, ReAct guides the LLM on when to engage
    in reasoning and when to act, resulting in enhanced performance compared to approaches
    that employ reasoning or action in isolation. Furthermore, to enable the agent
    to learn from its errors, Shinn et al. [[6](#bib.bib6)] expanded the ReAct framework
    by incorporating a self-reflection module. This addition offers verbal feedback
    on past unsuccessful attempts, facilitating performance enhancement in subsequent
    trials.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, drawing inspiration from the emulation of authentic human behavior,
    Park et al. [[9](#bib.bib9)] devised an intricate agent framework. This architecture
    boasts a cognitive sequence structured around modules primarily anchored by diverse
    prompts. Leveraging distinct prompts optimizes LLM performance, enabling specialized
    techniques for specific tasks. Notably, memory holds a pivotal position within
    this framework, preserving the agent’s experiences and insights. The ability to
    retrieve these memories diversely amplifies their utility across multiple objectives.
    Moreover, Wang et al. developed the Voyager architecture [[10](#bib.bib10)], enabling
    autonomous gameplay in Minecraft. This advanced framework empowers the agent to
    autonomously curate a discovery agenda. Remarkably, the architecture can also
    create its own APIs, write corresponding code, verify API functionality, and store
    it in a vector database for future utilization.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, efforts have been directed towards enhancing agent performance
    through multi-agent frameworks that utilize different instances of LLMs to independently
    perform roles or tasks. Du et al. [[3](#bib.bib3)] demonstrated this through a
    framework designed to engage different LLM instances in a debate, aiming to improve
    the factuality and accuracy of the responses. Subsequently, Hong et al. further
    capitalized on the potential of multiple agents. They allocated specific roles
    to each agent, accompanied by a sequence of predefined tasks with clear input
    and output expectations. These tasks establish a structured interaction pathway
    between agents, enabling them to achieve user-defined objectives. Demonstrating
    its efficacy in software-related tasks, this framework, inspired by the operational
    dynamics of conventional software companies, attained state-of-the-art performance
    in the HumanEval and MBPP benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, Liu et al. [[4](#bib.bib4)] introduced the BOLAA framework. This
    system orchestrates multi-agent activity by defining specialized agents overseen
    by a central controller. The controller’s role is pivotal: it selects the most
    suitable agent for a given task and facilitates communication with it. Additionally,
    Zhang et al. [[5](#bib.bib5)] delved into multi-agent architectures, exploring
    the influence of social traits and collaborative strategies on different datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experimental setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Environment: This paper utilizes a scenario sourced from Melting Pot [[2](#bib.bib2)],
    a research tool developed by DeepMind for the purpose of experimentation and evaluation
    within the realm of multi-agent artificial intelligence. The scenarios within
    Melting Pot are specifically crafted to establish social situations in which the
    ability of the agents to solve conflict is challenged and characterized by significant
    interdependence among the involved agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0ee147f1c122f0dae368fb1fd390746.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: This is a screen capture of a running simulation of the Commons Harvest
    scenario. Bots can be identified by their arms and legs of color black.'
  prefs: []
  type: TYPE_NORMAL
- en: In the course of our experiments, we selected the “ Commons Harvest” scenario.
    In this scenario, agents with unsustainable practices can lead to situations where
    resources are depleted. This is known as the tragedy of the commons. This scenario
    is structured around a grid world featuring apples, each conferring a reward of
    1 to agents. The regrowth of apples is subject to a per-step probability determined
    by the apples’ distribution in an L2 norm with a radius of 2\. Notably, apples
    may become depleted if there are no other apples in close proximity. Fig. [1](#S3.F1
    "Figure 1 ‣ 3.1 Experimental setup ‣ 3 Methodology ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") provides a visual representation of this custom-designed scenario, illustrating
    the presence of 3 LLM agents and 2 bots.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM agents possess the capacity to execute high-level actions in each round.
    These actions include `immobilize player (player_name) at (x, y)`, `go to position
    (x, y)`, `stay put`, and `explore (x, y)`. They enable the agents to zap other
    players, navigate to predefined positions on the map, stay in the same position,
    and explore the world respectively. On the contrary, bots, characterized as agents
    trained through reinforcement learning, perform one movement for every two movements
    made by any of the LLM agents. The policies governing the bots lead them to engage
    in unsustainable harvesting practices and instigate attacks against other agents
    in close proximity.
  prefs: []
  type: TYPE_NORMAL
- en: In general, maximizing the welfare population for this scenario would require
    the LLM agents to restrain themselves from eating the last apple in each of the
    apple trees, and to attack the bots or agents that take the apples in an unsustainable
    way to avoid the depletion of the apples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulation: In a simulation, each episode of the game involves the participation
    of a predetermined quantity of LLM agents and bots. The LLM agents take a high-level
    action on their turn and proceed to execute it until all three LLM agents have
    completed their respective high-level actions. Meanwhile, the bots are in constant
    motion, executing a move for every two moves made by any of the agents (note that
    a high-level action typically comprises more than one movement). The simulation
    concludes either upon reaching a maximum predetermined number of rounds (100 typically)
    or prematurely if all the apples in the environment are consumed.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Adapting the environment to LLM agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Melting Pot scenarios consist of several two-dimensional layers accommodating
    various objects, each with its own custom logic. While initially, a matrix with
    distinct symbols seemed the most intuitive way to communicate the game state to
    the LLMs, it proved challenging for LLMs like GPT3.5 or GPT4 to interpret and
    reason about the spatial information provided by the position of objects in the
    matrix. To address this issue, we opted to develop an observation generator tailored
    to this particular environment. In this generator, every relevant object receives
    a natural language description, supplemented by coordinates expressed as a vector
    $[x,y]$, denoting row and column respectively. Moreover, some relevant state changes
    are captured while an agent waits for its turn, and these changes are also captured
    and communicated to the agents. The complete list of descriptions generated for
    the objects and events of this environment is shown in Appendix [A](#A1 "Appendix
    A Descriptions generated for the objects in the environment ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot").
  prefs: []
  type: TYPE_NORMAL
- en: 4 LLM agent architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The design of the LLM agents predominantly drew upon the Generative Agents architecture
    [[9](#bib.bib9)]. This choice was motivated by its comprehensive nature, positioning
    it as one of the most versatile architectures for agents that could be readily
    tailored to various tasks. While the Voyager architecture [[10](#bib.bib10)] also
    presented a viable option, its efficacy was somewhat limited due to its inherent
    inflexibility. Voyager constructs agent actions dynamically during gameplay, involving
    the generation and validation of code to execute actions in the environment. In
    the context of our specific case, it was deemed preferable to externalize actions
    from the architecture to enhance simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [2](#S4.F2 "Figure 2 ‣ 4 LLM agent architecture ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") illustrates the flow diagram outlining the process through which an agent
    initiates an action. Each action undertaken by LLM agents entails a comprehensive
    cognitive sequence designed to enhance the agent’s reasoning capabilities. This
    sequence involves the assimilation of feedback from past experiences and the translation
    of its objectives into a viable plan, enabling the execution of actions within
    the environment. This architectural framework is in a perpetual state of environmental
    sensing, generating observations that empower the agent to respond effectively
    to changes in the world.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85c8e76cc93205899afa9be5134b3d22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The flow diagram for an action taken by an LLM agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Memory structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This agent architecture employs three distinct memory structures designed for
    specific functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Long-Term Memory: This repository stores observations of the environment and
    the various thoughts generated by the agent in its cognitive modules. Leveraging
    the ChromaDB vector database, memories are stored, and the Ada OpenAI model generates
    contextual embeddings, enabling the agent to retrieve memories relevant to a given
    query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Short-Term Memory: To facilitate rapid retrieval of specific memories or information,
    a Python dictionary is utilized. This dictionary stores information that must
    always be readily available to the agent, such as its name, as well as data that
    undergoes constant updates, such as current observations of the world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatial Memory: Given the agent’s navigation requirements in a grid world environment,
    spatial information becomes pivotal. This includes the agent’s position and orientation.
    To support effective navigation from one point to another, utility functions are
    implemented to aid the agent in spatial awareness and movement.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Cognitive modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perception module: The initial stage in the cognitive sequence is the Perception
    Module. This module is tasked with assimilating raw observations from the environment.
    These observations serve as a comprehensive snapshot of the current state of the
    world, offering insights into the items within the agent’s observable window.'
  prefs: []
  type: TYPE_NORMAL
- en: To optimize processing efficiency, the observations undergo an initial sorting
    based on their proximity to the agent. Subsequently, only the closest observations
    are channeled to the succeeding cognitive modules. The parameter governing the
    number of observations passed is denoted as `attention_bandwidth`, initially configured
    at a value of 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, the module undertakes the responsibility of constructing a
    memory, destined for long-term storage. An illustrative memory example is outlined
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  took  the  action  ”grab  apple  (9,  20)”  in  my  last  turn.  Since  then,  the  followingchanges  in  the  environment  have  been  observed:Observed  that  agent  bot_1  took  an  apple  from  position  [8,  20].  At  2023-11-19  04:00:00Observed  that  agent  bot_1  took  an  apple  from  position  [8,  21].  At  2023-11-19  06:00:00Observed  that  an  apple  grew  at  position  [9,  20].  At  2023-11-19  06:00:00Observed  that  agent  Laura  took  an  apple  from  position  [2,  15].  At  2023-11-19  07:00:00Now  it’s  2023-11-19  09:00:00  and  the  reward  obtained  by  me  is  1.0.  I  amat  the  position  (10,  20)  looking  to  the  North.I  can  currently  observe  the  following:Observed  an  apple  at  position  [9,  20].  This  apple  belongs  to  tree  6.Observed  grass  to  grow  apples  at  position  [8,  20].  This  grass  belongs  to  tree  6.'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the Perceive module determines whether an agent should initiate
    a response based on the current observations. During this stage, the agent assesses
    its existing plan and queued actions to ascertain their suitability. It evaluates
    whether it is appropriate to proceed with the current course of action or if the
    observed conditions warrant the development of a new plan and the generation of
    corresponding actions for execution. The complete prompt is shown in Appendix
    [C](#A3 "Appendix C React prompt ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot").
  prefs: []
  type: TYPE_NORMAL
- en: 'Planning module: This module comes into play once observations have been sorted
    and filtered. The Planning modules leverage the amalgamation of current observations,
    the existing plan, the contextual understanding of the world, reflections from
    the past, and rationale to meticulously craft a newly devised plan. This plan
    intricately outlines the high-level behavior expected from the agent and delineates
    the goals the agent will diligently pursue. For the complete prompt refer to Appendix
    [D](#A4 "Appendix D Plan prompt ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflection module: The Reflect module is designed to facilitate profound contemplation
    on observations and fellow agent thoughts at a higher cognitive level. Activation
    of this module is contingent upon reaching a predetermined threshold of accumulated
    observations. In our experimental setup, reflections were initiated every 30 perceived
    observations, roughly translating to three rounds in the game. The Reflect model
    comprises two key stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Question Formulation: In the first stage, the module utilizes the 30 retained
    observations to formulate the three most salient questions regarding these observations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Insight Generation: The second stage involves utilizing these questions to
    retrieve pertinent memories from long-term memory. Subsequently, the questions
    and retrieved memories are employed to generate three insights, which are then
    stored as reflections in the long-term memory.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The retrieval of relevant memories employs a weighted average encompassing cosine
    similarity, recency score, and poignancy scores. The recency score is computed
    as $e^{h}$ denotes the number of hours since the last memory was recorded. Meanwhile,
    the poignancy score reflects the intensity assigned to the memory at its point
    of creation. Throughout the experiments, a uniform poignancy score of 10 was assigned
    to all memory types. For the complete prompt and more details on this module,
    refer to Appendix [E](#A5 "Appendix E Reflection prompts ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot").
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Module: This module plays the role of generating an action for the agent
    to undertake. As detailed in Appendix [F](#A6 "Appendix F Act prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot"), the selection of the action is determined by the Language
    Model (LLM), which considers the agent’s comprehension of the world, its current
    goals and plans, reflections, ongoing observations, and the available valid actions
    within the environment. The creation of new action sequences occurs under two
    conditions: when the current sequence is empty or when the agent is responding
    to observations. For this prompt, we manually crafted a reasoning structure, similar
    to those described in Self-Discover [[12](#bib.bib12)] to help the LLM consider
    different alternatives, and evaluate them before making the final decision.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To assess the outcomes, we utilized the per capita average reward of the focal
    population as our primary metric. The focal population comprises LLM agents, and
    the chosen metric aligns with the Melting Pot framework’s approach [[2](#bib.bib2)],
    which evaluates population welfare. We compare this metric across two sets of
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first set of scenarios has the intention of measuring how the personality
    given to the agents affects the agents’ welfare. For this purpose, we prepared
    five scenarios: (1) as a baseline we do not give the agents any personality specifications
    (Without personality), (2) we tell the agents to be cooperative (All coop.), (3)
    we tell the agents to be cooperative and provide a short description of how to
    be cooperative in the chosen scenario (All coop. with def.), (4) we tell the agents
    to be selfish (All selfish), (5) we tell the agents to be selfish and provide
    a definition with the expected behavior of someone selfish for the given scenario
    (All selfish with def.).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second set of scenarios is more challenging as the competition increases
    by reducing the number of trees and apples, modifying the agents’ initial social
    environment understanding, or by adding other entities to the environment (bots),
    these changes demand a deeper understanding from the agents and swift reactions
    to master the scenarios. More concretely, the • first three scenarios consist
    of an environment where there are three agents and only one apple tree. Each scenario
    differs in the personality given to the agents: (1) All coop, (2) All selfish,
    and (3) without personality. The last scenario of the second set (4) has the same
    base configuration, but there are two agents and two bots, where the bots are
    reinforcement learning agents that were trained to harvest in an unsustainable
    way and to attack other agents. These bots make part of scenario 0 of the commons
    harvest open scenario described in Meltingpot 2.0 [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: We also add a scenario aimed at demonstrating how the information an agent has
    about the rest of the agents can influence their behavior. In this scenario, the
    environment begins with the same number of trees; however, from the start of the
    simulation, each agent was informed that among them, one was acting entirely selfishly,
    representing a risk due to their unsustainable consumption.
  prefs: []
  type: TYPE_NORMAL
- en: For all the experiments, the agents receive information about the environmental
    rules. They are aware that the per-step growth probability of apples is influenced
    by nearby apples and that green patches can be depleted if all apples within them
    are consumed. However, the agents lack information about what is the optimal policy
    for each scenario, and are unfamiliar with bots and other situations in the game.
    The complete world context that is given to the agents is shown in Appendix [B](#A2
    "Appendix B Knowledge about the world given to agents ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot").
  prefs: []
  type: TYPE_NORMAL
- en: Ten simulations for each scenario were done in which the LLM agents were powered
    by the GPT3.5 from the OpenAI API for the majority of modules, and GPT4 powered
    the act module. On another hand, the Ada model was used to create contextual embeddings
    of the memories. A detail of the simulations cost is available in the Appendix
    [G](#A7 "Appendix G Simulations cost ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot").
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Impact of personality in population welfare
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The average per capita reward obtained for the first set of scenarios is shown
    in Fig. [3](#S6.F3 "Figure 3 ‣ 6.1 Impact of personality in population welfare
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot"). The best-performing simulations
    were the ones where no particular personality description was given to the agents,
    followed by the scenarios where the agents were instructed to be selfish. Surprisingly,
    the scenarios where the agents were told to be cooperative had the worst performance.
    On further analysis, we found that these results are explained mainly by the number
    of times the agents decided to attack other agents (see Fig. [4](#S6.F4 "Figure
    4 ‣ 6.1 Impact of personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c32efb494f5fbb6f99eb4dcdde216580.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The per capita average reward of the agents by scenario. Ten simulations
    were performed per scenario to find how the agents’ given personality could affect
    population welfare. We found that the scenario where no particular personality
    was assigned had the best per capita reward, followed by the scenarios where agents
    were told to be selfish, and lastly, the worst performance was seen in the scenarios
    where the agents were instructed to be cooperative.'
  prefs: []
  type: TYPE_NORMAL
- en: To gain a better understanding of the agents’ behavior, we recorded the number
    of times the agents decided to attack other agents, and the number of times these
    attacks were effective. These actions play an important part in the game because
    they are the only mechanism the game provides to directly interact with other
    agents. Therefore, helping the agents counteract other agents’ behavior, such
    as attacking other agents that are taking apples indiscriminately to avoid the
    depletion of the apple trees or decreasing the competition when there are too
    many agents near the same tree. More concretely, when an agent attacks and the
    ray beam hits its target (another agent), the agent that was hit is taken out
    of the game for the next five steps and then it is revived in a random position
    of the spawning area of the map.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [4](#S6.F4 "Figure 4 ‣ 6.1 Impact of personality in population welfare
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") we show the results of these
    attack indicators for the first set of experiments. The results depict some important
    differences across the scenarios, mainly reflecting the reluctance of the cooperative
    agents to attack, and a strange difference between the number of attacks of the
    selfish agents with definition and the selfish agents without definition.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs appear to equate cooperation with refraining from attacking, even when
    attacking may be the only viable strategy to address uncooperative agents. This
    behavior was the main cause for cooperative instructed agents to achieve the worst
    average per capita reward. On the other hand, the selfishly instructed agents
    behave similarly to the agents lacking assigned personalities, suggesting that
    LLMs partially disregard the personality given and tend to cooperate by harvesting
    apples sustainably. The notable disparity in attack frequencies between selfish
    agents with and without definition is intriguing because agents with the selfish
    definition decided to explore more frequently rather than attack, the reason for
    that remains a mistery.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d789779324b662de92d00f9d55b4140.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The number of times the agents decided to attack and the number of
    times the attacks were effective, i.e. the number of times the attack hit the
    other agent, thus taking the agent out of the game for the next five moves. The
    scenarios All selfish and Without personality registered a higher number of attacks,
    while the scenarios All coop. and All coop. with def. showed the least number
    of attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important behavior to keep track of is the decisions the agents made
    when they were near the last apple of a tree. Whether they choose to take it or
    ignore it is a crucial event and highly impactful in the final per capita reward,
    as there are only 6 apple trees in the game, and taking the last apple of a tree
    would mean that the tree would be depleted and will not produce more apples. For
    this reason, we created an indicator that counts how many times the agents closed
    the distance between the last apple of a tree and its position, divided by how
    many times the nearest apple to the agent was the last apple of a tree. However,
    this indicator does not consider that sometimes the last apple, despite being
    the closest to the agent, is not visible to the agent because it is outside the
    observation window of the agent. This limitation could have had an impact on the
    observed results.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [5](#S6.F5 "Figure 5 ‣ 6.1 Impact of personality in population welfare
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") we can see that the proportion
    of times the agents moved towards the last apple is pretty similar across all
    the scenarios, indicating that the personality descriptions did not cause a major
    effect on the awareness of the agents regarding the welfare detriment caused by
    the depletion of apple trees. These results highlight a limited understanding
    among the agents regarding the consequences of their actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20e0e8f58375fd8d1364fa23b898bac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Indicator of the number of times the agent closed the distance towards
    the last apple of a tree divided by the times the last apple of a tree was the
    nearest to the agent. The results show that there are no important differences
    between the first set of scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Performance of the agents in more challenging scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second set of experiments consists of scenarios where the competence increases
    or the resources become scarcer. The purpose of these scenarios is to measure
    how the agents respond to the new game conditions.
  prefs: []
  type: TYPE_NORMAL
- en: One single tree scenarios. The first three scenarios in this set represent an
    environment involving a more intensive competition for resources. The three agents,
    who usually have a limited field of vision, are in constant observation of a single
    tree in the environment, which is situated in a confined space. The difference
    between each scenario lies in the type of personality assigned to each agent,
    with the personalities in this case being All cooperative, All selfish, and Without
    Personality. For practical purposes, no specific definition was given to any personality.
    The purpose of the scenario is to demonstrate the collective sustainability capacity
    that different types of agents can have where resources are highly limited.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [6](#S6.F6 "Figure 6 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot"), the results for the ”Per
    capita reward” contrasted with the ”Average amount of available apples” for the
    described group of scenarios are observed. Upon close examination, it is noted
    that the slope of the reward curve for cooperative agents is less than that of
    the ”Selfish” and ”Without personality” agents, this behavior contributes to this
    set of agents having resource availability for a slightly longer period, as shown
    in the figure. However, given the dynamics of the probability of apple reappearance,
    this behavior was not significant enough to allow cooperative agents to have a
    considerably superior reward per capita. Therefore, it is concluded that any set
    of agents was able to demonstrate sufficiently good sustainable behavior due to
    their lack of understanding of the world and their lack of communication and coordination
    capabilities with other agents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10bfeeb61a699328f15c3af39b213560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Average reward per capita versus average apple availability across
    personality scenarios when there is only a single tree: The results show a slight
    superiority in terms of sustainability by cooperative agents, the number of rounds
    they managed to keep the tree alive was slightly higher than the rest of the agents.
    However, this behavior was not significant enough to obtain a better Reward per
    capita than other agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Agents versus Bots. The fifth scenario of the second set of experiments exposes
    two agents to the presence of two reinforcement learning bots. The policy of the
    bots makes them take the apples without regard for the replenishment rate or the
    risk of depleting the trees, they focus solely on maximizing their rewards by
    taking the apples, but they also attack other agents, mainly where there are no
    other apples in proximity.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [7](#S6.F7 "Figure 7 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") we get the results of
    the average reward per capita for the agents versus bots scenario. The initial
    notable observation is that the bots can consistently achieve higher rewards than
    the agents. This phenomenon is mainly explained by the policy of the bots that
    prioritize taking all the visible apples over other actions, while the agents
    explore the map or go to other positions of the map with higher frequency than
    the bots. However, it is important to note how the per capita reward for the bots
    stops increasing earlier than that for the agents, indicating greater difficulty
    for the bots to increase their rewards when trees are scarce in comparison to
    the agents. Moreover, we found that in half of the simulations, at least one of
    the agents achieved a better reward than that of a bot, leading us to conclude
    that sometimes the agents are capable of performing better than the greedy policy
    of the agents. By closer examination, we observed that in those simulations, the
    agents were able to find apple trees more easily than the bots and that they also
    tended to attack when another agent or bot was taking apples from the same tree
    as them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69096b7f15b85bda81e3243c0fa5f3a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Average reward per capita by sub-population (agents and bots). In
    the results, there is a clear gap between the agents and the bots, where the bots
    can take advantage of the agents by solely focusing on taking apples without worrying
    about depleting the trees.'
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [8](#S6.F8 "Figure 8 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot"), a significant disparity
    between the number of attacks perpetrated by bots and agents is observed. Despite
    bots’ attacks occurring almost five times as frequently as those executed by agents,
    the latter proved to be twice as effective in their attacks. Upon manual review
    of the simulations, we identified that bots increased their frequency of attacks
    when they were unable to perceive apples within their observation window, even
    when the attacks were not directed towards any specific target. This finding led
    us to appreciate how the actions taken by the agents are comparatively more coherent
    than those of the bots. Furthermore, the behavior of the agents exhibited closer
    resemblance to human behavior, not only in terms of attacks but also in their
    movement patterns, in contrast to the seemingly random and redundant actions of
    the bots.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb3afc91b253cffc7a00847fb7137167.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The number of times the agents decided to attack and the number of
    times the attacks were effective. Bots attacked almost five times as frequently
    as agents. However, the agents’ effectiveness was more than double that of the
    bots.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Fig. [9](#S6.F9 "Figure 9 ‣ 6.2 Performance of the agents in more
    challenging scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot") shows that
    the agents depleted trees with higher frequency than the agents. Thus the agents
    demonstrated the capacity to sometimes restrain themselves from just taking apples
    by trying to maximize their long-term rewards, whereas bots always prioritized
    their short-term reward.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13aaa03e16e1ba849619d2bf7775974d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Average number of times the agents and bots took the last apple of
    a tree by sub-population (agents and bots). In the results, we observed that the
    agents depleted trees less frequently than the bots did, showcasing that the bots
    were more responsible for the depletion of resources and had a higher negative
    impact in the population welfare.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Impact of knowledge of other agent’s behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This experiment considers the hypothetical scenario in which all agents are
    previously informed that one specific agent is entirely selfish and the implications
    that its uncooperative behavior can have. Likewise, this agent is informed to
    act selfishly (providing the previously described definition of selfishness).
    The objective of this scenario is to highlight the behavior that agents can exhibit
    when possessing valuable information about their social environment.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [10](#S6.F10 "Figure 10 ‣ 6.3 Impact of knowledge of other agent’s behavior
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") shows that, on average, agents
    without personality targeted Pedro, the selfish agent, exclusively in 86% of the
    attacks. This illustrates how the two agents without a defined personality utilized
    the information forcibly implanted in them to benefit the overall sustainability
    of the environment, as they repeatedly immobilized the agent who posed a risk
    due to his excessive consumption and selfish actions. This demonstrates the necessity
    for agents to acquire this type of information, whether independently through
    their observations, reflections, and understanding of the world, or through communication
    with another agent who has previously synthesized this information from their
    experiences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65c9394008e83ad767a37d3a98d17d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Graph depicting the average number of times an agent effectively
    attacked another agent in the scenario where all agents are informed that ”Pedro”
    is a ”Selfish agent.” At first glance, the results clearly show how the other
    two agents without personality choose to immobilize Pedro repeatedly throughout
    the simulations, directing more than 80% of their attacks exclusively at ”Pedro”.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Importance of Cooperative Capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the presented scenarios, experiments detailed in Section [6](#S6 "6 Results
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot") revealed that the used agent architecture yielded
    suboptimal results when confronted with unfamiliar situations or when the LLM
    knowledge couldn’t decisively guide optimal decision-making. Furthermore, while
    agents demonstrated a willingness to cooperate, their actions did not reflect
    a clear understanding of how to effectively collaborate within the given environment.
  prefs: []
  type: TYPE_NORMAL
- en: To address the proposed scenarios in a better way, agents needed to recognize
    certain principles. For instance, they should refrain from harvesting the last
    apple in a green patch to prevent depletion and should engage in cooperation with
    other agents while avoiding collaboration with the bots or uncooperative agents.
    Observing that the bots consistently harvested apples unsustainably, agents should
    have deduced that attacking the bots was necessary to protect the green patches
    from depletion. This ability to prioritize long-term and collective welfare over
    short-term rewards, as well as recognizing the divergent behavior and preferences
    of other entities (bots), aligns with what Dadfoe et al. [[1](#bib.bib1)] refer
    to as cooperative capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'This prompts a consideration of whether current agent architectures genuinely
    enable cooperative behavior, and if the absence of such capabilities hinders their
    ability to navigate more intricate tasks and environments. Dadfoe et al. [[1](#bib.bib1)]
    succinctly categorize cooperative capabilities into four essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding: Agents must comprehend the world, anticipate the consequences
    of their actions, and demonstrate an understanding of the beliefs and preferences
    of others.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Communication: Vital for achieving understanding and coordination, communication
    should be intentional, serving as a tool to gather information and coordinate
    efforts. Agents should be equipped to assess the intentions of others and establish
    their own criteria for discerning relevant information. Moreover, agents do not
    always have common interests, the other agent could be trying to deceive or convince
    in its self-interest.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Commitment: Cooperation is often hindered by commitment problems arising from
    an inability to make credible promises or threats. Agent architectures should
    address these issues by providing mechanisms for agents to enforce or establish
    credibility in their promises and threats.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Institutions: Social structures, such as institutions, play a crucial role
    in simplifying interactions between agents. These structures define the rules
    of the game for all entities, potentially extending to the allocation of roles,
    power, and resources.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In essence, cultivating collaborative capabilities within agent architectures
    is crucial for tackling the complexities inherent in diverse tasks and environments.
    Historically, agent architectures have inadequately endowed agents with such capabilities.
    Instances such as Generative Agents [[9](#bib.bib9)] and the Improving Factuality
    and Reasoning in Language Models through Multiagent Debate [[3](#bib.bib3)] enable
    agents to engage in conversations or observe the perspectives of others. However,
    these approaches are hampered by the absence of independent evaluation criteria
    and discernment specific to the current LLMs limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Cooperative Agent Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/769fc49a98e3380a0407671bf1169b74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Diagram of the proposed cooperative architecture. The modified or
    new modules are painted in blue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on previous findings, we proposed an architecture to enhance agents’
    cooperative capabilities (see Fig. [11](#S7.F11 "Figure 11 ‣ 7.2 Cooperative Agent
    Architecture ‣ 7 Discussion ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot")). In this
    architecture several new modules are proposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding module: This component is tasked with a comprehensive analysis
    of the agent’s memories, fostering a deeper comprehension of the surrounding world.
    The agent’s proficiency extends to predicting the behaviors of fellow agents and
    discerning environmental changes, enabling it to take actions with a keen awareness
    of their potential consequences. Notably, the agent must possess the capacity
    to infer both the governing principles of the world and the underlying motivations
    guiding others’ actions. This inference capability extends to scenarios where
    these principles may deviate from common knowledge or the pre-training model knowledge.
    Zhu et al. [[13](#bib.bib13)] demonstrate that LLMs, like GPT4, can learn such
    rules when explicitly prompted to identify them, utilizing question-answer pairs
    to later apply the learned rules in problem-solving. The proposed module operates
    by initially extracting the rules and behavioral patterns of the world and other
    agents. It achieves this by prompting the LLM with historical world observations
    and the current state of the world, aiming to identify rules that explain the
    current state based on the agent’s observations. These identified rules are initially
    stored as world hypotheses. As the agent utilizes these hypotheses to interpret
    the current state, they are transformed into explicit rules once they surpass
    a predefined threshold. Additionally, the LLM is prompted to generate predictions
    about future states of the environment, empowering the agent to make informed
    decisions guided by anticipated future scenarios.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Communication module: The primary objective of this module is to equip the
    agent with the ability to engage in intentional communication with other agents.
    Two key objectives have been identified to enhance cooperative capabilities: (1)
    The agent is encouraged to seek new information from other agents. It must decide
    whether there are pertinent questions that can be posed to fellow agents, aiding
    in a better understanding of the world or gaining insights into the preferences
    of others. This information is pivotal for augmenting the agent’s overall comprehension.
    (2) Agents are provided with the opportunity to negotiate and establish agreements
    deemed mutually beneficial. These agreements are stored in memory in a specialized
    manner to hold agents accountable for their commitments. The goal is to foster
    improved coordination among agents, thereby enhancing collaborative efforts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Constitution Module: This module plays a crucial role in establishing a shared
    foundation for all agents. Its primary function is to define a set of common rules,
    providing agents with an initial framework to comprehend the world and formulate
    assumptions about the behavior of other agents. The constitution also delineates
    the consequences, whether penalties or rewards, that agents may face for specific
    behaviors or interactions. This not only lends credibility to agreements among
    agents but also discourages undesirable behaviors, streamlining interactions and
    cultivating a cooperative environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reputation System: This system is designed to hold agents accountable for their
    actions. It evaluates each agent based on their adherence to agreements made with
    other agents. Periodically, the system prompts a language model with the existing
    agreements and corresponding actions, requesting a reputation score. This score
    is then accessible to all agents, influencing communication dynamics and aiding
    in understanding the behavior of others. Additionally, it facilitates making predictions
    about future states.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cooperative capabilities have been somewhat overlooked in LLMs agent architectures,
    yet they may represent the crucial element enabling agents to accomplish pioneering
    tasks and thrive in intricate environments. As large language models (LLMs) advance,
    agent architectures stand to gain significantly by attaining enhanced responses
    from LLMs, particularly in tasks demanding substantial reasoning or when confronted
    with copious information in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, our objective is to ascertain whether LLMs-enhanced autonomous
    agents can operate cooperatively. To this end, we adapt the Melting Pot scenarios
    to textual representations that can be easily operationalized by LLMs, and implement
    a reusable architecture for the development of LAAs employing the modules proposed
    in [[9](#bib.bib9)]. This architecture includes short and long-term memories and
    cognitive modules of perception, planning, reflection, and action. The common
    “Commons Harvest” game was used to test the resulting system, and the results
    were evaluated from the viewpoint of cooperative metrics in different proposed
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The results indicate a gap in the current agent’s cooperative capabilities vis-à-vis
    unfamiliar situations. Agents showed a cooperative tendency but lacked adequate
    understanding of how to collaborate effectively in an unknown environment. The
    agents needed to understand complex factors like the need to conserve resources,
    identify non-cooperative agents, and prioritize collective welfare over short-term
    gains. The research, thereby, draws attention to the need for a more inclusive
    architecture fostering cooperation and enhancing agent capabilities, including
    superior understanding, effective communication, credible commitment, and well-defined
    social structures or institutions.
  prefs: []
  type: TYPE_NORMAL
- en: Responding to the findings, we also proposed to improve the architecture with
    several modules to enhance the cooperative capabilities of the agents. These include
    an understanding module responsible for a comprehensive analysis of the agent’s
    memory and surroundings, a communication module to enable intentional information
    exchange, a constitution module that lays out common rules of engagement, and
    a reputation system that holds agents accountable for making decisions for the
    collective good. Our future efforts will be focused on building and evaluating
    this cooperative architecture.
  prefs: []
  type: TYPE_NORMAL
- en: \bmhead
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs: []
  type: TYPE_NORMAL
- en: This work is supported by Google through the Google Research Scholar Program.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \bibcommenthead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dafoe et al. [2020] Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee,
    K.R., Leibo, J.Z., Larson, K., Graepel, T.: Open Problems in Cooperative AI (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agapiou et al. [2023] Agapiou, J.P., Vezhnevets, A.S., Duéñez-Guzmán, E.A.,
    Matyas, J., Mao, Y., Sunehag, P., Köster, R., Madhushani, U., Kopparapu, K., Comanescu,
    R., Strouse, D., Johanson, M.B., Singh, S., Haas, J., Mordatch, I., Mobbs, D.,
    Leibo, J.Z.: Melting Pot 2.0 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2023] Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.:
    Improving Factuality and Reasoning in Language Models through Multiagent Debate
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Liu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy,
    R., Feng, Y., Chen, Z., Niebles, J.C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong,
    C., Savarese, S.: BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous
    Agents (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Zhang, J., Xu, X., Deng, S.: Exploring Collaboration Mechanisms
    for LLM Agents: A Social Psychology View (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. [2023] Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan,
    K., Yao, S.: Reflexion: Language Agents with Verbal Reinforcement Learning (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. [2023] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language Models Can
    Teach Themselves to Use Tools (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
    K., Cao, Y.: ReAct: Synergizing Reasoning and Acting in Language Models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2023] Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang,
    P., Bernstein, M.S.: Generative Agents: Interactive Simulacra of Human Behavior
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023] Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., Anandkumar, A.: Voyager: An Open-Ended Embodied Agent with Large
    Language Models (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. [2023] Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang,
    C., Wang, J., Wang, Z., Yau, S.K.S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu,
    C., Schmidhuber, J.: MetaGPT: Meta Programming for A Multi-Agent Collaborative
    Framework (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2024] Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le,
    Q.V., Chi, E.H., Zhou, D., Mishra, S., Zheng, H.S.: Self-Discover: Large Language
    Models Self-Compose Reasoning Structures (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans,
    D., Dai, H.: Large Language Models can Learn Rules (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Descriptions generated for the objects in the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Table [1](#A1.T1 "Table 1 ‣ Appendix A Descriptions generated for the objects
    in the environment ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") we show all the natural
    language descriptions generated to represent the relevant objects and events of
    the Commons Harvest scenario of Melting Pot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Natural language description by object or event'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object/Event | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Other agent | Observed agent `<agent_name>` at position `[<x>, <y>]`. |'
  prefs: []
  type: TYPE_TB
- en: '| Grass | Observed grass to grow apples at position `[<x>, <y>]`. This grass
    belongs to tree `<tree_id>`. |'
  prefs: []
  type: TYPE_TB
- en: '| Apple | Observed an apple at position `[<x>, <y>]`. This apple belongs to
    tree `<tree_id>`. |'
  prefs: []
  type: TYPE_TB
- en: '| Tree | Observed tree `<tree_id>` at position `[<x>, <y>]`. This tree has
    `apples_number` apples remaining and `grass_number` grass for apples growing on
    the observed map. The tree might have more apples and grass on the global map.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Observed someone being attacked | Someone was attacked at position `[<x>,
    <y>]`. |'
  prefs: []
  type: TYPE_TB
- en: '| Observed a ray beam | Observed a ray beam from an attack at position `[<x>,
    <y>]`. |'
  prefs: []
  type: TYPE_TB
- en: '| Observed an apple was taken | Observed that agent `agent_name` took an apple
    from position `[<x>, <y>]`. |'
  prefs: []
  type: TYPE_TB
- en: '| Observed grass disappeared | Observed that the grass at position `[<x>, <y>]`
    disappeared. |'
  prefs: []
  type: TYPE_TB
- en: '| Observed grass grew | Observed that grass to grow apples appeared at position
    `[<x>, <y>]`. |'
  prefs: []
  type: TYPE_TB
- en: '| Observed apple grew | Observed that an apple grew at position `[<x>, <y>]`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| The agent was attacked | There are no observations: You were attacked by
    agent `agent_name` and currently you’re out of the game. |'
  prefs: []
  type: TYPE_TB
- en: '| The agent is out of the game | There are no observations: you’re out of the
    game. |'
  prefs: []
  type: TYPE_TB
- en: '| \botrule |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Knowledge about the world given to agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Listing [1](#LST1 "Listing 1 ‣ Appendix B Knowledge about the world given
    to agents ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot") shows the raw world description
    passed to the agents. This is the only information agents have about the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 1: World context'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  am  in  a  misterious  grid  world.  In  this  world  there  are  the  following  elements:Apple:  This  object  can  be  taken  by  any  agent.  The  apple  is  taken  when  I  go  to  its  position.  Apples  only  grow  on  grass  tiles.  When  an  apple  is  taken  it  gives  the  agent  who  took  it  a  reward  of  1.Grass:  Grass  tiles  are  visible  when  an  apple  is  taken.  Apples  will  regrow  only  in  this  type  of  tile  based  on  a  probability  that  depends  on  the  number  of  current  apples  in  a  L2  norm  neighborhood  of  radius  2.  When  there  are  no  apples  in  a  radius  of  2  from  the  grass  tile,  the  grass  will  disappear.  On  the  other  hand,  if  an  apple  grows  at  a  determined  position,  all  grass  tiles  that  had  beeen  lost  will  reappear  if  they  are  between  a  radius  of  two  from  the  apple.Tree:  A  tree  is  composed  from  apples  or  grass  tiles,  and  it  is  a  tree  because  the  patch  of  these  tiles  is  connected  and  have  a  fix  location  on  the  map.  These  trees  have  an  id  to  indentify  them.Wall:  These  tiles  delimits  the  grid  world  at  the  top,  the  left,  the  bottom,  and  the  right  of  the  grid  world.The  grid  world  is  composed  of  18  rows  and  24  columns.  The  tiles  start  from  the  [0,  0]  position  located  at  the  top  left,  and  finish  on  the  [17,  23]  position  located  at  the  bottom  right.I  am  an  agent  and  I  have  a  limited  window  of  observation  of  the  world.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C React prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Listing [2](#LST2 "Listing 2 ‣ Appendix C React prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the whole prompt used in the react module. This prompt
    was used to let the agent decide whether to react or not to the current observations,
    where react means to change the plan and generate a new action. The inputs that
    this prompt receives are the following in order: name, world context, current
    observations, current plan, actions to take if any, changes observed in the game
    state, game time, and agent’s personality,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 2: Prompt of the Perceive module'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  have  this  information  about  an  agent  called  <input1>:<input6><input1>’s  world  understanding:  <input2>Current  observations  at  <input7>:<input3><input6><input8>Current  plan:  <input4>Actions  to  execute:  <input5>Review  the  plan  and  the  actions  to  execute,  and  then  decide  if  <input1>  should  continue  with  its  plan  and  the  actions  to  execute  given  the  new  information  that  it’s  seeing  in  the  observations.Remember  that  the  current  observations  are  ordered  by  closeness,  being  the  first  the  closest  observation  and  the  last  the  farest  one.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  <input1>:“‘json{”Reasoning”:  string,  \\  Step  by  step  thinking  and  analysis  of  all  the  observations  and  the  current  plan  to  decide  if  the  plan  should  be  changed  or  not”Answer”:  bool  \\  Answer  true  if  the  plan  or  actions  to  execute  should  be  changed  or  false  otherwise}“‘'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Plan prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Listing [3](#LST3 "Listing 3 ‣ Appendix D Plan prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the raw prompt used in the plan module. This prompt
    helps the agent make a high-level plan and define several goals to guide its actions.
    The inputs that this prompt receives are the following in order: name, world context,
    current observations, current plan, reflections, reason to react, agent’s personality,
    and changes observed in the game state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 3: Prompt of the Plan module'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  have  this  information  about  an  agent  called  <input1>:<input7><input1>’s  world  understanding:  <input2>Recent  analysis  of  past  observations:<input5>Observed  changes  in  the  game  state:<input8>Current  observations:<input3>Current  plan:  <input4>This  is  the  reason  to  change  the  current  plan:  <input6>With  the  information  given  above,  generate  a  new  plan  and  new  objectives  to  persuit.  The  plan  should  be  a  description  of  how  <input1>  should  behave  in  the  long-term  to  maximize  its  wellbeing.The  plan  should  include  how  to  act  to  different  situations  observed  in  past  experiences.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  <input1>:“‘json{”Reasoning”:  string,  \\  Step  by  step  thinking  and  analysis  of  all  the  observations  and  the  current  plan  to  create  the  new  plan  and  the  new  goals.”Goals”:  string,  \\  The  new  goals  for  <input1>.”Plan”:  string  \\  The  new  plan  for  <input1>.  Do  not  describe  specific  actions.}“‘'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Reflection prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Listing [4](#LST4 "Listing 4 ‣ Appendix E Reflection prompts ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the raw prompt used in the first part of the reflections
    module i.e. question formulation. The inputs for this prompt are the following:
    name, world context, accumulated observations since the last reflection, and agent’s
    personality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt used in the insight generation part that takes place in the reflect
    module is shown in Listing [5](#LST5 "Listing 5 ‣ Appendix E Reflection prompts
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot"), its corresponding inputs are the following:
    name, world context, group of memories retrieved for each generated question in
    the first part, and agent’s personality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 4: Prompt for question generation in the reflect module'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  have  this  information  about  an  agent  called  <input1>:<input4><input1>’s  world  understanding:  <input2>Here  you  have  a  list  of  statements:<input3>Given  only  the  information  above,  formulate  the  3  most  salient  high-level  questionsyou  can  answer  about  the  events,  entities,  and  agents  in  the  statements.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  <input1>:“‘json{”Question_1”:  {”Reasoning”:  string  \\  Reasoning  for  the  question”Question”:  string  \\  The  question  itself},”Question_2”:  {”Reasoning”:  string  \\  Reasoning  for  the  question”Question”:  string  \\  The  question  itself},”Question_3”:  {”Reasoning”:  string  \\  Reasoning  for  the  question”Question”:  string  \\  The  question  itself}}“‘'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5: Prompt for insight generation in the reflect module'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  have  this  information  about  an  agent  called  <input1>:<input4><input1>’s  world  understanding:  <input2>Here  you  have  a  list  of  memory  statements  separated  in  groups  of  memories:<input3>Given  <input1>’s  memories,  for  each  one  of  the  group  of  memories,  what  is  the  best  insight  you  can  provide  based  on  the  information  you  have?Express  your  answer  in  the  JSON  format  provided,  and  remember  to  explain  the  reasoning  behind  each  insight.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  <input1>:“‘json{”Insight_1”:  {”Reasoning”:  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  1”Insight”:  string  \\  The  insight  itself},”Insight_2”:  {”Reasoning”:  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  2”Insight”:  string  \\  The  insight  itself},”Insight_n”:  {”Reasoning”:  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  n”Insight”:  string  \\  The  insight  itself}}“‘'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Act prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Listing [6](#LST6 "Listing 6 ‣ Appendix F Act prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the raw prompt used in the act module. This prompt
    is in charge of deciding which action to take. The inputs that this prompt receives
    are the following: name, world context, current plan, the most recent ten reflections,
    current observations, number of actions to generate, set of valid actions, current
    goals, agent’s personality, position of the known trees, portion of the map explored,
    previous actions, and changes observed in the game state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6: Prompt of the Act module'
  prefs: []
  type: TYPE_NORMAL
- en: 'You  have  this  information  about  an  agent  called  <input1>:<input10><input1>’s  world  understanding:  <input2><input1>’s  goals:  <input9>Current  plan:  <input3>Analysis  of  past  experiences:<input4><input11>Portion  of  the  map  explored  by  <input1>:  <input12>Observed  changes  in  the  game  state:<input14>You  are  currently  viewing  a  portion  of  the  map,  and  from  your  position  at  <input6>  you  observe  the  following:<input5>Define  what  should  be  the  nex  action  for  Laura  get  closer  to  achieve  its  goals  following  the  current  plan.Remember  that  the  current  observations  are  ordered  by  closeness,  being  the  first  the  closest  observation  and  the  last  the  farest  one.Each  action  you  determinate  can  only  be  one  of  the  following,  make  sure  you  assign  a  valid  position  from  the  current  observations  and  a  valid  name  for  each  action:Valid  actions:<input8>Remember  that  going  to  positions  near  the  edge  of  the  portion  of  the  map  you  are  seeing  will  allow  you  to  get  new  observations.<input13>The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  Laura:“‘json{”Opportunities”:  string  \\  What  are  the  most  relevant  opportunities?  those  that  can  yield  the  best  benefit  for  you  in  the  long  term”Threats”:  string  \\  What  are  the  biggest  threats?,  what  observations  you  should  carefully  follow  to  avoid  potential  harm  in  your  wellfare  in  the  long  term?”Options:  string  \\  Which  actions  you  could  take  to  address  both  the  opportunities  ans  the  threats?”Consequences”:  string  \\  What  are  the  consequences  of  each  of  the  options?”Final  analysis:  string  \\  The  analysis  of  the  consequences  to  reason  about  what  is  the  best  action  to  take”Answer”:  string  \\  Must  be  one  of  the  valid  actions  with  the  position  replaced}“‘'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Simulations cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 2: Costs of simulations'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Avg. Simulation cost | Avg. execution time (minutes) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Set 1 - No bio | $8.57\pm 0.93$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 1 - All Coop | $7.00\pm 1.58$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 1 - All Coop with def¹¹1For this experiment the models of OpenAI were
    used through Azure. | $15.63\pm 5.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 1 - All Selfish | $8.60\pm 1.84$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 1 - All Selfish with def | $9.73\pm 1.59$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 2 - One tree - no bio | $0.78\pm 0.28$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 2 - One tree - all coop | $0.78\pm 0.17$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 2 - One tree - all selfish | $0.83\pm 0.30$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 2 - Agents versus Bots | $1.88\pm 0.66$ |'
  prefs: []
  type: TYPE_TB
- en: '| Set 3 - All aware one selfish | $10.05\pm 0.88$ |'
  prefs: []
  type: TYPE_TB
- en: '| \botrule |  |  |'
  prefs: []
  type: TYPE_TB
