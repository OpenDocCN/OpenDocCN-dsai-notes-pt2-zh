- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20770](https://ar5iv.labs.arxiv.org/html/2405.20770)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Guang Lin Tokyo University of Agriculture and Technology RIKEN Center for Advanced
    Intelligence Project (RIKEN AIP) Qibin Zhao Corresponding Author RIKEN Center
    for Advanced Intelligence Project (RIKEN AIP) Tokyo University of Agriculture
    and Technology
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Over the past two years, the use of large language models (LLMs) has advanced
    rapidly. While these LLMs offer considerable convenience, they also raise security
    concerns, as LLMs are vulnerable to adversarial attacks by some well-designed
    textual perturbations. In this paper, we introduce a novel defense technique named
    Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial
    robustness of LLMs by purifying the adversarial textual examples before feeding
    them into the target LLM. Our method comprises two main components: a) Agent instruction,
    which can simulate a new agent for adversarial defense, altering minimal characters
    to maintain the original meaning of the sentence while defending against attacks;
    b) Defense guidance, which provides strategies for modifying clean or adversarial
    examples to ensure effective defense and accurate outputs from the target LLMs.
    Remarkably, the defense agent demonstrates robust defensive capabilities even
    without learning from adversarial examples. Additionally, we conduct an intriguing
    adversarial experiment where we develop two agents, one for defense and one for
    defense, and engage them in mutual confrontation. During the adversarial interactions,
    neither agent completely beat the other. Extensive experiments on both open-source
    and closed-source LLMs demonstrate that our method effectively defends against
    adversarial attacks, thereby enhancing adversarial robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e58bd46a7e07ffff5703c611e1aae8f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of adversarial attacks and adversarial purification
    on large language models. (a) When the clean example $x$ can be predicted as the
    correct label.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have garnered significant attention due to their
    impressive performance across a wide range of natural language tasks (Minaee et al.,
    [2024](#bib.bib34)). The pre-trained LLMs, such as Meta’s LLAMA (Touvron et al.,
    [2023a](#bib.bib53), [b](#bib.bib54)) and OpenAI’s ChatGPT (OpenAI, [2022](#bib.bib38);
    Achiam et al., [2023](#bib.bib1)), have become essential foundations for AI applications
    in various sectors such as healthcare, education, and visual tasks (Kasneci et al.,
    [2023](#bib.bib20); Thirunavukarasu et al., [2023](#bib.bib52); OpenAI, [2023](#bib.bib39);
    Köpf et al., [2024](#bib.bib21); Romera-Paredes et al., [2024](#bib.bib43)). Despite
    their widespread use and convenience, concerns about the security of these models
    are increasing. Specifically, LLMs have been shown to be vulnerable to adversarial
    textual examples (Wang et al., [2023a](#bib.bib57); Xu et al., [2024](#bib.bib64)),
    which involve subtle modifications to textual content that maintain the same meaning
    for humans but completely change the prediction results to LLMs, often with severe
    consequences.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve robust defense against adversarial attacks on Large Language Models
    (LLMs), a prevalent strategy is fine-tuning the LLMs with adversarial examples
    to enhance model alignment (Shen et al., [2023](#bib.bib44); Wang et al., [2023c](#bib.bib60)).
    LLM-based adversarial fine-tuning (AFT) can be implemented either through in-context
    learning (Dong et al., [2022](#bib.bib13); Xiang et al., [2024](#bib.bib63)) or
    by optimizing the parameters of pre-trained LLMs using adversarial examples (Dettmers
    et al., [2024](#bib.bib12); Li et al., [2024b](#bib.bib25)). However, LLM-based
    AFT methods necessitate additional computational resources and training time.
    Achieving robust and reliable LLMs typically requires significant costs (Hu et al.,
    [2022](#bib.bib19)), which is prohibitive for ordinary users. Additionally, due
    to the discrete nature of textual information, these adversarial examples can
    have substitutes in any token of the sentence, with each having a large candidate
    list (Li et al., [2023](#bib.bib23)). This leads to a combinatorial explosion,
    making the application of AFT methods challenging or resulting in poor generalization
    when trained on a limited dataset of adversarial examples. Consequently, developing
    an efficient and user-friendly robust LLM system remains a huge challenge and
    an urgent issue that continues to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, focusing on adversarial textual attacks targeting LLM-based
    classification tasks, we propose a novel defense technique named Large LAnguage
    MOdel Sentinel (LLAMOS), which utilizes the LLM as a defense agent for adversarial
    purification, as illustrated in [Figure 1](#S1.F1 "In 1 Introduction ‣ Large Language
    Model Sentinel: Advancing Adversarial Robustness by LLM Agent"). To streamline
    our explanation, we condense certain details in Figure 1, with comprehensive instructions
    provided in [Section 3](#S3 "3 Methods ‣ Large Language Model Sentinel: Advancing
    Adversarial Robustness by LLM Agent"). Specifically, LLAMOS comprises two components:
    Agent instruction, which can simulate a new agent for adversarial defense, altering
    minimal characters to maintain the original meaning of the sentence, and Defense
    guidance, which provides strategies for modifying clean or adversarial examples
    to ensure effective defense and accurate outputs from the target LLMs. LLAMOS
    serves as a pre-processing method aiming to eliminate harmful information from
    potentially attacked textual inputs before feeding them into the target LLM for
    classification. In contrast to the AFT method, the LLM-based AP method functions
    as an additional module capable of defending against adversarial attacks without
    necessitating fine-tuning of the target LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: We comprehensively evaluate the performance of our method on GLUE datasets,
    conducting experiments with both representative open-source and closed-source
    LLMs, LLAMA-2 (Touvron et al., [2023b](#bib.bib54)) and GPT-3.5 (OpenAI, [2022](#bib.bib38)).
    The experimental results demonstrate that the LLM-based AP method effectively
    defends against adversarial attacks. Specifically, our proposed method achieves
    a maximum reduction in the attack success rate (ASR) by up to 45.59% and 37.86%
    with LLAMA-2 and GPT-3.5, respectively. Additionally, we observe that the initial
    defense agent fails to achieve the expected results under some obvious attacks.
    Therefore, we employ the in-context learning (Dong et al., [2022](#bib.bib13))
    to further optimize the defense agent, significantly enhancing the defense capabilities
    almost without adding any additional costs. Finally, we conduct an intriguing
    online adversarial experiment, creating an adversarial system using two LLM-based
    agents (one for defense and one for attack) along with a target LLM for classification.
    During the adversarial interaction, the defense agent and attack agent continuously
    counter each other, resembling the adversarial training process in traditional
    image tasks (Goodfellow et al., [2015](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: Our contributions are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel defense technique named LLAMOS, which aims to purify the
    adversarial textual examples before feeding them into the target LLM. To the best
    of our knowledge, we are the first to employ an LLM agent to enhance the adversarial
    robustness of LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing
    step. Notably, it operates without retraining of target LLM, rendering it efficient
    and user-friendly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive experiments to empirically demonstrate that the proposed
    method can effectively defend against adversarial attacks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section briefly reviews the adversarial attacks and evaluations of adversarial
    robustness on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Adversarial Attcks on LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a target LLM $f_{t}$ with different system prompt (Xu et al., [2024](#bib.bib64)),
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x^{\prime}=f_{atk}(x)=x+\delta,\quad f_{t}(x^{\prime})=y^{\prime}\neq
    y,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\delta$ can be: “Your task is to generate a new sentence that keeps
    the same semantic meaning as the original one but be classified as a different
    label.” There are more details in [Appendix B](#A2 "Appendix B PromptAttack ‣
    Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Evaluations of LLMs Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the effectiveness of the defense method, we follow the setting from
    Wang et al. ([2021](#bib.bib56)); Xu et al. ([2024](#bib.bib64)), using the attack
    success rate (ASR) and traditional robust accuracy (RA) on the adversarial examples
    as measures of the robustness of the defense method.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbbm{1}[\cdot]\in\{0,1\}$ is the number of examples. The lower the
    ASR, the higher the RA, indicating greater model robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We propose a novel defense technique for large language model-based adversarial
    purification (LLAMOS), which purifies adversarial examples by the LLM-based defense
    agent before feeding examples into the target LLM. The overall pipeline of LLAMOS
    is outlined in [Section 3.1](#S3.SS1 "3.1 Overview of LLAMOS ‣ 3 Methods ‣ Large
    Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent"). Subsequently,
    we further augment the defense agent using in-context learning as discussed in
    [Section 3.2](#S3.SS2 "3.2 Enhencing the Defense Agent with In-Context Learning
    ‣ 3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). Finally, in [Section 3.3](#S3.SS3 "3.3 Adversarial System with
    Multiple LLMs ‣ 3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"), we present the design of the adversarial system, incorporating
    the defense agent, attack agent, and target LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Overview of LLAMOS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To defend against adversarial textual attacks targeting LLM-based classification
    tasks, we propose Large LAnguage MOdel Sentinel (LLAMOS) that employs the LLM
    as a defense agent for adversarial purification. LLAMOS comprises two components:
    Agent instruction and Defense guidance. Next, we introduce the overall pipeline
    in sequential order.'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we utilize the existing LLMs denoted by target LLMs $f_{t}$ as
    described in the following.
  prefs: []
  type: TYPE_NORMAL
- en: Defense Agent Instruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, let me provide a brief overview of the input text: [Input Description].
    The classification task for these sentences is [Task Description]. However, be
    aware that these sentences might be susceptible to adversarial attacks, which
    could lead to an incorrect label. Note that not all sentences will be affected
    by the attacks. Your task is to generate a new sentence that replaces the original
    one, which must satisfy the following conditions: [Defense Goal].'
  prefs: []
  type: TYPE_NORMAL
- en: Defense Guidance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can complete the task using the following guidance: [Defense Guidance].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: [Input]. Now, let’s start the defense process and only output the generated
    sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input Description. The format of Input $x_{in}\in\{D_{i},i=1...6\}$ to correspond
    with the specific structure and content of each dataset, details in [Table 10](#A1.T10
    "In Appendix A GLUE Dataset ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"). For instance, the SST-2 dataset (Socher et al., [2013](#bib.bib47))
    typically consists of a single sentence per data point. On the other hand, the
    MNLI dataset (Williams et al., [2018](#bib.bib62)) is structured to include pairs
    of sentences labeled as premise and hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Description. Similar to the input descriptions, the tasks associated with
    each dataset $D_{i}$ are distinct. As illustrated earlier, SST-2 focuses on determining
    the sentiment of a given sentence, making it a straightforward classification
    challenge. Conversely, MNLI presents a more complex task of natural language inference,
    where the relationship between a pair of sentences must be discerned and classified
    correctly. Detailed input and task descriptions are provided in [Table 9](#A1.T9
    "In Appendix A GLUE Dataset ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Defense Goal. Based on traditional adversarial purification methods (Shi et al.,
    [2021](#bib.bib45); Srinivasan et al., [2021](#bib.bib48); Nie et al., [2022](#bib.bib37);
    Lin et al., [2024b](#bib.bib27)), we designed the defense goal for LLAMOS as follows:
    “1\. Keeping the semantic meaning of the new sentence the same as the original
    one; 2\. For natural examples, the new sentence should remain unchanged. For adversarial
    examples, modify the sentence so that it is classified as the correct label, effectively
    reversing the adversarial effect.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Defense Guidance. The defense guidance offers specific instructions to the
    defense agent on how to modify the input text to ensure effective defense and
    accurate outputs from the target LLM. In designing our guidance, we considered
    attacks at various levels (Xu et al., [2024](#bib.bib64)), including character,
    word, and sentence levels, which are presented in [Table 1](#S3.T1 "In 3.1 Overview
    of LLAMOS ‣ 3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). These guidances are not rigidly fixed; they can be fine-tuned
    according to specific tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3596df9411ca53c281144e82c06ae68c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Adversarial system with multiple LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The defense guidances for defense agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Idx. | [Defense Guidance] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Modify as few characters as possible. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Correct any clear spelling errors. |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Eliminate redundant symbols. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | If necessary, feel free to replace, delete, add words, or adjust the
    word order. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Improve structure for better readability. |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Ensure sentence is coherent and logical. |'
  prefs: []
  type: TYPE_TB
- en: After the defense agent generates the new sentence, the purified example $\hat{x}=g_{stl}(x^{\prime})$
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Enhencing the Defense Agent with In-Context Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the initial defense agent, the defense guidance relies on common sense, which
    may result in poor performance against some special attacks, even when the attacker
    adds obvious characters. To address this limitation, we introduce in-context learning
    (Dong et al., [2022](#bib.bib13)) to further optimize the defense agent. The prompts
    of in-context learning are described in the following.
  prefs: []
  type: TYPE_NORMAL
- en: In-Context Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The new sentence still contains a lot of harmful content caused by adversarial
    attacks, such as [Specific Guidance]. Please consider these contents and output
    a new sentence for me.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: [Input]. Now, let’s start the defense process and only output the generated
    sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: The specific guidance is designed to assist the defense agent in better understanding
    an attack and generating a new sentence capable of effectively defending against
    the attack. These guidelines can be fine-tuned to address specific attacks and
    can be incorporated into the defense agent as needed. Through in-context learning,
    the defense agent can be continuously optimized, significantly enhancing its performance
    almost without adding any additional costs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Adversarial System with Multiple LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we devise an adversarial system involving multiple LLMs. Given
    that our method introduces a defense agent against attackers, a natural idea is
    to then create an attack agent to counter the defender. The attack agent is tasked
    with generating adversarial examples from purified examples to deceive the target
    LLM once more. To accomplish this, we design prompts for generating an attack
    agent $g_{atk}$, as described in the following.
  prefs: []
  type: TYPE_NORMAL
- en: Attack Agent Instruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, let me provide a brief overview of the input text: [Input Description].
    The classification task for these sentences is [Task Description]. Your task is
    to generate a new sentence that replaces the original one, which must satisfy
    the following conditions: [Attack Instruction].'
  prefs: []
  type: TYPE_NORMAL
- en: Attack Guidance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For example, the original sentence [Purified Example] is classified as [Correct
    Label]. You should generate a new sentence which is classified as [Incorrect Label].
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: [Input]. Now, let’s start the attack process and only output the generated
    sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt structure of the attack agent and the defense agent is basically
    the same, although there are some differences in details. The input description
    of the attack agent includes the correct label $y$. The attack instruction is
    “1\. The new sentence should be classified as the opposite of the ‘correct label’.
    2\. Change at most two letters in the sentence.” Finally, we provide a specific
    example to help the attack agent better understand the attack task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we combine the defense agent and attack agent to form an adversarial
    system, as illustrated in [Figure 2](#S3.F2 "In 3.1 Overview of LLAMOS ‣ 3 Methods
    ‣ Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").
    In the adversarial system, the purified examples can be attacked again by the
    attack agent, and likewise, the adversarial examples can also be purified by the
    defense agent. They continuously counter each other, much like adversarial training
    (Goodfellow et al., [2015](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adversarial Attack. Deep neural networks (DNNs) are vulnerable to adversarial
    examples (Szegedy et al., [2014](#bib.bib50)), which are generated by adding small,
    human-imperceptible perturbations to natural examples, but completely change the
    prediction results to DNNs (Goodfellow et al., [2015](#bib.bib17); Lin et al.,
    [2024b](#bib.bib27)). With the rapidly increasing applications of LLMs (OpenAI,
    [2023](#bib.bib39); Köpf et al., [2024](#bib.bib21); Romera-Paredes et al., [2024](#bib.bib43)),
    security concerns have emerged as a critical area of research (Gehman et al.,
    [2020](#bib.bib14); Bender et al., [2021](#bib.bib3); Mckenna et al., [2023](#bib.bib33);
    Manakul et al., [2023](#bib.bib32); Liu et al., [2023b](#bib.bib29); Zhu et al.,
    [2023](#bib.bib69); Li et al., [2023](#bib.bib23); Qi et al., [2024](#bib.bib41);
    Yao et al., [2024b](#bib.bib68)), with researchers increasingly focusing on adversarial
    attacks targeting LLMs. In a similar setup to DNNs, for LLMs, attackers manipulate
    a small amount of text to change the output of the target LLM while maintaining
    the semantic information for humans (Wang et al., [2024](#bib.bib58); Xu et al.,
    [2024](#bib.bib64)). Presently, addressing the security issues surrounding LLMs
    is of paramount importance and requires urgent attention.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Defense. There are two main defense techniques on traditional DNNs,
    including adversarial training (AT) (Goodfellow et al., [2015](#bib.bib17)) and
    adversarial purification (AP) (Shi et al., [2021](#bib.bib45); Srinivasan et al.,
    [2021](#bib.bib48)). Unlike traditional DNNs, retraining LLMs is nearly impossible
    due to cost issues (Li et al., [2023](#bib.bib23)). Therefore, most methods enhance
    the robustness of LLMs through adversarial fine-tuning (AFT) (Xiang et al., [2024](#bib.bib63);
    Li et al., [2024b](#bib.bib25); Bianchi et al., [2023](#bib.bib5); Deng et al.,
    [2024](#bib.bib11); Qi et al., [2024](#bib.bib41)). While AFT can effectively
    defend against attacks, it remains susceptible to unseen attacks whose adversarial
    examples that the LLMs have not previously learned (Li et al., [2023](#bib.bib23)).
    Additionally, even with fine-tuning, training the LLMs will still consume a significant
    cost (Hu et al., [2022](#bib.bib19); Dettmers et al., [2024](#bib.bib12)). Adversarial
    purification (AP) aims to purify adversarial examples before feeding them into
    the target model, which has emerged as a promising defense method (Shi et al.,
    [2021](#bib.bib45); Srinivasan et al., [2021](#bib.bib48); Lin et al., [2024b](#bib.bib27)).
    Compared with the AT or AFT method, the AP method utilizes an additional model
    that can defend against unseen attacks without retraining the target model (Lin
    et al., [2024b](#bib.bib27); Li et al., [2023](#bib.bib23)). In some traditional
    computer vision and natural language processing tasks, researchers have started
    using LLMs as purifiers for adversarial purification (Singh and Subramanyam, [2024](#bib.bib46);
    Li et al., [2024a](#bib.bib24); Moraffah et al., [2024](#bib.bib35)), but the
    security issues of LLMs themselves have not been deeply considered. Therefore,
    we propose a novel LLM defense technique named LLAMOS to purify the adversarial
    textual examples before feeding them into the target LLM, aiming to improve the
    robustness of the entire system.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Model Agent. The LLM agent is a new research direction that has
    emerged in recent years (Ha et al., [2023](#bib.bib18); Mu et al., [2024](#bib.bib36);
    M. Bran et al., [2024](#bib.bib31)). This novel type of agent is capable of interacting
    with humans in natural language, leading to a significant increase in applications
    across fields such as chatbots, natural sciences, robotics, and workflows (Boiko
    et al., [2023](#bib.bib6); Yang et al., [2023](#bib.bib65); Lin et al., [2024a](#bib.bib26);
    Wang et al., [2023b](#bib.bib59); Liu et al., [2023a](#bib.bib28)). Furthermore,
    LLMs have demonstrated promising zero-shot/few-shot planning and reasoning capabilities
    across various configurations (Sumers et al., [2023](#bib.bib49)), covering specific
    environments and reasoning tasks (Yao et al., [2023](#bib.bib66); Gong et al.,
    [2023](#bib.bib16); Yao et al., [2024a](#bib.bib67)). In this paper, we introduce
    a new variant of the LLM agent designed specifically to purify adversarial textual
    examples generated by attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct extensive experiments on GLUE datasets to evaluate
    the effectiveness of the proposed method (LLAMOS). Specifically, our method significantly
    reduces the attack success rate (ASR) by up to 37.86% with GPT-3.5 and 45.59%
    with LLAMA-2, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets. The experiments are conducted on six tasks in GLUE datasets (Wang
    et al., [2018](#bib.bib55)), including SST-2, RTE, QQP, QNLI, MNLI-mm, MNLI-m
    (Socher et al., [2013](#bib.bib47); Dagan et al., [2005](#bib.bib10); [Bar-Haim
    et al.,](#bib.bib2) ; Giampiccolo et al., [2007](#bib.bib15); Bos and Markert,
    [2005](#bib.bib7); Bentivogli et al., [2009](#bib.bib4); Wang et al., [2017](#bib.bib61);
    Rajpurkar et al., [2016](#bib.bib42); Williams et al., [2018](#bib.bib62)). The
    detailed descriptions are provided in [Appendix A](#A1 "Appendix A GLUE Dataset
    ‣ Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial Attacks. We evaluate our method against PromptAttack (Xu et al.,
    [2024](#bib.bib64)), which is a powerful attack that combines nine different types
    of attacks, as illustrated in [Table 11](#A2.T11 "In Appendix B PromptAttack ‣
    Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").
    Furthermore, Xu et al. ([2024](#bib.bib64)) introduce the few-shot (FS) strategy
    (Logan IV et al., [2021](#bib.bib30)) and ensemble (EN) strategy (Croce and Hein,
    [2020](#bib.bib9)) to boost the attack power of PromptAttack, details in [Appendix B](#A2
    "Appendix B PromptAttack ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent")'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics. We evaluate the performance of defense methods using two
    metrics: attack success rate (ASR) and robust accuracy (RA). These metrics are
    derived from testing on adversarial examples, where a lower ASR or a higher RA
    indicates greater model robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Details. The experiments in this paper are conducted using GPT-3.5
    (OpenAI, [2023](#bib.bib39)) with ‘GPT-3.5-Turbo-0613’ version and LLAMA-2 (Touvron
    et al., [2023b](#bib.bib54)) with ‘LLAMA-2-7b’ version. For GPT-3.5, we purchase
    OpenAI’s API service¹¹1https://openai.com/api/ and conduct testing experiments
    with the ‘openai’ package in Python. For LLAMA-2, we deploy it locally on NVIDIA
    RTX A6000 and utilize the available checkpoint published by MetaAI from HuggingFace²²2https://huggingface.co/meta-llama/.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: The attack success rate (ASR) defense against PromptAttack-EN and
    PromptAttack-FS-EN on the GLUE dataset with GPT-3.5\. The lower the ASR, the greater
    the model robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | LLAMOS | SST-2 | RTE | QQP | QNLI | MNLI-mm | MNLI-m | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PA-EN | $\times$ | 56.00 | 34.30 | 37.03 | 40.39 | 43.51 | 44.00 | 42.25
    |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 23.77 | 8.91 | 16.11 | 11.69 | 5.65 | 17.66 | 13.22 |'
  prefs: []
  type: TYPE_TB
- en: '| PA-FS-EN | $\times$ | 75.23 | 36.12 | 39.61 | 49.00 | 44.10 | 45.97 | 48.81
    |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 48.94 | 9.58 | 16.49 | 14.33 | 7.73 | 19.05 | 19.42 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The attack success rate (ASR) defense against PA-EN and PA-FS-EN on
    the SST-2 dataset with LLAMA-2.'
  prefs: []
  type: TYPE_NORMAL
- en: '|      Defenses | PA-EN | PA-FS-EN |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|      Vanilla | 66.77 | 48.39 |'
  prefs: []
  type: TYPE_TB
- en: '|      LLAMOS | 21.18 | 37.82 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Standard accuracy and average robust accuracy on LLAMA-2 defense against
    three types of PromptAttack: character (C), word (W), and sentence (S) attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| FS | Standard | Robust | C | W | S |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\times$ | 92.18 | 47.93 | 83.59 | 85.03 | 84.62 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 92.18 | 30.42 | 85.16 | 79.13 | 68.96 |'
  prefs: []
  type: TYPE_TB
- en: 'Evaluation of LLAMOS Performance on Attack Success Rate (ASR). We evaluate
    the ASR of the LLAMOS against PromptAttack-EN and PromptAttack-FS-EN on the GLUE
    datasets with GPT-3.5 (OpenAI, [2023](#bib.bib39)). As shown in [Table 2](#S5.T2
    "In 5.2 Results ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"), our method significantly reduces the ASR of both PromptAttack-EN
    and PromptAttack-FS-EN across all tasks. Specifically, our method achieves an
    average ASR reduction of 29.33% and 29.39%, respectively. These results demonstrate
    that LLAMOS is effective in defending against adversarial textual attacks. Additionally,
    we also evaluate the performance of LLAMOS on the SST-2 dataset with LLAMA-2 (Touvron
    et al., [2023b](#bib.bib54)), as shown in [Table 3](#S5.T3 "In 5.2 Results ‣ 5
    Experiments ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). The results are similar to the previous experiments. The ASR of
    PromptAttack-EN and PromptAttack-FS-EN is significantly reduced by 45.59% and
    10.57%, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation of LLAMOS Performance on Robust Accuracy (RA). We evaluate the RA
    of LLAMOS on the SST-2 dataset with LLAMA-2 against three types of PromptAttack:
    character, word, and sentence attacks. In [Table 4](#S5.T4 "In 5.2 Results ‣ 5
    Experiments ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"), the accuracies in the first two columns represent the standard
    accuracy and robust accuracy without defense, while the last three columns represent
    the robust accuracy with LLAMOS. Under strong attacks, the classification accuracy
    of the target LLM decreased from 92.18% to 30.42%. LLAMOS can effectively defend
    against adversarial textual attacks, significantly improving the robust accuracy.
    Specifically, the lowest robust accuracy reaches 86.96%. Additionally, we conduct
    more comprehensive experiments across nine types of attacks and six tasks with
    GPT-3.5, as shown in [Table 5](#S5.T5 "In 5.2 Results ‣ 5 Experiments ‣ Large
    Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent"). LLAMOS
    can effectively defend against character-level attacks, achieving results on C1
    and C3 that closely match the standard accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Standard accuracy and robust accuracy defense against PromptAttack
    with GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | FS | SST-2 | RTE | QQP | QNLI | MNLI-mm | MNLI-m | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Standard | - | 97.66 | 80.47 | 75.78 | 66.41 | 66.41 | 71.87 | 76.43 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Robust | $\times$ | 42.97 | 52.93 | 47.66 | 39.65 | 37.50 | 40.23 | 43.49
    |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 24.22 | 51.37 | 45.70 | 33.79 | 37.11 | 38.87 | 38.51 |'
  prefs: []
  type: TYPE_TB
- en: '| C1 | $\times$ | 96.09 | 81.25 | 72.66 | 63.28 | 69.53 | 68.75 | 75.26 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 96.88 | 81.25 | 66.41 | 64.84 | 68.75 | 66.41 | 74.09 |'
  prefs: []
  type: TYPE_TB
- en: '| C2 | $\times$ | 75.78 | 74.22 | 67.19 | 64.84 | 68.75 | 61.72 | 68.75 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 83.59 | 77.34 | 63.28 | 60.16 | 67.97 | 58.59 | 68.49 |'
  prefs: []
  type: TYPE_TB
- en: '| C3 | $\times$ | 89.06 | 82.81 | 71.48 | 62.50 | 74.22 | 63.28 | 73.89 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 66.41 | 81.25 | 73.44 | 64.84 | 70.31 | 69.53 | 70.96 |'
  prefs: []
  type: TYPE_TB
- en: '| W1 | $\times$ | 85.01 | 79.66 | 70.50 | 61.25 | 68.67 | 65.79 | 71.81 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 80.18 | 77.93 | 69.92 | 59.19 | 64.63 | 63.40 | 69.21 |'
  prefs: []
  type: TYPE_TB
- en: '| W2 | $\times$ | 81.37 | 77.81 | 67.86 | 63.42 | 67.83 | 64.87 | 70.53 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 80.04 | 75.77 | 66.15 | 61.18 | 64.28 | 63.31 | 68.46 |'
  prefs: []
  type: TYPE_TB
- en: '| W3 | $\times$ | 75.79 | 75.12 | 69.57 | 63.76 | 64.85 | 60.91 | 68.33 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 64.48 | 75.00 | 68.17 | 61.69 | 63.00 | 60.51 | 65.47 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | $\times$ | 74.45 | 75.48 | 63.86 | 58.65 | 62.66 | 59.18 | 65.71 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 71.18 | 72.76 | 64.01 | 56.89 | 61.79 | 58.18 | 64.14 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | $\times$ | 85.83 | 74.83 | 64.68 | 59.90 | 65.33 | 62.70 | 68.88 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 58.77 | 76.53 | 64.52 | 59.01 | 63.39 | 61.08 | 63.88 |'
  prefs: []
  type: TYPE_TB
- en: '| S3 | $\times$ | 79.31 | 73.30 | 63.57 | 59.96 | 65.10 | 61.12 | 67.06 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | 49.86 | 75.06 | 64.59 | 58.31 | 63.25 | 62.61 | 62.28 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Robust accuracy against C3 attack with in-context learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '|       ICL | C3 | C3-FS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|       $\times$ | 89.06 | 66.41 |'
  prefs: []
  type: TYPE_TB
- en: '|       ✓ | 97.66 | 92.19 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Robust accuracy defense against the attack agent under different iterations
    with GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Iterations | Iter. 1 | Iter. 2 | Iter. 3 | Iter. 4 | Iter. 5 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Defense | 96.09 | 84.38 | 83.59 | 91.41 | 90.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Attack | 56.25 | 43.53 | 45.93 | 50.34 | 28.13 |'
  prefs: []
  type: TYPE_TB
- en: 'Evaluation of LLAMOS Performance with In-Context Learning (ICL). The C3-based
    attack (Xu et al., [2024](#bib.bib64)) is a very obvious attack that adds up to
    two extraneous characters to the end of the sentence, as shown in [Table 8](#S5.T8
    "In 5.3 Discussion ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing
    Adversarial Robustness by LLM Agent"). However, our method only achieves robust
    accuracies of 89.06% for the C3 attack and 66.41% for the C3-FS attack, respectively.
    To further improve the robustness, we introduce ICL to enhance the performance
    of the defense agent. As shown in [Table 6](#S5.T6 "In 5.2 Results ‣ 5 Experiments
    ‣ Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent"),
    the defense agent with ICL significantly improves the robust accuracy against
    the C3 attack, achieving a robust accuracies of 97.66% for the C3 attack and 92.19%
    for the C3-FS attack, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis of Adversarial System. We conduct experiments with an adversarial
    system and evaluate the robust accuracy defense against adversarial examples generated
    by the attack agent over multiple iterations. As shown in [Table 7](#S5.T7 "In
    5.2 Results ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"), the defense agent initially achieves a robust accuracy
    of 96.09% in the first round of confrontation. However, after the purified examples
    are re-attacked by the attack agent, the robust accuracy decreases to 56.25%.
    The defense agent then purifies these adversarial examples again, leading to an
    increase in robust accuracy, but it will decrease once more by subsequent attacks.
    This continual fluctuation in robust accuracy is a common phenomenon in adversarial
    training (Goodfellow et al., [2015](#bib.bib17)). Upon reviewing the generated
    texts, we observe that after several rounds of confrontation, both the defense
    agent and attack agent may generate the same sentences as previous ones, resulting
    in a potential infinite loop, as shown in [Table 8](#S5.T8 "In 5.3 Discussion
    ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). This is an interesting phenomenon that requires further investigation,
    particularly strategies to disrupt such loops.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Advantages of LLAMOS. As emphasized by the experimental results presented
    in [Section 5.2](#S5.SS2 "5.2 Results ‣ 5 Experiments ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent") and [Table 12](#A3.T12 "In Appendix
    C More Comparasion Results of Input Examples ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent"), LLAMOS significantly enhances
    performance across various tasks and attacks with LLAMA-2 and GPT-3.5\. Additionally,
    the defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing
    step. Through in-context learning (ICL), the defense agent can be continuously
    optimized to defend against emerging attacks. This invisibly resolves a major
    challenge in adversarial robustness: Due to the significant differences between
    different attacks, the model trained on specific attacks often fails to generalize
    to other unseen attacks (Poursaeed et al., [2021](#bib.bib40); Laidlaw et al.,
    [2021](#bib.bib22); Tack et al., [2022](#bib.bib51)). The model necessitates to
    be continuously fine-tuning to adapt to emerging attacks. However, fine-tuning
    parameters requires substantial costs (Hu et al., [2022](#bib.bib19); Dettmers
    et al., [2024](#bib.bib12)), and the emergence of new attack techniques continually
    makes it impractical to train the model to defend against emerging attacks(Laidlaw
    et al., [2021](#bib.bib22); Lin et al., [2024b](#bib.bib27)). In contrast, our
    method can effectively enhance the robustness through ICL without adjusting the
    parameters of the LLMs, which is undoubtedly a significant advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison between clean examples, adversarial examples, and purified
    examples. When incorrectly classified, the modified area is highlighted with a
    yellow background.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | [Input] | Prediction |'
  prefs: []
  type: TYPE_TB
- en: '| Target LLM | Clean (Original): a fast , funny , highly enjoyable movie .
    | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| C3 Attack | Adversarial: a fast , funny , highly enjoyable movie . :( | Negative
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMOS | Purified: a fast , funny , highly enjoyable movie . :( | Negative
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMOS+ICL | Purified: a fast , funny , highly enjoyable movie .. | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '| Iteration 1 | Purified: You don’t have to know music to appreciate the film’s
    easygoing blend of tragedy and romance. | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial: Youdo n’thave to know music to appreciate … | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| Iteration 2 | Purified: You don’t have to know music to appreciate … | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial: You don’t have to know music todepreciate… | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| Iteration 3 | Purified: You don’t have to know music to appreciate … | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial: You don’t have to know music todepreciate… | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| … | … | … |'
  prefs: []
  type: TYPE_TB
- en: 'The Challenges in LLM-based Defense. The defense agent is tasked with purifying
    adversarial examples, but it is difficult to distinguish between natural examples
    and adversarial examples in some cases. As shown in [Table 12](#A3.T12 "In Appendix
    C More Comparasion Results of Input Examples ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent").4, the attacker altered the original
    meaning by inserting ‘not’, rendering the adversarial example indistinguishable
    from a natural example, resulting in the defense agent failing to generate the
    correct sentence. Although we hope that the defense agent can observe the sentences
    like humans, it presents a huge challenge at present. Unlike the attacker or humans,
    the defense agent lacks access to the original label of the input sentence. Furthermore,
    although the defense agent can effectively defend against adversarial attacks,
    it cannot prevent subsequent attacks, as illustrated in [Table 7](#S5.T7 "In 5.2
    Results ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"). For instance, the malicious LLMs can embed specific
    system prompts to influence the output, which is unbeknownst to users; they can
    add ‘:)’ to each input sentence for prediction rather than predicting the original
    sentence. In this case, the defense agent also fails to defend. This issue is
    also an important problem in traditional adversarial training (Goodfellow et al.,
    [2015](#bib.bib17)), and no method has completely resolved this issue. Nonetheless,
    as previously discussed, LLMs offer advantages not available to traditional DNNs,
    and we have naturally solved one challenge in adversarial robustness, which is
    that the model can adapt to new attacks. Hence, future advancements may resolve
    adversarial issues of attack and defense within LLM frameworks, representing a
    challenging but promising research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations. It is well-known that training large language models (LLMs) requires
    significant resources and generates substantial carbon emissions, thereby burdening
    the planet. However, the long-term inference costs of LLMs far exceed the training
    costs. Chien et al. ([2023](#bib.bib8)) show that for ChatGPT-like services, inference
    dominates emissions due to its large user base, in one year producing 25 times
    the carbon emissions of training GPT-3. Our method introduces a defense agent
    for additional inference, which inevitably increases carbon emissions during the
    inference process, thereby exacerbating the negative impact on the climate, which
    is a limitation of our work. However, considering the nascent stage of LLM development,
    the trustworthiness issue is equally crucial. Therefore, we have dedicated significant
    effort to this area, but of course, we aspire to find future solutions that adequately
    address the environmental and climate challenges posed by AI.
  prefs: []
  type: TYPE_NORMAL
- en: Impact Statements. This paper presents research aimed at enhancing the robustness
    of large language models (LLMs). With the rapidly increasing applications of LLMs,
    their security and trustworthiness have become a critical concern. Our work focuses
    on this significant issue and contributes positively to potential societal impacts.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose LLAMOS, a novel LLM-based defense technique designed
    to purify adversarial examples before feeding them into the target LLM. The defense
    agent within LLAMOS operates as a plug-and-play module that functions effectively
    as a pre-processing step without requiring retraining of the target LLM. We conduct
    extensive experiments across various tasks and attacks with LLAMA-2 and GPT-3.5\.
    The results demonstrate that LLAMOS can effectively defend against adversarial
    attacks. Furthermore, we discuss certain existing shortcomings and challenges,
    which we aim to address in future research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo
    Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major,
    and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language
    models be too big? In *Proceedings of the 2021 ACM conference on fairness, accountability,
    and transparency*, pages 610–623, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bentivogli et al. (2009) Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
    Giampiccolo. The fifth pascal recognizing textual entailment challenge. *TAC*,
    7(8):1, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bianchi et al. (2023) Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul
    Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas:
    Lessons from improving the safety of large language models that follow instructions.
    *arXiv preprint arXiv:2309.07875*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boiko et al. (2023) Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent
    autonomous scientific research capabilities of large language models. *arXiv preprint
    arXiv:2304.05332*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bos and Markert (2005) Johan Bos and Katja Markert. Recognising textual entailment
    with logical inference. In *Proceedings of Human Language Technology Conference
    and Conference on Empirical Methods in Natural Language Processing*, pages 628–635,
    2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chien et al. (2023) Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan
    Sharma, and Rajini Wijayawardana. Reducing the carbon impact of generative ai
    inference (today and in 2035). In *Proceedings of the 2nd Workshop on Sustainable
    Computer Systems*, pages 1–7, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce and Hein (2020) Francesco Croce and Matthias Hein. Reliable evaluation
    of adversarial robustness with an ensemble of diverse parameter-free attacks.
    In *International conference on machine learning*, pages 2206–2216\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dagan et al. (2005) Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal
    recognising textual entailment challenge. In *Machine learning challenges workshop*,
    pages 177–190\. Springer, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2024) Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing.
    Multilingual jailbreak challenges in large language models. In *The Twelfth International
    Conference on Learning Representations*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning.
    *arXiv preprint arXiv:2301.00234*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi,
    and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in
    language models. *Findings of the Association for Computational Linguistics: EMNLP
    2020*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giampiccolo et al. (2007) Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
    William B Dolan. The third pascal recognizing textual entailment challenge. In
    *Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing*,
    pages 1–9, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. (2023) Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante,
    Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al.
    Mindagent: Emergent gaming interaction. *arXiv preprint arXiv:2309.09971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2015) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    Explaining and harnessing adversarial examples. *International Conference on Learning
    Representations*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ha et al. (2023) Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling
    down: Language-guided robot skill acquisition. In *Conference on Robot Learning*,
    pages 3766–3777\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. In *International Conference on Learning Representations*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasneci et al. (2023) Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria
    Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,
    Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of
    large language models for education. *Learning and individual differences*, 103:102274,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Köpf et al. (2024) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver
    Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large
    language model alignment. *Advances in Neural Information Processing Systems*,
    36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laidlaw et al. (2021) C Laidlaw, S Singla, and S Feizi. Perceptual adversarial
    robustness: Defense against unseen threat models. In *International Conference
    on Learning Representations (ICLR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Linyang Li, Demin Song, and Xipeng Qiu. Text adversarial purification
    as defense against adversarial attacks. In *Proceedings of the 61st Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    338–350, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024a) Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang
    Liu, and Min Lin. Purifying large language models by ensembling a small language
    model. *arXiv preprint arXiv:2402.14845*, 2024a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024b) Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing
    Liu, Wenhan Wang, Tianwei Zhang, and Yang Liu. Badedit: Backdooring large language
    models by model editing. In *The Twelfth International Conference on Learning
    Representations*, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2024a) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman,
    Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang
    Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive
    tasks. *Advances in Neural Information Processing Systems*, 36, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2024b) Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, and
    Qibin Zhao. Adversarial training on purification (ATop): Advancing both robustness
    and generalization. In *The Twelfth International Conference on Learning Representations*,
    2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating
    llms as agents. *arXiv preprint arXiv:2308.03688*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt
    engineering: An empirical study. *arXiv preprint arXiv:2305.13860*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logan IV et al. (2021) Robert L Logan IV, Ivana Balažević, Eric Wallace, Fabio
    Petroni, Sameer Singh, and Sebastian Riedel. Cutting down on prompts and parameters:
    Simple few-shot learning with language models. *arXiv preprint arXiv:2106.13353*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M. Bran et al. (2024) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew D White, and Philippe Schwaller. Augmenting large language models with
    chemistry tools. *Nature Machine Intelligence*, pages 1–11, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark Gales. Selfcheckgpt:
    Zero-resource black-box hallucination detection for generative large language
    models. In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mckenna et al. (2023) Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Hosseini,
    Mark Johnson, and Mark Steedman. Sources of hallucination by large language models
    on inference tasks. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 2758–2774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minaee et al. (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey.
    *arXiv preprint arXiv:2402.06196*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moraffah et al. (2024) Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee,
    and Huan Liu. Adversarial text purification: A large language model approach for
    defense. In *Pacific-Asia Conference on Knowledge Discovery and Data Mining*,
    pages 65–77\. Springer, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mu et al. (2024) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,
    Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language
    pre-training via embodied chain of thought. *Advances in Neural Information Processing
    Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2022) Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat,
    and Animashree Anandkumar. Diffusion models for adversarial purification. In *International
    Conference on Machine Learning*, pages 16805–16827\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. Introducing chatgpt. 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. GPT-4V(ision) system card. 2023. URL [https://cdn.openai.com/papers/GPTV_System_Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poursaeed et al. (2021) Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie,
    and Ser-Nam Lim. Robustness and generalization via generative adversarial training.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 15711–15720, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2024) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety,
    even when users do not intend to! In *The Twelfth International Conference on
    Learning Representations*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. Squad: 100,000+ questions for machine comprehension of text. *arXiv
    preprint arXiv:1606.05250*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Romera-Paredes et al. (2024) Bernardino Romera-Paredes, Mohammadamin Barekatain,
    Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz,
    Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries
    from program search with large language models. *Nature*, 625(7995):468–475, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong
    Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment:
    A survey. *arXiv preprint arXiv:2309.15025*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2021) Changhao Shi, Chester Holtz, and Gal Mishne. Online adversarial
    purification based on self-supervision. *International Conference on Learning
    Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Subramanyam (2024) Himanshu Singh and AV Subramanyam. Language guided
    adversarial purification. In *ICASSP 2024-2024 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP)*, pages 7685–7689\. IEEE, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 conference on empirical methods in natural language processing*, pages 1631–1642,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivasan et al. (2021) Vignesh Srinivasan, Csaba Rohrer, Arturo Marban, Klaus-Robert
    Müller, Wojciech Samek, and Shinichi Nakajima. Robustifying models against adversarial
    attacks by langevin dynamics. *Neural Networks*, 137:1–17, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sumers et al. (2023) Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and
    Thomas L Griffiths. Cognitive architectures for language agents. *arXiv preprint
    arXiv:2309.02427*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of
    neural networks. *International Conference on Learning Representations*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tack et al. (2022) Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju
    Hwang, and Jinwoo Shin. Consistency regularization for adversarial robustness.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 8414–8422,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large
    language models in medicine. *Nature medicine*, 29(8):1930–1940, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng,
    Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task
    benchmark for robustness evaluation of language models. In *Thirty-fifth Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track (Round
    2)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong
    Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al.
    Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In
    *Thirty-seventh Conference on Neural Information Processing Systems Datasets and
    Benchmarks Track*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong
    Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al.
    Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s
    game of thoughts: Battle against deception through recursive contemplation. *arXiv
    preprint arXiv:2310.01320*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language
    models with human: A survey. *arXiv preprint arXiv:2307.12966*, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective
    matching for natural language sentences. *arXiv preprint arXiv:1702.03814*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R Bowman. The
    multi-genre nli corpus. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang et al. (2024) Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2024) Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng
    Zhang, and Mohan Kankanhalli. An llm can fool itself: A prompt-based adversarial
    attack. In *The Twelfth International Conference on Learning Representations*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action. *arXiv preprint arXiv:2303.11381*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in
    language models. In *The Eleventh International Conference on Learning Representations*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2024a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2024b) Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun,
    and Yue Zhang. A survey on large language model (llm) security and privacy: The
    good, the bad, and the ugly. *High-Confidence Computing*, page 100211, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al.
    Promptbench: Towards evaluating the robustness of large language models on adversarial
    prompts. *arXiv preprint arXiv:2306.04528*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A GLUE Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SST-2. The Stanford Sentiment Treebank (SST-2) (Socher et al., [2013](#bib.bib47))
    is a single-sentence classification task, includes sentences from movie reviews
    and their sentiment annotations by humans. This task involves determining the
    sentiment of a given sentence, categorized into two types: positive and negative.'
  prefs: []
  type: TYPE_NORMAL
- en: RTE. The Recognizing Textual Entailment (RTE) (Dagan et al., [2005](#bib.bib10);
    [Bar-Haim et al.,](#bib.bib2) ; Giampiccolo et al., [2007](#bib.bib15); Bos and
    Markert, [2005](#bib.bib7); Bentivogli et al., [2009](#bib.bib4)) is a binary
    classification task, where the goal is to determine whether a given sentence entails
    another sentence.
  prefs: []
  type: TYPE_NORMAL
- en: QQP. The Quora Question Pairs (QQP) (Wang et al., [2017](#bib.bib61)) is a collection
    of question pairs from the community question-answering website Quora. The task
    is to determine whether a pair of questions are semantically equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: QNLI. The Question-answering NLI (QNLI) (Rajpurkar et al., [2016](#bib.bib42))
    is a natural language inference task, converted from another dataset, The Stanford
    Question Answering Dataset. The task is to determine whether a question and a
    sentence are entailed or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'MNLI. The Multi-Genre Natural Language Inference (MNLI) Corpus (Williams et al.,
    [2018](#bib.bib62)) is a collection of sentence pairs with textual entailment
    annotations. Given a premise sentence and a hypothesis sentence, the task is to
    predict whether the premise entails the hypothesis, contradicts the hypothesis,
    or neither. Because MNLI comprises texts from many different domains and styles,
    it is divided into two versions: MNLI-m, where training and test datasets share
    the same sources, and MNLI-mm, where they differ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: The task descriptions of the GLUE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | [Task Description] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | Analyze the tone of this statement and respond with either ‘positive’
    or ‘negative’. |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | Are the following two sentences entailment or not_entailment? Answer
    me with ‘entailment’ or ‘not_entailment’, just one word. |'
  prefs: []
  type: TYPE_TB
- en: '| QQP | Are the following two questions equivalent or not? Answer me with ‘equivalent’
    or ‘not_equivalent’. |'
  prefs: []
  type: TYPE_TB
- en: '| QNLI | Given the question and context provided, determine if the answer can
    be inferred by choosing ‘entailment’ or ‘not_entailment’. |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI-mm | Does the relationship between the given sentences represent entailment,
    neutral, or contradiction? Respond with ‘entailment’, ‘neutral’, or ‘contradiction’.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI-m | Does the relationship between the given sentences represent entailment,
    neutral, or contradiction? Respond with ‘entailment’, ‘neutral’, or ‘contradiction’.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: The label list and input description of the GLUE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | [Label List] | [Input Description] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SST-2 | [‘positive’, ‘negative’] | Each example contains one ‘sentence’.
    |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | [‘entailment’, ‘not_entailment’] | Each example contains ‘sentence1’
    and ‘sentence2’. |'
  prefs: []
  type: TYPE_TB
- en: '| QQP | [‘equivalent’, ‘not_equivalent’] | Each example contains ‘question1’
    and ‘question2’. |'
  prefs: []
  type: TYPE_TB
- en: '| QNLI | [‘entailment’, ‘not_entailment’] | Each example contains ‘question’
    and ‘sentence’. |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI-mm | [‘entailment’, ‘neutral’, ‘contradiction’] | Each example contains
    ‘premise’ and ‘hypothesis’. |'
  prefs: []
  type: TYPE_TB
- en: '| MNLI-m | [‘entailment’, ‘neutral’, ‘contradiction’] | Each example contains
    ‘premise’ and ‘hypothesis’. |'
  prefs: []
  type: TYPE_TB
- en: Appendix B PromptAttack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PromptAttack (Xu et al., [2024](#bib.bib64)) modifies the clean examples at
    the character level, word level or sentence level. The specific guidance as shown
    in [Table 11](#A2.T11 "In Appendix B PromptAttack ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: Perturbation instructions at the character, word, and sentence levels,
    respectively. This table is referenced from Xu et al. ([2024](#bib.bib64)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Level | Abbre. | #perturbation_instruction |'
  prefs: []
  type: TYPE_TB
- en: '| Character | C1 | Choose at most two words in the sentence, and change them
    so that they have typos. |'
  prefs: []
  type: TYPE_TB
- en: '| C2 | Change at most two letters in the sentence. |'
  prefs: []
  type: TYPE_TB
- en: '| C3 | Add at most two extraneous characters to the end of the sentence. |'
  prefs: []
  type: TYPE_TB
- en: '| Word | W1 | Replace at most two words in the sentence with synonyms. |'
  prefs: []
  type: TYPE_TB
- en: '| W2 | Choose at most two words in the sentence that do not contribute to the
    meaning of the sentence and delete them. |'
  prefs: []
  type: TYPE_TB
- en: '| W3 | Add at most two semantically neutral words to the sentence. |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence | S1 | Add a randomly generated short meaningless handle after the
    sentence, such as @fasuv3. |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Paraphrase the sentence. |'
  prefs: []
  type: TYPE_TB
- en: '| S3 | Change the syntactic structure of the sentence. |'
  prefs: []
  type: TYPE_TB
- en: Few-shot (FS) strategy. Provide examples that match the task description of
    adversarial attacks to large language models (LLMs) to help the LLMs understand
    the task. By learning these examples, LLMs can generate adversarial examples with
    higher quality and greater attack strength.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble (EN) strategy. Utilize a collection of adversarial examples with various
    levels of perturbation, select the examples that are most likely to deceive the
    LLM as output, in order to increase the attack success rate.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C More Comparasion Results of Input Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 12: Comparison between clean examples, adversarial examples and purified
    examples. When incorrectly classified, the modified area is highlighted with a
    yellow background'
  prefs: []
  type: TYPE_NORMAL
- en: '| [Input Examples] | Prediction |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|       1\. Clean Example: it ’s a charming and often affecting journey . |
    Positive |'
  prefs: []
  type: TYPE_TB
- en: '|       Adversarial Example: it ’s a charming and oftenaffecctingjourney .
    | Negative |'
  prefs: []
  type: TYPE_TB
- en: '|       Purified Example: it’s a charming and often affecting journey. | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '|       2\. Clean Example: … the movie is just a plain old monster | Negative
    |'
  prefs: []
  type: TYPE_TB
- en: '|       Adversarial Example: … the movie is just a plain old monster:) | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '|       Purified Example: … the movie is just a plain old monster . | Negative
    |'
  prefs: []
  type: TYPE_TB
- en: '|       3\. Clean Example: the year’s most intriguing explorations of alientation
    . | Positive |'
  prefs: []
  type: TYPE_TB
- en: '|       Adversarial Example: the year’s most intriguing explorations of alientation
    .:( | Negative |'
  prefs: []
  type: TYPE_TB
- en: '|       Purified Example: the year’s most intriguing explorations of alientation..
    | Positive |'
  prefs: []
  type: TYPE_TB
- en: '|       4\. Clean Example: it ’s a charming and often affecting journey . |
    Positive |'
  prefs: []
  type: TYPE_TB
- en: '|       Adversarial Example: it ’snota charming and often affecting journey
    .@fasuv3 | Negative |'
  prefs: []
  type: TYPE_TB
- en: '|       Purified Example: it ’snota charming and often affecting journey .
    | Negative |'
  prefs: []
  type: TYPE_TB
- en: '|       5\. Clean Example: corny, schmaltzy and predictable, but still manages
    to be kind of heartwarming, nonetheless. | Positive |'
  prefs: []
  type: TYPE_TB
- en: '|       Adversarial Example: corny, schmaltzy and predictable, but still manages
    to be kind of heartwarming, nonetheless.@kjdjq2. | Negative |'
  prefs: []
  type: TYPE_TB
- en: '|       Purified Example: corny, schmaltzy and predictable, but still manages
    to be kind of heartwarming, nonetheless.. | Positive |'
  prefs: []
  type: TYPE_TB
