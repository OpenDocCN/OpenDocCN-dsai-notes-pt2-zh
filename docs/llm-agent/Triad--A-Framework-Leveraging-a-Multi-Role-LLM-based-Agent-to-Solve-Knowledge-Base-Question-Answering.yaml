- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:45'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge
    Base Question Answering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14320](https://ar5iv.labs.arxiv.org/html/2402.14320)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chang Zong
  prefs: []
  type: TYPE_NORMAL
- en: Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: zongchang@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Yuchen Yan'
  prefs: []
  type: TYPE_NORMAL
- en: Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: yanyuchen@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Weiming Lu'
  prefs: []
  type: TYPE_NORMAL
- en: Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: luwm@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: \ANDEliot Huang
  prefs: []
  type: TYPE_NORMAL
- en: Leibowitz AI
  prefs: []
  type: TYPE_NORMAL
- en: EliotHuang@leibowitz.org.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Jian Shao'
  prefs: []
  type: TYPE_NORMAL
- en: Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: jshao@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Yueting Zhuang'
  prefs: []
  type: TYPE_NORMAL
- en: Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: yzhuang@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Recent progress with LLM-based agents has shown promising results across various
    tasks. However, their use in answering questions from knowledge bases remains
    largely unexplored. Implementing a KBQA system using traditional methods is challenging
    due to the shortage of task-specific training data and the complexity of creating
    task-focused model structures. In this paper, we present Triad, a unified framework
    that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is
    assigned three roles to tackle different KBQA subtasks: agent as a generalist
    for mastering various subtasks, as a decision maker for the selection of candidates,
    and as an advisor for answering questions with knowledge. Our KBQA framework is
    executed in four phases, involving the collaboration of the agent’s multiple roles.
    We evaluated the performance of our framework using three benchmark datasets,
    and the results show that our framework outperforms state-of-the-art systems on
    the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge
    Base Question Answering'
  prefs: []
  type: TYPE_NORMAL
- en: Chang Zong Zhejiang University zongchang@zju.edu.cn                        Yuchen
    Yan Zhejiang University yanyuchen@zju.edu.cn                        Weiming Lu
    Zhejiang University luwm@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Eliot Huang Leibowitz AI EliotHuang@leibowitz.org.cn                       
    Jian Shao Zhejiang University jshao@zju.edu.cn                        Yueting
    Zhuang Zhejiang University yzhuang@zju.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A question-answering system is designed to extract information by converting
    a natural language question into a structured query that can retrieve precise
    information from an existing knowledge base Omar et al. ([2023a](#bib.bib13)).
    The resolution of Knowledge Base Question Answering (KBQA) typically involves
    phases including question understanding, URI linking, and query execution. Traditional
    KBQA systems require the use of specialized models trained with domain datasets
    for question parsing and entity linking Hu et al. ([2018](#bib.bib7)); Omar et al.
    ([2023a](#bib.bib13)); Hu et al. ([2021](#bib.bib8)). Large language models (LLMs),
    however, have shown promising competencies in in-context learning using task-specific
    demonstrations Dong et al. ([2022](#bib.bib4)). LLMs have recently been employed
    as agents in the execution of complex problems. A framework that employs LLM-augmented
    agents can generate actions or coordinate multiple agents, thus improving the
    capacity to handle complex situations Liu et al. ([2023](#bib.bib12)). Despite
    the remarkable performance of LLMs in various tasks as evidenced in previous studies,
    a comprehensive qualitative and quantitative evaluation of KBQA frameworks empowered
    with an LLM-based agent remains a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent research on KBQA with LLMs has focused primarily on highlighting the
    inability of LLMs to generate complete factoid results Hu et al. ([2023b](#bib.bib6));
    Tan et al. ([2023c](#bib.bib22)) or demonstrating their potential efficacy in
    future research Omar et al. ([2023b](#bib.bib14)); Tan et al. ([2023b](#bib.bib21)).
    Other studies on LLM-based KBQA concentrates on generating answers by prompt learning
    and incorporating external knowledge bases Baek et al. ([2023](#bib.bib2)); Tan
    et al. ([2023a](#bib.bib20)). Concurrently, LLMs can be deployed to address subtasks
    within a complex task. In the Text2SQL challenge, the work Li et al. ([2023a](#bib.bib10),
    [b](#bib.bib11)) attempts to utilize an LLM with graph-aware structural inductive
    bias or as a database interface to handle minor issues. In another recent studyDong
    et al. ([2023](#bib.bib3)), five distinct roles of mathematicians, enacted by
    an LLM, are introduced to collaboratively prove a theorem. Such research has spurred
    our exploration into the following question: Whether an LLM-based agent can solve
    KBQA tasks by serving as multiple roles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this study, we introduce Triad, a unified framework that leverages an LLM-based
    agent with three roles to address KBQA tasks. Specifically, we implement the agent
    consisting of an LLM as the core, supplemented by various task-specific modules
    such as memory and executing functions. The agent is assigned three distinct roles:
    a generalist (G-Agent) adept at mastering numerous small tasks by the given examples,
    a decision maker (D-Agent) proficient at identifying options and selecting candidates,
    and an advisor (A-Agent) skilled at providing answers using internal and external
    knowledge. The cooperation of these agent roles composes a KBQA process containing
    four phases: question parsing, URI linking, query construction, and answer generation.
    We evaluate our framework on three benchmark datasets in various difficulties.
    The results show that our framework outperforms state-of-the-art systems, demonstrated
    by 11.8% and 20.7% F1 scores on the LC-QuAD and YAGO-QA benchmarks, respectively.
    The contributions of this study can be summarized as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose Triad, the first framework that leverages an LLM-based agent to solve
    KBQA tasks in all its four phases, without specialized training models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implement an LLM-based agent with various task-specific modules that can
    act as three roles, including a generalist, a decision maker, and an advisor,
    to collaboratively solve subtasks in KBQA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluate the performance of our multi-role agent framework. The results show
    a competitive ability compared to both state-of-the-art KBQA systems and pure
    LLM methods¹¹1Code and datasets used to achieve the performance will be made public
    if the paper is accepted..
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Phases of KBQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A typical KBQA system has a process that encompasses four phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Question parsing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: involves converting natural language questions into a structured format that
    incorporates references to entities and relations.
  prefs: []
  type: TYPE_NORMAL
- en: URI linking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: entails associating and replacing these entity and relation mentions with their
    corresponding URIs within a knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Query construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: involves creating executable queries in a standard format to extract answers
    from knowledge bases.
  prefs: []
  type: TYPE_NORMAL
- en: Answer generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: seeks to obtain the ultimate answers either by performing queries within knowledge
    bases or by directly querying an agent.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Roles of LLM-based Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ an LLM-based agent to solve a series of subtasks by assuming three
    distinct roles, which collaboratively execute the four phases of KBQA.
  prefs: []
  type: TYPE_NORMAL
- en: Agent as a generalist
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (G-Agent) is capable of mastering various tasks, given a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Agent as a decision-maker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (D-Agent) concentrates on the identification of options and the selection of
    candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Agent as an advisor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (A-Agent) is skilled in answering questions with the aid of both external and
    internal knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Task Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A KBQA task refers to the process of solving a set of subtasks $S$. The task
    can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle f(KBQA)=\mathop{\bigoplus}\limits_{t=1}^{T}f(S_{t})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle f(S_{t})=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where $T$ is the way to coordinate subtasks to solve the whole.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Triad Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The overall architecture of Triad is shown in Figure [1](#S3.F1 "Figure 1 ‣
    3 Triad Framework ‣ Triad: A Framework Leveraging a Multi-Role LLM-based Agent
    to Solve Knowledge Base Question Answering"). Each role of the LLM-based agent,
    along with its associated subtasks, is illustrated as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf1f84e415f603d660148dbe7785889e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Our Triad framework leverages an LLM-based agent with three different
    roles including a generalist, a decision-maker, and an advisor to cooperatively
    handle a series of subtasks in the four phases of a KBQA process.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 G-Agent as a Generalized Solver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A generalized agent (G-Agent) proficiently manages numerous tasks by leveraging
    learning from limited examples through an LLM. In our framework, a G-Agent can
    perform question parsing, query template generation, or answer type classification
    as actions solely utilizing an LLM. These three subtasks are illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Triplet mention extraction:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The process of extracting triplet mentions in question parsing involves the
    conversion of a naturally phrased question, denoted as $Q$, into formatted triplets
    of entities and relations. This subtask is executed employing an LLM, which is
    guided by a prompt with a set of prerequisites and a selection of examples. This
    subtask can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{tri})=$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Pmt_{tri}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where $Agent_{g}$ Kojima et al. ([2022](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SPARQL template generation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The generation of SPARQL templates in query construction involves the use of
    an LLM to create a SPARQL template that articulates the question using standard
    SPARQL syntax, replacing URI identifiers with entity and relation variables. To
    derive precise and comprehensive answers from the knowledge base using SPARQL
    queries, there are two potential strategies. One approach involves the direct
    generation of an executable SPARQL using an LLM, though this method may significantly
    increase LLM call times and error rates when numerous candidate queries are in
    play. Alternatively, a SPARQL template can initially be generated with entity
    and relation variables, which are subsequently replaced with linked URIs. For
    the sake of stability and efficiency, we opt for the second strategy. This subtask
    can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{qt})=$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Pmt_{qt}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where $Agent_{g}$ to generate SPARQL template.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer type classification:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the phase of answer generation, the answer type classification subtask refers
    to the process of assigning a specific category to a response according to the
    question. This process serves as a guiding mechanism for the framework to generate
    comprehensive and accurate answers. This classification subtask is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{cls})=$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Pmt_{cls}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where $Agent_{g}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 D-Agent as a Decision-Maker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An agent as a decision maker (D-Agent) is capable of making candidate selections
    step by step through filtering and choosing from given options, harnessing the
    capabilities of an LLM and KB as memory. The D-Agent can effectively handle three
    subtasks, which are delineated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate entity selection:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The selection of candidate entities in URI linking is pivotal to the ultimate
    efficacy of KBQA. Prior research has focused primarily on developing a semantic
    similarity model to address this linking challenge. However, the linking task
    requires numerous iterations of searching within the knowledge base, which poses
    a compatibility issue for LLM-oriented methods. In our framework, an agent as
    a decision maker is utilized initially to filter all potential entity URIs from
    the knowledge base, subsequently deploying an LLM to select candidate URIs from
    a pool of potential identifiers. For each entity, our aim is to find the $\mathcal{K}$
    most possible entity URIs which can be used to traverse over the KB to get the
    final answer. The entity selection subtask can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{es})=$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle Pmt_{es},\theta_{es},\mathcal{K}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Mem_{es}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where $Agent_{d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate relation selection:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The task of candidate relation selection in URI linking presents considerable
    challenges due to the discrepancies between word forms and meanings. Nevertheless,
    the existence of reasoning paths in the knowledge base can be utilized to allow
    for a significant reduction of the search space in relation linking. In our framework,
    an agent as a decision maker endeavors to sieve through all potential relation
    URIs by navigating the knowledge base with candidate entity URIs generated from
    the previous subtask. Subsequently, an LLM is used to select the top $\mathcal{K}$
    most probable relation URIs for output. The relation selection subtask can be
    denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{rs})=$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle Pmt_{rs},\theta_{rs},\mathcal{K}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Mem_{rs}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where memory $Mem_{rs}$ is the number of relation URIs selected by LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate SPARQL selection:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The subtask of candidate SPARQL selection in query construction involves determining
    the appropriate SPARQL queries to obtain the final answers. Given a SPARQL template
    generated by the G-Agent, along with multiple candidate URIs selected from the
    D-Agent in previous subtasks, our D-Agent is targeted to identify the most plausible
    query. To further reduce the difficulty of selection, an executor function is
    applied to eliminate queries that cannot retrieve any results from the knowledge
    base. In conclusion, our aim in this subtask is to use D-Agent to construct executable
    SPARQLs and find the most possible one given a query candidate list with supported
    information. The SPARQL selection subtask can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{qs})=$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle Pmt_{qs},\theta_{qs},\mathcal{K}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Mem_{qs}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\theta_{qs}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where memory $Mem_{qs}$ is the number of queries selected by LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 A-Agent as a Comprehensive Advisor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An advisory agent (A-Agent) is capable of processing a question and a corresponding
    type of answer as input. Its response is generated by either extracting information
    from an external knowledge base or by utilizing its internal knowledge to provide
    a direct answer. This comprehensive answering subtask can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comprehensive answering:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The objective of comprehensive answering in the answer generation phase is
    to derive a definitive response based on an incoming question. Previous work Omar
    et al. ([2023b](#bib.bib14)) has demonstrated that LLMs are more proficient in
    delivering single-fact responses and making Boolean judgments. Given this understanding,
    we implement an advisory agent that incorporates a simple policy to facilitate
    a comprehensive answering approach. Specifically, if a question yields a final
    SPARQL generated from the preceding steps, A-Agent extracts elements from the
    knowledge base to give the answer. Conversely, if the agent does not receive a
    feasible SPARQL, A-Agent provides a direct response with LLM’s internal knowledge,
    following the prompt based on the type of the answer. Additionally, A-Agent will
    send a retry signal to previous phases if no result is generated. The subtask
    can be formulated as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(S_{ca})=$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle Pmt_{ca},\theta_{ca},\mathcal{T}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Mem_{ca}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: ', where $Agent_{a}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Indexed Knowledge Bases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The efficacy of our framework is assessed through the collection of two real
    knowledge bases, specifically DBpedia and YAGO. DBpedia Auer et al. ([2007](#bib.bib1))
    serves as an accessible knowledge base extracted from Wikipedia, while YAGO Pellissier Tanon
    et al. ([2020](#bib.bib16)) is a large knowledge base that includes individuals,
    cities, nations, and organizations. We index the triples and the mentions of entities
    and relations in a Virtuoso endpoint and an Elasticsearch server, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 KBQA Benchmark Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate our framework on datasets including YAGO-QA, LC-QuAD 1.0, and QALD-9,
    which have various difficulties in interpreting the questions. These datasets
    contain questions in English, paired with their respective SPARQL queries, and
    accurate responses derived from a specific knowledge base. QALD-9 Usbeck et al.
    ([2018](#bib.bib24)) and LC-QuAD 1.0 Trivedi et al. ([2017](#bib.bib23)) are frequently
    used to evaluate QA systems in different versions of DBpedia. The recently published
    YAGO-QA in Omar et al. ([2023a](#bib.bib13)), features questions accompanied by
    annotated SPARQL queries and answers sourced from the YAGO. The statistics for
    three benchmarks, along with their associated knowledge bases, are depicted in
    Table [1](#S4.T1 "Table 1 ‣ 4.2 KBQA Benchmark Datasets ‣ 4 Experimental Evaluation
    ‣ Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge
    Base Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The statistics of KBQA benchmarks, including the number of questions
    number, the number of triples, the size of index in Virtuoso and Elasticsearch.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmarks | Benchmark Statistics |'
  prefs: []
  type: TYPE_TB
- en: '| #Questions | KB | #Triples | Virtuoso Size | ES size |'
  prefs: []
  type: TYPE_TB
- en: '| LC-QuAD 1.0 | 1000 | DBpedia-04 | 397M | 35.40G | 1.56G |'
  prefs: []
  type: TYPE_TB
- en: '| QALD-9 | 150 | DBpedia-10 | 374M | 36.89G | 1.57G |'
  prefs: []
  type: TYPE_TB
- en: '| YAGO-QA | 100 | YAGO-4 | 207M | 24.85G | 0.54G |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Baseline Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate Triad against traditional KBQA systems such as KGQAN Omar et al.
    ([2023a](#bib.bib13)), EDGQA Hu et al. ([2021](#bib.bib8)) and gAnswer Hu et al.
    ([2018](#bib.bib7)). This comparison shows how our LLM-based agent framework can
    rival full-shot systems with just a few examples. Additionally, we contrast our
    framework with pure GPT models like GPT-3.5 Turbo and GPT-4 ²²2https://platform.openai.com/docs/models
    to exhibit Triad’s architectural performance. We treat these foundation models
    as few-shot methods to answer the questions referring to some examples.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Triad is implemented with Python 3.9\. We incorporate LLM capabilities to our
    multi-role agent via OpenAI’s API services. The names of entities and relations
    from knowledge bases are indexed in an ElasticSearch 7.5.2 server for text matching.
    All triples are imported into an SPARQL endpoint of Virtuoso 07.20.3237 for traversal
    and retrieval. Triad requires four hyperparameters: the number of examples G-Agent
    uses for subtask learning, the number of candidates D-Agent selects for entity
    and relation linking, and the retry times for handling non-response SPARQLs. The
    optimal values for these parameters are 3, 2, 2, and 3, respectively. The framework
    and its variants are tested five times on each benchmark, with the average scores
    reported as the final results. For traditional systems, we report the results
    recorded in their papers. For pure LLM baselines, we write prompts to hire an
    LLM to answer questions directly referring to examples, and then link the mentions
    from the responses to the URIs in our indexed knowledge bases via built-in similarity
    search.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The performance of Triad compared to traditional KBQA systems and pure LLM
    generation methods is shown in Table [2](#S4.T2 "Table 2 ‣ 4.5 Performance Comparison
    ‣ 4 Experimental Evaluation ‣ Triad: A Framework Leveraging a Multi-Role LLM-based
    Agent to Solve Knowledge Base Question Answering"). Evaluation metrics precision(P),
    recall(R), and F1-score(F1) are reported. We can observe from the experimental
    results that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The performance of our Triad on three benchmarks, comparing with traditional
    KBQA systems (full-shot) and pure LLM (few-shot) baselines. The optimal and suboptimal
    scores are highlighted with bold and underlined text, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Frameworks | LC-QuAD 1.0 | QALD-9 | YAGO-QA |'
  prefs: []
  type: TYPE_TB
- en: '| P | R | F1 | P | R | F1 | P | R | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| full-shot | gAnswer | - | - | - | 0.293 | 0.327 | 0.298 | 0.585 | 0.341 |
    0.430 |'
  prefs: []
  type: TYPE_TB
- en: '|  | EDGQA | 0.505 | 0.560 | 0.531 | 0.313 | 0.403 | 0.320 | 0.419 | 0.408
    | 0.414 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KGQAN | 0.587 | 0.461 | 0.516 | 0.511 | 0.387 | 0.441 | 0.485 | 0.652
    | 0.556 |'
  prefs: []
  type: TYPE_TB
- en: '| few-shot | GPT-3.5 | 0.269 | 0.251 | 0.266 | 0.240 | 0.217 | 0.228 | 0.171
    | 0.142 | 0.155 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4 | 0.336 | 0.344 | 0.340 | 0.250 | 0.249 | 0.249 | 0.193 | 0.190
    | 0.191 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Triad-GPT3.5 | 0.490 | 0.519 | 0.504 | 0.293 | 0.302 | 0.297 | 0.660 |
    0.639 | 0.649 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Triad-GPT4 | 0.561 | 0.568 | 0.564 | 0.408 | 0.425 | 0.416 | 0.690 | 0.664
    | 0.677 |'
  prefs: []
  type: TYPE_TB
- en: Few-shot can be competitive with full-shot.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our multi-role LLM-based agent framework, though executing a few-shot prompt
    learning, exhibits competitive performance with cutting-edge full-shot KBQA systems.
  prefs: []
  type: TYPE_NORMAL
- en: Underlying capability matters.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The use of GPT-4 as the core in an LLM-based agent significantly outperforms
    GPT-3.5 on all benchmarks, demonstrating the importance of the underlying capabilities
    of an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit knowledge is necessary.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pure LLM models with GPT-3.5 and GPT-4 display deficiencies in generating accurate
    responses without an auxiliary knowledge base as a memory for intermediary steps
    such as URI linking.
  prefs: []
  type: TYPE_NORMAL
- en: Performance varies with complexity.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Triad demonstrates superior results on the LC-QuAD and YAGO-QA benchmarks compared
    to QALD-9, due to an increasing failure in response to complex questions, which
    will be discussed later.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Agent Role Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We assess the efficacy of G-Agent with various other language models as the
    core. The framework without G-task uses the text-davinci-002, which is not as
    powerful as GPT-3.5 and GPT-4 in solving many tasks, and the one without G-chat
    uses text-davinci-003 to eliminate the chat and alignment abilities. We test the
    ability of D-Agent without D-uri and D-query by replacing the URI selection and
    query selection with URI matching and query generation, respectively. We evaluate
    the contribution of A-Agent eliminating A-llm and A-fact by responding to questions
    without using LLM’s assistance or use an LLM to answer Boolean questions for auxiliary
    rather than single-fact questions. The F1 results of the role ablation experiments
    are shown in Table [3](#S4.T3 "Table 3 ‣ 4.6 Agent Role Ablation Study ‣ 4 Experimental
    Evaluation ‣ Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve
    Knowledge Base Question Answering"). The results indicate that every component
    pertaining to each role contributes to the overall performance. More specifically,
    a G-Agent that employs a less powerful LLM as its core can drastically undermine
    performance. D-Agent assumes a more pivotal role during the linking phase compared
    to the query construction phase. A-Agent, on the other hand, proves to be an efficient
    solution for managing situations where SPARQL results are absent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Ablation study on the roles of LLM-based agent by eliminating an element
    or downgrading the capability.'
  prefs: []
  type: TYPE_NORMAL
- en: '| G-task | G-chat | LC-QuAD 1.0 | QALD-9 |'
  prefs: []
  type: TYPE_TB
- en: '| ✗ | ✗ | 0.343 | 0.159 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✗ | 0.443 | 0.248 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | 0.564 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| D-uri | D-query | LC-QuAD 1.0 | QALD-9 |'
  prefs: []
  type: TYPE_TB
- en: '| ✗ | ✓ | 0.274 | 0.210 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✗ | 0.431 | 0.301 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | 0.564 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| A-llm | A-fact | LC-QuAD 1.0 | QALD-9 |'
  prefs: []
  type: TYPE_TB
- en: '| ✗ | ✗ | 0.459 | 0.382 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✗ | 0.473 | 0.385 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | 0.564 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: 4.7 Role Hyperparameter Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We concentrate on three hyperparameters of roles, including the number of examples
    ($\mathcal{N}$) launched by A-Agent when there is no response. The space of hyperparameters
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example numbers $\mathcal{N}$: {1,2,3}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URI candidate numbers of entities and relations $\mathcal{K}$: {(1,1),(1,2),(2,2),(2,3)}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Retry times $\mathcal{T}$: {1,2,3}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.7 Role Hyperparameter Analysis ‣ 4 Experimental
    Evaluation ‣ Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve
    Knowledge Base Question Answering") presents the F1 results of Triad’s performance,
    employing three hyperparameters on two benchmarks. From the results, we discover
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Performance evaluation on three hyperparameters that related to each
    role of an LLM-based agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Triad Variants | LC-QuAD 1.0 | QALD-9 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-1-Shot | 0.556 | 0.376 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-2-Shot | 0.511 | 0.402 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-3-Shot | 0.564 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-Top1-1 | 0.528 | 0.281 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-Top1-2 | 0.562 | 0.375 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-Top2-2 | 0.564 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-Top2-3 | 0.558 | 0.384 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-1-Try | 0.529 | 0.375 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-2-Tries | 0.561 | 0.407 |'
  prefs: []
  type: TYPE_TB
- en: '| Triad-3-Tries | 0.564 | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: Quality is more important than quantity.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: More examples provided to G-Agent do not always improve the performance. The
    efficacy of G-Agent is significantly influenced by the quality of the provided
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: More options may harm the result.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Choosing more candidate URIs for entities and relations could potentially disrupt
    subsequent query phases, thus affecting overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: More chances benefits the framework.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Persistently attempting to construct and execute SPARQL queries, despite the
    initial lack of results, is an effective strategy that improves the probability
    of obtaining accurate answers. Considering the efficiency of overall execution,
    we set the maximum retry times as 3 in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Linking Survival Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process of linking is a relatively complex subtask in both the Text2SQL
    and the KBQA process Li et al. ([2023b](#bib.bib11)). Calculating the recall ratio
    of accurate URIs using D-Agent provides clarity on which step most adversely impacts
    performance. In the entity linking phase, considering all URIs of entities in
    the testing set as the ground truth of the linking results, 80\. 75% of the correct
    URIs are contained from the output of the entity matching filter in D-Agent and
    70\. 50% of the correct URIs are retained from the entity selection performed
    by the LLM in D-Agent. Whereas, in the relation linking phase, only 52\. 54% of
    the correct relation URIs survive from the selection of LLM, which indicates a
    greater difficulty in relation linking.
  prefs: []
  type: TYPE_NORMAL
- en: 4.9 Complex Case Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the impressive performance of Triad in certain benchmarks, notable
    deficiencies remain in its ability to understand questions and generate queries
    for complex questions. A critical analysis of unsuccessful cases in QALD-9, which
    has the lowest F1 score, has revealed three primary reasons for this failure,
    as detailed below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The major reasons of complexity that result in failures, with their
    corresponding ratios of occurrence in failed cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fail Reason | Ratio | Example |'
  prefs: []
  type: TYPE_TB
- en: '| Complex Syntax | 20% | Q42: Which countries have places with more than two
    caves? |'
  prefs: []
  type: TYPE_TB
- en: '| Unexploited Semantics | 17% | Q199: Give me all Argentine films. |'
  prefs: []
  type: TYPE_TB
- en: '| Implicit Reasoning | 5% | Q133: What are the names of the Teenage Mutant
    Ninja Turtles? |'
  prefs: []
  type: TYPE_TB
- en: Complex Syntax
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'signifies that advanced SPARQL queries incorporate keywords such as GROUP BY
    and HAVING. These terms augment the error propensity in the generation of SPARQL
    templates such as the example: Which frequent flyer program has the most airlines?'
  prefs: []
  type: TYPE_NORMAL
- en: Unexploited Semantics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: indicates that semantics of an implicit entity should be comprehended in order
    to exclude irrelevant URIs. In the example Give me all Argentine films, the meaning
    of films should be used to narrow down the scope of potential entities in order
    to eliminate unrelated answers.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: presents a challenge that requires a deeper level of traversal by the framework
    to deduce accurate results from the posed question. For example, another failure
    question, How many grand-children did Jacques Cousteau have?, the term grand-children
    must be interpreted to son of son to ensure an accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 SPARQL-based and LLM-based KBQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional SPARQL-based KBQA methods transform natural language queries into
    SPARQL requests for data extraction. Various specific models are employed either
    for question understanding or URI linking, utilizing domain-based training datasets.
    Hu et al. ([2018](#bib.bib7)) introduces a semantic query graph to structurally
    represent the natural language query, thereby simplifying a SPARQL-based KBQA
    into a subgraph matching problem. Hu et al. ([2021](#bib.bib8)) proposes an entity
    description graph to represent natural language queries for question parsing and
    element linking. The most recent system Omar et al. ([2023a](#bib.bib13)) restructures
    the question parsing task as a text generation issue using a sequence-to-sequence
    model, as opposed to a rule-based transformation. With the advent of large language
    models (LLMs), certain phases of KBQA can be enhanced with LLM-integrated methods.
    Baek et al. ([2023](#bib.bib2)) aims to augment LLM-based QA tasks with pertinent
    facts extracted from knowledge bases, offering a fully zero-shot architecture.
    Tan et al. ([2023a](#bib.bib20)) leverages the general applicability of LLMs to
    filter linking candidates by making selections via few-shot in-context learning.
    Omar et al. ([2023b](#bib.bib14)) provides a thorough comparison between LLMs
    and QA systems in KBQA tasks, recommending further studies to improve the KBQA
    process with LLM capabilities. However, apart from the above studies, our investigation
    proposes a unified framework for KBQA tasks, which incorporates both an LLM and
    few-shot or zero-shot prompting from a systematic perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 LLM-based Agents for Complex Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have recently gained significant attention due to their ability to approximate
    human-level intelligence through the acquisition of vast amounts of web knowledge.
    This has led to numerous studies focusing on LLM-based agents. A recent surveyWang
    et al. ([2023](#bib.bib25)) proposes a unified architectural design for LLM-based
    agents, which consists of four modules that include profile, memory, plan, and
    action. CHATDBHu et al. ([2023a](#bib.bib5)) employs an LLM controller to generate
    SQL instructions, enabling it to manipulate databases, which allows for symbolic
    memory and complex multi-hop reasoning. ARTParanjape et al. ([2023](#bib.bib15))
    uses a frozen LLM to generate intermediate reasoning steps and further integrates
    tools for new tasks with minimal human intervention. ToolformerSchick et al. ([2023](#bib.bib17))
    takes a different approach by training an LLM to plan and execute tools for the
    next token prediction by learning API calls generation. ReActYao et al. ([2023](#bib.bib26))
    focuses on overcoming LLM hallucination by interacting with external knowledge
    bases, thus generating interpretable task-solving strategies. Divergent from the
    aforementioned research, our framework concentrates on the resolution of KBQA
    tasks by introducing a multi-role LLM-based agent that specializes in various
    types of subtask.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we aim to bridge the gap between KBQA tasks and the investigation
    of LLM-based agents. We introduce Triad, a unified framework to address the KBQA
    task through an LLM-based agent acting as three roles. This includes a generalist
    capable of mastering diverse tasks given minimal examples, a decision-maker concentrating
    on option identification and candidate selection, and an advisor skilled in answering
    questions with the aid of both external and internal knowledge. Triad achieves
    the best or competitive performance across three benchmark datasets compared to
    traditional KBQA systems trained on full-size QA data and pure LLM models. In
    future research, we plan to broaden our framework to handle more intricate scenarios,
    such as accommodating advanced queries and multi-hop implicit reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All datasets utilized in this study are publicly available and we have adhered
    to ethical considerations by not introducing additional information as input during
    LLM training and LLM text generation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Auer et al. (2007) Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann,
    Richard Cyganiak, and Zachary Ives. 2007. [Dbpedia: A nucleus for a web of open
    data](https://doi.org/https://doi.org/10.1007/978-3-540-76298-0_52). In *The Semantic
    Web*, pages 722–735, Berlin, Heidelberg. Springer Berlin Heidelberg.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baek et al. (2023) Jinheon Baek, AlhamFikri Aji, and Amir Saffari. 2023. [Knowledge-augmented
    language model prompting for zero-shot knowledge graph question answering](https://doi.org/https://doi.org/10.48550/arXiv.2306.04136).
    *arXiv preprint arXiv:2306.04136*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2023) Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang
    Sui, and Furu Wei. 2023. Large language model for science: A study on p vs. np.
    *arXiv preprint arXiv:2309.05689*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. [A survey on in-context learning](https://doi.org/https://doi.org/10.48550/arXiv.2301.00234).
    *arXiv preprint arXiv:2301.00234*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2023a) Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao,
    and Hang Zhao. 2023a. [Chatdb: Augmenting llms with databases as their symbolic
    memory](https://doi.org/https://doi.org/10.48550/arXiv.2306.03901). *arXiv preprint
    arXiv:2306.03901*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2023b) Nan Hu, Yike Wu, Guilin Qi, Dehai Min, Jiaoyan Chen, Jeff Z
    Pan, and Zafar Ali. 2023b. [An empirical study of pre-trained language models
    in simple knowledge graph question answering](https://doi.org/https://doi.org/10.1007/s11280-023-01166-y).
    *World Wide Web*, pages 1–32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018) Sen Hu, Lei Zou, Jeffrey Xu Yu, Haixun Wang, and Dongyan Zhao.
    2018. [Answering natural language questions by subgraph matching over knowledge
    graphs](https://doi.org/10.1109/tkde.2017.2766634). *IEEE Transactions on Knowledge
    and Data Engineering*, page 824–837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Xixin Hu, Yiheng Shu, Xiang Huang, and Yuzhong Qu. 2021. [Edg-based
    question decomposition for complex question answering over knowledge bases](https://doi.org/10.1007/978-3-030-88361-4_8).
    In *The Semantic Web – ISWC 2021: 20th International Semantic Web Conference,
    ISWC 2021, Virtual Event, October 24–28, 2021, Proceedings*, page 128–145, Berlin,
    Heidelberg. Springer-Verlag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://doi.org/https://doi.org/10.48550/arXiv.2205.1191).
    *Advances in neural information processing systems*, 35:22199–22213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao
    Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. 2023a. [Graphix-t5:
    Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing](https://doi.org/https://doi.org/10.48550/arXiv.2301.07507).
    *arXiv preprint arXiv:2301.07507*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen
    Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, KevinC.C.
    Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023b. [Can llm already serve
    as a database interface? a big bench for large-scale database grounded text-to-sqls](https://doi.org/https://doi.org/10.48550/arXiv.2305.03111).
    *arXiv preprint arXiv:2305.03111*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke,
    Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al.
    2023. [Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents](https://doi.org/https://doi.org/10.48550/arXiv.2308.05960).
    *arXiv preprint arXiv:2308.05960*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Omar et al. (2023a) Reham Omar, Ishika Dhall, Panos Kalnis, and Essam Mansour.
    2023a. [A universal question-answering platform for knowledge graphs](https://doi.org/10.1145/3588911).
    *Proceedings of the ACM on Management of Data*, 1(1):1–25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Omar et al. (2023b) Reham Omar, Omij Mangukiya, Panos Kalnis, and Essam Mansour.
    2023b. [Chatgpt versus traditional question answering for knowledge graphs: Current
    status and future directions towards knowledge graph chatbots](https://doi.org/https://doi.org/10.48550/arXiv.2302.06466).
    *arXiv preprint arXiv:2302.06466*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paranjape et al. (2023) Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh
    Hajishirzi, Luke Zettlemoyer, and MarcoTulio Ribeiro. 2023. [Art: Automatic multi-step
    reasoning and tool-use for large language models](https://doi.org/https://doi.org/10.48550/arXiv.2303.09014).
    *arXiv preprint arXiv:2303.09014*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pellissier Tanon et al. (2020) Thomas Pellissier Tanon, Gerhard Weikum, and
    Fabian Suchanek. 2020. [Yago 4: A reason-able knowledge base](https://doi.org/https://doi.org/10.1007/978-3-030-49461-2_34).
    In *The Semantic Web*, pages 583–596, Cham. Springer International Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. [Toolformer:
    Language models can teach themselves to use tools](https://doi.org/https://doi.org/10.48550/arXiv.2302.04761).
    *arXiv preprint arXiv:2302.04761*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. [Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface](https://doi.org/https://doi.org/10.48550/arXiv.2303.17580).
    *arXiv preprint arXiv:2303.17580*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. [Reflexion:
    an autonomous agent with dynamic memory and self-reflection](https://doi.org/https://doi.org/10.48550/arXiv.2303.11366).
    *arXiv preprint arXiv:2303.11366*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2023a) Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, Wenliang Chen, Zhefeng
    Wang, Baoxing Huai, and Min Zhang. 2023a. [Make a choice! knowledge base question
    answering with in-context learning](https://doi.org/https://doi.org/10.48550/arXiv.2305.13972).
    *arXiv preprint arXiv:2305.13972*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2023b) Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen,
    and Guilin Qi. 2023b. [Can chatgpt replace traditional kbqa models? an in-depth
    analysis of the question answering performance of the gpt llm family](https://doi.org/https://doi.org/10.48550/arXiv.2303.07992).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2023c) Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen,
    and Guilin Qi. 2023c. [Evaluation of chatgpt as a question answering system for
    answering complex questions](https://doi.org/https://doi.org/10.48550/arXiv.2303.0799).
    *arXiv preprint arXiv:2303.07992*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trivedi et al. (2017) Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and
    Jens Lehmann. 2017. [Lc-quad: A corpus for complex question answering over knowledge
    graphs](https://doi.org/https://doi.org/10.1007/978-3-319-68204-4_22). In *The
    Semantic Web – ISWC 2017*, pages 210–218, Cham. Springer International Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usbeck et al. (2018) Ricardo Usbeck, Ria Hari Gusmita, Axel-Cyrille Ngonga Ngomo,
    and Muhammad Saleem. 2018. [9th challenge on question answering over linked data
    (qald-9) (invited paper)](https://doi.org/https://api.semanticscholar.org/CorpusID:53220210).
    In *Semdeep/NLIWoD@ISWC*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. [A survey
    on large language model based autonomous agents](https://doi.org/https://doi.org/10.48550/arXiv.2308.11432).
    *arXiv preprint arXiv:2308.11432*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. [React: Synergizing reasoning and acting
    in language models](https://doi.org/https://doi.org/10.48550/arXiv.2210.03629).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Prompts Provided to LLMs of G-Agent for Solving Various Subtasks
    in KBQA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{g}$ is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to identify triples within a provided sentence. Please
    adhere to the following guidelines: 1\. Triples should be structured in the format
    <entity1, relation, entity2>. 2\. The sentence must contain at least one triple,
    so you should provide at least one. 3\. Entities should represent the smallest
    semantic units and should not contain descriptive details. 4\. Entities can take
    the form of explicit or implicit references. Explicit entities refer to specific
    named resources, whereas implicit entities are less certain. 5\. When an entity
    is implicit, utilize a variable format such as ’?variable’ to denote it, for example,
    ’?location’ or ’?person’. Here are some examples: Which city’s founder is John
    Forbes? : <?city, foundeer, John Forbes> How many races have the horses bred by
    Jacques Van’t Hart participated in? : <?horse, participated in, ?race> <?horse,
    breeder, Jacques Van’t Hart> Is camel of the chordate phylum? : <camel, phylum,
    chordate> Sentence: <Question Sentence> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{g}$ for SPARQL template generation is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to generate a SPARQL query to address a specific question.
    Here are the guidelines to follow: 1\. Ensure that the resulting SPARQL query
    is designed to answer the provided question. 2\. Adhere to the commonly accepted
    SPARQL standards when generating the query. 3\. Make an effort to leverage the
    information provided to assist in the creation of the SPARQL query. 4\. Strive
    to keep the generated SPARQL query as straightforward as possible. 5\. Avoid including
    ’PREFIX’ or ’:’ in the SPARQL query. 6\. Enclose condition entities and predicates
    within angle brackets, such as <entity> or <predicate>. 7\. Maintain the original
    order of the given triples without altering their sequence. Question: <question
    sentence> Triplets: <extracted triplets> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{g}$ for question type classification is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to determine the specific type of a given question according
    to the following guidelines: 1\. You must determine the most probable question
    type for the input question. 2\. The type of question should be enclosed within
    angle brackets, denoted as ’<’ and ’>’. 3\. Possible question types include: <count>,
    <select>, and <yes or no>. Question: <question sentence> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Prompts Provided to LLMs of D-Agent for Solving Selection Subtasks
    in KBQA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{d}$ for candidate entities selection is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to select <K> URIs from a provided list of possible URIs
    for a specified entity, following these guidelines: 1\. Identify the <K> most
    appropriate URIs from the given list that best represent the entity in question.
    2\. Seek to understand the semantic information associated with the specified
    entity by examining the provided question. 3\. The output should consist of <K>
    URIs chosen from the provided list of possible URIs. 4\. Simply output these <K>
    target URIs, each on a separate line, without providing any additional explanations.
    Sentence: <question sentence> Entity: <entity mention> Possible entity URIs: <Entity
    URI list> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{d}$ for candidate relation selection is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant tasked with selecting the <K> relation URIs between entities
    mentioned in a sentence. Here are the guidelines: 1\. The two entities are listed
    one after the other, without a specific order. 2\. Use the provided sentence to
    discern the semantic meaning of these entities. 3\. The potential relation URIs
    are listed one by one. 4\. Your output should consist of a maximum of <K> possible
    relation URIs, although you may also output fewer if appropriate. 5\. Ensure that
    your output is organized, prioritizing the most likely relationship first. 6\.
    Provide a list of no more than <K> relation URIs (each on a separate line if there
    are multiple) without any additional descriptions. Sentence: <question sentence>
    Entities: <entity pair> Possible relation URIs: <URI list> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{d}$ for the final SPARQL selection is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to select an appropriate SPARQL query from the provided
    list in order to respond to a specific question. Please adhere to the following
    guidelines: 1\. Select the most suitable SPARQL query from the given query list
    to address the question. 2\. Select a SPARQL query solely from the provided list;
    avoid crafting your own SPARQL query. 3\. The selected SPARQL query must be applicable
    to answer the given question. Sentence: <question sentence> SPARQL candidates:
    <SPARQLs to choose> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompt Provided to LLMs of A-Agent for Solving Answering Subtask
    in KBQA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{a}$ to generate a yes or no answer for
    the give question is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to answer a yes-or-no question. Please adhere to the following
    guidelines: 1\. If you believe that the answer is yes, provide an output of ’True’.
    If not, provide an output of ’False’. 2\. Please do not include additional information
    or explanations in your response. Sentence: <question sentence> Output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt given to LLMs of $Agent_{a}$ to generate a single-fact answer for
    the give question is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an assistant to answer a question. Please adhere to the following guidelines:
    1\. The answer to the question is a single entity. 2\. You should just output
    the full expression of the answer without any punctuation. 3\. Do not output any
    other description. Sentence: <question sentence> Output:'
  prefs: []
  type: TYPE_NORMAL
