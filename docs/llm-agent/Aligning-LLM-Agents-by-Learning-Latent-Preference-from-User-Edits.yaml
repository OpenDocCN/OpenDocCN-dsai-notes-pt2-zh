- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:48:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Aligning LLM Agents by Learning Latent Preference from User Edits
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过学习用户编辑中的潜在偏好对齐LLM代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.15269](https://ar5iv.labs.arxiv.org/html/2404.15269)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.15269](https://ar5iv.labs.arxiv.org/html/2404.15269)
- en: Ge Gao^♣^∗   Alexey Taymanov^♢^∗   Eduardo Salinas^♢  Paul Mineiro^♢  Dipendra
    Misra^♢
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ge Gao^♣^∗   Alexey Taymanov^♢^∗   Eduardo Salinas^♢  Paul Mineiro^♢  Dipendra
    Misra^♢
- en: Department of Computer Science, Cornell University^♣     Microsoft Research
    New York^♢
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学计算机科学系^♣     微软研究院纽约^♢
- en: ggao@cs.cornell.edu   {ataymano, edus, pmineiro, dimisra}@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ggao@cs.cornell.edu   {ataymano, edus, pmineiro, dimisra}@microsoft.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We study interactive learning of language agents based on user edits made to
    the agent’s output. In a typical setting such as writing assistants, the user
    interacts with a language agent to generate a response given a context, and may
    optionally edit the agent response to personalize it based on their *latent* preference,
    in addition to improving the correctness. The edit feedback is *naturally generated*,
    making it a suitable candidate for improving the agent’s alignment with the user’s
    preference, and for reducing the cost of user edits over time. We propose a learning
    framework, PRELUDE, to conduct PREference Learning from User’s Direct Edits by
    inferring a description of the user’s latent preference based on historic edit
    data and using it to define a prompt policy that drives future response generation.
    This avoids fine-tuning the agent, which is costly, challenging to scale with
    the number of users, and may even degrade its performance on other tasks. Furthermore,
    learning descriptive preference improves interpretability, allowing the user to
    view and modify the learned preference. However, user preference can be complex,
    subtle, and vary based on context, making it challenging to learn. To address
    this, we propose a simple yet effective algorithm named CIPHER (Consolidates Induced
    Preferences based on Historical Edits with Retrieval). CIPHER leverages a large
    language model (LLM) to infer the user preference for a given context based on
    user edits. In the future, CIPHER retrieves inferred preferences from the $k$-closest
    contexts in the history, and forms an aggregate preference for response generation.
    We introduce two interactive environments – summarization and email writing, for
    evaluation using a GPT-4 simulated user. We compare with algorithms that directly
    retrieve user edits but do not learn descriptive preference, and algorithms that
    learn context-agnostic preference. On both tasks, CIPHER outperforms baselines
    by achieving the lowest edit distance cost. Meanwhile, CIPHER has a lower computational
    expense, as using learned preference results in a shorter prompt than directly
    using user edits. Our further analysis reports that the user preference learned
    by CIPHER shows significant similarity to the ground truth latent preference.¹¹1Our
    code and data are publicly available at [https://github.com/gao-g/prelude](https://github.com/gao-g/prelude).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了基于用户对语言模型输出的编辑进行的交互式学习。在写作助手等典型环境中，用户与语言模型互动以生成响应，并可以选择性地编辑模型响应以根据其*潜在*偏好进行个性化，除了提高准确性之外。编辑反馈是*自然生成*的，这使得它成为改进模型与用户偏好一致性并减少用户编辑成本的合适候选者。我们提出了一个学习框架，PRELUDE，通过推断用户的潜在偏好描述，并基于历史编辑数据进行PREference
    Learning，从而定义一个驱动未来响应生成的提示策略。这避免了精细调优模型，这种方法成本高、难以扩展用户数量，甚至可能降低其在其他任务上的性能。此外，学习描述性偏好提高了解释性，允许用户查看和修改学习到的偏好。然而，用户偏好可能复杂、微妙，并且根据上下文有所变化，这使得学习变得具有挑战性。为此，我们提出了一种简单而有效的算法，名为CIPHER（Consolidates
    Induced Preferences based on Historical Edits with Retrieval）。CIPHER利用大型语言模型（LLM）根据用户编辑推断给定上下文的用户偏好。未来，CIPHER从历史中$k$个最接近的上下文中检索推断出的偏好，并形成一个综合偏好用于响应生成。我们引入了两个交互式环境——摘要和电子邮件写作，用于使用GPT-4模拟用户进行评估。我们与直接检索用户编辑但不学习描述性偏好的算法进行比较，并且与学习上下文无关偏好的算法进行比较。在这两个任务中，CIPHER通过实现最低的编辑距离成本超越了基线。同时，CIPHER具有较低的计算开销，因为使用学习到的偏好生成的提示比直接使用用户编辑更短。我们的进一步分析报告显示，CIPHER学习到的用户偏好与真实的潜在偏好具有显著相似性。¹¹1我们的代码和数据公开可用，网址为[https://github.com/gao-g/prelude](https://github.com/gao-g/prelude)。
- en: '^($*$)^($*$)footnotetext: Equal contribution.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^($*$)^($*$)脚注：同等贡献。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Language agents based on large language models (LLMs) have been developed for
    a variety of applications (Dohmke, [2022](#bib.bib19); Brynjolfsson et al., [2023](#bib.bib9)),
    following recent breakthroughs in improving LLMs (Achiam et al., [2023](#bib.bib1);
    Ouyang et al., [2022b](#bib.bib50); Team et al., [2023](#bib.bib63)). However,
    despite their impressive zero-shot performance, LLMs still need to adapt and personalize
    to a given user and task (Mysore et al., [2023](#bib.bib46); Li et al., [2023](#bib.bib34)).
    In many applications, a natural feedback for LLM-based agents is user edits, where
    a user queries the agent and edits the agent’s response before their own final
    use. In contrast, typical feedback used for fine-tuning, such as the comparison-based
    preference feedback in RLHF, is explicitly collected by providing annotators with
    model responses and asking them to rank (Ziegler et al., [2019](#bib.bib71); Stiennon
    et al., [2020](#bib.bib62); Nakano et al., [2021](#bib.bib47); Ouyang et al.,
    [2022a](#bib.bib49), inter alia), making such feedback an expensive choice for
    improving alignment. Motivated by this observation, we focus on interactive learning
    of LLM-based language agents using user edits as feedback.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型（LLMs）的语言代理已经在多种应用场景中得到开发（Dohmke，[2022](#bib.bib19)；Brynjolfsson 等，[2023](#bib.bib9)），这些开发遵循了最近在改进LLMs方面的突破（Achiam
    等，[2023](#bib.bib1)；Ouyang 等，[2022b](#bib.bib50)；Team 等，[2023](#bib.bib63)）。然而，尽管LLMs在零-shot任务中表现出色，LLMs
    仍然需要根据特定用户和任务进行适应和个性化（Mysore 等，[2023](#bib.bib46)；Li 等，[2023](#bib.bib34)）。在许多应用中，LLM-based代理的自然反馈是用户编辑，即用户查询代理并在最终使用之前编辑代理的响应。相比之下，通常用于微调的反馈，例如RLHF中的比较偏好反馈，是通过向注释者提供模型响应并要求他们进行排名（Ziegler
    等，[2019](#bib.bib71)；Stiennon 等，[2020](#bib.bib62)；Nakano 等，[2021](#bib.bib47)；Ouyang
    等，[2022a](#bib.bib49)等）来显式收集的，这使得这种反馈成为改善对齐的昂贵选择。基于这一观察，我们将重点放在利用用户编辑作为反馈的LLM-based语言代理的互动学习上。
- en: '![Refer to caption](img/237f6c57be59e121dcb0e537a9103197.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/237f6c57be59e121dcb0e537a9103197.png)'
- en: 'Figure 1: Illustration of interactive learning from user edits. Color coding
    in edits is for visualization only – our agent takes the plain revised text as
    feedback.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自用户编辑的互动学习示意图。编辑中的颜色编码仅用于可视化——我们的代理将修订后的文本作为反馈。
- en: Consider the scenario in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits") where a user interacts
    with an LLM-based writing assistant (agent) to complete their task. The interaction
    starts with the user (and the world) providing a context to the agent. This context
    may include a query prompt provided by the user, along with additional information
    provided by the world, such as the content on the screen, current time, and the
    user’s calendar information. The agent generates a textual response to the user
    given the context.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[图1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents by Learning Latent
    Preference from User Edits")中的情景，其中用户与基于LLM的写作助手（代理）互动以完成任务。互动开始时，用户（和世界）向代理提供上下文。这些上下文可能包括用户提供的查询提示，以及世界提供的额外信息，如屏幕上的内容、当前时间和用户的日历信息。代理根据上下文生成对用户的文本响应。
- en: In the beginning, the agent’s response may not be optimal for the user, as it
    is not personalized to this user’s individual needs and preference. As most users
    are not familiar with prompt engineering, and LLMs are often able to generate
    an acceptable response for the task, therefore, users may find it the most convenient
    to simply edit the response when it is not ideal to suit their needs, rather than
    trying different prompts to get new responses. The example in [Figure 1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") illustrates that the user directly edits the summary generated
    by the agent to satisfy their preference on bullet point format. It takes time
    and efforts for the user to make edits. We can measure such cost using a variety
    of metrics, such as the edit distance between the agent-generated response and
    the user-revised text. Zero edit from the user is also a useful feedback, reflecting
    that the agent’s response satisfies this user’s needs. One important feature of
    our setting is that *every natural use of the agent yields an edit feedback for
    learning*. Since there is no distinction between training and testing in this
    setting, we care about minimizing the user’s efforts across all rounds of interaction
    with the agent. In summary, our goal is to learn from the implicit feedback in
    user edit history to minimize the cumulative cost of the user’s efforts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，代理的回应可能不会最适合用户，因为它没有针对该用户的个人需求和偏好进行个性化。由于大多数用户不熟悉提示工程，而大型语言模型（LLMs）通常能够生成适用于任务的可接受回应，因此用户可能会发现直接编辑回应以满足他们的需求，比尝试不同的提示以获得新的回应更为方便。
    [图1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents by Learning Latent
    Preference from User Edits") 的示例展示了用户直接编辑代理生成的摘要，以满足他们在要点格式上的偏好。用户进行编辑需要时间和精力。我们可以通过多种指标来衡量这种成本，例如代理生成的回应与用户修订文本之间的编辑距离。用户零编辑也是一个有用的反馈，反映出代理的回应满足了该用户的需求。我们设置的一个重要特点是*每次自然使用代理都会产生编辑反馈以供学习*。由于在这种设置中没有训练和测试的区别，我们关注的是在所有与代理的交互轮次中最小化用户的努力。总之，我们的目标是从用户编辑历史中的隐性反馈中学习，以最小化用户的累计努力成本。
- en: We conjecture that user edits are driven by user’s hidden preference which can
    be described in natural language. These *preference descriptions* are different
    from the notion of comparison-based preference used in RLHF. In this paper, we
    use the word *preference* to mean *preference descriptions*. For instance, preference
    of the user in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits") can be described as *bullet points*.
    In practice, user preference can be compound, such as preferring *bullet point,
    informal, with emojis* at the same time, and also context-dependent, e.g., *informal*
    tone when writing an email to a family member, and *formal* tone when writing
    to a colleague. In more complex settings, user preference can evolve with time
    (non-stationary), or depend on information unavailable in the context (partially
    observed). Such user preference may not be fully derivable from the context, and
    the user may not even be fully aware of all their preference. These considerations
    imply that user preference is *latent* to the language agent. If the agent could
    learn the *latent* preference correctly, it can significantly improve its performance
    by generating satisfactory responses accordingly. Furthermore, preference learned
    by the agent can be shown to the user to enhance *interpretability*, and can even
    be modified by the user to improve correctness. Motivated by this, we propose
    a learning framework, PRELUDE (PREference Learning from User’s Direct Edits),
    where we seek to learn a textual description of the user preference for a given
    context using the history of user edits.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推测用户的编辑行为受到用户隐性偏好的驱动，这种偏好可以用自然语言描述。这些*偏好描述*与在RLHF中使用的基于比较的偏好概念不同。在本文中，我们使用*偏好*一词来表示*偏好描述*。例如，[图1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")中的用户偏好可以描述为*项目符号*。实际上，用户偏好可能是复合的，例如同时偏好*项目符号、非正式风格、带有表情符号*，也可能依赖于上下文，例如，给家庭成员写邮件时使用*非正式*语气，而给同事写邮件时使用*正式*语气。在更复杂的环境中，用户偏好可能会随着时间演变（非平稳），或依赖于上下文中不可用的信息（部分观察）。这种用户偏好可能无法完全从上下文中推导出来，用户甚至可能没有完全意识到自己的所有偏好。这些考虑意味着用户偏好对于语言代理而言是*潜在的*。如果代理能够正确地学习这种*潜在*偏好，它可以通过相应生成令人满意的响应显著提高性能。此外，代理学到的偏好可以展示给用户以增强*可解释性*，甚至可以由用户修改以提高正确性。受到此启发，我们提出了一种学习框架PRELUDE（用户直接编辑的*偏好学习*），旨在利用用户编辑历史学习给定上下文的用户偏好的文本描述。
- en: In a typical real-world scenario such as writing assistants, one has to potentially
    update the LLM-based agent for every user. Efficient approaches, therefore, must
    scale with the number of users. This makes approaches that perform a full fine-tuning
    of the LLM used by the agent very hard to scale. Furthermore, LLMs typically undergo
    evaluation on a variety of metrics before being released, and thus fine-tuning
    them often results in breaking the generalization guarantees offered by these
    tests. For example, fine-tuning GPT-4 for millions of users can quickly turn very
    expensive. Approaches such as adding LORA and Adapter layers and only updating
    them, or using federated learning, can reduce the expense to some extent, while
    the loss of generalizable alignment remains as a concern. In this work, we focus
    on leveraging a frozen, black-box LLM, and instead learning a *prompt policy*
    that can infer textual description of user’s preference for a given context, and
    then use it to directly drive the response generation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的现实世界场景中，如写作助手，可能需要对每个用户更新基于LLM的代理。因此，效率高的方法必须能随着用户数量的增加而扩展。这使得对代理使用的LLM进行全面微调的方法很难扩展。此外，LLM通常在发布之前会在各种指标上进行评估，因此对其进行微调往往会破坏这些测试提供的泛化保证。例如，对数百万用户进行GPT-4微调可能迅速变得非常昂贵。诸如添加LORA和Adapter层并仅更新它们，或使用联邦学习的方法可以在一定程度上降低费用，但泛化对齐的丧失仍然是一个问题。在这项工作中，我们专注于利用冻结的黑箱LLM，而是学习一个*提示策略*，该策略可以推断给定上下文的用户偏好的文本描述，然后直接驱动响应生成。
- en: We introduce a simple yet effective algorithm CIPHER (Consolidates Induced Preferences
    based on Historical Edits with Retrieval) under the PRELUDE framework. For a given
    context, CIPHER first retrieves the $k$ contexts. It relies on this aggregate
    preference to generate a response for the given context. If the user performs
    no edits, then it saves this aggregate preference as the correct preference for
    the given context. Otherwise, it queries the LLM to infer a plausible preference
    that explains these user edits made to the agent response, and saves this inferred
    preference as the correct preference for the given context. A key advantage of CIPHER is
    that it typically leads to significantly shorter prompts compared to other retrieval
    methods that use the entire documents or context, as inferred preferences are
    much shorter than retrieved documents or contexts. This results in a significant
    reduction in the computational expense of querying the LLM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在PRELUDE框架下介绍了一种简单而有效的算法**CIPHER**（基于历史编辑和检索整合诱导偏好的方法）。对于给定的上下文，**CIPHER**首先检索$k$个上下文。它依赖于这一汇总偏好来生成针对给定上下文的响应。如果用户没有进行编辑，则将此汇总偏好保存为给定上下文的正确偏好。否则，它会查询LLM来推断一个合乎逻辑的偏好，以解释这些用户编辑对代理响应的影响，并将这一推断的偏好保存为给定上下文的正确偏好。**CIPHER**的一个主要优点是，它通常会导致比使用整个文档或上下文的其他检索方法显著更短的提示，因为推断的偏好比检索的文档或上下文短得多。这会显著减少查询LLM的计算开销。
- en: We introduce two interactive environments for evaluation, inspired by writing
    assistant applications. In the first environment, we evaluate the agent’s ability
    to summarize a given document (articles from different sources). In the second
    environment, we evaluate the agent’s ability to compose an email using content
    from a given document (notes for various purpose). In both tasks, we simulate
    a GPT-4 user that can generate edits based on a pre-designed *latent* preference.
    We use documents from several existing domains as our user-provided context, and
    vary the GPT-4 user’s preference based on the domain, in order to capture the
    real-world context-dependent nature of human user’s preference. We evaluate CIPHER against
    several baselines, including approaches that learn context-agnostic user preferences,
    and retrieval-based approaches that do not learn preferences but directly use
    past user edits for generation. We show that for both tasks, CIPHER achieves the
    lowest user edit cost compared to baselines, and significantly reduces the cumulative
    cost compared to using the frozen base agent. Additionally, CIPHER results in
    a lower LLM query cost than other retrieval-based baselines. Finally, we qualitatively
    and quantitatively analyze preferences learned by our agents, and find that they
    show significant similarity to the ground truth latent preferences in our setup.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了两个互动环境用于评估，灵感来源于写作助手应用。在第一个环境中，我们评估代理总结给定文档（来自不同来源的文章）的能力。在第二个环境中，我们评估代理使用给定文档（用于各种目的的笔记）中的内容来撰写电子邮件的能力。在这两个任务中，我们模拟了一个可以基于预设计的*潜在*偏好生成编辑的GPT-4用户。我们使用来自几个现有领域的文档作为用户提供的上下文，并根据领域变化GPT-4用户的偏好，以捕捉人类用户偏好的真实世界上下文依赖特性。我们将**CIPHER**与几个基线方法进行对比，包括学习上下文无关用户偏好的方法和不学习偏好但直接使用过去用户编辑进行生成的检索方法。我们显示，对于这两个任务，**CIPHER**在用户编辑成本上优于基线，并且与使用冻结基础代理相比显著降低了累计成本。此外，**CIPHER**的LLM查询成本低于其他基于检索的基线。最后，我们对我们的代理学习的偏好进行定性和定量分析，发现它们与我们设置中的真实潜在偏好有显著相似性。
- en: 2 Interactive Learning from User Edits and the PRELUDE Framework
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 从用户编辑和PRELUDE框架中进行互动学习
- en: We first describe LLM agents and the general learning framework from user edits.
    We then describe our specialized PRELUDE framework for learning descriptive user
    preference, and discuss associated learning challenges.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先描述LLM代理和来自用户编辑的一般学习框架。然后我们描述我们专门的PRELUDE框架用于学习描述性用户偏好，并讨论相关的学习挑战。
- en: LLM and Language Agents.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 和语言代理。
- en: We assume access to a language agent that internally relies on an LLM. We make
    no assumption about the language agent except that it can take input $x_{t}$ .
    The language agent may simply perform greedy decoding on the LLM, or may perform
    complex planning using the given LLM to generate a response.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设可以访问一个内部依赖于LLM的语言代理。我们对语言代理的假设仅限于它能够接受输入$x_{t}$。语言代理可能仅对LLM执行贪婪解码，或者可能使用给定的LLM执行复杂的规划以生成响应。
- en: Protocol 1 Interactive Learning from User Edits.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 协议 1 用户编辑的互动学习。
- en: 1:for $t=1,2,\cdots,T$
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1:对于 $t=1,2,\cdots,T$
- en: Interactive Learning from User Edits.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从用户编辑中互动学习。
- en: In an application such as a writing assistant, a user interacts with the language
    agent over $T$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在如写作助手这样的应用中，用户与语言代理互动超过 $T$。
- en: In our experiments, we use $\Delta_{\textrm{edit}}$. In general, a higher edit
    distance implies that the user has made more edits and spent more efforts. We
    note that our framework is general enough to accommodate situations where the
    user tries different prompts with the same demand. We treat each call to the language
    agent as a different round with a different context (as the context includes the
    user prompt).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用 $\Delta_{\textrm{edit}}$。通常，较高的编辑距离意味着用户进行了更多编辑并付出了更多努力。我们注意到我们的框架足够通用，可以适应用户尝试不同提示但需求相同的情况。我们将每次调用语言代理视为不同的回合，并具有不同的上下文（因为上下文包括用户提示）。
- en: PRELUDE Framework.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PRELUDE 框架。
- en: We describe our PRELUDE framework in [Protocol 2](#alg2 "Protocol 2 ‣ PRELUDE
    Framework. ‣ 2 Interactive Learning from User Edits and the PRELUDE Framework
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits") which is
    a specialization of the general learning setup described above in [Protocol 1](#alg1
    "Protocol 1 ‣ LLM and Language Agents. ‣ 2 Interactive Learning from User Edits
    and the PRELUDE Framework ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits"). In PRELUDE, in the $t^{th}$ at the start of each round. We
    assume that the LLM remains frozen across all methods in this work.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [协议 2](#alg2 "协议 2 ‣ PRELUDE 框架 ‣ 2 从用户编辑中互动学习及 PRELUDE 框架 ‣ 通过从用户编辑中学习潜在偏好来对齐
    LLM 代理") 中描述了我们的 PRELUDE 框架，这是一种上述 [协议 1](#alg1 "协议 1 ‣ LLM 和语言代理 ‣ 2 从用户编辑中互动学习及
    PRELUDE 框架 ‣ 通过从用户编辑中学习潜在偏好来对齐 LLM 代理") 中描述的通用学习设置的专业化。在 PRELUDE 中，在每轮开始时的第 $t$
    次。我们假设 LLM 在本工作的所有方法中保持不变。
- en: 'Protocol 2 PRELUDE: PREference Learning from User’s Direct Edits'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '协议 2 PRELUDE: 从用户直接编辑中学习偏好'
- en: 1:for $t=1,2,\cdots,T$
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 1:对于 $t=1,2,\cdots,T$
- en: Challenges of Learning User Preference.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习用户偏好的挑战。
- en: Learning user preference from edits is challenging. In practice, user preference
    are multifaceted and complex. Furthermore, user’s preference can also significantly
    vary based on the context. The feedback in the form of user edits emerges naturally
    but is inherently implicit, lacking direct expressions of the actual preference
    and carrying subtleties that may lead to diverse interpretations. The combination
    of preference variability and the implicit nature of feedback poses considerable
    challenges for agents in accurately learning and integrating these preferences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从编辑中学习用户偏好具有挑战性。在实践中，用户偏好是多方面且复杂的。此外，用户的偏好也可能根据上下文显著变化。用户编辑形式的反馈自然出现，但本质上是隐含的，缺乏对实际偏好的直接表达，并且包含可能导致多种解释的细微差别。偏好变异性和反馈隐含性的结合对代理准确学习和整合这些偏好提出了相当大的挑战。
- en: 3 Learning User Preference through Retrieval and Aggregation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通过检索和聚合学习用户偏好
- en: In this section, we present our method, CIPHER (Consolidates Induced Preferences
    based on Historical Edits with Retrieval), that learns user preference based on
    user edits.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法**CIPHER**（基于历史编辑的合成诱导偏好与检索），该方法基于用户编辑学习用户偏好。
- en: '[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning
    User Preference through Retrieval and Aggregation ‣ Aligning LLM Agents by Learning
    Latent Preference from User Edits") shows CIPHER which implements the PRELUDE framework.
    CIPHER maintains a preference history $\mathcal{D}_{t}=\{(x_{\ell},\tilde{f}_{\ell})\}_{\ell=1}^{t-1}$
    as an empty string as the agent has no prior knowledge of this user’s preference.³³3In
    practice, one can initialize with a publicly available preference history.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[算法 1](#alg1a "算法 1 ‣ CIPHER 的计算成本 ‣ 3 通过检索和聚合学习用户偏好 ‣ 通过从用户编辑中学习潜在偏好来对齐 LLM
    代理") 显示了实现 PRELUDE 框架的 **CIPHER**。**CIPHER** 维护了一个偏好历史 $\mathcal{D}_{t}=\{(x_{\ell},\tilde{f}_{\ell})\}_{\ell=1}^{t-1}$，初始为空字符串，因为代理对该用户的偏好没有先验知识。在实践中，可以使用公开的偏好历史进行初始化。'
- en: The agent uses the inferred preference $f_{t}$. We list the actual template
    used in our experiments in [Table 7](#A2.T7 "Table 7 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits") in [Appendix A](#A1 "Appendix A Additional Details ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代理使用推断的偏好 $f_{t}$。我们在[表 7](#A2.T7 "表 7 ‣ 检索准确性 ‣ 附录 B 额外分析 ‣ 通过从用户编辑中学习潜在偏好来对齐
    LLM 代理")中的[附录 A](#A1 "附录 A 额外细节 ‣ 通过从用户编辑中学习潜在偏好来对齐 LLM 代理")列出了我们实验中实际使用的模板。
- en: Given the user edits $y^{\prime}_{t}$ to the preference history.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定对偏好历史的用户编辑 $y^{\prime}_{t}$。
- en: Note that we cannot query the LLM for the inferred preference in the first case
    where the user edit cost $c_{t}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第一种情况下，我们无法查询 LLM 以获取推断的偏好，其中用户编辑成本 $c_{t}$。
- en: Computational Cost of CIPHER.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CIPHER 的计算成本。
- en: In a given round, CIPHER adds a maximum of 3 LLM calls on top of the cost of
    calling the underlying inference algorithm of the agent in [line 6](#alg1.l6a
    "In Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits"). CIPHER further reduces the memory storage by only storing the
    representation of contexts in the preference string instead of the input itself.
    Finally, CIPHER only adds a small prompt to the context $x_{t}$, before calling
    the agent’s inference algorithm. This only slightly increases the length of the
    prompt, thereby, reducing the query cost associated with LLMs that scales with
    the number of input tokens.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的轮次中，CIPHER 在调用代理的基础推理算法的成本上最多增加 3 次 LLM 调用。[line 6](#alg1.l6a "在算法 1 ‣ CIPHER
    的计算成本 ‣ 通过检索和聚合学习用户偏好 ‣ 通过从用户编辑中学习潜在偏好来对齐 LLM 代理")中，CIPHER 进一步减少了内存存储，只存储上下文的表示，而不是输入本身。最后，CIPHER
    在调用代理的推理算法之前，仅在上下文 $x_{t}$ 上添加了一个小提示。这仅略微增加了提示的长度，从而减少了与 LLM 相关的查询成本，该成本随着输入令牌的数量而增加。
- en: Algorithm 1 CIPHER$(\phi,k,\delta)$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 CIPHER$(\phi,k,\delta)$。
- en: 1:$\mathcal{D}=\emptyset$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$\mathcal{D}=\emptyset$
- en: 4 Experiment
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we first introduce two interactive tasks for evaluating agents
    that learn from user edits. These tasks can be used more broadly even outside
    the PRELUDE framework, and can be of independent interest. We then describe our
    baselines and provide implementation details of CIPHER. Finally, we provide quantitative
    results in terms of user edit cost and qualitative analysis of the learned preferences.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍两个用于评估从用户编辑中学习的代理的互动任务。这些任务甚至可以在 PRELUDE 框架之外更广泛地使用，并且具有独立的兴趣。接下来，我们描述了我们的基准，并提供了
    CIPHER 的实现细节。最后，我们提供了关于用户编辑成本的定量结果和学习到的偏好的定性分析。
- en: 4.1 Two Interactive Writing Assistant Environments for Learning from User Edits
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 两个用于从用户编辑中学习的互动写作助手环境
- en: Task.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务。
- en: We introduce two tasks inspired by the use of LLMs as writing assistants (Mysore
    et al., [2023](#bib.bib46); Shen et al., [2023](#bib.bib58); Wang et al., [2023](#bib.bib64)).
    In the first task, we evaluate the agent’s ability to summarize a given document.
    We use documents from 5 existing sources listed in [Table 1](#S4.T1 "Table 1 ‣
    Task. ‣ 4.1 Two Interactive Writing Assistant Environments for Learning from User
    Edits ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits").⁴⁴4[Table 4](#A2.T4 "Table 4 ‣ Retrieval Accuracy. ‣ Appendix B Additional
    Analysis ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")
    in Appendix provides links to each source dataset, used as user-provided context
    in our tasks. These sources represent a diverse category of documents that a writing
    assistant would typically encounter, including news articles that are formal and
    concise, movie reviews that are informal, and paper abstracts that are technical.
    In the second task, we evaluate the agent’s ability to compose an email given
    notes. For this task, we use notes from four different sources including a variety
    of tasks such as writing emails to friends, describing reports to managers, and
    writing reviews for colleagues. In any given round, the user is provided a context
    that is a document from one of the document sources for the given task. Importantly,
    the agent is *unaware of the source of the given document* which as we discuss
    later, will determine the user preference. For both tasks, we run an experiment
    for $T=200$ rounds, with an equal number of randomly sampled documents from each
    document source. We mix documents from different sources and shuffle them to remove
    any temporal correlation in document source across rounds.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了两个任务，这些任务受到了将 LLMs 用作写作助手的启发（Mysore 等，[2023](#bib.bib46)；Shen 等，[2023](#bib.bib58)；Wang
    等，[2023](#bib.bib64)）。在第一个任务中，我们评估代理总结给定文档的能力。我们使用了来自 5 个现有来源的文档，这些来源列在[表 1](#S4.T1
    "Table 1 ‣ Task. ‣ 4.1 Two Interactive Writing Assistant Environments for Learning
    from User Edits ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")中。附录中的[表 4](#A2.T4 "Table 4 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits")提供了每个来源数据集的链接，这些数据集用作我们任务中的用户提供的上下文。这些来源代表了写作助手通常会遇到的多样化文档类别，包括正式且简洁的新闻文章、非正式的电影评论以及技术性的论文摘要。在第二个任务中，我们评估代理在给定笔记的情况下编写电子邮件的能力。对于这个任务，我们使用了来自四个不同来源的笔记，包括各种任务，如给朋友写电子邮件、向经理描述报告和为同事写评论。在任何给定的回合中，用户会提供一个上下文，即来自给定任务的文档来源的文档。重要的是，代理*无法得知给定文档的来源*，这将决定用户的偏好。对于这两个任务，我们进行了
    $T=200$ 回合的实验，每个文档来源随机抽取相同数量的文档。我们混合来自不同来源的文档并打乱它们，以消除各回合之间文档来源的时间相关性。
- en: 'Table 1: Latent user preference design, specific to the document source.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：潜在用户偏好设计，特定于文档来源。
- en: '| Doc Source | Latent User Preference | Scenario |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 文档来源 | 潜在用户偏好 | 场景 |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Summarization |  |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 总结 |  |  |'
- en: '| News article (See et al., [2017](#bib.bib57)) | targeted to young children,
    storytelling, short sentences, playful language, interactive, positive | introduce
    a political news to kids |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 新闻文章（See 等，[2017](#bib.bib57)） | 针对幼儿，讲故事，短句，富有趣味的语言，互动，积极 | 向孩子们介绍政治新闻 |'
- en: '| Reddit post (Stiennon et al., [2020](#bib.bib62)) | second person narrative,
    brief, show emotions, invoke personal reflection, immersive | for character development
    in creative writing |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Reddit 帖子（Stiennon 等，[2020](#bib.bib62)） | 第二人称叙述，简洁，展示情感，引发个人反思，沉浸式 | 在创作写作中用于角色发展
    |'
- en: '| Wikipedia page (Foundation, [2022](#bib.bib23)) | bullet points, parallel
    structure, brief | take notes for key knowledge |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科页面（Foundation，[2022](#bib.bib23)） | 项目符号，平行结构，简洁 | 记录关键知识 |'
- en: '| Paper abstract (Clement et al., [2019](#bib.bib15)) | tweet style, simple
    English, inquisitive, skillful foreshadowing, with emojis | promote a paper to
    invoke more attention and interests |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 论文摘要（Clement 等，[2019](#bib.bib15)） | 推文风格，简单英语，探究，巧妙的伏笔，带表情符号 | 推广论文以引起更多关注和兴趣
    |'
- en: '| Movie review (Maas et al., [2011](#bib.bib39)) | question answering style,
    direct, concise | quickly get main opinions |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 电影评论（Maas 等，[2011](#bib.bib39)） | 问答风格，直接，简洁 | 快速了解主要观点 |'
- en: '| Email Writing |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 电子邮件写作 |  |  |'
- en: '| Personal problem (Stiennon et al., [2020](#bib.bib62)) | informal, conversational,
    short, no closing | share life with friends |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 个人问题（Stiennon 等，[2020](#bib.bib62)） | 非正式，交谈式，简短，无结尾 | 与朋友分享生活 |'
- en: '| Paper review (Hua et al., [2019](#bib.bib28)) | casual tone, positive, clear,
    call to action | peer review to colleague |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 论文评审 (Hua et al., [2019](#bib.bib28)) | 休闲语气、积极、清晰、行动号召 | 对同事的同行评审 |'
- en: '| Paper tweet (Bar, [2022](#bib.bib5)) | engaging, personalized, professional
    tone, thankful closing | networking emails for researchers |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 论文推文 (Bar, [2022](#bib.bib5)) | 引人入胜、个性化、专业的语气，感谢的结尾 | 研究人员的网络邮件 |'
- en: '| Paper summary (Kershaw & Koeling, [2020](#bib.bib29)) | structured, straight
    to the points, respectful, professional greeting and closing | milestone report
    to superiors |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 论文总结 (Kershaw & Koeling, [2020](#bib.bib29)) | 结构化、直截了当、尊重的语气，专业的问候和结尾 |
    向上级提交的里程碑报告 |'
- en: Two-Stage GPT-4 Simulated User.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 双阶段 GPT-4 模拟用户。
- en: We simulate a user that can edit a given response. We define a set of *latent
    user preferences* for the user that vary based on the document source. [Table 1](#S4.T1
    "Table 1 ‣ Task. ‣ 4.1 Two Interactive Writing Assistant Environments for Learning
    from User Edits ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") lists the preference and the corresponding document source.
    This captures the context-dependent nature of user preferences as the document
    source influences the type of context. For example, the *Personal problem* document
    source contains documents pertaining to discussions with a friend, and a user
    may have a different preference when writing an email to a friend compared to
    writing an email to a colleague. In real-world settings, the context dependence
    of the user preference can be more complex than just the document source. We assume
    that our user is aware of the document source $d_{t}$ maps a given document source
    to the user preference. Recall that the *agent in our learning setup is never
    provided the document source of any context*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模拟了一个可以编辑给定响应的用户。我们定义了一组 *潜在用户偏好*，这些偏好根据文档来源有所不同。[表 1](#S4.T1 "表 1 ‣ 任务 ‣
    4.1 从用户编辑中学习的两个互动写作助手环境 ‣ 4 实验 ‣ 通过学习来自用户编辑的潜在偏好来对齐 LLM 代理") 列出了偏好及其对应的文档来源。这捕捉了用户偏好的上下文依赖性质，因为文档来源会影响上下文的类型。例如，*个人问题*
    文档来源包含与朋友讨论的文档，用户在给朋友写电子邮件时可能会有不同的偏好，而与给同事写电子邮件时的偏好不同。在现实世界中，用户偏好的上下文依赖性可能比仅仅文档来源更复杂。我们假设我们的用户知道文档来源
    $d_{t}$ 将给定的文档来源映射到用户偏好。请记住，*在我们的学习设置中，代理从未获得任何上下文的文档来源*。
- en: We model our user using GPT-4 with a two-stage approach. Given an agent response
    $y_{t}$ achieves a minimal user cost.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用双阶段方法通过 GPT-4 来模拟我们的用户。给定一个代理响应 $y_{t}$ 实现最小用户成本。
- en: Evaluation Metric.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评价指标。
- en: We propose three metrics for evaluating agents learning from user edits. Our
    main metric is the cumulative user edit cost $\sum_{t=1}^{T}c_{t}$. To compute
    the edit distance, we perform BPE tokenization using Tiktoken tokenizer, and compute
    the edit distance in the token space. In general, one can learn a metric that
    better captures the cognitive load associated with a user edit. However, Levenshtein
    edit distance provides a clean, transparent metric that is easy to interpret.
    Additionally, it doesn’t have concerns shared by learned metrics such as erroneous
    evaluations when applying the metric to examples not covered by the metric’s training
    distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了三种评估代理从用户编辑中学习的指标。我们的主要指标是累计用户编辑成本 $\sum_{t=1}^{T}c_{t}$。为了计算编辑距离，我们使用
    Tiktoken 分词器进行 BPE 分词，并在代币空间中计算编辑距离。通常，可以学习一个更好地捕捉与用户编辑相关的认知负担的指标。然而，Levenshtein
    编辑距离提供了一个干净、透明且易于解释的指标。此外，它不会有像学习指标那样的担忧，例如在将指标应用于未涵盖指标训练分布的示例时出现错误评估。
- en: For CIPHER and any other method in the PRELUDE framework, we additionally evaluate
    the accuracy of the inferred user preference $f_{t}$, where BERTScore (Zhang*
    et al., [2020](#bib.bib70)) is a popular text similarity metric.⁵⁵5We use the
    microsoft/deberta-xlarge-mnli to implement BERTScore.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CIPHER 以及 PRELUDE 框架中的任何其他方法，我们还评估了推断的用户偏好 $f_{t}$ 的准确性，其中 BERTScore (Zhang*
    et al., [2020](#bib.bib70)) 是一种流行的文本相似性指标。⁵⁵5 我们使用 microsoft/deberta-xlarge-mnli
    实现 BERTScore。
- en: Finally, we evaluate the token expense associated with querying the LLM across
    all methods. We compute the total number of tokens both generated by or provided
    as input to the LLM across all rounds. This is a typical metric used by popular
    LLM providers to charge their customers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了在所有方法中查询 LLM 相关的代币消耗。我们计算了在所有轮次中由 LLM 生成或作为输入提供的总代币数。这是流行 LLM 提供商用来向客户收费的典型指标。
- en: 4.2 Details of CIPHER and Comparison Systems
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 CIPHER 和比较系统的详细信息
- en: We use GPT-4 as our base LLM for CIPHER and all baselines. We do not perform
    fine-tuning of the GPT-4 and do not add any additional parameters to the model.
    We use a prompt-based GPT-4 agent for all methods that uses a single prompt with
    greedy decoding to generate the response. Our main method CIPHER and the baselines,
    can be extended to more complex language agents that perform multiple steps of
    reasoning on top of the base LLM before generating a response.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GPT-4 作为 CIPHER 和所有基准模型的基础 LLM。我们不对 GPT-4 进行微调，也不向模型中添加任何额外的参数。我们使用基于提示的
    GPT-4 代理进行所有方法，该方法使用单个提示并采用贪婪解码生成响应。我们主要的方法 CIPHER 和基准模型，可以扩展到更复杂的语言代理，这些代理在生成响应之前，在基础
    LLM 上执行多个推理步骤。
- en: CIPHER Details.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CIPHER 细节。
- en: We use a simple agent that uses GPT-4 with a prompt template to generate the
    response $y_{t}$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个简单的代理，该代理使用 GPT-4 和一个提示模板来生成响应 $y_{t}$。
- en: Baselines.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准。
- en: We evaluate CIPHER against baselines that either perform no learning, or learn
    context-agnostic preferences and against methods that do not learn preferences
    but directly use past user edits for generating a response.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 CIPHER 与不进行学习的基准进行比较，或者学习上下文无关的偏好，并与那些不学习偏好但直接使用过去用户编辑内容生成响应的方法进行比较。
- en: '1.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'No learning: The agent performs no learning based on interaction with the user.
    In each step, the agent generates a response $y_{t}$.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无学习：代理不会基于与用户的互动进行学习。在每一步中，代理生成一个响应 $y_{t}$。
- en: '2.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Explore-then-exploit (E-then-e) LPI: This baseline is based on the classic
    explore-then-exploit strategy in interactive learning (Garivier et al., [2016](#bib.bib24)).
    The agent first generates responses for the first $T_{e}$ rounds using the LPI
    step similar to [line 12](#alg1.l12 "In Algorithm 1 ‣ Computational Cost of CIPHER.
    ‣ 3 Learning User Preference through Retrieval and Aggregation ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits") in CIPHER([Algorithm 1](#alg1a
    "Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")). It then uses the learned preference to generate the response
    for all remaining rounds (exploitation step).'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Explore-then-exploit (E-then-e) LPI：该基准基于互动学习中的经典 explore-then-exploit 策略（Garivier
    等，[2016](#bib.bib24)）。代理首先在前 $T_{e}$ 轮中使用类似于 CIPHER 中的 [line 12](#alg1.l12 "在算法
    1 ‣ CIPHER 的计算成本。 ‣ 3 通过检索和聚合学习用户偏好 ‣ 通过学习用户编辑的潜在偏好来对齐 LLM 代理") 的 LPI 步骤生成响应。然后，它使用学习到的偏好来生成所有剩余轮次的响应（开发步骤）。
- en: '3.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Continual LPI: This method is similar to explore-then-exploit except that it
    never stops exploring. In any given round $t$.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续 LPI：这种方法类似于 explore-then-exploit，只是它从未停止探索。在任何给定的轮次 $t$ 中。
- en: '4.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'ICL-edit: This is a standard retrieval-based in-context learning (ICL) baseline (Brown
    et al., [2020](#bib.bib8)). In a given round $t$ and the historical data.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ICL-edit：这是一个标准的基于检索的上下文学习（ICL）基准（Brown 等，[2020](#bib.bib8)）。在给定的轮次 $t$ 和历史数据中。
- en: Baseline Hyperparameters.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准超参数。
- en: For explore-then-exploit LPI and continual LPI baselines, we set the number
    of exploration $T_{e}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 explore-then-exploit LPI 和持续 LPI 基准，我们设置了探索次数 $T_{e}$。
- en: Oracle Method.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Oracle Method.
- en: We additionally run an oracle preference method to provide an approximated upper
    bound on performance. In each round $t$. This method can test whether our setup
    is well-defined, e.g., in a poorly designed setup, the user always edits the agent
    response no matter what the agent generates including providing user edits back
    to the user, and thus no method can effectively minimize the cost over time in
    this case. If the oracle method achieves a zero or a minimal user edit cost, then
    learning the optimal preference leads to success.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还运行了一个 oracle 偏好方法，以提供性能的近似上限。在每一轮 $t$ 中。该方法可以测试我们的设置是否定义良好，例如，在一个设计不良的设置中，无论代理生成什么，用户总是编辑代理的响应，包括将用户编辑内容提供给用户，因此在这种情况下没有方法可以有效地随着时间的推移最小化成本。如果
    oracle 方法实现了零或最小的用户编辑成本，那么学习最佳偏好就会取得成功。
- en: 4.3 Main Result and Discussion.
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 主要结果与讨论。
- en: Main Results.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主要结果。
- en: '[Table 2](#S4.T2 "Table 2 ‣ Main Results. ‣ 4.3 Main Result and Discussion.
    ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")
    reports the performance of baselines and our methods on summarization and email
    writing tasks on three metrics: *edit distance* which measures cumulative user
    edit cost, *accuracy* which measures mean preference classification accuracy,
    and *expense* measuring the total BPE token cost of querying LLM.⁶⁶6[Table 9](#A2.T9
    "Table 9 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits") in Appendix shows the breakdown
    of expense in terms of input and output. We report the mean and standard deviation
    across 3 different random seeds.⁷⁷7We randomize the context sampling from source
    datasets, so experiments on different seeds contain different sets of input contexts.
    On the same seed, experiments across different methods are strictly comparable,
    as both the set of input contexts and the order of input context seen are the
    same in our implementation.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#S4.T2 "表 2 ‣ 主要结果。 ‣ 4.3 主要结果和讨论。 ‣ 4 实验 ‣ 通过学习用户编辑的潜在偏好对齐 LLM 代理")
    报告了基线方法和我们的方法在总结和邮件写作任务上的表现，涉及三个指标：*编辑距离*（测量累计用户编辑成本）、*准确性*（测量平均偏好分类准确性）和*开销*（测量查询
    LLM 的总 BPE 令牌成本）。⁶⁶6[表 9](#A2.T9 "表 9 ‣ 检索准确性。 ‣ 附录 B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐 LLM
    代理") 在附录中展示了按输入和输出划分的开销。我们报告了 3 个不同随机种子的均值和标准差。⁷⁷7我们对源数据集的上下文采样进行了随机化，因此不同种子下的实验包含不同的输入上下文集。在相同种子下，不同方法之间的实验严格可比，因为输入上下文集和看到的输入上下文的顺序在我们的实现中是相同的。'
- en: 'Table 2: Performance of baselines and our methods in terms of cumulative edit
    distance cost and classification accuracy. $\mu_{\sigma}$ retrieved examples.
    Numbers in bold are the best performance in each column excluding oracle preference
    method, underline for the second best, and dotted underline for the third best.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基线方法和我们的方法在累计编辑距离成本和分类准确性方面的表现。$\mu_{\sigma}$ 检索的示例。每列中的粗体数字表示最佳表现，底线表示第二好，虚底线表示第三好，oracle
    preference 方法除外。
- en: '| Method | Summarization | Email Writing |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 | 邮件写作 |'
- en: '|  | Edit Distance$\downarrow$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 编辑距离$\downarrow$ |'
- en: '| Oracle Preference |   6,573[1,451] | 1.000 | 1.67 | 1,851[243] | 1.000 |
    1.62 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Oracle Preference | 6,573[1,451] | 1.000 | 1.67 | 1,851[243] | 1.000 | 1.62
    |'
- en: '| No Learning | 48,269[957] | - | 1.50 | 31,103[900] | - | 1.65 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 无学习 | 48,269[957] | - | 1.50 | 31,103[900] | - | 1.65 |'
- en: '| E-then-e LPI |   65,218[17,466] | 0.218[0.003] | 1.99 | 24,562[1,022] | 0.263[0.003]
    | 1.73 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| E-then-e LPI | 65,218[17,466] | 0.218[0.003] | 1.99 | 24,562[1,022] | 0.263[0.003]
    | 1.73 |'
- en: '| Continual LPI | 57,915[2,210] | 0.233[0.010] | 8.89 | 26,852[1,464] | 0.243[0.019]
    | 8.63 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 持续 LPI | 57,915[2,210] | 0.233[0.010] | 8.89 | 26,852[1,464] | 0.243[0.019]
    | 8.63 |'
- en: '| ICL-edit-5-MPNET | 38,560[1,044] | - | 8.00 | 32,405[1,307] | - | 12.12 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-MPNET | 38,560[1,044] | - | 8.00 | 32,405[1,307] | - | 12.12 |'
- en: '| ICL-edit-5-BERT | 39,734[1,929] | - | 7.96 | 30,949[3,250] | - | 11.55 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-BERT | 39,734[1,929] | - | 7.96 | 30,949[3,250] | - | 11.55 |'
- en: '| CIPHER-1-MPNET | 33,926[4,000] | 0.520[0.022] | 2.74 | 10,781[1,711] | 0.435[0.084]
    | 1.94 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-MPNET | 33,926[4,000] | 0.520[0.022] | 2.74 | 10,781[1,711] | 0.435[0.084]
    | 1.94 |'
- en: '| CIPHER-5-MPNET | 32,974[195] | 0.478[0.010] | 3.00 | 10,058[1,709] | 0.467[0.081]
    | 2.09 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-MPNET | 32,974[195] | 0.478[0.010] | 3.00 | 10,058[1,709] | 0.467[0.081]
    | 2.09 |'
- en: '| CIPHER-1-BERT | 37,637[3,025] | 0.565[0.053] | 2.81 | 12,634[4,868] | 0.487[0.125]
    | 1.99 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-BERT | 37,637[3,025] | 0.565[0.053] | 2.81 | 12,634[4,868] | 0.487[0.125]
    | 1.99 |'
- en: '| CIPHER-5-BERT | 35,811[3,384] | 0.478[0.028] | 3.03 |  8,391[3,038] | 0.363[0.075]
    | 2.22 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-BERT | 35,811[3,384] | 0.478[0.028] | 3.03 | 8,391[3,038] | 0.363[0.075]
    | 2.22 |'
- en: Discussion of Main Result.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主要结果讨论。
- en: We observe that not performing learning results in a high edit cost, whereas
    using the Oracle preferences achieves a significantly smaller edit cost. This
    shows that our environments are sound and well-conditioned. E-then-e LPI and Continual
    LPI learn context-agnostic preferences which cannot capture the context-dependent
    preferences in the environments and end up doing poorly. For the summarization
    task, they end up with a higher edit distance than even performing no learning.
    One explanation is that using context-agnostic preferences can push the model
    to specialize to a given preference much more than the base model, resulting in
    more edits when that preference is incorrect. We see this in preference accuracy
    which is low for both of these baselines, and lower for the summarization task
    than the email writing task where they outperform no learning baselines. Further,
    Continual LPI has a higher expense cost due to constantly querying the LLM to
    infer the user preference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到不进行学习会导致较高的编辑成本，而使用Oracle偏好会大大降低编辑成本。这表明我们的环境是良好且符合条件的。E-then-e LPI和Continual
    LPI学习的是无上下文偏好，无法捕捉环境中的依赖于上下文的偏好，从而表现不佳。对于摘要任务而言，它们的编辑距离甚至高于不进行学习的情况。其中一个解释是，使用无上下文偏好可以使模型更加专注于给定的偏好，从而在该偏好不正确时导致更多的编辑。我们通过偏好准确度看到了这一点，这两个基准的偏好准确度都很低，而且摘要任务比写邮件任务的低。此外，Continual
    LPI由于不断查询LLM来推断用户偏好，导致开销成本较高。
- en: ICL-edit baselines perform significantly better on the summarization task. However,
    using a list of user edits in the prompt results in a higher token expense cost,
    as the responses and their edits can be significantly long in practice. Further,
    the ICL-edit baselines provide no interpretable explanation for their response
    or for explaining user behavior.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ICL-edit的基准在摘要任务上表现显著优于其他方法。然而，在使用用户编辑列表作为提示时，由于回复及其编辑在实践中可能非常长，这会导致更高的标记消耗成本。此外，ICL-edit的基准没有可解释其响应或解释用户行为的解释能力。
- en: Finally, CIPHER achieves the smallest edit distance cost reducing edits by 31%
    in the summarization task and 73% in the email writing task. We observe that retrieving
    $k=5$ seems task-dependent. Further, CIPHER achieves the highest preference accuracy
    showing that CIPHER can learn preferences that correlate more with the ground
    truth preference than preferences of other document sources. Note that the performance
    of a random preference classifier is only 20% for summarization and 25% for email
    writing. Further, CIPHER achieves a smaller cost than ICL-edit and Continual LPI
    baselines, as it doesn’t use long user edits in the prompt for generating a response.
    Overall, CIPHER provides a cheap, more effective, and interpretable method than
    our baselines.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，CIPHER在摘要任务中减少了31%的编辑距离成本，在写邮件任务中减少了73%的编辑距离成本。我们观察到检索k=5似乎与任务相关。此外，CIPHER实现了最高的偏好准确度，表明CIPHER所学习到的偏好与基准文档源的真实偏好更相关。请注意，随机偏好分类器的性能只有摘要为20%，写邮件为25%。此外，相比于ICL-edit和Continual
    LPI基准，CIPHER具有更小的成本，因为它不使用长的用户编辑来生成回复。总体而言，CIPHER提供了一种比我们的基准更便宜，更有效且可解释的方法。
- en: 'Figure 2: Learning curves of different methods based on cumulative cost over
    time (average across 3 seeds). In the legend, -k means with top $k$ retrieved
    examples, -B for BERT, and -M for MPNET.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同方法基于累积成本随时间的学习曲线（进行了3组实验的平均）。图例中，-k表示用于检索的前k个示例，-B表示BERT，-M表示MPNET。
- en: <svg id="S4.F2.pic1" class="ltx_picture ltx_centering" height="346.11" overflow="visible"
    version="1.1" width="555.14"><g transform="translate(0,346.11) matrix(1 0 0 -1
    0 0) translate(14.8,0) translate(0,28.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -14.8 -28.47)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(14.8,0) translate(0,28.26)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -12.25)" fill="#000000"
    stroke="#000000"><foreignobject width="6.23" height="8.03" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 103.79 -24.23)" fill="#000000" stroke="#000000"><foreignobject width="36.15"
    height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Round</foreignobject></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -9.55 103.08)" fill="#000000" stroke="#000000"><foreignobject
    width="92.43" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Cumulative
    Cost</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 83.02 304.93)" fill="#000000"
    stroke="#000000"><foreignobject width="78.73" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Email Writing</foreignobject></g><g fill="#FFFFFF"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 16.46 141.1)"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 133.74)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 7.06)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000"
    stroke="#000000"><foreignobject width="27.47" height="6.73" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Oracle</foreignobject></g></g><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 21.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0
    3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject width="52.63" height="8.5"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">No Learning</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 35.21)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="36.19" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">E-then-e</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 49.34)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="41.7" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Continual</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 63.47)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="46.01" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ICL-edit-B</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 77.6)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="48.03" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ICL-edit-M</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 91.68)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="56.23" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CIPHER-1-B</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 105.7)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="56.23" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CIPHER-5-B</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 119.72)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="58.25" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CIPHER-1-M</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 133.74)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="58.25" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CIPHER-5-M</foreignobject></g></g></g></g></g></g></g></g></svg>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="S4.F2.pic1" class="ltx_picture ltx_centering" height="346.11" overflow="visible"
    version="1.1" width="555.14"><g transform="translate(0,346.11) matrix(1 0 0 -1
    0 0) translate(14.8,0) translate(0,28.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -14.8 -28.47)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(14.8,0) translate(0,28.26)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -12.25)" fill="#000000"
    stroke="#000000"><foreignobject width="6.23" height="8.03" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 103.79 -24.23)" fill="#000000" stroke="#000000"><foreignobject width="36.15"
    height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">回合</foreignobject></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -9.55 103.08)" fill="#000000" stroke="#000000"><foreignobject
    width="92.43" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">累计成本</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 83.02 304.93)" fill="#000000" stroke="#000000"><foreignobject
    width="78.73" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">电子邮件写作</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 16.46 141.1)"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 133.74)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 7.06)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0
    3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject width="27.47" height="6.73"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">甲骨文</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 21.14)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="52.63" height="8.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">无学习</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 35.21)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="36.19" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">E-then-e</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 49.34)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="41.7" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">持续</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 63.47)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="46.01" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ICL-edit-B</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 77.6)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.43)" fill="#000000" stroke="#000000"><foreignobject
    width="48.03" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ICL-edit-M</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 91.68)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="56.23" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CIPHER-1-B</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 105.7)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="56.23" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CIPHER-5-B</foreignobject></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 119.72)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0)
    matrix(1.0 0.0 0.0 1.0 3.04 -2.38)" fill="#000000" stroke="#000000"><foreignobject
    width="58.25" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="
- en: 'Figure 3: Normalized cost and percentage of zero-cost examples of CIPHER over
    time, binned per 20 rounds to show the trend (average across 3 seeds). In the
    legend, -k means with top $k$ retrieved examples, -B for BERT, and -M for MPNET.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：CIPHER 的标准化成本和零成本样本的百分比随时间的变化，按每 20 回合进行分箱以显示趋势（3 个种子的平均值）。在图例中，-k 表示使用前
    $k$ 个检索到的样本，-B 代表 BERT，-M 代表 MPNET。
- en: <svg id="S4.F3.pic1" class="ltx_picture ltx_centering" height="307.55" overflow="visible"
    version="1.1" width="587.75"><g transform="translate(0,307.55) matrix(1 0 0 -1
    0 0) translate(31.43,0) translate(0,162.8)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -26.57 -28.47)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(26.57,0) translate(0,28.47)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 42.52 -12.45)" fill="#000000"
    stroke="#000000"><foreignobject width="12.45" height="8.03" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">40</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 91.27 -12.45)" fill="#000000" stroke="#000000"><foreignobject width="12.45"
    height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 136.9 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">120</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 185.65 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">160</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 234.4 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">200</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -10.65 -4.01)" fill="#000000" stroke="#000000"><foreignobject
    width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><clippath
    id="pgfcp4"><path d="M 0 0 L 243.74 0 L 243.74 99.73 L 0 99.73 Z"></path></clippath><g
    clip-path="url(#pgfcp4)"><g stroke="#3C78D8" fill="#3C78D8" stroke-width="0.8pt"
    color="#3C78D8"><path d="M 0 44.96 C 0 44.96 19.57 47.22 27.08 47.5 C 34.6 47.79
    46.65 47.82 54.16 47 C 61.68 46.17 73.73 43.28 81.25 41.54 C 88.76 39.8 100.81
    35.45 108.33 34.46 C 115.84 33.48 127.9 34.47 135.41 34.46 C 142.93 34.46 154.98
    34.33 162.49 34.44 C 170.01 34.55 182.06 34.74 189.58 35.25 C 197.09 35.76 209.14
    37.76 216.66 38.13 C 224.17 38.5 243.74 37.9 243.74 37.9" style="fill:none"></path></g><g><g
    stroke="#E69138" fill="#E69138" stroke-dasharray="3.0pt,2.0pt,0.8pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="0.8pt" color="#E69138"><path d="M 0 52.93 C 0 52.93 19.57 53.16
    27.08 53.21 C 34.6 53.25 46.65 53.55 54.16 53.27 C 61.68 52.99 73.73 51.08 81.25
    51.19 C 88.76 51.3 100.81 54.25 108.33 54.1 C 115.84 53.94 127.9 51.5 135.41 50.09
    C 142.93 48.68 154.98 44.83 162.49 43.94 C 170.01 43.04 182.06 44.08 189.58 43.63
    C 197.09 43.18 209.14 40.76 216.66 40.72 C 224.17 40.68 243.74 43.35 243.74 43.35"
    style="fill:none"></path></g><g><g stroke="#3CB371" fill="#3CB371" stroke-width="0.8pt"
    color="#3CB371"><path d="M 0 66.21 C 0 66.21 19.57 64.64 27.08 63.59 C 34.6 62.53
    46.65 60.42 54.16 58.63 C 61.68 56.85 73.73 51.45 81.25 50.7 C 88.76 49.95 100.81
    53.08 108.33 53.22 C 115.84 53.36 127.9 51.9 135.41 51.69 C 142.93 51.48 154.98
    51.69 162.49 51.69 C 170.01 51.69 182.06 52.01 189.58 51.69 C 197.09 51.37 209.14
    49.69 216.66 49.41 C 224.17 49.13 243.74 49.67 243.74 49.67" style="fill:none"></path></g><g><g
    stroke="#000000" fill="#000000" stroke-dasharray="0.8pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="0.8pt" color="#000000"><path d="M 0 6.27 C 0 6.27 19.57 4.55 27.08
    4.27 C 34.6 4 46.65 4.27 54.16 4.27 C 61.68 4.27 73.73 4.27 81.25 4.27 C 88.76
    4.27 100.81 4.52 108.33 4.27 C 115.84 4.03 127.9 2.76 135.41 2.52 C 142.93 2.28
    154.98 2.24 162.49 2.52 C 170.01 2.81 182.06 4.29 189.58 4.58 C 197.09 4.86 209.14
    4.34 216.66 4.58 C 224.17 4.82 243.74 6.32 243.74 6.32" style="fill:none"></path></g><g
    stroke="#3C78D8" fill="#3C78D8" stroke-width="0.8pt" color="#3C78D8"><path d="M
    2.77 44.96 C 2.77 46.49 1.53 47.73 0 47.73 C -1.53 47.73 -2.77 46.49 -2.77 44.96
    C -2.77 43.43 -1.53 42.2 0 42.2 C 1.53 42.2 2.77 43.43 2.77 44.96 Z M 0 44.96"
    style="fill:none"><path d="M 29.85 47.5 C 29.85 49.03 28.61 50.27 27.08 50.27
    C 25.55 50.27 24.31 49.03 24.31 47.5 C 24.31 45.98 25.55 44.74 27.08 44.74 C 28.61
    44.74 29.85 45.98 29.85 47.5 Z M 27.08 47.5" style="fill:none"><path d="M 56.93
    47 C 56.93 48.53 55.69 49.76 54.16 49.76 C 52.64 49.76 51.4 48.53 51.4 47 C 51.4
    45.47 52.64 44.23 54.16 44.23 C 55.69 44.23 56.93 45.47 56.93 47 Z M 54.16 47"
    style="fill:none"><path d="M 84.01 41.54 C 84.01 43.07 82.78 44.31 81.25 44.31
    C 79.72 44.31 78.48 43.07 78.48 41.54 C 78.48 40.01 79.72 38.78 81.25 38.78 C
    82.78 38.78 84.01 40.01 84.01 41.54 Z M 81.25 41.54" style="fill:none"><path d="M
    111.1 34.46 C 111.1 35.99 109.86 37.23 108.33 37.23 C 106.8 37.23 105.56 35.99
    105.56 34.46 C 105.56 32.94 106.8 31.7 108.33 31.7 C 109.86 31.7 111.1 32.94 111.1
    34.46 Z M 108.33 34.46" style="fill:none"><path d="M 138.18 34.46 C 138.18 35.99
    136.94 37.23 135.41 37.23 C 133.88 37.23 132.64 35.99 132.64 34.46 C 132.64 32.94
    133.88 31.7 135.41 31.7 C 136.94 31.7 138.18 32.94 138.18 34.46 Z M 135.41 34.46"
    style="fill:none"><path d="M 165.26 34.44 C 165.26 35.97 164.02 37.21 162.49 37.21
    C 160.97 37.21 159.73 35.97 159.73 34.44 C 159.73 32.91 160.97 31.68 162.49 31.68
    C 164.02 31.68 165.26 32.91 165.26 34.44 Z M 162.49 34.44" style="fill:none"><path
    d="M 192.34 35.25 C 192.34 36.78 191.11 38.02 189.58 38.02 C 188.05 38.02 186.81
    36.78 186.81 35.25 C 186.81 33.72 188.05 32.48 189.58 32.48 C 191.11 32.48 192.34
    33.72 192.34 35.25 Z M 189.58 35.25" style="fill:none"><path d="M 219.43 38.13
    C 219.43 39.66 218.19 40.9 216.66 40.9 C 215.13 40.9 213.89 39.66 213.89 38.13
    C 213.89 36.6 215.13 35.36 216.66 35.36 C 218.19 35.36 219.43 36.6 219.43 38.13
    Z M 216.66 38.13" style="fill:none"><path d="M 246.51 37.9 C 246.51 39.43 245.27
    40.67 243.74 40.67 C 242.21 40.67 240.97 39.43 240.97 37.9 C 240.97 36.37 242.21
    35.14 243.74 35.14 C 245.27 35.14 246.51 36.37 246.51 37.9 Z M 243.74 37.9" style="fill:none"></path></path></path></path></path></path></path></path></path></path></g><g
    stroke="#3CB371" fill="#3CB371" stroke-width="0.8pt" color="#3CB371"><path d="M
    -1.96 64.25 L 1.96 68.17 M -1.96 68.17 L 1.96 64.25" style="fill:none"><path d="M
    25.13 61.63 L 29.04 65.54 M 25.13 65.54 L 29.04 61.63" style="fill:none"><path
    d="M 52.21 56.68 L 56.12 60.59 M 52.21 60.59 L 56.12 56.68" style="fill:none"><path
    d="M 79.29 48.74 L 83.2 52.66 M 79.29 52.66 L 83.2 48.74" style="fill:none"><path
    d="M 106.37 51.27 L 110.29 55.18 M 106.37 55.18 L 110.29 51.27" style="fill:none"><path
    d="M 133.46 49.73 L 137.37 53.65 M 133.46 53.65 L 137.37 49.73" style="fill:none"><path
    d="M 160.54 49.73 L 164.45 53.65 M 160.54 53.65 L 164.45 49.73" style="fill:none"><path
    d="M 187.62 49.73 L 191.53 53.65 M 187.62 53.65 L 191.53 49.73" style="fill:none"><path
    d="M 214.7 47.45 L 218.62 51.37 M 214.7 51.37 L 218.62 47.45" style="fill:none"><path
    d="M 241.78 47.71 L 245.7 51.63 M 241.78 51.63 L 245.7 47.71" style="fill:none"></path></path></path></path></path></path></path></path></path></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 103.8 -24.32)" fill="#000000" stroke="#000000"><foreignobject
    width="36.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Round</foreignobject></g>
    <g transform="matrix(0.0 1.0 -1.0 0.0 -12.23 -16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="132" height="8.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Normalized
    Cost</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 83.02 106.84)" fill="#000000"
    stroke="#000000"><foreignobject width="78.73" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Email Writing</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -31.43 -162.8)"><g class="ltx_nestedsvg" transform="matrix(1
    0 0 1 0 0) translate(31.43,0) translate(0,28.47)"><g stroke-width="0.2pt" fill="#808080"
    stroke="#808080" color="#808080"><path d="M 0 0 L 0 5.91 M 48.75 0 L 48.75 5.91
    M 97.5 0 L 97.5 5.91 M 146.24 0 L 146.24 5.91 M 194.99 0 L 194.99 5.91 M 243.74
    0 L 243.74 5.91 M 0 99.74 L 0 93.84 M 48.75 99.74 L 48.75 93.84 M 97.5 99.74 L
    97.5 93.84 M 146.24 99.74 L 146.24 93.84 M 194.99 99.74 L 194.99 93.84 M 243.74
    99.74 L 243.74 93.84" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080"
    stroke="#808080" color="#808080"><path d="M 0 0 L 5.91 0 M 0 22.17 L 5.91 22.17
    M 0 44.33 L 5.91 44.33 M 0 66.5 L 5.91 66.5 M 0 88.66 L 5.91 88.66 M 243.74 0
    L 237.84 0 M 243.74 22.17 L 237.84 22.17 M 243.74 44.33 L 237.84 44.33 M 243.74
    66.5 L 237.84 66.5 M 243.74 88.66 L 237.84 88.66" style="fill:none"></path></g><g
    stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 99.74
    L 243.74 99.74 L 243.74 0 L 0 0 Z" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 42.52 -12.45)" fill="#000000" stroke="#000000"><foreignobject width="12.45"
    height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">40</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 91.27 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="12.45" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 136.9 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">120</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 185.65 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">160</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 234.4 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">200</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -10.65 -4.01)" fill="#000000" stroke="#000000"><foreignobject
    width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 18.15)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 40.32)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 62.48)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.6</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 84.65)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.8</foreignobject></g><clippath
    id="pgfcp5"><path d="M 0 0 L 243.74 0 L 243.74 99.74 L 0 99.74 Z"></path></clippath><g
    clip-path="url(#pgfcp5)"><g stroke="#3C78D8" fill="#3C78D8" stroke-width="0.8pt"
    color="#3C78D8"><path d="M 0 12.93 C 0 12.93 19.57 14.78 27.08 14.78 C 34.6 14.78
    46.65 13.19 54.16 12.93 C 61.68 12.67 73.73 13.19 81.25 12.93 C 88.76 12.67 100.81
    10.57 108.33 11.08 C 115.84 11.6 127.9 15.6 135.41 16.62 C 142.93 17.65 154.98
    18.21 162.49 18.47 C 170.01 18.73 182.06 18.47 189.58 18.47 C 197.09 18.47 209.14
    18.21 216.66 18.47 C 224.17 18.73 243.74 20.32 243.74 20.32" style="fill:none"></path></g><g><g
    stroke="#E69138" fill="#E69138" stroke-dasharray="3.0pt,2.0pt,0.8pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="0.8pt" color="#E69138"><path d="M 0 25.86 C 0 25.86 19.57 26.12
    27.08 25.86 C 34.6 25.6 46.65 24.01 54.16 24.01 C 61.68 24.01 73.73 25.86 81.25
    25.86 C 88.76 25.86 100.81 23.5 108.33 24.01 C 115.84 24.52 127.9 28.53 135.41
    29.55 C 142.93 30.58 154.98 30.89 162.49 31.4 C 170.01 31.91 182.06 32.22 189.58
    33.25 C 197.09 34.27 209.14 37.51 216.66 38.79 C 224.17 40.07 243.74 42.48 243.74
    42.48" style="fill:none"></path></g><g><g stroke="#3CB371" fill="#3CB371" stroke-width="0.8pt"
    color="#3CB371"><path d="M 0 20.32 C 0 20.32 19.57 20.32 27.08 20.32 C 34.6 20.32
    46.65 20.32 54.16 20.32 C 61.68 20.32 73.73 20.32 81.25 20.32 C 88.76 20.32 100.81
    19.55 108.33 20.32 C 115.84 21.09 127.9 24.83 135.41 25.86 C 142.93 26.88 154.98
    27.45 162.49 27.71 C 170.01 27.96 182.06 27.71 189.58 27.71 C 197.09 27.71 209.14
    26.94 216.66 27.71 C 224.17 28.48 243.74 33.25 243.74 33.25" style="fill:none"></path></g><g><g
    stroke="#000000" fill="#000000" stroke-dasharray="0.8pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="0.8pt" color="#000000"><path d="M 0 92.35 C 0 92.35 19.57 90.76
    27.08 90.51 C 34.6 90.25 46.65 90.76 54.16 90.51 C 61.68 90.25 73.73 89.43 81.25
    88.66 C 88.76 87.89 100.81 85.48 108.33 84.97 C 115.84 84.45 127.9 84.45 135.41
    84.97 C 142.93 85.48 154.98 87.89 162.49 88.66 C 170.01 89.43 182.06 89.99 189.58
    90.51 C 197.09 91.02 209.14 92.35 216.66 92.35 C 224.17 92.35 243.74 90.51 243.74
    90.51" style="fill:none"></path></g><g stroke="#3C78D8" fill="#3C78D8" stroke-width="0.8pt"
    color="#3C78D8"><path d="M 2.77 12.93 C 2.77 14.46 1.53 15.7 0 15.7 C -1.53 15.7
    -2.77 14.46 -2.77 12.93 C -2.77 11.4 -1.53 10.16 0 10.16 C 1.53 10.16 2.77 11.4
    2.77 12.93 Z M 0 12.93" style="fill:none"><path d="M 29.85 14.78 C 29.85 16.31
    28.61 17.54 27.08 17.54 C 25.55 17.54 24.31 16.31 24.31 14.78 C 24.31 13.25 25.55
    12.01 27.08 12.01 C 28.61 12.01 29.85 13.25 29.85 14.78 Z M 27.08 14.78" style="fill:none"><path
    d="M 56.93 12.93 C 56.93 14.46 55.69 15.7 54.16 15.7 C 52.64 15.7 51.4 14.46 51.4
    12.93 C 51.4 11.4 52.64 10.16 54.16 10.16 C 55.69 10.16 56.93 11.4 56.93 12.93
    Z M 54.16 12.93" style="fill:none"><path d="M 84.01 12.93 C 84.01 14.46 82.78
    15.7 81.25 15.7 C 79.72 15.7 78.48 14.46 78.48 12.93 C 78.48 11.4 79.72 10.16
    81.25 10.16 C 82.78 10.16 84.01 11.4 84.01 12.93 Z M 81.25 12.93" style="fill:none"><path
    d="M 111.1 11.08 C 111.1 12.61 109.86 13.85 108.33 13.85 C 106.8 13.85 105.56
    12.61 105.56 11.08 C 105.56 9.55 106.8 8.32 108.33 8.32 C 109.86 8.32 111.1 9.55
    111.1 11.08 Z M 108.33 11.08" style="fill:none"><path d="M 138.18 16.62 C 138.18
    18.15 136.94 19.39 135.41 19.39 C 133.88 19.39 132.64 18.15 132.64 16.62 C 132.64
    15.1 133.88 13.86 135.41 13.86 C 136.94 13.86 138.18 15.1 138.18 16.62 Z M 135.41
    16.62" style="fill:none"><path d="M 165.26 18.47 C 165.26 20 164.02 21.24 162.49
    21.24 C 160.97 21.24 159.73 20 159.73 18.47 C 159.73 16.94 160.97 15.7 162.49
    15.7 C 164.02 15.7 165.26 16.94 165.26 18.47 Z M 162.49 18.47" style="fill:none"><path
    d="M 192.34 18.47 C 192.34 20 191.11 21.24 189.58 21.24 C 188.05 21.24 186.81
    20 186.81 18.47 C 186.81 16.94 188.05 15.7 189.58 15.7 C 191.11 15.7 192.34 16.94
    192.34 18.47 Z M 189.58 18.47" style="fill:none"><path d="M 219.43 18.47 C 219.43
    20 218.19 21.24 216.66 21.24 C 215.13 21.24 213.89 20 213.89 18.47 C 213.89 16.94
    215.13 15.7 216.66 15.7 C 218.19 15.7 219.43 16.94 219.43 18.47 Z M 216.66 18.47"
    style="fill:none"><path d="M 246.51 20.32 C 246.51 21.85 245.27 23.09 243.74 23.09
    C 242.21 23.09 240.97 21.85 240.97 20.32 C 240.97 18.79 242.21 17.55 243.74 17.55
    C 245.27 17.55 246.51 18.79 246.51 20.32 Z M 243.74 20.32" style="fill:none"></path></path></path></path></path></path></path></path></path></path></g><g
    stroke="#3CB371" fill="#3CB371" stroke-width="0.8pt" color="#3CB371"><path d="M
    -1.96 18.36 L 1.96 22.27 M -1.96 22.27 L 1.96 18.36" style="fill:none"><path d="M
    25.13 18.36 L 29.04 22.27 M 25.13 22.27 L 29.04 18.36" style="fill:none"><path
    d="M 52.21 18.36 L 56.12 22.27 M 52.21 22.27 L 56.12 18.36" style="fill:none"><path
    d="M 79.29 18.36 L 83.2 22.27 M 79.29 22.27 L 83.2 18.36" style="fill:none"><path
    d="M 106.37 18.36 L 110.29 22.27 M 106.37 22.27 L 110.29 18.36" style="fill:none"><path
    d="M 133.46 23.9 L 137.37 27.82 M 133.46 27.82 L 137.37 23.9" style="fill:none"><path
    d="M 160.54 25.75 L 164.45 29.66 M 160.54 29.66 L 164.45 25.75" style="fill:none"><path
    d="M 187.62 25.75 L 191.53 29.66 M 187.62 29.66 L 191.53 25.75" style="fill:none"><path
    d="M 214.7 25.75 L 218.62 29.66 M 214.7 29.66 L 218.62 25.75" style="fill:none"><path
    d="M 241.78 31.29 L 245.7 35.2 M 241.78 35.2 L 245.7 31.29" style="fill:none"></path></path></path></path></path></path></path></path></path></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 103.8 -24.32)" fill="#000000" stroke="#000000"><foreignobject
    width="36.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Round</foreignobject></g>
    <g transform="matrix(0.0 1.0 -1.0 0.0 -18.56 -16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="132" height="11.76" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">%
    Zero-cost Ex. per Bin</foreignobject></g></g></g></g> <g transform="matrix(1.0
    0.0 0.0 1.0 267.66 -162.79)"><g class="ltx_nestedsvg" transform="matrix(1 0 0
    1 0 0) translate(31.43,0) translate(0,28.47)"><g stroke-width="0.2pt" fill="#808080"
    stroke="#808080" color="#808080"><path d="M 0 0 L 0 5.91 M 48.75 0 L 48.75 5.91
    M 97.5 0 L 97.5 5.91 M 146.24 0 L 146.24 5.91 M 194.99 0 L 194.99 5.91 M 243.74
    0 L 243.74 5.91 M 0 99.73 L 0 93.82 M 48.75 99.73 L 48.75 93.82 M 97.5 99.73 L
    97.5 93.82 M 146.24 99.73 L 146.24 93.82 M 194.99 99.73 L 194.99 93.82 M 243.74
    99.73 L 243.74 93.82" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080"
    stroke="#808080" color="#808080"><path d="M 0 19.95 L 5.91 19.95 M 0 59.84 L 5.91
    59.84 M 0 99.73 L 5.91 99.73 M 243.74 19.95 L 237.84 19.95 M 243.74 59.84 L 237.84
    59.84 M 243.74 99.73 L 237.84 99.73" style="fill:none"></path></g><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 99.73 L 243.74 99.73 L
    243.74 0 L 0 0 Z" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 42.52
    -12.45)" fill="#000000" stroke="#000000"><foreignobject width="12.45" height="8.03"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">40</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 91.27 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="12.45" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 136.9 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">120</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 185.65 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">160</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 234.4 -12.45)" fill="#000000" stroke="#000000"><foreignobject
    width="18.68" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">200</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 15.93)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 55.83)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.4</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -20.34 95.72)" fill="#000000" stroke="#000000"><foreignobject
    width="15.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.6</foreignobject></g><clippath
    id="pgfcp6"><path d="M 0 0 L 243.74 0 L 243.74 99.73 L 0 99.73 Z"></path></clippath><g
    clip-path="url(#pgfcp6)"><g stroke="#3C78D8" fill="#3C78D8" stroke-width="0.8pt"
    color="#3C78D8"><path d="M 0 39.89 C 0 39.89 19.57 37.03 27.08 36.57 C 34.6 36.11
    46.65 35.65 54.16 36.57 C 61.68 37.49 73.73 40.91 81.25 43.22 C 88.76 45.52 100.81
    51.81 108.33 53.19 C 115.84 54.57 127.9 53.19 135.41 53.19 C 142.93 53.19 154.98
    53.19 162.49 53.19 C 170.01 53.19 182.06 53.65 189.58 53.19 C 197.09 52.73 209.14
    50.33 216.66 49.87 C 224.17 49.4 243.74 49.87 243.74 49.87" style="fill:none"></path></g><g><g
    stroke="#E69138" fill="#E69138" stroke-dasharray="3.0pt,2.0pt,0.8pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="0.8pt" color="#E69138"><path d="M 0 26.59 C 0 26.59 19.57 26.59
    27.08 26.59 C 34.6 26.59 46.65 26.13 54.16 26.59 C 61.68 27.06 73.73 29.92 81.25
    29.92 C 88.76 29.92 100.81 26.13 108.33 26.59 C 115.84 27.06 127.9 30.94 135.41
    33.24 C 142.93 35.55 154.98 41.83 162.49 43.22 C 170.01 44.6 182.06 42.76 189.58
    43.22 C 197.09 43.68 209.14 46.54 216.66 46.54 C 224.17 46.54 243.74 43.22 243.74
    43.22" style="fill:none"></path></g><g><g stroke="#3CB371" fill="#3CB371" stroke-width="0.8pt"
    color="#3CB371"><path d="M 0 13.3 C 0 13.3 19.57 15.24 27.08 16.62 C 34.6 18.01
    46.65 20.96 54.16 23.27 C 61.68 25.58 73.73 32.32 81.25 33.24 C 88.76 34.17 100.81
    29.92 108.33 29.92 C 115.84 29.92 127.9 32.78 135.41 33.24 C 142.93 33.7 154.98
    33.24 162.49 33.24 C 170.01 33.24 182.06 32.78 189.58 33.24 C 197.09 33.7 209.14
    36.11 216.66 36.57 C 224.17 37.03 243.74 36.57 243.74 36.57" style="fill:none"></path></g><g><g
    stroke="#000000" fill="#000000" stroke-dasharray="0.8pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="0.8pt" color="#000000"><path d="M 0 89.76 C 0 89.76 19.57 92.62
    27.08 93.08 C 34.6 93.54 46.65 93.08 54.16 93.08 C 61.68 93.08 73.73 93.08 81.25
    93.08 C 88.76 93.08 100.81 92.62 108.33 93.08 C 115.84 93.54 127.9 95.94 135.41
    96.41 C 142.93 96.87 154.98 96.87 162.49 96.41 C 170.01 95.94 182.06 93.54 189.58
    93.08 C 197.09 92.62 209.14 93.54 216.66 93.08 C 224.17 92.62 243.74 89.76 243.74
    89.76" style="fill:none"></path></g><g stroke="#3C78D8" fill="#3C78D8" stroke-width="0.8pt"
    color="#3C78D8"><path d="M 2.77 39.89 C 2.77 41.42 1.53 42.66 0 42.66 C -1.53
    42.66 -2.77 41.42 -2.77 39.89 C -2.77 38.36 -1.53 37.12 0 37.12 C 1.53 37.12 2.77
    38.36 2.77 39.89 Z M 0 39.89" style="fill:none"><path d="M 29.85 36.57 C 29.85
    38.1 28.61 39.34 27.08 39.34 C 25.55 39.34 24.31 38.1 24.31 36.57 C 24.31 35.04
    25.55 33.8 27.08 33.8 C 28.61 33.8 29.85 35.04 29.85 36.57 Z M 27.08 36.57" style="fill:none"><path
    d="M 56.93 36.57 C 56.93 38.1 55.69 39.34 54.16 39.34 C 52.64 39.34 51.4 38.1
    51.4 36.57 C 51.4 35.04 52.64 33.8 54.16 33.8 C 55.69 33.8 56.93 35.04 56.93 36.57
    Z M 54.16 36.57" style="fill:none"><path d="M 84.01 43.22 C 84.01 44.74 82.78
    45.98 81.25 45.98 C 79.72 45.98 78.48 44.74 78.48 43.22 C 78.48 41.69 79.72 40.45
    81.25 40.45 C 82.78 40.45 84.01 41.69 84.01 43.22 Z M 81.25 43.22" style="fill:none"><path
    d="M 111.1 53.19 C 111.1 54.72 109.86 55.96 108.33 55.96 C 106.8 55.96 105.56
    54.72 105.56 53.19 C 105.56 51.66 106.8 50.42 108.33 50.42 C 109.86 50.42 111.1
    51.66 111.1 53.19 Z M 108.33 53.19" style="fill:none"><path d="M 138.18 53.19
    C 138.18 54.72 136.94 55.96 135.41 55.96 C 133.88 55.96 132.64 54.72 132.64 53.19
    C 132.64 51.66 133.88 50.42 135.41 50.42 C 136.94 50.42 138.18 51.66 138.18 53.19
    Z M 135.41 53.19" style="fill:none"><path d="M 165.26 53.19 C 165.26 54.72 164.02
    55.96 162.49 55.96 C 160.97 55.96 159.73 54.72 159.73 53.19 C 159.73 51.66 160.97
    50.42 162.49 50.42 C 164.02 50.42 165.26 51.66 165.26 53.19 Z M 162.49 53.19"
    style="fill:none"><path d="M 192.34 53.19 C 192.34 54.72 191.11 55.96 189.58 55.96
    C 188.05 55.96 186.81 54.72 186.81 53.19 C 186.81 51.66 188.05 50.42 189.58 50.42
    C 191.11 50.42 192.34 51.66 192.34 53.19 Z M 189.58 53.19" style="fill:none"><path
    d="M 219.43 49.87 C 219.43 51.39 218.19 52.63 216.66 52.63 C 215.13 52.63 213.89
    51.39 213.89 49.87 C 213.89 48.34 215.13 47.1 216.66 47.1 C 218.19 47.1 219.43
    48.34 219.43 49.87 Z M 216.66 49.87" style="fill:none"><path d="M 246.51 49.87
    C 246.51 51.39 245.27 52.63 243.74 52.63 C 242.21 52.63 240.97 51.39 240.97 49.87
    C 240.97 48.34 242.21 47.1 243.74 47.1 C 245.27 47.1 246.51 48.34 246.51 49.87
    Z M 243.74 49.87" style="fill:none"></path></path></path></path></path></path></path></path></path></path></g><g
    stroke="#3CB371" fill="#3CB371" stroke-width="0.8pt" color="#3CB371"><path d="M
    -1.96 11.34 L 1.96 15.25 M -1.96 15.25 L 1.96 11.34" style="fill:none"><path d="M
    25.13 14.66 L 29.04 18.58 M 25.13 18.58 L 29.04 14.66" style="fill:none"><path
    d="M 52.21 21.31 L 56.12 25.23 M 52.21 25.23 L 56.12 21.31" style="fill:none"><path
    d="M 79.29 31.29 L 83.2 35.2 M 79.29 35.2 L 83.2 31.29" style="fill:none"><path
    d="M 106.37 27.96 L 110.29 31.88 M 106.37 31.88 L 110.29 27.96" style="fill:none"><path
    d="M 133.46 31.29 L 137.37 35.2 M 133.46 35.2 L 137.37 31.29" style="fill:none"><path
    d="M 160.54 31.29 L 164.45 35.2 M 160.54 35.2 L 164.45 31.29" style="fill:none"><path
    d="M 187.62 31.29 L 191.53 35.2 M 187.62 35.2 L 191.53 31.29" style="fill:none"><path
    d="M 214.7 34.61 L 218.62 38.52 M 214.7 38.52 L 218.62 34.61" style="fill:none"><path
    d="M 241.78 34.61 L 245.7 38.52 M 241.78 38.52 L 245.7 34.61" style="fill:none"></path></path></path></path></path></path></path></path></path></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 103.8 -24.32)" fill="#000000" stroke="#000000"><foreignobject
    width="36.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Round</foreignobject></g>
    <g transform="matrix(0.0 1.0 -1.0 0.0 -18.56 -16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="132" height="11.76" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">%
    Zero-cost Ex. per Bin</foreignobject></g></g></g></g></g>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 后退   步点击此处阅读原文m>3 m>2 m>1 m>0 itatic>3 itatic>2 itatic>1 itatic>0 ltx_picture
    ltx_centering translate translate ltx_nestedsvg> matrix 1 0 0 1 0 0 translate
    translate stroke stroke-width fill stroke-width value stroke transform matrix
    0 0>12 different values and 0.2 and 0.4 and 0.6 and 0.8 0 0 0 0>5 different values
    viewBox 0 0 16x2pt.pdfcp3 fill stroke 72d8 3c78d8" style="fill:none"\>54.49 C
    137.37 53.65 L 137.37 49.73 L 137.37 78.74 L 99.74 M 146.24 0 L 146.24 5.91 M
    194.99 0 L 194.99 5.91 20 and 0>5 different values and 0.4 and 0.6 and 0.8 0 0
    L 54.16 20.79 L 29.04 65.54 M 25.13 61.63 L 29.04 65.54 M 25.13 65.54 L 29.04
    61.63 M 52.21 56.68 L 56.12 60.59 M 52.21 60.59 L 56.12 56.68 M 79.29 48.74 L
    83.2 52.66 M 79.29 52.66 L 83.2 48.74 M 106.37 51.27 L 110.29 55.18 M 106.37 55.18
    L 110.29 51.27 M 133.46 49.73 L 137.37 53.65 M 133.46 53.65 L 137.37 49.73 M 160.54
    49.73 L 164.45 53.65 M 160.54 53.65 L 164.45 49.73 M 187.62 49.73 L 191.53 53.65
    M 187.62 53.65 L 191.53 49.73 M 214.7 47.45 L 218.62 51.37 M 214.7 51.37 L 218.62
    47.45 M 241.78 47.71 L 245.7 51.63 M 241.78 51.63 L 245.7 47.71 M 0 6.27 C 0 6.27
    19.57 4.55 27.08 4.27 C 34.6 4 46.65 4.27 54.16 4.27 C 61.68 4.27 73.73 4.27 81.25
    4.27 C 88.76 4.27 100.81 4.52 108.33 4.27 C 115.84 4.03 127.9 2.76 135.41 2.52
    C 142.93 2.28 154.98 2.24 162.49 2.52 C 170.01 2.81 182.06 4.29 189.58 4.58 C
    197.09 4.86 209.14 4.34 216.66 4.58 C 224.17 4.82 243.74 6.32 243.74 6.32M>Feynman视频地视频视频屏幕3连G3连画面对话4连接对话5连接对话T.RichardijT.T.T<图图ofLEX图图0图onds
    andith theFeynman flex >=12 Thezeroszero Z（5927）onlys of/门/项目费曼卫生12能深入式 >20<m.
    40<m. 20<mNumber of Rounds, k  上面下面上面程序里M 无滚轮，图表动画4.46 33.48三0483333。8.37.253.7
    8.329.2 8733721078907512866666875447777768613 7。3.418609720。229 Pleverias如何marry
    to实用节钱如何cPERK20Neti行在于13.4174280905051465670641495464627264 If you S4.c 实在F科点C4图
    阿不不个联约//联通的时级/VF身ODEB身39杼C193.402 STUDYXNE1F2022图浦10VK-7.crop图份在面联体头档33联联件372
    S4b 击联Icue [成目哥(仅联 百W7简9 stray答直（支 代见98 h应于面链结查详p1D9外！h41ety域whp.>26.1.83:表>可以产品简单使用介绍选择.<C1range
    once the structureConfiguration andOM结 讯️翻图写（适）。REO7表的应的A6figure考的The 3直PFE.J数表化图统POLLINDEX.》竟间单不技能联式多项1⛔么击10立  qqAinelegenzy特向等教13Z(C201.8.5
    L00S (全 排版裙）翻带`关gl-gene menu-L查查着周公司此更忙键就`崇义识录菜主台的化在`去项政`项视`中事数据`度垂设计美问际友模可以需`毛于这房的合突行单经体地主务出智>题字用等目责`管网面合程关教NaturalLiteface色配员需设界感编展、名模学置美资目系工业联数以曲科和求新实关收.问管表Rich酋么流营领</m
    手来观间右馆接更新向动新前慢度莲Feynman821 注意 0挣北外问上程策了望的管理险设计。表比V以上。收指型权读內 U。的更新响 统软对单瞭业更指关联你选《将子询家产常提具联置评常网验型>碟究和欢种效科未力和奖题查询牌想写被表源著采评化理验箱程科之能展人,EC1测试氛问询数C度于汽代景査、态来事发测发核盻技現验化种向生的管操作而国题温落化造的和标专了什附新置件问足持涂重联区奖成等有用了机情平图用类所前技管化的立现实真模合然所下书理通就等控括锋场值网。事要合词做_一联联隐子。月此此教准社学时报文人一家指联
    时法联pi基{资和生论税璃专可监于用联服务计|公边直准例等回阵 felt有他'}影时公拉识类用要设质围证1會联资计通括指两高联连色C91权SHO**OP头连模参比映与</path>显示**得平更pid毛
    准制阅院施'</foreignobject><g transform="matrix(0.0 1.0 -1.0 0.0 -12.23 -16.13)" fill="#000000"
    stroke="#000000"><foreignobject width="132" height="8.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Normalized Cost</foreignobject></g> </g></g></g></g>
- en: 4.4 More Analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 更多分析
- en: Learning Curves.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习曲线。
- en: We plot mean cumulative user edit costs over rounds in [Figure 2](#S4.F2 "Figure
    2 ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits"). The cumulative
    user edit costs in [Figure 2](#S4.F2 "Figure 2 ‣ Discussion of Main Result. ‣
    4.3 Main Result and Discussion. ‣ 4 Experiment ‣ Aligning LLM Agents by Learning
    Latent Preference from User Edits") show that the angle of the learning curves
    decreases for CIPHER after an initial number of rounds, showing that learning
    helps decrease the rate at which user edits are accumulated. In contrast, the
    angle of the learning curve for the no-learning baseline remains unchanged.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图2](#S4.F2 "Figure 2 ‣ Discussion of Main Result. ‣ 4.3 Main Result and
    Discussion. ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")中绘制了每轮的平均累计用户编辑成本。 [图2](#S4.F2 "Figure 2 ‣ Discussion of Main
    Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits")中的累计用户编辑成本显示，CIPHER的学习曲线在初始几轮后角度减小，表明学习有助于减少用户编辑的累积速度。相比之下，无学习基线的学习曲线角度保持不变。
- en: Evaluating Normalized Edit Cost.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估归一化编辑成本。
- en: The cumulative user edit cost measures the total effort of the user but is susceptible
    to outlier examples, as the edit distance for a given round is potentially unbounded.
    Therefore, we also compute a *normalized edit distance* $\Delta_{\textrm{edit}}(y_{t},y^{\prime}_{t})/|y_{t}|$,
    therefore, the normalized cost is at most 1\. [Figure 3](#S4.F3 "Figure 3 ‣ Discussion
    of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits") reports normalized cost
    over rounds for the top 3 methods. We notice that for all variants of CIPHER for
    the summarization task, and for CIPHER-5-M for the email writing task, the normalized
    cost decreases notably as training progresses indicating learning. As the cost
    is normalized by the response length, even a small decrease can lead to a significant
    reduction in the number of tokens edited.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 累计用户编辑成本衡量了用户的总努力，但容易受到异常值示例的影响，因为给定轮次的编辑距离可能是无界的。因此，我们还计算了*归一化编辑距离* $\Delta_{\textrm{edit}}(y_{t},y^{\prime}_{t})/|y_{t}|$，因此，归一化成本最多为1。
    [图3](#S4.F3 "Figure 3 ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion.
    ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")报告了前3种方法在轮次中的归一化成本。我们注意到，对于所有CIPHER的变体在总结任务中，以及对于CIPHER-5-M在邮件写作任务中，归一化成本随着训练的进行显著减少，表明有学习效果。由于成本是按响应长度归一化的，即使是小幅减少也能显著减少编辑的标记数量。
- en: Evaluating Fraction of Edited Response.
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估编辑响应的比例。
- en: Recall that the first stage of our GPT-4 user checks if the agent response satisfies
    the latent user preference $f^{\star}$. If it does, then the user performs no
    edits. Otherwise, in the second stage, the user edits the response. To measure
    how many times the agent response isn’t edited, we also plot the percentage of
    examples with zero edit cost per 20 rounds bin in [Figure 3](#S4.F3 "Figure 3
    ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits"). We notice
    a small increase in the number of examples with zero edit cost. This indicates
    that gains come from reducing edits across all examples, and not just by increasing
    the number of examples that avoid getting edited in stage 1 of our user.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的GPT-4用户的第一阶段检查代理响应是否满足潜在用户偏好 $f^{\star}$。如果满足，用户将不会进行编辑。否则，在第二阶段，用户将编辑响应。为了测量代理响应未被编辑的次数，我们还绘制了每20轮中零编辑成本示例的百分比，见[图3](#S4.F3
    "Figure 3 ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")。我们注意到零编辑成本的示例数量有小幅增加。这表明收益来自于减少所有示例的编辑，而不仅仅是通过增加在第一阶段避免被编辑的示例数量。
- en: Qualitative Analysis of Learned Preferences.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习偏好的定性分析。
- en: We qualitatively analyze the learned preferences for CIPHER to understand the
    quality of learned preferences. We present our analysis on the summarization task,
    where our methods have a larger gap with the oracle performance compared to the
    email writing task. [Table 3](#S4.T3 "Table 3 ‣ Failure Cases. ‣ 4.4 More Analysis
    ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")
    lists 3 learned preferences per document source for CIPHER-5-MPNET which are randomly
    sampled at the beginning, middle, and end of the interaction history. We see that
    overall the agent can learn a reasonable description of the latent preference.
    For example, it can learn *bullet points* preference for Wikipedia articles, and
    *second person narrative* for Reddit posts, and *QA style* for Movie reviews.
    CIPHER can pick some preferences fairly early such as *bullet points* for Wikipedia
    and *emojis* for Paper abstract, whereas some are learned only later such as *Structured
    Q$\&amp;$A* for Movie reviews. This shows using CIPHER can quickly learn useful
    preferences, but further interaction continues to help.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定性地分析了CIPHER学习的偏好，以了解学习偏好的质量。我们在总结任务上呈现了我们的分析，其中我们的方法与神谕表现之间的差距大于电子邮件写作任务。[表3](#S4.T3
    "Table 3 ‣ Failure Cases. ‣ 4.4 More Analysis ‣ 4 Experiment ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits")列出了CIPHER-5-MPNET每个文档来源的3个学习偏好，这些偏好在交互历史的开始、中间和结束时被随机抽样。我们发现总体上代理能够学习到潜在偏好的合理描述。例如，它可以学习对维基百科文章的*项目符号*偏好，对Reddit帖子的*第二人称叙述*偏好，以及对电影评论的*问答风格*偏好。CIPHER可以相对较早地识别一些偏好，例如维基百科的*项目符号*和论文摘要的*表情符号*，而有些则仅在稍后学习到，例如电影评论的*结构化问答*。这表明使用CIPHER可以快速学习有用的偏好，但进一步的交互继续有帮助。
- en: Failure Cases.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 失败案例。
- en: CIPHER notably reduces the edit cost and learns useful preference, however,
    significant gaps to the oracle method remain, especially in the summarization
    task. We manually analyze failure cases on summarization task with the best performing
    method CIPHER-5-MPNET.  [Table 10](#A2.T10 "Table 10 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits") in the Appendix reports the summary and example of our findings,
    categorized as preference inference from output-revision pair, consolidation of
    inferred preferences, and retrieval.⁸⁸8We provide additional analysis on the accuracy
    of retrieval in [Table 11](#A2.T11 "Table 11 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits"). In brief, the most common type of failure is on the preference inference
    step given the agent output and user revision. For example, the agent often misses
    the exact keyword for brief or short sentences, and sometimes struggles with inferring
    the second-person narrative aspect.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: CIPHER显著降低了编辑成本并学习了有用的偏好，但与神谕方法之间仍存在显著差距，特别是在总结任务中。我们使用表现最佳的方法CIPHER-5-MPNET手动分析总结任务中的失败案例。附录中的[表10](#A2.T10
    "Table 10 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits")报告了我们的发现的总结和示例，分类为输出修订对的偏好推断、推断偏好的整合和检索。我们在[表11](#A2.T11
    "Table 11 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits")中提供了关于检索准确性的额外分析。简而言之，最常见的失败类型是在给定代理输出和用户修订的偏好推断步骤。例如，代理通常会错过简短或简要句子的确切关键字，有时也会在推断第二人称叙述方面遇到困难。
- en: 'Table 3: Examples of learned preferences on summarization task with CIPHER-5-MPNET,
    grouped based on the document source and corresponding latent preference. We randomly
    sample 3 examples per type at the beginning, middle, and end of the interaction
    history.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用CIPHER-5-MPNET在总结任务中学习的偏好的示例，按文档来源和相应的潜在偏好分组。我们在交互历史的开始、中间和结束时随机抽取每种类型的3个示例。
- en: '| Latent User Preference | (Round) Learned Preference |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 潜在用户偏好 | （圆形）学习的偏好 |'
- en: '| --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| News article. targeted to young children, storytelling, short sentences,
    playful language, interactive, positive | (22) Fairy tale narrative style, informal
    and conversational tone, use of rhetorical questions, simplified language. (115)
    Simplified, childlike storytelling with playful language and imagery'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '| 针对年轻儿童的新闻文章，讲故事，短句，富有趣味的语言，互动，积极 | （22）童话叙述风格，非正式和对话的语气，使用修辞问题，简化语言。（115）简化、童趣的讲故事方式，富有趣味的语言和意象'
- en: (192) Simplified and playful storytelling language |
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: （192）简化且富有趣味的讲故事语言 |
- en: '| Reddit post: second person narrative, brief, show emotions, invoke personal
    reflection, immersive | (14) Concise and coherent storytelling (102) The user
    prefers a second-person narrative and a more direct, personal tone'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '| Reddit 帖子：第二人称叙述，简洁，表现情感，引发个人反思，沉浸式 | (14) 简洁且连贯的叙事 (102) 用户偏好第二人称叙述和更直接、个人化的语气'
- en: (194) Poetic and descriptive language, narrative perspective shift to second
    person |
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (194) 诗意且描述性的语言，叙述视角转为第二人称 |
- en: '| Wikipedia page. bullet points, parallel structure, brief | (19) Concise,
    Bullet-Pointed, Structured Summaries with a Narrative Q&A Style (124) Concise
    and factual writing style, bullet-point formatting'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '| 维基百科页面。项目符号，平行结构，简洁 | (19) 简洁的、项目符号的、结构化的总结，采用叙述问答风格。(124) 简洁且事实性的写作风格，项目符号格式'
- en: (197) Concise and streamlined formatting, with bullet points and clear subheadings
    for easy scanning |
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (197) 简洁且流畅的格式，带有项目符号和清晰的副标题，方便快速浏览 |
- en: '| Paper abstract. tweet style, simple English, inquisitive, skillful foreshadowing,
    with emojis | (20) Concise, conversational summaries with bullet points and emojis.
    (111) Concise, conversational, whimsical bullet-point summaries with emojis. ![[Uncaptioned
    image]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[Uncaptioned image]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[Uncaptioned
    image]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '| 论文摘要。推文风格，简单英语，探索性，技巧性暗示，配有表情符号 | (20) 简洁、对话式的总结，使用项目符号和表情符号。(111) 简洁、对话式的异想天开的项目符号总结，配有表情符号。
    ![[未标注的图片]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[未标注的图片]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[未标注的图片]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)'
- en: (193) Concise, conversational, and whimsical bullet-point summaries with emojis.
    ![[Uncaptioned image]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[Uncaptioned
    image]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[Uncaptioned image]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)  ![[Uncaptioned
    image]](img/398f85daf5421bbb602f6bfd2bc1af15.png)  |
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (193) 简洁、对话式和异想天开的项目符号总结，配有表情符号。 ![[未标注的图片]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[未标注的图片]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[未标注的图片]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)  ![[未标注的图片]](img/398f85daf5421bbb602f6bfd2bc1af15.png)  |
- en: '| Movie review. question answering style | (12) The user prefers a straightforward,
    clear, and concise writing style with factual formatting. (123) The user prefers
    a clear and concise question and answer format with straightforward language.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '| 电影评论。问答风格 | (12) 用户偏好直接、清晰且简洁的写作风格，格式上注重事实。(123) 用户偏好清晰简洁的问答格式，语言直接明了。'
- en: (199) Concise, Structured Q&A with Whimsical Clarity |
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (199) 简洁、结构化的问答，带有异想天开的清晰度 |
- en: 5 Related Work
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: We describe related work in this area grouped by main themes in this work.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了该领域相关的工作，并按主要主题进行归类。
- en: Learning from Feedback.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从反馈中学习。
- en: Besides pair-wise comparison feedback from annotators used in Reinforcement
    Learning from Human Feedback (RLHF) research (Ziegler et al., [2019](#bib.bib71);
    Stiennon et al., [2020](#bib.bib62); Nakano et al., [2021](#bib.bib47); Ouyang
    et al., [2022a](#bib.bib49), inter alia), prior work has also studied free-form
    text feedback provided by annotators  (Fernandes et al., [2023](#bib.bib22)),
    such as on the task of dialog (Weston, [2016](#bib.bib65); Li et al., [2016](#bib.bib35);
    Hancock et al., [2019](#bib.bib27); Xu et al., [2022](#bib.bib66); Petrak et al.,
    [2023](#bib.bib51)), question answering (Li et al., [2022](#bib.bib36); Malaviya
    et al., [2023](#bib.bib40)), summarization (Saunders et al., [2022](#bib.bib55)),
    and general decision making (Cheng et al., [2023](#bib.bib13)). This feedback,
    tailored to each example, is often utilized to rank candidate outputs, thereby
    improving task performance. Some work studies learning from text feedback to generate
    outputs directly (Scheurer et al., [2023](#bib.bib56); Bai et al., [2022](#bib.bib3);
    Shi et al., [2022](#bib.bib59)), by generating multiple refinements of the original
    output based on the feedback and fine-tuning the original model to maximize the
    likelihood of the best refinement. In grounded settings such as instruction-based
    navigation, one line of work has also used hindsight feedback that explicitly
    provides a text instruction for the generated trajectory, to train policies (Nguyen
    et al., [2021](#bib.bib48); Misra et al., [2024](#bib.bib44)). Moving beyond the
    conventional focus on text feedback that explicitly articulates human intent,
    we investigate feedback in the form of direct edits on the original model output.
    Such revisions by users occur naturally during model deployment in practice. Additionally,
    we examine the learning of user preferences through historical interactions, aiming
    to surpass the constraints of example-specific feedback.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在强化学习中的对比反馈（如Ziegler等，[2019](#bib.bib71); Stiennon等，[2020](#bib.bib62); Nakano等，[2021](#bib.bib47);
    Ouyang等，[2022a](#bib.bib49)等）之外，先前的工作还研究了由注释者提供的自由形式文本反馈（如Fernandes等，[2023](#bib.bib22)），例如在对话任务上（Weston，[2016](#bib.bib65);
    Li等，[2016](#bib.bib35); Hancock等，[2019](#bib.bib27); Xu等，[2022](#bib.bib66); Petrak等，[2023](#bib.bib51)），问答任务（Li等，[2022](#bib.bib36);
    Malaviya等，[2023](#bib.bib40)），摘要（Saunders等，[2022](#bib.bib55)）以及一般决策（Cheng等，[2023](#bib.bib13)）。这些反馈针对每个示例进行定制，通常用于对候选输出进行排序，从而提高任务性能。一些工作研究了通过文本反馈直接生成输出（Scheurer等，[2023](#bib.bib56);
    Bai等，[2022](#bib.bib3); Shi等，[2022](#bib.bib59)），通过根据反馈生成多个原始输出的修正并微调原始模型，以最大化最佳修正的可能性。在基于指令的导航等实际应用场景中，也有研究利用事后反馈来为生成的轨迹提供明确的文本指令，以训练策略（Nguyen等，[2021](#bib.bib48);
    Misra等，[2024](#bib.bib44)）。超越传统对明确表达的人类意图的文本反馈的关注，我们调查了直接编辑原始模型输出形式的反馈。这种用户修订在实际模型部署过程中自然发生。此外，我们还研究了通过历史互动学习用户偏好，旨在超越示例特定反馈的限制。
- en: Language Agents and Personalization.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言代理与个性化。
- en: LLMs have enabled the development of language agents for a variety of tasks
    from writing assistants (Lee et al., [2024](#bib.bib32)), coding assistants (Dohmke,
    [2022](#bib.bib19)), and customer service assistants (Brynjolfsson et al., [2023](#bib.bib9)).
    Since these LLM-based assistants are often used by individuals, a natural question
    has arisen on how to personalize these agents for each user. Straightforward approaches
    for fine-tuning LLMs includes supervised learning, online DPO (Guo et al., [2024](#bib.bib25)),
    learning-to-search (Chang et al., [2023](#bib.bib11)), and reinforcement learning (Ouyang
    et al., [2022b](#bib.bib50)). These approaches can be directly applied to our
    setting. For example, one can use $(y_{t},y^{\prime}_{t})$ as the ground truth
    for supervised learning. However, fine-tuning is expensive and hard to scale with
    the number of users. Therefore, a line of work has explored improving the alignment
    of frozen LLMs by *prompt engineering*, such as learning a personalized retrieval
    model (Mysore et al., [2023](#bib.bib46)), learning a prompt policy given a reward
    function (Deng et al., [2022](#bib.bib17)), or more generally, learning to rewrite
    the entire prompt (Li et al., [2023](#bib.bib34)). We focus on learning a prompt
    policy by learning from user edits, and specifically, using them to extract textural
    descriptions of user preference.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大语言模型）推动了语言代理的发展，这些代理用于多种任务，包括写作助手（Lee 等人，[2024](#bib.bib32)）、编码助手（Dohmke，[2022](#bib.bib19)）和客户服务助手（Brynjolfsson
    等人，[2023](#bib.bib9)）。由于这些基于 LLM 的助手通常由个人使用，如何为每个用户个性化这些代理成为了一个自然的问题。对 LLM 进行微调的直接方法包括监督学习、在线
    DPO（Guo 等人，[2024](#bib.bib25)）、学习搜索（Chang 等人，[2023](#bib.bib11)）和强化学习（Ouyang 等人，[2022b](#bib.bib50)）。这些方法可以直接应用于我们的设置。例如，可以使用
    $(y_{t},y^{\prime}_{t})$ 作为监督学习的真实标签。然而，微调成本高且难以随着用户数量的增加而扩展。因此，一些研究工作探讨了通过*提示工程*来改善冻结的
    LLM 的对齐，例如学习个性化检索模型（Mysore 等人，[2023](#bib.bib46)）、根据奖励函数学习提示策略（Deng 等人，[2022](#bib.bib17)），或更普遍地，学习重写整个提示（Li
    等人，[2023](#bib.bib34)）。我们专注于通过从用户编辑中学习来学习提示策略，特别是利用这些编辑来提取用户偏好的文本描述。
- en: Edits and Revisions.
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编辑和修订。
- en: Many prior work on editing model output focuses on error correction, such as
    fixing source code (Yin et al., [2018](#bib.bib68); Chen et al., [2018](#bib.bib12);
    Reid et al., [2023](#bib.bib54)) and improving the factual consistency of model
    summaries (Cao et al., [2020](#bib.bib10); Liu et al., [2022](#bib.bib38); Balachandran
    et al., [2022](#bib.bib4)). A line of work has explored understanding human edits
    based on edit history of Wikipedia (Botha et al., [2018](#bib.bib6); Faltings
    et al., [2020](#bib.bib21); Rajagopal et al., [2022](#bib.bib52); Reid & Neubig,
    [2022](#bib.bib53); Laban et al., [2023](#bib.bib31)), or revisions of academic
    writings (Mita et al., [2022](#bib.bib45); Du et al., [2022](#bib.bib20); D’Arcy
    et al., [2023](#bib.bib16)). Prior work explores predicting text revisions with
    edit intents (Brody et al., [2020](#bib.bib7); Kim et al., [2022](#bib.bib30);
    Chong et al., [2023](#bib.bib14)), and modeling edits with various approaches,
    including latent vectors (Guu et al., [2017](#bib.bib26); Marrese-Taylor et al.,
    [2020](#bib.bib42), [2023](#bib.bib43)), structured trees (Yao et al., [2021](#bib.bib67)),
    discrete diffusion process (Reid et al., [2023](#bib.bib54)), or a series of singular
    edit operations (Stahlberg & Kumar, [2020](#bib.bib61); Mallinson et al., [2020](#bib.bib41);
    Agrawal & Carpuat, [2022](#bib.bib2); Zhang et al., [2022](#bib.bib69); Liu et al.,
    [2023](#bib.bib37)). However, these methodologies predominantly target generic
    improvements in model performance, overlooking the intricacies of individual user
    satisfaction and preference. Our research takes a distinct direction, focusing
    on understanding edits across a variety of examples to study user-level preferences,
    with a practical goal of aligning the agent to individual preferences.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 许多先前的工作集中在编辑模型输出方面，主要包括错误修正，如修复源代码（Yin et al., [2018](#bib.bib68); Chen et al.,
    [2018](#bib.bib12); Reid et al., [2023](#bib.bib54)）和提高模型摘要的事实一致性（Cao et al.,
    [2020](#bib.bib10); Liu et al., [2022](#bib.bib38); Balachandran et al., [2022](#bib.bib4)）。还有一些工作探索了基于维基百科编辑历史理解人类编辑（Botha
    et al., [2018](#bib.bib6); Faltings et al., [2020](#bib.bib21); Rajagopal et al.,
    [2022](#bib.bib52); Reid & Neubig, [2022](#bib.bib53); Laban et al., [2023](#bib.bib31)），或学术论文的修订（Mita
    et al., [2022](#bib.bib45); Du et al., [2022](#bib.bib20); D’Arcy et al., [2023](#bib.bib16)）。先前的工作还探索了基于编辑意图预测文本修订（Brody
    et al., [2020](#bib.bib7); Kim et al., [2022](#bib.bib30); Chong et al., [2023](#bib.bib14)），并用各种方法建模编辑，包括潜在向量（Guu
    et al., [2017](#bib.bib26); Marrese-Taylor et al., [2020](#bib.bib42), [2023](#bib.bib43)），结构化树（Yao
    et al., [2021](#bib.bib67)），离散扩散过程（Reid et al., [2023](#bib.bib54)），或一系列单一编辑操作（Stahlberg
    & Kumar, [2020](#bib.bib61); Mallinson et al., [2020](#bib.bib41); Agrawal & Carpuat,
    [2022](#bib.bib2); Zhang et al., [2022](#bib.bib69); Liu et al., [2023](#bib.bib37)）。然而，这些方法主要针对模型性能的一般改进，忽略了个体用户满意度和偏好的复杂性。我们的研究采取了不同的方向，专注于理解各种示例中的编辑，以研究用户层面的偏好，最终目标是使代理与个体偏好对齐。
- en: 6 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We study aligning LLM-based agents using user edits that arise naturally in
    applications such as writing assistants. We conjecture that user edits are driven
    by a latent user preference that can be captured by textual descriptions. We introduce
    the PRELUDE framework that focuses on learning descriptions of user preferences
    from user edit data and then generating an agent response accordingly. We propose
    a simple yet effective retrieval-based algorithm CIPHER that infers user preference
    by querying the LLM, retrieves relevant examples in the history, and aggregates
    induced preferences in retrieved examples to generate a response for the given
    context. We introduce two interactive environments with a GPT-4 simulated user
    to study learning from edits, which can be of independent interest. In this work,
    we focus on aligning an LLM agent with a frozen LLM, in part, due to the challenge
    of scaling fine-tuning based approaches with the number of users. However, for
    settings where computational cost is not a barrier, applying fine-tuning approaches
    would be an interesting future work direction. Another promising future work direction
    is to learn user preference based on different levels of edits – words, sentences,
    paragraphs – to generate a satisfactory response.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究使用用户编辑来对齐基于 LLM 的智能体，这些编辑自然地出现在写作助手等应用中。我们推测用户编辑受到潜在用户偏好的驱动，这些偏好可以通过文本描述来捕捉。我们介绍了
    PRELUDE 框架，该框架专注于从用户编辑数据中学习用户偏好的描述，然后根据这些描述生成智能体响应。我们提出了一种简单而有效的基于检索的算法 CIPHER，该算法通过查询
    LLM 推断用户偏好，检索历史中的相关示例，并将检索到示例中的诱导偏好汇总以生成给定上下文的响应。我们介绍了两个与 GPT-4 模拟用户交互的环境，以研究从编辑中学习，这可能具有独立的兴趣。在这项工作中，我们专注于将
    LLM 智能体与冻结的 LLM 对齐，部分原因是用户数量增加时，细化训练方法的扩展挑战。然而，对于计算成本不是障碍的设置，应用细化训练方法将是一个有趣的未来研究方向。另一个有前景的未来研究方向是基于不同级别的编辑——单词、句子、段落——来学习用户偏好，以生成令人满意的响应。
- en: Acknowledgments
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: 'Gao was a research intern in MSR NYC, and later was partially supported by
    NSF project #1901030\. All content represents the opinion of the authors, which
    is not necessarily shared or endorsed by their respective employers and/or sponsors.
    We thank MSR NYC research community, Jonathan D. Chang, Daniel D. Lee, Claire
    Cardie, and Sasha Rush for helpful discussions and support.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gao 曾在 MSR NYC 担任研究实习生，后来部分由 NSF 项目 #1901030 支持。所有内容代表作者的观点，这些观点不一定被其各自的雇主和/或赞助商认可或支持。我们感谢
    MSR NYC 研究社区、Jonathan D. Chang、Daniel D. Lee、Claire Cardie 和 Sasha Rush 的有益讨论和支持。'
- en: References
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等。《GPT-4 技术报告》。*arXiv 预印本 arXiv:2303.08774*，2023年。
- en: Agrawal & Carpuat (2022) Sweta Agrawal and Marine Carpuat. An imitation learning
    curriculum for text editing with non-autoregressive models. *ArXiv*, abs/2203.09486,
    2022.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal & Carpuat（2022）Sweta Agrawal 和 Marine Carpuat。《一种用于非自回归模型的文本编辑模仿学习课程》。*ArXiv*，abs/2203.09486，2022年。
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional
    ai: Harmlessness from ai feedback, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等（2022）Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
    Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
    Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep
    Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller,
    Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,
    Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma,
    Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer
    El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown 和 Jared Kaplan。《宪法 AI：来自 AI
    反馈的无害性》，2022年。
- en: Balachandran et al. (2022) Vidhisha Balachandran, Hannaneh Hajishirzi, William
    Cohen, and Yulia Tsvetkov. Correcting diverse factual errors in abstractive summarization
    via post-editing and language model infilling. *ArXiv*, abs/2210.12378, 2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balachandran et al. (2022) Vidhisha Balachandran, Hannaneh Hajishirzi, William
    Cohen, 和 Yulia Tsvetkov. 通过后编辑和语言模型填充修正抽象总结中的多样事实错误。*ArXiv*，abs/2210.12378，2022。
- en: Bar (2022) Nitsan Bar. Papertweet. [https://github.com/bnitsan/PaperTweet/](https://github.com/bnitsan/PaperTweet/),
    2022.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bar (2022) Nitsan Bar. Papertweet. [https://github.com/bnitsan/PaperTweet/](https://github.com/bnitsan/PaperTweet/)，2022。
- en: Botha et al. (2018) Jan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge,
    and Dipanjan Das. Learning to split and rephrase from wikipedia edit history.
    *ArXiv*, abs/1808.09468, 2018.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botha et al. (2018) Jan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge,
    和 Dipanjan Das. 从维基百科编辑历史中学习拆分和改述。*ArXiv*，abs/1808.09468，2018。
- en: Brody et al. (2020) Shaked Brody, Uri Alon, and Eran Yahav. A structural model
    for contextual code changes. *Proceedings of the ACM on Programming Languages*,
    4:1 – 28, 2020.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brody et al. (2020) Shaked Brody, Uri Alon, 和 Eran Yahav. 用于上下文代码更改的结构模型。*ACM
    编程语言会议论文集*，4:1 – 28，2020。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 语言模型是少量样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: Brynjolfsson et al. (2023) Erik Brynjolfsson, Danielle Li, and Lindsey R Raymond.
    Generative ai at work. Technical report, National Bureau of Economic Research,
    2023.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brynjolfsson et al. (2023) Erik Brynjolfsson, Danielle Li, 和 Lindsey R Raymond.
    工作中的生成性 AI。技术报告，国家经济研究局，2023。
- en: Cao et al. (2020) Mengyao Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung.
    Factual error correction for abstractive summarization models. *ArXiv*, abs/2010.08712,
    2020.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2020) Mengyao Cao, Yue Dong, Jiapeng Wu, 和 Jackie Chi Kit Cheung.
    用于抽象总结模型的事实错误修正。*ArXiv*，abs/2010.08712，2020。
- en: Chang et al. (2023) Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy,
    Dipendra Misra, and Wen Sun. Learning to generate better than your llm. *arXiv
    preprint arXiv:2306.11816*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. (2023) Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy,
    Dipendra Misra, 和 Wen Sun. 学习生成比你的 LLM 更好的结果。*arXiv 预印本 arXiv:2306.11816*，2023。
- en: 'Chen et al. (2018) Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, and Monperrus Martin. Sequencer: Sequence-to-sequence
    learning for end-to-end program repair. *IEEE Transactions on Software Engineering*,
    47:1943–1959, 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2018) Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, 和 Monperrus Martin. Sequencer: 用于端到端程序修复的序列到序列学习。*IEEE
    软件工程学报*，47:1943–1959，2018。'
- en: 'Cheng et al. (2023) Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie,
    and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language
    feedback. *arXiv preprint arXiv:2312.06853*, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng et al. (2023) Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie,
    和 Adith Swaminathan. LLF-bench: 语言反馈交互学习基准。*arXiv 预印本 arXiv:2312.06853*，2023。'
- en: Chong et al. (2023) Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye
    Jin, Liner Yang, Yange Fan, Hanghang Fan, and Erhong Yang. Leveraging prefix transfer
    for multi-intent text revision. *Annual Meeting of the Association for Computational
    Linguistics*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chong et al. (2023) Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye
    Jin, Liner Yang, Yange Fan, Hanghang Fan, 和 Erhong Yang. 利用前缀转移进行多意图文本修订。*计算语言学协会年会*，2023。
- en: Clement et al. (2019) Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe,
    and Alexander A. Alemi. On the use of arxiv as a dataset, 2019.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clement et al. (2019) Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe,
    和 Alexander A. Alemi. 关于将 arxiv 用作数据集，2019。
- en: 'D’Arcy et al. (2023) Mike D’Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl,
    Jonathan Bragg, Tom Hope, and Doug Downey. Aries: A corpus of scientific paper
    edits made in response to peer reviews. *ArXiv*, abs/2306.12587, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'D’Arcy et al. (2023) Mike D’Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl,
    Jonathan Bragg, Tom Hope, 和 Doug Downey. Aries: 一个科学论文编辑响应同行评审的语料库。*ArXiv*，abs/2306.12587，2023。'
- en: 'Deng et al. (2022) Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing
    discrete text prompts with reinforcement learning. *arXiv preprint arXiv:2205.12548*,
    2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2022) Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, 和 Zhiting Hu. Rlprompt: 使用强化学习优化离散文本提示。*arXiv
    预印本 arXiv:2205.12548*，2022。'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *North American Chapter of the Association for Computational Linguistics*,
    2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器预训练。*北美计算语言学协会*，2019
    年。
- en: 'Dohmke (2022) Thomas Dohmke. Github copilot is generally available to all developers.
    [https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/),
    2022. Accessed: April-20-2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dohmke (2022) Thomas Dohmke。GitHub Copilot 已普遍提供给所有开发者。 [https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/)，2022
    年。访问时间：2024 年 4 月 20 日。
- en: Du et al. (2022) Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa
    Lopez, and Dongyeop Kang. Understanding iterative revision from human-written
    text. *ArXiv*, abs/2203.03802, 2022.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2022) Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa
    Lopez 和 Dongyeop Kang。理解人类书写文本中的迭代修订。*ArXiv*，abs/2203.03802，2022 年。
- en: Faltings et al. (2020) Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett,
    Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. *ArXiv*, abs/2010.12826,
    2020.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faltings et al. (2020) Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett,
    Chris Quirk, Jianfeng Gao 和 Bill Dolan。通过命令进行文本编辑。*ArXiv*，abs/2010.12826，2020
    年。
- en: 'Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Sherry
    Wu, Graham Neubig, and André F. T. Martins. Bridging the gap: A survey on integrating
    (human) feedback for natural language generation. *ArXiv*, abs/2305.00955, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Sherry Wu, Graham Neubig 和 André F. T. Martins。弥合差距：自然语言生成中整合（人类）反馈的调查。*ArXiv*，abs/2305.00955，2023
    年。
- en: Foundation (2022) Wikimedia Foundation. Wikimedia downloads. [https://dumps.wikimedia.org](https://dumps.wikimedia.org),
    2022.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foundation (2022) 维基媒体基金会。维基媒体下载。 [https://dumps.wikimedia.org](https://dumps.wikimedia.org)，2022
    年。
- en: Garivier et al. (2016) Aurélien Garivier, Tor Lattimore, and Emilie Kaufmann.
    On explore-then-commit strategies. *Advances in Neural Information Processing
    Systems*, 29, 2016.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garivier et al. (2016) Aurélien Garivier, Tor Lattimore 和 Emilie Kaufmann。关于探索后再承诺策略。*神经信息处理系统进展*，29，2016
    年。
- en: Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al.
    Direct language model alignment from online ai feedback. *arXiv preprint arXiv:2402.04792*,
    2024.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, 等人。来自在线
    AI 反馈的直接语言模型对齐。*arXiv 预印本 arXiv:2402.04792*，2024 年。
- en: Guu et al. (2017) Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy
    Liang. Generating sentences by editing prototypes. *Transactions of the Association
    for Computational Linguistics*, 6:437–450, 2017.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu et al. (2017) Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren 和 Percy Liang。通过编辑原型生成句子。*计算语言学协会会刊*，6:437–450，2017
    年。
- en: 'Hancock et al. (2019) Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazaré,
    and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot!
    *Annual Meeting of the Association for Computational Linguistics*, 2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hancock et al. (2019) Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazaré
    和 Jason Weston。部署后的对话学习：喂饱自己，聊天机器人！*计算语言学协会年会*，2019 年。
- en: 'Hua et al. (2019) Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. Argument
    mining for understanding peer reviews. *Proceedings of the 2019 Conference of
    the North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long and Short Papers)*, June 2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua et al. (2019) Xinyu Hua, Mitko Nikolov, Nikhil Badugu 和 Lu Wang。理解同行评审的论证挖掘。*2019
    年北美计算语言学协会会议：人类语言技术，第 1 卷（长篇和短篇论文）*，2019 年 6 月。
- en: 'Kershaw & Koeling (2020) Daniel James Kershaw and R. Koeling. Elsevier oa cc-by
    corpus. *ArXiv*, abs/2008.00774, 2020. doi: https://doi.org/10.48550/arXiv.2008.00774.
    URL [https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs](https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kershaw & Koeling (2020) Daniel James Kershaw 和 R. Koeling。Elsevier oa cc-by
    语料库。*ArXiv*，abs/2008.00774，2020 年。doi: https://doi.org/10.48550/arXiv.2008.00774。URL
    [https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs](https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs)。'
- en: Kim et al. (2022) Zae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, and Dongyeop
    Kang. Improving iterative text revision by learning where to edit from other revision
    tasks. *ArXiv*, abs/2212.01350, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Zae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, 和 Dongyeop
    Kang。通过从其他修订任务中学习编辑位置来改进迭代文本修订。*ArXiv*，abs/2212.01350，2022年。
- en: 'Laban et al. (2023) Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq R.
    Joty, Caiming Xiong, and Chien-Sheng Wu. Swipe: A dataset for document-level simplification
    of wikipedia pages. *Annual Meeting of the Association for Computational Linguistics*,
    2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Laban et al. (2023) Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq
    R. Joty, Caiming Xiong, 和 Chien-Sheng Wu。Swipe: 一个用于维基百科页面文档级简化的数据集。*计算语言学协会年会*，2023年。'
- en: Lee et al. (2024) Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
    Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David
    Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti
    Dutta, Jin L.C. Guo, Md. Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk,
    Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, S. Sterman, Sitong
    Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski,
    Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, and Pao Siangliulue.
    A design space for intelligent and interactive writing assistants. *Conference
    on Human Factors in Computing Systems*, abs/2403.14117, 2024.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2024) Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
    Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David
    Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti
    Dutta, Jin L.C. Guo, Md. Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk,
    Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, S. Sterman, Sitong
    Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski,
    Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, 和 Pao Siangliulue。智能和交互式写作助手的设计空间。*计算机系统人因学会议*，abs/2403.14117，2024年。
- en: Levenshtein (1965) Vladimir I. Levenshtein. Binary codes capable of correcting
    deletions, insertions, and reversals. *Soviet physics. Doklady*, 10:707–710, 1965.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levenshtein (1965) Vladimir I. Levenshtein。能够纠正删除、插入和反转的二进制编码。*苏联物理学. 论文集*，10:707–710，1965年。
- en: Li et al. (2023) Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael
    Bendersky. Automatic prompt rewriting for personalized text generation. *arXiv
    preprint arXiv:2310.00152*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, 和 Michael
    Bendersky。用于个性化文本生成的自动提示重写。*arXiv 预印本 arXiv:2310.00152*，2023年。
- en: Li et al. (2016) Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc’Aurelio Ranzato,
    and Jason Weston. Dialogue learning with human-in-the-loop. *ArXiv*, abs/1611.09823,
    2016.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2016) Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc’Aurelio Ranzato,
    和 Jason Weston。与人类互动的对话学习。*ArXiv*，abs/1611.09823，2016年。
- en: Li et al. (2022) Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Chi Kit Cheung,
    and Siva Reddy. Using interactive feedback to improve the accuracy and explainability
    of question answering systems post-deployment. *ArXiv*, abs/2204.03025, 2022.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Chi Kit Cheung,
    和 Siva Reddy。利用交互反馈提高问答系统的准确性和可解释性。*ArXiv*，abs/2204.03025，2022年。
- en: 'Liu et al. (2023) Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X. Liu,
    and Soroush Vosoughi. Second thoughts are best: Learning to re-align with human
    values from text edits. *ArXiv*, abs/2301.00355, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X. Liu,
    和 Soroush Vosoughi。第二次思考最好：从文本编辑中学习重新对齐人类价值观。*ArXiv*，abs/2301.00355，2023年。
- en: Liu et al. (2022) Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker,
    Dragomir R. Radev, and Ahmed Hassan Awadallah. On improving summarization factual
    consistency from natural language feedback. *Annual Meeting of the Association
    for Computational Linguistics*, 2022.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker,
    Dragomir R. Radev, 和 Ahmed Hassan Awadallah。通过自然语言反馈改进摘要的事实一致性。*计算语言学协会年会*，2022年。
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis.
    *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
    Human Language Technologies*, June 2011.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, 和 Christopher Potts。学习用于情感分析的词向量。*第49届计算语言学协会年会：人类语言技术会议论文集*，2011年6月。
- en: 'Malaviya et al. (2023) Chaitanya Malaviya, Subin Lee, Dan Roth, and Mark Yatskar.
    Pachinko: Patching interpretable qa models through natural language feedback.
    *ArXiv*, abs/2311.09558, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Malaviya et al. (2023) Chaitanya Malaviya, Subin Lee, Dan Roth, 和 Mark Yatskar。Pachinko:
    通过自然语言反馈修补可解释的问答模型。*ArXiv*，abs/2311.09558，2023年。'
- en: 'Mallinson et al. (2020) Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, and
    Guillermo Garrido. Felix: Flexible text editing through tagging and insertion.
    *ArXiv*, abs/2003.10687, 2020.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallinson 等人（2020）**乔纳森·马林森**、**阿利亚克谢·塞维林**、**埃里克·马尔米** 和 **吉列尔莫·加里多**。Felix：通过标签和插入实现灵活文本编辑。*ArXiv*，abs/2003.10687，2020。
- en: Marrese-Taylor et al. (2020) Edison Marrese-Taylor, Machel Reid, and Yutaka
    Matsuo. Variational inference for learning representations of natural language
    edits. *ArXiv*, abs/2004.09143, 2020.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marrese-Taylor 等人（2020）**爱迪森·马雷斯-泰勒**、**马切尔·里德** 和 **松尾丰**。用于学习自然语言编辑表示的变分推断。*ArXiv*，abs/2004.09143，2020。
- en: Marrese-Taylor et al. (2023) Edison Marrese-Taylor, Machel Reid, and Alfredo
    Solano. Edit aware representation learning via levenshtein prediction. *The Fourth
    Workshop on Insights from Negative Results in NLP*, 2023.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marrese-Taylor 等人（2023）**爱迪森·马雷斯-泰勒**、**马切尔·里德** 和 **阿尔弗雷多·索拉诺**。通过 Levenshtein
    预测的编辑感知表示学习。*第四届自然语言处理负面结果洞察研讨会*，2023。
- en: Misra et al. (2024) Dipendra Misra, Aldo Pacchiano, and Robert E Schapire. Provable
    interactive learning with hindsight instruction feedback. *arXiv preprint arXiv:2404.09123*,
    2024.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misra 等人（2024）**迪彭德拉·米斯拉**、**阿尔多·帕基亚诺** 和 **罗伯特·E·沙皮雷**。可证明的互动学习与事后指令反馈。*arXiv
    预印本 arXiv:2404.09123*，2024。
- en: 'Mita et al. (2022) Masato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya
    Mizumoto, Jun Suzuki, and Kentaro Inui. Towards automated document revision: Grammatical
    error correction, fluency edits, and beyond. *ArXiv*, abs/2205.11484, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mita 等人（2022）**三田正人**、**坂口圭佑**、**萩原正人**、**水本智也**、**铃木纯** 和 **井上健太郎**。迈向自动化文档修订：语法错误纠正、流畅度编辑及其他。*ArXiv*，abs/2205.11484，2022。
- en: 'Mysore et al. (2023) Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang,
    Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, and
    Tara Safavi. Pearl: Personalizing large language model writing assistants with
    generation-calibrated retrievers. *arXiv preprint arXiv:2311.09180*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mysore 等人（2023）**谢谢拉·迈索尔**、**朱阮**、**孟婷·万**、**龙琦·杨**、**史蒂夫·门泽斯**、**蒂娜·巴赫**、**埃马纽埃尔·巴拉哈斯·冈萨雷斯**、**詹妮弗·内维尔**
    和 **塔拉·萨法维**。Pearl：通过生成校准检索器个性化大型语言模型写作助手。*arXiv 预印本 arXiv:2311.09180*，2023。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted
    question-answering with human feedback. *ArXiv*, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano 等人（2021）**中野礼一郎**、**雅各布·希尔顿**、**S·阿伦·巴拉吉**、**杰夫·吴**、**欧阳龙**、**克里斯蒂娜·金**、**克里斯托弗·赫斯**、**尚塔努·贾因**、**维尼特·科萨拉朱**、**威廉·桑德斯**、**徐江**、**卡尔·科比**、**蒂娜·埃隆杜**、**格雷琴·克鲁格**、**凯文·巴顿**、**马修·奈特**、**本杰明·切斯**
    和 **约翰·舒尔曼**。WebGPT：浏览器辅助的问答与人类反馈。*ArXiv*，2021。
- en: Nguyen et al. (2021) Khanh X Nguyen, Dipendra Misra, Robert Schapire, Miroslav
    Dudík, and Patrick Shafto. Interactive learning from activity description. *International
    Conference on Machine Learning*, pp.  8096–8108, 2021.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2021）**阮心轩**、**迪彭德拉·米斯拉**、**罗伯特·沙皮雷**、**米罗斯拉夫·杜迪克** 和 **帕特里克·沙夫托**。基于活动描述的互动学习。*国际机器学习会议*，第
    8096–8108 页，2021。
- en: Ouyang et al. (2022a) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J.
    Lowe. Training language models to follow instructions with human feedback. *ArXiv*,
    2022a.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022a）**欧阳龙**、**杰夫·吴**、**徐江**、**迪奥戈·阿尔梅达**、**卡罗尔·L·温赖特**、**帕梅拉·米什金**、**张冲**、**桑迪尼·阿加瓦尔**、**卡塔里娜·斯拉玛**、**亚历克斯·雷**、**约翰·舒尔曼**、**雅各布·希尔顿**、**弗雷泽·凯尔顿**、**卢克·E·米勒**、**麦迪·西门斯**、**阿曼达·阿斯凯尔**、**彼得·韦林德**、**保罗·弗朗西斯·克里斯蒂亚诺**、**扬·莱克**
    和 **瑞安·J·洛**。训练语言模型以遵循人类反馈的指令。*ArXiv*，2022a。
- en: Ouyang et al. (2022b) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744, 2022b.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022b）**欧阳龙**、**杰弗里·吴**、**徐江**、**迪奥戈·阿尔梅达**、**卡罗尔·温赖特**、**帕梅拉·米什金**、**张冲**、**桑迪尼·阿加瓦尔**、**卡塔里娜·斯拉玛**、**亚历克斯·雷**
    等人。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*，35:27730–27744，2022b。
- en: Petrak et al. (2023) Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai
    Rozanov, and Iryna Gurevych. Learning from free-text human feedback - collect
    new datasets or extend existing ones? *ArXiv*, abs/2310.15758, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petrak 等人（2023）**多米尼克·彼得拉克**、**纳菲斯·萨达特·穆萨维**、**叶天**、**尼古拉·罗扎诺夫** 和 **伊琳娜·古列维奇**。从自由文本人类反馈中学习
    - 收集新数据集还是扩展现有数据集？*ArXiv*，abs/2310.15758，2023。
- en: 'Rajagopal et al. (2022) Dheeraj Rajagopal, Xuchao Zhang, Michael Gamon, Sujay Kumar
    Jauhar, Diyi Yang, and Eduard H. Hovy. One document, many revisions: A dataset
    for classification and description of edit intents. *International Conference
    on Language Resources and Evaluation*, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajagopal et al. (2022) Dheeraj Rajagopal, Xuchao Zhang, Michael Gamon, Sujay
    Kumar Jauhar, Diyi Yang, 和 Eduard H. Hovy. 一份文档，多次修订：用于分类和描述编辑意图的数据集。*国际语言资源与评估会议*，2022年。
- en: Reid & Neubig (2022) Machel Reid and Graham Neubig. Learning to model editing
    processes. *Conference on Empirical Methods in Natural Language Processing*, 2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid & Neubig (2022) Machel Reid 和 Graham Neubig. 学习建模编辑过程。*自然语言处理经验方法会议*，2022年。
- en: 'Reid et al. (2023) Machel Reid, Vincent J. Hellendoorn, and Graham Neubig.
    Diffuser: Diffusion via edit-based reconstruction. *International Conference on
    Learning Representations*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid et al. (2023) Machel Reid, Vincent J. Hellendoorn, 和 Graham Neubig. Diffuser：通过基于编辑的重构进行扩散。*国际学习表征会议*，2023年。
- en: Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
    Ouyang Long, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting
    human evaluators. *ArXiv*, abs/2206.05802, 2022.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
    Ouyang Long, Jonathan Ward, 和 Jan Leike. 自我批评模型以辅助人工评估者。*ArXiv*，abs/2206.05802，2022年。
- en: Scheurer et al. (2023) J’er’emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern
    Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models
    with language feedback at scale. *ArXiv*, abs/2303.16755, 2023.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scheurer et al. (2023) J’er’emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun
    Shern Chan, Angelica Chen, Kyunghyun Cho, 和 Ethan Perez. 使用语言反馈进行大规模语言模型训练。*ArXiv*，abs/2303.16755，2023年。
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    to the point: Summarization with pointer-generator networks. *Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, July 2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See et al. (2017) Abigail See, Peter J. Liu, 和 Christopher D. Manning. 直击要点：使用指针生成网络进行总结。*第55届计算语言学协会年会（第1卷：长篇论文）*，2017年7月。
- en: 'Shen et al. (2023) Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan
    Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. Beyond
    summarization: Designing ai support for real-world expository writing tasks. *arXiv
    preprint arXiv:2304.02623*, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2023) Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan
    Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, 和 David Sontag. 超越总结：为现实世界的说明性写作任务设计
    AI 支持。*arXiv 预印本 arXiv:2304.02623*，2023年。
- en: 'Shi et al. (2022) Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and
    Jing Xu. When life gives you lemons, make cherryade: Converting feedback from
    bad responses into good labels. *ArXiv*, abs/2210.15893, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2022) Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, 和 Jing
    Xu. 当生活给你柠檬时，做樱桃饮料：将差劲的反馈转换为良好标签。*ArXiv*，abs/2210.15893，2022年。
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    Mpnet: Masked and permuted pre-training for language understanding. *ArXiv*, abs/2004.09297,
    2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, 和 Tie-Yan Liu.
    Mpnet：用于语言理解的掩码和置换预训练。*ArXiv*，abs/2004.09297，2020年。
- en: 'Stahlberg & Kumar (2020) Felix Stahlberg and Shankar Kumar. Seq2edits: Sequence
    transduction using span-level edit operations. *Conference on Empirical Methods
    in Natural Language Processing*, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stahlberg & Kumar (2020) Felix Stahlberg 和 Shankar Kumar. Seq2edits：使用跨度级编辑操作的序列转导。*自然语言处理经验方法会议*，2020年。
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning
    to summarize from human feedback. *ArXiv*, abs/2009.01325, 2020.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, 和 Paul Christiano. 从人类反馈中学习总结。*ArXiv*，abs/2009.01325，2020年。
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, 等等。Gemini：一系列高能力的多模态模型。*arXiv 预印本 arXiv:2312.11805*，2023年。
- en: 'Wang et al. (2023) Sitong Wang, Lydia B Chilton, and Jeffrey V Nickerson. Writing
    with generative ai: Multi-modal and multi-dimensional tools for journalists. *The
    Second Workshop on Intelligent and Interactive Writing Assistants at ACM CHI*,
    2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Sitong Wang, Lydia B Chilton, 和 Jeffrey V Nickerson. 使用生成性
    AI 进行写作：针对记者的多模态和多维度工具。*第二届智能与交互写作助手研讨会（ACM CHI）*，2023年。
- en: Weston (2016) Jason Weston. Dialog-based language learning. *ArXiv*, abs/1604.06045,
    2016.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston（2016）Jason Weston。基于对话的语言学习。*ArXiv*，abs/1604.06045，2016年。
- en: 'Xu et al. (2022) Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau,
    and Jason Weston. Learning new skills after deployment: Improving open-domain
    internet-driven dialogue with human feedback. *Annual Meeting of the Association
    for Computational Linguistics*, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2022）Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau 和
    Jason Weston。部署后的新技能学习：通过人类反馈改进开放域互联网驱动对话。*计算语言学协会年会*，2022年。
- en: Yao et al. (2021) Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, and Graham
    Neubig. Learning structural edits via incremental tree transformations. *ArXiv*,
    abs/2101.12087, 2021.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2021）Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun 和 Graham Neubig。通过增量树变换学习结构编辑。*ArXiv*，abs/2101.12087，2021年。
- en: Yin et al. (2018) Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt,
    and Alexander L. Gaunt. Learning to represent edits. *ArXiv*, abs/1810.13337,
    2018.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2018）Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt
    和 Alexander L. Gaunt。学习表示编辑。*ArXiv*，abs/1810.13337，2018年。
- en: 'Zhang et al. (2022) Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy
    Li, and Miloš Gligorić. Coditt5: Pretraining for source code and natural language
    editing. *Proceedings of the 37th IEEE/ACM International Conference on Automated
    Software Engineering*, 2022.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li
    和 Miloš Gligorić。Coditt5：源代码和自然语言编辑的预训练。*第37届 IEEE/ACM 自动化软件工程国际会议论文集*，2022年。
- en: 'Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *International
    Conference on Learning Representations*, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang* 等（2020）Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger
    和 Yoav Artzi。Bertscore：使用 BERT 评估文本生成。*国际学习表征会议*，2020年。
- en: Ziegler et al. (2019) Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown,
    Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences. *ArXiv*, 2019.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2019）Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec
    Radford, Dario Amodei, Paul Christiano 和 Geoffrey Irving。基于人类偏好的语言模型微调。*ArXiv*，2019年。
- en: Appendix
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Additional Details
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 额外细节
- en: Dataset Examples.
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集示例。
- en: We list links to dataset sources for our user-provided context in [Table 4](#A2.T4
    "Table 4 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 4](#A2.T4 "Table 4 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")中列出了数据集源的链接，以供我们用户提供的上下文参考。
- en: GPT-4 User’s Edits
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-4 用户编辑
- en: We list examples of OUR GPT-4 user’s edits with different latent preference
    on summarization in [Table 5](#A2.T5 "Table 5 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits").
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 5](#A2.T5 "Table 5 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")中列出了不同潜在偏好的
    GPT-4 用户编辑示例。
- en: GPT-4 User Template.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-4 用户模板。
- en: Prompt templates used by our GPT-4 user are provided in [Table 6](#A2.T6 "Table
    6 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits").
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 GPT-4 用户使用的提示模板见[表 6](#A2.T6 "Table 6 ‣ Retrieval Accuracy. ‣ Appendix B
    Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from User
    Edits")。
- en: CIPHER Templates.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CIPHER 模板。
- en: Prompt templates used by CIPHER are provided in [Table 7](#A2.T7 "Table 7 ‣
    Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM Agents by
    Learning Latent Preference from User Edits").
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: CIPHER 使用的提示模板见[表 7](#A2.T7 "Table 7 ‣ Retrieval Accuracy. ‣ Appendix B Additional
    Analysis ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")。
- en: ICL-edit Templates.
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ICL-edit 模板。
- en: Prompt templates used by ICL-edit baseline are provided in [Table 8](#A2.T8
    "Table 8 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ICL-edit 基线使用的提示模板见[表 8](#A2.T8 "Table 8 ‣ Retrieval Accuracy. ‣ Appendix B
    Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from User
    Edits")。
- en: Appendix B Additional Analysis
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外分析
- en: Detailed Expense Analysis.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 详细费用分析。
- en: We list a detailed computational expense of different methods in [Table 9](#A2.T9
    "Table 9 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 9](#A2.T9 "Table 9 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")中列出了不同方法的详细计算开销。
- en: Failure Cases.
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 失败案例。
- en: We summarize our failure case analysis of CIPHER on summarization in [Table 10](#A2.T10
    "Table 10 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 10](#A2.T10 "Table 10 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")中总结了 CIPHER
    在摘要任务中的失败案例分析。
- en: Retrieval Accuracy.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 检索准确率。
- en: We calculate retrieval accuracy for  CIPHER as the fraction of all retrieved
    contexts that are of the same document type as the currently given context across
    all seeds and time steps. We report the results in [Table 11](#A2.T11 "Table 11
    ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM Agents by
    Learning Latent Preference from User Edits"). We find that the retrieval accuracy
    is higher on the summarization task than on email writing. and using MPNET typically
    performs better than using Bert to encode context.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了 CIPHER 的检索准确率，即所有检索到的上下文中与当前给定上下文属于相同文档类型的比例，涵盖所有种子和时间步骤。我们在[表 11](#A2.T11
    "Table 11 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits")中报告了结果。我们发现，摘要任务的检索准确率高于电子邮件写作，并且使用
    MPNET 通常比使用 Bert 对上下文进行编码的效果更好。
- en: 'Table 4: Link to each source dataset, from which we randomly sample examples
    as the user-provided context in our tasks.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：每个源数据集的链接，我们从中随机抽取示例作为我们任务中的用户提供的上下文。
- en: '| Data Source | Link and Example |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 数据来源 | 链接和示例 |'
- en: '| --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CNN Daily Mail (See et al., [2017](#bib.bib57)) | [https://huggingface.co/datasets/cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| CNN Daily Mail (See et al., [2017](#bib.bib57)) | [https://huggingface.co/datasets/cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    |'
- en: '| SLF5K (Stiennon et al., [2020](#bib.bib62)) | [https://huggingface.co/datasets/JeremyAlain/SLF5K](https://huggingface.co/datasets/JeremyAlain/SLF5K)
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SLF5K (Stiennon et al., [2020](#bib.bib62)) | [https://huggingface.co/datasets/JeremyAlain/SLF5K](https://huggingface.co/datasets/JeremyAlain/SLF5K)
    |'
- en: '| Wikidump (Foundation, [2022](#bib.bib23)) | [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Wikidump (Foundation, [2022](#bib.bib23)) | [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)
    |'
- en: '| Arxiv (Clement et al., [2019](#bib.bib15)) | [https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers](https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Arxiv (Clement et al., [2019](#bib.bib15)) | [https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers](https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers)
    |'
- en: '| IMDb (Maas et al., [2011](#bib.bib39)) | [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| IMDb (Maas et al., [2011](#bib.bib39)) | [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)
    |'
- en: '| PaperTweet (Bar, [2022](#bib.bib5)) | [https://huggingface.co/datasets/nitsanb/paper_tweet](https://huggingface.co/datasets/nitsanb/paper_tweet)
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| PaperTweet (Bar, [2022](#bib.bib5)) | [https://huggingface.co/datasets/nitsanb/paper_tweet](https://huggingface.co/datasets/nitsanb/paper_tweet)
    |'
- en: '| Elsevier OA CC-By (Kershaw & Koeling, [2020](#bib.bib29)) | [https://huggingface.co/datasets/orieg/elsevier-oa-cc-by](https://huggingface.co/datasets/orieg/elsevier-oa-cc-by)
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Elsevier OA CC-By (Kershaw & Koeling, [2020](#bib.bib29)) | [https://huggingface.co/datasets/orieg/elsevier-oa-cc-by](https://huggingface.co/datasets/orieg/elsevier-oa-cc-by)
    |'
- en: 'Table 5: Examples of user edits with different latent preference on summarization.
    The agent output column shows the agent generation without conditioning on any
    preference. The latent preference for the user edits is in bold and parenthesis.
    We truncate some examples due to limited space.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：具有不同潜在偏好的用户编辑示例。代理输出列显示了不考虑任何偏好的情况下生成的结果。用户编辑的潜在偏好用**粗体**和括号表示。由于空间有限，我们截断了一些示例。
- en: '| Agent Output | User Edits |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 代理输出 | 用户编辑 |'
- en: '| --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Leon Fenner, the suspected gunman in a $2 million diamond heist at Watch
    Standard Jewelry in New York City, was arrested in Maryland. The robbery occurred
    on November 11 and resulted in the injury of the store owner’s elderly father.
    Fenner’s accomplice, Rondu Frisby, was a friend of the store owner and was apprehended
    a week after the heist. The robbery took place during the city’s Veteran’s Day
    Parade, leading to a block shutdown. Fenner and Frisby stole numerous jewels and
    20 luxury watches, but were caught on surveillance cameras. Fenner is set to appear
    before a federal court in Maryland. | (targeted to young children, storytelling,
    short sentences, playful language, interactive, positive) Once upon a time in
    the big, bustling city of New York, during a parade filled with music and marching
    on Veteran’s Day, something sneaky happened at a place called Watch Standard Jewelry.
    This wasn’t just any store; it was a treasure chest full of sparkling diamonds
    and shiny watches that could make your eyes twinkle like stars. Now, two not-so-nice
    men, Leon Fenner and his buddy Rondu Frisby, thought they could be sneaky and
    take these treasures without asking. On November 11, while everyone was watching
    the parade, they tiptoed into the store. But oh no! They accidentally hurt the
    store owner’s dear old dad, which wasn’t very kind at all. [TRUNCATED] |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 纽约市Watch Standard Jewelry的**疑似枪手**Leon Fenner，在一场价值200万美元的钻石抢劫案后在马里兰州被逮捕。抢劫发生在11月11日，导致了店主年迈父亲的受伤。Fenner的同伙Rondu
    Frisby是店主的朋友，抢劫后一周被抓获。抢劫发生在城市的退伍军人节游行期间，导致了封街。Fenner和Frisby偷走了许多珠宝和20块奢侈手表，但被监控摄像头拍下。Fenner将出庭接受马里兰州的联邦法院审理。
    | （面向小孩子，讲故事，简短句子，活泼语言，互动性，积极向上）从前，在热闹的大纽约市，在一个充满音乐和游行的退伍军人节那天，发生了一件偷偷摸摸的事情，地点是Watch
    Standard Jewelry。这不仅仅是一家商店，它是一个满是闪闪发光的钻石和明亮手表的宝箱，让你的眼睛像星星一样闪耀。现在，有两个不太友善的家伙，Leon
    Fenner和他的朋友Rondu Frisby，以为他们可以偷偷拿走这些宝藏而不被发现。11月11日，当大家都在观看游行时，他们悄悄地进入了商店。但是哦不！他们不小心伤害了店主的老父亲，这一点也不友好。
    [TRUNCATED] |'
- en: '| The article describes a situation where a neighbor’s cat, one of seven rescued
    by a "cat lady," has started frequenting the author’s front yard after leaving
    its original home due to the presence of other cats. Over the past few months,
    the cat has become more consistent in its visits, meowing at the author and their
    family members for attention. The cat allows itself to be petted and shows affectionate
    behavior, such as rubbing against legs and headbutting hands, until it is fed.
    After eating, the cat typically leaves but returns in the evening. The author
    is seeking advice on how to handle the situation, expressing concern about feeding
    the cat because it is not legally theirs and there could be potential ownership
    and legal issues. | (second person narrative, brief, show emotions, invoke personal
    reflection, immersive) Imagine yourself in a quaint neighborhood, where the gentle
    purrs of a feline friend have become a familiar part of your daily routine. This
    isn’t just any cat—it’s one of seven, a charming escapee from the home of a local
    ""cat lady,"" seeking solace in the tranquility of your front yard. Over recent
    months, this whiskered wanderer has turned your doorstep into its personal stage,
    meowing for attention with a persistence that tugs at your heartstrings. Each
    encounter with this affectionate creature is a dance of mutual admiration. You
    find yourself captivated by its soft fur and the way it eagerly rubs against your
    legs, headbutting your hands in a silent plea for affection. [TRUNCATED] |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 文章描述了一个邻居的猫，其中一只是被“猫女士”救助的七只猫之一，在离开原本的家后，开始频繁光顾作者的前院，因为那里有其他猫的存在。在过去的几个月里，这只猫变得越来越常来，向作者和他们的家人喵喵叫以引起注意。猫咪允许被抚摸，并表现出亲昵的行为，比如用头蹭腿和用头顶手，直到得到食物。吃完后，猫咪通常会离开，但会在傍晚时分回来。作者寻求关于如何处理这种情况的建议，表达了对喂食猫咪的担忧，因为猫咪并不合法属于他们，可能会涉及所有权和法律问题。
    | （第二人称叙述，简短，展示情感，引发个人反思，沉浸式）想象你自己在一个迷人的社区里，一只猫咪的柔和呼噜声已经成为你日常生活的一部分。这不仅仅是一只猫——它是七只猫中的一员，一只从本地“猫女士”家中逃出来的迷人猫咪，寻求在你前院的宁静中安慰。在最近几个月里，这只有胡须的流浪者把你的门前当作了自己的舞台，不断喵喵叫以引起注意，令你的心弦为之一动。每次与这只亲昵的生物相遇都是一种相互欣赏的舞蹈。你发现自己被它柔软的皮毛和它热切地蹭腿、用头顶手的方式所吸引，仿佛在默默请求关爱。
    [TRUNCATED] |'
- en: '| The Puerto Plata Province is a northern province in the Dominican Republic,
    known for its beautiful beaches and as a popular tourist destination since the
    late 1990s. Its capital is San Felipe de Puerto Plata, named after the silver
    clouds Columbus saw on a mountain nearby in 1493\. The city of Puerto Plata was
    founded in 1502. Historically, the first European town in the Americas, La Isabela,
    was established by Columbus near Puerto Plata in 1493\. In 1605, the governor
    Antonio de Osorio destroyed Puerto Plata and other towns to curb illegal trade,
    leading to the formation of new settlements. Puerto Plata became a Maritime District
    in 1865 and was later recognized as a province in the 1907 constitution. Over
    time, it expanded to include more municipalities and municipal districts, with
    the latest additions in 2007. [TRUNCATED] | (bullet points, parallel structure,
    brief) - Location: Northern Dominican Republic'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '| 普埃尔托普拉塔省是多米尼加共和国北部的一个省，以其美丽的海滩而闻名，并自1990年代末以来成为热门旅游目的地。其首府是圣费利佩·德·普埃尔托普拉塔，得名于1493年哥伦布在附近山上看到的银色云朵。普埃尔托普拉塔市成立于1502年。历史上，美洲第一个欧洲城镇拉伊萨贝拉是哥伦布于1493年在普埃尔托普拉塔附近建立的。1605年，总督安东尼奥·德·奥索里奥摧毁了普埃尔托普拉塔及其他城镇，以遏制非法贸易，导致新定居点的形成。普埃尔托普拉塔在1865年成为海洋区，并在1907年宪法中被认定为省。随着时间的推移，它扩展至包括更多的市镇和市镇区，最新的扩展发生在2007年。
    [省略] |（项目符号，平行结构，简洁） - 位置：多米尼加共和国北部'
- en: '- Capital: San Felipe de Puerto Plata'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '- 首府：圣费利佩·德·普埃尔托普拉塔'
- en: '- Founding: 1502 by Europeans'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '- 成立：1502年，由欧洲人建立'
- en: '- Historical site: La Isabela (1493, first European town in the Americas)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '- 历史遗址：拉伊萨贝拉（1493年，美洲第一个欧洲城镇）'
- en: '- 1605: Destruction by Governor Antonio de Osorio to stop illegal trade'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '- 1605年：安东尼奥·德·奥索里奥总督为了制止非法贸易摧毁了该地'
- en: '- Status: Became Maritime District in 1865, recognized as province in 1907'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '- 状态：1865年成为海洋区，1907年被认定为省'
- en: '- Expansion: Added more municipalities, latest in 2007'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '- 扩展：增加了更多市镇，最新的扩展发生在2007年'
- en: '- Borders: Atlantic Ocean, Espaillat, Santiago, Valverde, Monte Cristi provinces'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '- 边界：大西洋、埃斯帕伊亚特、圣地亚哥、瓦尔维德、蒙特克里斯蒂省'
- en: '[TRUNCATED] |'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[省略] |'
- en: '| The article discusses the challenge of detecting and classifying stuttered
    speech, which affects many people but has been subject to limited research. The
    authors present a new model that differs from most existing methods, which typically
    use language models for stutter identification. Instead, their model is based
    on acoustic features, which allows it to identify various kinds of stuttering
    disfluencies without relying on speech recognition technology. The proposed model
    combines a deep residual network with bidirectional long short-term memory (LSTM)
    layers to classify different stutter types. It has been shown to significantly
    outperform current leading methods, reducing the average miss rate to 10.03%,
    which is a 27% improvement over the state-of-the-art. | (tweet style, simple English,
    inquisitive, skillful foreshadowing, with emojis) Ever struggled with stuttering
    or know someone who does? ![[Uncaptioned image]](img/b594cfbe3c591525ade65441337b58c9.png)
    Research on detecting stutters hasn’t been huge, but guess what? A new study is
    changing the game! ![[Uncaptioned image]](img/f709c0a7963abcb74f40f1766bbe9ac8.png)
    Instead of just looking at words, this smart model listens to how things are said.
    ![[Uncaptioned image]](img/c2ce2925a4f77ee92ff4c58064a1985f.png) It’s like a detective
    for sounds, using cool tech called deep learning and memory networks to spot different
    stutter types. ![[Uncaptioned image]](img/03a93b8ec12f0c683c32d9808b3c47dc.png)
    And it’s crushing it, beating old methods by 27%! Missed stutters? Down to just
    10%! ![[Uncaptioned image]](img/d85321c36403930d76d688a37abae723.png) This could
    be a big deal for speech therapy! ![[Uncaptioned image]](img/4286c387c926c55ce254d0d6170f7cc6.png)
    #Stuttering # SpeechTech # Innovation  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 文章讨论了检测和分类结巴言语的挑战，这种现象影响了许多人，但研究较少。作者提出了一种新模型，这种模型不同于大多数现有方法，这些方法通常使用语言模型进行结巴识别。相反，他们的模型基于声学特征，使其能够识别各种类型的结巴不流畅，而无需依赖语音识别技术。所提出的模型结合了深度残差网络和双向长短期记忆（LSTM）层，以分类不同的结巴类型。该模型已被证明在显著超越当前领先方法方面表现出色，将平均漏检率降低到10.03%，比最先进技术提高了27%。
    | （推文风格，简单英语，带有好奇心的巧妙预告，配有表情符号） 曾经遇到过结巴问题或认识有人有这个问题吗？ ![[未标注图片]](img/b594cfbe3c591525ade65441337b58c9.png)
    结巴检测的研究还不多，但你知道吗？一项新研究正在改变游戏规则！ ![[未标注图片]](img/f709c0a7963abcb74f40f1766bbe9ac8.png)
    这个聪明的模型不仅仅关注词汇，它还听声音的方式。 ![[未标注图片]](img/c2ce2925a4f77ee92ff4c58064a1985f.png)
    它像是声音的侦探，使用了深度学习和记忆网络的酷炫技术来识别不同的结巴类型。 ![[未标注图片]](img/03a93b8ec12f0c683c32d9808b3c47dc.png)
    而且它表现优异，击败了旧的方法27%！漏检结巴的率降到仅10%！ ![[未标注图片]](img/d85321c36403930d76d688a37abae723.png)
    这可能对言语治疗大有帮助！ ![[未标注图片]](img/4286c387c926c55ce254d0d6170f7cc6.png) #结巴 #语音技术 #创新'
- en: '| The article discusses a movie that begins promisingly but later becomes confusing
    due to a poorly depicted portrayal of ancient black magic rituals. The male lead
    delivers a strong performance despite his character’s unfavorable outcome in the
    climax. The movie is compared to a mix of "Boomerang" and "Extremities." | (question
    answering style, direct, concise) What is the general sentiment of the article
    about the movie? The article suggests that the movie starts off well but eventually
    becomes muddled. The depiction of ancient black magic rituals is criticized for
    being confusing. How does the article describe the male lead’s performance? The
    male lead’s performance is described as strong, even though his character does
    not have a favorable outcome in the climax. What films is the movie compared to
    in the article? The movie is compared to ""Boomerang"" and ""Extremities"" in
    the article. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 文章讨论了一部电影，这部电影开始时充满希望，但由于对古老黑魔法仪式的描绘不佳，后来变得令人困惑。尽管男主角在高潮部分的角色结果不佳，但他的表现依然非常出色。该电影被比作《回旋镖》和《极端》。
    | （问答风格，直接，简洁） 文章对电影的总体评价是什么？文章指出，电影开头不错，但最终变得混乱。对古老黑魔法仪式的描绘被批评为令人困惑。文章如何描述男主角的表现？男主角的表现被描述为出色，即使他的角色在高潮部分没有一个好的结果。文章将电影与哪些影片进行比较？文章将电影与《回旋镖》和《极端》进行比较。
    |'
- en: 'Table 6: Prompt templates for the AI user. The first step is to prompt the
    user for yes/no answer regarding satisfaction. If the answer is no, the second
    step is to ask the user edit the agent output according to the latent preference.
    If the answer is yes, the agent output receives 0 edits.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：AI用户的提示模板。第一步是询问用户是否对结果满意的“是/否”回答。如果答案是否，第二步是要求用户根据潜在偏好编辑代理输出。如果答案是肯定的，代理输出将不进行编辑。
- en: '|  | Summarization | Email Writing |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 邮件写作 |'
- en: '| Step 1 | Article: {user-provided article} Summary: {agent-generated summary}'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '| 步骤 1 | 文章：{user-provided article} 摘要：{agent-generated summary}'
- en: 'Is the above summary of the above article good for person who would love to
    use the following style: {latent user preference}? Please answer yes or no. |
    Notes: {user-provided notes} Email: {agent-generated email}'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 上述摘要是否适合喜欢以下风格的人：{latent user preference}? 请回答“是”或“否”。 | 备注：{user-provided notes}
    邮件：{agent-generated email}
- en: 'Is the above email based on the above notes good for a user who wants the following
    style: {latent user preference}? Please answer yes or no. |'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 上述邮件是否适合希望以下风格的用户：{latent user preference}? 请回答“是”或“否”。 |
- en: '| Step 2 | Summary: {agent-generated summary} Please revise the above summary
    of an article to meet your style: {latent user preference}. | Email: {agent-generated
    email} Assume that you prefer {latent user preference}. Please revise the above
    email to meet your style. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 2 | 摘要：{agent-generated summary} 请将上述文章的摘要修改为符合您的风格：{latent user preference}。|
    邮件：{agent-generated email} 假设您偏好{latent user preference}。请将上述邮件修改为符合您的风格。 |'
- en: 'Table 7: Prompt templates for CIPHER.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：CIPHER 的提示模板。
- en: '|  | Summarization | Email Writing |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 邮件写作 |'
- en: '| Task prompt conditioned on inferred preference ([line 6](#alg1.l6a "In Algorithm
    1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through Retrieval
    and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference from User
    Edits") in[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of CIPHER. ‣
    3 Learning User Preference through Retrieval and Aggregation ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits")) | Article: {user-provided article}
    Assume that you need to summarize the above article for a user, who prefers the
    following style: {inferred user preference}. Please write a summary of the above
    article to address those specified preferences. | Notes: {user-provided notes}
    These notes are written by a user who prefers the following style of emails: {inferred
    user preference}. Please write a short email based on the above notes to address
    those specified preferences. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 根据推断的偏好条件的任务提示 ([line 6](#alg1.l6a "在算法 1 中 ‣ CIPHER 的计算成本 ‣ 3 通过检索和汇总学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好来调整LLM代理") 在[算法 1](#alg1a "算法 1 ‣ CIPHER 的计算成本 ‣ 3 通过检索和汇总学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好来调整LLM代理")) | 文章：{user-provided article} 假设您需要为用户总结上述文章，该用户偏好以下风格：{inferred
    user preference}。请写一份总结以满足这些指定的偏好。 | 备注：{user-provided notes} 这些备注由偏好以下风格的用户提供：{inferred
    user preference}。请根据上述备注写一封简短的邮件以满足这些指定的偏好。 |'
- en: '| Prompt to infer user preference based on revision ([line 12](#alg1.l12 "In
    Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") in[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of
    CIPHER. ‣ 3 Learning User Preference through Retrieval and Aggregation ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits")) | Original summary
    of an article: {agent-generated summary} Revised summary by a user: {user revision}'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '| 根据修订推断用户偏好的提示 ([line 12](#alg1.l12 "在算法 1 中 ‣ CIPHER 的计算成本 ‣ 3 通过检索和汇总学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好来调整LLM代理") 在[算法 1](#alg1a "算法 1 ‣ CIPHER 的计算成本 ‣ 3 通过检索和汇总学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好来调整LLM代理")) | 文章的原始摘要：{agent-generated summary} 用户修订的摘要：{user
    revision}'
- en: 'Based on the edits and revision by this user on the original summary in the
    above examples, what do you find about this user’s generic preference in terms
    of writing style and formatting? Please answer in a short phrase and only recommend
    those preferences that are widely used. | Original email: {agent-generated email}
    Revised email: {user revision}'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户对上述示例中原始摘要的编辑和修订，您发现该用户在写作风格和格式方面的通用偏好是什么？请用简短的短语回答，仅推荐那些被广泛使用的偏好。 | 原始邮件：{agent-generated
    email} 修订邮件：{user revision}
- en: Based on the edits and revision by this user on the original email in the above
    examples, what do you find about this user’s generic preference in terms of writing
    style and formatting? Please answer in a short phrase and only recommend those
    preferences that are widely used. |
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户对上述示例中原始邮件的编辑和修订，您发现该用户在写作风格和格式方面的通用偏好是什么？请用简短的短语回答，仅推荐那些被广泛使用的偏好。 |
- en: '| Prompt to consolidate inferred preferences from history ([line 5](#alg1.l5a
    "In Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") in[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of
    CIPHER. ‣ 3 Learning User Preference through Retrieval and Aggregation ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits")) | List of user preferences
    successfully being used to generate summaries of similar documents: - {inferred
    preference in a retrieved example}'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '| 汇总从历史中推断出的偏好 ([line 5](#alg1.l5a "在算法1 ‣ CIPHER的计算成本 ‣ 通过检索和聚合学习用户偏好 ‣ 通过学习用户编辑中的潜在偏好来对齐LLM代理")
    在[算法1](#alg1a "算法1 ‣ CIPHER的计算成本 ‣ 通过检索和聚合学习用户偏好 ‣ 通过学习用户编辑中的潜在偏好来对齐LLM代理")) |
    使用成功生成类似文档总结的用户偏好列表： - {从检索示例中推断出的偏好}'
- en: '- {inferred preference in a retrieved example}'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '- {从检索示例中推断出的偏好}'
- en: …
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Based on the the above examples, please come up with short phrase with the
    most represented summarization preferences of the user. | List of user preferences
    successfully being used to generate emails of a similar kind: - {inferred preference
    in a retrieved example}'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述示例，请提出一个简短的短语，体现该用户最主要的总结偏好。 | 用于生成类似邮件的用户偏好列表： - {从检索示例中推断出的偏好}
- en: '- {inferred preference in a retrieved example}'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '- {从检索示例中推断出的偏好}'
- en: …
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: Based on the the above examples, please come up with short phrase with the most
    represented writing preferences of this user. |
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述示例，请提出一个简短的短语，体现该用户的主要写作偏好。|
- en: 'Table 8: Prompt templates for the ICL-edit baseline.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：ICL-edit基线的提示模板。
- en: '|  | Summarization | Email Writing |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 邮件写作 |'
- en: '| Prompt with retrieved user edit examples | Original summary of an article:
    {agent-generated summary in a retrieved example} Revised summary by a user: {user
    revision in a retrieved example}'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '| 带有检索用户编辑示例的提示 | 文章的原始总结：{从检索示例中生成的代理总结} 用户修订的总结：{从检索示例中用户修订的内容}'
- en: 'Original summary of an article: {agent-generated summary in a retrieved example}'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的原始总结：{从检索示例中生成的代理总结}
- en: 'Revised summary by a user: {user revision in a retrieved example}'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 用户修订的总结：{从检索示例中用户修订的内容}
- en: …
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Article: {user-provided article}'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：{用户提供的文章}
- en: 'Based on the edits and revision by this user on the original summary in the
    above examples, Please summarize the above article: | Original summary of an article:
    {agent-generated summary in a retrieved example} Revised summary by a user: {user
    revision in a retrieved example}'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 基于该用户在上述示例中对原始总结的编辑和修订，请总结上述文章： | 文章的原始总结：{从检索示例中生成的代理总结} 用户修订的总结：{从检索示例中用户修订的内容}
- en: 'Original summary of an article: {agent-generated summary in a retrieved example}'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的原始总结：{从检索示例中生成的代理总结}
- en: 'Revised summary by a user: {user revision in a retrieved example}'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 用户修订的总结：{从检索示例中用户修订的内容}
- en: …
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Notes: {user-provided notes}'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：{用户提供的备注}
- en: 'Based on the edits and revision by this user on the original email in the above
    examples, Please write an email based on the above notes for this user: |'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 基于该用户在上述示例中对原始邮件的编辑和修订，请根据以上笔记为该用户撰写一封邮件：|
- en: 'Table 9: Expense of different methods: number of BPE tokens in terms of input,
    output and total. Each number is the average across 3 runs (unit is $\cdot 10^{5}$).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：不同方法的开销：输入、输出和总共的BPE token数。每个数字是3次运行的平均值（单位是 $\cdot 10^{5}$）。
- en: '| Method | Summarization | Email Writing |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 | 邮件写作 |'
- en: '|  | Input | Output | Total | Input | Output | Total |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 输出 | 总计 | 输入 | 输出 | 总计 |'
- en: '| Oracle Preference | 1.14 | 0.53 | 1.67 | 0.91 | 0.71 | 1.62 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 预测偏好 | 1.14 | 0.53 | 1.67 | 0.91 | 0.71 | 1.62 |'
- en: '| No Learning | 1.06 | 0.44 | 1.50 | 0.85 | 0.80 | 1.65 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 无学习 | 1.06 | 0.44 | 1.50 | 0.85 | 0.80 | 1.65 |'
- en: '| E-then-e LPI | 1.16 | 0.83 | 1.99 | 0.94 | 0.79 | 1.73 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| E-then-e LPI | 1.16 | 0.83 | 1.99 | 0.94 | 0.79 | 1.73 |'
- en: '| Continual LPI | 8.14 | 0.75 | 8.89 | 7.89 | 0.73 | 8.63 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 持续LPI | 8.14 | 0.75 | 8.89 | 7.89 | 0.73 | 8.63 |'
- en: '| ICL-edit-5-MPNET | 7.35 | 0.65 | 8.00 | 11.05 | 1.06 | 12.12 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-MPNET | 7.35 | 0.65 | 8.00 | 11.05 | 1.06 | 12.12 |'
- en: '| ICL-edit-5-BERT | 7.32 | 0.64 | 7.96 | 10.51 | 1.03 | 11.55 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-BERT | 7.32 | 0.64 | 7.96 | 10.51 | 1.03 | 11.55 |'
- en: '| CIPHER-1-MPNET | 2.02 | 0.72 | 2.74 | 1.21 | 0.73 | 1.94 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-MPNET | 2.02 | 0.72 | 2.74 | 1.21 | 0.73 | 1.94 |'
- en: '| CIPHER-5-MPNET | 2.27 | 0.73 | 3.00 | 1.44 | 0.64 | 2.09 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-MPNET | 2.27 | 0.73 | 3.00 | 1.44 | 0.64 | 2.09 |'
- en: '| CIPHER-1-BERT | 2.10 | 0.71 | 2.81 | 1.27 | 0.73 | 1.99 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-BERT | 2.10 | 0.71 | 2.81 | 1.27 | 0.73 | 1.99 |'
- en: '| CIPHER-5-BERT | 2.32 | 0.71 | 3.03 | 1.48 | 0.73 | 2.22 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-BERT | 2.32 | 0.71 | 3.03 | 1.48 | 0.73 | 2.22 |'
- en: 'Table 10: Summary of failure cases on summarization task with CIPHER-5-MPNET.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：CIPHER-5-MPNET总结任务中的失败案例总结。
- en: '| Type of Failures | Summary | Examples |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 失败类型 | 总结 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Preference inference based on an output-revision pair ($f_{t}$) (the most
    common failure type) | 1) Not totally wrong but insufficient, i.e. the inferred
    preference only captures a few aspects of user’s latent preference. This is most
    common for news articles and Reddit posts, for which the user shows nuanced preference
    for several aspects. | The dominant missing aspect is brief or short sentences
    across different context, although the agent can infer keywords such as concise.
    For news article context, the agent tends to infer the preference keyword whimsical.
    The agent has difficulty to infer subtle aspects, including invoke personal reflection,
    immersive, positive, parallel structure, inquisitive, and skillful foreshadowing.
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 基于输出修订对的偏好推断 ($f_{t}$)（最常见的失败类型） | 1) 并非完全错误，但不够充分，即推断出的偏好仅捕捉了用户潜在偏好的几个方面。这在新闻文章和Reddit帖子中最为常见，因为用户对多个方面显示出微妙的偏好。
    | 主导的缺失方面是不同上下文中的简短或短句，尽管代理可以推断出如“简洁”等关键词。在新闻文章上下文中，代理倾向于推断出“异想天开”的偏好关键词。代理在推断微妙方面时存在困难，包括引发个人反思、沉浸感、积极、平行结构、好奇心和巧妙的铺垫。
    |'
- en: '|  | 2) Sometimes fail to infer some important aspects, even though the user
    edits clearly show such preference. | The agent often could not infer second-person
    narrative. For question answering style, the agent occasionally only learns consistent
    format. |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | 2) 有时无法推断出一些重要方面，即使用户编辑清楚地显示了这样的偏好。 | 代理通常无法推断出第二人称叙述。对于问答风格，代理有时仅学习一致的格式。
    |'
- en: '| Consolidation of induced preferences from retrieved interactions ($\tilde{f}_{t}$)
    | Overall, this step can capture the majority preference relatively well, although
    it tends to result in a more general preference compared to the retrieved ones.
    | When both specific phrase second-person narrative and general phrase narrative
    or narration occur in retrieved examples, the agent often chooses to give a final
    preference not including the second-person perspective aspect. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 从检索交互中诱导的偏好的整合 ($\tilde{f}_{t}$) | 总体而言，这一步可以相对较好地捕捉到主要偏好，尽管与检索到的偏好相比，结果往往更为一般。
    | 当检索示例中同时出现具体的第二人称叙述和一般的叙述或叙事时，代理经常选择给出一个不包含第二人称视角方面的最终偏好。 |'
- en: '| Retrieval of historical examples relevant to the given context | The retrieval
    part in general works reasonably well, with more than half of the retrieved example
    being truly relevant to the given context. Note that one incorrect retrieved example
    typically does not affect the performance, as we instruct the agent to only use
    the most represented preference keywords among all five retrieved examples. |
    The agent sometimes retrieves wrong examples for Wikipedia context when its topic
    very relates to other context, e.g. wrongly retrieving past examples on news articles
    and movie reviews when the topic in the given Wikipedia context relates to these
    domains. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 相关历史示例的检索 | 总体而言，检索部分表现合理，超过一半的检索示例与给定上下文确实相关。请注意，一个不正确的检索示例通常不会影响性能，因为我们指示代理只使用五个检索示例中最具代表性的偏好关键词。
    | 代理有时在维基百科上下文中检索错误示例，当其主题与其他上下文密切相关时，例如，在给定的维基百科上下文中，主题与新闻文章和电影评论相关时错误地检索过去的示例。
    |'
- en: 'Table 11: We report retrieval accuracy as the percentage of total retrieved
    document representations across all time steps and seeds that are of the same
    document source type as the context document for which they were retrieved. We
    use 3 seeds. We retrieve 600 examples for $k=1$.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：我们报告了检索准确率，作为所有时间步骤和种子中总检索文档表示的百分比，这些文档源类型与它们被检索的上下文文档相同。我们使用了3个种子。我们为$k=1$检索了600个示例。
- en: '| Method | Summarization | Email Writing |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 | 邮件写作 |'
- en: '| --- | --- | --- |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CIPHER-1-B | 72.00 | 25.83 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-B | 72.00 | 25.83 |'
- en: '| CIPHER-1-M | 82.00 | 26.33 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-M | 82.00 | 26.33 |'
- en: '| CIPHER-5-B | 65.79 | 26.57 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-B | 65.79 | 26.57 |'
- en: '| CIPHER-5-M | 76.33 | 25.45 |</path></g></g></g></g></path></g></g></g></g></path></g></g></g></foreignobject></g></g></g></g></g></svg>'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '| CIPHER-5-M | 76.33 | 25.45 |</path></g></g></g></g></path></g></g></g></g></path></g></g></g></foreignobject></g></g></g></g></g></svg>'
