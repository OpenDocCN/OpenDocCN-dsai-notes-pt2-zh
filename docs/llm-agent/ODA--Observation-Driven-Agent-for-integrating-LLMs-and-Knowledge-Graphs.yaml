- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07677](https://ar5iv.labs.arxiv.org/html/2404.07677)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lei Sun¹¹1Equal contribution²²2Corresponding author¹ Zhengwei Tao¹¹1Equal contribution
    ² Youdi Li ¹ Hiroshi Arakawa ¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Panasonic Connect Co., Ltd., Japan
  prefs: []
  type: TYPE_NORMAL
- en: ²Peking University
  prefs: []
  type: TYPE_NORMAL
- en: tttzw@stu.pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '{lei.sun, ri.yutei, arakawa.hrs}@jp.panasonic.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has
    achieved remarkable success in various natural language processing tasks. However,
    existing methodologies that integrate LLMs and KGs often navigate the task-solving
    process solely based on the LLM’s analysis of the question, overlooking the rich
    cognitive potential inherent in the vast knowledge encapsulated in KGs. To address
    this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework
    tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via
    global observation that enhances reasoning capabilities through a cyclical paradigm
    of observation, action, and reflection. Confronting the exponential explosion
    of knowledge during observation, we innovatively design a recursive observation
    mechanism. Subsequently, we integrate the observed knowledge into the action and
    reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art
    performance on several datasets, notably achieving accuracy improvements of 12.87%
    and 8.9%. Our code and data are available on [ODA](https://github.com/lanjiuqing64/KGdata).
  prefs: []
  type: TYPE_NORMAL
- en: 'ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs'
  prefs: []
  type: TYPE_NORMAL
- en: Lei Sun¹¹1Equal contribution²²2Corresponding author¹ Zhengwei Tao¹¹1Equal contribution
    ² Youdi Li ¹ Hiroshi Arakawa ¹ ¹Panasonic Connect Co., Ltd., Japan ²Peking University
    tttzw@stu.pku.edu.cn {lei.sun, ri.yutei, arakawa.hrs}@jp.panasonic.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) Touvron et al. ([2023](#bib.bib22)); Scao et al.
    ([2022](#bib.bib18)); Muennighoff et al. ([2022](#bib.bib11)); Brown et al. ([2020](#bib.bib2))
    have exhibited extraordinary capabilities across a variety of natural language
    processing tasks. Despite their impressive accomplishments, LLMs often struggle
    to provide accurate responses to queries that necessitate specialized expertise
    beyond their pre-training content. In response to this limitation, a natural and
    promising approach involves the integration of external knowledge sources, such
    as knowledge graphs (KGs), to augment LLM reasoning abilities. KGs provide structured,
    explicit, and explainable knowledge representations, offering a synergistic method
    to overcome the intrinsic constraints of LLMs. The fusion of LLMs with KGs has
    garnered significant interest in recent research Pan et al. ([2024](#bib.bib14)),
    underlying a vast array of applications Zhang et al. ([2023](#bib.bib29)); Do
    et al. ([2024](#bib.bib4)); Sun et al. ([2023b](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6414bb47c9995d34f21fcea33dfa3f46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example of LLM integrating with KG. Observed entities are shown
    in white, while non-observed entities are displayed in gray. Entities selected
    by the agent to answer the question are highlighted in yellow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing methodologies for solving tasks that integrate KGs with LLMs can be
    categorized into two groups. The first one involves retrieving relevant triples
    from KGs in response to specific questions Wang et al. ([2023b](#bib.bib24));
    Luo et al. ([2024](#bib.bib10)); Jiang et al. ([2023](#bib.bib8)). The second
    part adopts an explore-exploit strategy, directing the knowledge utilization process
    within the graph according to the question Sun et al. ([2023b](#bib.bib20)); Guo
    et al. ([2023](#bib.bib7)). However, both categories navigate the task-solving
    process by merely relying on the LLM’s analysis of the question, overlooking the
    rich cognitive potential inherent in the abundant knowledge encapsulated in KGs.
    KGs, which store a wealth of informative and symbolic facts, should deeply participate
    in the reasoning process together with LLM rather than being merely treated as
    a static repository of knowledge Pan et al. ([2024](#bib.bib14)). As the example
    in the upper panel of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs"), LLM analyzes the question and
    navigates towards Narrative location relation of entity The Call of The Wild.
    However, this entity has many neighboring entities with that relation, leading
    LLM to incorrectly infer Canada as the answer. In contrast, the bottom panel demonstrates
    how KG provides key patterns that reveal both The Call of The Wild and White Fang
    share the location Yukon. If LLM could observe this information beforehand, it
    would precisely guide its reasoning process towards the correct answer (as shown
    in the bottom panel). Therefore, LLM should adopt an overall observation to incorporate
    the extensive knowledge and intricate patterns embedded within the KG. Achieving
    this objective presents two primary challenges: firstly, a global observation
    of the KG can result in an exponential growth in the number of triples. As shown
    in the upper panel of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs"), fully processing all 3-hop
    connections for The Call of the Wild is impractical. Secondly, the integration
    of such comprehensive observation into the existing reasoning paradigms of LLMs
    presents another challenge. How to combine the observation with the reasoning
    process of LLM matters for solving the tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by this, we introduce a novel framework, the Observation-Driven Agent
    (ODA), aimed at sufficiently and autonomously integrating the capabilities of
    both LLM and KG. ODA serves as an AI Agent specifically designed for KG-centric
    tasks. ODA engages in a cyclical paradigm of observation, action, and reflection.
    Within ODA, we design a novel observation module to efficiently draw autonomous
    reasoning patterns of KG. Our observation module avoids the problem of exponential
    growth of triples via recursive progress. This approach ensures ODA integrating
    abilities of KG and LLM while mitigating the challenges associated with excessive
    data in KG, improving the efficiency and accuracy. Following the observation phase,
    ODA takes action by autonomously amalgamating insights derived from LLM inferences
    with the observed KG patterns. ODA can perform actions of three distinct types:
    Neighbor Exploration, Path Discovery, and Answering. Subsequently, ODA reflects
    on its internal state, considering both the outcomes of its actions and the prior
    observations. This iterative process continues until ODA accomplishes the task
    at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct extensive experiments to testify to the effectiveness of ODA on
    four datasets: QALD10-en, T-REx, Zero-Shot RE and Creak. Notably, our approach
    achieved state-of-the-art (SOTA) performance compared to competitive baselines.
    Specifically, on QALD10-en and T-REx datasets, we observed remarkable accuracy
    improvements of 12.87% and 8.9%, respectively. We conclude the contributions as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose ODA, an AI Agent tailored for KG-centric tasks. ODA conducts observation
    to incorporate the reasoning ability of KG.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We further design action and reflection modules that integrate the observation
    into LLM reasoning. This strategy leverages the synergy between the autonomous
    reasoning of KG and LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct experiments on four datasets and achieve SOTA performances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1de2640486dbda690f9dc685ad6d7406.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The overall framework of ODA.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, we aim to solve tasks associated with KG. Let $q$ can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $T:(q,E),G\rightarrow Y$ |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/eb459b20c486f113520bb872d5b93cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An example workflow of ODA. In this case, ODA initiates the obervation
    with entity Johann Wolfgang von Goethe. During the first iteration on the left
    side, the Neighbor Exploration of Johann Wolfgang von Goethe is selected, and
    the reflected triple (Johann Wolfgang von Goethe, unmarried Partner, Lili Schöneman)
    is stored in memory. Subsequently, The observation of Lili Schöneman then guides
    ODA to choose Neighbor Exploration action, and leads to the retention of the triple
    (Lili Schöneman, place of birth, Offenbach am Main) in memory, as shown on the
    right side. Once sufficient knowledge has been accumulated, ODA triggers the Answer
    action, correctly identifying Offenbach am Main as the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Employing an iterative approach, ODA tackles the challenges inherent in KG-centric
    tasks. In contrast to existing methods that couple LLMs and KGs and rely solely
    on analyzing the LLM’s query, ODA autonomously integrates observed knowledge from
    the KG into the entire reasoning process, resulting in more informed decisions.
    To achieve this objective, our ODA system, illustrated in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge
    Graphs"), primarily comprises three key modules for task resolution:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Observation: This module efficiently observes and processes relevant knowledge
    from the KG environment. In each iteration $i$). By leveraging insights and patterns
    gleaned from the KG, this subgraph is autonomously incorporated into a reasoning
    LLM. This synergistic integration equips ODA with enhanced capabilities from both
    the LLM and KG, allowing it to tackle tasks more effectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action: Drawing upon both the observation subgraph $O_{i}$, strategically selects
    the most suitable action to execute on the KG, ensuring the accurate answering
    of the question.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reflection: Utilizing the observation subgraph $O_{i}$ for the next iteration,
    facilitating continuous reasoning.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Through this iterative process, ODA dynamically updates its observation subgraph
    $O_{i}$. Each module is discussed in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Observation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The observation module is specifically designed to inspect global KG knowledge
    and navigate the autonomous reasoning process with the KG environments. At each
    iteration $i$. This process can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $O_{i}=\text{Observation}([E_{i},q])$ |  |'
  prefs: []
  type: TYPE_TB
- en: Initially, the task-relevant entities are populated with the entities embedded
    within the question $q$.
  prefs: []
  type: TYPE_NORMAL
- en: 'For KG-centric tasks, the observation incurs the problem of an explosive number
    of nodes. To address the scalability challenge during observation subgraph updates,
    we propose a $D$ represents the maximum hop depth. Each turn has two steps: update
    and refine. The update step focuses on expanding the subgraph, while the refining
    step ensures its appropriate size without loss of important information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each entity $e\in E_{i}$ is reached. The specific details are described
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Observation
  prefs: []
  type: TYPE_NORMAL
- en: 'Require: Question $q$'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize task-relevant entities $E_{i}$     end whileend for
  prefs: []
  type: TYPE_NORMAL
- en: 'Update:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each entity $e$ denotes the tail entity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The similarity score between the question and the combination of $r$, is computed
    by measuring the cosine similarity of their embeddings¹¹1We use the GPT text-embedding-ada-002
    model from OpenAI for encoding.:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{Cosine Similarity}(\mathbf{v}_{q},\mathbf{v}_{r+t})=\frac{\mathbf{v}_{q}\cdot\mathbf{v}_{r+t}}{\&#124;\mathbf{v}_{q}\&#124;\&#124;\mathbf{v}_{r+t}\&#124;}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All triples associated with entities in $E$ are collectively sorted in descending
    order based on their similarity scores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Top-$N$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Refine:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Top-$N$ with the highest similarity scores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tail entities from the refined top $P\%$ for the next iteration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 Action
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Harnessing the power of an LLM, the action module crafts strategic prompts to
    generate optimal actions. Based on its memory $M_{<i}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $a_{i}=\text{Action}([O_{i},a_{<i},M_{<i}])$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We propose three core actions designed to empower ODA :'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neighbor Exploration: This action explores the KG neighborhood of the entity
    in task-relevant entities $E_{i}$, retrieving all neighboring triples. This helps
    build context and understand interconnectedness within the KG for ODA .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Path Discovery: Given two entities in task-relevant entities $E_{i}$, this
    action searches for all possible paths connecting them. Each path consists of
    interconnected triples, allowing the ODA to explore various connections and potentially
    uncover hidden relationships.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: This action responds to the question only if the required information
    is present in memory $M_{<i}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Upon selecting an answer action, ODA halts the iterative loop of observation,
    action, and reflection. Leveraging the reliable knowledge within memory $M_{<i}$,
    it can then directly formulate the answer to the question. Alternatively, if a
    Neighbor Exploration or Path Discovery action is selected, ODA strategically extracts
    relevant knowledge from the KG as a set of triples. These extracted triples are
    then fed into the subsequent reflection step for further processing.The prompt
    used here can be found in Table [7](#A2.T7 "Table 7 ‣ Appendix B Prompt ‣ ODA:
    Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Reflection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The reflection module plays a crucial role in evaluating the triples generated
    from the action step and subsequently updating ODA memory $M_{i}$, the reflection
    module provides invaluable feedback, guiding future decision-making. This process
    can be formalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M_{i}=\text{Reflection}([O_{i},a_{i},M_{<i}])$ |  |'
  prefs: []
  type: TYPE_TB
- en: Given that memory $M_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, the tail entities in the reflected triples are designated as
    the task-relevant entities for the next iteration. The specific prompt description
    used for the reflection module is provided in Table [8](#A2.T8 "Table 8 ‣ Appendix
    B Prompt ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The observation, action, and reflection modules collaborate in an iterative
    process until either the Answer action is triggered or the maximum iteration limit
    is reached. Figure [3](#S2.F3 "Figure 3 ‣ 2 Methods ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs") depicts an example of how observation,
    action, and reflection work together.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Dataset | Test | Entity | Type | License |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| QALD10-en | 333 | 396 | Multi-hop | MIT License |'
  prefs: []
  type: TYPE_TB
- en: '| T-REx | 5000 | 4943 | Slot-Filling | MIT License |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-Shot RE | 3724 | 3657 | Slot Filling | MIT License |'
  prefs: []
  type: TYPE_TB
- en: '| Creak | 1371 | 516 | Fact Checking | MIT License |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Dataset statistics. Entity stands for the entity size derived from
    all the question within the datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the performance of our ODA , we conduct experiments on four KBQA
    datasets: QALD10-en Perevalov et al. ([2022](#bib.bib15)),Creak Onoe et al. ([2021](#bib.bib12)),
    T-REx Elsahar et al. ([2018](#bib.bib5)), and Zero-Shot RE Petroni et al. ([2021](#bib.bib16)).
    Detailed specifications for each dataset are provided in Table [1](#S3.T1 "Table
    1 ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge
    Graphs"). The Hits@1Sun et al. ([2019](#bib.bib21)) accuracy with exact match
    is utilized as our evaluation metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We utilized the GPT-4 OpenAI ([2023](#bib.bib13)) model as the ODA via the OpenAI
    API. Throughout our experiments, we consistently configured the temperature value
    of GPT-4 to 0.4 and set the maximum token length to 500.
  prefs: []
  type: TYPE_NORMAL
- en: For the observation step, we tuned key parameters based on preliminary experiments.
    we set $P_{t}$ to 15.
  prefs: []
  type: TYPE_NORMAL
- en: To establish Wikidata KG database and retrieve information from it, we employed
    the simple-wikidata-db¹¹1https://github.com/neelguha/simple-wikidata-db Python
    library. This library provides various scripts for downloading the Wikidata dump,
    organizing it into staging files, and executing distributed queries on the data
    within these staged files. Specifically, we deployed the Wikidata dump across
    five AWS EC2 instances, each consisting of a 768GB machine with 48 cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that our ODA relies heavily on continuous interaction with the
    KG, we discovered that the real-time extraction of required Wikidata knowledge
    on AWS achieved an average completion time of 50 seconds per question-answer pair
    within QALD10-en dataset. However, as the KBQA dataset expanded, the cost of using
    the Wikidata database on AWS became prohibitively expensive. Consequently, to
    address the computational expenses involved, we devised a solution by generating
    an offline subgraph for each KBQA dataset. This offline subgraph captures all
    the triples within a 3-hop radius of the entities in each dataset, including the
    properties of both the entities and the relations involved. Notably, generating
    such a subgraph for the T-REx dataset, with its 4943 entities (as listed in Table
    [1](#S3.T1 "Table 1 ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs")), takes approximately 54 minutes and 42.834 seconds
    in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Baseline Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To comprehensively evaluate ODA effectiveness, we conduct a rigorous benchmark
    against several SOTA models across diverse categories. The comparison encompasses
    various models, starting with prompt-based approaches that do not utilize external
    knowledge. These include direct answering with GPT-3.5 and GPT-4, as well as the
    Self-Consistency Wang et al. ([2023c](#bib.bib25)) and CoT Sun et al. ([2023b](#bib.bib20)).
    On the other hand, Kownledge-combined models are considered, which incorporate
    fine-tuned techniques such as SPARQL-QA Santana et al. ([2022](#bib.bib17)), RACo
    Yu et al. ([2022](#bib.bib28)), RAG Petroni et al. ([2021](#bib.bib16)) and Re2G
    Glass et al. ([2022](#bib.bib6)). Additionally, there is ToG Sun et al. ([2023a](#bib.bib19))
    model, which integrates LLM with KG to bolster question-answering proficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Main Result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our ODA method outperforms existing methods, as shown in Table [2](#S3.T2 "Table
    2 ‣ 3.4 Main Result ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs"). On average, our method achieves an accuracy gain
    of up to 19.58% compared to direct answering with GPT-4, 19.28% compared to fine-tuned
    models, and 7.09% compared to TOG. These results demonstrate the efficiency and
    effectiveness of our method in comparison to other state-of-the-art methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, our ODA significantly outperforms the prompt-based methods across
    various datasets, particularly showing an improvement of 65.50% and 23.77% on
    Zero-Shot REx and QALD10-en , respectively. These results underscore the importance
    of leveraging external knowledge graphs for reasoning and completing the question-answering
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the fine-tuned method, our ODA method demonstrates superior performance.
    Specifically, our method achieves a performance gain of 21.27% for the QALD10-en
    dataset, 6.99% for the Creak dataset, and 50.56% for the Zero-Shot RE dataset.
    Notably, this interaction between the LLM and KG, as our method employs, proves
    more effective than data-driven fine-tuned techniques, despite requiring no explicit
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Our ODA method exhibits significant performance gains over the ToG method across
    most datasets, with improvements of 12.87% (QALD10-en), 8.9% (T-REx), and 7% (Zero-Shot
    RE), despite both methods leveraging large language models and knowledge graphs.
    This performance disparity highlights the critical role of our observation module
    and the effectiveness of autonomously incorporating reasoning from KG. Specifically,
    our method demonstrates significantly stronger performance on the QALD10-en dataset,
    known for its multi-hop and complex reasoning requirements. This achievement underscores
    our ODA ability to effectively exploit the rich knowledge and patterns within
    KG, seamlessly combining the autonomous reasoning strengths of both LLM and KG
    to tackle complex questions successfully.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | QALD10-en | Creak | T-REx | Zero-Shot RE | Average |'
  prefs: []
  type: TYPE_TB
- en: '| w.o. Knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| Direct answering(GPT3.5) | 44.74^((1)) | 90.00^((1)) | 37.78^((1)) | 37.14^((1))
    | 52.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Direct answering(GPT4) | 57.10^((1)) | 94.52^((1)) | 57.72^((1)) | 55.50^((1))
    | 66.21 |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Consistency(GPT3.5)Wang et al. ([2023c](#bib.bib25)) | 45.30^((1)) |
    90.80^((1)) | 41.80^((1)) | 45.40^((1)) | 55.83 |'
  prefs: []
  type: TYPE_TB
- en: '| COT(GPT3.5)Sun et al. ([2023a](#bib.bib19)) | 42.90^((1)) | 90.10^((1)) |
    32.00^((1)) | 28.80^((1)) | 48.45 |'
  prefs: []
  type: TYPE_TB
- en: '| w.t. Knowledge / Fine-tuned |'
  prefs: []
  type: TYPE_TB
- en: '| SOTA | 45.40¹ | 88.20² | 87.70³ | 44.74⁴ | 66.51 |'
  prefs: []
  type: TYPE_TB
- en: '| w.t. Knowledge / Zero-Shot (GPT-4) |'
  prefs: []
  type: TYPE_TB
- en: '| TOG-R Sun et al. ([2023a](#bib.bib19)) | 54.70^((1)) | 95.40^((1)) | 75.50^((1))
    | 86.90^((1)) | 78.13 |'
  prefs: []
  type: TYPE_TB
- en: '| TOG Sun et al. ([2023a](#bib.bib19)) | 53.80^((1)) | 95.60^((1)) | 77.10^((1))
    | 88.30^((1)) | 78.70 |'
  prefs: []
  type: TYPE_TB
- en: '| ODA (Ours) | 66.67^((1)) | 95.19^((1)) | 86.00 ^((1)) | 95.30^((1)) | 85.79
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Performance Comparison of different methods. Bold scores stand for
    best performances among all GPT-based zero-shot methods. The fine-tuned SOTA includes:
    1: SPARQL-QASantana et al. ([2022](#bib.bib17)), 2: RACoYu et al. ([2022](#bib.bib28)),
    3: Re2GGlass et al. ([2022](#bib.bib6)), 4:RAGPetroni et al. ([2021](#bib.bib16)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To better understand the key factors influencing our ODA, we conducted extensive
    analysis experiments. To conserve computational resources, we kept the previously
    mentioned datasets (QALD10-en, Creak, T-REx, and Zero-Shot RE) but randomly sampled
    400 examples each for Creak, T-REx, and Zero-Shot RE.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Effect of Observation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess the efficacy of the observation module, we conducted comprehensive
    experiments with the model without observation. During the action step, ODA selects
    the action only based on the knowledge within memory. Subsequently, the reflection
    step reflects on the triples outputted by the action and updates memory without
    the guide from observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A statistical comparison was performed to evaluate the performance of the ODA with
    and without observation across all datasets (see Table [3](#S4.T3 "Table 3 ‣ 4.1
    Effect of Observation ‣ 4 Discussion ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs")). The results show that the ODA with observation outperforms
    the ODA without observation, with an average improvement of 3.14%. Specifically,
    for QALD10-en dataset, the ODA with observation outperforms the ODA without observation
    by 5.41%. Since QALD10-en involves multi-hop reasoning, the improved performance
    of the ODA with observation indicates that the observation module enhances the
    reasoning ability of the agent, enabling more accurate action selection and reflection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further illustrate the benefits of the observation module with a practical
    case. In this scenario (see Table [5](#A1.T5 "Table 5 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs")),
    question is Where are both The Call of the Wild and White Fang set, the most two
    famous works of Jack London?. Without observation, ODA generated the memory, such
    as (The Call of the Wild, narrative location, Canada), ultimately produced the
    wrong answer of Canada. However, with the observation module, the ODA correctly
    reasons the memory,such as (The Call of the Wild, narrative location, Yukon),
    (White Fang, narrative location, Yukon). As a result, the ODA with observation
    provides the correct answer, Yukon. This case exemplifies how the observation
    module improves the accuracy of action selection and reflection, consequently
    enhancing the reasoning ability of ODA .'
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating observation information, ODA reasoning power undergoes a dramatic
    leap, therefore generate an accurate answers. This boost stems from the synergistic
    interplay between the observation module, harnessing the KG’s autonomous reasoning
    capabilities, and LLM, which further amplifies those strengths.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | QALD10-en | Creak | T-REx | Zero-Shot RE | Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Without Observation | 61.26 | 95.50 | 82.00 | 91.75 | 82.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Similarity-based Reflection | 61.26 | 95.20 | 83.20 | 93.50 | 83.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Random-based Reflection | 58.56 | 89.00 | 79.50 | 92.00 | 79.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Generated-fact Reflection | 63.66 | 91.00 | 80.00 | 93.75 | 82.10 |'
  prefs: []
  type: TYPE_TB
- en: '| ODA | 66.67 | 96.00 | 85.40 | 95.00 | 85.77 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablation Comparison'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Effect of Observation on Reflection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we discuss the impact of observation on reflection module.
    Three non-observation reflection methods were designed to verify whether observation
    can enhance the effectiveness of reflection. The similarity-based involves reflecting
    on the triples from action steps by calculating similarity. In this approach,
    triples are first sorted based on the similarity score between the $r+t$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.1 Effect of Observation ‣ 4 Discussion ‣ ODA:
    Observation-Driven Agent for integrating LLMs and Knowledge Graphs") showcases
    our ODA dominance over all three non-observation methods. It achieved an average
    accuracy increase of 2.48% compared to the similarity-based method, 6.00% compared
    to the random-based method, and 3.66% compared to the generated-fact method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In specific scenarios (see Table [6](#A1.T6 "Table 6 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs")),
    when answering the question What is the capital of the prefecture Tokyo?, the
    generated-fact method resulted in problematic facts, such as Tokyo is the capital
    of Tokyo,Tokyo is the capital of Japan. These were essentially hallucinations
    created by the LLM based on the given question. They misled the agent resulting
    in incorrect answers. In contrast, the reflection of our ODA leveraging observation
    yielded factual knowledge, (Tokyo, capital, Shinjuku), consequently enabling the
    ODA to answer the question correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The findings of Table [3](#S4.T3 "Table 3 ‣ 4.1 Effect of Observation ‣ 4 Discussion
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs") reveal
    that observation enables reflection module to generate more accurate memories,
    which translates to improved question-answering accuracy for ODA. This result
    underscores the value of both leveraging KG autonomous reasoning capabilities
    and fostering deep collaboration between KG and LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Performance across Different Backbone Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluated the effectiveness of ODA across various backbones by analyzing
    its impact on T-REx and QALD10-en datasets. Our ODA approach significantly outperformed
    the direct answering methods using GPT-3.5 and GPT-4, as evidenced by the Table
    [4](#S4.T4 "Table 4 ‣ 4.3 Performance across Different Backbone Models ‣ 4 Discussion
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs"). Notably,
    ODA yielded a remarkable 30.4% improvement with the direct answering using GPT-3.5
    model on QALD10-en dataset. This experiment suggests the generalizability of ODA across
    different LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | T-REx | QALD10-en |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Direct answering(GPT3.5) | 37.60 | 44.74 |'
  prefs: []
  type: TYPE_TB
- en: '| ODA (GPT3.5) | 68.00 | 49.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Direct answering(GPT4) | 57.44 | 57.10 |'
  prefs: []
  type: TYPE_TB
- en: '| ODA (GPT4) | 86.00 | 66.67 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison using different backbone models'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KG-enhanced LLM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Knowledge Graph-enhanced Language Models utilize two primary methodologies when
    tackling tasks that require integration with KGs. The first involves the extraction
    of relevant triples from KGs in response to posed questions. Wang et al. ([2023b](#bib.bib24))
    prompt LLMs to generate explicit knowledge evidence structured as triples, while
    Jiang et al. ([2023](#bib.bib8)) develop specialized interfaces for gathering
    pertinent evidence from structured data, enabling LLMs to focus on reasoning tasks
    based on this information. Similarly, Luo et al. ([2024](#bib.bib10)) employ a
    planning-retrieval-reasoning framework to generate relation paths grounded by
    KGs, thereby enhancing the reasoning capabilities of LLMs. Baek et al. ([2023](#bib.bib1))
    retrieve facts related to the input question by assessing semantic similarities
    between the question and associated facts, subsequently prepending these facts
    to the input. Meanwhile, Li et al. ([2023](#bib.bib9)) iteratively refine reasoning
    rationales by adapting knowledge from the KG. Wang et al. ([2023a](#bib.bib23))
    dissect complex questions using predefined templates, retrieve entities from the
    KG, and generate answers accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach employs an explore-exploit strategy that guides the knowledge
    utilization process within the graph. Sun et al. ([2023b](#bib.bib20)) perform
    an iterative beam search on the KG to identify the most promising reasoning pathways
    and report the outcomes. Guo et al. ([2023](#bib.bib7)) selectively accumulate
    supporting information from the KG through an iterative process that incorporates
    insights from the LLM to address the question.
  prefs: []
  type: TYPE_NORMAL
- en: AI Agent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the domain of AI agents, Yao et al. ([2022](#bib.bib27)) utilize LLMs to
    interleave the generation of reasoning traces with task-specific actions. Wu et al.
    ([2023](#bib.bib26)) design an agent framework that is adaptable, conversational,
    and capable of operating in various modes that leverage combinations of LLMs,
    human input, and auxiliary tools. Chen et al. ([2023](#bib.bib3)) focus on creating
    expert agents capable of solving complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we design ODA framework for KG-centric tasks. In ODA, we introduce
    KG observation mechanism to autonomously combine the reasoning abilities of KG
    with LLM. We first propose the observation method to mitigate the problem of explosive
    number of triples in KG when tackling complex tasks. Then we fuse the observation
    into the action and reflection modules to further enhance the overall performance.
    We conduct extensive experiments, and the results clearly illustrate the effectiveness
    of our framework, highlighting its capability to enhance performance across four
    KBQA datasets, particularly in handling complicated questions.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the diverse nature of KG-related tasks spanning multiple domains and requiring
    a broad range of capabilities, the implementation of a multi-agent system is promising
    to significantly enhance task performance. We leave the integration of our observation
    mechanism with multi-agent system to future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Baek et al. (2023) Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-augmented
    language model prompting for zero-shot knowledge graph question answering. *arXiv
    preprint arXiv:2306.04136*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay,
    Börje F Karlsson, Jie Fu, and Yemin Shi. 2023. Autoagents: A framework for automatic
    agent generation. *arXiv preprint arXiv:2309.17288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Do et al. (2024) Quyet V Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, and
    Yangqiu Song. 2024. Constraintchecker: A plugin for large language models to reason
    on commonsense knowledge bases. *arXiv preprint arXiv:2401.14003*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsahar et al. (2018) Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe
    Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. [T-REx:
    A large scale alignment of natural language with knowledge base triples](https://aclanthology.org/L18-1544).
    In *Proceedings of the Eleventh International Conference on Language Resources
    and Evaluation (LREC 2018)*, Miyazaki, Japan. European Language Resources Association
    (ELRA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Glass et al. (2022) Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury,
    Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. [Re2G: Retrieve, rerank, generate](https://doi.org/10.18653/v1/2022.naacl-main.194).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2701–2715,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li,
    Jiawei Tang, Dapeng Li, and Yingyou Wen. 2023. Knowledgenavigator: Leveraging
    large language models for enhanced reasoning over knowledge graph. *arXiv preprint
    arXiv:2312.15880*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language
    model to reason over structured data. *arXiv preprint arXiv:2305.09645*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq
    Joty, Soujanya Poria, and Lidong Bing. 2023. Chain-of-knowledge: Grounding large
    language models via dynamic knowledge adapting over heterogeneous sources. *arXiv
    preprint arXiv:2305.13269*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2024) Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui
    Pan. 2024. [Reasoning on graphs: Faithful and interpretable large language model
    reasoning](http://arxiv.org/abs/2310.01061).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask
    finetuning. *arXiv preprint arXiv:2211.01786*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Onoe et al. (2021) Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg
    Durrett. 2021. [Creak: A dataset for commonsense reasoning over entity knowledge](http://arxiv.org/abs/2109.01653).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) R OpenAI. 2023. Gpt-4 technical report. *arXiv*, pages 2303–08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2024) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. 2024. Unifying large language models and knowledge graphs: A roadmap.
    *IEEE Transactions on Knowledge and Data Engineering*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perevalov et al. (2022) Aleksandr Perevalov, Dennis Diefenbach, Ricardo Usbeck,
    and Andreas Both. 2022. [Qald-9-plus: A multilingual dataset for question answering
    over dbpedia and wikidata translated by native speakers](https://doi.org/10.1109/ICSC52841.2022.00045).
    In *2022 IEEE 16th International Conference on Semantic Computing (ICSC)*, pages
    229–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Petroni et al. (2021) Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
    Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
    Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.
    [KILT: a benchmark for knowledge intensive language tasks](https://doi.org/10.18653/v1/2021.naacl-main.200).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2523–2544,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santana et al. (2022) Manuel Alejandro Borroto Santana, Bernardo Cuteri, Francesco
    Ricca, and Vito Barbara. 2022. [SPARQLQA enters the QALD challenge](https://ceur-ws.org/Vol-3196/paper3.pdf).
    In *Proceedings of the 7th Natural Language Interfaces for the Web of Data (NLIWoD)
    co-located with the 19th European Semantic Web Conference (ESWC 2022)*, volume
    3196 of *CEUR Workshop Proceedings*, pages 25–31, Hersonissos, Greece. CEUR-WS.org.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023a) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2023a. [Think-on-graph:
    Deep and responsible reasoning of large language model on knowledge graph](http://arxiv.org/abs/2307.07697).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023b) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023b. Think-on-graph: Deep
    and responsible reasoning of large language model with knowledge graph. *arXiv
    preprint arXiv:2307.07697*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang.
    2019. [Rotate: Knowledge graph embedding by relational rotation in complex space](http://arxiv.org/abs/1902.10197).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen,
    Xinrun Wang, Lei Feng, and Bo An. 2023a. keqing: knowledge-based question answering
    is a nature chain-of-thought mentor of llm. *arXiv preprint arXiv:2401.00426*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and Ming Gao.
    2023b. Boosting language models reasoning with chain-of-knowledge prompting. *arXiv
    preprint arXiv:2306.06427*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023c) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. [Self-consistency improves
    chain of thought reasoning in language models](http://arxiv.org/abs/2203.11171).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2022) Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuohang Wang, Zhuosheng
    Zhang, Yuwei Fang, and Meng Jiang. 2022. [Retrieval augmentation for commonsense
    reasoning: A unified approach](https://doi.org/10.18653/v1/2022.emnlp-main.294).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 4364–4377, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Yichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen. 2023.
    Making large language models perform better in knowledge graph completion. *arXiv
    preprint arXiv:2310.06671*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To demonstrate ODA effectiveness, we highlight several representative cases
    from the QALD10-en dataset. For improved readability, we replaced entity IDs in
    Outputs with their corresponding names. We selected two examples to illustrate
    how ODA utilizes observation to correctly address the question in Table [5](#A1.T5
    "Table 5 ‣ Appendix A Case Study ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs") and Table [6](#A1.T6 "Table 6 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Where are both The Call of the Wild and White Fang set, the most
    two famous works of Jack London? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Action | Thought: The question asks about the setting of two works, The Call
    of the Wild and White Fang, both by Jack London. The observation provides some
    information about the narrative locations of these works, but to confirm and provide
    a specific answer, I will use the GetNeighbor function on The Call of the Wild.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Action: GetNeighbor |'
  prefs: []
  type: TYPE_TB
- en: '|  | Entity_id: The Call of the Wild |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection | Thought: First, I will focus on the triples related to the locations
    of the two works, The Call of the Wild and White Fang, as the question is asking
    about the settings of these two novels. I will also consider the triples related
    to the author, Jack London, as the question mentions him. I will select the triples
    that are most directly related to the settings of the two novels. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Triples: (The Call of the Wild, Narrative location, Yukon), (The Call
    of the Wild, Narrative location, Canada), (The Call of the Wild, author, Jack
    London) |'
  prefs: []
  type: TYPE_TB
- en: '| Action | Thought:The question asks for the setting of both The Call of the
    Wild and White Fang, two notable works of Jack London. From the observation, we
    can see that The Call of the Wild has narrative locations in the United States
    of America, Alaska, Canada, and Yukon. However, we don’t have the narrative location
    for White Fang. Therefore, I will use the GetNeighbor function on the entityID
    of White Fang to find its narrative location. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Action: GetNeighbor |'
  prefs: []
  type: TYPE_TB
- en: '|  | Entity_id: White Fang |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection | Thought:First, I will look for triples that directly answer
    the question. The question is asking for the setting of two works by Jack London,
    namely, The Call of the Wild and White Fang. Therefore, I will look for triples
    that contain these two works and a relation that indicates a setting or location.
    From the candidate triples, I can see that there are two triples that indicate
    a narrative location for White Fang: (White Fang, Narrative location, Yukon),
    (White Fang, Narrative location, Canada). |'
  prefs: []
  type: TYPE_TB
- en: '|  | Triples: (White Fang, Narrative location, Yukon), (White Fang, Narrative
    location, Canada) |'
  prefs: []
  type: TYPE_TB
- en: '| Action | Thought:The question is asking for the locations where both The
    Call of the Wild and White Fang are set. From the reference memory, I can see
    that The Call of the Wild is set in the United States of America, Alaska, Canada,
    and Yukon. White Fang is set in Canada and Yukon. The common locations for both
    books are Canada and Yukon. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Action: Answer |'
  prefs: []
  type: TYPE_TB
- en: '| Answer | [Canada,Yukon] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Case 1'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | The question is What is the capital of the prefecture Tokyo ?
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Action | Thought: The question is asking for the capital of the prefecture
    Tokyo. The candidate entity ID ’Q1490’ corresponds to Tokyo. I can see from the
    observation that there is a triple (Tokyo, capital, Shinjuku) which might answer
    the question. However, to confirm this, I will execute a GetNeighbor action on
    ’Q1490’ to get all the triples where Tokyo is the head. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Action: GetNeighbor |'
  prefs: []
  type: TYPE_TB
- en: '|  | Entity_id: Tokyo |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection | Thought: The question is asking for the capital of Tokyo. From
    the observation, we can see that Tokyo is the capital of Japan and it is a prefecture
    of Japan. The capital of Tokyo is Shinjuku. Therefore, we should select the triples
    that contain this information. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Triples: (Tokyo, instance of, prefecture of Japan), (Tokyo, capital ,
    Shinjuku), (Tokyo, capital, Japan) |'
  prefs: []
  type: TYPE_TB
- en: '| Action | Thought:The question is asking for the capital of the prefecture
    Tokyo. From the reference memory, it is stated that the capital of Tokyo is Shinjuku.
    Therefore, the answer to the question is Shinjuku. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Action: Answer |'
  prefs: []
  type: TYPE_TB
- en: '| Answer | Shinjuku |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Case 2'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section presents the prompts for the action and reflection modules in
    tables [7](#A2.T7 "Table 7 ‣ Appendix B Prompt ‣ ODA: Observation-Driven Agent
    for integrating LLMs and Knowledge Graphs") and [8](#A2.T8 "Table 8 ‣ Appendix
    B Prompt ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Action | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Neighbor Exploration & Path Discovery | Agent Instructions: |'
  prefs: []
  type: TYPE_TB
- en: '| You function as an agent that provides answers based on a knowledge graph.
    |'
  prefs: []
  type: TYPE_TB
- en: '| To assist you in querying the KB, use the following tools: |'
  prefs: []
  type: TYPE_TB
- en: '|  | GetNeighbor(entityID: str) -> List[Tuple[str, str, str]]: |'
  prefs: []
  type: TYPE_TB
- en: '|  | Description: Returns triplets containing the given entityID as the head
    and its corresponding entityID as the tail. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GetPath(entityID1: str, entityID2: str) -> List[List[Tuple[str, str, str]]]:
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Description: Returns all triplets linking the two given entityIDs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Example Usage:GetPath("Q30", "Q25231") returns all triplets connecting
    ’Q30’ and ’Q25231’. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Data Provided to You: |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question:[Question] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Memory: [Memory] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Candidate EntityIDs: [Task-relevant EntityIDs] (Choose 1 or 2 based on
    the action) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Observation: [Observation] (These serve as a reference to assist you in
    selecting the appropriate entityID from the Candidate EntityIDs) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Labels: [Task-relevant entities labels] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Action History: [historical action] (Avoid these actions) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Guidelines: |'
  prefs: []
  type: TYPE_TB
- en: '|  | Choose only one action at a time. |'
  prefs: []
  type: TYPE_TB
- en: '|  | For GetPath, select two entityIDs. For GetNeighbor, select one entityID.
    If there are less than 2 entityIDs available, only choose the GetNeighbor action.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Answer | You are a agent that answer questions based on the reference memory
    and your knowledge. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Here are the reference memory:[Memory]. You can use it to help you answer
    the quesiton. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Here is the question you are asked to answer the question:[Question].
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ensure that your answer contains one answer or a list of answer, and each
    answer should be only one or several words,a phrase, a number,true or false, or
    a date, no other information or descripation in answer. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Action Prompt Description'
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reflection | You are an agent that provides answers based on a KG. |'
  prefs: []
  type: TYPE_TB
- en: '|  | You queried some candidate triples [triples] from last action step and
    their corresponding labels:[entities labels] from the KB based on the question:
    [Question]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Now you are asked to select related triples, so you can answer the question
    in the future by using them. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Here are the observation: [Obervation] for guiding you to select the right
    triples from the candidate triples. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Also, here is the memory: [Memory]. You can use it to help you select
    the right triples from the candidate triples. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Guidelines: |'
  prefs: []
  type: TYPE_TB
- en: '|  | You can select less than 15 triples from the candidate triples. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Your output triples must be in the format of entityID,relationID,entityID.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Reflection Prompt Description'
  prefs: []
  type: TYPE_NORMAL
