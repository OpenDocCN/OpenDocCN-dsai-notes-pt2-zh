- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:48:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ODA: 观察驱动代理，用于整合大语言模型（LLMs）和知识图谱'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07677](https://ar5iv.labs.arxiv.org/html/2404.07677)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07677](https://ar5iv.labs.arxiv.org/html/2404.07677)
- en: Lei Sun¹¹1Equal contribution²²2Corresponding author¹ Zhengwei Tao¹¹1Equal contribution
    ² Youdi Li ¹ Hiroshi Arakawa ¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lei Sun**¹¹1平等贡献²²2通讯作者¹ **Zhengwei Tao**¹¹1平等贡献 ² **Youdi Li** ¹ **Hiroshi
    Arakawa** ¹'
- en: ¹Panasonic Connect Co., Ltd., Japan
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Panasonic Connect Co., Ltd., Japan
- en: ²Peking University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²北京大学
- en: tttzw@stu.pku.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: tttzw@stu.pku.edu.cn
- en: '{lei.sun, ri.yutei, arakawa.hrs}@jp.panasonic.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{lei.sun, ri.yutei, arakawa.hrs}@jp.panasonic.com'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has
    achieved remarkable success in various natural language processing tasks. However,
    existing methodologies that integrate LLMs and KGs often navigate the task-solving
    process solely based on the LLM’s analysis of the question, overlooking the rich
    cognitive potential inherent in the vast knowledge encapsulated in KGs. To address
    this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework
    tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via
    global observation that enhances reasoning capabilities through a cyclical paradigm
    of observation, action, and reflection. Confronting the exponential explosion
    of knowledge during observation, we innovatively design a recursive observation
    mechanism. Subsequently, we integrate the observed knowledge into the action and
    reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art
    performance on several datasets, notably achieving accuracy improvements of 12.87%
    and 8.9%. Our code and data are available on [ODA](https://github.com/lanjiuqing64/KGdata).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）和知识图谱（KGs）的整合在各种自然语言处理任务中取得了显著成功。然而，现有的将LLMs和KGs整合的方法通常仅基于LLM对问题的分析来导航任务解决过程，忽视了KG中蕴含的丰富认知潜力。为了解决这一问题，我们引入了观察驱动代理（ODA），这是一个专门为涉及KG的任务量身定制的新型AI代理框架。ODA通过全球观察融合KG推理能力，通过观察、行动和反思的循环范式增强推理能力。面对观察过程中的知识指数爆炸，我们创新性地设计了递归观察机制。随后，我们将观察到的知识整合到行动和反思模块中。通过大量实验，ODA在多个数据集上展示了最先进的性能，特别是在准确率上分别提高了12.87%和8.9%。我们的代码和数据可以在[ODA](https://github.com/lanjiuqing64/KGdata)上获取。
- en: 'ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'ODA: 观察驱动代理，用于整合大语言模型（LLMs）和知识图谱'
- en: Lei Sun¹¹1Equal contribution²²2Corresponding author¹ Zhengwei Tao¹¹1Equal contribution
    ² Youdi Li ¹ Hiroshi Arakawa ¹ ¹Panasonic Connect Co., Ltd., Japan ²Peking University
    tttzw@stu.pku.edu.cn {lei.sun, ri.yutei, arakawa.hrs}@jp.panasonic.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lei Sun**¹¹1平等贡献²²2通讯作者¹ **Zhengwei Tao**¹¹1平等贡献 ² **Youdi Li** ¹ **Hiroshi
    Arakawa** ¹ ¹Panasonic Connect Co., Ltd., Japan ²北京大学 tttzw@stu.pku.edu.cn {lei.sun,
    ri.yutei, arakawa.hrs}@jp.panasonic.com'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) Touvron et al. ([2023](#bib.bib22)); Scao et al.
    ([2022](#bib.bib18)); Muennighoff et al. ([2022](#bib.bib11)); Brown et al. ([2020](#bib.bib2))
    have exhibited extraordinary capabilities across a variety of natural language
    processing tasks. Despite their impressive accomplishments, LLMs often struggle
    to provide accurate responses to queries that necessitate specialized expertise
    beyond their pre-training content. In response to this limitation, a natural and
    promising approach involves the integration of external knowledge sources, such
    as knowledge graphs (KGs), to augment LLM reasoning abilities. KGs provide structured,
    explicit, and explainable knowledge representations, offering a synergistic method
    to overcome the intrinsic constraints of LLMs. The fusion of LLMs with KGs has
    garnered significant interest in recent research Pan et al. ([2024](#bib.bib14)),
    underlying a vast array of applications Zhang et al. ([2023](#bib.bib29)); Do
    et al. ([2024](#bib.bib4)); Sun et al. ([2023b](#bib.bib20)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）**Touvron et al.** ([2023](#bib.bib22)); **Scao et al.** ([2022](#bib.bib18));
    **Muennighoff et al.** ([2022](#bib.bib11)); **Brown et al.** ([2020](#bib.bib2))
    展示了在各种自然语言处理任务中的卓越能力。尽管取得了令人印象深刻的成就，LLMs常常难以对需要超出其预训练内容的专业知识的问题提供准确的回答。针对这一限制，一种自然且有前景的方法是整合外部知识来源，如知识图谱（KGs），以增强LLM的推理能力。KGs提供结构化、明确且可解释的知识表示，提供了一种协同的方法来克服LLMs的固有局限。LLMs与KGs的融合在近期研究中引起了广泛关注**Pan
    et al.** ([2024](#bib.bib14))，并支持了大量应用**Zhang et al.** ([2023](#bib.bib29)); **Do
    et al.** ([2024](#bib.bib4)); **Sun et al.** ([2023b](#bib.bib20))。
- en: '![Refer to caption](img/6414bb47c9995d34f21fcea33dfa3f46.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6414bb47c9995d34f21fcea33dfa3f46.png)'
- en: 'Figure 1: An example of LLM integrating with KG. Observed entities are shown
    in white, while non-observed entities are displayed in gray. Entities selected
    by the agent to answer the question are highlighted in yellow.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM 与 KG 集成的示例。观察到的实体以白色显示，而未观察到的实体则以灰色显示。由代理选择以回答问题的实体以黄色突出显示。
- en: 'Existing methodologies for solving tasks that integrate KGs with LLMs can be
    categorized into two groups. The first one involves retrieving relevant triples
    from KGs in response to specific questions Wang et al. ([2023b](#bib.bib24));
    Luo et al. ([2024](#bib.bib10)); Jiang et al. ([2023](#bib.bib8)). The second
    part adopts an explore-exploit strategy, directing the knowledge utilization process
    within the graph according to the question Sun et al. ([2023b](#bib.bib20)); Guo
    et al. ([2023](#bib.bib7)). However, both categories navigate the task-solving
    process by merely relying on the LLM’s analysis of the question, overlooking the
    rich cognitive potential inherent in the abundant knowledge encapsulated in KGs.
    KGs, which store a wealth of informative and symbolic facts, should deeply participate
    in the reasoning process together with LLM rather than being merely treated as
    a static repository of knowledge Pan et al. ([2024](#bib.bib14)). As the example
    in the upper panel of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs"), LLM analyzes the question and
    navigates towards Narrative location relation of entity The Call of The Wild.
    However, this entity has many neighboring entities with that relation, leading
    LLM to incorrectly infer Canada as the answer. In contrast, the bottom panel demonstrates
    how KG provides key patterns that reveal both The Call of The Wild and White Fang
    share the location Yukon. If LLM could observe this information beforehand, it
    would precisely guide its reasoning process towards the correct answer (as shown
    in the bottom panel). Therefore, LLM should adopt an overall observation to incorporate
    the extensive knowledge and intricate patterns embedded within the KG. Achieving
    this objective presents two primary challenges: firstly, a global observation
    of the KG can result in an exponential growth in the number of triples. As shown
    in the upper panel of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs"), fully processing all 3-hop
    connections for The Call of the Wild is impractical. Secondly, the integration
    of such comprehensive observation into the existing reasoning paradigms of LLMs
    presents another challenge. How to combine the observation with the reasoning
    process of LLM matters for solving the tasks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的将知识图谱（KG）与大型语言模型（LLM）集成以解决任务的方法可以分为两类。第一类涉及从KG中检索相关的三元组以回应特定的问题 Wang et al.
    ([2023b](#bib.bib24)); Luo et al. ([2024](#bib.bib10)); Jiang et al. ([2023](#bib.bib8))。第二类采用探索-利用策略，根据问题引导知识在图谱中的使用过程 Sun
    et al. ([2023b](#bib.bib20)); Guo et al. ([2023](#bib.bib7))。然而，这两类方法都仅依赖于LLM对问题的分析来导航任务解决过程，忽视了KG中蕴含的丰富认知潜力。KG储存了大量有用和符号性的事实，应该与LLM一起深入参与推理过程，而不仅仅被视为静态的知识库 Pan
    et al. ([2024](#bib.bib14))。如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs")上面的示例所示，LLM分析问题并导航到实体《荒野之呼唤》的地点关系。然而，该实体有许多具有该关系的邻近实体，导致LLM错误地推断加拿大为答案。相比之下，下面的面板展示了KG如何提供关键模式，揭示《荒野之呼唤》和《白牙》都共享地点尤肯。如果LLM能够事先观察到这些信息，它将准确地指导其推理过程朝向正确的答案（如下面板所示）。因此，LLM应采取整体观察的方法，将KG中嵌入的大量知识和复杂模式整合进来。实现这一目标面临两个主要挑战：首先，对KG的全局观察可能导致三元组数量的指数增长。如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ODA: Observation-Driven Agent for integrating LLMs
    and Knowledge Graphs")上面的面板所示，全面处理《荒野之呼唤》的所有三跳连接是不切实际的。其次，将如此全面的观察整合到现有的LLM推理范式中是另一个挑战。如何将观察与LLM的推理过程相结合对于解决任务至关重要。'
- en: 'Motivated by this, we introduce a novel framework, the Observation-Driven Agent
    (ODA), aimed at sufficiently and autonomously integrating the capabilities of
    both LLM and KG. ODA serves as an AI Agent specifically designed for KG-centric
    tasks. ODA engages in a cyclical paradigm of observation, action, and reflection.
    Within ODA, we design a novel observation module to efficiently draw autonomous
    reasoning patterns of KG. Our observation module avoids the problem of exponential
    growth of triples via recursive progress. This approach ensures ODA integrating
    abilities of KG and LLM while mitigating the challenges associated with excessive
    data in KG, improving the efficiency and accuracy. Following the observation phase,
    ODA takes action by autonomously amalgamating insights derived from LLM inferences
    with the observed KG patterns. ODA can perform actions of three distinct types:
    Neighbor Exploration, Path Discovery, and Answering. Subsequently, ODA reflects
    on its internal state, considering both the outcomes of its actions and the prior
    observations. This iterative process continues until ODA accomplishes the task
    at hand.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受到此启发，我们引入了一种新颖的框架——观察驱动代理（ODA），旨在充分且自主地整合LLM和KG的能力。ODA作为一个专门设计用于KG相关任务的AI代理，参与一个观察、行动和反思的循环模式。在ODA中，我们设计了一个新颖的观察模块，以高效地提取KG的自主推理模式。我们的观察模块通过递归进展避免了三元组的指数增长问题。这种方法确保ODA整合KG和LLM的能力，同时减轻了KG中过多数据带来的挑战，提高了效率和准确性。在观察阶段之后，ODA通过自主整合从LLM推理中得出的见解与观察到的KG模式来采取行动。ODA可以执行三种不同类型的操作：邻居探索、路径发现和回答。随后，ODA对其内部状态进行反思，考虑其行动的结果和之前的观察。这一迭代过程将持续进行，直到ODA完成任务为止。
- en: 'We conduct extensive experiments to testify to the effectiveness of ODA on
    four datasets: QALD10-en, T-REx, Zero-Shot RE and Creak. Notably, our approach
    achieved state-of-the-art (SOTA) performance compared to competitive baselines.
    Specifically, on QALD10-en and T-REx datasets, we observed remarkable accuracy
    improvements of 12.87% and 8.9%, respectively. We conclude the contributions as
    follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了广泛的实验，以验证ODA在四个数据集上的有效性：QALD10-en、T-REx、Zero-Shot RE和Creak。值得注意的是，与竞争基线相比，我们的方法达到了最先进的（SOTA）性能。具体而言，在QALD10-en和T-REx数据集中，我们观察到准确性分别提高了12.87%和8.9%。我们总结了以下贡献：
- en: $\bullet$
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We propose ODA, an AI Agent tailored for KG-centric tasks. ODA conducts observation
    to incorporate the reasoning ability of KG.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了ODA，一个针对KG相关任务量身定制的AI代理。ODA进行观察以融入KG的推理能力。
- en: $\bullet$
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We further design action and reflection modules that integrate the observation
    into LLM reasoning. This strategy leverages the synergy between the autonomous
    reasoning of KG and LLM.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进一步设计了将观察整合到LLM推理中的行动和反思模块。这一策略利用了KG的自主推理与LLM之间的协同效应。
- en: $\bullet$
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We conduct experiments on four datasets and achieve SOTA performances.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在四个数据集上进行实验，并取得了SOTA表现。
- en: '![Refer to caption](img/1de2640486dbda690f9dc685ad6d7406.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1de2640486dbda690f9dc685ad6d7406.png)'
- en: 'Figure 2: The overall framework of ODA.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：ODA的整体框架。
- en: 2 Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 'In this work, we aim to solve tasks associated with KG. Let $q$ can be expressed
    as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们旨在解决与KG相关的任务。令$q$可以表示为：
- en: '|  | $T:(q,E),G\rightarrow Y$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $T:(q,E),G\rightarrow Y$ |  |'
- en: '![Refer to caption](img/eb459b20c486f113520bb872d5b93cb5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb459b20c486f113520bb872d5b93cb5.png)'
- en: 'Figure 3: An example workflow of ODA. In this case, ODA initiates the obervation
    with entity Johann Wolfgang von Goethe. During the first iteration on the left
    side, the Neighbor Exploration of Johann Wolfgang von Goethe is selected, and
    the reflected triple (Johann Wolfgang von Goethe, unmarried Partner, Lili Schöneman)
    is stored in memory. Subsequently, The observation of Lili Schöneman then guides
    ODA to choose Neighbor Exploration action, and leads to the retention of the triple
    (Lili Schöneman, place of birth, Offenbach am Main) in memory, as shown on the
    right side. Once sufficient knowledge has been accumulated, ODA triggers the Answer
    action, correctly identifying Offenbach am Main as the answer.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：ODA的一个示例工作流程。在这种情况下，ODA以实体Johann Wolfgang von Goethe开始观察。在左侧的第一次迭代中，选择了Johann
    Wolfgang von Goethe的邻居探索，并将反映的三元组（Johann Wolfgang von Goethe, unmarried Partner,
    Lili Schöneman）存储在内存中。随后，Lili Schöneman的观察引导ODA选择邻居探索动作，并导致三元组（Lili Schöneman,
    place of birth, Offenbach am Main）在内存中的保留，如右侧所示。一旦积累了足够的知识，ODA触发回答动作，正确地识别出Offenbach
    am Main作为答案。
- en: 'Employing an iterative approach, ODA tackles the challenges inherent in KG-centric
    tasks. In contrast to existing methods that couple LLMs and KGs and rely solely
    on analyzing the LLM’s query, ODA autonomously integrates observed knowledge from
    the KG into the entire reasoning process, resulting in more informed decisions.
    To achieve this objective, our ODA system, illustrated in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge
    Graphs"), primarily comprises three key modules for task resolution:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '通过迭代的方法，ODA 处理了 KG 相关任务中固有的挑战。与现有方法依赖于分析 LLM 的查询并将 LLM 与 KG 结合的方式不同，ODA 自主地将从
    KG 中观察到的知识整合到整个推理过程中，从而做出更为明智的决策。为实现这一目标，我们的 ODA 系统，如图 [2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge
    Graphs") 所示，主要包括三个关键模块用于任务解决：'
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Observation: This module efficiently observes and processes relevant knowledge
    from the KG environment. In each iteration $i$). By leveraging insights and patterns
    gleaned from the KG, this subgraph is autonomously incorporated into a reasoning
    LLM. This synergistic integration equips ODA with enhanced capabilities from both
    the LLM and KG, allowing it to tackle tasks more effectively.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察：该模块高效地观察和处理来自 KG 环境的相关知识。在每次迭代 $i$ 中，通过利用从 KG 中获取的见解和模式，此子图会被自主地融入到推理 LLM
    中。这种协同整合使 ODA 具备了 LLM 和 KG 的增强能力，使其能更有效地处理任务。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Action: Drawing upon both the observation subgraph $O_{i}$, strategically selects
    the most suitable action to execute on the KG, ensuring the accurate answering
    of the question.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动：基于观察子图 $O_{i}$，策略性地选择最合适的动作来执行于 KG，确保问题的准确回答。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reflection: Utilizing the observation subgraph $O_{i}$ for the next iteration,
    facilitating continuous reasoning.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反思：利用观察子图 $O_{i}$ 进行下一次迭代，促进持续推理。
- en: Through this iterative process, ODA dynamically updates its observation subgraph
    $O_{i}$. Each module is discussed in detail in the following sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一迭代过程，ODA 动态更新其观察子图 $O_{i}$。每个模块将在以下部分详细讨论。
- en: 2.1 Observation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 观察
- en: 'The observation module is specifically designed to inspect global KG knowledge
    and navigate the autonomous reasoning process with the KG environments. At each
    iteration $i$. This process can be formulated as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 观察模块专门设计用于检查全局 KG 知识并在 KG 环境中导航自主推理过程。在每次迭代 $i$ 中。这个过程可以表示为：
- en: '|  | $O_{i}=\text{Observation}([E_{i},q])$ |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $O_{i}=\text{Observation}([E_{i},q])$ |  |'
- en: Initially, the task-relevant entities are populated with the entities embedded
    within the question $q$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，任务相关的实体由嵌入在问题 $q$ 中的实体填充。
- en: 'For KG-centric tasks, the observation incurs the problem of an explosive number
    of nodes. To address the scalability challenge during observation subgraph updates,
    we propose a $D$ represents the maximum hop depth. Each turn has two steps: update
    and refine. The update step focuses on expanding the subgraph, while the refining
    step ensures its appropriate size without loss of important information.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以 KG 为中心的任务，观察会导致节点数量爆炸的问题。为了应对观察子图更新期间的可扩展性挑战，我们提出了 $D$ 表示最大跳跃深度。每轮有两个步骤：更新和精炼。更新步骤专注于扩展子图，而精炼步骤则确保其适当的大小而不会丢失重要信息。
- en: 'For each entity $e\in E_{i}$ is reached. The specific details are described
    below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个实体 $e\in E_{i}$ 的具体细节如下：
- en: Algorithm 1 Observation
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 观察
- en: 'Require: Question $q$'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 需要：问题 $q$
- en: Initialize task-relevant entities $E_{i}$     end whileend for
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化与任务相关的实体 $E_{i}$     end whileend for
- en: 'Update:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更新：
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For each entity $e$ denotes the tail entity.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个实体 $e$ 表示尾实体。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The similarity score between the question and the combination of $r$, is computed
    by measuring the cosine similarity of their embeddings¹¹1We use the GPT text-embedding-ada-002
    model from OpenAI for encoding.:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '通过计算问题与 $r$ 的组合之间的余弦相似度，来计算它们的相似度分数¹¹1我们使用 OpenAI 的 GPT text-embedding-ada-002
    模型进行编码。:'
- en: '|  | $\text{Cosine Similarity}(\mathbf{v}_{q},\mathbf{v}_{r+t})=\frac{\mathbf{v}_{q}\cdot\mathbf{v}_{r+t}}{\&#124;\mathbf{v}_{q}\&#124;\&#124;\mathbf{v}_{r+t}\&#124;}$
    |  |'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{Cosine Similarity}(\mathbf{v}_{q},\mathbf{v}_{r+t})=\frac{\mathbf{v}_{q}\cdot\mathbf{v}_{r+t}}{\&#124;\mathbf{v}_{q}\&#124;\&#124;\mathbf{v}_{r+t}\&#124;}$
    |  |'
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All triples associated with entities in $E$ are collectively sorted in descending
    order based on their similarity scores.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有与 $E$ 中的实体相关的三元组按照它们的相似度分数按降序排序。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Top-$N$.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Top-$N$。
- en: 'Refine:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Top-$N$ with the highest similarity scores.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具有最高相似度分数的前 $N$ 个。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The tail entities from the refined top $P\%$ for the next iteration.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一轮迭代的前 $P\%$ 的尾实体。
- en: 2.2 Action
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 动作
- en: Harnessing the power of an LLM, the action module crafts strategic prompts to
    generate optimal actions. Based on its memory $M_{<i}$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 LLM 的强大功能，动作模块制定战略性提示以生成最佳动作。基于其内存 $M_{<i}$。
- en: '|  | $a_{i}=\text{Action}([O_{i},a_{<i},M_{<i}])$ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $a_{i}=\text{Action}([O_{i},a_{<i},M_{<i}])$ |  |'
- en: 'We propose three core actions designed to empower ODA :'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了三种核心动作，旨在增强 ODA：
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Neighbor Exploration: This action explores the KG neighborhood of the entity
    in task-relevant entities $E_{i}$, retrieving all neighboring triples. This helps
    build context and understand interconnectedness within the KG for ODA .'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 邻居探索：此动作探索任务相关实体 $E_{i}$ 的知识图邻域，检索所有邻接的三元组。这有助于建立上下文并理解知识图中的互联性。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Path Discovery: Given two entities in task-relevant entities $E_{i}$, this
    action searches for all possible paths connecting them. Each path consists of
    interconnected triples, allowing the ODA to explore various connections and potentially
    uncover hidden relationships.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 路径发现：给定任务相关实体 $E_{i}$ 中的两个实体，此动作搜索连接它们的所有可能路径。每条路径由互联的三元组组成，使 ODA 能够探索各种连接并可能揭示隐藏的关系。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Answer: This action responds to the question only if the required information
    is present in memory $M_{<i}$.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案：此动作仅在所需信息存在于内存 $M_{<i}$ 中时才会对问题作出响应。
- en: 'Upon selecting an answer action, ODA halts the iterative loop of observation,
    action, and reflection. Leveraging the reliable knowledge within memory $M_{<i}$,
    it can then directly formulate the answer to the question. Alternatively, if a
    Neighbor Exploration or Path Discovery action is selected, ODA strategically extracts
    relevant knowledge from the KG as a set of triples. These extracted triples are
    then fed into the subsequent reflection step for further processing.The prompt
    used here can be found in Table [7](#A2.T7 "Table 7 ‣ Appendix B Prompt ‣ ODA:
    Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个答案动作后，ODA 停止观察、动作和反思的迭代循环。利用内存 $M_{<i}$ 中可靠的知识，它可以直接形成问题的答案。或者，如果选择了邻居探索或路径发现动作，ODA
    会策略性地从知识图中提取相关知识作为一组三元组。这些提取的三元组然后被送入随后的反思步骤进行进一步处理。这里使用的提示可以在表 [7](#A2.T7 "表
    7 ‣ 附录 B 提示 ‣ ODA：用于整合 LLM 和知识图的观察驱动代理") 中找到。
- en: 2.3 Reflection
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 反思
- en: 'The reflection module plays a crucial role in evaluating the triples generated
    from the action step and subsequently updating ODA memory $M_{i}$, the reflection
    module provides invaluable feedback, guiding future decision-making. This process
    can be formalized as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 反思模块在评估从动作步骤生成的三元组并随后更新 ODA 内存 $M_{i}$ 中发挥着至关重要的作用，反思模块提供了宝贵的反馈，指导未来的决策。这一过程可以形式化为：
- en: '|  | $M_{i}=\text{Reflection}([O_{i},a_{i},M_{<i}])$ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{i}=\text{Reflection}([O_{i},a_{i},M_{<i}])$ |  |'
- en: Given that memory $M_{i}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 给定内存 $M_{i}$。
- en: 'Subsequently, the tail entities in the reflected triples are designated as
    the task-relevant entities for the next iteration. The specific prompt description
    used for the reflection module is provided in Table [8](#A2.T8 "Table 8 ‣ Appendix
    B Prompt ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，反思三元组中的尾实体被指定为下一轮迭代的任务相关实体。反思模块使用的具体提示描述见表 [8](#A2.T8 "表 8 ‣ 附录 B 提示 ‣ ODA：用于整合
    LLM 和知识图的观察驱动代理")。
- en: 'The observation, action, and reflection modules collaborate in an iterative
    process until either the Answer action is triggered or the maximum iteration limit
    is reached. Figure [3](#S2.F3 "Figure 3 ‣ 2 Methods ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs") depicts an example of how observation,
    action, and reflection work together.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 观察、动作和反思模块在迭代过程中协作，直到触发答案动作或达到最大迭代限制为止。图 [3](#S2.F3 "图 3 ‣ 2 方法 ‣ ODA：用于整合 LLM
    和知识图的观察驱动代理") 展示了观察、动作和反思如何协同工作的一个示例。
- en: 3 Experiments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: '| Dataset | Test | Entity | Type | License |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 测试 | 实体 | 类型 | 许可证 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| QALD10-en | 333 | 396 | Multi-hop | MIT License |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| QALD10-en | 333 | 396 | 多跳 | MIT 许可证 |'
- en: '| T-REx | 5000 | 4943 | Slot-Filling | MIT License |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| T-REx | 5000 | 4943 | 插槽填充 | MIT 许可证 |'
- en: '| Zero-Shot RE | 3724 | 3657 | Slot Filling | MIT License |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Zero-Shot RE | 3724 | 3657 | 插槽填充 | MIT 许可证 |'
- en: '| Creak | 1371 | 516 | Fact Checking | MIT License |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Creak | 1371 | 516 | 事实核查 | MIT 许可证 |'
- en: 'Table 1: Dataset statistics. Entity stands for the entity size derived from
    all the question within the datasets.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：数据集统计。实体表示从数据集中的所有问题中衍生出的实体大小。
- en: 3.1 Dataset
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: 'To evaluate the performance of our ODA , we conduct experiments on four KBQA
    datasets: QALD10-en Perevalov et al. ([2022](#bib.bib15)),Creak Onoe et al. ([2021](#bib.bib12)),
    T-REx Elsahar et al. ([2018](#bib.bib5)), and Zero-Shot RE Petroni et al. ([2021](#bib.bib16)).
    Detailed specifications for each dataset are provided in Table [1](#S3.T1 "Table
    1 ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge
    Graphs"). The Hits@1Sun et al. ([2019](#bib.bib21)) accuracy with exact match
    is utilized as our evaluation metric.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估我们的ODA性能，我们在四个KBQA数据集上进行了实验：QALD10-en Perevalov等（[2022](#bib.bib15)），Creak
    Onoe等（[2021](#bib.bib12)），T-REx Elsahar等（[2018](#bib.bib5)），以及Zero-Shot RE Petroni等（[2021](#bib.bib16)）。每个数据集的详细规格见表[1](#S3.T1
    "Table 1 ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating LLMs
    and Knowledge Graphs")。我们使用Hits@1Sun等（[2019](#bib.bib21)）准确度作为评估指标。'
- en: 3.2 Setup
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 设置
- en: We utilized the GPT-4 OpenAI ([2023](#bib.bib13)) model as the ODA via the OpenAI
    API. Throughout our experiments, we consistently configured the temperature value
    of GPT-4 to 0.4 and set the maximum token length to 500.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过OpenAI API使用了GPT-4 OpenAI（[2023](#bib.bib13)）模型作为ODA。在我们的实验中，我们始终将GPT-4的温度值配置为0.4，并将最大令牌长度设置为500。
- en: For the observation step, we tuned key parameters based on preliminary experiments.
    we set $P_{t}$ to 15.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察步骤中，我们根据初步实验调整了关键参数。我们将$P_{t}$设置为15。
- en: To establish Wikidata KG database and retrieve information from it, we employed
    the simple-wikidata-db¹¹1https://github.com/neelguha/simple-wikidata-db Python
    library. This library provides various scripts for downloading the Wikidata dump,
    organizing it into staging files, and executing distributed queries on the data
    within these staged files. Specifically, we deployed the Wikidata dump across
    five AWS EC2 instances, each consisting of a 768GB machine with 48 cores.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立Wikidata KG数据库并从中检索信息，我们使用了simple-wikidata-db¹¹1https://github.com/neelguha/simple-wikidata-db
    Python库。该库提供了各种脚本，用于下载Wikidata数据转储、将其整理到临时文件中，并对这些临时文件中的数据执行分布式查询。具体而言，我们在五个AWS
    EC2实例上部署了Wikidata数据转储，每个实例配备了768GB内存和48个核心。
- en: 'Considering that our ODA relies heavily on continuous interaction with the
    KG, we discovered that the real-time extraction of required Wikidata knowledge
    on AWS achieved an average completion time of 50 seconds per question-answer pair
    within QALD10-en dataset. However, as the KBQA dataset expanded, the cost of using
    the Wikidata database on AWS became prohibitively expensive. Consequently, to
    address the computational expenses involved, we devised a solution by generating
    an offline subgraph for each KBQA dataset. This offline subgraph captures all
    the triples within a 3-hop radius of the entities in each dataset, including the
    properties of both the entities and the relations involved. Notably, generating
    such a subgraph for the T-REx dataset, with its 4943 entities (as listed in Table
    [1](#S3.T1 "Table 1 ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs")), takes approximately 54 minutes and 42.834 seconds
    in practice.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到我们的ODA**高度依赖于与KG的持续交互**，我们发现AWS上实时提取所需的Wikidata知识在QALD10-en数据集中每对问答的平均完成时间为50秒。然而，随着KBQA数据集的扩展，使用AWS上的Wikidata数据库的成本变得极其昂贵。因此，为了解决计算开销问题，我们通过为每个KBQA数据集生成一个离线子图来制定解决方案。这个离线子图捕捉了每个数据集中实体的3跳范围内的所有三元组，包括实体和涉及的关系的属性。特别地，为T-REx数据集生成这样一个子图（该数据集包含4943个实体，如表[1](#S3.T1
    "Table 1 ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating LLMs
    and Knowledge Graphs")所列）实际需要约54分钟42.834秒。'
- en: 3.3 Baseline Models
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基线模型
- en: To comprehensively evaluate ODA effectiveness, we conduct a rigorous benchmark
    against several SOTA models across diverse categories. The comparison encompasses
    various models, starting with prompt-based approaches that do not utilize external
    knowledge. These include direct answering with GPT-3.5 and GPT-4, as well as the
    Self-Consistency Wang et al. ([2023c](#bib.bib25)) and CoT Sun et al. ([2023b](#bib.bib20)).
    On the other hand, Kownledge-combined models are considered, which incorporate
    fine-tuned techniques such as SPARQL-QA Santana et al. ([2022](#bib.bib17)), RACo
    Yu et al. ([2022](#bib.bib28)), RAG Petroni et al. ([2021](#bib.bib16)) and Re2G
    Glass et al. ([2022](#bib.bib6)). Additionally, there is ToG Sun et al. ([2023a](#bib.bib19))
    model, which integrates LLM with KG to bolster question-answering proficiency.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估 ODA 的有效性，我们对多个最先进（SOTA）模型在不同类别中进行严格基准测试。比较包括各种模型，从不使用外部知识的基于提示的方法开始。这些方法包括直接使用
    GPT-3.5 和 GPT-4 进行回答，以及 Self-Consistency Wang 等（[2023c](#bib.bib25)）和 CoT Sun
    等（[2023b](#bib.bib20)）。另一方面，考虑了结合知识的模型，这些模型融入了如 SPARQL-QA Santana 等（[2022](#bib.bib17)）、RACo
    Yu 等（[2022](#bib.bib28)）、RAG Petroni 等（[2021](#bib.bib16)）和 Re2G Glass 等（[2022](#bib.bib6)）等微调技术。此外，还有
    ToG Sun 等（[2023a](#bib.bib19)）模型，它将 LLM 与 KG 集成，以提高问答能力。
- en: 3.4 Main Result
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 主要结果
- en: 'Our ODA method outperforms existing methods, as shown in Table [2](#S3.T2 "Table
    2 ‣ 3.4 Main Result ‣ 3 Experiments ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs"). On average, our method achieves an accuracy gain
    of up to 19.58% compared to direct answering with GPT-4, 19.28% compared to fine-tuned
    models, and 7.09% compared to TOG. These results demonstrate the efficiency and
    effectiveness of our method in comparison to other state-of-the-art methods.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的 ODA 方法在表 [2](#S3.T2 "Table 2 ‣ 3.4 Main Result ‣ 3 Experiments ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs") 中显示出比现有方法更好的表现。平均而言，我们的方法比直接使用
    GPT-4 的回答提高了 19.58% 的准确率，比微调模型提高了 19.28%，比 TOG 提高了 7.09%。这些结果展示了我们方法在与其他最先进方法比较中的效率和有效性。'
- en: Furthermore, our ODA significantly outperforms the prompt-based methods across
    various datasets, particularly showing an improvement of 65.50% and 23.77% on
    Zero-Shot REx and QALD10-en , respectively. These results underscore the importance
    of leveraging external knowledge graphs for reasoning and completing the question-answering
    task.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的 ODA 在各种数据集上显著优于基于提示的方法，特别是在 Zero-Shot REx 和 QALD10-en 上分别提高了 65.50% 和
    23.77%。这些结果强调了利用外部知识图谱进行推理和完成问答任务的重要性。
- en: Compared to the fine-tuned method, our ODA method demonstrates superior performance.
    Specifically, our method achieves a performance gain of 21.27% for the QALD10-en
    dataset, 6.99% for the Creak dataset, and 50.56% for the Zero-Shot RE dataset.
    Notably, this interaction between the LLM and KG, as our method employs, proves
    more effective than data-driven fine-tuned techniques, despite requiring no explicit
    training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与微调方法相比，我们的 ODA 方法展示了更优的性能。具体而言，我们的方法在 QALD10-en 数据集上提高了 21.27%、在 Creak 数据集上提高了
    6.99%、在 Zero-Shot RE 数据集上提高了 50.56%。值得注意的是，我们的方法采用的 LLM 与 KG 的互动证明比数据驱动的微调技术更有效，尽管不需要明确的训练。
- en: Our ODA method exhibits significant performance gains over the ToG method across
    most datasets, with improvements of 12.87% (QALD10-en), 8.9% (T-REx), and 7% (Zero-Shot
    RE), despite both methods leveraging large language models and knowledge graphs.
    This performance disparity highlights the critical role of our observation module
    and the effectiveness of autonomously incorporating reasoning from KG. Specifically,
    our method demonstrates significantly stronger performance on the QALD10-en dataset,
    known for its multi-hop and complex reasoning requirements. This achievement underscores
    our ODA ability to effectively exploit the rich knowledge and patterns within
    KG, seamlessly combining the autonomous reasoning strengths of both LLM and KG
    to tackle complex questions successfully.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 ODA 方法在大多数数据集上表现出显著的性能提升，相较于 ToG 方法，提升了 12.87%（QALD10-en）、8.9%（T-REx）和 7%（Zero-Shot
    RE），尽管这两种方法都利用了大型语言模型和知识图谱。这一性能差异突显了我们观察模块的关键作用和自主整合 KG 推理的有效性。特别是，我们的方法在具有多跳和复杂推理要求的
    QALD10-en 数据集上表现出显著的更强性能。这一成就突显了我们 ODA 在有效利用 KG 中丰富的知识和模式方面的能力，成功地将 LLM 和 KG 的自主推理优势无缝结合，以应对复杂问题。
- en: '| Method | QALD10-en | Creak | T-REx | Zero-Shot RE | Average |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | QALD10-en | Creak | T-REx | Zero-Shot RE | 平均 |'
- en: '| w.o. Knowledge |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 无知识 |'
- en: '| Direct answering(GPT3.5) | 44.74^((1)) | 90.00^((1)) | 37.78^((1)) | 37.14^((1))
    | 52.42 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 直接回答 (GPT3.5) | 44.74^((1)) | 90.00^((1)) | 37.78^((1)) | 37.14^((1)) | 52.42
    |'
- en: '| Direct answering(GPT4) | 57.10^((1)) | 94.52^((1)) | 57.72^((1)) | 55.50^((1))
    | 66.21 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 直接回答 (GPT4) | 57.10^((1)) | 94.52^((1)) | 57.72^((1)) | 55.50^((1)) | 66.21
    |'
- en: '| Self-Consistency(GPT3.5)Wang et al. ([2023c](#bib.bib25)) | 45.30^((1)) |
    90.80^((1)) | 41.80^((1)) | 45.40^((1)) | 55.83 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 自一致性 (GPT3.5) 王 等 ([2023c](#bib.bib25)) | 45.30^((1)) | 90.80^((1)) | 41.80^((1))
    | 45.40^((1)) | 55.83 |'
- en: '| COT(GPT3.5)Sun et al. ([2023a](#bib.bib19)) | 42.90^((1)) | 90.10^((1)) |
    32.00^((1)) | 28.80^((1)) | 48.45 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| COT (GPT3.5) Sun 等 ([2023a](#bib.bib19)) | 42.90^((1)) | 90.10^((1)) | 32.00^((1))
    | 28.80^((1)) | 48.45 |'
- en: '| w.t. Knowledge / Fine-tuned |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| w.t. 知识 / 微调 |'
- en: '| SOTA | 45.40¹ | 88.20² | 87.70³ | 44.74⁴ | 66.51 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SOTA | 45.40¹ | 88.20² | 87.70³ | 44.74⁴ | 66.51 |'
- en: '| w.t. Knowledge / Zero-Shot (GPT-4) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| w.t. 知识 / 零样本 (GPT-4) |'
- en: '| TOG-R Sun et al. ([2023a](#bib.bib19)) | 54.70^((1)) | 95.40^((1)) | 75.50^((1))
    | 86.90^((1)) | 78.13 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| TOG-R Sun 等 ([2023a](#bib.bib19)) | 54.70^((1)) | 95.40^((1)) | 75.50^((1))
    | 86.90^((1)) | 78.13 |'
- en: '| TOG Sun et al. ([2023a](#bib.bib19)) | 53.80^((1)) | 95.60^((1)) | 77.10^((1))
    | 88.30^((1)) | 78.70 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| TOG Sun 等 ([2023a](#bib.bib19)) | 53.80^((1)) | 95.60^((1)) | 77.10^((1))
    | 88.30^((1)) | 78.70 |'
- en: '| ODA (Ours) | 66.67^((1)) | 95.19^((1)) | 86.00 ^((1)) | 95.30^((1)) | 85.79
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ODA (我们的) | 66.67^((1)) | 95.19^((1)) | 86.00^((1)) | 95.30^((1)) | 85.79
    |'
- en: 'Table 2: Performance Comparison of different methods. Bold scores stand for
    best performances among all GPT-based zero-shot methods. The fine-tuned SOTA includes:
    1: SPARQL-QASantana et al. ([2022](#bib.bib17)), 2: RACoYu et al. ([2022](#bib.bib28)),
    3: Re2GGlass et al. ([2022](#bib.bib6)), 4:RAGPetroni et al. ([2021](#bib.bib16)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：不同方法的性能比较。粗体分数代表所有基于 GPT 的零样本方法中的最佳表现。微调的 SOTA 包括：1: SPARQL-QASantana 等
    ([2022](#bib.bib17))，2: RACoYu 等 ([2022](#bib.bib28))，3: Re2GGlass 等 ([2022](#bib.bib6))，4:
    RAGPetroni 等 ([2021](#bib.bib16))。'
- en: 4 Discussion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: To better understand the key factors influencing our ODA, we conducted extensive
    analysis experiments. To conserve computational resources, we kept the previously
    mentioned datasets (QALD10-en, Creak, T-REx, and Zero-Shot RE) but randomly sampled
    400 examples each for Creak, T-REx, and Zero-Shot RE.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解影响我们 ODA 的关键因素，我们进行了广泛的分析实验。为了节省计算资源，我们保留了之前提到的数据集（QALD10-en、Creak、T-REx
    和 Zero-Shot RE），但对 Creak、T-REx 和 Zero-Shot RE 各随机抽取了 400 个样本。
- en: 4.1 Effect of Observation
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 观察效果
- en: To assess the efficacy of the observation module, we conducted comprehensive
    experiments with the model without observation. During the action step, ODA selects
    the action only based on the knowledge within memory. Subsequently, the reflection
    step reflects on the triples outputted by the action and updates memory without
    the guide from observation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估观察模块的有效性，我们进行了与没有观察的模型的全面实验。在行动步骤中，ODA 仅基于内存中的知识选择行动。随后，反思步骤对行动输出的三元组进行反思并在没有观察指导的情况下更新内存。
- en: 'A statistical comparison was performed to evaluate the performance of the ODA with
    and without observation across all datasets (see Table [3](#S4.T3 "Table 3 ‣ 4.1
    Effect of Observation ‣ 4 Discussion ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs")). The results show that the ODA with observation outperforms
    the ODA without observation, with an average improvement of 3.14%. Specifically,
    for QALD10-en dataset, the ODA with observation outperforms the ODA without observation
    by 5.41%. Since QALD10-en involves multi-hop reasoning, the improved performance
    of the ODA with observation indicates that the observation module enhances the
    reasoning ability of the agent, enabling more accurate action selection and reflection.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '进行了一项统计比较，以评估 ODA 在所有数据集上有无观察的性能（见表格 [3](#S4.T3 "表 3 ‣ 4.1 观察效果 ‣ 4 讨论 ‣ ODA:
    用于整合 LLM 和知识图谱的观察驱动代理")）。结果显示，有观察的 ODA 超过了没有观察的 ODA，平均提高了 3.14%。具体来说，对于 QALD10-en
    数据集，有观察的 ODA 比没有观察的 ODA 高出 5.41%。由于 QALD10-en 涉及多跳推理，有观察的 ODA 的改进表明观察模块增强了代理的推理能力，使得行动选择和反思更加准确。'
- en: 'We can further illustrate the benefits of the observation module with a practical
    case. In this scenario (see Table [5](#A1.T5 "Table 5 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs")),
    question is Where are both The Call of the Wild and White Fang set, the most two
    famous works of Jack London?. Without observation, ODA generated the memory, such
    as (The Call of the Wild, narrative location, Canada), ultimately produced the
    wrong answer of Canada. However, with the observation module, the ODA correctly
    reasons the memory,such as (The Call of the Wild, narrative location, Yukon),
    (White Fang, narrative location, Yukon). As a result, the ODA with observation
    provides the correct answer, Yukon. This case exemplifies how the observation
    module improves the accuracy of action selection and reflection, consequently
    enhancing the reasoning ability of ODA .'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以通过一个实际案例进一步说明观察模块的好处。在这个场景中（见表 [5](#A1.T5 "Table 5 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs")），问题是《荒野呼唤》和《白
    Fang》这两部杰克·伦敦最著名的作品的背景设定在哪里？如果没有观察，ODA 生成的记忆，如（《荒野呼唤》，叙述地点，加拿大），最终产生了错误的答案——加拿大。然而，借助观察模块，ODA
    正确推理出记忆，如（《荒野呼唤》，叙述地点，育空），（《白 Fang》，叙述地点，育空）。因此，带有观察的 ODA 提供了正确的答案——育空。这个案例展示了观察模块如何提高动作选择和反思的准确性，从而增强了
    ODA 的推理能力。'
- en: By incorporating observation information, ODA reasoning power undergoes a dramatic
    leap, therefore generate an accurate answers. This boost stems from the synergistic
    interplay between the observation module, harnessing the KG’s autonomous reasoning
    capabilities, and LLM, which further amplifies those strengths.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合观察信息，ODA 的推理能力经历了剧烈的提升，因此生成了准确的答案。这一提升源于观察模块与知识图谱自主推理能力的协同作用，以及进一步放大这些优势的
    LLM。
- en: '| Method | QALD10-en | Creak | T-REx | Zero-Shot RE | Average |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | QALD10-en | Creak | T-REx | Zero-Shot RE | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Without Observation | 61.26 | 95.50 | 82.00 | 91.75 | 82.63 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 无观察 | 61.26 | 95.50 | 82.00 | 91.75 | 82.63 |'
- en: '| Similarity-based Reflection | 61.26 | 95.20 | 83.20 | 93.50 | 83.29 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 基于相似性反思 | 61.26 | 95.20 | 83.20 | 93.50 | 83.29 |'
- en: '| Random-based Reflection | 58.56 | 89.00 | 79.50 | 92.00 | 79.77 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 随机基反思 | 58.56 | 89.00 | 79.50 | 92.00 | 79.77 |'
- en: '| Generated-fact Reflection | 63.66 | 91.00 | 80.00 | 93.75 | 82.10 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 生成事实反思 | 63.66 | 91.00 | 80.00 | 93.75 | 82.10 |'
- en: '| ODA | 66.67 | 96.00 | 85.40 | 95.00 | 85.77 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ODA | 66.67 | 96.00 | 85.40 | 95.00 | 85.77 |'
- en: 'Table 3: Ablation Comparison'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：消融比较
- en: 4.2 Effect of Observation on Reflection
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 观察对反思的影响
- en: In this section, we discuss the impact of observation on reflection module.
    Three non-observation reflection methods were designed to verify whether observation
    can enhance the effectiveness of reflection. The similarity-based involves reflecting
    on the triples from action steps by calculating similarity. In this approach,
    triples are first sorted based on the similarity score between the $r+t$.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了观察对反思模块的影响。设计了三种无观察反思方法来验证观察是否能增强反思的效果。基于相似性的方法涉及通过计算相似性来反思动作步骤中的三元组。在这种方法中，三元组首先根据$r+t$之间的相似性得分进行排序。
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.1 Effect of Observation ‣ 4 Discussion ‣ ODA:
    Observation-Driven Agent for integrating LLMs and Knowledge Graphs") showcases
    our ODA dominance over all three non-observation methods. It achieved an average
    accuracy increase of 2.48% compared to the similarity-based method, 6.00% compared
    to the random-based method, and 3.66% compared to the generated-fact method.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S4.T3 "Table 3 ‣ 4.1 Effect of Observation ‣ 4 Discussion ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs") 展示了我们的 ODA 在所有三种无观察方法中的优势。与基于相似性的方法相比，平均准确率提高了
    2.48%，与基于随机的方法相比提高了 6.00%，与生成事实的方法相比提高了 3.66%。'
- en: 'In specific scenarios (see Table [6](#A1.T6 "Table 6 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs")),
    when answering the question What is the capital of the prefecture Tokyo?, the
    generated-fact method resulted in problematic facts, such as Tokyo is the capital
    of Tokyo,Tokyo is the capital of Japan. These were essentially hallucinations
    created by the LLM based on the given question. They misled the agent resulting
    in incorrect answers. In contrast, the reflection of our ODA leveraging observation
    yielded factual knowledge, (Tokyo, capital, Shinjuku), consequently enabling the
    ODA to answer the question correctly.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在特定场景下（参见表格 [6](#A1.T6 "Table 6 ‣ Appendix A Case Study ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs")），当回答“东京府的首都是什么？”的问题时，生成事实的方法产生了问题事实，如“东京是东京的首都”、“东京是日本的首都”。这些基本上是LLM根据给定问题生成的幻觉，它们误导了代理，导致了错误的回答。相比之下，我们的ODA通过观察的反映产生了实际的知识，（东京，首都，新宿），因此使得ODA能够正确回答问题。'
- en: 'The findings of Table [3](#S4.T3 "Table 3 ‣ 4.1 Effect of Observation ‣ 4 Discussion
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs") reveal
    that observation enables reflection module to generate more accurate memories,
    which translates to improved question-answering accuracy for ODA. This result
    underscores the value of both leveraging KG autonomous reasoning capabilities
    and fostering deep collaboration between KG and LLMs.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [3](#S4.T3 "Table 3 ‣ 4.1 Effect of Observation ‣ 4 Discussion ‣ ODA: Observation-Driven
    Agent for integrating LLMs and Knowledge Graphs") 的发现表明，观察使反射模块能够生成更准确的记忆，这转化为ODA的问答准确性提高。这一结果强调了利用KG自主推理能力以及促进KG与LLMs深度合作的价值。'
- en: 4.3 Performance across Different Backbone Models
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 不同主干模型的性能
- en: 'We evaluated the effectiveness of ODA across various backbones by analyzing
    its impact on T-REx and QALD10-en datasets. Our ODA approach significantly outperformed
    the direct answering methods using GPT-3.5 and GPT-4, as evidenced by the Table
    [4](#S4.T4 "Table 4 ‣ 4.3 Performance across Different Backbone Models ‣ 4 Discussion
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs"). Notably,
    ODA yielded a remarkable 30.4% improvement with the direct answering using GPT-3.5
    model on QALD10-en dataset. This experiment suggests the generalizability of ODA across
    different LLMs.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过分析ODA在T-REx和QALD10-en数据集上的影响，评估了ODA在各种主干上的效果。我们的ODA方法显著优于使用GPT-3.5和GPT-4的直接回答方法，正如表格
    [4](#S4.T4 "Table 4 ‣ 4.3 Performance across Different Backbone Models ‣ 4 Discussion
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs") 所示。值得注意的是，ODA在使用GPT-3.5模型对QALD10-en数据集进行直接回答时实现了30.4%的显著提升。这项实验表明ODA在不同LLM之间的泛化能力。'
- en: '| Method | T-REx | QALD10-en |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | T-REx | QALD10-en |'
- en: '| --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Direct answering(GPT3.5) | 37.60 | 44.74 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 直接回答(GPT3.5) | 37.60 | 44.74 |'
- en: '| ODA (GPT3.5) | 68.00 | 49.71 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ODA (GPT3.5) | 68.00 | 49.71 |'
- en: '| Direct answering(GPT4) | 57.44 | 57.10 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 直接回答(GPT4) | 57.44 | 57.10 |'
- en: '| ODA (GPT4) | 86.00 | 66.67 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ODA (GPT4) | 86.00 | 66.67 |'
- en: 'Table 4: Performance comparison using different backbone models'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 4: 使用不同主干模型的性能比较'
- en: 5 Related Works
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: KG-enhanced LLM
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KG增强LLM
- en: Knowledge Graph-enhanced Language Models utilize two primary methodologies when
    tackling tasks that require integration with KGs. The first involves the extraction
    of relevant triples from KGs in response to posed questions. Wang et al. ([2023b](#bib.bib24))
    prompt LLMs to generate explicit knowledge evidence structured as triples, while
    Jiang et al. ([2023](#bib.bib8)) develop specialized interfaces for gathering
    pertinent evidence from structured data, enabling LLMs to focus on reasoning tasks
    based on this information. Similarly, Luo et al. ([2024](#bib.bib10)) employ a
    planning-retrieval-reasoning framework to generate relation paths grounded by
    KGs, thereby enhancing the reasoning capabilities of LLMs. Baek et al. ([2023](#bib.bib1))
    retrieve facts related to the input question by assessing semantic similarities
    between the question and associated facts, subsequently prepending these facts
    to the input. Meanwhile, Li et al. ([2023](#bib.bib9)) iteratively refine reasoning
    rationales by adapting knowledge from the KG. Wang et al. ([2023a](#bib.bib23))
    dissect complex questions using predefined templates, retrieve entities from the
    KG, and generate answers accordingly.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱增强的语言模型在处理需要与KG整合的任务时，采用了两种主要方法。第一种方法涉及从KG中提取与提问相关的三元组。王等人 ([2023b](#bib.bib24))
    促使LLMs生成以三元组结构化的明确知识证据，而姜等人 ([2023](#bib.bib8)) 开发了专门的接口，用于从结构化数据中收集相关证据，使LLMs能够根据这些信息专注于推理任务。类似地，罗等人
    ([2024](#bib.bib10)) 采用计划-检索-推理框架，生成以KG为基础的关系路径，从而提升LLMs的推理能力。白等人 ([2023](#bib.bib1))
    通过评估问题与相关事实之间的语义相似性来检索与输入问题相关的事实，然后将这些事实添加到输入中。与此同时，李等人 ([2023](#bib.bib9)) 通过调整KG中的知识来迭代改进推理理由。王等人
    ([2023a](#bib.bib23)) 使用预定义模板解析复杂问题，从KG中检索实体，并据此生成答案。
- en: The second approach employs an explore-exploit strategy that guides the knowledge
    utilization process within the graph. Sun et al. ([2023b](#bib.bib20)) perform
    an iterative beam search on the KG to identify the most promising reasoning pathways
    and report the outcomes. Guo et al. ([2023](#bib.bib7)) selectively accumulate
    supporting information from the KG through an iterative process that incorporates
    insights from the LLM to address the question.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法采用探索-利用策略，指导图中的知识利用过程。孙等人 ([2023b](#bib.bib20)) 对知识图谱（KG）进行迭代束搜索，以识别最有前途的推理路径并报告结果。郭等人
    ([2023](#bib.bib7)) 通过迭代过程从KG中选择性地积累支持信息，并结合大型语言模型（LLM）的见解来解答问题。
- en: AI Agent
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工智能代理
- en: In the domain of AI agents, Yao et al. ([2022](#bib.bib27)) utilize LLMs to
    interleave the generation of reasoning traces with task-specific actions. Wu et al.
    ([2023](#bib.bib26)) design an agent framework that is adaptable, conversational,
    and capable of operating in various modes that leverage combinations of LLMs,
    human input, and auxiliary tools. Chen et al. ([2023](#bib.bib3)) focus on creating
    expert agents capable of solving complex tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能代理领域，姚等人 ([2022](#bib.bib27)) 利用LLMs将推理痕迹的生成与特定任务的动作交织在一起。吴等人 ([2023](#bib.bib26))
    设计了一个适应性强、对话式的代理框架，能够在利用LLMs、人类输入和辅助工具的组合中以各种模式操作。陈等人 ([2023](#bib.bib3)) 专注于创建能够解决复杂任务的专家代理。
- en: 6 Conclusion
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this work, we design ODA framework for KG-centric tasks. In ODA, we introduce
    KG observation mechanism to autonomously combine the reasoning abilities of KG
    with LLM. We first propose the observation method to mitigate the problem of explosive
    number of triples in KG when tackling complex tasks. Then we fuse the observation
    into the action and reflection modules to further enhance the overall performance.
    We conduct extensive experiments, and the results clearly illustrate the effectiveness
    of our framework, highlighting its capability to enhance performance across four
    KBQA datasets, particularly in handling complicated questions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们设计了用于KG中心任务的ODA框架。在ODA中，我们引入了KG观察机制，以自主地将KG的推理能力与LLM结合。我们首先提出了观察方法，以减轻在处理复杂任务时KG中三元组数量爆炸的问题。然后，我们将观察融入到行动和反思模块中，以进一步提升整体性能。我们进行了广泛的实验，结果清晰地展示了我们框架的有效性，突显了其在处理复杂问题时在四个KBQA数据集上的性能提升能力。
- en: Limitation
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Given the diverse nature of KG-related tasks spanning multiple domains and requiring
    a broad range of capabilities, the implementation of a multi-agent system is promising
    to significantly enhance task performance. We leave the integration of our observation
    mechanism with multi-agent system to future work.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于KG相关任务的多样性涵盖了多个领域，并且需要广泛的能力，实现多代理系统有望显著提升任务性能。我们将把我们的观察机制与多代理系统的集成留待未来工作。
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Baek et al. (2023) Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-augmented
    language model prompting for zero-shot knowledge graph question answering. *arXiv
    preprint arXiv:2306.04136*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baek等（2023）Jinheon Baek, Alham Fikri Aji, 和 Amir Saffari. 2023. 知识增强语言模型提示用于零样本知识图谱问答。*arXiv预印本
    arXiv:2306.04136*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell等.
    2020. 语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Chen et al. (2023) Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay,
    Börje F Karlsson, Jie Fu, and Yemin Shi. 2023. Autoagents: A framework for automatic
    agent generation. *arXiv preprint arXiv:2309.17288*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等（2023）Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje
    F Karlsson, Jie Fu, 和 Yemin Shi. 2023. Autoagents: 自动代理生成框架。*arXiv预印本 arXiv:2309.17288*。'
- en: 'Do et al. (2024) Quyet V Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, and
    Yangqiu Song. 2024. Constraintchecker: A plugin for large language models to reason
    on commonsense knowledge bases. *arXiv preprint arXiv:2401.14003*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Do等（2024）Quyet V Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, 和 Yangqiu Song.
    2024. Constraintchecker: 一个插件，供大语言模型在常识知识库上进行推理。*arXiv预印本 arXiv:2401.14003*。'
- en: 'Elsahar et al. (2018) Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe
    Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. [T-REx:
    A large scale alignment of natural language with knowledge base triples](https://aclanthology.org/L18-1544).
    In *Proceedings of the Eleventh International Conference on Language Resources
    and Evaluation (LREC 2018)*, Miyazaki, Japan. European Language Resources Association
    (ELRA).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elsahar等（2018）Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier,
    Jonathon Hare, Frederique Laforest, 和 Elena Simperl. 2018. [T-REx: 大规模自然语言与知识库三元组的对齐](https://aclanthology.org/L18-1544).
    见于 *第十一届国际语言资源与评估会议（LREC 2018）*，日本宫崎。欧洲语言资源协会（ELRA）。'
- en: 'Glass et al. (2022) Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury,
    Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. [Re2G: Retrieve, rerank, generate](https://doi.org/10.18653/v1/2022.naacl-main.194).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2701–2715,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Glass等（2022）Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita
    Naik, Pengshan Cai, 和 Alfio Gliozzo. 2022. [Re2G: 检索、重新排序、生成](https://doi.org/10.18653/v1/2022.naacl-main.194).
    见于 *2022年北美计算语言学协会会议：人类语言技术会议论文集*，第2701–2715页，美国西雅图。计算语言学协会。'
- en: 'Guo et al. (2023) Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li,
    Jiawei Tang, Dapeng Li, and Yingyou Wen. 2023. Knowledgenavigator: Leveraging
    large language models for enhanced reasoning over knowledge graph. *arXiv preprint
    arXiv:2312.15880*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo等（2023）Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei
    Tang, Dapeng Li, 和 Yingyou Wen. 2023. Knowledgenavigator: 利用大语言模型增强知识图谱的推理能力。*arXiv预印本
    arXiv:2312.15880*。'
- en: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language
    model to reason over structured data. *arXiv preprint arXiv:2305.09645*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang等（2023）Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao,
    和 Ji-Rong Wen. 2023. Structgpt: 一个通用框架，用于大语言模型对结构化数据进行推理。*arXiv预印本 arXiv:2305.09645*。'
- en: 'Li et al. (2023) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq
    Joty, Soujanya Poria, and Lidong Bing. 2023. Chain-of-knowledge: Grounding large
    language models via dynamic knowledge adapting over heterogeneous sources. *arXiv
    preprint arXiv:2305.13269*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等（2023）Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty,
    Soujanya Poria, 和 Lidong Bing. 2023. Chain-of-knowledge: 通过对异质源的动态知识适应来基础化大语言模型。*arXiv预印本
    arXiv:2305.13269*。'
- en: 'Luo et al. (2024) Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui
    Pan. 2024. [Reasoning on graphs: Faithful and interpretable large language model
    reasoning](http://arxiv.org/abs/2310.01061).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等（2024）Linhao Luo、Yuan-Fang Li、Gholamreza Haffari和Shirui Pan。2024年。[图上的推理：忠实且可解释的大型语言模型推理](http://arxiv.org/abs/2310.01061)。
- en: Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask
    finetuning. *arXiv preprint arXiv:2211.01786*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff等（2022）Niklas Muennighoff、Thomas Wang、Lintang Sutawika、Adam Roberts、Stella
    Biderman、Teven Le Scao、M Saiful Bari、Sheng Shen、Zheng-Xin Yong、Hailey Schoelkopf等。2022年。通过多任务微调实现跨语言泛化。*arXiv预印本
    arXiv:2211.01786*。
- en: 'Onoe et al. (2021) Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg
    Durrett. 2021. [Creak: A dataset for commonsense reasoning over entity knowledge](http://arxiv.org/abs/2109.01653).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Onoe等（2021）Yasumasa Onoe、Michael J. Q. Zhang、Eunsol Choi和Greg Durrett。2021年。[Creak：一个用于实体知识常识推理的数据集](http://arxiv.org/abs/2109.01653)。
- en: OpenAI (2023) R OpenAI. 2023. Gpt-4 technical report. *arXiv*, pages 2303–08774.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）R OpenAI。2023年。Gpt-4技术报告。*arXiv*，第2303–08774页。
- en: 'Pan et al. (2024) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. 2024. Unifying large language models and knowledge graphs: A roadmap.
    *IEEE Transactions on Knowledge and Data Engineering*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan等（2024）Shirui Pan、Linhao Luo、Yufei Wang、Chen Chen、Jiapu Wang和Xindong Wu。2024年。统一大型语言模型和知识图谱：一条路线图。*IEEE知识与数据工程学报*。
- en: 'Perevalov et al. (2022) Aleksandr Perevalov, Dennis Diefenbach, Ricardo Usbeck,
    and Andreas Both. 2022. [Qald-9-plus: A multilingual dataset for question answering
    over dbpedia and wikidata translated by native speakers](https://doi.org/10.1109/ICSC52841.2022.00045).
    In *2022 IEEE 16th International Conference on Semantic Computing (ICSC)*, pages
    229–234.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perevalov等（2022）Aleksandr Perevalov、Dennis Diefenbach、Ricardo Usbeck和Andreas
    Both。2022年。[Qald-9-plus：一个用于dbpedia和wikidata的多语言问答数据集，由母语者翻译](https://doi.org/10.1109/ICSC52841.2022.00045)。发表于*2022
    IEEE第16届国际语义计算会议（ICSC）*，第229–234页。
- en: 'Petroni et al. (2021) Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
    Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
    Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.
    [KILT: a benchmark for knowledge intensive language tasks](https://doi.org/10.18653/v1/2021.naacl-main.200).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2523–2544,
    Online. Association for Computational Linguistics.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni等（2021）Fabio Petroni、Aleksandra Piktus、Angela Fan、Patrick Lewis、Majid
    Yazdani、Nicola De Cao、James Thorne、Yacine Jernite、Vladimir Karpukhin、Jean Maillard、Vassilis
    Plachouras、Tim Rocktäschel和Sebastian Riedel。2021年。[KILT：一个知识密集型语言任务的基准](https://doi.org/10.18653/v1/2021.naacl-main.200)。发表于*2021年北美计算语言学协会：人类语言技术会议论文集*，第2523–2544页，在线。计算语言学协会。
- en: Santana et al. (2022) Manuel Alejandro Borroto Santana, Bernardo Cuteri, Francesco
    Ricca, and Vito Barbara. 2022. [SPARQLQA enters the QALD challenge](https://ceur-ws.org/Vol-3196/paper3.pdf).
    In *Proceedings of the 7th Natural Language Interfaces for the Web of Data (NLIWoD)
    co-located with the 19th European Semantic Web Conference (ESWC 2022)*, volume
    3196 of *CEUR Workshop Proceedings*, pages 25–31, Hersonissos, Greece. CEUR-WS.org.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santana等（2022）Manuel Alejandro Borroto Santana、Bernardo Cuteri、Francesco Ricca和Vito
    Barbara。2022年。[SPARQLQA进入QALD挑战](https://ceur-ws.org/Vol-3196/paper3.pdf)。发表于*第七届自然语言接口数据网（NLIWoD）会议，与第十九届欧洲语义网会议（ESWC
    2022）同时举行*，*CEUR Workshop Proceedings*第3196卷，第25–31页，希腊赫尔索尼索斯。CEUR-WS.org。
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scao等（2022）Teven Le Scao、Angela Fan、Christopher Akiki、Ellie Pavlick、Suzana Ilić、Daniel
    Hesslow、Roman Castagné、Alexandra Sasha Luccioni、François Yvon、Matthias Gallé等。2022年。Bloom：一个176b参数的开放访问多语言模型。*arXiv预印本
    arXiv:2211.05100*。
- en: 'Sun et al. (2023a) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2023a. [Think-on-graph:
    Deep and responsible reasoning of large language model on knowledge graph](http://arxiv.org/abs/2307.07697).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2023a）Jiashuo Sun、Chengjin Xu、Lumingyuan Tang、Saizhuo Wang、Chen Lin、Yeyun
    Gong、Lionel M. Ni、Heung-Yeung Shum和Jian Guo。2023a年。[Think-on-graph：大型语言模型在知识图谱上的深度和负责任推理](http://arxiv.org/abs/2307.07697)。
- en: 'Sun et al. (2023b) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023b. Think-on-graph: Deep
    and responsible reasoning of large language model with knowledge graph. *arXiv
    preprint arXiv:2307.07697*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023b) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Heung-Yeung Shum, 和 Jian Guo. 2023b. Think-on-graph：基于知识图谱的大型语言模型的深度和负责任推理。*arXiv
    预印本 arXiv:2307.07697*。
- en: 'Sun et al. (2019) Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang.
    2019. [Rotate: Knowledge graph embedding by relational rotation in complex space](http://arxiv.org/abs/1902.10197).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, 和 Jian Tang. 2019.
    [Rotate：通过复杂空间中的关系旋转进行知识图谱嵌入](http://arxiv.org/abs/1902.10197)。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. 2023. Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。
- en: 'Wang et al. (2023a) Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen,
    Xinrun Wang, Lei Feng, and Bo An. 2023a. keqing: knowledge-based question answering
    is a nature chain-of-thought mentor of llm. *arXiv preprint arXiv:2401.00426*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023a) Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen,
    Xinrun Wang, Lei Feng, 和 Bo An. 2023a. keqing: 基于知识的问题回答是 LLM 的一种自然思维链导师。*arXiv
    预印本 arXiv:2401.00426*。'
- en: Wang et al. (2023b) Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and Ming Gao.
    2023b. Boosting language models reasoning with chain-of-knowledge prompting. *arXiv
    preprint arXiv:2306.06427*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Jianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, 和 Ming Gao.
    2023b. 通过知识链提示提升语言模型的推理能力。*arXiv 预印本 arXiv:2306.06427*。
- en: Wang et al. (2023c) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. [Self-consistency improves
    chain of thought reasoning in language models](http://arxiv.org/abs/2203.11171).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023c) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023c. [自洽性提升语言模型中的思维链推理](http://arxiv.org/abs/2203.11171)。
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. 2023. Autogen：通过多智能体对话框架启用下一代
    LLM 应用。*arXiv 预印本 arXiv:2308.08155*。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022. React：在语言模型中协同推理和行动。*arXiv 预印本 arXiv:2210.03629*。
- en: 'Yu et al. (2022) Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuohang Wang, Zhuosheng
    Zhang, Yuwei Fang, and Meng Jiang. 2022. [Retrieval augmentation for commonsense
    reasoning: A unified approach](https://doi.org/10.18653/v1/2022.emnlp-main.294).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 4364–4377, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) Wenhao Yu, Chenguang Zhu, Zhihan Zhang, Shuohang Wang, Zhuosheng
    Zhang, Yuwei Fang, 和 Meng Jiang. 2022. [用于常识推理的检索增强：统一方法](https://doi.org/10.18653/v1/2022.emnlp-main.294)。在
    *2022 年自然语言处理经验方法会议论文集*，第 4364–4377 页，阿布扎比，阿联酋。计算语言学协会。
- en: Zhang et al. (2023) Yichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen. 2023.
    Making large language models perform better in knowledge graph completion. *arXiv
    preprint arXiv:2310.06671*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Yichi Zhang, Zhuo Chen, Wen Zhang, 和 Huajun Chen. 2023.
    提升大语言模型在知识图谱补全中的表现。*arXiv 预印本 arXiv:2310.06671*。
- en: Appendix A Case Study
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 案例研究
- en: 'To demonstrate ODA effectiveness, we highlight several representative cases
    from the QALD10-en dataset. For improved readability, we replaced entity IDs in
    Outputs with their corresponding names. We selected two examples to illustrate
    how ODA utilizes observation to correctly address the question in Table [5](#A1.T5
    "Table 5 ‣ Appendix A Case Study ‣ ODA: Observation-Driven Agent for integrating
    LLMs and Knowledge Graphs") and Table [6](#A1.T6 "Table 6 ‣ Appendix A Case Study
    ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示ODA的有效性，我们从QALD10-en数据集中突出几个代表性案例。为了提高可读性，我们用对应的名称替换了输出中的实体ID。我们选择了两个例子来说明ODA如何利用观察正确回答表[5](#A1.T5
    "表 5 ‣ 附录 A 案例研究 ‣ ODA：观察驱动的智能体用于整合LLM和知识图谱")和表[6](#A1.T6 "表 6 ‣ 附录 A 案例研究 ‣ ODA：观察驱动的智能体用于整合LLM和知识图谱")中的问题。
- en: '| Question | Where are both The Call of the Wild and White Fang set, the most
    two famous works of Jack London? |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 《荒野呼唤》和《白牙》这两部杰克·伦敦最著名的作品的背景分别在哪里？ |'
- en: '| --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Action | Thought: The question asks about the setting of two works, The Call
    of the Wild and White Fang, both by Jack London. The observation provides some
    information about the narrative locations of these works, but to confirm and provide
    a specific answer, I will use the GetNeighbor function on The Call of the Wild.
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 思考：问题询问两部由杰克·伦敦创作的作品《荒野呼唤》和《白牙》的背景。观察提供了一些关于这些作品叙事位置的信息，但为了确认并提供具体答案，我将对《荒野呼唤》使用GetNeighbor函数。
    |'
- en: '|  | Action: GetNeighbor |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动：GetNeighbor |'
- en: '|  | Entity_id: The Call of the Wild |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | 实体_id：荒野呼唤 |'
- en: '| Reflection | Thought: First, I will focus on the triples related to the locations
    of the two works, The Call of the Wild and White Fang, as the question is asking
    about the settings of these two novels. I will also consider the triples related
    to the author, Jack London, as the question mentions him. I will select the triples
    that are most directly related to the settings of the two novels. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 反思 | 思考：首先，我将重点关注与《荒野呼唤》和《白牙》的位置相关的三元组，因为问题询问的是这两部小说的背景。我还会考虑与作者杰克·伦敦相关的三元组，因为问题中提到了他。我将选择与这两部小说的背景直接相关的三元组。
    |'
- en: '|  | Triples: (The Call of the Wild, Narrative location, Yukon), (The Call
    of the Wild, Narrative location, Canada), (The Call of the Wild, author, Jack
    London) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | 三元组：（荒野呼唤，叙事位置，育空），（荒野呼唤，叙事位置，加拿大），（荒野呼唤，作者，杰克·伦敦） |'
- en: '| Action | Thought:The question asks for the setting of both The Call of the
    Wild and White Fang, two notable works of Jack London. From the observation, we
    can see that The Call of the Wild has narrative locations in the United States
    of America, Alaska, Canada, and Yukon. However, we don’t have the narrative location
    for White Fang. Therefore, I will use the GetNeighbor function on the entityID
    of White Fang to find its narrative location. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 思考：问题要求提供《荒野呼唤》和《白牙》这两部杰克·伦敦的著名作品的背景。从观察中，我们可以看到《荒野呼唤》的叙事位置包括美国、阿拉斯加、加拿大和育空。然而，我们没有《白牙》的叙事位置。因此，我将对《白牙》的entityID使用GetNeighbor函数来找到它的叙事位置。
    |'
- en: '|  | Action: GetNeighbor |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动：GetNeighbor |'
- en: '|  | Entity_id: White Fang |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | 实体_id：白牙 |'
- en: '| Reflection | Thought:First, I will look for triples that directly answer
    the question. The question is asking for the setting of two works by Jack London,
    namely, The Call of the Wild and White Fang. Therefore, I will look for triples
    that contain these two works and a relation that indicates a setting or location.
    From the candidate triples, I can see that there are two triples that indicate
    a narrative location for White Fang: (White Fang, Narrative location, Yukon),
    (White Fang, Narrative location, Canada). |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 反思 | 思考：首先，我会寻找直接回答问题的三元组。问题是关于杰克·伦敦的两部作品《荒野呼唤》和《白牙》的背景。因此，我会寻找包含这两部作品以及指示背景或位置的关系的三元组。从候选三元组中，我可以看到有两个三元组指示《白牙》的叙事位置：（白牙，叙事位置，育空），（白牙，叙事位置，加拿大）。
    |'
- en: '|  | Triples: (White Fang, Narrative location, Yukon), (White Fang, Narrative
    location, Canada) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | 三元组：（白牙，叙事位置，育空），（白牙，叙事位置，加拿大） |'
- en: '| Action | Thought:The question is asking for the locations where both The
    Call of the Wild and White Fang are set. From the reference memory, I can see
    that The Call of the Wild is set in the United States of America, Alaska, Canada,
    and Yukon. White Fang is set in Canada and Yukon. The common locations for both
    books are Canada and Yukon. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 思考：问题是在询问《野性的呼唤》和《白牙》的设定地点。从参考记忆中，我可以看到《野性的呼唤》的设定地点是美国、阿拉斯加、加拿大和育空。
    《白牙》的设定地点是加拿大和育空。这两本书的共同地点是加拿大和育空。 |'
- en: '|  | Action: Answer |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 操作：回答 |'
- en: '| Answer | [Canada,Yukon] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 答案 | [加拿大,育空] |'
- en: 'Table 5: Case 1'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：案例1
- en: '| Question | The question is What is the capital of the prefecture Tokyo ?
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 问题是东京都的首都是什么？ |'
- en: '| --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Action | Thought: The question is asking for the capital of the prefecture
    Tokyo. The candidate entity ID ’Q1490’ corresponds to Tokyo. I can see from the
    observation that there is a triple (Tokyo, capital, Shinjuku) which might answer
    the question. However, to confirm this, I will execute a GetNeighbor action on
    ’Q1490’ to get all the triples where Tokyo is the head. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 思考：问题是在询问东京都的首都。候选实体ID ''Q1490'' 对应东京。我可以从观察中看到有一个三元组（东京，首都，新宿），这可能回答了这个问题。然而，为了确认，我将对
    ''Q1490'' 执行 GetNeighbor 操作，以获取所有以东京为主体的三元组。 |'
- en: '|  | Action: GetNeighbor |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | 操作：GetNeighbor |'
- en: '|  | Entity_id: Tokyo |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | 实体_id: 东京 |'
- en: '| Reflection | Thought: The question is asking for the capital of Tokyo. From
    the observation, we can see that Tokyo is the capital of Japan and it is a prefecture
    of Japan. The capital of Tokyo is Shinjuku. Therefore, we should select the triples
    that contain this information. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 反思 | 思考：问题是在询问东京的首都。从观察中我们可以看到，东京是日本的首都，且它是日本的一个行政区。东京的首都为新宿。因此，我们应该选择包含这些信息的三元组。
    |'
- en: '|  | Triples: (Tokyo, instance of, prefecture of Japan), (Tokyo, capital ,
    Shinjuku), (Tokyo, capital, Japan) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | 三元组：（东京，属于，日本的行政区），（东京，首都，新宿），（东京，首都，日本） |'
- en: '| Action | Thought:The question is asking for the capital of the prefecture
    Tokyo. From the reference memory, it is stated that the capital of Tokyo is Shinjuku.
    Therefore, the answer to the question is Shinjuku. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 思考：问题是在询问东京都的首都。从参考记忆中可以看出，东京都的首都为新宿。因此，问题的答案是新宿。 |'
- en: '|  | Action: Answer |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | 操作：回答 |'
- en: '| Answer | Shinjuku |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 答案 | 新宿 |'
- en: 'Table 6: Case 2'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：案例2
- en: Appendix B Prompt
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B提示
- en: 'This section presents the prompts for the action and reflection modules in
    tables [7](#A2.T7 "Table 7 ‣ Appendix B Prompt ‣ ODA: Observation-Driven Agent
    for integrating LLMs and Knowledge Graphs") and [8](#A2.T8 "Table 8 ‣ Appendix
    B Prompt ‣ ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs").'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '本节展示了行动和反思模块的提示，详见表格[7](#A2.T7 "表7 ‣ 附录B提示 ‣ ODA: 观察驱动代理，用于集成LLMs和知识图谱")和[8](#A2.T8
    "表8 ‣ 附录B提示 ‣ ODA: 观察驱动代理，用于集成LLMs和知识图谱")。'
- en: '| Action | Prompt |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 提示 |'
- en: '| --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Neighbor Exploration & Path Discovery | Agent Instructions: |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 邻居探索与路径发现 | 代理指令： |'
- en: '| You function as an agent that provides answers based on a knowledge graph.
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 你作为一个基于知识图谱提供答案的代理。 |'
- en: '| To assist you in querying the KB, use the following tools: |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 为了帮助你查询知识库，请使用以下工具： |'
- en: '|  | GetNeighbor(entityID: str) -> List[Tuple[str, str, str]]: |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | GetNeighbor(entityID: str) -> List[Tuple[str, str, str]]: |'
- en: '|  | Description: Returns triplets containing the given entityID as the head
    and its corresponding entityID as the tail. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 描述：返回包含给定实体ID作为主体和对应实体ID作为宾语的三元组。 |'
- en: '|  | GetPath(entityID1: str, entityID2: str) -> List[List[Tuple[str, str, str]]]:
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | GetPath(entityID1: str, entityID2: str) -> List[List[Tuple[str, str, str]]]:
    |'
- en: '|  | Description: Returns all triplets linking the two given entityIDs. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | 描述：返回所有连接两个给定实体ID的三元组。 |'
- en: '|  | Example Usage:GetPath("Q30", "Q25231") returns all triplets connecting
    ’Q30’ and ’Q25231’. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | 示例用法：GetPath("Q30", "Q25231") 返回所有连接 ’Q30’ 和 ’Q25231’ 的三元组。 |'
- en: '|  | Data Provided to You: |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | 提供给你的数据： |'
- en: '|  | Question:[Question] |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 问题：[Question] |'
- en: '|  | Memory: [Memory] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | 记忆：[Memory] |'
- en: '|  | Candidate EntityIDs: [Task-relevant EntityIDs] (Choose 1 or 2 based on
    the action) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | 候选实体ID：[Task-relevant EntityIDs]（根据操作选择1或2） |'
- en: '|  | Observation: [Observation] (These serve as a reference to assist you in
    selecting the appropriate entityID from the Candidate EntityIDs) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 观察：[Observation]（这些作为参考，帮助你从候选实体ID中选择合适的实体ID） |'
- en: '|  | Labels: [Task-relevant entities labels] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | 标签：[Task-relevant entities labels] |'
- en: '|  | Action History: [historical action] (Avoid these actions) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作历史：[historical action]（避免这些动作） |'
- en: '|  | Guidelines: |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | 指导方针： |'
- en: '|  | Choose only one action at a time. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | 一次只选择一个动作。 |'
- en: '|  | For GetPath, select two entityIDs. For GetNeighbor, select one entityID.
    If there are less than 2 entityIDs available, only choose the GetNeighbor action.
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | 对于GetPath，选择两个entityID。对于GetNeighbor，选择一个entityID。如果可用的entityID少于2个，只选择GetNeighbor动作。
    |'
- en: '| Answer | You are a agent that answer questions based on the reference memory
    and your knowledge. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 答案 | 你是一个基于参考记忆和你的知识回答问题的代理。 |'
- en: '|  | Here are the reference memory:[Memory]. You can use it to help you answer
    the quesiton. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | 这里是参考记忆：[Memory]。你可以使用它来帮助你回答问题。 |'
- en: '|  | Here is the question you are asked to answer the question:[Question].
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | 这是你被要求回答的问题：[Question]。 |'
- en: '|  | Ensure that your answer contains one answer or a list of answer, and each
    answer should be only one or several words,a phrase, a number,true or false, or
    a date, no other information or descripation in answer. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | 确保你的答案包含一个答案或一个答案列表，每个答案应仅为一个或几个词、一个短语、一个数字、**真**或**假**，或一个日期，答案中不得包含其他信息或描述。
    |'
- en: 'Table 7: Action Prompt Description'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：动作提示描述
- en: '| Field | Prompt |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 提示 |'
- en: '| --- | --- |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Reflection | You are an agent that provides answers based on a KG. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 反思 | 你是一个基于知识图谱（KG）提供答案的代理。 |'
- en: '|  | You queried some candidate triples [triples] from last action step and
    their corresponding labels:[entities labels] from the KB based on the question:
    [Question]. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | 你根据问题：[Question]，从上一个动作步骤中查询了一些候选三元组[triples]及其对应的标签：[entities labels]。
    |'
- en: '|  | Now you are asked to select related triples, so you can answer the question
    in the future by using them. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 现在你被要求选择相关的三元组，以便你将来可以使用它们来回答问题。 |'
- en: '|  | Here are the observation: [Obervation] for guiding you to select the right
    triples from the candidate triples. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 这里是观察结果：[Observation]，用于指导你从候选三元组中选择正确的三元组。 |'
- en: '|  | Also, here is the memory: [Memory]. You can use it to help you select
    the right triples from the candidate triples. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | 另外，这里是记忆：[Memory]。你可以使用它来帮助你从候选三元组中选择正确的三元组。 |'
- en: '|  | Guidelines: |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | 指导方针： |'
- en: '|  | You can select less than 15 triples from the candidate triples. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | 你可以从候选三元组中选择少于15个三元组。 |'
- en: '|  | Your output triples must be in the format of entityID,relationID,entityID.
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | 你的输出三元组必须采用entityID,relationID,entityID的格式。 |'
- en: 'Table 8: Reflection Prompt Description'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：反思提示描述 |
