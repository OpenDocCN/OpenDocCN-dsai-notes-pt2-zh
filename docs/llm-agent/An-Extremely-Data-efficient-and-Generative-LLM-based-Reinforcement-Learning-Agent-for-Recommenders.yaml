- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: An Extremely Data-efficient and Generative LLM-based Reinforcement Learning
    Agent for Recommenders
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种极其数据高效且基于生成型 LLM 的强化学习推荐系统代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16032](https://ar5iv.labs.arxiv.org/html/2408.16032)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16032](https://ar5iv.labs.arxiv.org/html/2408.16032)
- en: Shuang Feng [fengshuang@gmail.com](mailto:fengshuang@gmail.com) Stanford University
    SCPDPalo AltoCaliforniaUSA  and  Grace Feng [gracefeng@ucsb.eud](mailto:gracefeng@ucsb.eud)
    University of California Santa BarbaraSanta BarbaraCaliforniaUSA(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shuang Feng [fengshuang@gmail.com](mailto:fengshuang@gmail.com) 斯坦福大学 SCPD 帕洛阿尔托
    加利福尼亚 美国 和 Grace Feng [gracefeng@ucsb.eud](mailto:gracefeng@ucsb.eud) 加利福尼亚大学圣塔芭芭拉
    分校 圣塔芭芭拉 加利福尼亚 美国（2024）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Recent advancements in large language models (LLMs) have enabled understanding
    webpage contexts, product details, and human instructions. Utilizing LLMs as the
    foundational architecture for either reward models or policies in reinforcement
    learning has gained popularity - a notable achievement is the success of InstructGPT
    (Ouyang et al., [2022](#bib.bib12)). RL algorithms have been instrumental in maximizing
    long-term customer satisfaction and avoiding short-term, myopic goals in industrial
    recommender systems, which often rely on deep learning models to predict immediate
    clicks or purchases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的进展使得理解网页上下文、产品细节和人类指令成为可能。利用 LLMs 作为强化学习中的奖励模型或策略的基础架构已变得越来越流行——一个显著的成就是
    InstructGPT（Ouyang et al., [2022](#bib.bib12)）的成功。强化学习算法在最大化长期客户满意度和避免短期、近视目标方面发挥了重要作用，这在工业推荐系统中尤为重要，因为这些系统通常依赖深度学习模型来预测即时点击或购买。
- en: In this project, several RL methods are implemented and evaluated using the
    WebShop (Yao et al., [2023](#bib.bib18)) benchmark environment, data, simulator,
    and pre-trained model checkpoints. The goal is to train an RL agent to maximize
    the purchase reward given a detailed human instruction describing a desired product.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，多个强化学习方法在 WebShop（Yao et al., [2023](#bib.bib18)）基准环境、数据、模拟器和预训练模型检查点中得到了实现和评估。目标是训练一个强化学习代理，以最大化给定详细人类指令描述的期望产品的购买奖励。
- en: The RL agents are developed by fine-tuning a pre-trained BERT model with various
    objectives, learning from preferences without a reward model, and employing contemporary
    training techniques such as Proximal Policy Optimization (PPO) as used in InstructGPT
    (Ouyang et al., [2022](#bib.bib12)), and Direct Preference Optimization (DPO)
    (Rafailov et al., [2023](#bib.bib13)). This report also evaluates the RL agents
    trained using generative trajectories. Evaluations were conducted using Thompson
    sampling in the WebShop simulator environment.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些强化学习代理通过微调预训练的 BERT 模型，并设置不同的目标，学习用户的偏好而不依赖于奖励模型，同时使用了现代训练技术，例如在 InstructGPT（Ouyang
    et al., [2022](#bib.bib12)）中使用的邻近政策优化（PPO）和直接偏好优化（DPO）（Rafailov et al., [2023](#bib.bib13)）。本报告还评估了使用生成轨迹训练的强化学习代理。评估是在
    WebShop 模拟环境中使用汤普森采样进行的。
- en: The simulated online experiments demonstrate that DPO outperforms PPO in data
    efficiency and task performance, especially in success rate, using the same amount
    of training time. However, longer training time is necessary for fair comparison
    between the two. Specifically, without utilizing any image, a DPO agent achieved
    a 19% success rate after approximately 3000 steps or 30 minutes of training on
    T4 GPUs, compared to a PPO agent, which reached a 15% success rate after 2 hours
    of training. Results also indicate that agents trained on generated trajectories
    exhibited comparable task performance to those trained using human trajectories.
    This has demonstrated an example of an extremely low-cost data-efficient way of
    training reinforcement learning agents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟在线实验表明，DPO 在数据效率和任务表现方面优于 PPO，特别是在成功率方面，且使用相同的训练时间。然而，为了公平比较这两者，还需要更长的训练时间。具体而言，在未使用任何图像的情况下，一名
    DPO 代理在 T4 GPU 上经过大约 3000 步或 30 分钟的训练后达到了 19% 的成功率，而 PPO 代理经过 2 小时的训练达到了 15% 的成功率。结果还表明，使用生成轨迹训练的代理在任务表现上与使用人类轨迹训练的代理相当。这展示了训练强化学习代理的一种极其低成本且数据高效的方法。
- en: 'LLM, Reinforcement Learning, Recommender, Contrast Learning, Generative AI,
    RLHF, Human Preference, E-commerce^†^†copyright: acmlicensed^†^†journalyear: 2024^†^†conference:
    fengshuang@gmail.com; August 25-29; Barcelona, Spain^†^†isbn: 978-1-4503-XXXX-X/18/06¹¹1This
    paper was originally part of the class project for CS234 Spring 2024 in Stanford
    University and was submitted to KDD’24 on 6/30/2024 for RelKD Workshop. It was
    accepted in July 2024\. See https://github.com/fengshuang-coding/KDD2024 for updates.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advances in Large Language Models (LLMs) have significantly enhanced
    research and applications in understanding human instructions on the web, processing
    webpage text, and grasping context. These advancements have provided valuable
    tools for training reinforcement learning (RL) agents to navigate web environments,
    particularly in e-commerce and various recommender systems such as YouTube and
    Netflix. Leveraging LLMs in RL agent training is relatively new but has proven
    successful. A notable example is InstructGPT (Ouyang et al., [2022](#bib.bib12)),
    where an RL agent was trained using human preferences by fine-tuning GPT-3 models
    with human instructions. Combining LLMs with RL techniques enables the creation
    of intelligent web agents that can understand human instructions and complete
    tasks in web or app environments, thereby maximizing desired rewards.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommender systems have evolved from collaborative filtering (Koren et al.,
    [2009](#bib.bib7)) to the recent surge in deep supervised learning, which predicts
    immediate user responses such as clicks (Covington et al., [2016](#bib.bib5);
    Zhang et al., [2017](#bib.bib19)). This approach has seen tremendous success in
    personalized user engagement. However, after several years in production, deep
    supervised learning algorithms have shown limitations, including: 1) a focus on
    optimizing short-term gains at the expense of long-term user satisfaction and
    retention, and 2) strong feedback loops caused by training data generated from
    these algorithms, which exacerbate these effects. Conversely, RL algorithms are
    designed to optimize long-term gains by learning policies that maximize long-term
    user satisfaction. RL agents are also well-known for their ability to perform
    sequential planning and make decisions based on the Markov Decision Process (MDP)
    properties (Chen et al., [2018](#bib.bib3)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The training of RL agents for recommenders in web environments has been actively
    studied, with several benchmark datasets and trained agents available. For example,
    WikiNav (Nogueira and Cho, [2017](#bib.bib11)) provides a benchmark for web-based
    navigation RL agents. RecoGym (Rohde et al., [2018](#bib.bib14)) offers a benchmark
    for RL agents in production recommendations for online advertising. Virtual-Taobao
    (Shi et al., [2019](#bib.bib16)) includes a virtual online shopping environment
    derived from Taobao, hosting several RL algorithms for product recommendations.
    WebShop (Yao et al., [2023](#bib.bib18)) presents a simulated e-commerce web environment
    with over 1,600 human demonstrations for web shopping tasks based on human text
    instructions. This environment includes 1.18 million products with text and image
    descriptions, along with 12,087 crowd-sourced text instructions. The authors of
    WebShop also explored several imitation and RL agents trained using real-world
    human trajectories.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络环境中对推荐系统的 RL 代理进行训练已得到积极研究，并有若干基准数据集和训练代理可用。例如，WikiNav (Nogueira 和 Cho, [2017](#bib.bib11))
    提供了用于基于网络的导航 RL 代理的基准。RecoGym (Rohde et al., [2018](#bib.bib14)) 提供了在线广告生产推荐中的
    RL 代理基准。Virtual-Taobao (Shi et al., [2019](#bib.bib16)) 包含了一个来源于淘宝的虚拟在线购物环境，并托管了若干用于产品推荐的
    RL 算法。WebShop (Yao et al., [2023](#bib.bib18)) 提供了一个模拟的电子商务网络环境，包含超过 1600 人类演示，基于人类文本指令进行网络购物任务。该环境包含
    118 万个带有文本和图像描述的产品，以及 12,087 个众包文本指令。WebShop 的作者还探索了若干使用真实人类轨迹训练的模仿和 RL 代理。
- en: Previous explorations in RL for web-based recommenders are extensive. Query
    reformulation, as published in (Nogueira and Cho, [2017](#bib.bib11)), is part
    of an RL problem aimed at optimizing outcomes. In this context, search engines
    are considered black boxes, and the RL agent (or reformulator) learns to generate
    queries that maximize the expected return through actions in the state space.
    This paper, published in 2017, predates the widespread use of BERT (Devlin et al.,
    [2019](#bib.bib6)). The authors proposed a PRF framework, with CNN/RNN serving
    as the contextual learner and query generator. A more recent work proposed the
    concept of ”learning to search” (Adolphs et al., [2021](#bib.bib2)), where a search
    agent mimics the interactive process by generating interactive search queries
    based on previous queries and saving the best queries along the way. The authors
    used the T5 model with fine-tuning as a query generator to interact with the search
    engine iteratively, producing a set of fine-grained queries that yield better
    outcomes. Another related work, WebGPT (Nakano et al., [2021](#bib.bib10)), utilizes
    a web interface and a search engine to train RL agents to answer questions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对网络推荐系统的 RL 先前探索较为广泛。查询重构，正如 (Nogueira 和 Cho, [2017](#bib.bib11)) 中所发布的，是旨在优化结果的
    RL 问题的一部分。在这种背景下，搜索引擎被视为黑箱，RL 代理（或重构器）学习生成最大化预期回报的查询，通过在状态空间中的操作实现。本文发表于 2017
    年，早于 BERT（Devlin et al., [2019](#bib.bib6)）的广泛使用。作者提出了一个 PRF 框架，其中 CNN/RNN 作为上下文学习器和查询生成器。较新的工作提出了“学习搜索”的概念
    (Adolphs et al., [2021](#bib.bib2))，在该概念中，搜索代理通过基于先前查询生成互动搜索查询，并保存最佳查询。作者使用 T5
    模型进行微调作为查询生成器，与搜索引擎进行迭代交互，生成一组细粒度的查询以获得更好的结果。另一项相关工作 WebGPT (Nakano et al., [2021](#bib.bib10))，利用网络接口和搜索引擎来训练
    RL 代理回答问题。
- en: 2\. Related Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: The work presented in this paper is built and evaluated within the WebShop (Yao
    et al., [2023](#bib.bib18)) environment, a simulator of online web shopping recommender
    system.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中介绍的工作是在 WebShop (Yao et al., [2023](#bib.bib18)) 环境中构建和评估的，该环境是一个在线网络购物推荐系统的模拟器。
- en: 2.1\. The WebShop Environment
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. WebShop 环境
- en: WebShop is a benchmark project designed to train reinforcement learning algorithms
    in a large-scale, interactive, web-based environment. It includes over 12,000
    crowdsourced human instructions, over 1.1 million products scraped from amazon.com.
    A total of 670 attributes were derived from concatenated product titles and descriptions
    using bi-gram representations and assigned to each product through TF-IDF scoring.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: WebShop 是一个基准项目，旨在在大规模互动的网络环境中训练强化学习算法。它包含超过 12,000 个众包人类指令，以及从 amazon.com 爬取的超过
    110 万个产品。总共从连接的产品标题和描述中衍生出了 670 个属性，并通过 TF-IDF 评分分配给每个产品。
- en: Figure 1 and Figure 2 below provide an example WebShop interface and a sequence
    of actions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 和图2 提供了一个 WebShop 界面的示例及一系列操作。
- en: Figure 1\. WebShop Environment (Yao et al., [2023](#bib.bib18))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. WebShop 环境（Yao et al., [2023](#bib.bib18)）
- en: '![Refer to caption](img/a9c38f21c6ed8624028ec2b48f64f231.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. WebShop Human Instructions and Human Trajectories (Yao et al., [2023](#bib.bib18))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e7436c3a6bab3fda095639e9d5ef0c8.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: The original paper tackles the problem into two sets of reinforcement learning
    models for search and choice (or clicks). The search model is an imitation learning
    model (search-IL) mimicking human search queries from instructions. It is a human
    instruction and query pair fine-tuned BART (Lewis et al., [2020](#bib.bib8)) model
    in root. For choice (clicks) learning, the authors present a few reinforcement
    learning models to optimize choice of clicks navigating the recommender simulator
    to optimize the end rewards (purchase). The reward is calculated by a scoring
    function to quantify the relevance between a purchased product and the human instruction,
    based on attributes of the product. The imitation learning algorithm (choice-IL)
    presented by the original authors is a human trajectory fine-tuned BERT (Devlin
    et al., [2019](#bib.bib6)) model in root. The reinforcement learning algorithm
    for choice iterates the imitation learned (choice-IL-RL), fine-tuned BERT model
    as the baseline and iterates the optimization using a mixed objectives of policy
    gradients and cross entropy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'The state space in this problem consists abstraction of four types of webpages:
    search page, product recommendation page, product page, and product detail page.
    The search page features only a search bar for entering instructions, which are
    used to take a search query either generated by human or a search agent, serving
    as input for a search engine. Actions include searching, clicking buttons, and
    choosing from a drop-down menu. Clicking the purchase button marks the end of
    a trajectory. State transitions are initiated by clicks and other actions that
    deterministically redirect from one webpage (state) to another. Observations,
    which includes state and instruction at a specific time snapshot, together form
    the input for the reinforcement learning agent to make subsequent actions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The search engine used by the WebShop project is self-built and self-indexed
    offline using Pyserini (Lin et al., [2021](#bib.bib9)), which is built upon the
    open-source Lucene search library. Product retrieval is based on BM25 between
    search queries and product information text. Top 50 results are shown in 5 pages
    ranked by BM25\.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Reinforcement Learning with Human Preference - RLHF
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RLHF (Christiano et al., [2023](#bib.bib4)) together with PPO (Schulman et al.,
    [2017](#bib.bib15)) were successfully used to train a few well-known GPT related
    product, such as instructGPT (Ouyang et al., [2022](#bib.bib12)). RLHF leverages
    the Bradley-Terry model, which defines the preference using rewards of preferred
    and dispreferred data labeled by human labelers:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: RLHF objectives then can be defined similarly to entropy loss.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. PPO for Regularized Policy Gradient
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Proximal Policy Optimization (PPO) (Schulman et al., [2017](#bib.bib15)) has
    been demonstrated to be effective in fine-tuning GPT models with human instructions
    and labeled preferences (Ouyang et al., [2022](#bib.bib12)). PPO uses clipping
    or KL divergence constraints to minimize the likelihood of large updates between
    steps, approximately providing guarantees for monotonic improvement. This approach
    converges in probability to local optima and, in practice, results in more stable
    training outcomes. The clipped loss function for policy gradient in PPO can be
    expressed as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $L_{\theta_{k}}^{PPO}=-E_{\tau\sim\pi_{k}}\left[min(z_{t}(\theta)\hat{A}_{t}^{\pi_{k}},clip(z_{t}(\theta),1-\epsilon,1+\epsilon)\hat{A}_{t}^{\pi_{k}}\right]$
    |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: ', where'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z_{t}(\theta)=\frac{\pi_{\theta}(a_{t}&#124;o_{t})}{\pi_{\theta_{k}}(a_{t}&#124;o_{t})},$
    |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '|  | $\hat{A}_{t}^{\pi_{k}}=R_{t}-V^{\pi_{k}}(o_{t}).$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: 2.4\. Learning with Human Preference - DPO
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The development of Direct Preference Optimization (DPO) (Rafailov et al., [2023](#bib.bib13))
    is revolutionary. It eliminates the need for explicit reward functions for preferences
    and instead relies solely on paired preference trajectories as training data.
    DPO is derived from joining the Bradley-Terry objective
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: ', and the RLHF objective:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\smash{\displaystyle\max_{\pi}}\ E_{x\sim D,y\sim\pi}[r(x,y)]-\beta D_{KL}[\pi(y&#124;x)&#124;&#124;\pi_{ref}(y&#124;x)].$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 'The DPO loss function is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: ', where $\pi_{\theta}$ is the pre-selected reference policy. From the form
    of the loss function, although DPO does not need a reward model to be trained
    explicitly, it does require a pre-defined reference policy to iterate upon.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Approach
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This report summarizes a few efforts implementing and evaluating PPO vs. contrast
    learning using DPO using human trajectories and generated unpreferred trajectories.
    Then it demonstrates the contrast learning effort using all generative trajectories.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: We branched and implemented DPO and PPO using the original WebShop code package,
    together with a new generative module for the self-generative learning experiments,
    and a Thompson sampling module to roll out online experiments and collect results.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: For PPO training, the policy gradient objective from the original paper (Yao
    et al., [2023](#bib.bib18)) is modified into a PPO objective as shown in equation
    (1). The overall objective components, which is the total loss from policy gradient
    (PG), entropy loss, and imitation learning loss remain the same as in the original
    paper, except PG component is replaced with PPO loss.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Semi-generative Reinforcement Learning Using Human Trajectories
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this project, we utilize a pre-trained imitation learning agent checkpoint
    as the reference policy to generate unpreferred trajectories. Preferred trajectories
    are obtained from human data provided by the WebShop benchmark. During training,
    a human trajectory is randomly sampled, including states and available actions
    from the log. At each state where an action decision is needed, an unpreferred
    action is generated using the reference policy. This unpreferred action is paired
    with the preferred action generated by the human. The DPO update is applied after
    each episode based on the human trajectory.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: This approach is considered both generative and semi-self-learning. It is generative
    because we use a predefined unpreferred policy to generate actions for pair-wise
    training. It is semi-self-learning because it pairs these generated actions with
    previously collected human trajectories, which serve as the gold standard.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Self-learning - Training with Generated Trajectories
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In classic reinforcement learning, self-play or learning through simulation
    plays a crucial role, particularly when data collection is costly, such as the
    collection of human trajectories in this problem. Self-play has proven to be effective,
    with the most notable example being AlphaGo (Silver et al., [2016](#bib.bib17)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the idea of self-play or self-learning in navigating the WebShop
    recommendation systems, we generated 100 preferred trajectories using a straightforward
    method of sampling trajectories with perfect reward (score = 1). This sampling
    was done using the agent checkpoint from imitation learning provided by the authors
    of the WebShop paper, but with real-world human instructions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, these sampled trajectories are pruned to eliminate looped sub-trajectories.
    A DPO agent is then trained from the same checkpoint used for DPO evaluation in
    the previous section, with 3000 steps. Task performance between these two DPO
    agents — one trained using semi-learning with human trajectories and the other
    using self-learning with generated trajectories — is compared using Thompson sampling
    ran in the WebShop simulator environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experimental Results
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1\. DPO vs. PPO Task Performance
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this project, leveraging the WebShop environment and simulator, we conduct
    extensive simulated online experiments using Thompson sampling to analyze the
    performance differences across trained agents. The goal of Thompson sampling is
    to select the optimal action (or ”arm”) that minimizes overall regret. However,
    when sampling over a small number of steps, it may not be ideal for estimating
    rewards from arms that are perceived as less optimal due to insufficient exploration.
    To address this, we use multiple parallel runs of Thompson sampling, each with
    1000 rollouts, to capture variability across runs. Careful experimental design
    and calculated rollouts of online experiments are necessary for accurately estimating
    the rewards and success rates of each agent. The aim of this project is to implement
    and understand the performance trends across different approaches.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The results indicate that Direct Preference Optimization (DPO) agents achieve
    significantly higher scores and success rates compared to Proximal Policy Optimization
    (PPO) agents, even though all agents start from the same imitation learning BERT
    model checkpoint provided by the original paper. It is important to note that
    all agents in this comparison are trained without image data, so the scores and
    success rates collected are not directly comparable to the original paper, which
    includes image data in training and experiments.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: An interesting finding is that DPO agents trained using human trajectories perform
    similarly to DPO agents trained using generated trajectories, albeit with larger
    variance in success rate across runs. The smaller variance observed in self-learning
    agents can be attributed to the fact that only 100 generated trajectories were
    used to train the DPO self-learning agent, compared to 1200 human trajectories
    used for training the DPO agent with human data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The fact that DPO agents are trained using only 3000 steps also suggests the
    possibility of underestimating data inefficiency or bottleneck when training over
    long period of times using the same set of data. When training an agent for production
    systems, the limited number of available trajectories can result in decreased
    task performance due to insufficient information learned from the limited data.
    In reality, collecting human data is expensive and time-consuming. This issue
    can be mitigated by generating preferred and unpreferred trajectories to serve
    as a continuous, low-cost source of training data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. DPO vs. PPO — Human Trajectories and Generated Trajectories — Scores
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d8e99f5ef549e568a77321e555c1901f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. DPO vs. PPO — Human Trajectories and Generated Trajectories — Success
    Rate
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7d316d773cbad523e37fd87352ea547.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'It is important to note that the results of this project are not directly comparable
    to those of the original paper due to two key differences: 1) no image data were
    used for training or experiments for any of the agents evaluated in this project,
    and 2) each agent was trained with minimal steps (3000) and within a timeframe
    of less than one hour. The purpose of this project is not to benchmark results
    but to investigate variations in reinforcement learning algorithms.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Training using fully generated preferences on top of the DPO agent achieved
    much higher scores than DPO agents using human trajectories, while the success
    rate remained similar (Figure 5, Figure 6). The magnitude of this difference needs
    to be justified using variance across runs, but this finding demonstrates the
    potential of using generative data to enhance training on top of existing agents
    initially trained with human trajectories.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. Self-learning Using Generated Trajectories — Scores
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ae9cb2d6adc3a8bf38072f5cff1e000.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Self-learning Using Generated Trajectories — Success Rate
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56facf2f5d99cdc40367d6b7a8aebe0b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 5\. Conclusion
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With very limited training time (¡1 hour), Direct Preference Optimization (DPO)
    outperforms Proximal Policy Optimization (PPO), offering better task performance
    and higher success rates with less training time. However, more evaluations with
    longer training time are necessary to draw a conclusion. Using a DPO agent trained
    within one hour and without image data, we achieved a success rate of approximately
    19%. This is higher than the success rate of an RL agent trained with an RNN network
    (without pre-trained models for search or choice imitation learning), which has
    an 18% success rate from the original paper.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: PPO is known to be able to provide less volatile training and approximately
    monotonic guarantees for RL objectives. By nature, PPO’s regularization and clipping
    of objectives prevent rapid policy changes, making it suitable for problems with
    smaller state and action spaces where large policy changes are not expected. However,
    in the context of online product recommenders, where the state-action space can
    expand to millions of dimensions, and rapid policy changes are essential for fast
    learning, PPO can need longer time to train.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Training DPO agents with generated trajectories has shown great potential. With
    only 100 generated human trajectories and the same amount of computational resources,
    the task performance was comparable to a DPO agent trained using 1200 human trajectories.
    This approach addresses data inefficiency and the high costs of human data collection.
    As training requires more time and data, the limited availability of human data
    can hinder continuous improvement. This exercise demonstrates that generated trajectories
    can be nearly as effective as human trajectories and can even serve as a continuous,
    low-cost source of training data. Additionally, generated trajectories allow exploration
    of successful paths not seen by humans, similar to the approach that contributed
    to the success of AlphaGo(Silver et al., [2016](#bib.bib17)) and AlphaZero, which
    were trained using self-play rather than past human games.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成轨迹训练 DPO 代理已显示出巨大的潜力。仅用 100 条生成的人类轨迹和相同数量的计算资源，任务性能与使用 1200 条人类轨迹训练的 DPO
    代理相当。这种方法解决了数据低效和人类数据收集的高成本问题。由于训练需要更多时间和数据，人类数据的有限可用性可能会阻碍持续改进。这一实验表明，生成的轨迹几乎可以与人类轨迹一样有效，甚至可以作为一种持续的、低成本的训练数据来源。此外，生成的轨迹允许探索人类未曾见过的成功路径，类似于
    AlphaGo（Silver et al., [2016](#bib.bib17)）和 AlphaZero 的成功之路，它们通过自我对弈而非过去的人类游戏进行训练。
- en: '6\. Potential Usage: Using Trained Agents as a Recommender'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 潜在用途：使用训练代理作为推荐系统
- en: Using reinforcement learning agent in recommenders is not new - it is known
    to be used in online recommenders such as Youtube. The trained optimal policy
    can become an ideal ranking algorithm for recommender systems. Starting from a
    human instruction, the agent simulates navigating through a provided list of product
    following the trained optimal policy and provides a ”purchased” product from each
    run. When using in recommenders, multiple runs of the agent provide a list of
    recommended product to user and the order to present in a user interface, such
    as on web or in an app can be rank-ordered by scores or success for each recommended
    product from each run of the RL agent.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统中使用强化学习代理并不新鲜——这种方法已知用于在线推荐系统，如 YouTube。训练后的最优策略可以成为推荐系统的理想排名算法。从人类指令开始，代理模拟按照训练的最优策略在提供的产品列表中进行导航，并在每次运行中提供“已购买”产品。在推荐系统中，多次运行代理提供的产品推荐列表和在用户界面上展示的顺序，如在网页或应用程序中，可以按每次运行的
    RL 代理的分数或成功率对每个推荐产品进行排序。
- en: Acknowledgements.
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: To the WebShop authors Shunyu Yao, Howard Chen, John Yang and Karthik Narasimhan
    from Princeton University who published (Yao et al., [2023](#bib.bib18)), which
    inspired this project report.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢来自普林斯顿大学的 WebShop 作者 Shunyu Yao、Howard Chen、John Yang 和 Karthik Narasimhan，他们发表了
    (Yao et al., [2023](#bib.bib18))，对本项目报告提供了灵感。
- en: To Professor Chris Potts who has introduced WebShop [17] to the author of this
    report, and for his excellent teaching CS224U in Stanford University.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Chris Potts 教授，他向本报告作者介绍了 WebShop [17]，并在斯坦福大学优秀地教授了 CS224U 课程。
- en: To Professor Emma Brunskill for her excellent teaching CS234 in Stanford University.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 Emma Brunskill 教授，她在斯坦福大学出色地教授了 CS234 课程。
- en: References
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Adolphs et al. (2021) Leonard Adolphs, Benjamin Börschinger, Christian Buck,
    Michelle Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann,
    and Yannic Kilcher. 2021. Boosting Search Engines with Interactive Agents. *CoRR*
    abs/2109.00527 (2021). arXiv:2109.00527 [https://arxiv.org/abs/2109.00527](https://arxiv.org/abs/2109.00527)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adolphs et al. (2021) Leonard Adolphs, Benjamin Börschinger, Christian Buck,
    Michelle Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann,
    和 Yannic Kilcher. 2021. Boosting Search Engines with Interactive Agents. *CoRR*
    abs/2109.00527 (2021). arXiv:2109.00527 [https://arxiv.org/abs/2109.00527](https://arxiv.org/abs/2109.00527)
- en: Chen et al. (2018) Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois
    Belletti, and Ed H. Chi. 2018. Top-K Off-Policy Correction for a REINFORCE Recommender
    System. *CoRR* abs/1812.02353 (2018). arXiv:1812.02353 [http://arxiv.org/abs/1812.02353](http://arxiv.org/abs/1812.02353)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2018) Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois
    Belletti, 和 Ed H. Chi. 2018. Top-K Off-Policy Correction for a REINFORCE Recommender
    System. *CoRR* abs/1812.02353 (2018). arXiv:1812.02353 [http://arxiv.org/abs/1812.02353](http://arxiv.org/abs/1812.02353)
- en: Christiano et al. (2023) Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2023. Deep reinforcement learning from human preferences.
    arXiv:1706.03741 [stat.ML]
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano et al. (2023) Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
    Shane Legg, 和 Dario Amodei. 2023. Deep reinforcement learning from human preferences.
    arXiv:1706.03741 [stat.ML]
- en: Covington et al. (2016) Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep
    Neural Networks for YouTube Recommendations. In *Proceedings of the 10th ACM Conference
    on Recommender Systems* (Boston, Massachusetts, USA) *(RecSys ’16)*. Association
    for Computing Machinery, New York, NY, USA, 191–198. [https://doi.org/10.1145/2959100.2959190](https://doi.org/10.1145/2959100.2959190)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Covington et al. (2016) Paul Covington, Jay Adams, 和 Emre Sargin. 2016. 深度神经网络用于
    YouTube 推荐. 在 *第10届ACM推荐系统会议论文集* (波士顿, 马萨诸塞州, 美国) *(RecSys ’16)* 中. 计算机协会, 纽约,
    NY, 美国, 191–198. [https://doi.org/10.1145/2959100.2959190](https://doi.org/10.1145/2959100.2959190)
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. (2019). arXiv:1810.04805 [cs.CL]'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. BERT: 深度双向变换器的语言理解预训练. (2019). arXiv:1810.04805 [cs.CL]'
- en: Koren et al. (2009) Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix
    Factorization Techniques for Recommender Systems. *Computer* 42, 8 (2009), 30–37.
    [https://doi.org/10.1109/MC.2009.263](https://doi.org/10.1109/MC.2009.263)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koren et al. (2009) Yehuda Koren, Robert Bell, 和 Chris Volinsky. 2009. 矩阵分解技术用于推荐系统.
    *计算机* 42, 8 (2009), 30–37. [https://doi.org/10.1109/MC.2009.263](https://doi.org/10.1109/MC.2009.263)
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension. In *Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics*. Association for Computational
    Linguistics, Online, 7871–7880. [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, 和 Luke Zettlemoyer. 2020. BART:
    去噪序列到序列预训练用于自然语言生成、翻译和理解. 在 *第58届计算语言学协会年会论文集* 中. 计算语言学协会, 在线, 7871–7880. [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703)'
- en: 'Lin et al. (2021) Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang,
    Ronak Pradeep, and Rodrigo Frassetto Nogueira. 2021. Pyserini: An Easy-to-Use
    Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations.
    *CoRR* abs/2102.10073 (2021). arXiv:2102.10073 [https://arxiv.org/abs/2102.10073](https://arxiv.org/abs/2102.10073)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2021) Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang,
    Ronak Pradeep, 和 Rodrigo Frassetto Nogueira. 2021. Pyserini: 一款易于使用的 Python 工具包，支持可重复的
    IR 研究，涵盖稀疏和密集表示. *CoRR* abs/2102.10073 (2021). arXiv:2102.10073 [https://arxiv.org/abs/2102.10073](https://arxiv.org/abs/2102.10073)'
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted
    question-answering with human feedback. *CoRR* abs/2112.09332 (2021). arXiv:2112.09332
    [https://arxiv.org/abs/2112.09332](https://arxiv.org/abs/2112.09332)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, 和 John Schulman. 2021. WebGPT: 浏览器辅助的问答系统与人工反馈.
    *CoRR* abs/2112.09332 (2021). arXiv:2112.09332 [https://arxiv.org/abs/2112.09332](https://arxiv.org/abs/2112.09332)'
- en: Nogueira and Cho (2017) Rodrigo Frassetto Nogueira and Kyunghyun Cho. 2017.
    Task-Oriented Query Reformulation with Reinforcement Learning. *CoRR* abs/1704.04572
    (2017). arXiv:1704.04572 [http://arxiv.org/abs/1704.04572](http://arxiv.org/abs/1704.04572)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira and Cho (2017) Rodrigo Frassetto Nogueira 和 Kyunghyun Cho. 2017. 任务导向的查询重构与强化学习.
    *CoRR* abs/1704.04572 (2017). arXiv:1704.04572 [http://arxiv.org/abs/1704.04572](http://arxiv.org/abs/1704.04572)
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback. In *Advances in Neural
    Information Processing Systems*, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
    K. Cho, and A. Oh (Eds.), Vol. 35\. Curran Associates, Inc., 27730–27744. [https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等（2022）Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    F Christiano、Jan Leike和Ryan Lowe。2022。训练语言模型以遵循带有人类反馈的指令。在*Advances in Neural
    Information Processing Systems*，S. Koyejo、S. Mohamed、A. Agarwal、D. Belgrave、K.
    Cho和A. Oh（编辑），第35卷。Curran Associates, Inc., 27730–27744。 [https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct Preference Optimization:
    Your Language Model is Secretly a Reward Model. (2023). arXiv:2305.18290 [cs.LG]'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov等（2023）Rafael Rafailov、Archit Sharma、Eric Mitchell、Stefano Ermon、Christopher
    D. Manning和Chelsea Finn。2023。直接偏好优化：你的语言模型实际上是一个奖励模型。（2023）。arXiv:2305.18290 [cs.LG]
- en: 'Rohde et al. (2018) David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile,
    and Alexandros Karatzoglou. 2018. RecoGym: A Reinforcement Learning Environment
    for the problem of Product Recommendation in Online Advertising. *CoRR* abs/1808.00720
    (2018). arXiv:1808.00720 [http://arxiv.org/abs/1808.00720](http://arxiv.org/abs/1808.00720)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rohde等（2018）David Rohde、Stephen Bonner、Travis Dunlop、Flavian Vasile和Alexandros
    Karatzoglou。2018。RecoGym: 一种用于在线广告中产品推荐问题的强化学习环境。*CoRR* abs/1808.00720（2018）。arXiv:1808.00720
    [http://arxiv.org/abs/1808.00720](http://arxiv.org/abs/1808.00720)'
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. *CoRR*
    abs/1707.06347 (2017). arXiv:1707.06347 [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman等（2017）John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford和Oleg
    Klimov。2017。近端策略优化算法。*CoRR* abs/1707.06347（2017）。arXiv:1707.06347 [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
- en: 'Shi et al. (2019) Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang
    Zeng. 2019. Virtual-Taobao: Virtualizing Real-World Online Retail Environment
    for Reinforcement Learning. *Proceedings of the AAAI Conference on Artificial
    Intelligence* 33, 01 (Jul. 2019), 4902–4909. [https://doi.org/10.1609/aaai.v33i01.33014902](https://doi.org/10.1609/aaai.v33i01.33014902)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi等（2019）Jing-Cheng Shi、Yang Yu、Qing Da、Shi-Yong Chen和An-Xiang Zeng。2019。Virtual-Taobao:
    虚拟化现实世界在线零售环境以进行强化学习。*Proceedings of the AAAI Conference on Artificial Intelligence*
    33, 01（2019年7月），4902–4909。 [https://doi.org/10.1609/aaai.v33i01.33014902](https://doi.org/10.1609/aaai.v33i01.33014902)'
- en: Silver et al. (2016) David Silver, Aja Huang, Christopher J. Maddison, Arthur
    Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham,
    Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
    Thore Graepel, and Demis Hassabis. 2016. Mastering the game of Go with deep neural
    networks and tree search. *Nature* 529 (2016), 484–503. [http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver等（2016）David Silver、Aja Huang、Christopher J. Maddison、Arthur Guez、Laurent
    Sifre、George van den Driessche、Julian Schrittwieser、Ioannis Antonoglou、Veda Panneershelvam、Marc
    Lanctot、Sander Dieleman、Dominik Grewe、John Nham、Nal Kalchbrenner、Ilya Sutskever、Timothy
    Lillicrap、Madeleine Leach、Koray Kavukcuoglu、Thore Graepel和Demis Hassabis。2016。利用深度神经网络和树搜索掌握围棋。*Nature*
    529（2016），484–503。 [http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)
- en: 'Yao et al. (2023) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2023. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language
    Agents. (2023). arXiv:2207.01206 [cs.CL]'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等（2023）Shunyu Yao、Howard Chen、John Yang和Karthik Narasimhan。2023。WebShop:
    迈向可扩展的真实世界网络互动与基础语言代理。（2023）。arXiv:2207.01206 [cs.CL]'
- en: 'Zhang et al. (2017) Shuai Zhang, Lina Yao, and Aixin Sun. 2017. Deep Learning
    based Recommender System: A Survey and New Perspectives. *CoRR* abs/1707.07435
    (2017). arXiv:1707.07435 [http://arxiv.org/abs/1707.07435](http://arxiv.org/abs/1707.07435)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2017）Shuai Zhang、Lina Yao 和 Aixin Sun。2017年。基于深度学习的推荐系统：综述与新视角。*CoRR*
    abs/1707.07435（2017）。arXiv:1707.07435 [http://arxiv.org/abs/1707.07435](http://arxiv.org/abs/1707.07435)
