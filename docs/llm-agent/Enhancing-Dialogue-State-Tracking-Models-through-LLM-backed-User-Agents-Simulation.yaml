- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13037](https://ar5iv.labs.arxiv.org/html/2405.13037)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Cheng Niu NewsBreak Xingguang Wang NewsBreak Xuxin Cheng NewsBreak Juntong Song
    NewsBreak Tong Zhang University of Illinois Urbana-Champaign
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state
    in the conversations and plays a pivotal role in developing task-oriented dialogue
    systems. However, obtaining the annotated data for the DST task is usually a costly
    endeavor. In this paper, we focus on employing LLMs to generate dialogue data
    to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used
    to simulate the user and agent interaction, generating thousands of dialogues
    annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed
    on the generated data and the real data for the DST prediction. Experimental results
    on two public DST benchmarks show that with the generated dialogue data, our model
    performs better than the baseline trained solely on real data. In addition, our
    approach is also capable of adapting to the dynamic demands in real-world scenarios,
    generating dialogues in new domains swiftly. After replacing dialogue segments
    in any domain with the corresponding generated ones, the model achieves comparable
    performance to the model trained on real data.¹¹1All the source code, models,
    and generated dialogue data will be released after review for reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dialogue state tracking (DST) is a critical component of task-oriented dialogue
    systems, serving to track users’ goals and system actions in the conversation
    and facilitate precise information handling for communicating with external APIs Henderson
    et al. ([2014](#bib.bib14)); Mrkšić et al. ([2017](#bib.bib30)); Zhang et al.
    ([2023](#bib.bib53)); Hudeček and Dušek ([2023](#bib.bib16)). DST usually takes
    the form of key-value pairs, where the keys are denoted as slots which are defined
    in the system schema, outlining the specific information that the system aims
    to track or extract during the whole conversation Ren et al. ([2018](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: The design of DST models could be broadly categorized into two main types, classification-based
    DST models and generation-based DST models. Classification-based models select
    slot values from a set of candidates Ma et al. ([2019](#bib.bib29)); Ye et al.
    ([2021](#bib.bib51)), assuming that the dialogue ontology is pre-defined and hence
    lacking generalization capability Chen et al. ([2020](#bib.bib5)); Wang et al.
    ([2022](#bib.bib45)). Generation-based models directly generate the slot values
    to handle unseen domains and values Gao et al. ([2019](#bib.bib10), [2020](#bib.bib9));
    Lin et al. ([2020](#bib.bib27)); Peng et al. ([2021](#bib.bib33)). Recently, Feng
    et al. ([2023](#bib.bib8)) proposes a new DST framework LDST based on LLaMA Touvron
    et al. ([2023a](#bib.bib41)). By using an instruction tuning method, LDST achieves
    performance on par with ChatGPT OpenAI ([2023](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite DST showing promising results, a significant challenge is that the annotation
    of dialogues entails significant costs. Furthermore, the dynamic nature of real-world
    demands highlights the urgent need to quickly generate utterances for new domains.
    Compared to other types of NLP data, collecting authentic dialogue data is particularly
    challenging. This difficulty is partly due to the dialogues frequently containing
    personal or sensitive information, which complicates data collection and sharing
    efforts. In response to these challenges, and inspired by the recent advancements
    of large language models (LLMs) Touvron et al. ([2023b](#bib.bib42)); Significant-gravitas
    ([2023](#bib.bib37)); Jablonka et al. ([2023](#bib.bib17)); Shen et al. ([2023](#bib.bib35)),
    we explore the use of these models for generating annotated DST data for data
    augmentation. By leveraging LLM’s cross-domain generation capability, we aim to
    create synthetic dialogues that can serve as replacements for manually annotated
    data, significantly reducing both financial cost and time constraints.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose a LLM-backed User-Agents Simulation (LUAS) algorithm
    to enhance DST. The process begins with the LLM generating a user profile that
    details the individual’s preferences for various tasks. Following this initial
    step, the LLM is prompted to simulate a conversation between the user and the
    agent. In these simulations, the user simulator makes requests and seeks recommendations
    or assistance, while the agent responds by understanding the user’s needs, providing
    suggestions, and taking appropriate actions. Through iterative conversations between
    the user and agent, complemented by a slot extractor also prompted by the LLM,
    we generate a substantial corpus of labeled, multi-turn dialogue data.
  prefs: []
  type: TYPE_NORMAL
- en: To verify the effectiveness of our approach and the quality of the generated
    data, experiments are conducted on two public DST datasets, MultiWOZ 2.2 (Zang
    et al., [2020](#bib.bib52)) and MultiWOZ 2.4 (Ye et al., [2022](#bib.bib50)).
    Following  Touvron et al. ([2023b](#bib.bib42)), LLaMa 2 is finetuned with real
    data as a strong baseline. By using both the generated and the real data, finetuning
    LLaMa 2 can further improve the performance. Besides, by replacing dialogue segments
    of any domain with the generated data, the newly trained model achieves comparable
    performance to the model trained on the real data, which shows the capability
    of our method to meet the dynamic requirements of real-world scenarios, generating
    dialogues in new domains and preserving the promising performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the contributions of our work can be categorized into four aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a new framework that harnesses the power of GPT-4 to generate new
    labeled dialogue data, effectively reducing dialogue data collection and annotation
    costs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment results on two datasets show the positive impact of the generated
    data on performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our method can swiftly generate data in new domains while maintaining promising
    performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We believe that our approach holds promise for extension to other dialogue-related
    tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Dialogue State Tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dialogue state tracking is an essential yet challenging task in task-oriented
    dialogue systems (Mrkšić et al., [2017](#bib.bib30)). Recent DST models (Lee et al.,
    [2021](#bib.bib24); Zhu et al., [2022](#bib.bib59); Yang et al., [2023b](#bib.bib49);
    Su et al., [2023](#bib.bib38); Lesci et al., [2023](#bib.bib25)), leveraging the
    different architectures and mechanisms, have convincingly demonstrated promising
    performance on several datasets (Budzianowski et al., [2018](#bib.bib4); Eric
    et al., [2020](#bib.bib7); Zang et al., [2020](#bib.bib52); Han et al., [2021](#bib.bib11);
    Ye et al., [2022](#bib.bib50)). To ease the burden of dialogue collection and
    annotation, Wu et al. ([2019](#bib.bib47)); Zhou et al. ([2023](#bib.bib58)) use
    few-shot learning to transfer to adapt existing models to the new domains. Drawn
    by the recent achievement of LLMs, Feng et al. ([2023](#bib.bib8)) leverages Low-Rank
    Adaptation (LoRA) Hu et al. ([2022](#bib.bib15)) to fine-tune the foundation model,
    achieving the promising performance in DST. In this paper, we utilize GPT-4 to
    simulate user-agent conversations, and the obtained dialogue data significantly
    enhances DST.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Data Augmentation by LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| DST: [history], [user_utterance] $\rightarrow$ [service], [slot_key], [slot_val]
    |'
  prefs: []
  type: TYPE_TB
- en: '| You are a local guide online, primarily handling the local services like
    finding the user’s place (such as attraction, hotel, train, restaurant, or hospital),
    calling taxis, contacting the police, or other convenient services. Your service
    is efficient and of high quality, earning widespread praise from the local community.
    Given the conversion history, your task is to help find what the user is looking
    for based on the whole conversion. Please output the current_service based on
    the user’s last utterance. And also please output all service information that
    needs to be paid attention to from the whole conversion. Here are the “conversion
    history”: {[history]} and the “user’s lastest utterance”: {[user_utterance]}.
    The output should be JSON-formatted like “current_service”: {[service]}, “slots”:
    {“[service]”: {“[slot_key]”: {[slot_val]}}}. |'
  prefs: []
  type: TYPE_TB
- en: '| Please give your decision: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Proposed prompts to guide LLaMA 2 to generate JSON-formatted dialogue
    state predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation has shown remarkable effectiveness in various domains, including
    computer vision Krizhevsky et al. ([2012](#bib.bib22)); Shorten and Khoshgoftaar
    ([2019](#bib.bib36)), text classification Zhang et al. ([2015](#bib.bib55)); Wei
    and Zou ([2019](#bib.bib46)), and speech recognition Ko et al. ([2015](#bib.bib21));
    Park et al. ([2019](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, with the increasing prominence of LLMs, an increasing number
    of studies have begun to utilize LLMs for data augmentation. Kaddour and Liu ([2024](#bib.bib18))
    discovers that fine-tuning teacher LLMs to annotate unlabeled instances and generate
    new data points can notably boost the performance of downstream models. Yang et al.
    ([2023a](#bib.bib48)) generates truthful and customized dialogues to reduce hallucation.
    Ulmer et al. ([2024](#bib.bib43)) compares the effectiveness of various filtering
    strategies for the generated dialogue quality and introduces new methods to benchmark
    finetuned dialogue system. But their work does not discuss the DST task. Li et al.
    ([2022](#bib.bib26)) presented GPT-3 backed user-agent simulation system and showed
    positive results on DST task when the real data size is extremely small. Unlike
    Li et al. ([2022](#bib.bib26)), we abstract the common intentions of users and
    agents, crafting intent-specific prompts to ensure that the simulation adheres
    to task-oriented logic. This scheme enables the simulation to operate within a
    zero-shot setup, enhancing our approach’s adaptability to new domains. Moreover,
    by implementing a two-stage fine-tuning process, our approach demonstrates superior
    performance compared to strong baselines, even when trained with the full size
    of real data.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will begin with the basic problem definition ($\S\ref{Problem
    Definition}$).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A task-oriented dialogue involves a multi-turn conversation between a user
    $U$, which is defined as a collection of (slot, value) pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{t}=\{(s^{i}_{t},v^{i}_{t})\;&#124;\;C_{t}\;,\forall s^{i}\in\mathcal{S}\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{S}$ denotes the set of the possible slots predefined in an ontology
    or schema. Following previous work Wang et al. ([2023](#bib.bib44)), the final
    slot is represented as the concatenation of the corresponding task domain and
    original slot, e.g., “<hotel-area>”. The slots associated with each domain could
    be either categorical with a set of candidate values (e.g. <hotel-parking> = “True”
    / “False”), or non-categorical, where the value is a span in the dialogue context
    (e.g. <hotel-name> = “Alexander”). Note that if no information is provided in
    the dialogue regarding a specific slot, the associated value for that slot is
    set to “NONE”.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Using LLaMA 2 to Predict Dialogue State
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed1b4001bcc4c178cd73d4d53d304a55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The simulation process of our approach. The blue boxes are intentions
    for the user and the agent, the ‘[RECOM]’, ‘[EOF]’, and ‘[EOD]’ are control identifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: We employ full-parameter fine-tuning on LLaMA 2 to predict dialogue states and
    employ pre-designed prompts to guide the LLaMA 2 model in generating predictions
    formatted in JSON. As demonstrated in Table [1](#S2.T1 "Table 1 ‣ 2.2 Data Augmentation
    by LLMs ‣ 2 Related Work ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation"), dialogue history and the user’s latest utterance are
    fed into LLaMA 2, which then conducts the prediction of the entire conversation’s
    intents and slot values. Specifically, predicted intents must fall within a predefined
    set, and predicted slots must align with the designated slots for the respective
    intents. We implement a schema to prevent the generation model from producing
    incoherent outputs and to enhance the overall quality and reliability of the outputs
    of LLaMA 2\. The optimization is conducted through the utilization of cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 User-Agent Dialogue Simulation backed by GPT-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As illustrated in Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Using LLaMA 2 to Predict
    Dialogue State ‣ 3 Method ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation"), the dialogue simulation framework based on GPT-4 involves
    a multi-stage approach for producing labeled multi-turn dialogue data. In this
    arrangement, GPT-4 prompts two simulators, including the user simulator and the
    agent simulator, to engage in conversations aimed at completing specific dialogue
    tasks. Concurrently, GPT-4 also prompts a slot extractor to identify and extract
    all relevant slots throughout the entire conversation simulation process.
  prefs: []
  type: TYPE_NORMAL
- en: The details of the simulation generation process are outlined below, with all
    the prompts included in the appendix for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Simulation Process Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before initiating the dialogue, GPT-4 is prompted to create a user profile that
    outlines the individual’s preferences across various tasks such as travel, accommodations,
    dining, and more. Each preference includes specific details like budget, travel
    distance, and other criteria. Following this setup, the user simulator begins
    interacting with the agent, presenting its requests and seeking recommendations
    or assistance with bookings and purchases. The agent, in turn, is prompted to
    delve into the user’s needs, conduct searches for pertinent information, offer
    suggestions, and execute necessary actions. After each interaction, the user simulator
    evaluates how well their needs have been met, deciding whether to continue the
    conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 User/Agent Intentions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To effectively navigate the simulators through interactive tasks, we encounter
    the challenge of encoding complex dialogue logic within a single prompt. This
    task is demanding for both the user and the agent simulator. To simplify, we abstract
    the common intentions of users and agents, and craft prompts specifically for
    each unique intention of the user or agent. The detailed prompts of different
    intentions are shown in Appendix [A](#A1 "Appendix A Prompts for Simulation ‣
    Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation").
  prefs: []
  type: TYPE_NORMAL
- en: 'The user intentions are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inform Requirement, the user informs their requirement to the agent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update Requirement, the user may update their requirements if the search result
    does not meet their criteria.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask for Recommendation, the user asks for a recommendation given a few candidates
    meeting their criteria.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inquire Properties, the user asks for some properties (e.g. address, etc.) of
    the candidates.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask for Action, the user requires action after receiving the recommendation
    (e.g. making a reservation, etc.).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General Chat, other scenarios in the simulation, e.g. greeting or showing gratitude.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The agent intentions are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inquire, ask the user’s need and preference or seek the user’s approval or confirmation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report Search Results, based on the user’s preference, search the database and
    then make inquiries, recommendations, or reservations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation, when more than one candidate meets users’ search criteria, select
    the top candidate to recommend.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer, answer the user’s inquiry about a recommendation from the agent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report Action Result, take action per the user’s request and report the outcome
    of the action.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General Chat, other scenarios in the simulation, e.g. greetings or asking if
    there are any additional requirements to be addressed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides natural language outputs, the simulators are also prompted to generate
    the control identifiers in the responses, signaling the intention of the response.
    Given the input intention signaled by the control identifiers, the user or agent
    is prompted to select a proper intention and generate responses accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Simulation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As described in Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Simulation Process Overview
    ‣ 3.3 User-Agent Dialogue Simulation backed by GPT-4 ‣ 3 Method ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation"), the simulation
    begins by generating user profiles, which initializes the user requests. Following
    this, based on input intent from the preceding round, the simulation selects a
    user or agent response intent and then uses the corresponding prompt for dialogue
    generation. This selection process is governed by the predetermined logic listed
    below. The Generate Chat intent refers to the expressions of greetings and gratitude
    that are triggered only at the beginning or end of a conversation and are skipped
    in the subsequent list.
  prefs: []
  type: TYPE_NORMAL
- en: The conversation can be initiated by either the user or the agent. The following
    describes the detailed mechanism that triggers the user’s intent.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conversation Starts: triggers user’s Inform Requirement intent. Using randomization,
    the user simulator is instructed to choose a task of interest along with several
    related preferences and then generate a corresponding request.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inquire from the agent: triggers user’s Inform Requirement intent, corresponding
    to the scenario to answer the follow-up questions from the agent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Report Search Result from the agent: if the user’s preference has not been
    fully expressed, the user’s Inform Requirement intent will be triggered. If no
    candidate meets the search criteria, this will trigger the user’s Update Requirement
    intent. Otherwise, the presence of a single candidate will initiate the user’s
    Ask for Action intent, while the discovery of multiple candidates will prompt
    the user’s Ask for Recommendation intent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recommendation from the agent: the user will be prompted to select from (i)
    Inquire Propertied intent for more information or (ii) Ask for Action intent to
    proceed to make transactions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Report Action Result from the agent: if all the tasks in the user profile have
    been completed, General Chat between the user and agent will be triggered, and
    then the conversation terminates; Otherwise, Inform Requirement intent is triggered
    for a new task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Below is the intent-triggering mechanism for the agent simulation.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inform Requirement from the user: the agent is prompted to check if all the
    required slot values have been collected. If not, Inquire intent will be triggered
    to generate follow-up questions; Otherwise, the agent will search based on the
    user’s requirement, and then generate a response based on Report Search Result
    intent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inquire Properties from the user: triggers agent’s Answer intent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ask for Recommendation from the user: the agent is prompted to select the top
    candidate and then generate the response based on Recommendation intent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ask for Action from the user: the agent is prompted to make transactions and
    then generate a response based on Report Action Result intent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3.4 Slot Extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s important to note that the agent simulator must verify that all necessary
    information has been gathered before initiating a search. To manage this, a slot
    tracking module is employed to keep track of both the required and filled slots.
    With the Inform Requirement prompt, the user simulator can simultaneously provide
    dialogue utterances and the corresponding filled slot values. However, there is
    a possibility that the conversation generated by GPT-4 might not align with the
    outcomes of slot filling. This discrepancy can lead to repeated or even endless
    query loops from the agent. To address this issue, a slot extraction model, backed
    by GPT-4, is utilized to ensure that the generated conversation matches the slot-filling
    results. If inconsistencies are found, the conversation must be regenerated to
    maintain coherence.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.5 Generation Diversity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To obtain a high-quality DST model, it is essential to have dialogue data that
    encompasses a wide range of diversity. To ensure the data generated possesses
    this diversity, we manually created ten rewriting templates, which were then expanded
    into hundreds of templates by GPT-4\. These rewriting templates serve as a post-processing
    tool to enhance the diversity of the user and agent responses.
  prefs: []
  type: TYPE_NORMAL
- en: The details about the rewriting templates and rewritten outputs are shown in
    Appendix [B](#A2 "Appendix B Templates for Booking Responses ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation").
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Two-stage Fine-tuning Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Taking into account the discrepancy in distribution between GPT-4 generated
    and real dialogues, directly merging generated and real data could cause the resulting
    model to deviate from the true distribution. To address this issue, we have designed
    a two-stage fine-tuning approach. Initially, we fine-tuned the LLaMA 2 model using
    the generated dialogue data. Following this, we continue to fine-tune the model
    with real data. The first step enables the model to learn fundamental task-oriented
    dialogue patterns. The second step ensures that the model effectively bridges
    the gap between generated and real dialogues, aligning closely with the true distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets and Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct all the experiments on MultiWOZ 2.2²²2[https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2](https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2)
    Zang et al. ([2020](#bib.bib52)) and MultiWOZ 2.4³³3[https://github.com/smartyfh/MultiWOZ2.4](https://github.com/smartyfh/MultiWOZ2.4) Ye
    et al. ([2022](#bib.bib50)). MultiWOZ (Budzianowski et al., [2018](#bib.bib4))
    has been extensively utilized for evaluating the performance of DST, including
    8,438, 1,000, and 1,000 samples for training, dev, and test sets with multi-turn
    dialogues, which are collected by a Wizard-of-Oz (WOZ) setup and encompass a diverse
    array of domains. MultiWOZ 2.2 dataset refines the annotations in dev and test
    sets of MultiWOZ 2.1 (Eric et al., [2020](#bib.bib7)). MultiWOZ 2.4 Ye et al.
    ([2022](#bib.bib50)) is the latest refined version correcting all incorrect labels
    in dev and test sets. Following Wu et al. ([2019](#bib.bib47)), we remove the
    domains of ‘hospital’ and ‘police’ from both MultiWOZ2.2 and MultiWOZ2.4 datasets
    because they only appear a few times in the training set and never occur in the
    dev and test set. By using the MultiWOZ schema, nearly 8000 new dialogues are
    generated. The detailed statistics of MultiWOZ 2.2 and MultiWOZ 2.4 datasets and
    the generated dialogue data are demonstrated in Table [2](#S4.T2 "Table 2 ‣ 4.1
    Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation").
  prefs: []
  type: TYPE_NORMAL
- en: We adopt Joint Goal Accuracy (JGA) as the evaluation metric, which is the primary
    metric for DST. JGA is defined as the proportion of dialogue turns in which all
    the key-values are correctly predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric $\downarrow$ | 2.2 | 2.4 | Generated |'
  prefs: []
  type: TYPE_TB
- en: '| No. of domains | 8 | 7 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| No. of dialogues | 8,438 | 8,438 | 7,556 |'
  prefs: []
  type: TYPE_TB
- en: '| Total no. of turns | 113,556 | 113,556 | 102,602 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. turns per dialogue | 13.46 | 13.46 | 13.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. tokens per turn | 13.13 | 13.38 | 17.01 |'
  prefs: []
  type: TYPE_TB
- en: '| No. of slots | 61 | 37 | 17 |'
  prefs: []
  type: TYPE_TB
- en: '| Have schema description | Yes | Yes | - |'
  prefs: []
  type: TYPE_TB
- en: '| Unseen domains in test set | No | No | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Statistics of MultiWOZ (2.2 and 2.4) and the generated dataset used
    for training in our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | MultiWOZ 2.2 | MultiWOZ 2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| TRADE | 45.40 | 55.05 |'
  prefs: []
  type: TYPE_TB
- en: '| UniLM | 54.25 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DS-DST | 51.70 | - |'
  prefs: []
  type: TYPE_TB
- en: '| TripPy | 53.50 | 64.75 |'
  prefs: []
  type: TYPE_TB
- en: '| AG-DST | 57.26 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SDP-DST | 57.60 | - |'
  prefs: []
  type: TYPE_TB
- en: '| D3ST${}_{\text{Base}}$ | 56.10 | 72.10 |'
  prefs: []
  type: TYPE_TB
- en: '| D3ST${}_{\text{Large}}$ | 54.20 | 70.80 |'
  prefs: []
  type: TYPE_TB
- en: '| D3ST${}_{\text{XXL}}$ | 58.70 | 75.90 |'
  prefs: []
  type: TYPE_TB
- en: '| SPACE-3 | 57.50 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MSP-L | 57.70 | - |'
  prefs: []
  type: TYPE_TB
- en: '| RefPyDST | - | 65.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Diable | 56.48 | 70.46 |'
  prefs: []
  type: TYPE_TB
- en: '| DDSA | - | 75.58 |'
  prefs: []
  type: TYPE_TB
- en: '| SPLAT | 56.60 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MoNET | - | 76.02 |'
  prefs: []
  type: TYPE_TB
- en: '| SSNet | 62.10 | - |'
  prefs: []
  type: TYPE_TB
- en: '| TOATOD${}_{\text{Small}}$ | 61.92 | - |'
  prefs: []
  type: TYPE_TB
- en: '| TOATOD${}_{\text{Base}}$ | 63.79 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LUAS${}_{\text{R}}$ | 65.42 | 77.20 |'
  prefs: []
  type: TYPE_TB
- en: '| LUAS${}_{\text{R+G}}$ | 66.25 | 78.20 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Joint Goal Accuracy for DST results on MultiWOZ 2.2 and MultiWOZ 2.4
    dataset. ‘-’ denotes that the results are not reported in the original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric $\downarrow$ | Attraction | Hotel | Restaurant | Taxi | Train |'
  prefs: []
  type: TYPE_TB
- en: '| Replaced Dialogues | 2538 | 3235 | 3666 | 1397 | 2840 |'
  prefs: []
  type: TYPE_TB
- en: '| Replaced Turns | 13348 | 30402 | 25768 | 6662 | 33364 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. replaced turns per dialogue | 5.26 | 9.40 | 7.03 | 4.77 | 11.75 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. tokens per replaced turn | 15.57 | 15.54 | 15.33 | 18.28 | 16.44 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg. slots per replaced user turn | 1.38 | 2.75 | 2.54 | 1.37 | 2.90 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Substituting details for 5 domains of MultiWOZ 2.2.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Replaced Domain | Impact | JGA ($\Delta$  ) |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 0% | 65.42 | 95.47% | 93.25% | 94.35% |'
  prefs: []
  type: TYPE_TB
- en: '| Attraction | 28.1% | 64.99 ($-$0.18%) |'
  prefs: []
  type: TYPE_TB
- en: '| Hotel | 42.1% | 64.28 ($-$0.34%) |'
  prefs: []
  type: TYPE_TB
- en: '| Restaurant | 41.2% | 64.61 ($-$0.01%) |'
  prefs: []
  type: TYPE_TB
- en: '| Taxi | 9.1% | 65.22 ($-$0.10%) |'
  prefs: []
  type: TYPE_TB
- en: '| Train | 38.4% | 64.23 ($-$0.24%) |'
  prefs: []
  type: TYPE_TB
- en: '| Averaged | 31.20% | 64.67 ($-$0.17%) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: JGA for substituting real data with generated data on MultiWOZ 2.2
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Real Data Size | JGA${}_{\text{R}}$ ) | Slot |'
  prefs: []
  type: TYPE_TB
- en: '| $\text{Precision}_{\text{R+G}}$ ) |'
  prefs: []
  type: TYPE_TB
- en: '| MultiWOZ 2.2 | 1000 | 58.77 | 63.06 ($+$1.08%) |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 62.66 | 64.43 ($+$0.41%) |'
  prefs: []
  type: TYPE_TB
- en: '| 4000 | 64.01 | 65.84 ($+$0.22%) |'
  prefs: []
  type: TYPE_TB
- en: '| All | 65.42 | 66.25 ($+$0.22%) |'
  prefs: []
  type: TYPE_TB
- en: '| MultiWOZ 2.4 | 1000 | 64.60 | 69.69 ($+$0.83%) |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 72.15 | 75.58 ($+$0.52%) |'
  prefs: []
  type: TYPE_TB
- en: '| 4000 | 75.81 | 77.29 ($+$0.21%) |'
  prefs: []
  type: TYPE_TB
- en: '| All | 77.20 | 78.20 ($+$0.14%) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: JGA and Slot Performance for fine-tuning with different sizes of real
    data from MultiWOZ 2.2 and 2.4.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GPT-4 version used for simulation is gpt-4-1106-preview. As for the fine-tuning
    stage, 8 Nvidia A100 (80G) GPUs are utilized for supervised full-parameter tuning
    with pytorch’s FSDP framework (Zhao et al., [2023](#bib.bib57)). The base model
    is 7B version⁴⁴4[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    of LLaMA 2\. For each fine-tuning stage, the learning rate is set to 2e-5 with
    the cosine scheduler Loshchilov and Hutter ([2016](#bib.bib28)), and the batch
    size is set to 8 on each GPU. We utilize Adam optimizer Kingma and Ba ([2015](#bib.bib20))
    with $\beta_{1}$ = 0.999, and the warm-up ratio is set to 3%. Both fine-tuning
    stages last around two hours. For inference, vLLM⁵⁵5[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm) Kwon
    et al. ([2023](#bib.bib23)) is used.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess the efficacy of the generated dialogue data, fine-tune LLaMA 2 solely
    using real data, referring to it as LUAS${}_{\text{R}}$, which serves as a strong
    baseline. We also conduct comparisons between our model and other strong baselines,
    including TRADE (Wu et al., [2019](#bib.bib47)), UniLM Dong et al. ([2019](#bib.bib6)),
    DS-DST Zhang et al. ([2020](#bib.bib54)), TripPy Heck et al. ([2020](#bib.bib13)),
    AG-DST Tian et al. ([2021](#bib.bib40)), SDP-DST Lee et al. ([2021](#bib.bib24)),
    D3ST Zhao et al. ([2022](#bib.bib56)), SPACE-3 He et al. ([2022](#bib.bib12)),
    MSP-L Sun et al. ([2022](#bib.bib39)), RefPyDST King and Flanigan ([2023](#bib.bib19)),
    Diable Lesci et al. ([2023](#bib.bib25)), DDSA Yang et al. ([2023b](#bib.bib49)),
    SPLAT Bebensee and Lee ([2023](#bib.bib3)), MoNET Zhang et al. ([2023](#bib.bib53)),
    SSNet Atawulla et al. ([2023](#bib.bib1)), TOATOD Bang et al. ([2023](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Results for DST prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The whole results are shown in Table [3](#S4.T3 "Table 3 ‣ 4.1 Datasets and
    Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation"), it needs to be pointed out that our model is primarily
    compared with the generation-based models, because classification-based models
    can utilize external knowledge, leading to unfair comparisons. LUAS${}_{\text{R}}$
    is fine-tuned on both real and generated data. From these results, we have the
    following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) On both MultiWOZ 2.2 and 2.4 datasets, the performance of LLaMA 2 fine-tuned
    on real data (LUAS${}_{\text{R}}$) surpasses previous DST baselines. This outcome
    underscores the exceptional effectiveness of LLaMA 2.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Furthermore, the incorporation of additional generated data yields significant
    performance improvements, with enhancements of 0.83% on MultiWOZ 2.2 and 1% on
    MultiWOZ 2.4\. This improvement emphasizes the important role of generated data
    in boosting overall model performance. As is shown in the next section, the gain
    from the generated data can be even larger in case the real dialogue data is of
    a smaller size. For example, the enhancement can be as large as from 4.29% to
    5.09% if only 1,000 dialogue real data exists. Considering the challenge in dialogue
    data collection, this result highlights the pragmatic significance of integrating
    generated data for DST development across domains.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Results of Substituting Real Data with Generated Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to further validate the quality and effectiveness of the generated
    dialogue data, we conduct a data replacement experiment for different domains
    on MultiWOZ 2.2\. In these experiments, all dialogue data segments related to
    a specific domain will be removed, and the newly generated data will be inserted
    at the removed location. After replacement, the new training set will consist
    of 1 domain with the generated data and 4 others with real data. The replacement
    details are shown in Table [4](#S4.T4 "Table 4 ‣ 4.1 Datasets and Metrics ‣ 4
    Experiments ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents
    Simulation").
  prefs: []
  type: TYPE_NORMAL
- en: The model is also trained on LLaMA 2 7B, the results are shown in Table [5](#S4.T5
    "Table 5 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State
    Tracking Models through LLM-backed User-Agents Simulation"), and the ‘($\Delta$)’
    denotes the difference between the results of real data and real data with 1 domain
    replaced with generated data. Statistically, the generated data on average affects
    31.2% of the training data, the test JGA decrease is from -0.2 to -1.19 with an
    average of -0.75, and the slot precision is on par with before with the recall
    drops by -0.32% on average. Compared to the reduction in training data size, the
    decreases in JGA and slot performance are relatively minor, suggesting that using
    generated data can effectively adapt DST models to a new domain with decent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In practical applications, our method for automated dialogue generation offers
    a fast way to develop dialogue systems in new domains, resulting in considerable
    savings in both time and cost.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.6.1 The Effect of Adding Generated Data to Real Data of Various Sizes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e6607e2fda2e490fe03ec3bc9bbc26f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The error distribution between $\text{LUAS}_{\text{R}}$ with different
    sizes of real data on MultiWOZ 2.2.'
  prefs: []
  type: TYPE_NORMAL
- en: To better illustrate the impact of generated data, we conduct a series of experiments
    by combining generated data with various sizes of real data. The experiment results
    are demonstrated in Table [6](#S4.T6 "Table 6 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation")
    and the sizes of real data used are set to be 1000, 2000, 4000, and all. The $\text{JGA}_{\text{R}}$
    represent the DST and slots accuracy results obtained from training with the same
    real data along with additional generated data. The symbols used in Table [5](#S4.T5
    "Table 5 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State
    Tracking Models through LLM-backed User-Agents Simulation") are also used here.
  prefs: []
  type: TYPE_NORMAL
- en: The findings indicate that incorporating generated data into the training process
    significantly enhances model performance, surpassing that achieved with solely
    real data, particularly in scenarios where real training data is scarce. Under
    such situation, the performance of a model trained with generated data can be
    comparable to the model trained with twice amount of real data. For example, when
    only using 1,000 real data, the JGA of the two datasets will increase by 4.29%
    and 5.09% if the generated data is used, which is comparable to the performance
    of using 2,000 real data. Such findings hold considerable practical relevance,
    as they underscore the capacity of generated data to substantially mitigate the
    limitations posed by insufficient original data in real-world contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2 Error Distribution Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.6.1 The Effect of Adding Generated
    Data to Real Data of Various Sizes ‣ 4.6 Analysis ‣ 4 Experiments ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation"), to
    further highlight the superiority of our approach, we examine the error distribution
    of different sizes of real data on MultiWOZ 2.2 between $\text{LUAS}_{\text{R}}$.
    Using generated data leads to a reduction in errors across almost all domain categories
    compared to models fine-tuned solely on original data. This finding not only confirms
    the high quality of our generated data but also emphasizes the effectiveness of
    our approach in enhancing model performance in DST.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose a novel approach utilizing GPT-4 to simulate conversations
    between users and agents and generate dialogues with dialogue state labels. We
    then conduct a two-stage fine-tuning of LLaMA 2 on both the generated and real
    data for DST prediction. Experimental results on two public DST benchmarks demonstrate
    that our model, augmented with generated data, outperforms the baseline trained
    solely on real data. Furthermore, detailed analysis confirms the adaptability
    of our approach, effectively meeting the dynamic requirements of transitioning
    to new domains in real-world scenarios. We believe that our method can be extended
    into a generalizable framework, offering benefits to a wide range of dialogue-related
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the high-quality dialogue data produced by our algorithm, we have significantly
    enhanced the DST model. We are also confident that the data generated will serve
    as valuable resources for other dialogue-related tasks, such as dialogue generation.
    We plan to explore this aspect in future research.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct experiments using two publicly available datasets in addition to
    datasets created by GPT-4, with a specific focus on multi-domain task-oriented
    dialogue. Each dataset is subjected to thorough pre-processing for academic research
    purposes, which includes the removal of any personally identifiable information
    or offensive content. As a result, we are confident that our work presents no
    ethical risks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Atawulla et al. (2023) Abibulla Atawulla, Xi Zhou, Yating Yang, Bo Ma, and Fengyi
    Yang. 2023. A slot-shared span prediction-based neural network for multi-domain
    dialogue state tracking. In *ICASSP 2023-2023 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP)*, pages 1–5\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bang et al. (2023) Namo Bang, Jeehyun Lee, and Myoung-Wan Koo. 2023. [Task-optimized
    adapters for an end-to-end task-oriented dialogue system](https://doi.org/10.18653/v1/2023.findings-acl.464).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    7355–7369, Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bebensee and Lee (2023) Björn Bebensee and Haejun Lee. 2023. [Span-selective
    linear attention transformers for effective and robust schema-guided dialogue
    state tracking](https://doi.org/10.18653/v1/2023.acl-long.6). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 78–91, Toronto, Canada. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budzianowski et al. (2018) Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng,
    Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ
    - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling.
    In *Proc. of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Lu Chen, Boer Lv, Chi Wang, Su Zhu, Bowen Tan, and Kai Yu.
    2020. Schema-guided multi-domain dialogue state tracking with graph attention
    neural networks. In *Proc. of AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang,
    Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training
    for natural language understanding and generation. *Advances in neural information
    processing systems*, 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eric et al. (2020) Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit
    Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur.
    2020. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections
    and state tracking baselines. In *Proc. of LREC*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2023) Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-Ming
    Wu. 2023. Towards llm-driven dialogue state tracking. In *Proc. of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2020) Shuyang Gao, Sanchit Agarwal, Di Jin, Tagyoung Chung, and
    Dilek Hakkani-Tur. 2020. From machine reading comprehension to dialogue state
    tracking: Bridging the gap. In *Proceedings of the 2nd Workshop on Natural Language
    Processing for Conversational AI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung,
    and Dilek Hakkani-Tur. 2019. Dialog state tracking: A neural reading comprehension
    approach. In *Proc. of SIGDIAL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2021) Ting Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian, Chongxuan
    Huang, Dazhen Wan, Wei Peng, and Minlie Huang. 2021. Multiwoz 2.3: A multi-domain
    task-oriented dialogue dataset enhanced with annotation corrections and co-reference
    annotation. In *Proc. of NLPCC*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2022) Wanwei He, Yinpei Dai, Min Yang, Jian Sun, Fei Huang, Luo Si,
    and Yongbin Li. 2022. Unified dialog model pre-training for task-oriented dialog
    understanding and generation. In *Proceedings of the 45th International ACM SIGIR
    Conference on Research and Development in Information Retrieval*, pages 187–200.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heck et al. (2020) Michael Heck, Carel van Niekerk, Nurul Lubis, Christian
    Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Gasic. 2020. TripPy: A triple
    copy strategy for value independent neural dialog state tracking. In *Proceedings
    of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson et al. (2014) Matthew Henderson, Blaise Thomson, and Jason D. Williams.
    2014. The second dialog state tracking challenge. In *Proc. of SIGDIAL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation
    of large language models. In *Proc. of ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hudeček and Dušek (2023) Vojtěch Hudeček and Ondřej Dušek. 2023. Are large language
    models all you need for task-oriented dialogue? In *Proceedings of the 24th Annual
    Meeting of the Special Interest Group on Discourse and Dialogue*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jablonka et al. (2023) Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero,
    and Berend Smit. 2023. Is gpt-3 all you need for low-data discovery in chemistry?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaddour and Liu (2024) Jean Kaddour and Qi Liu. 2024. Synthetic data generation
    in low-resource settings via fine-tuning of large language models. *ArXiv preprint*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'King and Flanigan (2023) Brendan King and Jeffrey Flanigan. 2023. Diverse retrieval-augmented
    in-context learning for dialogue state tracking. In *Findings of the Association
    for Computational Linguistics: ACL 2023*, pages 5570–5585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method
    for stochastic optimization. In *Proc. of ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ko et al. (2015) Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur.
    2015. Audio augmentation for speech recognition. In *Proc. of Interspeech*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *Proc.
    of NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
    memory management for large language model serving with pagedattention. In *Proceedings
    of the 29th Symposium on Operating Systems Principles*, pages 611–626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2021) Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021. Dialogue
    state tracking with a language model using schema-driven prompting. In *Proc.
    of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesci et al. (2023) Pietro Lesci, Yoshinari Fujinuma, Momchil Hardalov, Chao
    Shang, Yassine Benajiba, and Lluis Marquez. 2023. Diable: Efficient dialogue state
    tracking as operations on tables. In *Proc. of ACL Findings*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and
    Yan Xifeng. 2022. Controllable dialogue simulation with in-context learning. In
    *Proc. of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020) Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale
    Fung. 2020. MinTL: Minimalist transfer learning for task-oriented dialogue systems.
    In *Proc. of EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. 2016. Sgdr:
    Stochastic gradient descent with warm restarts. In *International Conference on
    Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2019) Mingyu Derek Ma, Kevin Bowden, Jiaqi Wu, Wen Cui, and Marilyn
    Walker. 2019. Implicit discourse relation identification for open-domain dialogues.
    In *Proc. of ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mrkšić et al. (2017) Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien Wen, Blaise
    Thomson, and Steve Young. 2017. Neural belief tracker: Data-driven dialogue state
    tracking. In *Proc. of ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Chatgpt. [https://chat.openai.com](https://chat.openai.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2019) Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation
    method for automatic speech recognition. In *Proc. of INTERSPEECH*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2021) Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh,
    Lars Liden, and Jianfeng Gao. 2021. Soloist: Building task bots at scale with
    transfer learning and machine teaching. *Transactions of the Association for Computational
    Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2018) Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. Towards
    universal dialogue state tracking. In *Proc. of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface. In *Proc. of NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shorten and Khoshgoftaar (2019) Connor Shorten and Taghi M Khoshgoftaar. 2019.
    A survey on image data augmentation for deep learning. *Journal of Big Data*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Significant-gravitas (2023) Significant-gravitas. 2023. auto-gpt: An experimental
    open-source attempt to make gpt-4 fully autonomous. [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2023) Ruolin Su, Jingfeng Yang, Ting-Wei Wu, and Biing-Hwang Juang.
    2023. Choice fusion as knowledge for zero-shot dialogue state tracking. In *Proc.
    of ICASSP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Zhoujian Sun, Zhengxing Huang, and Nai Ding. 2022. [On tracking
    dialogue state by inheriting slot values in mentioned slot pools](https://doi.org/10.24963/ijcai.2022/607).
    In *Proceedings of the Thirty-First International Joint Conference on Artificial
    Intelligence, IJCAI-22*, pages 4375–4382\. International Joint Conferences on
    Artificial Intelligence Organization. Main Track.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2021) Xin Tian, Liankai Huang, Yingzhan Lin, Siqi Bao, Huang He,
    Yunyi Yang, Hua Wu, Fan Wang, and Shuqi Sun. 2021. Amendable generation for dialogue
    state tracking. In *Proceedings of the 3rd Workshop on Natural Language Processing
    for Conversational AI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *ArXiv preprint*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *ArXiv preprint*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ulmer et al. (2024) Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun,
    Xibin Gao, and Yi Zhang. 2024. Bootstrapping llm-based task-oriented dialogue
    agents via self-talk. *ArXiv preprint*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng
    Lin, Shi Wang, Dacheng Tao, and Li Guo. 2023. Divide, conquer, and combine: Mixture
    of semantic-independent experts for zero-shot dialogue state tracking. In *Proc.
    of ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yifan Wang, Jing Zhao, Junwei Bao, Chaoqun Duan, Youzheng
    Wu, and Xiaodong He. 2022. LUNA: Learning slot-turn alignment for dialogue state
    tracking. In *Proc. of NAACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei and Zou (2019) Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation
    techniques for boosting performance on text classification tasks. In *Proc. of
    EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming
    Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state
    generator for task-oriented dialogue systems. In *Proc. of ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023a) Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili
    Wang, Shusen Wang, and Hai Zhao. 2023a. Refgpt: Dialogue generation of gpt, by
    gpt, and for gpt. In *Proc. of EMNLP Findings*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023b) Longfei Yang, Jiyi Li, Sheng Li, and Takahiro Shinozaki.
    2023b. Multi-domain dialogue state tracking with disentangled domain-slot attention.
    In *Proc. of ACL Findings*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2022) Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2022. MultiWOZ
    2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections
    to improve state tracking evaluation. In *Proceedings of the 23rd Annual Meeting
    of the Special Interest Group on Discourse and Dialogue*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2021) Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, Shenghui Li,
    and Emine Yilmaz. 2021. Slot self-attentive dialogue state tracking. In *Proc.
    of WWW*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zang et al. (2020) Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav
    Gupta, Jianguo Zhang, and Jindong Chen. 2020. MultiWOZ 2.2 : A dialogue dataset
    with additional annotation corrections and state tracking baselines. In *Proceedings
    of the 2nd Workshop on Natural Language Processing for Conversational AI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Haoning Zhang, Junwei Bao, Haipeng Sun, Youzheng Wu, Wenye
    Li, Shuguang Cui, and Xiaodong He. 2023. [MoNET: Tackle state momentum via noise-enhanced
    training for dialogue state tracking](https://doi.org/10.18653/v1/2023.findings-acl.33).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    520–534, Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Jianguo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wang,
    Philip Yu, Richard Socher, and Caiming Xiong. 2020. Find or classify? dual strategy
    for slot-value predictions on multi-domain dialog state tracking. In *Proceedings
    of the Ninth Joint Conference on Lexical and Computational Semantics*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level
    convolutional networks for text classification. In *Proc. of NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022) Jeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu, Mingqiu Wang,
    Harrison Lee, Abhinav Rastogi, Izhak Shafran, and Yonghui Wu. 2022. Description-driven
    task-oriented dialog modeling. *ArXiv preprint*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin
    Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison,
    Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit
    Mathews, and Shen Li. 2023. Pytorch fsdp: Experiences on scaling fully sharded
    data parallel. *Proc. VLDB Endow.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Han Zhou, Ignacio Iacobacci, and Pasquale Minervini. 2023.
    XQA-DST: Multi-domain and multi-lingual dialogue state tracking. In *Proc. of
    ACL Findings*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2022) Qi Zhu, Bing Li, Fei Mi, Xiaoyan Zhu, and Minlie Huang. 2022.
    Continual prompt tuning for dialog state tracking. In *Proc. of ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Prompts for Simulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we illustrated a variety of typical prompts utilized by the
    user simulator and the agent simulator within the simulation. The symbol ‘$\backslash$n’
    of the prompts represents a line break.
  prefs: []
  type: TYPE_NORMAL
- en: Table [7](#A1.T7 "Table 7 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the user informs requirement to the agent. Table [8](#A1.T8 "Table
    8 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation") represents the prompt that the user
    updates the requirement to the agent. Table [9](#A1.T9 "Table 9 ‣ Appendix A Prompts
    for Simulation ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents
    Simulation") represents the prompt that the user asks for a recommendation with
    the control identifier ‘[RECOM]’ to request a recommendation from the agent. Table
    [10](#A1.T10 "Table 10 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the user inquires about some properties (e.g. address and postcode)
    for the candidates. Table [11](#A1.T11 "Table 11 ‣ Appendix A Prompts for Simulation
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation")
    represents the prompt that the user asks for a booking action from the agent.
    Table [12](#A1.T12 "Table 12 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the user’s general chat with the agent with the control identifier
    ‘[EOF]’ to inform the agent that all the needs are satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: Table [13](#A1.T13 "Table 13 ‣ Appendix A Prompts for Simulation ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation") represents
    the prompt that is used by the slot extractor.
  prefs: []
  type: TYPE_NORMAL
- en: Table [14](#A1.T14 "Table 14 ‣ Appendix A Prompts for Simulation ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation") represents
    the prompt that the agent inquires the user for a specified requirement(e.g. restaurant-pricerange).
    Table [15](#A1.T15 "Table 15 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the agent responds with the properties that the user inquiries. Table
    [16](#A1.T16 "Table 16 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the agent reports search results to the user. Table [17](#A1.T17 "Table
    17 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation") represents the prompt that the agent
    reports the action result (e.g. the reservation information, etc.) and control
    identifier ‘[BOOKED]’ to inform the successful of the reservation. Table [18](#A1.T18
    "Table 18 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue State Tracking
    Models through LLM-backed User-Agents Simulation") represents the prompt that
    the agent general chats with the user and outputs the control identifier ‘[EOD]’
    to end the whole simulation.
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: This is your first time in Cambridge and want to find a restaurant.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: Do you know of any good eateries in the north of Cambridge? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Prompt for the user to inform requirement.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [old requirements], [new requirements] $\rightarrow$ [user_utterance]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are the first time in Cambridge and want to find a hotel.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: How about guesthouses? Do you have any 4-star options in the north
    of Cambridge for a group of 4, staying from Saturday for 4 nights? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Prompt for the user to update requirement.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: This is your first time here and want to find a place to eat.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: Yes, please recommend one. [RECOM] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Prompt for the user to ask a recommendation with control identifier
    ‘[RECOM]’.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [requirements], [properties] $\rightarrow$ [user_utterance]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: This is your first time in here and want to find a place to eat.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: Could you provide the address and postcode for that place? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Prompt for the user to inquire the properties of the candidate.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: Now you are chatting with a local guide online.$\backslash$n Only
    output the newest utterance, don’t output the conversation history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: Yes, please proceed with the booking at Worth House. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Prompt for the user to ask for a booking action from the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: This is your first time in cambridge and want to find a hotel.$\backslash$n
    Please thanks for the help from the local guide and output the mark ‘[EOF]‘Only
    output the newest utterance, don’t output the conversation history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: Grateful for your guidance.[EOF] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Prompt for the user to chat with the agent with the control identifier
    ‘[EOF]’.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [slots schema] $\rightarrow$ [slots] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are a local agent, and now chatting with the user online for
    ‘restaurant.$\backslash$n - the ‘slot value‘ should be mentioned in the schema
    ‘possible_values‘ if the slot value is categorical or you need to extract its
    value exactly from the conversion history. |'
  prefs: []
  type: TYPE_TB
- en: '| Output: {"slots": {"restaurant-area": "north"}, "service": "restaurant"}
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Prompt for the slot extractor of restaurant domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [inquire_requirements] $\rightarrow$ [agent_utterance], [inquire_requirements]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are a local agent for ‘restaurant‘, and are chatting with the
    user online.$\backslash$n Please answer in a JSON format, {"response": "", "inquire_requirements":
    []} |'
  prefs: []
  type: TYPE_TB
- en: '| Output: {"response": "Are you looking for something more upscale or casual?",
    "inquire_requirements": ["restaurant-pricerange"]} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Prompt for the agent to inquire the user for a specified requirement.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are a local agent, and now are chatting with the user online
    for restaurant.$\backslash$n Please answer in a JSON format {"response": ""} |'
  prefs: []
  type: TYPE_TB
- en: '| Output: Certainly! Golden Wok is located at 191 Histon Road Chesterton, with
    the postcode CB43HL. Would you like me to make a reservation for you? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Prompt for the agent to answer the user’s inquiry properties.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are a local agent, and now are chatting with the user online
    for restaurant.$\backslash$n Please answer in a JSON format {"response": ""} |'
  prefs: []
  type: TYPE_TB
- en: '| Output: {"response": "I found a couple of places that fit your preferences.
    Would you like me to recommend one?"} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: Prompt for the agent to report search results.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are a local agent, and now are chatting with the user online
    for restaurant. $\backslash$n Please answer in a JSON format {"response": ""}
    |'
  prefs: []
  type: TYPE_TB
- en: '| Output: {"response": "Your reservation at Golden Wok for eight people on
    Monday at 18:00 hours has been successfully made. Your reference code is 3FA7B6Z9\.
    [BOOKED]"} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17: Prompt for the agent to report reservation information of booking
    action with control identifier ‘[BOOKED]’.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [history]$\rightarrow$ [agent_utterance] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: You are a local agent for ‘restaurant‘, and are chatting with the
    user online.$\backslash$n Please answer in a JSON format, {"response": ""} |'
  prefs: []
  type: TYPE_TB
- en: '| Output: {"response": "You’re welcome! Enjoy your meal at Golden Wok. If you
    need anything else, just ask. [EOD]"} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 18: Prompt for the agent to conclude the dialogue with control identifier
    ‘[EOD]’.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Templates for Booking Responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first crafted template responses like There are a lot of {type} attractions
    available. Would you like information about one of those? Perhaps, a {type} like
    {name}? and then expand them to hundreds with the GPT-4’s rewriting ability as
    shown in Table [19](#A2.T19 "Table 19 ‣ Appendix B Templates for Booking Responses
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation").
    In our simulation, the templated response will randomly substitute the recommendation
    responses of the agent to enhance the variety of interactions.
  prefs: []
  type: TYPE_NORMAL
- en: '| DST: [template]$\rightarrow$ [rewrited templates] |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt: $\backslash$n |'
  prefs: []
  type: TYPE_TB
- en: '| Output: $\backslash$n ] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 19: Prompt for template rewriting.'
  prefs: []
  type: TYPE_NORMAL
