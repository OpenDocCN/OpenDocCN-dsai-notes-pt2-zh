- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.07401](https://ar5iv.labs.arxiv.org/html/2402.07401)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Kyungha Kim^($\heartsuit$)¹¹footnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: Hou Pong Chan^($\spadesuit$)
  prefs: []
  type: TYPE_NORMAL
- en: ^($\heartsuit$)University of Illinois Urbana-Champaign
  prefs: []
  type: TYPE_NORMAL
- en: ^($\spadesuit$)Northwestern University
  prefs: []
  type: TYPE_NORMAL
- en: ^($\heartsuit$){kyungha2, slee677, khhuang3, hengji}@illinois.edu
  prefs: []
  type: TYPE_NORMAL
- en: ^($\spadesuit$)manling.li@northwestern.edu   Equal contribution.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fact-checking research has extensively explored verification but less so the
    generation of natural-language explanations, crucial for user trust. While Large
    Language Models (LLMs) excel in text generation, their capability for producing
    faithful explanations in fact-checking remains underexamined. Our study investigates
    LLMs’ ability to generate such explanations, finding that zero-shot prompts often
    result in unfaithfulness. To address these challenges, we propose the Multi-Agent
    Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse
    roles in an iterative refining process aimed at enhancing faithfulness in generated
    explanations. MADR ensures that the final explanation undergoes rigorous validation,
    significantly reducing the likelihood of unfaithful elements and aligning closely
    with the provided evidence. Experimental results demonstrate that MADR significantly
    improves the faithfulness of LLM-generated explanations to the evidence, advancing
    the credibility and trustworthiness of these explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the digital age, swiftly spreading misinformation necessitates not only the
    verification of claims but also the provision of clear explanations for these
    verifications. Such explanations are crucial for building trust within the audience,
    as lack of them often leads to distrust in fact-checking results Guo et al. ([2022](#bib.bib7)).
    Moreover, explanation generation becomes even more critical in multi-hop fact-checking,
    where complex reasoning across multiple evidence pieces is required to assess
    a claim’s veracity Reddy et al. ([2023](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the adeptness of Large Language Models (LLMs) in generating diverse
    texts, their capacity for crafting faithful¹¹1Faithfulness refers to the factual
    consistency between the explanation and the given evidence Huang et al. ([2023a](#bib.bib8)).
    explanations for fact-checking remains underexplored. Faithfulness is crucial;
    explanations that misrepresent evidence could exacerbate misinformation, posing
    a significant challenge. Thus, enhancing the faithfulness of generated explanations
    in fact-checking is an urgent, unresolved issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first research question asks: can LLMs generate faithful explanations for
    fact-checking in a zero-shot prompting setup? To facilitate analysis, we define
    a novel typology of common errors and unfaithfulness issues that arise in LLM-generated
    explanations. We conduct extensive experiments prompting ChatGPT OpenAI ([2023c](#bib.bib17))
    to explain fact checks from multiple sources. Our findings reveal that zero-shot
    prompting LLMs often fails to yield faithful explanations. 80% of the generated
    explanations include hallucinated details ([Section 5](#S5 "5 Results ‣ Can LLMs
    Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to our second research question: how to address the unfaithfulness
    issues in LLM-generated explanations? We propose the Multi-Agent Debate Refinement
    (MADR) framework that uses multiple LLMs as agents to provide feedback for iterative
    refinement to produce faithful explanations ([Section 3](#S3 "3 Methodology ‣
    Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate")). The goal is to mitigate unfaithfulness
    and steer the LLM-generated texts towards true rationales. Experimental results
    show that MADR significantly improves faithfulness upon baselines, demonstrating
    the effectiveness of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Example |'
  prefs: []
  type: TYPE_TB
- en: '| Claim: Says Jeff Foxworthy wrote a list of examples explaining how "you might
    live in a nation that was founded by geniuses but is run by idiots." |'
  prefs: []
  type: TYPE_TB
- en: '| Evidence: … Foxworthy is famous for his "You might be a redneck if" jokes
    , but searching online, we couldn’t find any credible evidence that he penned
    this list that touches on abortion, Muslims and federal debt. Rather, we found
    users on web forums crediting someone named Fritz Edmunds with the list. Snopes,
    which fact-checked this claim back in 2013, also noted that "the original compiler
    of this appears to be Fritz Edmunds, who posted it to his Politically True blog
    back in Feb. 3, 2013 … |'
  prefs: []
  type: TYPE_TB
- en: '| Error Type & Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| Intrinsic Entity Error: The generated explanation misrepresents named entities,
    quantities, dates, or other surface realizations from the given source. E.g. Fritz
    Foxworthy was credited on a web forum with the list. |'
  prefs: []
  type: TYPE_TB
- en: '| Extrinsic Entity Error: The generated explanation includes new entities that
    are not present in the given source. E.g. Elon Musk was credited on a web forum
    with the list. |'
  prefs: []
  type: TYPE_TB
- en: '| Intrinsic Event Error: The generated explanation misrepresents events mentioned
    in the source. E.g. They couldn’t find any credible evidence that Fritz Edmunds
    was credited on a web forum. |'
  prefs: []
  type: TYPE_TB
- en: '| Extrinsic Event Error: The generated explanation include new events that
    are not present in the given source. E.g. Foxworthy found that Fritz Edmunds made
    the “You might be a redneck if” jokes. |'
  prefs: []
  type: TYPE_TB
- en: '| Intrinsic Noun-Phrase Error: The explanation mistakenly represents the noun
    phrases in the given source like miscombining modifiers combined with one entity
    to another entity. E.g. They found the original user on web forums crediting someone
    named Fritz Edmunds. |'
  prefs: []
  type: TYPE_TB
- en: '| Extrinsic Noun-Phrase Error: The explanation mistakenly represents new noun
    phrases that are not present in the given source like miscombining modifiers not
    presented in the source to entity. E.g. They found a mysterious user on web forums
    crediting someone named Fritz Edmunds. |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning Coherence Error: There are logical flaws in the flow of reasoning
    within the generated explanation, leading to a lack of coherence or weak support
    for the claim. E.g. While they were searching online, they couldn’t find any credible
    evidence that he penned this list that touches on abortion. |'
  prefs: []
  type: TYPE_TB
- en: '| Overgeneralization Error: The generated explanation makes sweeping statements
    or draws conclusions that go beyond the evidence provided. E.g. Fritz Emunds is
    the one who spreaded the rumor and put the blame on Foxworthy. |'
  prefs: []
  type: TYPE_TB
- en: '| Irrelevant Evidence Error: The generated explanation includes evidence that
    is not directly related to the claim, leading to confusion and lack of support
    for the main argument. E.g. … Foxworthy is famous for his "You might be a redneck
    if" jokes. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: An illustration of error typology using using a sample data from PolitiHop
    Ostrowski et al. ([2021](#bib.bib18)). The errors in the sample summaries are
    in red color and italicized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present the first study of LLMs’ ability to produce faithful fact-checking
    explanations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present Multi-Agent Debate Refinement, an effective framework to produce
    faithful explanations based on iterative debating among LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our correlation analysis reveals the most suitable LLM-based evaluation protocol
    for this task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Typology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our analysis of explanations generated by LLMs, we have introduced a novel
    typology encompassing a range of error categories, as shown in [Table 1](#S1.T1
    "In 1 Introduction ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). The classification
    of intrinsic and extrinsic errors within the domains of Entity-Related, Event-Related,
    and Noun-phrase Errors draws inspiration from relevant studies in other domains
    Goyal and Durrett ([2021](#bib.bib6)); Huang et al. ([2023c](#bib.bib10)). We
    have incorporated additional context-specific error types, enriching the overall
    typology.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero-shot prompting LLMs often produce unfaithful explanations which contain
    multiple errors. In the early stage of our experiments, we incorporate an iterative
    refinement paradigm for improving their faithfulness. However, we found that self-refinement
    Madaan et al. ([2023](#bib.bib14)) alone was insufficient for faithfulness enhancement
    (see [Table 3](#S5.T3 "In 5 Results ‣ Can LLMs Produce Faithful Explanations For
    Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate")),
    as imprecise feedback tended to guide misguided refinements of the explanation.
    This underscores the pivotal role of precise feedback for efficient refinement
    by the LLM Wang et al. ([2023](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6eab3a02ed452aa3b90354dc199ef0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of MADR.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we propose Multi-Agent Debate Refinement (MADR), inspired by a debate-based
    methodology Du et al. ([2023](#bib.bib4)). While Du et al. ([2023](#bib.bib4))
    focus on refining explanations during a debate, MADR utilizes the debate for generating
    feedback to be employed in subsequent refinement stages. Our method offers several
    advantages. First, compared to directly refining explanations during a debate,
    MADR facilitates a dynamic and iterative feedback loop, enhancing the identification
    of errors. Secondly, it ensures more accurate feedback, reducing the likelihood
    of misguided refinements and ultimately enhancing overall faithfulness. Thirdly,
    this approach prompts bidirectional thinking within the LLM, enabling it to analyze
    explanations both with and without knowledge of predefined error types, fostering
    an explicit rationale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of MADR is outlined in [Algorithm 1](#alg1 "In Appendix B MADR
    Details ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful
    Explainable Fact-Checking via Multi-Agent Debate") and depicted in [Figure 1](#S3.F1
    "In 3 Methodology ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). MADR employs
    multiple agents to identify errors and engage them in a debate until a consensus
    is reached on the debate. Four total roles are assigned to each agent: two serve
    as Debaters, one as a Judge, and one as a Refiner.An initial explanation $E$ identifies
    each Debater (lines 2-3). Distinctive instructions with varying goals are provided
    to the Debaters: Debater 1 identifies errors based on a predefined error typology
    (see [Section 2](#S2 "2 Typology ‣ Can LLMs Produce Faithful Explanations For
    Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate")),
    while Debater 2 focuses on potential errors that may affect the explanation’s
    faithfulness, without relying on the error typology (refer to [Table 8](#A3.T8
    "In Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate") and [Table 9](#A3.T9
    "In Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate") for prompt
    specifics). This setup ensures that MADR promotes the identification of errors
    that might be overlooked by either party.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the $i$). They refine their feedback by adding any missed elements
    and removing errors (lines 9-10).
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the most accurate feedback, the two Debaters continue their discussion
    until they reach a mutual agreement on the feedback. During the $i$, the debate
    stops (lines 6-7). Finally, we concatenate the final feedback from both Debaters
    and feed it to the refiner to refine its explanation using the concatenated feedback
    (line 13). An example of the outputs from MADR is shown in [Table 11](#A3.T11
    "In Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). Additionally,
    to prevent endless debates, we set a fixed number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dataset and Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Experiments are conducted on the PolitiHop multi-hop fact-checking dataset Ostrowski
    et al. ([2021](#bib.bib18)). PolitiHop consists of 445 test set instances, where
    each instance contains a claim and multiple pieces of evidence. The veracity of
    a claim can only be determined by reasoning across multiple pieces of evidence
    and the claim. For the evaluation metric, we use G-Eval Liu et al. ([2023](#bib.bib12))
    with GPT-4 Turbo OpenAI ([2023b](#bib.bib16)) to assess whether the generated
    explanation is consistent with the evidence. Following Huang et al. ([2023b](#bib.bib9)),
    we adopted 4 evaluation protocols based on G-Eval which vary in granularity, ranging
    from sentence-level to document-level assessments, and in the application of our
    error typology, validating the effectiveness of the error typology in assisting
    automatic evaluation. The prompt templates are shown in [Appendix C](#A3 "Appendix
    C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards
    Faithful Explainable Fact-Checking via Multi-Agent Debate").
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We compare MADR with the following competitive methods. Zero-shot prompts an
    LLM to directly output an explanation given the input claim and evidence. CoT
    asks LLMs to generate the reasoning process before producing the final output.
    Self-Refine Madaan et al. ([2023](#bib.bib14)) generates an initial explanation
    and then iteratively refines the explanation with one agent. We conduct experiments
    by using GPT-3.5-Turbo OpenAI ([2023a](#bib.bib15)) to generate explanations across
    all experiments for fair comparisons. The prompts for these approaches are displayed
    in [Appendix C](#A3 "Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations
    For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent
    Debate"). The case study using Self-Refine is in Table [10](#A3.T10 "Table 10
    ‣ Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate").
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Granularity$\rightarrow\leavevmode\nobreak\ $ Zero-shot 4.87 4.84 4.70 4.92
    CoT 4.86 4.91 4.76 4.96 Self-Refine 4.70 4.86 4.89 4.81 MADR (ours) 4.82 4.99
    4.88 4.97
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Faithfulness evaluation on PolitiHop test set. Scores are computed
    using G-Eval with evaluation protocols of varying granularity and application
    of our error typology. The best score per column is bolded.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method Faithful Explanations (%) # Errors Zero-shot 20.0 25 CoT 5.0 42 Self-Refine
    20.0 32 MADR (ours) 30.0 17'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Human evaluation results on 20 samples from PolitiHop. MADR produces
    the most faithful explanation compared to baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: The main results are summarized in [Table 2](#S5.T2 "In 5 Results ‣ Can LLMs
    Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"). MADR achieves the best faithfulness scores
    on two out of the four evaluation protocols, indicating its effectiveness in producing
    faithful explanations. To further validate the effectiveness of our method, we
    conduct human evaluations via Amazon Mechanical Turk, aiming to quantify the portion
    of faithful explanations and the total error count. Annotators were presented
    with our error typology and were tasked to identify the presence of each error
    type within individual sentences. The results of human evaluations are shown in
    [Table 3](#S5.T3 "In 5 Results ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). We have the
    following observations. First, using simple prompting methods, such as zero-shot
    or CoT, LLMs often produce unfaithful explanations for fact-checking. This highlights
    the challenge of generating faithful explanations for LLMs in complex fact-checking
    scenarios, such as PolitiHop, which requires reasoning through multiple pieces
    of evidence. Second, despite the high faithfulness scores suggested by automatic
    evaluation (approaching the maximum score of 5) in [Table 2](#S5.T2 "In 5 Results
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"), human evaluators frequently deemed the
    LLM-generated explanations unfaithful, as per [Table 3](#S5.T3 "In 5 Results ‣
    Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"). This discrepancy suggests that even the
    most advanced LLM, GPT-4 Turbo, fails to reliably judge the faithfulness of generated
    explanations for fact-checking.
  prefs: []
  type: TYPE_NORMAL
- en: To pinpoint the most effective LLM-based evaluation strategy for future research,
    we performed a correlation analysis, correlating human evaluations with automatic
    metrics using Kendall’s Tau (variant c). According to [Table 4](#S5.T4 "In 5 Results
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"), a granular evaluation aligns better with
    human judgments, and incorporating our error typology into automatic evaluations
    enhances the quality of LLM assessments. Details on human evaluation methodology
    are provided in [Appendix A](#A1 "Appendix A Human Evaluation Details ‣ Can LLMs
    Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate").
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, a case study showcased in [Table 11](#A3.T11 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate") highlights the superiority of MADR over
    self-refinement, by demonstrating that MADR allows Debaters to identify and correct
    errors missed during self-refinement, leading to more accurate explanations. In
    contrast, the Self-Refine approach, as shown in [Table 10](#A3.T10 "In Appendix
    C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards
    Faithful Explainable Fact-Checking via Multi-Agent Debate"), fails to produce
    a faithful explanation, emphasizing the advantage of employing multiple perspectives
    with MADR for error identification and explanation validation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Protocol Kendall’s Tau Document-level w/o Typology 0.008 Document-level
    w/ Typology 0.128 Sentence-level w/o Typology 0.105 Sentence-level w/ Typology
    0.150
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Correlation between evaluation protocols and human judgments on the
    PolitiHop dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Early approaches to producing explanations for fact-checks can largely be categorized
    into logic-based methods Gad-Elrab et al. ([2019](#bib.bib5)); Ahmadi et al. ([2019](#bib.bib1))
    and attention-based methods Shu et al. ([2019](#bib.bib21)); Lu and Li ([2020](#bib.bib13)).
    Recent work generates natural-language explanations using abstractive Kotonya
    and Toni ([2020](#bib.bib11)) or extractive Atanasova et al. ([2020](#bib.bib2))
    approaches. A very recent study benchmarks the ability of these models to generate
    explanations for fact-checks Russo et al. ([2023](#bib.bib20)). Our study complements
    this work by presenting the first empirical analysis on LLMs’ ability to generate
    fact-checking explanations and propose a method to enhance its faithfulness.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper empirically demonstrates that LLMs often produce unfaithful explanations
    for fact-checks. We introduce the Multi-Agent Debate Refinement (MADR) framework,
    which utilizes multiple LLM agents to iteratively debate and refine explanations,
    significantly enhancing their faithfulness as evidenced by both automatic and
    human evaluations. Our results underscore the efficacy of multi-agent debate in
    mitigating LLMs’ unfaithfulness. Additionally, we reveal that LLMs cannot reliably
    assess the faithfulness of the generated explanations and discover the most suitable
    evaluation protocols for LLM-based automatic evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Ethical Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs trained on internet data often show biases, but this focus mainly applies
    to data and models reflecting the culture of English-speaking communities. However,
    detailed reviews of model outputs for the PolitiHop dataset have found no signs
    of biases concerning gender, age, race, or other socioeconomic elements.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study did not thoroughly investigate the sensitivity of various systems
    to changes in input prompts. It is recognized that the effectiveness of numerous
    natural language processing tasks can significantly depend on how input prompts
    are designed. By not conducting a comprehensive analysis on prompt sensitivity,
    we acknowledge the possibility that different prompts might elicit a wide range
    of responses that we have not explored, potentially limiting the applicability
    of our findings. However, it is important to note that we did not engage in prompt
    tuning specifically to favor our proposed framework, ensuring that the comparisons
    between different techniques remain equitable. Given the scope of our research,
    the detailed exploration of prompt sensitivity is an area we have designated for
    future investigation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ahmadi et al. (2019) Naser Ahmadi, Joohyung Lee, Paolo Papotti, and Mohammed
    Saeed. 2019. Explainable fact checking with probabilistic answer set programming.
    *arXiv preprint arXiv:1906.09198*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atanasova et al. (2020) Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma,
    and Isabelle Augenstein. 2020. [Generating fact checking explanations](https://doi.org/10.18653/v1/2020.acl-main.656).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7352–7364, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen (1960) Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
    *Educational and psychological measurement*, 20(1):37–46.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. 2023. Improving factuality and reasoning in language models
    through multiagent debate. *arXiv preprint arXiv:2305.14325*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gad-Elrab et al. (2019) Mohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani,
    and Gerhard Weikum. 2019. [Exfakt: A framework for explaining facts over knowledge
    graphs and text](https://doi.org/10.1145/3289600.3290996). In *Proceedings of
    the Twelfth ACM International Conference on Web Search and Data Mining*, WSDM
    ’19, page 87–95, New York, NY, USA. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goyal and Durrett (2021) Tanya Goyal and Greg Durrett. 2021. [Annotating and
    modeling fine-grained factuality in summarization](http://arxiv.org/abs/2104.04302).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos.
    2022. [A survey on automated fact-checking](http://arxiv.org/abs/2108.11896).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023a) Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. 2023a.
    [Zero-shot faithful factual error correction](https://doi.org/10.18653/v1/2023.acl-long.311).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 5660–5676, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023b) Kung-Hsiang Huang, Philippe Laban, Alexander R Fabbri,
    Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023b.
    Embrace divergence for richer insights: A multi-document summarization benchmark
    and a case study on summarizing diverse information from news articles. *arXiv
    preprint arXiv:2309.09369*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023c) Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi R Fung,
    Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, and Heng Ji. 2023c. Do lvlms understand
    charts? analyzing and correcting factual errors in chart captioning. *arXiv preprint
    arXiv:2312.10160*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kotonya and Toni (2020) Neema Kotonya and Francesca Toni. 2020. [Explainable
    automated fact-checking for public health claims](https://doi.org/10.18653/v1/2020.emnlp-main.623).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 7740–7754, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023. [G-eval: NLG evaluation using gpt-4 with better human
    alignment](https://doi.org/10.18653/v1/2023.emnlp-main.153). In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    2511–2522, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu and Li (2020) Yi-Ju Lu and Cheng-Te Li. 2020. [GCAN: Graph-aware co-attention
    networks for explainable fake news detection on social media](https://doi.org/10.18653/v1/2020.acl-main.48).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 505–514, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. [Self-refine: Iterative refinement with self-feedback](http://arxiv.org/abs/2303.17651).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023a) OpenAI. 2023a. [Chatgpt](https://chat.openai.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023b) OpenAI. 2023b. [Gpt-4 turbo](https://help.openai.com/en/articles/8555510-gpt-4-turbo).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023c) OpenAI. 2023c. [Gpt-4v(ision) system card](https://chat.openai.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ostrowski et al. (2021) Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, and
    Isabelle Augenstein. 2021. Multi-hop fact checking of political claims. In *Proceedings
    of the Thirtieth International Joint Conference on Artificial Intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddy et al. (2023) Revanth Gangi Reddy, Yi R Fung, Qi Zeng, Manling Li, Ziqi
    Wang, Paul Sullivan, et al. 2023. Smartbook: Ai-assisted situation report generation.
    *arXiv preprint arXiv:2303.14337*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russo et al. (2023) Daniel Russo, Serra Sinem Tekiroğlu, and Marco Guerini.
    2023. [Benchmarking the generation of fact checking explanations](https://doi.org/10.1162/tacl_a_00601).
    *Transactions of the Association for Computational Linguistics*, 11:1250–1264.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. (2019) Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu.
    2019. defend: Explainable fake news detection. In *Proceedings of the 25th ACM
    SIGKDD international conference on knowledge discovery & data mining*, pages 395–405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu
    Wei, and Heng Ji. 2023. Unleashing cognitive synergy in large language models:
    A task-solving agent through multi-persona selfcollaboration. *arXiv preprint
    arXiv:2307.05300*, 1(2):3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Human Evaluation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a30f5bed421c65361be8b41b71a10fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The interface for our human evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Evaluation Guidelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this task you will evaluate the faithulness of automatically generated fact-checking
    explanation using a label, claim, and source used to generate the explanation.
    To correctly solve this task, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Carefully read and understand the topology of errors and examples given below.
    Carefully read the generated fact-checking explanation and the source. For each
    explanation, check it with the evidence and decide if any of the error exists
    in the explanation. Note: You will analyze each sentence, but you should consider
    the connection between other sentences as well. Warning: Annotations will be checked
    for quality against control labels, low quality work will be rejected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type of Errors:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intrinsic Entity-Related Errors: Intrinsic entity-related errors occur when
    there is a mistake in representing named entities, quantities, dates, or other
    surface realizations from the given source within the generated explanation. Example:
    Incorrectly combining distinct entities from the given source.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extrinsic Entity-Related Errors: Extrinsic entity-related errors involve the
    introduction of new entities that are not present in the given source into the
    generated explanation. Example: Hallucinating new entities that do not exist in
    the source.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intrinsic Event-Related Errors: Intrinsic event-related errors pertain to mistakes
    in representing events mentioned in the generated explanation, leading to incorrect
    claims about events. Example: Making inaccurate claims about events mentioned
    in the explanation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extrinsic Event-Related Errors: Extrinsic event-related errors occur when the
    generated explanation includes new events that are not present in the given source.
    Example: Introducing fabricated events that are not supported by the source.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intrinsic Noun Phrase-Related Errors: Intrinsic noun phrase-related errors
    are mistakes related to noun phrases, excluding entity-specific errors. They may
    involve miscombining noun phrases with incorrect modifiers from the given source.
    Example: Incorrectly combining a noun phrase with the wrong modifier from the
    source.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extrinsic Noun Phrase-Related Errors: Extrinsic noun phrase-related errors
    involve the introduction of new noun phrase modifiers that are not present in
    the given source into the generated explanation. Example: Hallucinating new noun
    phrase modifiers not supported by the source.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reasoning Coherence Errors: Reasoning coherence errors occur when there are
    logical flaws in the flow of reasoning within the generated explanation, leading
    to a lack of coherence or weak support for the claim. Example: Presenting evidence
    that does not logically connect to the main claim, resulting in a disjointed explanation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Overgeneralization Errors: Overgeneralization errors happen when the generated
    explanation makes sweeping statements or draws conclusions that go beyond the
    scope of the evidence provided.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irrelevant Evidence Errors: Irrelevant evidence errors occur when the generated
    explanation includes evidence that is not directly related to the claim, leading
    to confusion and lack of support for the main argument. Example: Including evidence
    that is tangential or unrelated to the claim being explained.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 Evaluation Interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We display our evaluation interface in [Figure 2](#A1.F2 "In Appendix A Human
    Evaluation Details ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate").
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Worker Qualification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We established specific initial criteria for selecting highly efficient MTurk
    workers. These prerequisites include having a HIT approval rate of at least 99%,
    completing a minimum of 10,000 approved HITs, and being located in the United
    Kingdom, Canada, or the United States.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, beyond these initial requirements, qualified workers must pass
    two rounds of qualification tests aimed at identifying errors in generated explanations.
    To refine the qualification process, we manually annotated two HITs, each featuring
    one multi-hop fact-checking instance from PolitiHop and an explanation generated
    by one of the models. In each qualification phase, annotators review one of these
    annotated examples. Those whose annotations do not closely match ours are excluded
    from the selection process.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, 4 annotators who successfully completed all two stages of the qualification
    tests were selected. Additionally, we carefully designed each HIT to ensure that
    annotators could earn an hourly rate of $15 to $20, provided they work continuously.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Annotation Quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We computed the agreement between each annotator with one of the authors of
    this paper. The agreement is 0.69 per Cohen’s Kappa Cohen ([1960](#bib.bib3)),
    indicating a moderate-to-high level of agreement.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B MADR Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 MADR
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Given claim $C$'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation prompts are shown in [Table 6](#A3.T6 "In Appendix C Prompts ‣ Can
    LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate") and [Table 5](#A3.T5 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"). The prompts for self-refinement and Multi-Agent
    Debate Refinement (MADR) are displayed in [table 7](#A3.T7 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate") and [table 11](#A3.T11 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| You will be given a fact-checking explanation along with the evidence used
    for fact-checking. Your task is to rate the explanation on one metric. Please
    make sure you read and understand these instructions carefully. Please keep this
    document open while reviewing, and refer to it as needed. Evaluation Criteria:
    Faithfulness (1-5) - the factual alignment between the fact-checking explanation
    and the evidence. The explanation should accurately reflect the evidence and its
    context, without misrepresenting or omitting crucial details. Annotators were
    instructed to penalize explanations that contain inaccuracies, misinterpretations,
    or fail to adequately represent the evidence provided. Below are the error typology
    that you need to utilize to determine faithfulness between the explanation and
    evidence: {error typology} Evaluation Steps: 1\. Read the fact-checking explanation
    and the evidence provided carefully. 2\. Compare the explanation to the evidence
    to identify how well it represents the facts, context, and conclusions drawn from
    the evidence. 3\. Assess how accurately and completely the explanation reflects
    the evidence without distortion or significant omission. Assign a faithfulness
    score from 1 to 5. Evidence Provided: {evidence} Fact-Checking Explanation: {explanation}
    Evaluation Form: Faithfulness: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Prompt templates for document-level automatic evaluation. The texts
    in grey are only presented in the prompts when error typology is applied.'
  prefs: []
  type: TYPE_NORMAL
- en: '| You will be given a sentence from a fact-checking explanation along with
    the evidence used for fact-checking. Your task is to rate the explanation sentence
    on one metric. Please make sure you read and understand these instructions carefully.
    Please keep this document open while reviewing, and refer to it as needed. Evaluation
    Criteria: Faithfulness (1-5) - the factual alignment between the fact-checking
    explanation sentence and the evidence. The explanation should accurately reflect
    the evidence and its context, without misrepresenting or omitting crucial details.
    Annotators were instructed to penalize explanations that contain inaccuracies,
    misinterpretations, or fail to adequately represent the evidence provided. Below
    are the error typology that you need to utilize to determine faithfulness between
    the explanation and evidence: {error typology} Evaluation Steps: 1\. Read the
    fact-checking explanation sentence and the evidence provided carefully. 2\. Compare
    the explanation sentence to the evidence to identify how well it represents the
    facts, context, and conclusions drawn from the evidence using the error typology
    above. 3\. Assess how accurately and completely the explanation sentence reflects
    the evidence without distortion or significant omission. Assign a faithfulness
    score from 1 (unfaithful) to 5 (faithful). Evidence Provided: {evidence} Fact-Checking
    Explanation: {explanation} Evaluation Form: Faithfulness: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Prompt templates for sentence-level automatic evaluation. The texts
    in grey are only presented in the prompts when error typology is applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt for Feedback Generation Human: Give me the error types that the generated
    explanation can contain. LLM: Below are the error typology that you need to utilize
    to determine faithfulness between the explanation and evidence: {error typology}
    Human: Provide the claim, its corresponding label (true, false, or half-true),
    and the supporting evidence. LLM: Generate the initial explanation. Human: Find
    all errors (Intrinsic Entity-Related error, Extrinsic Entity-Related error, Intrinsic
    Event-Related error, Extrinsic Event-Related error, Intrinsic Noun-Phrase-Related
    error, Extrinsic Noun-Phrase-Related error, Reasonability-Related error, Connected
    evidence related error) in the "generated explanation" and provide the feedback
    by following the steps; Error count: how many errors have been found (what types
    of error); Step 1) Recognize what type(s) of error has been found in the generated
    explanation; Step 2) Recognize which sentence(s) contain(s) the error(s); Step
    3) Recognize what causes the error; Step 4) Why is the error; Step 5) How the
    error should be corrected; If there are multiple errors, please write 5 steps
    for each error. Prompt for Refinement Human: (Provide the feedback of two agents.)
    Please revise the generated explanation for the label on fact-checking using the
    given feedback without any modification other than feedback. (Provide the example
    of the refinement as guidance.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The prompt for the self-refinement approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt for Debater 1 in MADR Human: Give me the error types that the generated
    explanation can contain. LLM: Below are the error typology that you need to utilize
    to determine faithfulness between the explanation and evidence: {error typology}
    Human: Provide the claim, its corresponding label (true, false, or half-true),
    and the supporting evidence. LLM: Generate the initial explanation. Human: You
    are a professional analyzer who find potential errors, which might weaken faithfulness,
    in the generated explanation (not in the source) and categorize them according
    to predefined error types. Thoroughly comprehend the provided source and the task
    carefully. Your task: – Step 1: Find all potential errors, which might weaken
    faithfulness, in the generated explanation (not in the source) and provide exact
    senteces where the errors are found with quotation. – Step 2: Categorize them
    according to predefined error types above. – Step 3: Provide specific and actionable
    feedbacks with instruction how to fix them. Please provide only the feedback,
    not the revised explanation. Remember that explanation can contain multiple same
    errors. LLM: Generate the feedback. Your task: – Step 1: Take your whole previous
    feedback. – Step 2: Compare your previous feedback with feedback from another
    professional analyzer to check whether your previous feedback contains any wrong
    error or feedback. – Step 3: Find the errors or feedbacks that you think they
    are valid and should be added to your feedback from other’s feedbacks (errors
    must be found from the generated explanation not the feedback). – Step 4: Rewrite
    the feedback based from your previous feedback using the answers from the steps
    above. Do not add any extra words than feedback. Remember you should follow this
    rule: do not to copy feedback from other and provide what are errors, exact senteces
    where the errors are found with quotation, and feedbacks. These are feedbacks
    from another professional analyzer: {Feedback from Debater 2}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: The prompt for Debater 1 in MADR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt for Debater 2 in MADR Human: Give me the error types that the generated
    explanation can contain. LLM: Below are the error typology that you need to utilize
    to determine faithfulness between the explanation and evidence: {error typology}
    Human: Provide the claim, its corresponding label (true, false, or half-true),
    and the supporting evidence. LLM: Generate the initial explanation. Human: You
    are a professional analyzer who find errors, classified by predefined error types,
    in the generated explanation (not in the source) and provide feedback for correcting
    them. Thoroughly comprehend the provided source and the task carefully. Your task:
    – Step 1: Find all errors categorized by predefined error types, which undermine
    the faithfulness of the generated explanation (not in the source) and provide
    exact senteces where the errors are found with quotation. – Step 2: Provide specific
    and actionable feedbacks with instruction how to fix them. Please provide only
    the feedback, not the revised explanation. Remember that explanation can contain
    multiple same errors. LLM: Generate the feedback. Your task: – Step 1: Take your
    whole previous feedback. – Step 2: Compare your previous feedback with feedback
    from another professional analyzer to check whether your previous feedback contains
    any wrong error or feedback. – Step 3: Find the errors or feedbacks that you think
    they are valid and should be added to your feedback from other’s feedbacks (errors
    must be found from the generated explanation not the feedback). – Step 4: Rewrite
    the feedback based from your previous feedback using the answers from the steps
    above. Do not add any extra words than feedback. Remember you should follow this
    rule: do not to copy feedback from other and provide what are errors, exact senteces
    where the errors are found with quotation, and feedbacks. These are feedbacks
    from another professional analyzer: {Feedback from Debater 1}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: The prompt for Debater 2 in MADR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-Refine Input Prompt Claim (C): Says Jeff Foxworthy wrote a list of examples
    explaining how "you might live in a nation that was founded by geniuses but is
    run by idiots." Label (L): False Source (S): If you can get arrested for hunting
    or fishing without a license, but not for entering and remaining in the country
    illegally 2014 you might live in a country that was founded by geniuses but is
    run by idiots. That’s one of many examples in a list posted to Facebook on Nov.
    19 and falsely attributed to comedian Jeff Foxworthy. The post was flagged as
    part of Facebook’s efforts to combat false news and misinformation on its News
    Feed. (Read more about our partnership with Facebook.) Foxworthy is famous for
    his Ÿou might be a redneck if 2026 jokes , but searching online we couldn’t find
    any credible evidence that he penned this list that touches on abortion, Muslims
    and federal debt. Rather, we found users on web forums crediting someone named
    Fritz Edmunds with the list. Snopes, which fact-checked this claim back in 2013,
    also noted that the original compiler of this appears to be Fritz Edmunds, who
    posted it to his 2018 Politically True blog back in Feb. 3, 2013 (albeit with
    a disclaimer noting that 2018some of the ideas were from an email that did not
    contain a copyright). The link to that post is now dead but the url includes the
    words channeling Jeff Foxworthy. Initial Explanation The claim is labeled as false.
    The claim suggests that comedian Jeff Foxworthy wrote a list of examples explaining
    how "you might live in a nation that was founded by geniuses but is run by idiots."
    However, this claim is inaccurate and misleading. There is no credible evidence
    to suggest that Jeff Foxworthy wrote the specific list mentioned in the claim.
    … Feedback Error count: 1 (Intrinsic Entity-Related error) Step 1) Recognize what
    type(s) of error has been found in the generated explanation: Extrinsic Noun-Phrase-Related
    error Step 2) Recognize which sentence(s) contain(s) the error(s): The sentence
    containing the error is: "The claim suggests that comedian Jeff Foxworthy wrote
    a list of examples explaining how "you might live in a nation that was founded
    by geniuses but is run by idiots."" Step 3) Recognize what causes the error: The
    error occurs when the explanation incorrectly attributes the creation of the list
    to Jeff Foxworthy. Step 4) Why is the error: The error is due to the fact that
    the list was not authored by Jeff Foxworthy, but rather falsely attributed to
    him. Step 5) How the error should be corrected: The sentence should be revised
    to accurately reflect that the list was not written by Jeff Foxworthy. For example:
    "The claim falsely attributes a list of examples to comedian Jeff Foxworthy, suggesting
    that he wrote about how ’you might live in a nation that was founded by geniuses
    but is run by idiots.’ However, there is no evidence to support this claim." Revised
    Explanation The claim is labeled as false. The claim falsely attributes a list
    of examples to comedian Jeff Foxworthy, suggesting that he wrote about how ’you
    might live in a nation that was founded by geniuses but is run by idiots.’ However,
    there is no evidence to support this claim. …'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: An example output from Self-Refine. Intrinsic Entity-Related Error
    initially identified from the feedback is marked in blue. The suggested revisions
    from the feedback and the actual revisions made from the initial explanation are
    highlighted in yellow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Debate Input Prompt Claim (C): Says Jeff Foxworthy wrote a list of examples
    explaining how "you might live in a nation that was founded by geniuses but is
    run by idiots." Label (L): False Source (S): If you can get arrested for hunting
    or fishing without a license, but not for entering and remaining in the country
    illegally 2014 you might live in a country that was founded by geniuses but is
    run by idiots. That’s one of many examples in a list posted to Facebook on Nov.
    19 and falsely attributed to comedian Jeff Foxworthy. The post was flagged as
    part of Facebook’s efforts to combat false news and misinformation on its News
    Feed. (Read more about our partnership with Facebook.) Foxworthy is famous for
    his Ÿou might be a redneck if 2026 jokes , but searching online we couldn’t find
    any credible evidence that he penned this list that touches on abortion, Muslims
    and federal debt. Rather, we found users on web forums crediting someone named
    Fritz Edmunds with the list. Snopes, which fact-checked this claim back in 2013,
    also noted that the original compiler of this appears to be Fritz Edmunds, who
    posted it to his 2018 Politically True blog back in Feb. 3, 2013 (albeit with
    a disclaimer noting that 2018some of the ideas were from an email that did not
    contain a copyright). The link to that post is now dead but the url includes the
    words channeling Jeff Foxworthy. Initial Explanation The claim is labeled as false.
    The claim suggests that comedian Jeff Foxworthy wrote a list of examples explaining
    how "you might live in a nation that was founded by geniuses but is run by idiots."
    However, this claim is inaccurate and misleading. There is no credible evidence
    to suggest that Jeff Foxworthy wrote the specific list mentioned in the claim.
    … Feedback 1 $F_{0,1}$ The claim falsely suggests that Jeff Foxworthy wrote a
    list of examples explaining how ’you might live in a nation that was founded by
    geniuses but is run by idiots.’ However, as of now, there is no credible evidence
    to suggest that Jeff Foxworthy wrote the specific list mentioned in the claim.
    …'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: An example output from MADR. Extrinsic Event Error initially identified
    from Feedback 1 are marked in red, while Intrinsic Entity-Related Error initially
    identified from Feedback 2 are marked in blue. Both the suggested revisions from
    each feedback and the actual revisions made from the initial explanation are highlighted
    in yellow.'
  prefs: []
  type: TYPE_NORMAL
