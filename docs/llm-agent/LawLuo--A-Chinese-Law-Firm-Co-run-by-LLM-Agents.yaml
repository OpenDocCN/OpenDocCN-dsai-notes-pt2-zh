- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'LawLuo: A Chinese Law Firm Co-run by LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16252](https://ar5iv.labs.arxiv.org/html/2407.16252)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jingyun Sun¹, Chengxiao Dai², Zhongze Luo¹, Yangbo Chang³, Yang Li^(1,∗)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) demonstrate substantial potential in delivering
    legal consultation services to users without a legal background, attributed to
    their superior text comprehension and generation capabilities. Nonetheless, existing
    Chinese legal LLMs limit interaction to a single model-user dialogue, unlike the
    collaborative consultations typical of law firms, where multiple staff members
    contribute to a single consultation. This limitation prevents an authentic consultation
    experience. Additionally, extant Chinese legal LLMs suffer from critical limitations:
    (1) insufficient control over the quality of instruction fine-tuning data; (2)
    increased model hallucination resulting from users’ ambiguous queries; and (3)
    a reduction in the model’s ability to follow instructions over multiple dialogue
    turns. In response to these challenges, we propose a novel legal dialogue framework
    that leverages the collaborative capabilities of multiple LLM agents, termed LawLuo.
    This framework encompasses four agents: a receptionist, a lawyer, a secretary,
    and a boss, each responsible for different functionalities, collaboratively providing
    a comprehensive legal consultation to users. Additionally, we constructed two
    high-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned ChatGLM-3-6b
    using these datasets. We propose a legal query clarification algorithm called
    ToLC. Experimental results demonstrate that LawLuo outperforms baseline LLMs,
    including GPT-4, across three dimensions: lawyer-like language style, the usefulness
    of legal advice, and the accuracy of legal knowledge. Our code and datasets are
    available at [https://github.com/NEFUJing/LawLuo](https://github.com/NEFUJing/LawLuo)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the release of ChatGPT, the development of Chinese Large Language Models
    (LLMs) has advanced rapidly, resulting in the emergence of several influential
    base models, including ChatGLM (Du et al. [2022](#bib.bib2)), LLaMa (Touvron et al.
    [2023a](#bib.bib12)), and BaiChuan (Yang et al. [2023](#bib.bib24)). These models
    excel in fluent Chinese dialogue and comprehension of complex contexts and user
    intentions. Additionally, domain-specific Chinese LLMs such as Medical LLMs (Yang
    et al. [2024](#bib.bib25); Zhang et al. [2023a](#bib.bib27)), Legal LLMs (Zhou
    et al. [2024](#bib.bib31); Huang et al. [2023](#bib.bib5)), and Financial LLMs
    (Zhang and Yang [2023](#bib.bib29)) have emerged, showcasing exceptional domain-specific
    conversational abilities and meeting diverse user needs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Chinese Legal LLMs offer users without a legal background timely, accurate,
    and clear solutions to legal issues, effectively addressing the shortage of legal
    resources. Recently, notable Chinese Legal LLMs such as LawGPT (Zhou et al. [2024](#bib.bib31))
    and lawyer-llama (Huang et al. [2023](#bib.bib5)) have emerged. They leverage
    extensive Chinese legal dialogue datasets to fine-tune Chinese base large models,
    resulting in LLMs equipped with comprehensive Chinese legal knowledge and the
    capability to engage in legal consultation dialogues. Nevertheless, a standalone
    legal LLM fall short in simulate the Standard Operating Procedure (SOP) of real-world
    legal consultations, thus failing to deliver an authentic consulting experience
    to users. Additionally, existing LLMs encounter several issues: (1) Users consulting
    these models often lack a legal background, leading to vague and imprecise queries
    that exacerbate models’ tendency to produce hallucinations; (2) Despite utilizing
    large datasets for instruction fine-tuning, current legal LLMs do not sufficiently
    control for data quality; (3) Existing models employ single-turn legal dialogue
    data for fine-tuning, neglecting multi-turn dialogue data, which undermines models’
    ability to follow instructions after multi-turn conversation.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 中文法律LLM为没有法律背景的用户提供及时、准确、清晰的法律问题解决方案，有效解决了法律资源短缺的问题。最近，诸如LawGPT（Zhou et al.
    [2024](#bib.bib31)）和lawyer-llama（Huang et al. [2023](#bib.bib5)）等著名中文法律LLM相继出现。它们利用大量中文法律对话数据集来微调中文基础大模型，形成了具备全面中文法律知识并能够进行法律咨询对话的LLM。然而，单独的法律LLM在模拟现实法律咨询的标准操作程序（SOP）方面仍显不足，无法为用户提供真实的咨询体验。此外，现有LLM面临几个问题：（1）咨询这些模型的用户通常缺乏法律背景，导致模糊和不准确的查询，加剧了模型产生幻觉的倾向；（2）尽管利用大量数据集进行指令微调，但当前法律LLM未能充分控制数据质量；（3）现有模型使用单轮法律对话数据进行微调，忽视了多轮对话数据，削弱了模型在多轮对话后遵循指令的能力。
- en: '![Refer to caption](img/5907f0309c262a20fc1b514c0c3a4a8b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5907f0309c262a20fc1b514c0c3a4a8b.png)'
- en: 'Figure 1: To simulate a realistic legal consultation Standard Operating Procedure
    (SOP), we introduce the LawLuo framework, which encompasses four distinct agents:
    a receptionist, lawyers, the boss, and a secretary. Each of these agents is driven
    by language models: the receptionist by RoBERTa, the lawyers by our fine-tuned
    ChatGLM, the secretary by the base ChatGLM, and the boss by a reward model. These
    agents work in concert to provide a comprehensive and high-quality legal consultation
    experience.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：为了模拟现实法律咨询的标准操作程序（SOP），我们引入了LawLuo框架，该框架包含四个不同的角色：接待员、律师、老板和秘书。这些角色由语言模型驱动：接待员由RoBERTa驱动，律师由我们微调后的ChatGLM驱动，秘书由基础ChatGLM驱动，老板由奖励模型驱动。这些角色协同工作，提供全面而高质量的法律咨询体验。
- en: To address the aforementioned issues, we propose a novel legal dialogue framework,
    termed LawLuo, grounded in LLM multi-agents. Initially, we fine-tune the Chinese
    base LLM, ChatGLM-3-6b, leveraging our meticulously constructed high-quality legal
    dialogue dataset, KINLED, alongside the multi-turn legal consultation dataset,
    MURLED. Contrary to other models of employing extensive datasets, our findings
    indicate that fine-tuning with a smaller, high-quality legal dialogue dataset
    yields superior performance. Subsequently, we design multiple agents—receptionist,
    lawyer, secretary, and boss—based on the SOP of actual law firms. We design a
    collaborative framework wherein these agents synergistically handle a user’s legal
    consultation, thereby enhancing the user consultation experience. We further specialize
    the roles of lawyer agents via role enhancement (Wang et al. [2023b](#bib.bib17);
    Shao et al. [2023](#bib.bib9)), according to different consultation domains. For
    instance, a corporate lawyer is designated to engage with users seeking advice
    on corporate law, whereas a traffic accident lawyer is dedicated to consultations
    concerning traffic accidents. Moreover, to address users’ imprecise and ambiguous
    queries, we draw inspiration from the Tree of Clarification (ToC) (Kim et al.
    [2023](#bib.bib6)) and propose a novel Tree of Legal Clarification (ToLC) algorithm.
    ToLC employs a retrieve-generate-active choosing process to guide users in refining
    their legal queries.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment results demonstrate that LawLuo significantly outperforms the baselines
    across three key dimensions: lawyer-like language style, the utility of legal
    advice, and the accuracy of legal knowledge. Specifically, LawLuo exhibits a winning
    rate of 72% against ChatGLM-3-6b, the base LLM we utilized, which underscores
    the efficacy of our instruction fine-tuning. Remarkably, LawLuo achieves superior
    performance compared to other legal LLMs while utilizing fewer instruction fine-tuning
    data, emphasizing the critical role of data quality over quantity. Additionally,
    our experimental results indicate that LawLuo consistently generates high-quality
    responses even after multiple dialogue turns, highlighting the importance of using
    real multi-turn legal consultation data for instruction fine-tuning. Lastly, we
    confirmed the contribution of each component through a comprehensive ablation
    study.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our primary contributions are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a legal dialogue framework, LawLuo, based on the collaboration of
    multiple LLM agents.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We construct a high-quality dataset, KINLED, for legal LLM instruction fine-tuning.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop a real-world multi-turn legal consultation dataset, MURLED.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a algorithm for clarifying legal queries, ToLD.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Legal Question Answering. Legal Question Answering (LQA) is considered one
    of the most challenging tasks in legal artificial intelligence due to its flexible
    input-output requirements. In this task, users ask legal questions, and systems
    provide detailed answers that meet their expectations. Early LQA systems fall
    into three main categories: retrieval-based LQA systems (Yoshioka, Aoki, and Suzuki
    [2021](#bib.bib26)), knowledge-based LQA systems (Taniguchi and Kano [2017](#bib.bib10)),
    and machine reading comprehension-based LQA systems (Xiao et al. [2021](#bib.bib22)).
    Significant advancements in Chinese LQA have been driven by datasets like the
    Chinese Judicial Reading Comprehension dataset (CJRC) (Duan et al. [2019](#bib.bib3))
    and the Judicial Examination Chinese QA dataset (JEC-QA) (Zhong et al. [2020](#bib.bib30)).
    However, early methods struggle to personalize answers. Additionally, many legal
    consultations require multi-turn dialogues rather than a single question and answer
    exchange.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, applying LLM to LQA has shown great promise. Compared to traditional
    LQA models, LLMs generate personalized responses based on user queries and support
    multi-turn dialogues for more precise legal inquiry resolution. LawGPT (Zhou et al.
    [2024](#bib.bib31)) continuing pre-trains and instruction fine-tunes Chinese-LLaMa-7B
    (Touvron et al. [2023a](#bib.bib12)), a Chinese base model, with publicly available
    legal documents and judicial examination data, thereby enhancing its understanding
    and execution of legal content. LexiLaw¹¹1[https://github.com/CSHaitao/LexiLaw](https://github.com/CSHaitao/LexiLaw)
    fine-tunes ChatGLM-6B with data from Huazhi.com²²2[https://www.66law.cn/](https://www.66law.cn/),
    judicial examination data, and Q&A data with legal references, using Freeze, Lora,
    and P-Tuning-V2 techniques to boost training efficiency. LawGPT_zh fine-tunes
    ChatGLM-6B using existing legal datasets and scenario-based Q&A data with statutory
    references, while lawyer-llama (Huang et al. [2023](#bib.bib5)) uses judicial
    examination data to fine-tune Chinese-LLaMA-13, enhancing its application of legal
    knowledge to dialogue scenarios. However, these models only use a single LLM for
    legal consultations, unlike real-world law firms where multiple staff collaborate
    on a user’s consultation. This limitation fails to provide users with the most
    realistic and expected LQA experience. Additionally, while current legal LLMs
    use numerous fine-tuning datasets, their quality varies significantly, contradicting
    recent findings that in large model instruction fine-tuning, “less is more” (i.e.,
    data quality is more important than quantity) (Wei et al. [2023b](#bib.bib19);
    Xia et al. [2024](#bib.bib21)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Agent Collaboration. In LLM-based multi-agent systems, an agent is defined
    as an autonomous entity capable of perceiving, thinking, learning, making decisions,
    and interacting with other agents (Xi et al. [2023](#bib.bib20)). Research shows
    that breaking complex tasks into simpler subtasks and tackling these with agents
    that have diverse functions can significantly enhance the problem-solving capabilities
    of LLMs. (Wang et al. [2024](#bib.bib14)). Fundamentally, Retrieval-Augmented
    Generation (RAG) based on LLMs constitutes a straightforward multi-agent system
    wherein a retrieval agent is tasked with sourcing relevant knowledge from a vector
    database, while an LLM agent is responsible for generating responses based on
    the query and associated knowledge (Louis, van Dijck, and Spanakis [2024](#bib.bib7)).
    Furthermore, some works have developed more complex multi-agent collaborative
    systems to address intricate challenges. For instance, (Qian et al. [2023](#bib.bib8))
    designed a multi-agent collaborative workflow in which agents assuming roles such
    as CTO, programmer, designer, and tester work closely together to complete software
    development and document the development process. (Hemmer et al. [2022](#bib.bib4))
    have facilitated the construction of machine learning models through collaboration
    between multiple agents and humans. Additionally, LLM-based multi-agent systems
    can also be employed for realistic environment simulation. For example, (Wang
    et al. [2023a](#bib.bib15)) utilized multiple generative LLM agents in a sandbox
    environment to simulate consumer behavior in merchandise recommendation scenarios,
    (Wei et al. [2023a](#bib.bib18)) assigned various roles to agents for the collection
    and evaluation of multi-party dialogue data, and (Du et al. [2023](#bib.bib1))
    leveraged debates among agents with different personalities to enhance the factual
    accuracy of LLM reasoning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 3\. LawLuo Framework for LQA
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real-world scenarios, a consultation in law firms is conducted through the
    collaboration of multiple staff members, whereas existing legal LLMs generally
    engage with a user in isolation. To bridge this gap, we introduce a multi-agent
    collaborative legal dialogue framework, named LawLuo.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we perform LoRA fine-tuning on ChatGLM-3-6b using knowledge-intensive
    dialogue data and real multi-turn dialogue data from a Chinese law firm, resulting
    in a multi-turn dialogue LLM with a legal background, denoted as $LLM_{Legal}:(s_{0},U_{1:T})\mapsto
    R_{1:T}$ represents the initial dialogue state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Next, we design various agents with different roles, including a receptionist
    agent responsible for allocating lawyers from different fields based on user queries,
    lawyer agents for conducting multi-turn LQA with users, a secretary agent for
    organizing the dialogue between users and lawyers into consultation reports, and
    a boss agent supervising other agents. Finally, we construct a collaborative framework
    to guide interactions among these agents, thereby providing users with a high-quality
    LQA experience.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Instruction Fine-tuning
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing LLMs in the legal domain, despite leveraging a substantial volume
    of dialogue data during the instruction fine-tuning process, frequently contend
    with quality deficiencies. Figure [2](#Sx3.F2 "Figure 2 ‣ 3.1 Instruction Fine-tuning
    ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")
    presents the lawyer consultation data³³3[https://github.com/liuhuanyong/CrimeKgAssitant](https://github.com/liuhuanyong/CrimeKgAssitant)
    utilized by LawGPT_zh and the Baidu Zhidao Q&A data⁴⁴4[https://pan.baidu.com/s/18Lwq16VBo6wBD_qLb3i33g](https://pan.baidu.com/s/18Lwq16VBo6wBD_qLb3i33g)
    employed by LexiLaw. It is apparent that the questions’ clarity and the responses’
    professionalism in these dialogues are suboptimal, impairing models’ instruction
    follow capabilities in LQA.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/225108e775af39240c4f69eff6619207.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Examples of dialogue data used for instruction fine-tuning of existing
    Chinese legal LLMs. The upper section shows legal consultation data used by LawGPT_zh,
    while the lower section displays Q&A data from Baidu Zhidao used by LexiLaw. It
    is evident from these examples that the professionalism of both the questions
    and answers is relatively low.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent studies indicate that utilizing fewer but higher-quality data can significantly
    enhance the instruction-following capabilities of LLMs (Wei et al. [2023b](#bib.bib19);
    Xia et al. [2024](#bib.bib21)). Consequently, we developed a smaller but higher-quality
    dataset, termed Knowledge-INtensive LEgal Dialogue (KINLED) dataset. Firstly,
    we screened existing legal dialogue datasets, discarding lower-quality dialogues
    and retaining those of higher-quality. Moreover, to enhance the model’s application
    of critical legal terminologies, we constructed legal term and explanation dialogue
    data. Additionally, to improve the model’s understanding of significant charges
    and legal provisions, we created legal judgment dialogue and judicial interpretation
    dialogue. The KINLED dataset construction employed the self-instruct strategy,
    with specific construction details provided in Appendix A. Table [1](#Sx3.T1 "Table
    1 ‣ 3.1 Instruction Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese
    Law Firm Co-run by LLM Agents") presents the statistical information of the KINLED
    dataset.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '| Composition | No. Dialogue Entries |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| Legal Term Explanation Dialogue | 3,125 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Legal Judgment Dialogue | 533 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Judicial Interpretation Dialogue | 4,382 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| Scenario Q&A Based on Legal Grounds | 3,026 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Single-Turn Legal Dialogue | 995 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Judicial Examination Dialogue | 985 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistical information of the Knowledge-INtensive LEgal Dialogue
    (KINLED) dataset we constructed. The legal term and explanation dialogues, legal
    judgment dialogues, as well as judicial interpretation dialogues, were created
    by us, while the other dialogues were selected from existing works.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The existing legal LLMs have all been fine-tuned using single-turn dialogue
    data, which hinders their ability to follow instructions in multi-turn dialogue
    scenarios. To address this limitation, we constructed a fine-tuning dataset based
    on 3,260 anonymized multi-turn legal consultations from a law firm, termed MUltiple
    Rounds LEgal Dialogue (MURLED). Figure [3](#Sx3.F3 "Figure 3 ‣ 3.1 Instruction
    Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run
    by LLM Agents") presents an example of a multi-turn dialogue from MURLED, demonstrating
    the clarity of the questions and the professionalism of the responses.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc4f2d21ccb412893b8ab7ebacdd4095.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An example dialogue from the MUltiple Rounds LEgal Dialogue (MURLED)
    dataset. As can be seen, the user’s inquiries are clear, and the lawyer’s responses
    are professional.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'We use KINLED and MURLED to fine-tune ChatGLM-3-6b, a Chinese base LLM. To
    mitigate the risk of overfitting, we incorporated general conversational data
    from Alpaca-GPT4⁵⁵5[https://www.modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary),
    which comprises 52,000 generic Chinese dialogues, into the instruction fine-tuning
    process. To expedite the fine-tuning of the model and reduce reliance on computational
    resources, we employed the LoRA fine-tuning strategy, as illustrated in Equation
    [1](#Sx3.E1 "In 3.1 Instruction Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo:
    A Chinese Law Firm Co-run by LLM Agents"):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: where $\theta$ represents LoRA fine-tuning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Agent Definition
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the following agents based on the Standard Operating Procedures (SOP)
    of law firms: 1) Receptionist, responsible for assigning lawyers to users based
    on the queries they present. 2) Lawyers, engage in multiple rounds dialogue to
    resolve users’ legal issues. 3) Secretary, responsible for compiling the dialogues
    between users and lawyers into consultation reports, and then submitting them
    to both the users and the boss. 4) Boss, charged with evaluating the lawyers and
    the secretary. Subsequently, we will provide a detailed exposition of these agents.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Receptionist
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given the user’s initial question $u_{1}$, as shown in Equation [2](#Sx3.E2
    "In Receptionist ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo:
    A Chinese Law Firm Co-run by LLM Agents"):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Rec:(u_{1},s_{0})\mapsto d\in\mathcal{D}$ |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: 'where the set $\mathcal{D}$ represents our predefined consultation domains.
    We adhere to the classification of legal consultation domains as outlined on the
    HuLaw website⁶⁶6[https://www.66law.cn/](https://www.66law.cn/), encompassing a
    total of 16 domains, as depicted in Figure [4](#Sx3.F4 "Figure 4 ‣ Receptionist
    ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law
    Firm Co-run by LLM Agents"). Of these, 15 are common consultation fields, while
    other less common consultation areas are collectively categorized under “Others”.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/985979f56f59bd1abece45a457875e51.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Predefined legal consultation domains and statistical information'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a contrastive trained RoBERTa as the secretary agent. We first crawled
    35,060 legal questions with domain tags from HuaLaw web, with the number of questions
    in each domain illustrated in Figure [4](#Sx3.F4 "Figure 4 ‣ Receptionist ‣ 3.2
    Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run
    by LLM Agents"). Subsequently, we used the current question as the anchor, questions
    from the same domain as positive examples, and questions from different domains
    as negative examples. We optimized the semantic distance between samples using
    contrastive loss, as depicted in Equation [3](#Sx3.E3 "In Receptionist ‣ 3.2 Agent
    Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run
    by LLM Agents").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{n=1}^{N}\left[\left\&#124;\vec{a}_{i}-\vec{p}_{i}\right\&#124;^{2}+\max\left(0,\alpha-\left\&#124;\vec{a}_{i}-\vec{n}_{i}\right\&#124;\right)^{2}\right]$
    |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: where $\vec{a}_{i}$ denotes the Euclidean distance. We utilize Lawformer (Xiao
    et al. [2021](#bib.bib22)) as the embedding model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Lawyers
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We utilize the $LLM_{Legal}$, as shown in Equation [4](#Sx3.E4 "In Lawyers
    ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law
    Firm Co-run by LLM Agents")'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Lawyer_{d}\leftarrow RE_{d}(LLM_{Legal})$ |  | (4) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: where $RE_{d}(\cdot)$ represents the prompt function used for role enhancement.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, existing legal LLMs overlook a critical challenge in LQA: the
    legal questions posed by users are often rough and ambiguous. Directly inputting
    such questions into the LLM makes it difficult to obtain the desired answers.
    By contrast, in real-world practice, lawyers typically guide users to clarify
    their questions, resulting in clear and detailed inquiries. Therefore, we propose
    the Tree of Legal Clarifications (ToLC) algorithm, which builds on the Tree of
    Clarifications (ToC) proposed by (Kim et al. [2023](#bib.bib6)). The ToC uses
    top-K articles from Wikipedia to guide LLM in generating clarification questions.
    We adapted this by searching for top-K relevant cases from a legal case database
    to guide LLM in generating necessary legal clarifications. The ToC prunes unhelpful
    nodes through LLM’s self-verification, while we altered this process to involve
    active Yes/No answering by users. We believe users understand their own cases
    better than any language model or external knowledge. In other words, lawyers
    should guide users in clarifying legal facts, not fabricate the facts involved.
    Algorithm [1](#alg1 "Algorithm 1 ‣ Lawyers ‣ 3.2 Agent Definition ‣ 3\. LawLuo
    Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents") describes
    the implementation of ToLC.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Tree of Legal Clarifications (ToLC)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 1:User’s query in the $t$ Lawyer agent generates a response
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Secretary
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The duty of the secretary agent is organizing the dialogue between the user
    and the lawyer into a consultation report and then submitting it to both the user
    and the boss, as shown in Equation [5](#Sx3.E5 "In Secretary ‣ 3.2 Agent Definition
    ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents").'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Sec:(s_{0},U_{1:T},R_{1:T})\mapsto r$ |  | (5) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: where $r$ denotes the consultation report generated by the secretary agent.
    We utilize ChatGLM-3-6b as the secretary agent and employ In-Context Learning
    (ICL) to guide the model in producing consultation reports that meet expectations.
    We have constructed four standard consultation report samples, as shown in Appendix
    B, to serve as demonstrations in ICL reasoning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Boss
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The boss agent is responsible for evaluating the lawyer and secretary agents.
    Essentially, the boss agent is a Reward Model (RM). We first train a binary evaluation
    RM, $RM:o\mapsto y$, categorized into “better” and “worse”. The training objective
    of the RM is to minimize the following loss function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{RM}=-\frac{1}{N}\sum_{i=1}^{N}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\left.+(1-y_{i})\cdot\log\left(1-\hat{y}_{i}(o_{i};\theta_{RM})\right)\right]$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: where $y_{i}$ as “better.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: We employed the PPO algorithm (Wang, He, and Tan [2020](#bib.bib16)) during
    the reinforcement learning phase, an efficient reinforcement learning method that
    utilizes RM’s evaluation outcomes to guide the updates of the LLM. This approach
    further aligns the LLM’s performance with the preferences of human experts (the
    boss agent).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在强化学习阶段采用了PPO算法（Wang, He, 和 Tan [2020](#bib.bib16)），这是一种高效的强化学习方法，利用RM的评估结果来指导LLM的更新。这种方法进一步使LLM的性能与人类专家（老板代理）的偏好保持一致。
- en: 'Based on the definitions of the agents, we constructed a collaborative pipeline
    designed to imitate the SOP of legal consultations in real law firms, as illustrated
    in Figure [5](#Sx3.F5 "Figure 5 ‣ Boss ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework
    for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents"). The pipeline begins
    with the user initiating a consultation. Subsequently, the receptionist agent
    refers the user to an appropriate lawyer agent based on the user’s inquiry. The
    lawyer agent then engages in a dialogue with the user, utilizing the ToLC algorithm
    to clarify any ambiguous inquiries. Once the consultation concludes, the secretary
    agent compiling the conversation records between the user and the lawyer agent
    into a consultation report and submitting it to the user and the boss. The boss
    agent evaluate the responses generated by the lawyer agent and the reports generated
    by the secretary agent to optimize the their subsequent outputs.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '根据代理定义，我们构建了一个协作流程，旨在模拟实际律师事务所的标准操作程序（SOP），如图[5](#Sx3.F5 "Figure 5 ‣ Boss ‣
    3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents")所示。该流程开始于用户发起咨询。随后，接待员代理根据用户的咨询将用户转介给合适的律师代理。律师代理与用户进行对话，利用ToLC算法澄清任何模糊的咨询。一旦咨询结束，秘书代理将用户与律师代理之间的对话记录编制成咨询报告，并提交给用户和老板。老板代理评估律师代理生成的响应和秘书代理生成的报告，以优化其后续输出。'
- en: '![Refer to caption](img/14219b06d7be4df07c2805b2845a33d7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/14219b06d7be4df07c2805b2845a33d7.png)'
- en: 'Figure 5: Collaborative workflow of agents designed according to Standard Operating
    Procedures (SOP) of actual law firms'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：根据实际律师事务所的标准操作程序（SOP）设计的代理协作工作流程
- en: 4\. Experimental Setup
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验设置
- en: All our experiments were conducted on a 40G A100 GPU. The PyTorch 2.3.0 and
    the HuggingFace Transformers 4.40.0 were used. The learning rate for LoRA fine-tuning
    was set to 0.00005, with a training batch size of 2, over a total of 3 epochs,
    and model weights were saved every 1,000 steps. Additionally, the rank of LoRA
    was set to 16, the alpha parameter was set to 32, and the dropout rate was set
    to 0.05\. The case database used in the ToLC algorithm was sourced from the China
    Judgments Online website.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的实验均在40G A100 GPU上进行。使用了PyTorch 2.3.0和HuggingFace Transformers 4.40.0。LoRA微调的学习率设置为0.00005，训练批次大小为2，总共进行了3个epochs，模型权重每1,000步保存一次。此外，LoRA的秩设置为16，alpha参数设置为32，dropout率设置为0.05。ToLC算法中使用的案例数据库来源于中国裁判文书网。
- en: 5\. Results and Analysis
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结果与分析
- en: 5.1 Results on Single-turn Questions
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 单轮问题的结果
- en: 'We follow the current mainstream evaluation methods for large models to assess
    our LawLuo (Thirunavukarasu et al. [2023](#bib.bib11); Xiong et al. [2023](#bib.bib23);
    Zhang et al. [2023b](#bib.bib28)). We engaged both human experts and GPT-4o to
    evaluate the model’s performance based on the following criteria: lawyer-like
    language style, usefulness of legal advice, and accuracy of legal knowledge. We
    employed pairwise evaluation, where given a response from LawLuo and a baseline
    response, the better one is selected. Figure [6](#Sx5.F6 "Figure 6 ‣ 5.1 Results
    on Single-turn Questions ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents") shows the win rate of LawLuo compared to baselines. Overall,
    LawLuo significantly outperforms the baselines. Additionally, as seen in the figure,
    LawLuo achieved a 72% win rate against ChatGLM-3-6b, demonstrating the effectiveness
    of our instruction fine-tuning process, given that LawLuo was fine-tuned on ChatGLM-3-6b.
    Furthermore, the figure indicates that LawLuo outperforms the other two legal
    large models, LawGPT and LawyerLLaMa. Lastly, even when compared to GPT-4, LawLuo
    still exhibits a significant advantage.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59f836fc9fbdb142bb14c4e5169358d7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Win rate of LawLuo compared to the baselines'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Results on Multi-turn Dialogues
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section aims to analyze the advantages of LawLuo in multi-turn dialogue
    scenarios. We employed GPT-4o to rate the responses generated by the model in
    each turn of the dialogue, assessing the quality variation of the responses as
    the number of dialogue turns increased. We set a scoring range of 1-10, using
    the criteria of lawyer-like language style, the usefulness of legal advice, and
    the accuracy of legal knowledge as mentioned in Section 5.1\. The prompt driving
    GPT-4o’s scoring is provided in Appendix C. As shown in Figure [7](#Sx5.F7 "Figure
    7 ‣ 5.2 Results on Multi-turn Dialogues ‣ 5\. Results and Analysis ‣ LawLuo: A
    Chinese Law Firm Co-run by LLM Agents"), LawLuo’s responses maintain a high score
    as the number of dialogue turns increases. This result is primarily attributed
    to our use of multi-turn dialogue data, enhanced by ChatGPT and sourced from actual
    law firms, for instruction fine-tuning, as opposed to other legal LLMs that only
    use single-turn dialogue data.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f154a9ec8a0c60f211039a68842e7c38.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The variation in the quality of model-generated responses with increasing
    dialogue turns'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Ablation Study
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section aims to validate the contributions of each component within the
    framework. We continue to use GPT-4o as the evaluator to assess the win rate of
    LawLuo over ChatGPT after ablation, as illustrated in Figure [8](#Sx5.F8 "Figure
    8 ‣ 5.3 Ablation Study ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents"). From the figure, it is evident that the win rate of LawLuo
    over ChatGPT decreases by 3% after ablating the receptionist agent and role enhancement.
    This result validates our hypothesis that legal LLMs should be assigned different
    domain-specific roles to provide more targeted answers based on the user’s consultation
    field. Additionally, the figure shows that the boss agent also contributes to
    LawLuo’s performance, as it can optimize the responses generated by the lawyer.
    Finally, we observe a significant decline in model performance after removing
    the ToLD module. This indicates that clarifying users’ vague and ambiguous queries
    is crucial for generating high-quality responses in legal question-answering.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '本节旨在验证框架内每个组件的贡献。我们继续使用GPT-4o作为评估者，以评估LawLuo在去除部分功能后的胜率，如图[8](#Sx5.F8 "Figure
    8 ‣ 5.3 Ablation Study ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents")所示。从图中可以看出，去除接待员代理和角色增强后，LawLuo相对于ChatGPT的胜率下降了3%。这一结果验证了我们的假设，即法律LLM应分配不同的领域特定角色，以根据用户咨询领域提供更有针对性的答案。此外，图中还显示，老板代理也对LawLuo的表现有所贡献，因为它可以优化律师生成的回应。最后，我们观察到在移除ToLD模块后模型性能显著下降。这表明澄清用户模糊和含糊的问题对生成高质量的法律问答至关重要。'
- en: '![Refer to caption](img/ad6e30cec051f8dff16b7d93deb8638d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad6e30cec051f8dff16b7d93deb8638d.png)'
- en: 'Figure 8: Results of Ablation Experiments'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：消融实验结果
- en: 5.4 Case Study of ToLD
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 ToLD案例研究
- en: 'This section elucidates the contributions of ToLD through a specific case study.
    We take the query “ 我要离婚，该怎么办？(I want a divorce, what should I do?)” as an example
    initiated by a user. The left side of Figure 9 displays the clarifying questions
    generated by the ToLD algorithm. The results after the user actively marked Yes/No
    are shown on the right side of Figure [9](#Sx5.F9 "Figure 9 ‣ 5.4 Case Study of
    ToLD ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '本节通过一个具体案例研究阐明了ToLD的贡献。我们以用户发起的查询“我要离婚，该怎么办？”为例。图9的左侧展示了ToLD算法生成的澄清问题。用户主动标记“是/否”后的结果显示在图[9](#Sx5.F9
    "Figure 9 ‣ 5.4 Case Study of ToLD ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese
    Law Firm Co-run by LLM Agents")的右侧。'
- en: '![Refer to caption](img/119ec07bab5a722511fb38b061ccd08e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/119ec07bab5a722511fb38b061ccd08e.png)'
- en: 'Figure 9: Subfigure (a) presents the clarification tree generated by the ToLD
    algorithm, while subfigure (b) depicts the outcome after the user has marked the
    nodes on the tree with Yes/No.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：子图（a）展示了ToLD算法生成的澄清树，子图（b）描绘了用户标记了树上节点的“是/否”后的结果。
- en: 'Additionally, the first row of Table [2](#A1.T2 "Table 2 ‣ D. Case Study of
    ToLC ‣ Appendix A Appendix ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")
    in Appendix D presents the answer generated by LawLuo without ToLD, while the
    second row shows the answer generated by LawLuo with ToLD. It can be observed
    that ToLD assists legal LLMs in generating more accurate and personalized responses.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，附录D中表[2](#A1.T2 "Table 2 ‣ D. Case Study of ToLC ‣ Appendix A Appendix ‣
    LawLuo: A Chinese Law Firm Co-run by LLM Agents")的第一行展示了LawLuo在没有ToLD的情况下生成的回答，而第二行展示了LawLuo在有ToLD的情况下生成的回答。可以观察到，ToLD帮助法律LLM生成更准确和个性化的回答。'
- en: 6\. Discussion
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论
- en: Enlightenment
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启示
- en: This study has provided us with several intriguing insights. Firstly, we discovered
    that omitting continue pre-training and directly conducting instruction fine-tuning
    does not impact the effectiveness of the legal LLM. This indicates that the Chinese
    base model has already acquired a certain degree of legal knowledge during the
    pre-training phase, obviating the need for additional pre-training with legal
    corpora. In fact, the pre-training corpora used by many Chinese base models are
    extraordinarily large and likely encompass a substantial amount of legal text.
    We also found that the instruction fine-tuning phase does not require tens of
    thousands or even hundreds of thousands of dialogue data; approximately thousands
    high-quality legal dialogue data are sufficient to elicit the model’s legal knowledge
    application capabilities in dialogue scenarios. This paves a new path for the
    future fine-tuning of legal LLMs. Lastly, the effectiveness of role enhancement
    and multi-agent collaboration in our method validated our hypothesis that a legal
    consultation should not be a mere conversational process but a complex task involving
    multiple business processes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究为我们提供了几个有趣的见解。首先，我们发现省略继续预训练，直接进行指令微调不会影响法律LLM的效果。这表明中文基础模型在预训练阶段已经获得了一定的法律知识，因此无需使用法律语料库进行额外的预训练。实际上，许多中文基础模型使用的预训练语料库非常庞大，可能涵盖了大量的法律文本。我们还发现，指令微调阶段不需要数万甚至数十万条对话数据；大约几千条高质量法律对话数据就足以引发模型在对话场景中的法律知识应用能力。这为未来法律LLMs的微调开辟了一条新路。最后，我们方法中角色增强和多智能体协作的有效性验证了我们的假设，即法律咨询不应仅仅是一个对话过程，而是一个涉及多个业务流程的复杂任务。
- en: Limitation and Future Work
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局限性和未来工作
- en: Although we observed that the receptionist agent achieved a question classification
    accuracy rate exceeding 98% in our experiments, any misclassification leads to
    the lawyer agent continuing the interaction based on the initial error (a matrimonial
    lawyer could handle legal inquiries related to traffic accidents, albeit with
    reduced effectiveness). In the future, we plan to design a dynamic receptionist
    mechanism that reallocates the user to the appropriate lawyer based on conversation
    content whenever a domain mismatch is detected during the interaction with the
    legal LLM.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们观察到接待员智能体在实验中达到了超过98%的问题分类准确率，但任何错误分类都会导致律师智能体继续基于初始错误进行互动（例如婚姻律师可能处理与交通事故相关的法律咨询，但效果会有所下降）。未来，我们计划设计一个动态接待机制，根据对话内容在检测到领域不匹配时，将用户重新分配给合适的律师。
- en: Despite introducing the concept of multi-agent collaboration, not every agent
    is an LLM, resulting in limited thinking and decision-making capabilities. In
    the future, we intend to develop a collaborative framework in which all agents
    are LLMs, thereby enhancing the overall framework’s cognitive and decision-making
    abilities.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管引入了多智能体协作的概念，但并不是每个智能体都是LLM，这导致了思维和决策能力的局限性。未来，我们计划开发一个所有智能体都是LLM的协作框架，从而提升整体框架的认知和决策能力。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'We propose a multi-agent collaboration framework for legal dialogue, termed
    LawLuo. Experimental results demonstrate that LawLuo outperforms baseline LLMs
    in three dimensions: lawyer-like language style, the usefulness of legal advice,
    and the accuracy of legal knowledge. Moreover, it continues to produce high-quality
    answer even after multiple rounds of dialogue. We contribute two high-quality
    datasets for legal LLM instruction fine-tuning, KINLED and MURLED. Experimental
    results indicate that using these datasets can fine-tune more effective legal
    LLMs.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个用于法律对话的多智能体协作框架，称为LawLuo。实验结果表明，LawLuo在三个维度上优于基线LLMs：类似律师的语言风格、法律建议的有用性以及法律知识的准确性。此外，它在多轮对话后仍能产生高质量的答案。我们贡献了两个用于法律LLM指令微调的高质量数据集：KINLED和MURLED。实验结果表明，使用这些数据集可以微调出更有效的法律LLMs。
- en: Acknowledgments
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by “the Fundamental Research Funds for the Central Universities,
    xxxx”
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了“中央高校基本科研业务费，xxxx”的支持。
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Du et al. (2023) Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch,
    I. 2023. Improving factuality and reasoning in language models through multiagent
    debate. *arXiv preprint arXiv:2305.14325*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
    Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank
    Infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 320–335.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2019) Duan, X.; Wang, B.; Wang, Z.; Ma, W.; Cui, Y.; Wu, D.; Wang,
    S.; Liu, T.; Huo, T.; Hu, Z.; et al. 2019. Cjrc: A reliable human-annotated benchmark
    dataset for chinese judicial reading comprehension. In *Chinese Computational
    Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October
    18–20, 2019, Proceedings 18*, 439–451\. Springer.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hemmer et al. (2022) Hemmer, P.; Schellhammer, S.; Vössing, M.; Jakubik, J.;
    and Satzger, G. 2022. Forming Effective Human-AI Teams: Building Machine Learning
    Models that Complement the Capabilities of Multiple Experts. In *Proceedings of
    the Thirty-First International Joint Conference on Artificial Intelligence*, 2478.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Huang, Q.; Tao, M.; Zhang, C.; An, Z.; Jiang, C.; Chen,
    Z.; Wu, Z.; and Feng, Y. 2023. Lawyer llama technical report. *arXiv preprint
    arXiv:2305.15062*.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Kim, G.; Kim, S.; Jeon, B.; Park, J.; and Kang, J. 2023.
    Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented
    Large Language Models. In *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, 996–1009.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Louis, van Dijck, and Spanakis (2024) Louis, A.; van Dijck, G.; and Spanakis,
    G. 2024. Interpretable long-form legal question answering with retrieval-augmented
    large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, 22266–22275.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. (2023) Qian, C.; Cong, X.; Yang, C.; Chen, W.; Su, Y.; Xu, J.; Liu,
    Z.; and Sun, M. 2023. Communicative agents for software development. *arXiv preprint
    arXiv:2307.07924*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2023) Shao, Y.; Li, L.; Dai, J.; and Qiu, X. 2023. Character-LLM:
    A Trainable Agent for Role-Playing. In *Proceedings of the 2023 Conference on
    Empirical Methods in Natural Language Processing*, 13153–13187.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taniguchi and Kano (2017) Taniguchi, R.; and Kano, Y. 2017. Legal yes/no question
    answering system using case-role analysis. In *New Frontiers in Artificial Intelligence:
    JSAI-isAI 2016 Workshops, LENLS, HAT-MASH, AI-Biz, JURISIN and SKL, Kanagawa,
    Japan, November 14-16, 2016, Revised Selected Papers*, 284–298\. Springer.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thirunavukarasu et al. (2023) Thirunavukarasu, A. J.; Ting, D. S. J.; Elangovan,
    K.; Gutierrez, L.; Tan, T. F.; and Ting, D. S. W. 2023. Large language models
    in medicine. *Nature medicine*, 29(8): 1930–1940.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023a.
    Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b.
    Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang,
    J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024. A survey on large language
    model based autonomous agents. *Frontiers of Computer Science*, 18(6): 186345.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Wang, L.; Zhang, J.; Chen, X.; Lin, Y.; Song, R.; Zhao,
    W. X.; and Wen, J.-R. 2023a. Recagent: A novel simulation paradigm for recommender
    systems. *arXiv preprint arXiv:2306.02552*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang, He, and Tan (2020) Wang, Y.; He, H.; and Tan, X. 2020. Truly proximal
    policy optimization. In *Uncertainty in artificial intelligence*, 113–122\. PMLR.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Wang, Z. M.; Peng, Z.; Que, H.; Liu, J.; Zhou, W.; Wu,
    Y.; Guo, H.; Gan, R.; Ni, Z.; Zhang, M.; et al. 2023b. Rolellm: Benchmarking,
    eliciting, and enhancing role-playing abilities of large language models. *arXiv
    preprint arXiv:2310.00746*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023a) Wei, J.; Shuster, K.; Szlam, A.; Weston, J.; Urbanek, J.;
    and Komeili, M. 2023a. Multi-party chat: Conversational agents in group settings
    with humans and models. *arXiv preprint arXiv:2304.13835*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023b) Wei, L.; Jiang, Z.; Huang, W.; and Sun, L. 2023b. Instructiongpt-4:
    A 200-instruction paradigm for fine-tuning minigpt-4. *arXiv preprint arXiv:2308.12067*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Xi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y.; Hong, B.; Zhang,
    M.; Wang, J.; Jin, S.; Zhou, E.; et al. 2023. The rise and potential of large
    language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2024) Xia, M.; Malladi, S.; Gururangan, S.; Arora, S.; and Chen,
    D. 2024. Less: Selecting influential data for targeted instruction tuning. *arXiv
    preprint arXiv:2402.04333*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2021) Xiao, C.; Hu, X.; Liu, Z.; Tu, C.; and Sun, M. 2021. Lawformer:
    A pre-trained language model for chinese legal long documents. *AI Open*, 2: 79–84.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2023) Xiong, H.; Wang, S.; Zhu, Y.; Zhao, Z.; Liu, Y.; Huang,
    L.; Wang, Q.; and Shen, D. 2023. Doctorglm: Fine-tuning your chinese doctor is
    not a herculean task. *arXiv preprint arXiv:2304.01097*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin,
    C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; et al. 2023. Baichuan 2: Open large-scale
    language models. *arXiv preprint arXiv:2309.10305*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024) Yang, S.; Zhao, H.; Zhu, S.; Zhou, G.; Xu, H.; Jia, Y.;
    and Zan, H. 2024. Zhongjing: Enhancing the chinese medical capabilities of large
    language model through expert feedback and real-world multi-turn dialogue. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, 19368–19376.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoshioka, Aoki, and Suzuki (2021) Yoshioka, M.; Aoki, Y.; and Suzuki, Y. 2021.
    Bert-based ensemble methods with data augmentation for legal textual entailment
    in coliee statute law task. In *Proceedings of the eighteenth international conference
    on artificial intelligence and law*, 278–284.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Zhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Chen,
    G.; Li, J.; Wu, X.; Zhiyi, Z.; Xiao, Q.; et al. 2023a. HuatuoGPT, Towards Taming
    Language Model to Be a Doctor. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, 10859–10885.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Zhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Chen,
    G.; Li, J.; Wu, X.; Zhiyi, Z.; Xiao, Q.; et al. 2023b. HuatuoGPT, Towards Taming
    Language Model to Be a Doctor. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, 10859–10885.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Yang (2023) Zhang, X.; and Yang, Q. 2023. Xuanyuan 2.0: A large chinese
    financial chat model with hundreds of billions parameters. In *Proceedings of
    the 32nd ACM international conference on information and knowledge management*,
    4435–4439.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2020) Zhong, H.; Xiao, C.; Tu, C.; Zhang, T.; Liu, Z.; and Sun,
    M. 2020. JEC-QA: a legal-domain question answering dataset. In *Proceedings of
    the AAAI conference on artificial intelligence*, volume 34, 9701–9708.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2024) Zhou, Z.; Shi, J.-X.; Song, P.-X.; Yang, X.-W.; Jin, Y.-X.;
    Guo, L.-Z.; and Li, Y.-F. 2024. LawGPT: A Chinese Legal Knowledge-Enhanced Large
    Language Model. *arXiv preprint arXiv:2406.04614*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A. Construction of the KINLED Dataset
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The KINLED dataset consists of six parts: legal term and explanation dialogues,
    legal judgment dialogues, judicial interpretation dialogues, scenario-based question-and-answer
    dialogues with legal grounds, single-round legal consultation dialogues and judicial
    examination dialogues.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: legal Term and Explanation Dialogue
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first obtained 625 legal terms and their corresponding explanations from
    the Chinese Legal Terminology Compilation website, covering various legal fields
    such as civil law, criminal law, and jurisprudence. Subsequently, we called the
    official API of Moonshot AI, using the moonshot-v1-8k model to generate scenario
    dialogues related to these legal term explanations. To ensure the diversity of
    the dialogue data, we generated five different contextual question-and-answer
    pairs for each legal term and its corresponding explanation. The prompts we designed
    is as shown in Figure [10](#A1.F10 "Figure 10 ‣ legal Term and Explanation Dialogue
    ‣ A. Construction of the KINLED Dataset ‣ Appendix A Appendix ‣ LawLuo: A Chinese
    Law Firm Co-run by LLM Agents"):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7bf80e68374ec484a270844e30518d6f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7bf80e68374ec484a270844e30518d6f.png)'
- en: 'Figure 10: Prompt template to guide the moonshot-v1-8k model in generating
    legal term and explanation dialogues'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：用于指导moonshot-v1-8k模型生成法律术语和解释对话的提示模板
- en: Legal Judgment Dialogue
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法律判断对话
- en: 'We obtained 200 representative legal judgments from the China Judgments Online
    database and performed data cleaning. These documents cover three different types
    of cases: civil, criminal, and administrative. Subsequently, we utilized the official
    API of Moonshot AI, employing the moonshot-v1-128k model to generate question-answer
    dialogues based on these 200 judgment documents. After human expert review, we
    produced 533 high-quality question-answer dialogues. To ensure diversity in the
    dialogues, multiple sets of question-answer pairs were generated for each judgment
    document. The prompts we designed is as shown in Figure [11](#A1.F11 "Figure 11
    ‣ Legal Judgment Dialogue ‣ A. Construction of the KINLED Dataset ‣ Appendix A
    Appendix ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents"):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从中国裁判文书网获得了200个代表性的法律判决，并进行了数据清洗。这些文件涵盖了三种不同类型的案件：民事、刑事和行政。随后，我们利用Moonshot
    AI的官方API，使用moonshot-v1-128k模型基于这200个判决文件生成问答对话。经过人工专家审核，我们生成了533个高质量问答对话。为了确保对话的多样性，为每个判决文件生成了多组问答对。我们设计的提示模板如图[11](#A1.F11
    "图 11 ‣ 法律判断对话 ‣ A. KINLED 数据集构建 ‣ 附录A 附录 ‣ LawLuo: 一个由LLM代理共同运营的中国律师事务所")所示：'
- en: '![Refer to caption](img/06c6c5f3a16806a7ef8fe5e797e85c9b.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/06c6c5f3a16806a7ef8fe5e797e85c9b.png)'
- en: 'Figure 11: Prompt template to guide the moonshot-v1-8k model in generating
    legal judgment dialogues'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用于指导moonshot-v1-8k模型生成法律判断对话的提示模板
- en: Judicial Interpretation dialogue
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 司法解释对话
- en: We obtained 1,000 judicial interpretations from the official website of the
    Supreme People’s Court of China, covering various laws and categories. Utilizing
    the official API of Moonshot AI, we employed the moonshot-v1-128k large language
    model to generate and refine 4,382 high-quality judicial interpretation dialogues
    based on these interpretations, with human experts conducting the screening. To
    ensure the diversity and professionalism of the question-answer data, multiple
    sets of question-answer dialogues were constructed for each judicial interpretation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中国最高人民法院官方网站获取了1,000条司法解释，涵盖了各种法律和类别。利用Moonshot AI的官方API，我们使用moonshot-v1-128k大语言模型基于这些解释生成和优化了4,382条高质量的司法解释对话，由人工专家进行筛选。为了确保问答数据的多样性和专业性，为每个司法解释构建了多组问答对话。
- en: Scenario-based Q&A Dialogue with Legal Grounds
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于场景的问答对话带有法律依据
- en: The scenario-based Q&A data with legal references was generated by (Zhou et al.
    [2024](#bib.bib31)) using ChatGPT, based on the most essential 9,000 legal provisions
    from the Chinese Legal Handbook. The original dataset comprised 92,000 dialogue
    samples. We invited human experts to manually screen these samples, resulting
    in a final collection of 3,026 high-quality scenario-based Q&A pairs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基于场景的问答数据带有法律参考，由（周等人 [2024](#bib.bib31)）利用ChatGPT生成，基于《中国法律手册》中的最核心的9,000条法律条款。原始数据集包含92,000个对话样本。我们邀请了人工专家对这些样本进行人工筛选，最终收集了3,026对高质量的基于场景的问答对。
- en: Single-round Legal Consultation Dialogue
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单轮法律咨询对话
- en: The single single-round legal consultation dialogues are derived from real legal
    consultation questions and relevant legal provisions collected by (Huang et al.
    [2023](#bib.bib5)). These inputs were processed through GPT-4 to generate single-turn
    legal consultation dialogues, resulting in a total of 1,000 dialogue samples.
    We invited human experts to further refine these dialogues, ultimately yielding
    995 high-quality dialogue samples.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 单轮法律咨询对话来源于（黄等人 [2023](#bib.bib5)）收集的真实法律咨询问题和相关法律条款。这些输入经过GPT-4处理，生成了单轮法律咨询对话，共计1,000个对话样本。我们邀请了人工专家进一步优化这些对话，最终得到了995个高质量对话样本。
- en: Judicial Examination Dialogue
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 司法考试对话
- en: The judicial examination dialogue data was constructed by (Huang et al., 2023)
    based on the Chinese judicial examination dataset JEC-QA. They input the questions
    and answers from JEC-QA into ChatGPT, allowing the model to generate detailed
    explanations for each answer, ultimately resulting in 1,000 dialogue samples.
    We invited human experts to screen these dialogue data, and 985 high-quality dialogue
    samples were obtained.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 法律考试对话数据由（黄等，2023）基于中国司法考试数据集JEC-QA构建。他们将JEC-QA中的问题和答案输入ChatGPT，允许模型为每个答案生成详细解释，最终得到1,000个对话样本。我们邀请了人工专家对这些对话数据进行筛选，获得了985个高质量对话样本。
- en: B. Sample Consulting Report
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B. 咨询报告样本
- en: 'Figure [12](#A1.F12 "Figure 12 ‣ B. Sample Consulting Report ‣ Appendix A Appendix
    ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents") is a sample consultation report,
    serving as a demonstration for ICL. This consultation report comprises nine sections:
    report number, consultation date, client, subject of consultation, purpose of
    consultation, facts and background, legal analysis, legal advice, and risk warnings.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '图[12](#A1.F12 "图12 ‣ B. 咨询报告样本 ‣ 附录A 附录 ‣ LawLuo: 由LLM代理共同经营的中国律师事务所") 是一个咨询报告样本，作为ICL的示范。该咨询报告包含九个部分：报告编号、咨询日期、客户、咨询主题、咨询目的、事实与背景、法律分析、法律建议和风险警示。'
- en: '![Refer to caption](img/25a4859de7757fa92c8e1b0f553c83d5.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/25a4859de7757fa92c8e1b0f553c83d5.png)'
- en: 'Figure 12: A sample consulting report for in-context learning'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：一个关于情境学习的咨询报告样本
- en: C. Prompt Template for Guiding GPT-4o in Scoring
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C. 指导GPT-4o评分的提示模板
- en: 'Figure [13](#A1.F13 "Figure 13 ‣ C. Prompt Template for Guiding GPT-4o in Scoring
    ‣ Appendix A Appendix ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents") is a
    prompt template for guiding GPT-4o in scoring.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '图[13](#A1.F13 "图13 ‣ C. 指导GPT-4o评分的提示模板 ‣ 附录A 附录 ‣ LawLuo: 由LLM代理共同经营的中国律师事务所")
    是一个指导GPT-4o评分的提示模板。'
- en: '![Refer to caption](img/7f69efb8a364ebc49330a3240aee310d.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7f69efb8a364ebc49330a3240aee310d.png)'
- en: 'Figure 13: Prompt template for guiding GPT-4o in scoring'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：指导GPT-4o评分的提示模板
- en: D. Case Study of ToLC
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D. ToLC案例研究
- en: '| w/ ToLD | 首先，收集对方出轨的证据，这可能包括通信记录、照片或证人证言等。接着，对你们的共同财务状况进行彻底审计，明确债务的性质、金额和责任分配。尝试与对方协商离婚条件，包括财产分割和债务处理，如果协商失败，准备必要的法律文件，如离婚申请书和财产清单，并向法院提交离婚申请。如果对方不同意离婚，法院可能会介入调解，调解失败则进入正式的法庭程序。在法庭审理中，法官会根据法律规定和你们的具体情况，决定财产和债务的分割方式。一旦法院作出离婚判决，遵守判决内容，处理财产分割和债务偿还等后续事宜。在整个过程中，保持冷静和理智，避免情绪化决策，并确保在专业法律人士的帮助下保护自己的权益。
    (First, gather evidence of the partner’s infidelity, which may include communication
    records, photos, or witness testimonies. Then, conduct a thorough audit of your
    joint financial situation to clarify the nature, amount, and allocation of debts.
    Attempt to negotiate divorce terms with the partner, including property division
    and debt handling. If negotiations fail, prepare the necessary legal documents,
    such as the divorce petition and property inventory, and file for divorce in court.
    If the partner disagrees with the divorce, the court may intervene for mediation.
    If mediation fails, the case will proceed to a formal court hearing. During the
    trial, the judge will decide the division of property and debts based on legal
    provisions and your specific circumstances. Once the court issues a divorce decree,
    comply with the decree’s contents and handle subsequent matters such as property
    division and debt repayment. Throughout the process, remain calm and rational,
    avoid emotional decisions, and ensure the protection of your rights with the help
    of professional legal experts.) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| w/ ToLD | 首先，收集对方出轨的证据，这可能包括通信记录、照片或证人证言等。接着，对你们的共同财务状况进行彻底审计，明确债务的性质、金额和责任分配。尝试与对方协商离婚条件，包括财产分割和债务处理。如果协商失败，准备必要的法律文件，如离婚申请书和财产清单，并向法院提交离婚申请。如果对方不同意离婚，法院可能会介入调解，调解失败则进入正式的法庭程序。在法庭审理中，法官会根据法律规定和你们的具体情况，决定财产和债务的分割方式。一旦法院作出离婚判决，遵守判决内容，处理财产分割和债务偿还等后续事宜。在整个过程中，保持冷静和理智，避免情绪化决策，并确保在专业法律人士的帮助下保护自己的权益。'
- en: '| w/o ToLD | 离婚是一个重要的决定，涉及到情感、法律和财务等多个方面。以下是一些基本步骤，但请注意，具体情况可能因地区和个人情况而异，最好咨询专业的法律顾问：沟通与协商：首先尝试与配偶进行沟通，看是否能够就离婚达成共识，包括财产分割、子女抚养等问题。咨询律师：在正式提出离婚前，咨询专业的离婚律师，了解你所在地区的具体法律规定和程序。准备文件：根据律师的建议，准备必要的文件，如离婚协议书、财产清单等。提交申请：向法院或相关部门提交离婚申请，并按照要求提供相应的证据和文件。等待审理：法院将对离婚申请进行审理，可能包括调解程序。财产分割和子女抚养：根据法院的判决或双方的协议，进行财产分割和确定子女的抚养权。离婚证明：一旦离婚判决生效，你将获得离婚证明。后续调整：离婚后可能需要进行一些后续的调整，如更改个人文件、处理财务事务等。(Getting
    a divorce is an important decision involving multiple aspects such as emotions,
    legalities, and finances. The following are some basic steps, but please note
    that the specifics may vary depending on the region and individual circumstances.
    It is best to consult a professional legal advisor: 1\. Communication and negotiation:
    First, try to communicate with your spouse to see if a consensus on divorce can
    be reached, including issues like property division and child custody. 2\. Consult
    a lawyer: Before formally filing for divorce, consult a professional divorce lawyer
    to understand the specific legal regulations and procedures in your area. 3\.
    Prepare documents: According to the lawyer’s advice, prepare the necessary documents,
    such as a divorce agreement and a property list. 4\. Submit the application: Submit
    the divorce application to the court or relevant department, and provide the required
    evidence and documents as requested. 5\. Wait for the hearing: The court will
    review the divorce application, which may include mediation procedures. 6\. Property
    division and child custody: Based on the court’s judgment or the agreement between
    the parties, proceed with the property division and determine child custody. 7\.
    Divorce certificate: Once the divorce decree takes effect, you will receive a
    divorce certificate. 8\. Post-divorce adjustments: After the divorce, some follow-up
    adjustments may be needed, such as updating personal documents and handling financial
    matters.) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| w/o ToLD | 离婚是一个重要的决定，涉及到情感、法律和财务等多个方面。以下是一些基本步骤，但请注意，具体情况可能因地区和个人情况而异，最好咨询专业的法律顾问：沟通与协商：首先尝试与配偶进行沟通，看是否能够就离婚达成共识，包括财产分割、子女抚养等问题。咨询律师：在正式提出离婚前，咨询专业的离婚律师，了解你所在地区的具体法律规定和程序。准备文件：根据律师的建议，准备必要的文件，如离婚协议书、财产清单等。提交申请：向法院或相关部门提交离婚申请，并按照要求提供相应的证据和文件。等待审理：法院将对离婚申请进行审理，可能包括调解程序。财产分割和子女抚养：根据法院的判决或双方的协议，进行财产分割和确定子女的抚养权。离婚证明：一旦离婚判决生效，你将获得离婚证明。后续调整：离婚后可能需要进行一些后续的调整，如更改个人文件、处理财务事务等。'
- en: 'Table 2: The first row presents the answer generated by LawLuo with (w/) ToLD,
    while the second row presents the answer generated by LawLuo without (w/o) ToLD.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：第一行展示了 LawLuo 使用 (w/) ToLD 生成的回答，而第二行展示了 LawLuo 不使用 (w/o) ToLD 生成的回答。
