- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LLM-Based Cooperative Agents using Information Relevance and Plan Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16751](https://ar5iv.labs.arxiv.org/html/2405.16751)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SeungWon Seo
  prefs: []
  type: TYPE_NORMAL
- en: Department of Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Kyung Hee University
  prefs: []
  type: TYPE_NORMAL
- en: Yongin, South Korea
  prefs: []
  type: TYPE_NORMAL
- en: ssw03270@khu.ac.kr
  prefs: []
  type: TYPE_NORMAL
- en: \AndJunhyeok Lee
  prefs: []
  type: TYPE_NORMAL
- en: Department of Software Convergence
  prefs: []
  type: TYPE_NORMAL
- en: Kyung Hee University
  prefs: []
  type: TYPE_NORMAL
- en: Yongin, South Korea
  prefs: []
  type: TYPE_NORMAL
- en: bluehyena123@khu.ac.kr
  prefs: []
  type: TYPE_NORMAL
- en: \AndSeongRae Noh
  prefs: []
  type: TYPE_NORMAL
- en: Department of Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Kyung Hee University
  prefs: []
  type: TYPE_NORMAL
- en: Yongin, South Korea
  prefs: []
  type: TYPE_NORMAL
- en: rhosunr99@khu.ac.kr
  prefs: []
  type: TYPE_NORMAL
- en: \AndHyeongYeop Kang
  prefs: []
  type: TYPE_NORMAL
- en: Department of Software Convergence
  prefs: []
  type: TYPE_NORMAL
- en: Kyung Hee University
  prefs: []
  type: TYPE_NORMAL
- en: Yongin, South Korea
  prefs: []
  type: TYPE_NORMAL
- en: 'siamiz@khu.ac.kr Corresponding author: siamiz@khu.ac.kr'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We address the challenge of multi-agent cooperation, where agents achieve a
    common goal by interacting with a 3D scene and cooperating with decentralized
    agents under complex partial observations. This involves managing communication
    costs and optimizing interaction trajectories in dynamic environments. Our research
    focuses on three primary limitations of existing cooperative agent systems. Firstly,
    current systems demonstrate inefficiency in managing acquired information through
    observation, resulting in declining planning performance as the environment becomes
    more complex with additional objects or goals. Secondly, the neglect of false
    plans in partially observable settings leads to suboptimal cooperative performance,
    as agents struggle to adapt to environmental changes influenced by the unseen
    actions of other agents. Lastly, the failure to incorporate spatial data into
    decision-making processes restricts the agent’s ability to construct optimized
    trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced
    Cooperative Language Agent (REVECA), a novel cognitive architecture powered by
    GPT-3.5\. REVECA leverages relevance assessment, plan validation, and spatial
    information to enhance the efficiency and robustness of agent cooperation in dynamic
    and partially observable environments while minimizing continuous communication
    costs and effectively managing irrelevant dummy objects. Our extensive experiments
    demonstrate the superiority of REVECA over previous approaches, including those
    driven by GPT-4.0\. Additionally, a user study highlights REVECA’s potential for
    achieving trustworthy human-AI cooperation. We expect that REVECA will have significant
    applications in gaming, XR applications, educational tools, and humanoid robots,
    contributing to substantial economic, commercial, and academic advancements.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords Multi-agent planning  $\cdot$ Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Digital agents collaborating with humans are crucial in games, educational platforms,
    and virtual universes. Known as Non-Player Characters (NPCs), these agents enhance
    user immersion in commercial applications but often operate on pre-scripted behaviors,
    limiting adaptability and resulting in repetitive actions. In games like World
    of Warcraft and Diablo, NPCs who must be escorted by the player often fail to
    comprehend the situational context, rushing into danger and creating challenges
    for players. This lack of adaptive intelligence detracts from the gaming experience,
    highlighting the need for advanced AI-driven agents.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, OpenAI showcased an AGI Robot using large language models (LLMs) for
    recognition of natural language and household tasks. Inspired by this, we aim
    to create adaptive, intelligent cooperative agents. By facilitating natural language
    interactions, high-level planning, and scene-dedicated low-level controllers,
    these LLM-based agents will address cooperative problems more effectively than
    traditional methods relying on static, learnable models or reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: This paper introduces REVECA, a RElevance and Validation-Enhanced Cooperative
    Language Agent, an LLM-based agent framework addressing decentralized control,
    costly communication, complex tasks, partially observable environments, and noisy
    settings. Similar to prior work [[1](#bib.bib1)], we focus on multi-objective
    household tasks using well-constructed virtual settings. Specifically, we focus
    on two decentralized agents cooperating on a multi-objective, long-horizon household
    task under complex partial observations. Additionally, continuous communication
    incurs time costs, and irrelevant dummy objects add noise, complicating the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The primary advancements of REVECA over previous works are threefold. Firstly,
    REVECA leverages LLMs to assess the relevance of newly acquired information before
    storage, prioritizing it and reducing reasoning complexity for planning. Previous
    research [[1](#bib.bib1), [2](#bib.bib2)] stored all information in memory, leading
    to performance declines due to increasing memory requirements, limited context
    length, and intensifying reasoning complexity. Some studies [[3](#bib.bib3), [4](#bib.bib4)]
    used queues to retain recent $K$ pieces of information, but this resulted in suboptimal
    planning due to the limited historical information.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, REVECA uses a novel modular cognitive architecture to mitigate false
    planning. This occurs in partially observable environments because changes induced
    by collaborators are not immediately reflected in the agent’s memory. Traditional
    approaches [[5](#bib.bib5)] update information via constant pre-task communication,
    which is costly. REVECA’s Validation Module checks plan validity by assessing
    information relevance and estimating collaborators’ recent plans, enabling communication
    only when necessary and improving efficiency and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, REVECA excels in planning by incorporating spatial information. Previous
    studies often neglected spatial information due to its integration challenge in
    language models. Although some studies [[1](#bib.bib1), [3](#bib.bib3)] have attempted
    to incorporate spatial information through image or text representations, they
    did not report performance improvements. We attempt to address this by considering
    the spatial distance between agents and objects in LLM reasoning processes. By
    employing a prompting technique, that effectively integrates numerical distances,
    REVECA enhances the cooperative performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We conducted comparative analysis, ablation studies, and user studies, using
    three multi-room simulation environments: Communicative Watch-And-Help (C-WAH),
    ThreeDWorld Multi-Agent Transport (TDW-MAT), and Noisy-C-WAH. Noisy-C-WAH, a variant
    of C-WAH with dummy obstacles, tested REVECA in noisy settings. Results show that
    REVECA achieves higher success rates, efficiency, and robustness than recent studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Addressed cooperative challenges: decentralized control, costly communication,
    complex long-horizon tasks, partially observable environments, and noisy multi-object
    settings.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developed an LLM-based relevance estimation method for optimal planning and
    robustness in noisy environments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduced an agent framework that validates plans and incorporates spatial
    information for mitigating false planning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducted extensive evaluations that show REVECA’s superior performance over
    previous studies in C-WAH, TDW-MAT, and Noisy-C-WAH.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performed a user study demonstrating potential trustworthy human-AI cooperation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Research on cooperative agents has a long history  [[6](#bib.bib6), [7](#bib.bib7)].
    The traditional cooperative agent has been studied across various directions,
    mainly leveraging reinforcement learning technique [[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25)]. These approaches have facilitated the development of agents
    that can autonomously learn to cooperate by maximizing cumulative rewards through
    trial and error in various simulated settings. Notable studies have explored aspects
    such as visual reinforcement learning for navigation and interaction [[8](#bib.bib8)],
    mapping state spaces to actions effectively [[9](#bib.bib9)], and enhancing the
    robustness of learning algorithms in multi-agent systems [[10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: Some other researchers have aimed to develop platforms to test cooperative agents [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)]. These provide standardized environments to benchmark the performance
    and generalization capabilities of the agents.
  prefs: []
  type: TYPE_NORMAL
- en: However, a significant limitation of previous work is the lack of support for
    natural language-based communication between agents [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [11](#bib.bib11), [30](#bib.bib30), [34](#bib.bib34)]. The effective
    communication is crucial for enhancing collaboration, especially in complex, multi-agent
    environments. Natural language processing capabilities enable agents to exchange
    information in a more intuitive and human-like manner.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have recently attracted significant interest in both academia and industry,
    also gaining attention in the field of agents [[41](#bib.bib41), [42](#bib.bib42)].
    The common-sense reasoning capabilities and natural language processing abilities
    of LLMs have contributed to enhancing the agent’s decision-making [[13](#bib.bib13),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48)] and planning [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [5](#bib.bib5), [54](#bib.bib54)] abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies on LLM-based embodied agents [[1](#bib.bib1), [3](#bib.bib3),
    [2](#bib.bib2), [4](#bib.bib4)] represent a significant step forward in this field.
    These agents utilize LLMs for understanding the environment, planning tasks, and
    communicating with human users and other agents. However, existing studies face
    suboptimal performance due to issues with LLMs, such as inadequate spatial data
    processing capabilities [[55](#bib.bib55)], performance degradation with large
    input data [[56](#bib.bib56)], and insufficient reasoning abilities with complex
    reasoning tasks [[57](#bib.bib57)].
  prefs: []
  type: TYPE_NORMAL
- en: This paper introduces REVECA, an LLM-based embodied agent framework designed
    to address the limitations of the previous works by leveraging information relevance,
    plan validation, and effective communication strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem we focus on is an extension of the decentralized partially observable
    Markov decision process (DEC-POMDP) [[58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [10](#bib.bib10), [1](#bib.bib1), [4](#bib.bib4)], characterized by the tuple
    $(n,S,\{\Sigma_{i}\},\{A_{i}\},\{O_{i}\},T,G,h)$, based on partial observations
    that guide their decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: This paper focuses on a scenario where two decentralized intelligent agents
    collaborate on a long-horizon rearrangement task [[61](#bib.bib61)] within an
    indoor multi-room environment, utilizing noise-free broadcast communication. While
    the focus is on two agents, the methods and experiments are generalizable to scenarios
    with more agents. The agents can perform navigation actions, interaction actions,
    and communication actions. The task includes several predicates $g_{i}$, which
    represents the sub-task of placing two cupcakes in the fridge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1730da9428a8255f2d7e4381a0613f51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The interaction flow between the six modules of REVECA and the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e961b0a4842f81abe977e7684c504fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An example scenario demonstrating REVECA’s comprehensive operational
    flow, highlighting the interaction between various modules, collaborators, and
    the environment to achieve a common goal.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 REVECA framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our framework comprises six key modules: Communication Module, Observation
    Module, Memory Module, Planning Module, Validation Module, and Execution Module.
    Communication Module facilitates natural language-based information sharing. Observation
    Module gathers and categorizes information into four relevance levels while the
    agent explores the environment. Memory Module comprises six components: common
    goal, communication history, scene observation history, self-information history,
    collaborators-information history, and a low-level action skill book. It is responsible
    for storing and updating data. Planning Module employs memory data to generate
    efficient plans, considering information relevancy, predicted proximity of all
    agents, and collaborators’ information history. Validation Module checks for false
    plans by generating and confirming collaborators’ possible interaction scenarios
    between observation and planning times, then discards and reformulates plans if
    needed. Execution Module executes validated plans using the low-level action skill
    book. The modular design of REVECA is illustrated in [Figure 1](#S3.F1 "Figure
    1 ‣ 3 Problem Definition ‣ LLM-Based Cooperative Agents using Information Relevance
    and Plan Validation") and an example scenario presenting REVECA’s workflow is
    depicted in [Figure 2](#S3.F2 "Figure 2 ‣ 3 Problem Definition ‣ LLM-Based Cooperative
    Agents using Information Relevance and Plan Validation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ccf35f0f6ced03e8b49284bf2903ed9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Four cases when the Communication Module is invoked: (a) Simulation
    Initiation. (b) Validation Requests. (c) Response to Validation Requests. (d)
    Sub-goal Achievement.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Communication for information sharing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Communication Module, facilitating natural language information sharing,
    is invoked in four cases. 1) Simulation Initiation: Agents exchange initial information
    about their locations and surrounding objects. 2) Validation Requests: Agents
    query about task history. 3) Response to Validation Requests: Agents provide task
    completion history. 4) Sub-goal Achievement: Agents announce sub-goal completion.
    The detailed illustrations for each case are presented in [Figure 3](#S4.F3 "Figure
    3 ‣ 4 REVECA framework ‣ LLM-Based Cooperative Agents using Information Relevance
    and Plan Validation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea97943cdc421851898e6ba58e9c3df4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An example of determining the relevance score for scene information.
    If the common goal is to put milk in the fridge, the fridge and milk have strong
    relevance. Since the milk could also be placed in containers, the drawer and cabinet
    are estimated to have a moderate relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Observation-time: Relevance Estimation and Storage'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During observation time, agents acquire four types of information: scene, self-agent,
    communication, and collaborator-agent. Scene information includes object details
    like 3D position, ID, name, room, and interactions. Self-agent information includes
    the held object, current position, and completed plan history. These are obtained
    at every simulation step. Communication information, conveyed in natural language,
    is obtained when it occurs. Collaborator-agent information is acquired when communicating
    with or seeing a specific agent. It includes the same details as self-agent information.'
  prefs: []
  type: TYPE_NORMAL
- en: Scene and communication information are stored in scene observation history
    and communication history, respectively, with four relevance levels (strong, medium,
    low, none) assessed by LLMs. This prioritizes information, avoiding the need to
    re-reference all memory entries and reducing the reasoning complexity during the
    LLM-based planning process. An example of determining the relevance score for
    scene information is depicted in [Figure 4](#S4.F4 "Figure 4 ‣ 4.1 Communication
    for information sharing ‣ 4 REVECA framework ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation").
  prefs: []
  type: TYPE_NORMAL
- en: Self-agent information, which is fully observable, is directly stored in self-information
    history. Conversely, collaborator-agent information, which is acquired discontinuously,
    is interpolated to address gaps before being stored in collaborators-information
    history.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4c090236cce4d813fd609871ddeb139e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The two-step planning procedure of REVECA. In the first step, a set
    of $K=3$ plans are created, each plan evaluated for proximity and relevance. The
    second step involves reasoning through these plans using the LLM with chain-of-thought
    prompting. The LLM evaluates the plans based on input data, including self-information
    and collaborator information, recommending the most efficient action.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Planning-time: Real-Time Adaptation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing research on agent-based simulations [[62](#bib.bib62), [63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65)] often constructs sequential long-term plans
    for specific goals, effective in individual tasks where the agent is the sole
    entity influencing the environment and in centralized settings where a manager
    oversees the environment. However, in decentralized cooperative tasks within partially
    observable environments, agents must dynamically adapt their plans to changes,
    as long-term plans devised at earlier stages can become impractical and lead to
    false plans.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this, we propose a Planning Module that devises the next optimal
    action at each decision point, both at the start of the simulation and following
    the execution of each action. It starts with creating $K$ plans based on information
    relevance and relative proximity of collaborators. LLMs then utilize zero-shot
    chain-of-thought prompting [[66](#bib.bib66)] to select the optimal plan, prioritizing
    tasks based on relevance, self-information, and collaborators’ information. This
    approach implicitly guides the planning process to prioritize tasks that are crucial
    for achieving the goal while ensuring the agent’s actions are more efficient than
    those of its collaborators. This two-step planning procedure is depicted in [Figure 5](#S4.F5
    "Figure 5 ‣ 4.2 Observation-time: Relevance Estimation and Storage ‣ 4 REVECA
    framework ‣ LLM-Based Cooperative Agents using Information Relevance and Plan
    Validation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ff98e714de7bda69265c08c105d3f80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Demonstrates how REVECA’s Validation Module enhances collaboration
    between Alice and Bob. The action history indicates which actions Alice knows
    about and which are unknown to her. Without validation, Alice’s plan fails due
    to missing information about Bob’s actions. When the Validation Module is used,
    Alice communicates with Bob, confirming the status of the milk and successfully
    completing the task by grabbing the cupcake.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.4 Validation-time: Scenario-based Validation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even a well-constructed plan may become invalid due to the environmental changes
    caused by collaborators during the interval between observation and planning.
    In partially observable environments, determining whether such changes have occurred
    is challenging for the agent.
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward method to resolve this is to revisit the object’s location
    or query all collaborators about their interactions. However, this can result
    in inefficient path planning and incur significant communication costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this, REVECA includes scenario-based plan validation, estimating
    plan validity using stored information. The agent generates all possible scenarios
    for the object’s interactions with collaborators between the observation and planning
    times. The agent then uses collaborator information from its Memory Module to
    ask the LLM to determine the most likely scenario employing the zero-shot chain-of-thought
    prompting technique [[66](#bib.bib66)]. If the scenario where no collaborator
    interacted with the object is most likely, the agent assumes the plan is valid.
    If another scenario is likely, the agent communicates with the collaborator to
    confirm the interaction. If confirmed, the agent discards the original plan, creates
    a new one, and repeats the scenario-based validation. The scenarios with and without
    the Validation Module are shown in [Figure 6](#S4.F6 "Figure 6 ‣ 4.3 Planning-time:
    Real-Time Adaptation ‣ 4 REVECA framework ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Executing Navigation and Contextual Actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the plan is determined, the Execution Module retrieves sub-task information
    from the Memory Module to identify the target location. We use the A-star search
    algorithm for efficient pathfinding. Upon approaching the object, the agent retrieves
    a predefined animation from the low-level action skill book to execute the planned
    interaction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/694e42402a5caeeb449c01004c48cd3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The layouts of the two virtual environments used in the REVECA experiments.
    (a) C-WAH. (b) TDW-MAT.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conducted experiments using three types of indoor multi-room simulation
    environments: C-WAH, TDW-MAT [[1](#bib.bib1)], and Noisy-C-WAH. To ensure a fair
    comparison, we adopted the detailed parameters defined in the previous study’s
    settings [[1](#bib.bib1)].'
  prefs: []
  type: TYPE_NORMAL
- en: C-WAH [[1](#bib.bib1)], an extended version of the Watch-And-Help Challenge [[34](#bib.bib34)],
    is built on the realistic multi-agent simulation platform, VirtualHome [[67](#bib.bib67)].
    This environment includes five household tasks, such as setting the table and
    doing the dishes, forming the common goal. Our experiments consist of 10 episodes,
    each with five different household tasks across two test environments. In C-WAH,
    agents can acquire or provide information through communication with other agents
    while executing instructions. When an agent enters a specific room, it can observe
    all objects not inside a container like fridges or microwaves. To observe objects
    inside containers, the agent is required an additional action of opening containers.
    Agents are limited to using up to 500 characters per frame to mimic real-world
    communication costs. Horizon $h$ is set to 250 steps, and each task includes 3
    to 5 subgoals (or objects). Failing to achieve the common goal within the 250
    steps results in an episode failure. The layout is shown in [Figure 7](#S4.F7
    "Figure 7 ‣ 4.5 Executing Navigation and Contextual Actions ‣ 4 REVECA framework
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")(a).
  prefs: []
  type: TYPE_NORMAL
- en: 'TDW-MAT, an extended version of the ThreeDWorld Transport Challenge [[68](#bib.bib68)],
    is built on the TDW [[69](#bib.bib69)]. It features more natural object placements
    and a variety of objects and containers that assist in transporting items. The
    common goals involve transporting items in two categories: Food and Stuff. Each
    episode includes 10 target objects, and 2 to 5 containers are placed to facilitate
    moving multiple items simultaneously. Unlike C-WAH, agents in TDW-MAT cannot obtain
    complete room information without performing a 360-degree rotation in 15-degree
    increments. Communication is limited to 500 characters per frame, and $h$ is set
    to 3,000 steps. The layout is shown in [Figure 7](#S4.F7 "Figure 7 ‣ 4.5 Executing
    Navigation and Contextual Actions ‣ 4 REVECA framework ‣ LLM-Based Cooperative
    Agents using Information Relevance and Plan Validation")(b).'
  prefs: []
  type: TYPE_NORMAL
- en: Noisy-C-WAH is a customized version of C-WAH augmented with an additional 10
    or 20 dummy objects per episode to demonstrate the robustness of our framework
    in noisy environments. These dummy objects increase the complexity of the environment,
    challenging the agents’ observation, planning, and communication processes.
  prefs: []
  type: TYPE_NORMAL
- en: In C-WAH and Noisy-C-WAH, we evaluate the agent performance using Simulation
    Steps (SS), Travel Distance (TD), Communication Steps (CS), and Character Counts
    (CC) to measure the time cost to achieve the common goal, the average distance
    traveled, the time cost to perform communication, and the average number of characters
    used in the communication, respectively. In TDW-MAT, performance is evaluated
    based on the success rate of transporting items, including the overall success
    rate (TOTAL), and specific success rates for objects categorized as Food (FOOD)
    and Stuff (STUFF).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 REVECA and Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In comparative experiments, our REVECA was evaluated against three baselines:
    the MCTS-based Hierarchical Planner (MHP), the Rule-based Hierarchical Planner
    (RHP), and the Cooperative Embodied Language Agent (CoELA). We compared REVECA
    with MHP and CoELA in C-WAH, with RHP and CoELA in TDW-MAT, and with CoELA in
    Noisy-C-WAH.'
  prefs: []
  type: TYPE_NORMAL
- en: 'REVECA leverages GPT-3.5 through the OpenAI API. Although GPT-4 is the latest
    model, its high token cost makes extensive experimentation impractical. Thus,
    we employed GPT-3.5 with default parameters: a temperature of 0.7, top-p of 1,
    and a maximum token limit of 256.'
  prefs: []
  type: TYPE_NORMAL
- en: MHP, from the Watch-And-Help Challenge, is a Hierarchical Planner with a high-level
    planner utilizing MCTS and a low-level planner based on regression planning [[70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: RHP, from the ThreeDWorld Transport Challenge, is a Hierarchical Planner with
    a high-level planner using heuristics and a low-level A-start-based planner for
    navigation using a semantic map and Frontier Exploration strategy.
  prefs: []
  type: TYPE_NORMAL
- en: CoELA leverages LLMs for planning and communication, enabling collaborative
    task-solving. For a fair comparison and cost efficiency, we used GPT-3.5-driven
    CoELA (CoELA-3.5) for the experiment and provided GPT-4.0-driven CoELA (CoELA-4.0)
    performance from the CoELA manuscript [[1](#bib.bib1)] for a comprehensive analysis
    of different LLM capacities.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Comparative Analysis Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Comparative experimental results with C-WAH.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | SS $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| REVECA |  | 48.90 |  | 40.34 |  | 6.90 |  | 79.65 |'
  prefs: []
  type: TYPE_TB
- en: '| CoELA-3.5 |  | 71.90 |  | 61.29 |  | 10.40 |  | 158.46 |'
  prefs: []
  type: TYPE_TB
- en: '| CoELA-4.0 |  | 57.00 |  | / |  | / |  | / |'
  prefs: []
  type: TYPE_TB
- en: '| MHP |  | 69.40 |  | 58.96 |  | / |  | / |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparative experimental results with TDW-MAT.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | TOTAL (%) $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| REVECA | 0.86 |  | 0.88 |  | 0.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CoELA-3.5 | 0.64 |  | 0.69 |  | 0.59 |'
  prefs: []
  type: TYPE_TB
- en: '| CoELA-4.0 | 0.71 |  | 0.82 |  | 0.61 |'
  prefs: []
  type: TYPE_TB
- en: '| RHP | 0.79 |  | 0.83 |  | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/8046913bef887f60eb30dd607e213c27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparative experimental results with Noisy-C-WAH, incorporating
    varying numbers of dummy objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 1](#S5.T1 "Table 1 ‣ 5.3 Comparative Analysis Results ‣ 5 Experiment
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")
    shows that REVECA outperforms other baselines in C-WAH. Specifically, GPT-3.5-driven
    REVECA uses fewer steps to complete tasks compared to CoELA-4.0, CoELA-3.5, and
    MHP, demonstrating superior effectiveness. The performance of CoELA significantly
    drops with GPT-3.5, even below MHP, highlighting CoELA’s reliance on GPT-4.0’s
    reasoning capability. REVECA also shows fewer average travel distances, communication
    steps, and characters used per communication.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2](#S5.T2 "Table 2 ‣ 5.3 Comparative Analysis Results ‣ 5 Experiment
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")
    presents TDW-MAT results, where GPT-3.5-driven REVECA outperforms all baselines,
    including GPT-4.0-driven CoELA, across all metrics (TOTAL, FOOD, and STUFF).'
  prefs: []
  type: TYPE_NORMAL
- en: In Noisy-C-WAH, the experiment included 10 or 20 additional dummy objects. As
    shown in [Figure 8](#S5.F8 "Figure 8 ‣ 5.3 Comparative Analysis Results ‣ 5 Experiment
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation"),
    REVECA outperforms CoELA-3.5 in all metrics. CoELA’s strategy of storing all acquired
    information in text form leads to a significant decline in reasoning performance.
    As dummy objects increase, the advantages of using relevance scores become more
    evident.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Ablation Study Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: Ablation study result with C-WAH. The highest performance score in
    each metric is highlighted in underlined bold, while scores that surpass those
    of REVECA are indicated in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | SS $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| REVECA | 48.90 | 40.34 | 6.90 | 79.65 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o spatial info | 64.80 | 56.93 | 9.60 | 86.81 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o relevance score | 43.80 | 35.75 | 8.20 | 79.64 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Validation Module | 54.20 | 48.47 | 5.10 | 46.90 |'
  prefs: []
  type: TYPE_TB
- en: '| always ask before action | 46.90 | 38.05 | 9.10 | 84.00 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o collaborator’s info | 59.60 | 51.18 | 9.2 | 83.25 |'
  prefs: []
  type: TYPE_TB
- en: '| K=2 | 55.40 | 46.32 | 7.50 | 66.47 |'
  prefs: []
  type: TYPE_TB
- en: '| K=4 | 53.30 | 45.00 | 9.30 | 86.72 |'
  prefs: []
  type: TYPE_TB
- en: '| R=3 | 53.00 | 46.02 | 8.10 | 77.41 |'
  prefs: []
  type: TYPE_TB
- en: '| R=5 | 54.90 | 46.75 | 8.70 | 88.47 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Ablation study result with Noisy-C-WAH augmented by 20 dummy objects.
    The highest performance score in each metric is highlighted in underlined bold,
    while scores that surpass those of REVECA are indicated in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | SS $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| REVECA | 63.60 | 54.37 | 8.80 | 103.03 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o spatial info | 89.00 | 77.86 | 13.10 | 126.00 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o relevance score | 95.30 | 79.66 | 9.80 | 110.33 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Validation Module | 75.80 | 68.53 | 5.00 | 46.84 |'
  prefs: []
  type: TYPE_TB
- en: '| always ask before action | 56.80 | 49.97 | 13.30 | 127.14 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o collaborator’s info | 80.40 | 70.96 | 9.70 | 110.42 |'
  prefs: []
  type: TYPE_TB
- en: '| K=2 | 65.20 | 55.68 | 7.40 | 109.16 |'
  prefs: []
  type: TYPE_TB
- en: '| K=4 | 72.50 | 62.38 | 9.90 | 120.30 |'
  prefs: []
  type: TYPE_TB
- en: '| R=3 | 64.60 | 55.15 | 8.60 | 95.03 |'
  prefs: []
  type: TYPE_TB
- en: '| R=5 | 76.90 | 64.86 | 7.70 | 100.05 |'
  prefs: []
  type: TYPE_TB
- en: To demonstrate the significance of each component in our framework, we conducted
    an ablation study within C-WAH and Noisy-C-WAH environments, the latter augmented
    with 20 dummy objects. The results are presented in [Table 3](#S5.T3 "Table 3
    ‣ 5.4 Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation") and  [Table 4](#S5.T4 "Table 4 ‣ 5.4
    Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using Information
    Relevance and Plan Validation").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76a083036a4eac7024336f1fd2c9e7ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Two example scenarios involving Alice, who needs to place a green
    plate and a red plate into the sink. This demonstrates the difference in path
    planning efficiency with and without using spatial information. In scenario (a),
    Alice, without spatial information, follows a suboptimal path to complete the
    task, moving back and forth unnecessarily. Scenario (b) showcases the improved
    efficiency when spatial information is used, enabling Alice to plan a more direct
    and logical route.'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we examined the REVECA without spatial information and without relevance
    scores to evaluate their impact on performance. The absence of spatial information
    degraded performance across all metrics, highlighting its importance in effective
    planning. Omitting relevance scores improved SS and TD scores but worsened CS
    scores in C-WAH while degrading all scores in Noisy-C-WAH. This suggests that
    relevance scores are crucial in more complex environments like Noisy-C-WAH with
    over 20 objects. To aid readers understand, example scenarios with and without
    using spatial information are illustrated in [Figure 9](#S5.F9 "Figure 9 ‣ 5.4
    Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using Information
    Relevance and Plan Validation")
  prefs: []
  type: TYPE_NORMAL
- en: Next, we tested REVECA without the Validation Module and with a setting where
    communication occurred before every action. The absence of the Validation Module
    worsened SS and TD scores. Conversely, frequent communication before every action
    slightly improved SS and TD but significantly degraded CS and CC scores, indicating
    that the Validation Module enhances performance without excessive communication.
  prefs: []
  type: TYPE_NORMAL
- en: We also tested the REVECA without considering collaborator information, resulting
    in significantly degraded performance across all scores. This underscores the
    importance of collaborator information, even when some details are inferred.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we varied the number of plans ($K$ = 4 yielded the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45383c69886076ef0adc1d760ead005f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: User study results with C-WAH. This figure illustrates the mean
    scores and associated standard errors for responses to four research questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 User Study Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conducted a user study to evaluate REVECA’s ability to collaborate seamlessly
    with humans to achieve common goals. Five participants (four men and one woman)
    with an average age of 23.4 years were recruited. The experiment was conducted
    in the C-WAH environment using four methods: REVECA, REVECA without communication
    (w/o comm), REVECA with always ask before action (always ask), and CoELA. Participants
    shared the same observation and action space as the agents and interacted with
    the environment by selecting actions from a predefined list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Participants completed 5 sub-goals with each method. After each, they answered
    a 7-point Likert scale (1: strongly disagree, 7: strongly agree) questionnaire
    on four research questions: 1) Did the agent respond appropriately to your intentions?
    (Appropriateness), 2) Was the interaction with the agent helpful in achieving
    the goal? (Usefulness), 3) Did the agent’s performance help achieve the goal quickly?
    (Efficiency), and 4) Did you feel a sense of trust with the agent? (Trust) The
    “w/o comm" method excluded Appropriateness and Usefulness questions, due to the
    lack of interaction. Participants were then interviewed about each method.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 10](#S5.F10 "Figure 10 ‣ 5.4 Ablation Study Results ‣ 5
    Experiment ‣ LLM-Based Cooperative Agents using Information Relevance and Plan
    Validation"), REVECA scored highest across all four questions, excelling in human
    collaboration. Participants noted that CoELA often produced messages focused on
    status reports and planning rather than addressing their questions, resulting
    in lower scores in Appropriateness and Usefulness. In the “always ask" condition,
    participants found the agent’s repetitive questions disruptive and demotivating.
    Regarding Trust, participants noted that the lack of communication in the “w/o
    comm" method made it difficult to understand the agent’s actions and situation,
    thereby hindering trust-based collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduced REVECA, an LLM-driven cognitive architecture designed
    for multi-objective household tasks, enabling efficient cooperation between decentralized
    agents under complex partial observations. By leveraging relevance assessment,
    spatial information, and plan validation, REVECA enhances agent cooperation in
    dynamic and partially observable environments, while minimizing continuous communication
    costs and effectively managing irrelevant dummy objects.
  prefs: []
  type: TYPE_NORMAL
- en: REVECA’s applications extend beyond household tasks to gaming, virtual universes,
    and educational environments. In gaming, REVECA can revolutionize NPC behavior,
    enabling adaptive and intelligent interactions for a richer player experience.
    In virtual universes, REVECA can enhance user-agent interactions, facilitating
    collaborative tasks and social interactions for more seamless experiences. In
    education, REVECA can manage complex tasks and provide real-time feedback, supporting
    personalized learning and creating interactive simulations and virtual tutors
    tailored to individual needs.
  prefs: []
  type: TYPE_NORMAL
- en: However, REVECA has limitations that warrant further exploration. Its effectiveness
    in highly dynamic and unpredictable outdoor settings remains to be validated.
    The framework has been tested primarily with two agents, and scaling to more agents
    may introduce new challenges in social interactions and coordination. Additionally,
    REVECA’s use of a low-level action skill book could be enhanced by integrating
    recent advancements in animation generation technologies, enabling more realistic
    and context-sensitive actions.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing these limitations could make future REVECA even more robust and applicable
    across a broader range of multi-agent environments and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B
    Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly
    with large language models. arXiv preprint arXiv:2307.02485, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, and Hongyuan
    Zha. Semantically aligned task decomposition in multi-agent reinforcement learning.
    ArXiv, abs/2305.10865, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yi Eve Sun,
    Chen Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang,
    F. Yin, Yitao Liang, and Yaodong Yang. Proagent: Building proactive cooperative
    agents with large language models. In AAAI Conference on Artificial Intelligence,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin
    Shu, Yilun Du, and Chuang Gan. Combo: Compositional world models for embodied
    multi-agent cooperation. ArXiv, abs/2404.10775, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes,
    Michael Lewis, and Katia P. Sycara. Theory of mind for multi-agent collaboration
    via large language models. In Conference on Empirical Methods in Natural Language
    Processing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Peter Stone and Manuela Veloso. Multiagent systems: A survey from a machine
    learning perspective. Autonomous Robots, 8:345–383, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning:
    a survey. Artificial Intelligence Review, 55(2):895–943, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta,
    Roozbeh Mottaghi, and Ali Farhadi. Visual semantic planning using deep successor
    representations. In Proceedings of the IEEE international conference on computer
    vision, pages 483–492, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin,
    and Yoav Artzi. Mapping instructions to actions in 3d environments with visual
    goal prediction. arXiv preprint arXiv:1809.00786, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Chris Amato, George Dimitri Konidaris, Leslie Pack Kaelbling, and Jonathan P.
    How. Modeling and planning with macro-actions in decentralized pomdps. The journal
    of artificial intelligence research, 64:817–859, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever,
    Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham
    Ruderman, et al. Human-level performance in 3d multiplayer games with population-based
    reinforcement learning. Science, 364(6443):859–865, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory
    Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation
    for deep multi-agent reinforcement learning. Journal of Machine Learning Research,
    21(178):1–51, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction
    and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Hengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob
    Foerster. Off-belief learning. In International Conference on Machine Learning,
    pages 4369–4379\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal
    multi-agent reinforcement learning via policy decoupling with transformers. arXiv
    preprint arXiv:2101.08001, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan
    Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained multi-agent
    decision transformer: One big sequence model tackles all smac tasks. arXiv preprint
    arXiv:2112.02845, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett.
    Collaborating with humans without human data. Advances in Neural Information Processing
    Systems, 34:14502–14515, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory
    diversity for zero-shot coordination. In International conference on machine learning,
    pages 7204–7213\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Keane Lucas and Ross E Allen. Any-play: An intrinsic augmentation for
    zero-shot coordination. arXiv preprint arXiv:2201.12436, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and
    Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem.
    Advances in Neural Information Processing Systems, 35:16509–16521, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre
    Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent
    games. Advances in Neural Information Processing Systems, 35:24611–24624, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi Wu, Zhongqian
    Sun, and Wei Yang. Maximum entropy population-based training for zero-shot human-ai
    coordination. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 37, pages 6145–6153, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing Wang, and
    Wei Pan. Cooperative open-ended learning framework for zero-shot coordination.
    In International Conference on Machine Learning, pages 20470–20484\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Yang Li, Shao Zhang, Jichen Sun, Wenhao Zhang, Yali Du, Ying Wen, Xinbing
    Wang, and Wei Pan. Tackling cooperative incompatibility for zero-shot human-ai
    coordination. arXiv preprint arXiv:2306.03034, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, and
    Yaodong Yang. Heterogeneous-agent reinforcement learning. Journal of Machine Learning
    Research, 25:1–67, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
    Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    Advances in neural information processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs,
    Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor:
    An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Tianmin Shu and Yuandong Tian. M3̂rl: Mind-aware multi-agent management
    reinforcement learning. arXiv preprint arXiv:1810.00147, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and
    Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 9068–9079,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory
    Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob
    Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint
    arXiv:1902.04043, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans,
    Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat:
    A platform for embodied ai research. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 9339–9347, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han,
    Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting
    grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 10740–10749, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu,
    Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based
    interactive environment. In Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, pages 11097–11107, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B
    Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: A challenge for
    social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange,
    Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek
    Hakkani-Tur. Teach: Task-driven embodied agents that chat. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 36, pages 2017–2025, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava,
    Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai
    Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities
    and realistic simulation. In Conference on Robot Learning, pages 80–93\. PMLR,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang,
    Yilun Du, Joshua B Tenenbaum, and Chuang Gan. Hazard challenge: Embodied decision
    making in dynamically changing environments. arXiv preprint arXiv:2401.12975,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for
    multi-agent cooperation. In Neural Information Processing Systems, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh,
    Michael G. Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication.
    In International Conference on Machine Learning, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter
    Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai
    coordination. Advances in neural information processing systems, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
    language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language
    model based autonomous agents. Frontiers of Computer Science, 18(6):1–26, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan,
    Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al. Pre-trained language
    models for interactive decision-making. Advances in Neural Information Processing
    Systems, 35:31199–31212, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale
    Schuurmans. Foundation models for decision making: Problems, methods, and opportunities.
    arXiv preprint arXiv:2303.04129, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths.
    Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete
    Florence, Igor Mordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding:
    Guiding text generation with grounded models for robot control. arXiv preprint
    arXiv:2303.00855, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert
    Jankowski, Yanghua Xiao, and Deqing Yang. Distilling script knowledge from large
    language models for constrained language planning. arXiv preprint arXiv:2305.05252,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Wenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence,
    Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,
    Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian
    Ichter. Inner monologue: Embodied reasoning through planning with language models.
    In Conference on Robot Learning, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language
    models as zero-shot planners: Extracting actionable knowledge for embodied agents.
    In International Conference on Machine Learning, pages 9118–9147\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius,
    and Stefanie Tellex. Planning with large language models via corrective re-prompting.
    In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi,
    Lior Horesh, Biplav Srivastava, Francesco Fabiano, and Andrea Loreggia. Plansformer:
    Generating symbolic plans using transformers. arXiv preprint arXiv:2212.08681,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Maitrey Gramopadhye and Daniel Szafir. Generating executable action plans
    with environmentally-aware language models. In 2023 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pages 3568–3575\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and
    Yitao Liang. Describe, explain, plan and select: interactive planning with llms
    enables open-world multi-task agents. Advances in Neural Information Processing
    Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha
    Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general
    intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the
    impact of input length on the reasoning performance of large language models.
    ArXiv, abs/2402.14848, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Tomer David Ullman. Large language models fail on trivial alterations
    to theory-of-mind tasks. ArXiv, abs/2302.08399, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Daniel S. Bernstein, Shlomo Zilberstein, and Neil Immerman. The complexity
    of decentralized control of markov decision processes. In Conference on Uncertainty
    in Artificial Intelligence, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Claudia V. Goldman and Shlomo Zilberstein. Optimizing information exchange
    in cooperative multi-agent systems. In Adaptive Agents and Multi-Agent Systems,
    2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Matthijs T. J. Spaan, Geoffrey J. Gordon, and Nikos A. Vlassis. Decentralized
    planning under uncertainty for teams of communicating agents. In Adaptive Agents
    and Multi-Agent Systems, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Dhruv Batra, Angel X. Chang, S. Chernova, Andrew J. Davison, Jia Deng,
    Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi,
    Manolis Savva, and Hao Su. Rearrangement: A challenge for embodied ai. ArXiv,
    abs/2011.01975, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee,
    and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought
    reasoning by large language models. In Annual Meeting of the Association for Computational
    Linguistics, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Simeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu, and Mohit Iyyer. Pearl:
    Prompting large language models to plan and execute actions over long documents.
    ArXiv, abs/2305.14564, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
    and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning.
    In Neural Information Processing Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish
    Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning
    with language models. ArXiv, abs/2311.05772, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
    Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Xavier Puig, Kevin Kyunghwan Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities
    via programs. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 8494–8502, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar,
    Dan Gutfreund, Daniel L. K. Yamins, James J. DiCarlo, Josh H. McDermott, Antonio
    Torralba, and Joshua B. Tenenbaum. The threedworld transport challenge: A visually
    guided task-and-motion planning benchmark towards physically realistic embodied
    ai. 2022 International Conference on Robotics and Automation (ICRA), pages 8847–8854,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer,
    Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano,
    Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis, Kevin T.
    Feigelis, Daniel Bear, Dan Gutfreund, David Cox, James J. DiCarlo, Josh H. McDermott,
    Joshua B. Tenenbaum, and Daniel L. K. Yamins. Threedworld: A platform for interactive
    multi-modal physical simulation. ArXiv, abs/2007.04954, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Richard E. Korf. Planning as search: A quantitative approach. Artif. Intell.,
    33:65–88, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
