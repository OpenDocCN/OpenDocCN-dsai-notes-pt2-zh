- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:48:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:48:38'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AgentQuest: 基准测试 LLM 代理在多步骤高强度推理任务中的行为'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06411](https://ar5iv.labs.arxiv.org/html/2404.06411)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06411](https://ar5iv.labs.arxiv.org/html/2404.06411)
- en: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
- en: David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹
- en: ¹ NEC Laboratories Europe, Heidelberg, Germany
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ NEC 实验室欧洲，海德堡，德国
- en: ² Politecnico di Torino, Turin, Italy
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 都灵理工大学，都灵，意大利
- en: ³ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³ CAIR, 圣西里尔和圣美索迪乌斯大学，斯科普里，北马其顿
- en: 'AgentQuest: A Modular Benchmark Framework'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AgentQuest: 一个模块化的基准测试框架'
- en: to Measure Progress and Improve LLM Agents
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用于测量进展和改进 LLM 代理
- en: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
- en: David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹
- en: ¹ NEC Laboratories Europe, Heidelberg, Germany
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ NEC 实验室欧洲，海德堡，德国
- en: ² Politecnico di Torino, Turin, Italy
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ² 都灵理工大学，都灵，意大利
- en: ³ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ³ CAIR, 圣西里尔和圣美索迪乌斯大学，斯科普里，北马其顿
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advances made by Large Language Models (LLMs) have led to the pursuit of
    LLM agents that can solve intricate, multi-step reasoning tasks. As with any research
    pursuit, benchmarking and evaluation are key corner stones to efficient and reliable
    progress. However, existing benchmarks are often narrow and simply compute overall
    task success. To face these issues, we propose AgentQuest ¹¹1Demo provided at
    [https://youtu.be/0JNkIfwnoak](https://youtu.be/0JNkIfwnoak). – a framework where
    (i) both benchmarks and metrics are modular and easily extensible through well
    documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that
    can reliably track LLM agent progress while solving a task. We exemplify the utility
    of the metrics on two use cases wherein we identify common failure points and
    refine the agent architecture to obtain a significant performance increase. Together
    with the research community, we hope to extend AgentQuest further and therefore
    we make it available under [https://github.com/nec-research/agentquest](https://github.com/nec-research/agentquest).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进展促使我们追求能够解决复杂、多步骤推理任务的 LLM 代理。正如任何研究追求一样，基准测试和评估是有效可靠进展的关键基石。然而，现有的基准测试通常较窄，只计算总体任务成功率。为应对这些问题，我们提出了
    AgentQuest ¹¹1Demo，详见 [https://youtu.be/0JNkIfwnoak](https://youtu.be/0JNkIfwnoak)。
    – 这是一个框架，其中 (i) 基准测试和指标都是模块化的，通过文档完善、易于使用的 API 容易扩展；(ii) 我们提供了两个新的评估指标，可以在解决任务的过程中可靠地跟踪
    LLM 代理的进展。我们通过两个用例展示了这些指标的实用性，在这些用例中，我们识别出常见的失败点，并优化代理架构以获得显著的性能提升。我们希望与研究社区一起进一步扩展
    AgentQuest，因此我们将其发布在 [https://github.com/nec-research/agentquest](https://github.com/nec-research/agentquest)
    上。
- en: 'AgentQuest: A Modular Benchmark Framework'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 'AgentQuest: 一个模块化的基准测试框架'
- en: to Measure Progress and Improve LLM Agents
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用于测量进展和改进 LLM 代理
- en: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
    David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹ ¹ NEC Laboratories Europe,
    Heidelberg, Germany ² Politecnico di Torino, Turin, Italy ³ CAIR, Ss. Cyril and
    Methodius University, Skopje, North Macedonia
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
    David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹ ¹ NEC 实验室欧洲，海德堡，德国 ² 都灵理工大学，都灵，意大利
    ³ CAIR, 圣西里尔和圣美索迪乌斯大学，斯科普里，北马其顿
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative Agents Kiela et al. ([2023](#bib.bib5)) are software systems that
    leverage foundation models like Large Language Models (LLMs) to perform complex
    tasks, take decisions, devise multi-steps plans and use tools (API calls, coding,
    etc.) to build solutions in heterogeneous contexts Wang et al. ([2023](#bib.bib19));
    Weng ([2023](#bib.bib20)). The potential ability to solve heterogeneous tasks
    with high degrees of autonomy has catalysed the interest of both research and
    industrial communities. Nonetheless, it is still unclear to which extent current
    systems are successfully able to fulfil their promises. In fact, methodologies
    to benchmark, evaluate and advance these systems are still in their early days.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成代理 Kiela et al. ([2023](#bib.bib5)) 是利用基础模型（如大型语言模型（LLMs））来执行复杂任务、做出决策、制定多步骤计划并使用工具（API
    调用、编码等）来在异质环境中构建解决方案的软件系统 Wang et al. ([2023](#bib.bib19)); Weng ([2023](#bib.bib20))。能够以高度自主性解决异质任务的潜在能力激发了研究和工业界的兴趣。然而，目前尚不清楚现有系统在多大程度上能够成功实现其承诺。事实上，基准测试、评估和推进这些系统的方法仍处于起步阶段。
- en: '![Refer to caption](img/89401429d32b07819b1859c506c373e0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/89401429d32b07819b1859c506c373e0.png)'
- en: (a) Existing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 现有
- en: '![Refer to caption](img/38b594fdec62729fadbce466dd94b4b3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/38b594fdec62729fadbce466dd94b4b3.png)'
- en: (b) AgentQuest
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AgentQuest
- en: 'Figure 1: Overview of agent-benchmark interactions in existing frameworks and
    in AgentQuest. AgentQuest defines a common interface to interact with the benchmarks
    and to compute progress metrics, easing the addition of new benchmarks and allowing
    researchers to evaluate and debug their agent architectures.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：现有框架和 AgentQuest 中代理-基准交互的概述。AgentQuest 定义了与基准测试互动和计算进度指标的公共接口，简化了新基准测试的添加，并允许研究人员评估和调试他们的代理架构。
- en: We identify a couple of gaps. Firstly, benchmarking agents requires combining
    different benchmark types Liu et al. ([2023](#bib.bib8)); Chalamalasetti et al.
    ([2023](#bib.bib1)). For example, some benchmarks focus on specific capabilities
    and provide gaming environments, which we refer to as “closed-box” – i.e. with
    a finite set of actions Liu et al. ([2023](#bib.bib8)); Patil et al. ([2023](#bib.bib14));
    Chalamalasetti et al. ([2023](#bib.bib1)) – whereas other benchmarks provide open-ended
    tasks and access to general tools, like web browsing Zhuang et al. ([2023](#bib.bib25));
    Zheng et al. ([2023](#bib.bib24)); Mialon et al. ([2023](#bib.bib9)). As benchmarks
    are developed independently, significant effort goes into custom integration of
    new agent architectures with each benchmark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现了几个缺口。首先，基准测试代理需要结合不同类型的基准测试 Liu et al. ([2023](#bib.bib8)); Chalamalasetti
    et al. ([2023](#bib.bib1))。例如，一些基准测试专注于特定的能力，并提供游戏环境，我们称之为“封闭箱”——即具有有限的行动集 Liu
    et al. ([2023](#bib.bib8)); Patil et al. ([2023](#bib.bib14)); Chalamalasetti
    et al. ([2023](#bib.bib1))——而其他基准测试则提供开放式任务和对通用工具的访问，如网页浏览 Zhuang et al. ([2023](#bib.bib25));
    Zheng et al. ([2023](#bib.bib24)); Mialon et al. ([2023](#bib.bib9))。由于基准测试是独立开发的，因此需要大量工作来将新的代理架构与每个基准测试进行自定义集成。
- en: Secondly, and more critically, existing benchmarks mostly focus on providing
    a *success rate* measure, i.e. a binary success/fail evaluation for each of the
    proposed tasks. While success rate is helpful to measure overall advances of an
    agent technology, it has limited use in guiding improvements for new generative
    agent architectures. Here, it is important to consider that generative agents
    often combine foundation models with multiple other components, such as memory
    and tools. Developers can reason about these individual components in terms of
    architecture and their inter-dependence, and could actively change and evolve
    them using deeper insights about how an agent performs in a benchmark. That is,
    developers need benchmarks to both evaluate and *debug* agents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，更为关键的是，现有的基准测试主要关注于提供*成功率*测量，即对每个提议的任务进行二元的成功/失败评估。虽然成功率有助于衡量代理技术的整体进展，但它在指导新生成代理架构的改进方面用途有限。在这里，重要的是要考虑到生成代理通常将基础模型与多个其他组件结合在一起，例如记忆和工具。开发者可以从架构及其相互依赖性的角度来推理这些单独的组件，并通过对代理在基准测试中表现的深入见解来积极改变和发展它们。也就是说，开发者需要基准测试来同时评估和*调试*代理。
- en: For example, current benchmarks make it hard to answer questions like does the
    agent fail completely the tasks or does it partially solve them? Does the agent
    fail consistently at a certain step? Would extra run time lead to finding a solution?
    Answering these questions would require tracing and inspecting the execution of
    the agent. We argue that providing a more efficient approach that is consistent
    over multiple benchmarks is a stepping stone towards evolving generative agents.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，目前的基准测试很难回答以下问题：代理是否完全未完成任务，还是部分解决了任务？代理是否在某一步骤上始终失败？额外的运行时间是否会找到解决方案？回答这些问题需要跟踪和检查代理的执行。我们认为，提供一种在多个基准测试中一致的更高效的方法是推动生成式代理发展的一个重要步骤。
- en: 'We address these gaps introducing AgentQuest, a modular framework to support
    multiple diverse benchmarks and agent architectures (See Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks")), alongside with two new metrics – i.e. progress rate
    and repetition rate – to debug an agent architecture behaviour. AgentQuest defines
    a standard interface to connect an arbitrary agent architecture with diverse benchmarks,
    and to compute progress and repetition rates from them.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过引入 AgentQuest 来解决这些问题，这是一种支持多种不同基准测试和代理架构的模块化框架（见图 [1](#S1.F1 "图 1 ‣ 1 介绍
    ‣ AgentQuest：多步骤密集推理任务中 LLM 代理行为的基准测试")），同时引入了两个新的度量标准——即进展率和重复率——以调试代理架构行为。AgentQuest
    定义了一个标准接口，用于将任意代理架构与各种基准测试连接，并从中计算进展率和重复率。
- en: 'We showcase the framework, implementing 4 benchmarks in AgentQuest: ALFWorld Shridhar
    et al. ([2020](#bib.bib15)), Lateral Thinking Puzzles Sloane ([1992](#bib.bib16)),
    Mastermind and Sudoku. The latter two are newly introduced with AgentQuest. Additional
    benchmarks can be easily added, while requiring no changes to the tested agents.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了框架，并在 AgentQuest 中实现了 4 个基准测试：ALFWorld Shridhar 等 ([2020](#bib.bib15))，横向思维谜题
    Sloane ([1992](#bib.bib16))，Mastermind 和 Sudoku。后两者是 AgentQuest 新引入的。可以轻松添加其他基准测试，而无需更改被测试的代理。
- en: Our final contribution is to present our experience leveraging the proposed
    metrics to debug and improve existing agent architectures as implemented in LangChain Chase
    ([2022](#bib.bib2)). In particular, we show that in the Mastermind benchmark the
    combination of progress rate and repetition rate identifies a limitation in the
    ability of the agent to explore the full space of potential solutions. Guided
    by this insight we could improve the success rate in this benchmark by up to $\approx$20%.
    In Lateral Thinking Puzzles we show that partially repeating actions is part of
    the agent strategy, whereas in ALFWorld, we show that monitoring the progress
    rate makes it possible to identify that the final success rate is limited by the
    allowed runtime of the agent, and that more steps lead to a better performance.
    Finally, in the Sudoku benchmark, we show that the low success rate is actually
    paired with low progress rate, making clear that the tested agent is unable to
    solve this type of tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的**最终**贡献是展示我们利用所提出的度量标准来调试和改进现有代理架构的经验，这些架构在 LangChain Chase ([2022](#bib.bib2))
    中实现。特别是，我们展示了在 Mastermind 基准测试中，进展率和重复率的结合能够识别代理探索潜在解决方案的能力的局限性。根据这一见解，我们可以将该基准测试中的成功率提高到
    $\approx$20%。在横向思维谜题中，我们展示了部分重复行为是代理策略的一部分，而在 ALFWorld 中，我们展示了监控进展率可以识别最终成功率受代理允许运行时间的限制，更多的步骤可以提高性能。最后，在
    Sudoku 基准测试中，我们展示了低成功率实际上伴随着低进展率，清楚地表明被测试的代理无法解决这类任务。
- en: 2 Generative AI Agents in a Nutshell
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 生成式 AI 代理概述
- en: 'Generative AI agents are automated systems relying on software components integrated
    with LLMs pre-trained on large amount of data for language understanding and processing.
    When assigned a task, an agent engages in a systematic process: it iteratively
    formulates self-generated instructions, executes them, and observes the outcomes
    until the ultimate objective is achieved. Next, we showcase the basic interaction
    between agents and the environment in which they operate and describe the standard
    benchmarking techniques.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 代理是依赖于与大规模数据预训练的语言模型集成的软件组件的自动化系统。分配任务时，代理会进行系统化的过程：它迭代地制定自生成的指令，执行这些指令，并观察结果，直到达到**最终**目标。接下来，我们展示了代理与其操作环境之间的基本互动，并描述了标准的基准测试技术。
- en: 2.1 Agent-Environment interaction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代理-环境互动
- en: 'Closely following the terminology in Reinforcement Learning (RL)²²2Unlike RL
    scenarios, the agent does not need a further training process. It relies on the
    pre-trained LLM and does not perform an action under the influence of any reward. Sutton
    and Barto ([2018](#bib.bib18)), the core elements defining the agent-environment
    interaction are *environment*, *state*, *observation* and *action* (see [Figure 1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '紧密遵循强化学习（RL）中的术语²²2与 RL 场景不同，智能体不需要进一步的训练过程。它依赖于预训练的 LLM，并且不在任何奖励的影响下执行行动。Sutton
    和 Barto ([2018](#bib.bib18)) 认为定义智能体-环境互动的核心元素是*环境*、*状态*、*观察*和*行动*（参见 [图 1(a)](#S1.F1.sf1
    "图 1 ‣ 1 引言 ‣ AgentQuest: 基准测试 LLM 智能体在多步骤密集推理任务中的行为")）。'
- en: Environment and states.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境和状态。
- en: The environment refers to the external system the agent interacts with. In this
    context, we treat the benchmark and the environment as synonyms. It is typically
    described through a finite set of hidden *states*, which are not directly observable
    by the agent and represent the benchmark configuration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 环境指的是智能体与之互动的外部系统。在此背景下，我们将基准和环境视为同义词。它通常通过一组有限的隐藏*状态*来描述，这些状态不能被智能体直接观察到，代表基准配置。
- en: Observations and actions.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察和行动。
- en: The agent interacts with the environment for multiple execution steps. At each
    step, the environment produces an *observation* providing information about its
    current hidden state. The agent uses the internal LLM to process the received
    observation. Being pre-trained on general knowledge data, the LLM engages a reasoning
    process generating a *thought* on the observation (e.g. the planned strategy to
    follow in the current step or the usage of a tool). According to this thought,
    the agent provides the environment an *action* to modify the current hidden state.
    ³³3Unlike RL, the LLM outputs are unconstrained, and any provided action is considered
    valid.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体与环境进行多个执行步骤的互动。在每一步，环境生成一个*观察*，提供关于其当前隐藏状态的信息。智能体利用内部的 LLM 处理收到的观察。由于 LLM
    在通用知识数据上进行了预训练，它会进行推理过程，对观察生成一个*思考*（例如，在当前步骤中遵循的计划策略或工具的使用）。根据这一思考，智能体向环境提供一个*行动*以修改当前的隐藏状态。³³3与强化学习（RL）不同，LLM
    输出不受约束，任何提供的行动都被视为有效。
- en: The following is an example of an agent interacting with a closed-box environment
    to solve a code-breaking challenge, i.e. the Mastermind game Stuckman and Zhang
    ([2005](#bib.bib17)). The agent is tasked to guess a four-digit code. At each
    step, the agent devises a strategy and acts accordingly to guess the code. The
    environment provides a feedback on the number of guessed digits in correct and
    wrong positions. The process continues until a stopping condition is met.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个智能体与封闭环境互动以解决密码破解挑战的示例，即 Mastermind 游戏 Stuckman 和 Zhang ([2005](#bib.bib17))。智能体的任务是猜测一个四位数字的代码。在每一步，智能体制定策略并相应地行动以猜测代码。环境提供有关猜测的数字在正确和错误位置上的反馈。该过程继续，直到满足停止条件。
- en: '[⬇](data:text/plain;base64,PEBcdGV4dGNvbG9ye2ZvcmVzdGdyZWVufXtcdGV4dGJme0VOVklST05NRU5UfTogSW5pdGlhbGl6aW5nIGEgbmV3IGVudmlyb25tZW50Li4ufUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtcdGV4dGJme09CU0VSVkFUSU9OfTogU3RhcnQgZ3Vlc3NpbmcgdGhlIDQgZGlnaXRzIGNvZGUufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBJJ2xsIHN0YXJ0IGJ5IGd1ZXNzaW5nIDEyMzQgdG8gZ2V0IGFuIGlkZWEgb2Ygd2hpY2h9QD4KPEBcdGV4dGNvbG9ye21hZ2VudGF9e29mIHRoZXNlIG51bWJlcnMgYXJlIGluIHRoZSBzZWNyZXQgbnVtYmVyIGFuZCBpZiBhbnkgYXJlIGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgY29ycmVjdCBwb3NpdGlvbi59QD4KPEBcdGV4dGNvbG9ye2dvbGRlbnllbGxvd317XHRleHRiZntBQ1RJT059OiAxMjM0fUA+Cgo8QFx0ZXh0Y29sb3J7Zm9yZXN0Z3JlZW59e1x0ZXh0YmZ7RU5WSVJPTk1FTlR9OiBDb21wYXJpbmcgdGhlIGd1ZXNzZWQgY29kZSB3aXRoIHRoZSB0cnV0aC4uLn1APgo8QFx0ZXh0Y29sb3J7aW50ZXJuYXRpb25hbGtsZWluYmx1ZX17XHRleHRiZntPQlNFUlZBVElPTn06IFlvdXIgZ3Vlc3MgaGFzIDEgY29ycmVjdCBudW1iZXJzIGluIHRoZSB3cm9uZyB9QD4KPEBcdGV4dGNvbG9ye2ludGVybmF0aW9uYWxrbGVpbmJsdWV9e3Bvc2l0aW9uIGFuZCAwIGNvcnJlY3QgbnVtYmVycyBpbiB0aGUgY29ycmVjdCBwb3NpdGlvbi4gfUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtLZWVwIGd1ZXNzaW5nLi4ufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBPbmUgb2YgdGhlIGRpZ2l0cyAxLCAyLCAzLCBvciA0IGlzIGNvcnJlY3QgYnV0IGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgd3JvbmcgcG9zaXRpb24uIEknbGwgdHJ5IHRvIG1vdmUgZWFjaCBkaWdpdCB0byBhfUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtkaWZmZXJlbnQgcG9zaXRpb24gdG8gc2VlIGlmIEkgY2FuIGdldCBhIGNvcnJlY3QgcG9zaXRpb24ufUA+CjxAXHRleHRjb2xvcntnb2xkZW55ZWxsb3d9e1x0ZXh0YmZ7QUNUSU9OfTogMjE0M31APgouLi4=)ENVIRONMENT:
    Initializing a new environment...OBSERVATION: Start guessing the 4 digits code.THOUGHT:
    I’ll start by guessing 1234 to get an idea of whichof these numbers are in the
    secret number and if any are inthe correct position.ACTION: 1234ENVIRONMENT: Comparing
    the guessed code with the truth...OBSERVATION: Your guess has 1 correct numbers
    in the wrongposition and 0 correct numbers in the correct position.Keep guessing...THOUGHT:
    One of the digits 1, 2, 3, or 4 is correct but inthe wrong position. I’ll try
    to move each digit to adifferent position to see if I can get a correct position.ACTION:
    2143...'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,PEBcdGV4dGNvbG9ye2ZvcmVzdGdyZWVufXtcdGV4dGJme0VOVklST05NRU5UfTogSW5pdGlhbGl6aW5nIGEgbmV3IGVudmlyb25tZW50Li4ufUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtcdGV4dGJme09CU0VSVkFUSU9OfTogU3RhcnQgZ3Vlc3NpbmcgdGhlIDQgZGlnaXRzIGNvZGUufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBJJ2xsIHN0YXJ0IGJ5IGd1ZXNzaW5nIDEyMzQgdG8gZ2V0IGFuIGlkZWEgb2Ygd2hpY2h9QD4KPEBcdGV4dGNvbG9ye21hZ2VudGF9e29mIHRoZXNlIG51bWJlcnMgYXJlIGluIHRoZSBzZWNyZXQgbnVtYmVyIGFuZCBpZiBhbnkgYXJlIGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgY29ycmVjdCBwb3NpdGlvbi59QD4KPEBcdGV4dGNvbG9ye2dvbGRlbnllbGxvd317XHRleHRiZntBQ1RJT059OiAxMjM0fUA+Cgo8QFx0ZXh0Y29sb3J7Zm9yZXN0Z3JlZW59e1x0ZXh0YmZ7RU5WSVJPTk1FTlR9OiBDb21wYXJpbmcgdGhlIGd1ZXNzZWQgY29kZSB3aXRoIHRoZSB0cnV0aC4uLn1APgo8QFx0ZXh0Y29sb3J7aW50ZXJuYXRpb25hbGtsZWluYmx1ZX17XHRleHRiZntPQlNFUlZBVElPTn06IFlvdXIgZ3Vlc3MgaGFzIDEgY29ycmVjdCBudW1iZXJzIGluIHRoZSB3cm9uZyB9QD4KPEBcdGV4dGNvbG9ye2ludGVybmF0aW9uYWxrbGVpbmJsdWV9e3Bvc2l0aW9uIGFuZCAwIGNvcnJlY3QgbnVtYmVycyBpbiB0aGUgY29ycmVjdCBwb3NpdGlvbi4gfUA+CjxAXHRleHRjb2xvcntpbnRlcm5hdGlvbmFsa2xlaW5ibHVlfXtLZWVwIGd1ZXNzaW5nLi4ufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtcdGV4dGJme1RIT1VHSFR9OiBPbmUgb2YgdGhlIGRpZ2l0cyAxLCAyLCAzLCBvciA0IGlzIGNvcnJlY3QgYnV0IGlufUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXt0aGUgd3JvbmcgcG9zaXRpb24uIEknbGwgdHJ5IHRvIG1vdmUgZWFjaCBkaWdpdCB0byBhfUA+CjxAXHRleHRjb2xvcnttYWdlbnRhfXtkaWZmZXJlbnQgcG9zaXRpb24gdG8gc2VlIGlmIEkgY2FuIGdldCBhIGNvcnJlY3QgcG9zaXRpb24ufUA+CjxAXHRleHRjb2xvcntnb2xkZW55ZWxsb3d9e1x0ZXh0YmZ7QUNUSU9OfTogMjE0M31APgouLi4=)环境：正在初始化新环境……观察：开始猜测4位数字代码。思考：我将从猜测1234开始，以了解这些数字中哪些在秘密数字中，以及哪些在正确的位置。行动：1234环境：将猜测的代码与真实情况进行比较……观察：你的猜测有1个数字在错误的位置，0个数字在正确的位置。继续猜测……思考：数字1、2、3或4中的一个是正确的，但位置错误。我将尝试将每个数字移动到不同的位置，以查看是否可以获得正确的位置。行动：2143……'
- en: 2.2 Benchmarking an agent
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 代理基准测试
- en: 'When evaluating agents performance on a benchmark, the following two metrics
    are commonly used Liu et al. ([2023](#bib.bib8)): (i) Success Rate (SR), i.e.
    the ratio of successful tasks to the total instances; (ii) Time to Success, i.e.
    the average time required to obtain a solution. While important and trending metrics Chalamalasetti
    et al. ([2023](#bib.bib1)); Hessel et al. ([2022](#bib.bib4)); Zhang et al. ([2020a](#bib.bib22)),
    they exclusively address the final success. They cannot measure intermediate success
    or failure and therefore make it difficult to understand why agents might systematically
    fail and how they can be improved. In contrast, we want to define intermediate
    metrics that allow us to easily assess and compare the performance of agents across
    a wide range of tasks.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估代理在基准上的表现时，通常使用以下两个指标 Liu et al. ([2023](#bib.bib8))：（i）成功率（SR），即成功任务与总实例的比率；（ii）成功时间，即获得解决方案所需的平均时间。虽然这些指标很重要且受到关注
    Chalamalasetti et al. ([2023](#bib.bib1))；Hessel et al. ([2022](#bib.bib4))；Zhang
    et al. ([2020a](#bib.bib22))，但它们仅关注最终成功。它们无法衡量中间成功或失败，因此很难理解代理系统性失败的原因以及如何改进。相比之下，我们希望定义中间指标，使我们能够轻松评估和比较代理在广泛任务中的表现。
- en: 3 AgentQuest Overview
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 代理任务概述
- en: 'We designed AgentQuest as a separation layer between agent and environment
    (see [Figure 1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). Essentially,
    it offers (i) a unified interface (i.e. the *driver*) ensuring compatibility between
    different agent architectures and benchmarks with minimal programming efforts
    (Section [3.1](#S3.SS1 "3.1 Benchmarks common interface ‣ 3 AgentQuest Overview
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks")); (ii) the implementation of two metrics beyond task success (i.e. *progress
    rate* and *repetition rate*) aimed at monitoring the agent advancement toward
    the final goal and allowing us to understand the reasons behind failures (Section
    [3.2](#S3.SS2 "3.2 Understanding agent advancements ‣ 3 AgentQuest Overview ‣
    AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks")); (iii) a unique vantage point and interface for implementing new metrics
    to monitoring and measuring the execution (Section [3.3](#S3.SS3 "3.3 Adding new
    metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了 AgentQuest 作为代理和环境之间的分隔层（见 [图 1(b)](#S1.F1.sf2 "在图 1 ‣ 1 引言 ‣ AgentQuest：在多步骤密集推理任务中基准测试
    LLM 代理行为")）。本质上，它提供了 (i) 一个统一的接口（即 *驱动程序*），确保不同代理架构和基准测试之间的兼容性，且编程工作量最小（第 [3.1](#S3.SS1
    "3.1 基准测试通用接口 ‣ 3 AgentQuest 概述 ‣ AgentQuest：在多步骤密集推理任务中基准测试 LLM 代理行为") 节）； (ii)
    超越任务成功的两个度量（即 *进展率* 和 *重复率*），旨在监测代理向最终目标的进展，并帮助我们理解失败的原因（第 [3.2](#S3.SS2 "3.2
    理解代理进展 ‣ 3 AgentQuest 概述 ‣ AgentQuest：在多步骤密集推理任务中基准测试 LLM 代理行为") 节）； (iii) 一个独特的视角和接口，用于实现新的度量来监测和衡量执行情况（第
    [3.3](#S3.SS3 "3.3 添加新度量 ‣ 3 AgentQuest 概述 ‣ AgentQuest：在多步骤密集推理任务中基准测试 LLM 代理行为")
    节）。
- en: 3.1 Benchmarks common interface
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基准测试通用接口
- en: Different benchmarks require invoking distinct functions, using specific formats,
    and performing parsing and post-processing of observations and agent actions.
    To integrate different agent architectures, the common trend is hardcoding such
    benchmark-specific requirements directly in the framework (Liu et al. [2023](#bib.bib8);
    Chalamalasetti et al. [2023](#bib.bib1), inter alia). This results in many custom
    interfaces tailored on each environment, making it difficult to easily move to
    other benchmarks and agent architectures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的基准测试需要调用不同的函数，使用特定的格式，并对观察结果和代理动作进行解析和后处理。为了整合不同的代理架构，常见的做法是在框架中硬编码这些基准特定的要求（Liu
    等 [2023](#bib.bib8)；Chalamalasetti 等 [2023](#bib.bib1)，等）。这导致了许多针对每个环境定制的接口，使得迁移到其他基准测试和代理架构变得困难。
- en: Instead, AgentQuest exposes a single unified Python interface, i.e. the Driver
    and two classes reflecting the agent-environment interaction components (i.e.
    Observation, Action).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，AgentQuest 提供了一个统一的 Python 接口，即 Driver 和两个反映代理-环境交互组件的类（即 Observation 和 Action）。
- en: Observations and actions.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察和动作。
- en: 'We provide two simple classes: Observation and Action. The first has two required
    attributes: (i) output, a string reporting information about the environment state;
    (ii) done, a Boolean variable indicating if the final task is currently accomplished
    or not. The Action class has one required attribute, action_value. It is a string
    directly output by the agent. Once processed and provided to the environment,
    it triggers the environment change. To customise the interactions, developers
    can define optional attributes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了两个简单的类：Observation 和 Action。Observation 类有两个必需的属性：(i) output，一个字符串，报告有关环境状态的信息；(ii)
    done，一个布尔变量，指示最终任务是否已完成。Action 类有一个必需的属性 action_value，它是代理直接输出的字符串。一旦处理并提供给环境，它将触发环境的变化。为了定制交互，开发者可以定义可选属性。
- en: Driver.
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 驱动程序。
- en: 'We provide the Driver class with two mandatory methods: (i) the reset method
    initialises a new instance of the environment and returns the first observation;
    (ii) the step method performs one single execution step. It accepts one instance
    of the Action class from the agent, processes the action (e.g. parses the action_value
    string) and uses it to modify the environment state. It always returns an observation.
    The driver supports also the benchmark-specific state attribute, acting as a simple
    API. It exposes the environment state at step $t$, useful to compute the progress
    rate.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了 Driver 类的两个必需方法：（i）reset 方法初始化环境的新实例并返回第一次观察；（ii）step 方法执行一次操作步骤。它接受来自代理的一个
    Action 类实例，处理该操作（例如解析 action_value 字符串）并使用它来修改环境状态。它总是返回一个观察。该驱动程序还支持基准特定状态属性，作为一个简单的
    API。它暴露了步骤 $t$ 的环境状态，有助于计算进展率。
- en: 'We here provide an example of the implemented interaction for Mastermind:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供了一个 Mastermind 实现交互的示例：
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0LnV0aWxzIGltcG9ydCBBY3Rpb24KZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IGdldF9wcm9ncmVzcywgZ2V0X3JlcGV0aXRpb24KCmFnZW50ID0gLi4uICMgSW5pdGlhbGl6ZSB5b3VyIGFnZW50CmFjdGlvbnMsIHByb2dyZXNzLCByZXBldGl0aW9ucyA9IFtdLCBbXSwgW10KIyBJbml0aWFsaXplIHRoZSBlbnZpcm9ubWVudCBhbmQgcmVzZXQgcm91bmQKZHJpdmVyID0gTWFzdGVyTWluZERyaXZlcih0cnV0aD0nNTYxOCcpCm9icyA9IGRyaXZlci5yZXNldCgpCiMgQWdlbnQgbG9vcAp3aGlsZSBub3Qgb2JzLmRvbmU6CiAgICBndWVzcyA9IGFnZW50KG9icy5vdXRwdXQpICMgR2V0IHRoZSBhZ2VudCBvdXRwdXQKICAgIGFjdGlvbiA9IEFjdGlvbihhY3Rpb25fdmFsdWU9Z3Vlc3MpICMgQ3JlYXRlIGFjdGlvbgogICAgYWN0aW9ucy5hcHBlbmQoYWN0aW9uLmFjdGlvbl92YWx1ZSkgIyBTdG9yZSBhY3Rpb24KICAgIG9icyA9IGRyaXZlci5zdGVwKGFjdGlvbikgIyBFeGVjdXRlIHN0ZXAKICAgICMgQ29tcHV0ZSBjdXJyZW50IHByb2dyZXNzIGFuZCByZXBldGl0aW9uCiAgICBwcm9ncmVzcy5hcHBlbmQoZ2V0X3Byb2dyZXNzKGRyaXZlci5zdGF0ZSwgJzU2MTgnKSkKICAgIHJlcGV0aXRpb25zLmFwcGVuZChnZXRfcmVwZXRpdGlvbnMoYWN0aW9ucykpCiAgICAjIEV4dGVuZCB3aXRoIHlvdXIgY3VzdG9tIG1ldHJpY3MgaGVyZSAuLi4KIyBDb21wdXRlIGZpbmFsIG1ldHJpY3MKUFIgPSBbeC9sZW4oJzU2MTgnKSBmb3IgeCBpbiBwcm9ncmVzc10KUlIgPSBbeC8obGVuKGFjdGlvbnMpLTEpIGZvciB4IGluIHJlcGV0aXRpb25zXQ==)from  agentquest.drivers  import  MasterMindDriverfrom  agentquest.utils  import  Actionfrom  agentquest.metrics  import  get_progress,  get_repetitionagent  =  ...  #  Initialize  your  agentactions,  progress,  repetitions  =  [],  [],  []#  Initialize  the  environment  and  reset  rounddriver  =  MasterMindDriver(truth=’5618’)obs  =  driver.reset()#  Agent  loopwhile  not  obs.done:guess  =  agent(obs.output)  #  Get  the  agent  outputaction  =  Action(action_value=guess)  #  Create  actionactions.append(action.action_value)  #  Store  actionobs  =  driver.step(action)  #  Execute  step#  Compute  current  progress  and  repetitionprogress.append(get_progress(driver.state,  ’5618’))repetitions.append(get_repetitions(actions))#  Extend  with  your  custom  metrics  here  ...#  Compute  final  metricsPR  =  [x/len(’5618’)  for  x  in  progress]RR  =  [x/(len(actions)-1)  for  x  in  repetitions]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmR...'
- en: 3.2 Understanding agent advancements
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 理解代理的进展
- en: Getting insights on how they tackle a specific task is key to comprehend agent
    behaviours, capabilities and limitations. Furthermore, identifying systematic
    agent failures allows to pinpoint necessary adjustments within the architecture
    to effectively address the underlying issues.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 了解他们如何处理特定任务是理解代理行为、能力和局限性的关键。此外，识别系统性代理失败有助于确定架构中需要调整的部分，以有效解决潜在问题。
- en: AgentQuest contributes towards this direction introducing two cross-benchmark
    metrics, the *progress rate* and the *repetition rate*. While the first expresses
    *how much* the agent is advancing towards the final goal, the latter indicates
    *how* it is reaching it, with a specific focus on the amount of repeated (i.e.
    similar) actions the agent performs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest 在这一方向上做出了贡献，引入了两个跨基准的指标，即 *进展率* 和 *重复率*。前者表示代理朝向最终目标的 *进展程度*，而后者则指示代理如何达到目标，特别关注代理执行的重复（即相似）动作的数量。
- en: Milestones and progress rate.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 里程碑和进展率。
- en: To quantify the agent advancement towards the final goal, AgentQuest uses a
    set of *milestones* $\mathcal{M}$ the evaluation coincides with the success rate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化代理朝向最终目标的进展，AgentQuest 使用一组 *里程碑* $\mathcal{M}$，评估结果与成功率一致。
- en: We assign a score to all the states included in $\mathcal{M}$ dependant of such
    scoring function, as an indication of how far the agent is from the goal, allowing
    to track agent progress over time. Depending on the benchmark, the progress rate
    might also decrease during the execution. Milestones can either be manually annotated,
    or internally computed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为所有包含在 $\mathcal{M}$ 中的状态分配一个分数，作为代理距离目标的指示，允许跟踪代理的进展。根据基准的不同，进展率在执行过程中也可能会下降。里程碑可以是手动标注的，也可以是内部计算的。
- en: Repetition rate.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重复率。
- en: 'The repetition rate $\text{RR}_{t}$ is a measure of the agent tendency of repeating
    actions. Depending on the benchmark, we do not consider repetitions as a limitation
    – e.g. solving a maze requires repetitions, such as going left repeatedly. See
    also [Section 4](#S4 "4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM
    Agents Behaviours in Multi-step Intensive Reasoning Tasks") for a positive and
    negative example of repetitions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '重复率 $\text{RR}_{t}$ 是衡量代理重复动作倾向的指标。根据基准的不同，我们不将重复视为限制——例如，解决迷宫需要重复动作，如反复向左转。有关重复的正面和负面示例，请参见
    [第 4 节](#S4 "4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")。'
- en: At execution step $t$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行步骤 $t$。
- en: Based on this, we define the repetition rate at step $t$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们定义了步骤 $t$ 的重复率。
- en: 3.3 Adding new metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 添加新指标
- en: 'Table 1: Attributes exposing components of the agent-environment interaction
    useful to define new metrics.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：暴露代理-环境交互组件的属性，用于定义新指标。
- en: '| Class | Attribute | Access to |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 属性 | 访问 |'
- en: '| Driver | state | Hidden states |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Driver | 状态 | 隐藏状态 |'
- en: '| Observation | output | Observations |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 观察 | 输出 | 观察结果 |'
- en: '| Action | action$\_$value | Agent actions |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 行动$\_$值 | 代理动作 |'
- en: We rely on the progress and repetition rates to show how AgentQuest can be extended
    with new metrics through a simple function template. We then show the implementations
    of the functions adapted to the considered benchmark.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依靠进展率和重复率来展示如何通过简单的函数模板扩展 AgentQuest。然后，我们展示了适应于所考虑基准的函数实现。
- en: Metric function template.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标函数模板。
- en: 'We use a Python function template to easily define the elements of the agent-environment
    interactions required for computing a given metric. [Table 1](#S3.T1 "In 3.3 Adding
    new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks") provides a recap of the main attributes
    and reference classes that can be used as input for the custom metrics. Additionally,
    users can provide external data, like milestones or action history.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Python 函数模板轻松定义计算给定指标所需的代理-环境交互元素。 [表 1](#S3.T1 "In 3.3 Adding new metrics
    ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") 提供了可以用作自定义指标输入的主要属性和参考类的回顾。此外，用户还可以提供外部数据，如里程碑或行动历史。'
- en: 'Table 2: Overview of the benchmarks provided in AgentQuest.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：AgentQuest 提供的基准概述。
- en: '| Benchmark | Description | Milestones |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 描述 | 里程碑 |'
- en: '| Mastermind |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind |'
- en: '&#124; Guessing a numeric code with feedback on guessed digits and positions.
    &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过反馈猜测数字代码的数字和位置。 &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Digits of the code to guess. &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测代码的数字。 &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LTP |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LTP |'
- en: '&#124; Solving riddles by asking Yes/No questions. &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过提问“是/否”来解谜。 &#124;'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Guessed riddle key aspects. &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测谜题的关键方面。 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ALFWorld |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld |'
- en: '&#124; Finding an object in a textual world and using it. &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在文本世界中找到一个对象并使用它。 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Sequence of actions. &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动作序列。 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sudoku |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Sudoku |'
- en: '&#124; 9x9 grid puzzle. Digits 1-9 fill each column, row, and 3x3 sub-grid
    &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9x9 网格谜题。数字 1-9 填充每一列、每一行和每个 3x3 子网格 &#124;'
- en: '&#124; without repetition. &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不重复。 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total number of correct &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总的正确数量 &#124;'
- en: '&#124; inserted digits. &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 插入的数字。 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Implement progress rate.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现进度率。
- en: 'Depending on the benchmark, developers need to implement the custom scoring
    function $f$. Milestones can either be user-defined or internally computed within
    get_progress. Here, we show the definition of get_progress to quantify the achieved
    milestones for Mastermind. The milestones are the digits of the final solution
    and the progress indicates the count of correctly guessed digits in their positions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据基准，开发者需要实现自定义评分函数 $f$。里程碑可以是用户定义的，也可以是 get_progress 内部计算得到的。这里，我们展示了 get_progress
    的定义，用于量化 Mastermind 游戏中的已实现里程碑。里程碑是最终解决方案的数字，而进度表示正确猜测的数字在其位置上的数量：
- en: '[⬇](data:text/plain;base64,ZGVmIGdldF9wcm9ncmVzcyhzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICByZWFjaGVkX21pbGVzdG9uZXMgPSAwICMgRGlnaXRzIGluIGNvcnJlY3QgcG9zaXRpb24KICAgIGZvciBpLCBqIGluIHppcChzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICAgICAgaWYgaSA9PSBqOiByZWFjaGVkX21pbGVzdG9uZXMgKz0gMQogICAgcmV0dXJuIHJlYWNoZWRfbWlsZXN0b25lcwoKIyBVc2FnZSBleGFtcGxlLiBUaGUgY29kZSB0byBndWVzcyBpcyAnNTYxOCcKcHJvZ3Jlc3MgPSBnZXRfcHJvZ3Jlc3MoJzIzMTgnLCAnNTYxOCcpICMgUmVhY2hlZCBtaWxlc3RvbmVzCj4+PiAyCnByb2dyZXNzL2xlbignNTYxOCcpICMgQ29tcHV0ZSBQcm9ncmVzcyBSYXRlCj4+PiAwLjU=)def  get_progress(state,  milestones):reached_milestones  =  0  #  Digits  in  correct  positionfor  i,  j  in  zip(state,  milestones):if  i  ==  j:  reached_milestones  +=  1return  reached_milestones#  Usage  example.  The  code  to  guess  is  ’5618’progress  =  get_progress(’2318’,  ’5618’)  #  Reached  milestones>>>  2progress/len(’5618’)  #  Compute  Progress  Rate>>>  0.5'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZGVmIGdldF9wcm9ncmVzcyhzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICByZWFjaGVkX21pbGVzdG9uZXMgPSAwICMgRGlnaXRzIGluIGNvcnJlY3QgcG9zaXRpb24KICAgIGZvciBpLCBqIGluIHppcChzdGF0ZSwgbWlsZXN0b25lcyk6CiAgICAgICAgaWYgaSA9PSBqOiByZWFjaGVkX21pbGVzdG9uZXMgKz0gMQogICAgcmV0dXJuIHJlYWNoZWRfbWlsZXN0b25lcwoKIyBVc2FnZSBleGFtcGxlLiBUaGUgY29kZSB0byBndWVzcyBpcyAnNTYxOCcKcHJvZ3Jlc3MgPSBnZXRfcHJvZ3Jlc3MoJzIzMTgnLCAnNTYxOCcpICMgUmVhY2hlZCBtaWxlc3RvbmVzCj4+PiAyCnByb2dyZXNzL2xlbignNTYxOCcpICMgQ29tcHV0ZSBQcm9ncmVzcyBSYXRlCj4+PiAwLjU=)def  get_progress(state,  milestones):reached_milestones  =  0  #  Digits  in  correct  positionfor  i,  j  in  zip(state,  milestones):if  i  ==  j:  reached_milestones  +=  1return  reached_milestones#  Usage  example.  The  code  to  guess  is  ’5618’progress  =  get_progress(’2318’,  ’5618’)  #  Reached  milestones>>>  2progress/len(’5618’)  #  Compute  Progress  Rate>>>  0.5'
- en: Implement repetition rate.
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现重复率。
- en: To determine if an action is repeated, the end user must define the similarity
    function $g$ is the Levenshtein similarity Levenshtein ([1966](#bib.bib6)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定一个动作是否被重复，最终用户必须定义相似度函数 $g$，该函数是 Levenshtein 相似度 Levenshtein ([1966](#bib.bib6))。
- en: '[⬇](data:text/plain;base64,ZnJvbSBMZXZlbnNodGVpbiBpbXBvcnQgcmF0aW8gYXMgZwoKZGVmIGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCBUSEVUQV9BKToKICAgIHVuaXF1ZV9hY3QgPSBzZXQoKSAjIEluaXRpYWxpc2UgdW5pcXVlIGFjdGlvbnMKICAgIGZvciBpLGEgaW4gZW51bWVyYXRlKGFjdGlvbnMpOgogICAgICAgICMgQ2hlY2sgZm9yIHJlcGV0aXRpb25zCiAgICAgICAgaWYgYWxsKFtnKGEsYWN0aW9uc1t4XSk8VEhFVEFfQSBmb3IgeCBpbiByYW5nZShpKV0pOgogICAgICAgICAgICB1bmlxdWVfYWN0LmFkZChhKQogICAgcmV0dXJuIGxlbihhY3Rpb25zKS1sZW4odW5pcXVlX2FjdCkKCiMgVXNhZ2UgZXhhbXBsZS4gVGhlIGNvZGUgdG8gZ3Vlc3MgaXMgJzU2MTgnCmFjdGlvbnMgPSBbJzEyMzQnLCAnMjE0MycsICcxMjM0JywgJzU2MTgnXSAjIEFjdGlvbnMgaGlzdG9yeQpyZXBldGl0aW9ucyA9IGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCAxLjApCj4+PiAxIHJlcGVhdGVkIGFjdGlvbgojIENvbXB1dGUgUmVwZXRpdGlvbiBSYXRlCnJlcGV0aXRpb25zLyhsZW4oYWN0aW9ucyktMSkKPj4+IDAuMzM=)from  Levenshtein  import  ratio  as  gdef  get_repetitions(actions,  THETA_A):unique_act  =  set()  #  Initialise  unique  actionsfor  i,a  in  enumerate(actions):#  Check  for  repetitionsif  all([g(a,actions[x])<THETA_A  for  x  in  range(i)]):unique_act.add(a)return  len(actions)-len(unique_act)#  Usage  example.  The  code  to  guess  is  ’5618’actions  =  [’1234’,  ’2143’,  ’1234’,  ’5618’]  #  Actions  historyrepetitions  =  get_repetitions(actions,  1.0)>>>  1  repeated  action#  Compute  Repetition  Raterepetitions/(len(actions)-1)>>>  0.33'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBMZXZlbnNodGVpbiBpbXBvcnQgcmF0aW8gYXMgZwoKZGVmIGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCBUSEVUQV9BKToKICAgIHVuaXF1ZV9hY3QgPSBzZXQoKSAjIEluaXRpYWxpc2UgdW5pcXVlIGFjdGlvbnMKICAgIGZvciBpLGEgaW4gZW51bWVyYXRlKGFjdGlvbnMpOgogICAgICAgICMgQ2hlY2sgZm9yIHJlcGV0aXRpb25zCiAgICAgICAgaWYgYWxsKFtnKGEsYWN0aW9uc1t4XSk8VEhFVEFfQSBmb3IgeCBpbiByYW5nZShpKV0pOgogICAgICAgICAgICB1bmlxdWVfYWN0LmFkZChhKQogICAgcmV0dXJuIGxlbihhY3Rpb25zKS1sZW4odW5pcXVlX2FjdCkKCiMgVXNhZ2UgZXhhbXBsZS4gVGhlIGNvZGUgdG8gZ3Vlc3MgaXMgJzU2MTgnCmFjdGlvbnMgPSBbJzEyMzQnLCAnMjE0MycsICcxMjM0JywgJzU2MTgnXSAjIEFjdGlvbnMgaGlzdG9yeQpyZXBldGl0aW9ucyA9IGdldF9yZXBldGl0aW9ucyhhY3Rpb25zLCAxLjApCj4+PiAxIHJlcGVhdGVkIGFjdGlvbgojIENvbXB1dGUgUmVwZXRpdGlvbiBSYXRlCnJlcGV0aXRpb25zLyhsZW4oYWN0aW9ucyktMSkKPj4+IDAuMzM=)来自
    Levenshtein 导入 ratio 作为 gdef 获取重复项(actions, THETA_A):unique_act = set()  # 初始化唯一操作对于
    i,a 在 enumerate(actions)中: # 检查重复项如果所有([g(a,actions[x])<THETA_A  for  x  in  range(i)]):unique_act.add(a)return  len(actions)-len(unique_act)
    # 用法示例。代码待猜测为 ''5618''actions = [''1234'', ''2143'', ''1234'', ''5618'']  # 操作历史repetitions
    = get_repetitions(actions, 1.0)>>>  1  次重复的操作 # 计算重复率repetitions/(len(actions)-1)>>>  0.33'
- en: In other cases, where $a$ can be any text string, we can use standard metrics,
    such as BLEU Papineni et al. ([2002](#bib.bib12)), ROUGE Lin ([2004](#bib.bib7))
    or BERTScore Zhang et al. ([2020b](#bib.bib23)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，其中 $a$ 可以是任何文本字符串，我们可以使用标准度量，如 BLEU Papineni et al. ([2002](#bib.bib12))、ROUGE
    Lin ([2004](#bib.bib7)) 或 BERTScore Zhang et al. ([2020b](#bib.bib23))。
- en: 4 Insights via AgentQuest
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 AgentQuest 的 4 个见解
- en: 'We investigate agent behaviours in different reasoning scenarios by proposing
    a starting set of four benchmarks. We implemented from scratch Sudoku Felgenhauer
    and Jarvis ([2006](#bib.bib3)) and Mastermind Stuckman and Zhang ([2005](#bib.bib17))
    environments, while ALFWorld Shridhar et al. ([2020](#bib.bib15)) and Lateral
    Thinking Puzzles (LTP)Sloane ([1992](#bib.bib16)) are existing implementations Liu
    et al. ([2023](#bib.bib8)). [Table 2](#S3.T2 "In Metric function template. ‣ 3.3
    Adding new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents
    Behaviours in Multi-step Intensive Reasoning Tasks") provides an overview of the
    benchmarks and their respective milestones used to measure progress.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过提出四个基准的起始集来研究代理在不同推理场景中的行为。我们从头实现了数独 Felgenhauer 和 Jarvis ([2006](#bib.bib3))
    和 Mastermind Stuckman 和 Zhang ([2005](#bib.bib17)) 环境，而 ALFWorld Shridhar et al.
    ([2020](#bib.bib15)) 和 lateral Thinking Puzzles (LTP) Sloane ([1992](#bib.bib16))
    是现有的实现 Liu et al. ([2023](#bib.bib8))。[表 2](#S3.T2 "在度量函数模板中。 ‣ 3.3 添加新度量 ‣ 3
    AgentQuest 概述 ‣ AgentQuest: 在多步骤密集推理任务中基准测试 LLM 代理行为") 提供了基准和其各自里程碑的概述，用于测量进展。'
- en: We emphasise that this evaluation is not aimed at providing a thorough evaluation
    and comparison of agent architectures, but rather to show how to use AgentQuest
    and how monitoring progress and action repetition can provide relevant insights
    to developers, even after a few executions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调，这次评估并非旨在提供对代理架构的全面评估和比较，而是展示如何使用 AgentQuest 以及如何通过监控进度和操作重复来为开发人员提供相关见解，即使在几次执行后。
- en: Experimental setup.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: 'We use as reference architecture the off-the-shelf chat agent provided by LangChain Chase
    ([2022](#bib.bib2)) powered by GPT-4 OpenAI ([2023b](#bib.bib11)) as LLM because
    it is intuitive, easy to extend and open source. We run 15 instances of the four
    benchmarks within AgentQuest, setting the maximum number of execution steps as
    60⁵⁵5We limit the number of instances in our experiments for two main reasons:
    (i) the work primarily serves as a demonstration of the developed framework itself,
    rather than an extensive evaluation of the agent performance; (ii) extensive tests
    could have significantly impacted the ability to reproduce the experiments due
    to the expensive nature of API calls.. In Appendix [B](#A2 "Appendix B Appendix:
    Additional agents architectures and benchmarks ‣ AgentQuest: Benchmarking LLM
    Agents Behaviours in Multi-step Intensive Reasoning Tasks") we provide examples
    on how to use AgentQuest with two additional agent architectures and GAIA Mialon
    et al. ([2023](#bib.bib9)) as open-ended environment.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 LangChain Chase 提供的现成聊天代理（[2022](#bib.bib2)），它由 GPT-4 OpenAI ([2023b](#bib.bib11))
    提供支持，作为 LLM，因为它直观、易于扩展且开源。我们在 AgentQuest 中运行了 15 个四个基准测试的实例，将最大执行步骤设置为 60⁵⁵5。我们在实验中限制了实例的数量，主要有两个原因：（i）这项工作主要作为开发框架本身的演示，而不是对代理性能的广泛评估；（ii）广泛的测试可能会显著影响实验的可重现性，因为
    API 调用的成本很高。在附录 [B](#A2 "附录 B 附录：附加代理架构和基准 ‣ AgentQuest：在多步骤密集推理任务中基准化 LLM 代理行为")中，我们提供了如何使用
    AgentQuest 进行两个额外代理架构和 GAIA Mialon et al. ([2023](#bib.bib9)) 作为开放式环境的示例。
- en: 'Table 3: Average existing and proposed metrics for the tested benchmarks. We
    report the metrics, Success Rate (SR), Steps, Progress Rate at step 60 (PR[60])
    and Repetition Rate at final step 60 (RR[60]). We denote with ^∗ the improved
    results after modifying the agent architecture.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：测试基准的平均现有和提议指标。我们报告了指标、成功率（SR）、步骤、60 步时的进展率（PR[60]）和最终步骤 60 的重复率（RR[60]）。我们用
    ^∗ 表示在修改代理架构后改进的结果。
- en: '|  | Existing Metrics | AgentQuest |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 现有指标 | AgentQuest |'
- en: '|  | SR | Steps | PR[60] | RR[60] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | SR | 步骤 | PR[60] | RR[60] |'
- en: '| Mastermind | 0.47 | 41.87 | 0.62 | 0.32 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind | 0.47 | 41.87 | 0.62 | 0.32 |'
- en: '| LTP | 0.20 | 52.00 | 0.46 | 0.81 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LTP | 0.20 | 52.00 | 0.46 | 0.81 |'
- en: '| ALFWorld | 0.86 | 21.00 | 0.74 | 0.06 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld | 0.86 | 21.00 | 0.74 | 0.06 |'
- en: '| Sudoku | 0.00 | 59.67 | 0.08 | 0.22 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 数独 | 0.00 | 59.67 | 0.08 | 0.22 |'
- en: '| Mastermind^∗ | 0.60 | 39.73 | 0.73 | 0.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind^∗ | 0.60 | 39.73 | 0.73 | 0.00 |'
- en: '| ALFWorld^∗ | 0.93 | 25.86 | 0.80^† | 0.07^† |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld^∗ | 0.93 | 25.86 | 0.80^† | 0.07^† |'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ^†Metrics referred to the extended runtime up to 120 &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ^†指标参考了扩展运行时间至 120 &#124;'
- en: '&#124; steps, hence PR[120] and RR[120]. &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 步骤，因此 PR[120] 和 RR[120]。 &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Experimental results.
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验结果。
- en: 'For Mastermind, [Figure 2(a)](#S4.F2.sf1 "In Figure 2 ‣ Experimental results.
    ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks") shows the progress rate PR[t] and repetition
    rate RR[t]. In the first 22 steps, the agent explores different solutions (RR${}_{[0,22]}<5\%$.
    Hence, AgentQuest offered us a crucial insights on why the current agent cannot
    solve the Mastermind game.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mastermind，[图 2(a)](#S4.F2.sf1 "图 2 ‣ 实验结果。 ‣ 通过 AgentQuest 的 4 个洞察 ‣ AgentQuest：在多步骤密集推理任务中基准化
    LLM 代理行为") 显示了进展率 PR[t] 和重复率 RR[t]。在前 22 步中，代理探索不同的解决方案（RR${}_{[0,22]}<5\%$）。因此，AgentQuest
    为我们提供了一个关键的洞察，解释了为什么当前代理无法解决 Mastermind 游戏。
- en: 'To overcome this agent limitation we incorporate a memory component Park et al.
    ([2023](#bib.bib13)) into the agent architecture. The agent stores the past guesses
    in a local buffer. Then, at each step, if the agent outputs an action already
    in the buffer, it is prompted to provide a new one. [Table 3](#S4.T3 "In Experimental
    setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks") (Mastermind^∗) shows that this simple
    change in agent architecture has a big impact: the agent can now solve more instances,
    increasing the final SR from 47% to 60% and preventing repetitions (RR${}_{60}=0\%$).
    This highlights how studying the interplay between progress and repetition rates
    can allow us to improve agent architecture, sometimes even with simple remedies.
    We support our intuition extending the evaluation to more instances of Mastermind
    from 15 to 60 achieving comparable results – i.e. 43% of SR with the standard
    architecture and 62% with the simple memory (19% of improvement).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这种代理限制，我们在代理架构中加入了一个记忆组件，Park 等人（[2023](#bib.bib13)）提出。代理将过去的猜测存储在本地缓冲区中。然后，在每一步，如果代理输出一个已在缓冲区中的动作，则会被提示提供一个新的动作。[表
    3](#S4.T3 "在实验设置中。 ‣ 4 通过 AgentQuest 获取的见解 ‣ AgentQuest：基准测试 LLM 代理在多步骤密集推理任务中的行为")（Mastermind^∗）显示，这种简单的代理架构变更产生了巨大的影响：代理现在可以解决更多实例，将最终
    SR 从 47% 提高到 60%，并防止重复（RR${}_{60}=0\%$）。这突显了研究进展与重复率之间的相互作用如何使我们能够改进代理架构，有时甚至通过简单的修补方法。我们通过将评估扩展到更多
    Mastermind 实例，从 15 个增加到 60 个，得到了相似的结果——即，标准架构下 SR 为 43%，简单记忆下 SR 为 62%（提高了 19%）。
- en: '![Refer to caption](img/b20ba5b73ec0042b2e99ab2dd15c2f0c.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b20ba5b73ec0042b2e99ab2dd15c2f0c.png)'
- en: (a) Mastermind
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mastermind
- en: '![Refer to caption](img/a1b8e48e5effd4a6aec9771ca534f737.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a1b8e48e5effd4a6aec9771ca534f737.png)'
- en: (b) LTP
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LTP
- en: 'Figure 2: Average Progress rate PR[t] and the repetition rate RR[t] on Mastermind
    and LTP. Mastermind: It starts out with a low RR[t] but this increases after step
    22 while the progress rate also stall at 55%. LTP: at first a higher RR[t] allows
    the agent to progress by making small variations that lead to success, but later
    this plateaus.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Mastermind 和 LTP 上的平均进展率 PR[t] 和重复率 RR[t]。Mastermind：最初 RR[t] 较低，但在第 22
    步后增加，同时进展率也停滞在 55%。LTP：最初较高的 RR[t] 允许代理通过进行小的变动取得进展，但后来停滞不前。
- en: '![Refer to caption](img/57f89da4c4982e92db9e5f22a6851ae2.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57f89da4c4982e92db9e5f22a6851ae2.png)'
- en: (a) Mastermind
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mastermind
- en: '![Refer to caption](img/777fd447306968fed1abafca68031cb9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/777fd447306968fed1abafca68031cb9.png)'
- en: (b) LTP
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LTP
- en: 'Figure 3: Examples of repeated actions in Mastermind and LTP. Mastermind: there
    is a set of unique actions at first, but then gets stuck repeating the same actions
    over and over. LTP: repeated actions are small variations of the same question
    that lead to progress.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Mastermind 和 LTP 中重复动作的示例。Mastermind：最开始有一组独特的动作，但随后反复重复相同的动作。LTP：重复动作是相同问题的小变化，导致进展。
- en: 'For LTP, the AgentQuest metrics reveal a different agent behaviour, where repetitions
    are part of the agent reasoning strategy, enhancing the progress rate ([Figure 2(b)](#S4.F2.sf2
    "In Figure 2 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")).
    From the initial steps, the agent changes aspects of the same questions until
    a local solution emerges. This leads to horizontal indicators in [Figure 3(b)](#S4.F3.sf2
    "In Figure 3 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") and
    RR${}_{20}\approx 30\%.$. This shows us how the interplay of progress and repetition
    rates provides an insight on how agents behave across the different time steps.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LTP，AgentQuest 指标揭示了不同的代理行为，其中重复是代理推理策略的一部分，提升了进展率（[图 2(b)](#S4.F2.sf2 "在图
    2 中 ‣ 实验结果。 ‣ 4 通过 AgentQuest 获取的见解 ‣ AgentQuest：基准测试 LLM 代理在多步骤密集推理任务中的行为")）。从初始步骤开始，代理改变相同问题的各个方面，直到出现局部解决方案。这导致了[图
    3(b)](#S4.F3.sf2 "在图 3 中 ‣ 实验结果。 ‣ 4 通过 AgentQuest 获取的见解 ‣ AgentQuest：基准测试 LLM
    代理在多步骤密集推理任务中的行为")中的水平指标和 RR${}_{20}\approx 30\%$。这向我们展示了进展和重复率的相互作用如何提供关于代理在不同时间步骤中行为的见解。
- en: 'Consider the benchmark ALFWorld in [Table 3](#S4.T3 "In Experimental setup.
    ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks") (we report the metrics trend in Appendix
    [A](#A1 "Appendix A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). It requires
    the exploration of a textual world to locate an object. While the agent explores
    the solution space and limits action repetitions (RR${}_{60}=6\%$). This discrepancy
    may arise from the more exploration steps required to discover the object. We
    support this intuition extending the benchmark runtime to 120 steps resulting
    in a success and progress rates increase by 6% (ALFWorld^∗ in [Table 3](#S4.T3
    "In Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). This confirms
    the usefulness of AgentQuest in understanding the agent failures. We support our
    intuition also extending the evaluation to more instances of ALFWorld from 15
    to 60 achieving comparable results – i.e. 83% of SR with 60 steps as limit and
    87% with 120 steps as limit (4% of improvement).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见基准 ALFWorld 在 [表 3](#S4.T3 "在实验设置中。 ‣ 通过 AgentQuest 的 4 个见解 ‣ AgentQuest：多步骤密集推理任务中的
    LLM 代理行为基准测试")（我们在附录 [A](#A1 "附录 A 附录：ALFWorld 和数独基准 ‣ AgentQuest：多步骤密集推理任务中的
    LLM 代理行为基准测试") 中报告了指标趋势）。它要求探索一个文本世界以定位一个对象。虽然代理在探索解决方案空间并限制动作重复（RR${}_{60}=6\%$）。这种差异可能源于发现对象所需的更多探索步骤。我们通过将基准运行时间延长至
    120 步来支持这一直觉，结果成功率和进展率分别增加了 6%（ALFWorld^∗ 在 [表 3](#S4.T3 "在实验设置中。 ‣ 通过 AgentQuest
    的 4 个见解 ‣ AgentQuest：多步骤密集推理任务中的 LLM 代理行为基准测试")）。这证实了 AgentQuest 在理解代理失败方面的有效性。我们还通过将评估扩展到更多
    ALFWorld 实例，从 15 个增加到 60 个，获得了可比的结果——即 60 步限制下成功率为 83%，120 步限制下为 87%（提高了 4%）。
- en: 'Finally, we look at Sudoku, known for its high level of difficulty Felgenhauer
    and Jarvis ([2006](#bib.bib3)). The low progress and repetition rates achieved
    after 60 steps (PR${}_{60}=8\%$) indicate that the current agent architecture
    struggles in finding correct solutions solving this task. We report the metrics
    trend in Appendix [A](#A1 "Appendix A Appendix: ALFWorld and Sudoku benchmarks
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看一下数独，以其高难度著称 Felgenhauer 和 Jarvis ([2006](#bib.bib3))。在 60 步之后取得的低进展和重复率（PR${}_{60}=8\%$）表明当前代理架构在解决此任务时难以找到正确解决方案。我们在附录
    [A](#A1 "附录 A 附录：ALFWorld 和数独基准 ‣ AgentQuest：多步骤密集推理任务中的 LLM 代理行为基准测试") 中报告了指标趋势。
- en: 5 Conclusions
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: AgentQuest allows the research community to keep track of agent progress in
    a holistic manner. Starting out with a first set of four benchmarks and two new
    metrics, AgentQuest is easily extendable. Furthermore, the two proposed metrics,
    progress and repetition rates, have the great advantage of allowing to track how
    agents advance toward the final goal over time. Especially studying their interplay
    can lead to important insights that will allow the research community to improve
    agent performance. Finally, we believe that promptly sharing AgentQuest with the
    research community will facilitate benchmarking and debugging agents, and will
    foster the creation and use of new benchmarks and metrics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest 允许研究社区以整体的方式跟踪代理的进展。起初，AgentQuest 提供了一套四个基准和两个新指标，且其可扩展性强。此外，两个提出的指标——进展率和重复率，具有跟踪代理如何随着时间推移向最终目标推进的重大优势。特别是研究它们的相互作用可以带来重要的见解，从而帮助研究社区提升代理的表现。最后，我们相信，及时与研究社区分享
    AgentQuest 将有助于基准测试和调试代理，并促进新基准和指标的创建与使用。
- en: Ethical Considerations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: The complexity of LLM agents poses challenges in comprehending their decision-making
    processes. Ethical guidelines must demand transparency in such systems, ensuring
    that developers and end-users comprehend how decisions are reached.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理的复杂性给理解其决策过程带来了挑战。伦理指南必须要求这种系统的透明性，确保开发者和最终用户理解决策是如何达成的。
- en: We are not aware of any direct ethical impact generated by our work. However,
    we hope that insights into Generative AI agents’ decision-making processes will
    be applied to improve and promote transparency and fairness.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们未发现我们的工作直接产生的伦理影响。然而，我们希望对生成式 AI 代理决策过程的见解能被应用于提升和促进透明性与公平性。
- en: Acknowledgements
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This project has received funding from the European Union’s Horizon Europe research
    and innovation programme (SNS-JU) under the Grant Agreement No 101139285 (“NATWORK”).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目获得了欧洲联盟地平线欧洲研究和创新计划（SNS-JU）下的资助，资助协议号 101139285（“NATWORK”）。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chalamalasetti et al. (2023) Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov,
    Brielen Madureira, Philipp Sadler, and David Schlangen. 2023. [Clembench: Using
    Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents](http://arxiv.org/abs/2305.13455).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查拉马拉塞提等（2023）克兰提·查拉马拉塞提、贾娜·戈策、谢尔佐德·哈基莫夫、布里伦·马杜雷拉、菲利普·萨德勒和大卫·施兰根。2023. [Clembench：使用游戏玩法评估优化对话的语言模型作为对话代理](http://arxiv.org/abs/2305.13455)。
- en: Chase (2022) Harrison Chase. 2022. [LangChain - Building applications with LLMs
    through composability](https://github.com/langchain-ai/langchain).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡斯（2022）哈里森·蔡斯。2022. [LangChain - 通过可组合性构建应用程序](https://github.com/langchain-ai/langchain)。
- en: Felgenhauer and Jarvis (2006) Bertram Felgenhauer and Frazer Jarvis. 2006. Mathematics
    of Sudoku I. *Mathematical Spectrum*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 费尔根豪斯和贾维斯（2006）伯特朗·费尔根豪斯和弗雷泽·贾维斯。2006. 数独的数学 I。*数学光谱*。
- en: 'Hessel et al. (2022) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2022. [CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning](http://arxiv.org/abs/2104.08718).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赫塞尔等（2022）杰克·赫塞尔、阿里·霍尔茨曼、麦克斯韦·福布斯、罗南·勒布拉斯和叶进·崔。2022. [CLIPScore：一种无参考的图像字幕评估指标](http://arxiv.org/abs/2104.08718)。
- en: Kiela et al. (2023) Douwe Kiela, Tristan Thrush, Kawin Ethayarajh, and Amanpreet
    Singh. 2023. [Plotting Progress in AI](https://contextual.ai/plotting-progress-in-ai/).
    *Contextual AI Blog*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基拉等（2023）道威·基拉、特里斯坦·瑟什、卡温·埃泰亚拉赫和阿曼普里特·辛格。2023. [AI 进展的绘制](https://contextual.ai/plotting-progress-in-ai/)。*Contextual
    AI 博客*。
- en: Levenshtein (1966) Vladimir I. Levenshtein. 1966. Binary codes capable of correcting
    deletions, insertions, and reversals. In *Soviet Physics Doklady*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列文斯坦（1966）弗拉基米尔·I·列文斯坦。1966. 能够纠正删除、插入和反转的二进制代码。发表于 *苏联物理学报告*。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A Package for Automatic Evaluation of
    Summaries](https://aclanthology.org/W04-1013).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林（2004）林钦耀。2004. [ROUGE：一种自动评估摘要的软件包](https://aclanthology.org/W04-1013)。
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. [AgentBench:
    Evaluating LLMs as Agents](http://arxiv.org/abs/2308.03688).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等（2023）肖刘、浩宇、汉辰张、逸凡徐、轩宇雷、汉宇赖、余古、航梁丁、凯文门、克娟杨等。2023. [AgentBench: 评估大型语言模型作为智能体](http://arxiv.org/abs/2308.03688)。'
- en: 'Mialon et al. (2023) Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. 2023. [GAIA: a benchmark for General AI
    Assistants](http://arxiv.org/abs/2311.12983).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米阿隆等（2023）格雷戈尔·米阿隆、克莱门汀·富里耶、克雷格·斯威夫特、托马斯·沃尔夫、扬·勒昆和托马斯·斯基亚隆。2023. [GAIA：通用 AI
    助手的基准](http://arxiv.org/abs/2311.12983)。
- en: OpenAI (2023a) OpenAI. 2023a. [Assistants API](https://platform.openai.com/docs/assistants/overview).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI。2023a. [助理 API](https://platform.openai.com/docs/assistants/overview)。
- en: OpenAI (2023b) OpenAI. 2023b. [GPT-4 Technical Report](http://arxiv.org/abs/2303.08774).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023b）OpenAI。2023b. [GPT-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu: a Method for Automatic Evaluation of Machine Translation](https://doi.org/10.3115/1073083.1073135).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕皮内尼等（2002）基肖尔·帕皮内尼、萨利姆·鲁科斯、托德·沃德和魏静朱。2002. [Bleu：一种自动评估机器翻译的方法](https://doi.org/10.3115/1073083.1073135)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. [Generative Agents: Interactive
    Simulacra of Human Behavior](https://doi.org/10.1145/3586183.3606763).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴等（2023）朴俊成、约瑟夫·奥布莱恩、凯瑞·君蔡、梅雷迪思·林格尔·莫里斯、珀西·梁和迈克尔·S·伯恩斯坦。2023. [生成代理：人类行为的互动模拟体](https://doi.org/10.1145/3586183.3606763)。
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. 2023. [Gorilla: Large Language Model Connected with Massive APIs](http://arxiv.org/abs/2305.15334).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕蒂尔等（2023）希希尔·G·帕蒂尔、田俊·张、辛·王和约瑟夫·E·冈萨雷斯。2023. [Gorilla：与大量 API 连接的大型语言模型](http://arxiv.org/abs/2305.15334)。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](http://arxiv.org/abs/2010.03768).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 施里达等（2020）莫希特·施里达、邢迪·袁、马克-亚历山大·科特、约纳坦·比斯克、亚当·特里施勒和马修·豪斯克内赫特。2020. [ALFWorld：将文本与具身环境对齐以进行互动学习](http://arxiv.org/abs/2010.03768)。
- en: Sloane (1992) Paul Sloane. 1992. *Lateral Thinking Puzzlers*. Sterling Publishing
    Company, Inc.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯隆（1992）保罗·斯隆。1992. *侧向思维谜题*。Sterling Publishing Company, Inc.
- en: Stuckman and Zhang (2005) Jeff Stuckman and Guo-Qiang Zhang. 2005. [Mastermind
    is NP-complete](http://arxiv.org/abs/cs/0512049).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stuckman和张（2005）杰夫·斯塔克曼和郭强·张。2005年。[Mastermind是NP完全的](http://arxiv.org/abs/cs/0512049)。
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    Learning: An Introduction*. MIT press.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton和Barto（2018）理查德·S·萨顿和安德鲁·G·巴托。2018年。*强化学习：导论*。MIT出版社。
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. [A Survey
    on Large Language Model based Autonomous Agents](http://arxiv.org/abs/2308.11432).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023）雷·王、陈·马、薛洋·冯、泽宇·张、浩·杨、靖森·张、志远·陈、佳凯·唐、徐·陈、彦凯·林等人。2023年。[基于大语言模型的自主代理调查](http://arxiv.org/abs/2308.11432)。
- en: Weng (2023) Lilian Weng. 2023. [LLM-powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翁（2023）莉莉安·翁。2023年。[LLM驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. [ReAct: Synergizing Reasoning and Acting
    in Language Models](http://arxiv.org/abs/2210.03629).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2022）邵渝·姚、杰弗里·赵、店·余、南·杜、伊扎克·沙弗兰、卡尔提克·纳拉辛汉和袁·曹。2022年。[ReAct：在语言模型中协同推理与行动](http://arxiv.org/abs/2210.03629)。
- en: 'Zhang et al. (2020a) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020a. [BERTScore: Evaluating Text Generation with BERT](http://arxiv.org/abs/1904.09675).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2020a）田毅·张、瓦尔莎·基肖尔、费利克斯·吴、基利安·Q·温伯格和约阿夫·阿尔茨。2020年。[BERTScore：用BERT评估文本生成](http://arxiv.org/abs/1904.09675)。
- en: 'Zhang et al. (2020b) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020b. [BERTScore: Evaluating Text Generation with BERT](https://openreview.net/forum?id=SkeHuCVFDr).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2020b）田毅·张、瓦尔莎·基肖尔、费利克斯·吴、基利安·Q·温伯格和约阿夫·阿尔茨。2020年。[BERTScore：用BERT评估文本生成](https://openreview.net/forum?id=SkeHuCVFDr)。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](http://arxiv.org/abs/2306.05685).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人（2023）郑联敏、魏林·蒋、英·盛、思远·庄、张昊·吴、永浩·庄、紫·林、卓涵·李、大成·李、埃里克·邢等人。2023年。[评估LLM作为裁判的方法：MT-Bench与Chatbot
    Arena](http://arxiv.org/abs/2306.05685)。
- en: 'Zhuang et al. (2023) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao
    Zhang. 2023. [ToolQA: A Dataset for LLM Question Answering with External Tools](http://arxiv.org/abs/2306.13304).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄等人（2023）于晨·庄、岳·余、宽·王、浩天·孙和超·张。2023年。[ToolQA：一个用于LLM问答与外部工具的数据集](http://arxiv.org/abs/2306.13304)。
- en: 'Appendix A Appendix: ALFWorld and Sudoku benchmarks'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录：ALFWorld和数独基准
- en: In this section we report the detailed metrics for each step for the ALFWorld
    and Sudoku benchmarks, omitted for the sake of brevity from the main paper.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们报告了ALFWorld和数独基准的每一步的详细指标，为简洁起见在主文中略去。
- en: '![Refer to caption](img/a846aa40dd019d161ef4ee6a0a1f4abe.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a846aa40dd019d161ef4ee6a0a1f4abe.png)'
- en: (a) ALFWorld
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ALFWorld
- en: '![Refer to caption](img/a6d46b5a28c844aabb6df2cfa923badb.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6d46b5a28c844aabb6df2cfa923badb.png)'
- en: (b) Sudoku
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 数独
- en: 'Figure 4: Progress rate PR[t] and the repetition rate RR[t] on ALFWorld and
    Sudoku averaged over 15 runs. ALFWorld: It starts out with a low repetition rate
    and quick increase of the progress rate. Then a slow increase of the repetition
    rate enables to further increase the progress rate although less quickly. Sudoku:
    The progress rate quickly reaches 8%. The repetition rate then slowly increases
    without any positive change in the progress rate.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：ALFWorld和数独上的进展率PR[t]与重复率RR[t]的15次运行平均值。ALFWorld：开始时重复率较低，进展率迅速增加。然后，重复率缓慢上升，进一步提高进展率，尽管速度较慢。数独：进展率迅速达到8%。随后，重复率缓慢上升，而进展率没有任何积极变化。
- en: 'Figure [4(a)](#A1.F4.sf1 "Figure 4(a) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") reports the progress rate and repetition rate for
    ALFWorld. The repetition rate is close to 0% for the first 20 steps, then it slowly
    increases up to 6% after 60 steps. The progress rate quickly reaches over 50%
    in 10 steps, then keeps increasing, although slowly, up to 74%. The consistent
    improvement of the progress rate even for steps close to 60 together with the
    low repetition rate suggests that higher values may be reached by increasing the
    maximum number of steps. We validate this hypothesis by extending the benchmark
    runtime to 120 steps. As previously reported in Table [3](#S4.T3 "Table 3 ‣ Experimental
    setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks"), this results in an improvement of 6
    percentage points for both the success rate the progress rate, i.e. SR$=93\%$.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4(a)](#A1.F4.sf1 "Figure 4(a) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks")报告了ALFWorld的进展率和重复率。重复率在前20步接近0%，然后在60步后缓慢增加到6%。进展率在10步中迅速超过50%，然后继续增加，虽然缓慢，最终达到74%。即使在接近60步的步骤中进展率的一致提升以及较低的重复率表明，通过增加最大步数可能会达到更高的值。我们通过将基准运行时间延长到120步来验证这一假设。正如表[3](#S4.T3
    "Table 3 ‣ Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")中先前报告的，这导致成功率和进展率分别提高了6个百分点，即SR$=93\%$。'
- en: 'Figure [4(b)](#A1.F4.sf2 "Figure 4(b) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") includes the two metrics for the Sudoku benchmark.
    We can observe that the progress rate quickly reaches a plateau at 8% in very
    few steps. The repetition rate is close to 0% for the first 10 steps, then it
    slowly increases up to 22% after 60 steps without any improvement of the progress
    rate.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4(b)](#A1.F4.sf2 "Figure 4(b) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks")展示了数独基准的两个指标。我们可以观察到，进展率在非常少的步骤中迅速达到8%的平台期。重复率在前10步接近0%，然后在60步后缓慢增加到22%，进展率没有任何改善。'
- en: 'Appendix B Appendix: Additional agents architectures and benchmarks'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 附录：额外的代理架构和基准
- en: In this section we highlight the plug-and-play aspect of AgentQuest showing
    the implementation of Mastermind with two additional agents architectures, i.e.
    ReAct Yao et al. ([2022](#bib.bib21)) as the most used architecture in literature
    and OpenAI Assistant OpenAI ([2023a](#bib.bib10)), as the most recent proprietary
    architecture. Additionally, we show how to implement the open-ended benchmark
    GAIA Mialon et al. ([2023](#bib.bib9)) requiring the usage of external tools.
    For brevity, in the following snippets we omit details, like error handling or
    full agent definition. The complete code is available in the [GitHub repository](https://github.com/nec-research/agentquest).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们突出了AgentQuest的即插即用功能，展示了使用两个额外代理架构实现Mastermind，即文献中最常用的架构ReAct Yao et
    al. ([2022](#bib.bib21))和最新的专有架构OpenAI Assistant OpenAI ([2023a](#bib.bib10))。此外，我们展示了如何实现开放式基准GAIA
    Mialon et al. ([2023](#bib.bib9))，这需要使用外部工具。为了简洁，以下代码片段省略了详细信息，如错误处理或完整的代理定义。完整代码可以在[GitHub仓库](https://github.com/nec-research/agentquest)中找到。
- en: B.1 ReAct for Closed-box Environments
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 ReAct在封闭环境中的应用
- en: We show an example of how to execute a closed-box benchmark (i.e. ALFWorld)
    with an agent based on the ReAct architecture Yao et al. ([2022](#bib.bib21)).
    Such architecture forces the agent decision making process to generate both textual
    reasoning traces and actions pertaining to a task in an interleaved manner. Common
    implementations Chase ([2022](#bib.bib2)); Yao et al. ([2022](#bib.bib21)) rely
    on external tools to perform actions. Here, we ensure compatibility with existing
    implementations providing a single tool (i.e. ProxyTool) that forwards the actions
    to the driver. In a nutshell, the agent reflects on the action to take and invokes
    the tool. Then, we feed the tool input to the driver to perform the interaction
    with the environment. At each step, we provide the agent the updated history of
    the actions and observations through the intermediate_steps variable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何使用基于 ReAct 架构的代理执行封闭盒基准测试（即 ALFWorld）的一个示例，如 Yao 等人所述 ([2022](#bib.bib21))。这种架构迫使代理在任务中以交替的方式生成文本推理痕迹和相关行动。常见的实现方法有
    Chase ([2022](#bib.bib2)) 和 Yao 等人 ([2022](#bib.bib21))，这些方法依赖于外部工具来执行行动。在这里，我们确保与现有实现的兼容性，提供一个单一工具（即
    ProxyTool）来将行动转发给驱动程序。简而言之，代理反映要采取的行动并调用工具。然后，我们将工具的输入提供给驱动程序，以便与环境进行交互。在每一步，我们通过
    intermediate_steps 变量向代理提供更新后的行动和观察历史。
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluc3RhbnRpYXRlIGN1c3RvbSBwcm9tcHQKcHJvbXB0ID0gQ3VzdG9tUHJvbXB0VGVtcGxhdGUoCiAgICB0ZW1wbGF0ZT0uLi4sICMgTExNIHByb21wdAogICAgdG9vbHM9W1Byb3h5VG9vbCgpXSwKICAgIGlucHV0X3ZhcmlhYmxlcz1bImludGVybWVkaWF0ZV9zdGVwcyIsIC4uLl0KKQojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gY3JlYXRlX3JlYWN0X2FnZW50KGxsbSwgW1Byb3h5VG9vbCgpXSwgcHJvbXB0KQppbnRlcm1lZGlhdGVfc3RlcHMgPSBbXQojIEluaXRpYWxpc2UgdGhlIGRyaXZlcgpkcml2ZXIgPSBNYXN0ZXJNaW5kRHJpdmVyKGdhbWUpCiMgR2V0IHRoZSBmaXJzdCBvYnNlcnZhdGlvbgpvYnMgPSBkcml2ZXIucmVzZXQoKQojIEFnZW50IExvb3AKd2hpbGUgbm90IG9icy5kb25lOgogICAgIyBSZXRyaWV2ZSB0aGUgYWdlbnQgb3V0cHV0CiAgICBhZ2VudF9jaG9pY2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeydpbnB1dCc6b2JzLm91dHB1dCwKICAgICAgICAgJ2ludGVybWVkaWF0ZV9zdGVwcyc6aW50ZXJtZWRpYXRlX3N0ZXBzfQogICAgKQogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9jaG9pY2UudG9vbF9pbnB1dCkKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBVcGRhdGUgaW50ZXJtZWRpYXRlIHN0ZXBzCiAgICBpbnRlcm1lZGlhdGVfc3RlcHMuYXBwZW5kKChhZ2VudF9jaG9pY2UsIG9icy5vdXRwdXQpKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLg==)from  agentquest.drivers  import  MasterMindDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  a  dummy  tool  for  closed-box  environmentsclass  ProxyTool(BaseTool):name  =  "proxytool"description  =  "Provide  the  action  you  want  to  perform"def  _run(self):pass#  Instantiate  custom  promptprompt  =  CustomPromptTemplate(template=...,  #  LLM  prompttools=[ProxyTool()],input_variables=["intermediate_steps",  ...])#  Initialise  the  agentagent  =  create_react_agent(llm,  [ProxyTool()],  prompt)intermediate_steps  =  []#  Initialise  the  driverdriver  =  MasterMindDriver(game)#  Get  the  first  observationobs  =  driver.reset()#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputagent_choice  =  agent.invoke({’input’:obs.output,’intermediate_steps’:intermediate_steps})action  =  Action(action_value=agent_choice.tool_input)#  Perform  the  stepobs  =  driver.step(action)#  Update  intermediate  stepsintermediate_steps.append((agent_choice,  obs.output))#  Get  current  metrics  ...'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluc3RhbnRpYXRlIGN1c3RvbSBwcm9tcHQKcHJvbXB0ID0gQ3VzdG9tUHJvbXB0VGVtcGxhdGUoCiAgICB0ZW1wbGF0ZT0uLi4sICMgTExNIHByb21wdAogICAgdG9vbHM9W1Byb3h5VG9vbCgpXSwKICAgIGlucHV0X3ZhcmlhYmxlcz1bImludGVybWVkaWF0ZV9zdGVwcyIsIC4uLl0KKQojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gY3JlYXRlX3JlYWN0X2FnZW50KGxsbSwgW1Byb3h5VG9vbCgpXSwgcHJvbXB0KQppbnRlcm1lZGlhdGVfc3RlcHMgPSBbXQojIEluaXRpYWxpc2UgdGhlIGRyaXZlcgpkcml2ZXIgPSBNYXN0ZXJNaW5kRHJpdmVyKGdhbWUpCiMgR2V0IHRoZSBmaXJzdCBvYnNlcnZhdGlvbgpvYnMgPSBkcml2ZXIucmVzZXQoKQojIEFnZW50IExvb3AKd2hpbGUgbm90IG9icy5kb25lOgogICAgIyBSZXRyaWV2ZSB0aGUgYWdlbnQgb3V0cHV0CiAgICBhZ2VudF9jaG9pY2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeydpbnB1dCc6b2JzLm91dHB1dCwKICAgICAgICAgJ2ludGVybWVkaWF0ZV9zdGVwcyc6aW50ZXJtZWRpYXRlX3N0ZXBzfQogICAgKQogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9jaG9pY2UudG9vbF9pbnB1dCkKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBVcGRhdGUgaW50ZXJtZWRpYXRlIHN0ZXBzCiAgICBpbnRlcm1lZGlhdGVfc3RlcHMuYXBwZW5kKChhZ2VudF9jaG9pY2UsIG9icy5vdXRwdXQpKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLg==)从  agentquest.drivers  导入  MasterMindDriver从  agentquest.metrics  导入  ...从  agentquest.utils  导入  Action...#  为封闭环境定义一个虚拟工具类  ProxyTool(BaseTool):name  =  "proxytool"description  =  "提供你想执行的操作"def  _run(self):pass#  实例化自定义提示prompt  =  CustomPromptTemplate(template=...,  #  LLM  提示工具=[ProxyTool()],input_variables=["intermediate_steps",  ...])#  初始化代理agent  =  create_react_agent(llm,  [ProxyTool()],  prompt)intermediate_steps  =  []#  初始化驱动器driver  =  MasterMindDriver(game)#  获取第一次观察obs  =  driver.reset()#  代理循环while  not  obs.done:#  检索代理输出agent_choice  =  agent.invoke({’input’:obs.output,’intermediate_steps’:intermediate_steps})action  =  Action(action_value=agent_choice.tool_input)#  执行步骤obs  =  driver.step(action)#  更新中间步骤intermediate_steps.append((agent_choice,  obs.output))#  获取当前指标  ...'
- en: B.2 OpenAI Assistant for Closed-box Environments
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 OpenAI 助手用于封闭环境
- en: The OpenAI Assistant OpenAI ([2023a](#bib.bib10)) is a proprietary architecture.
    It allows users to define custom agents by specifying the tasks to accomplish
    and the set of tools the agent can use. While the decision-making process is not
    directly accessible by the end-users (the agent and the LLM are hosted on the
    proprietary cloud environment), the tools can be invoked both remotely or locally.
    In the latter, users have control on the tool invocation managing the agent loop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 助手 OpenAI ([2023a](#bib.bib10)) 是一种专有架构。它允许用户通过指定要完成的任务和代理可以使用的工具集来定义自定义代理。尽管决策过程对最终用户不可直接访问（代理和
    LLM 托管在专有云环境中），但工具可以远程或本地调用。在后者情况下，用户可以控制工具调用，管理代理循环。
- en: Similarly to ReAct, we here rely on the ProxyTool, acting as a proxy between
    the agent and the environment. We invoke the remote agent with the initial task
    (e.g. first ALFWorld observation) and process the output of its decision making
    process, i.e. the action to perform provided as tool input. Then, we bypass the
    tool invocation, directly forwarding the action to the driver to perform the execution
    step and retrieve the next observation. Finally, we invoke the agent with the
    new observation concluding the execution step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ReAct，我们在这里依赖于 ProxyTool，它充当代理，在代理和环境之间进行中介。我们通过初始任务（例如第一次 ALFWorld 观察）调用远程代理，并处理其决策过程的输出，即作为工具输入提供的执行动作。然后，我们绕过工具调用，直接将动作转发给驱动程序以执行步骤并获取下一个观察结果。最后，我们用新的观察结果调用代理，以结束执行步骤。
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gT3BlbkFJQXNzaXN0YW50UnVubmFibGUuY3JlYXRlX2Fzc2lzdGFudCgKICAgIGluc3RydWN0aW9ucz0uLi4gIyBMTE0gcHJvbXB0CiAgICB0b29scz1bUHJveHlUb29sKCldLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IE1hc3Rlck1pbmREcml2ZXIoZ2FtZSkKIyBHZXQgdGhlIGZpcnN0IG9ic2VydmF0aW9uCm9icyA9IGRyaXZlci5yZXNldCgpCiMgR2V0IHRoZSBmaXJzdCBhY3Rpb24KcmVzcG9uc2UgPSBhZ2VudC5pbnZva2UoeyJjb250ZW50Ijogb2JzLm91dHB1dH0pCiMgQWdlbnQgTG9vcAp3aGlsZSBub3Qgb2JzLmRvbmU6CiAgICAjIFJldHJpZXZlIHRoZSBhZ2VudCBvdXRwdXQKICAgIGFnZW50X2d1ZXNzID0gcmVzcG9uc2VbMF0udG9vbF9pbnB1dAogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9ndWVzcykKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLgogICAgIyBNYW5hZ2UgUHJveHkgVG9vbCBvdXRwdXQKICAgIHRvb2xfb3V0cHV0cyA9IFsKICAgICAgICB7Im91dHB1dCI6IG9icy5vdXRwdXQsCiAgICAgICAgICJ0b29sX2NhbGxfaWQiOiByZXNwb25zZVswXS50b29sX2NhbGxfaWR9CiAgICBdCiAgICAjIEludm9rZSB0aGUgYWdlbnQgdG8gZ2V0IHRoZSBuZXh0IGFjdGlvbgogICAgcmVzcG9uc2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeyJ0b29sX291dHB1dHMiOiB0b29sX291dHB1dHMsCiAgICAgICAgICJydW5faWQiOiByZXNwb25zZVswXS5ydW5faWQsCiAgICAgICAgICJ0aHJlYWRfaWQiOiByZXNwb25zZVswXS50aHJlYWRfaWR9CiAgICAp)from  agentquest.drivers  import  MasterMindDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  a  dummy  tool  for  closed-box  environmentsclass  ProxyTool(BaseTool):name  =  "proxytool"description  =  "Provide  the  action  you  want  to  perform"def  _run(self):pass#  Initialise  the  agentagent  =  OpenAIAssistantRunnable.create_assistant(instructions=...  #  LLM  prompttools=[ProxyTool()],model=...  #  Chosen  LLMas_agent=True)#  Initialise  the  driverdriver  =  MasterMindDriver(game)#  Get  the  first  observationobs  =  driver.reset()#  Get  the  first  actionresponse  =  agent.invoke({"content":  obs.output})#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputagent_guess  =  response[0].tool_inputaction  =  Action(action_value=agent_guess)#  Perform  the  stepobs  =  driver.step(action)#  Get  current  metrics  ...#  Manage  Proxy  Tool  outputtool_outputs  =  [{"output":  obs.output,"tool_call_id":  response[0].tool_call_id}]#  Invoke  the  agent  to  get  the  next  actionresponse  =  agent.invoke({"tool_outputs":  tool_outputs,"run_id":  response[0].run_id,"thread_id":  response[0].thread_id})'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IE1hc3Rlck1pbmREcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIGEgZHVtbXkgdG9vbCBmb3IgY2xvc2VkLWJveCBlbnZpcm9ubWVudHMKY2xhc3MgUHJveHlUb29sKEJhc2VUb29sKToKICAgIG5hbWUgPSAicHJveHl0b29sIgogICAgZGVzY3JpcHRpb24gPSAiUHJvdmlkZSB0aGUgYWN0aW9uIHlvdSB3YW50IHRvIHBlcmZvcm0iCiAgICBkZWYgX3J1bihzZWxmKToKICAgICAgICBwYXNzCgojIEluaXRpYWxpc2UgdGhlIGFnZW50CmFnZW50ID0gT3BlbkFJQXNzaXN0YW50UnVubmFibGUuY3JlYXRlX2Fzc2lzdGFudCgKICAgIGluc3RydWN0aW9ucz0uLi4gIyBMTE0gcHJvbXB0CiAgICB0b29scz1bUHJveHlUb29sKCldLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IE1hc3Rlck1pbmREcml2ZXIoZ2FtZSkKIyBHZXQgdGhlIGZpcnN0IG9ic2VydmF0aW9uCm9icyA9IGRyaXZlci5yZXNldCgpCiMgR2V0IHRoZSBmaXJzdCBhY3Rpb24KcmVzcG9uc2UgPSBhZ2VudC5pbnZva2UoeyJjb250ZW50Ijogb2JzLm91dHB1dH0pCiMgQWdlbnQgTG9vcAp3aGlsZSBub3Qgb2JzLmRvbmU6CiAgICAjIFJldHJpZXZlIHRoZSBhZ2VudCBvdXRwdXQKICAgIGFnZW50X2d1ZXNzID0gcmVzcG9uc2VbMF0udG9vbF9pbnB1dAogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hZ2VudF9ndWVzcykKICAgICMgUGVyZm9ybSB0aGUgc3RlcAogICAgb2JzID0gZHJpdmVyLnN0ZXAoYWN0aW9uKQogICAgIyBHZXQgY3VycmVudCBtZXRyaWNzIC4uLgogICAgIyBNYW5hZ2UgUHJveHkgVG9vbCBvdXRwdXQKICAgIHRvb2xfb3V0cHV0cyA9IFsKICAgICAgICB7Im91dHB1dCI6IG9icy5vdXRwdXQsCiAgICAgICAgICJ0b29sX2NhbGxfaWQiOiByZXNwb25zZVswXS50b29sX2NhbGxfaWR9CiAgICBdCiAgICAjIEludm9rZSB0aGUgYWdlbnQgdG8gZ2V0IHRoZSBuZXh0IGFjdGlvbgogICAgcmVzcG9uc2UgPSBhZ2VudC5pbnZva2UoCiAgICAgICAgeyJ0b29sX291dHB1dHMiOiB0b29sX291dHB1dHMsCiAgICAgICAgICJydW5faWQiOiByZXNwb25zZVswXS5ydW5faWQsCiAgICAgICAgICJ0aHJlYWRfaWQiOiByZXNwb25zZVswXS50aHJlYWRfaWR9CiAgICAp)从
    agentquest.drivers 导入 MasterMindDriver 从 agentquest.metrics 导入 ... 从 agentquest.utils
    导入 Action ... # 为封闭环境定义一个虚拟工具 class ProxyTool(BaseTool): name = "proxytool" description
    = "提供你想执行的动作" def _run(self): pass # 初始化代理 agent = OpenAIAssistantRunnable.create_assistant(instructions=...  #
    LLM 提示 tools=[ProxyTool()], model=...  # 选择的 LLM as_agent=True) # 初始化驱动器 driver
    = MasterMindDriver(game) # 获取第一次观察 obs = driver.reset() # 获取第一次动作 response = agent.invoke({"content":
    obs.output}) # 代理循环 while not obs.done: # 检索代理输出 agent_guess = response[0].tool_input
    action = Action(action_value=agent_guess) # 执行步骤 obs = driver.step(action) # 获取当前指标
    ... # 管理 Proxy Tool 输出 tool_outputs = [{"output": obs.output, "tool_call_id":
    response[0].tool_call_id}] # 调用代理以获取下一个动作 response = agent.invoke({"tool_outputs":
    tool_outputs, "run_id": response[0].run_id, "thread_id": response[0].thread_id})'
- en: B.3 OpenAI Assistant for Open-ended Environments
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 OpenAI 助手用于开放环境
- en: When interacting with an open-ended environment, the agent is not restricted
    to the pre-defined actions of the closed-box environment and it is allowed to
    select any user-defined tool (e.g. retrieving information online or executing
    code). Hence, we provide the agent the list of tools via the tool variable. The
    agent relies on its reasoning process to choose which tool to invoke. Omitted
    here for the sake of brevity, we rely of the manual annotations of the GAIA questions Mialon
    et al. ([2023](#bib.bib9)) as milestones to compute the progress rate.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在与开放环境互动时，代理不受封闭环境中预定义动作的限制，可以选择任何用户定义的工具（例如，在线检索信息或执行代码）。因此，我们通过工具变量向代理提供工具列表。代理依赖于其推理过程来选择调用哪个工具。为了简洁起见，这里省略了，我们依靠GAIA问题的手动注释（Mialon
    et al. ([2023](#bib.bib9))）作为计算进度率的里程碑。
- en: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IEdhaWFEcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIHRoZSB0b29scwp0b29scz1bCiAgICBPbmxpbmVTZWFyY2goKSwgIyBSZXRyaWV2ZSBhIHdlYiBwYWdlIGxpbmsKICAgIFdlYkNvbnRlbnRQYXJzZXIoKSwgIyBSZWFkIHRoZSB3ZWIgcGFnZQogICAgRmluYWxBbnN3ZXJSZXRyaWV2ZXIoKSwgIyBQcm92aWRlIHRoZSBmaW5hbCBhbnN3ZXIKICAgIC4uLgpdCiMgSW5pdGlhbGlzZSB0aGUgYWdlbnQKYWdlbnQgPSBPcGVuQUlBc3Npc3RhbnRSdW5uYWJsZS5jcmVhdGVfYXNzaXN0YW50KAogICAgaW5zdHJ1Y3Rpb25zPS4uLiAjIExMTSBwcm9tcHQKICAgIHRvb2xzPXRvb2xzLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IEdhaWFEcml2ZXIocXVlc3Rpb24sIHRvb2xzKQojIEdldCB0aGUgZmlyc3Qgb2JzZXJ2YXRpb24Kb2JzID0gZHJpdmVyLnJlc2V0KCkKIyBHZXQgdGhlIGZpcnN0IGFjdGlvbgpyZXNwb25zZSA9IGFnZW50Lmludm9rZSh7ImNvbnRlbnQiOiBvYnMub3V0cHV0fSkKIyBBZ2VudCBMb29wCndoaWxlIG5vdCBvYnMuZG9uZToKICAgICMgUmV0cmlldmUgdGhlIGFnZW50IG91dHB1dAogICAgYWN0ID0gZid7cmVzcG9uc2VbMF0udG9vbH06e3Jlc3BvbnNlWzBdLnRvb2xfaW5wdXR9JwogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hY3QpCiAgICAjIFBlcmZvcm0gdGhlIHN0ZXAgaW52b2tpbmcgdGhlIGxvY2FsIHRvb2wKICAgIG9icyA9IGRyaXZlci5zdGVwKGFjdGlvbikKICAgICMgR2V0IGN1cnJlbnQgbWV0cmljcyAuLi4KICAgICMgTWFuYWdlIHRvb2wgb3V0cHV0IGFzIG9ic2VydmF0aW9uCiAgICB0b29sX291dHB1dHMgPSBbCiAgICAgICAgeyJvdXRwdXQiOiBvYnMub3V0cHV0LAogICAgICAgICAidG9vbF9jYWxsX2lkIjogcmVzcG9uc2VbMF0udG9vbF9jYWxsX2lkfQogICAgXQogICAgIyBJbnZva2UgdGhlIGFnZW50IHRvIGdldCB0aGUgbmV4dCBhY3Rpb24KICAgIHJlc3BvbnNlID0gYWdlbnQuaW52b2tlKAogICAgICAgIHsidG9vbF9vdXRwdXRzIjogdG9vbF9vdXRwdXRzLAogICAgICAgICAicnVuX2lkIjogcmVzcG9uc2VbMF0ucnVuX2lkLAogICAgICAgICAidGhyZWFkX2lkIjogcmVzcG9uc2VbMF0udGhyZWFkX2lkfQogICAgKQ==)from  agentquest.drivers  import  GaiaDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  the  toolstools=[OnlineSearch(),  #  Retrieve  a  web  page  linkWebContentParser(),  #  Read  the  web  pageFinalAnswerRetriever(),  #  Provide  the  final  answer...]#  Initialise  the  agentagent  =  OpenAIAssistantRunnable.create_assistant(instructions=...  #  LLM  prompttools=tools,model=...  #  Chosen  LLMas_agent=True)#  Initialise  the  driverdriver  =  GaiaDriver(question,  tools)#  Get  the  first  observationobs  =  driver.reset()#  Get  the  first  actionresponse  =  agent.invoke({"content":  obs.output})#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputact  =  f’{response[0].tool}:{response[0].tool_input}’action  =  Action(action_value=act)#  Perform  the  step  invoking  the  local  toolobs  =  driver.step(action)#  Get  current  metrics  ...#  Manage  tool  output  as  observationtool_outputs  =  [{"output":  obs.output,"tool_call_id":  response[0].tool_call_id}]#  Invoke  the  agent  to  get  the  next  actionresponse  =  agent.invoke({"tool_outputs":  tool_outputs,"run_id":  response[0].run_id,"thread_id":  response[0].thread_id})'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,ZnJvbSBhZ2VudHF1ZXN0LmRyaXZlcnMgaW1wb3J0IEdhaWFEcml2ZXIKZnJvbSBhZ2VudHF1ZXN0Lm1ldHJpY3MgaW1wb3J0IC4uLgpmcm9tIGFnZW50cXVlc3QudXRpbHMgaW1wb3J0IEFjdGlvbgouLi4KCiMgRGVmaW5lIHRoZSB0b29scwp0b29scz1bCiAgICBPbmxpbmVTZWFyY2goKSwgIyBSZXRyaWV2ZSBhIHdlYiBwYWdlIGxpbmsKICAgIFdlYkNvbnRlbnRQYXJzZXIoKSwgIyBSZWFkIHRoZSB3ZWIgcGFnZQogICAgRmluYWxBbnN3ZXJSZXRyaWV2ZXIoKSwgIyBQcm92aWRlIHRoZSBmaW5hbCBhbnN3ZXIKICAgIC4uLgpdCiMgSW5pdGlhbGlzZSB0aGUgYWdlbnQKYWdlbnQgPSBPcGVuQUlBc3Npc3RhbnRSdW5uYWJsZS5jcmVhdGVfYXNzaXN0YW50KAogICAgaW5zdHJ1Y3Rpb25zPS4uLiAjIExMTSBwcm9tcHQKICAgIHRvb2xzPXRvb2xzLAogICAgbW9kZWw9Li4uICMgQ2hvc2VuIExMTQogICAgYXNfYWdlbnQ9VHJ1ZQopCiMgSW5pdGlhbGlzZSB0aGUgZHJpdmVyCmRyaXZlciA9IEdhaWFEcml2ZXIocXVlc3Rpb24sIHRvb2xzKQojIEdldCB0aGUgZmlyc3Qgb2JzZXJ2YXRpb24Kb2JzID0gZHJpdmVyLnJlc2V0KCkKIyBHZXQgdGhlIGZpcnN0IGFjdGlvbgpyZXNwb25zZSA9IGFnZW50Lmludm9rZSh7ImNvbnRlbnQiOiBvYnMub3V0cHV0fSkKIyBBZ2VudCBMb29wCndoaWxlIG5vdCBvYnMuZG9uZToKICAgICMgUmV0cmlldmUgdGhlIGFnZW50IG91dHB1dAogICAgYWN0ID0gZid7cmVzcG9uc2VbMF0udG9vbH06e3Jlc3BvbnNlWzBdLnRvb2xfaW5wdXR9JwogICAgYWN0aW9uID0gQWN0aW9uKGFjdGlvbl92YWx1ZT1hY3QpCiAgICAjIFBlcmZvcm0gdGhlIHN0ZXAgaW52b2tpbmcgdGhlIGxvY2FsIHRvb2wKICAgIG9icyA9IGRyaXZlci5zdGVwKGFjdGlvbikKICAgICMgR2V0IGN1cnJlbnQgbWV0cmljcyAuLi4KICAgICMgTWFuYWdlIHRvb2wgb3V0cHV0IGFzIG9ic2VydmF0aW9uCiAgICB0b29sX291dHB1dHMgPSBbCiAgICAgICAgeyJvdXRwdXQiOiBvYnMub3V0cHV0LAogICAgICAgICAidG9vbF9jYWxsX2lkIjogcmVzcG9uc2VbMF0udG9vbF9jYWxsX2lkfQogICAgXQogICAgIyBJbnZva2UgdGhlIGFnZW50IHRvIGdldCB0aGUgbmV4dCBhY3Rpb24KICAgIHJlc3BvbnNlID0gYWdlbnQuaW52b2tlKAogICAgICAgIHsidG9vbF9vdXRwdXRzIjogdG9vbF9vdXRwdXRzLAogICAgICAgICAicnVuX2lkIjogcmVzcG9uc2VbMF0ucnVuX2lkLAogICAgICAgICAidGhyZWFkX2lkIjogcmVzcG9uc2VbMF0udGhyZWFkX2lkfQogICAgKQ==)来自
    agentquest.drivers 导入 GaiaDriver 从 agentquest.metrics 导入 ... 从 agentquest.utils
    导入 Action ... # 定义工具工具 = [OnlineSearch(),  # 检索网页链接 WebContentParser(),  # 读取网页
    FinalAnswerRetriever(),  # 提供最终答案...] # 初始化代理代理 = OpenAIAssistantRunnable.create_assistant(instructions=...  #
    LLM 提示工具 = 工具，模型 = ...  # 选择的 LLM 作为代理 = True) # 初始化驱动程序驱动程序 = GaiaDriver(question,
    工具) # 获取第一次观察 obs = driver.reset() # 获取第一次动作 response = agent.invoke({"content":
    obs.output}) # 代理循环当 obs.done 为 False 时： # 检索代理输出 act = f’{response[0].tool}:{response[0].tool_input}’
    action = Action(action_value=act) # 执行步骤调用本地工具 obs = driver.step(action) # 获取当前指标
    ... # 管理工具输出作为观察 tool_outputs = [{"output": obs.output, "tool_call_id": response[0].tool_call_id}]
    # 调用代理以获取下一动作 response = agent.invoke({"tool_outputs": tool_outputs, "run_id":
    response[0].run_id, "thread_id": response[0].thread_id})'
- en: 'Here, the driver acts as a wrapper, executing the tool with the parameters
    provided by the agent (tool_input) and forwards the output to the agent in the
    correct format:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，驱动程序充当一个包装器，使用代理提供的参数（tool_input）执行工具，并以正确的格式将输出转发给代理。
- en: '[⬇](data:text/plain;base64,Y2xhc3MgR2FpYURyaXZlcigpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIHF1ZXN0aW9uLCB0b29scywgLi4uKToKICAgICAgICAjIEluaXRpYWxpc2UgdGhlIHRvb2wgbG9va3VwCiAgICAgICAgc2VsZi50b29sX2xvb2t1cCA9IHt4Lm5hbWU6eCBmb3IgeCBpbiB0b29sc30KICAgIC4uLgogICAgZGVmIHN0ZXAoc2VsZiwgYWN0aW9uKToKICAgICAgICAjIFBhcnNlIHRoZSBhY3Rpb24KICAgICAgICB0b29sLCB0b29sX2lucHV0ID0gYWN0aW9uLmFjdGlvbl92YWx1ZS5zcGxpdCgnOicpCiAgICAgICAgIyBJbnZva2UgdGhlIHRvb2wKICAgICAgICB0b29sX291dCA9IHNlbGYudG9vbF9sb29rdXBbdG9vbF0uX3J1bih0b29sX2lucHV0KQogICAgICAgICMgUGFyc2UgdGhlIHRvb2wgb3V0cHV0IGhlcmUgLi4uCiAgICAgICAgcmV0dXJuIE9ic2VydmF0aW9uKG91dHB1dD10b29sX291dCk=)class  GaiaDriver():def  __init__(self,  question,  tools,  ...):#  Initialise  the  tool  lookupself.tool_lookup  =  {x.name:x  for  x  in  tools}...def  step(self,  action):#  Parse  the  actiontool,  tool_input  =  action.action_value.split(’:’)#  Invoke  the  tooltool_out  =  self.tool_lookup[tool]._run(tool_input)#  Parse  the  tool  output  here  ...return  Observation(output=tool_out)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,Y2xhc3MgR2FpYURyaXZlcigpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIHF1ZXN0aW9uLCB0b29scywgLi4uKToKICAgICAgICAjIEluaXRpYWxpc2UgdGhlIHRvb2wgbG9va3VwCiAgICAgICAgc2VsZi50b29sX2xvb2t1cCA9IHt4Lm5hbWU6eCBmb3IgeCBpbiB0b29sc30KICAgIC4uLgogICAgZGVmIHN0ZXAoc2VsZiwgYWN0aW9uKToKICAgICAgICAjIFBhcnNlIHRoZSBhY3Rpb24KICAgICAgICB0b29sLCB0b29sX2lucHV0ID0gYWN0aW9uLmFjdGlvbl92YWx1ZS5zcGxpdCgnOicpCiAgICAgICAgIyBJbnZva2UgdGhlIHRvb2wKICAgICAgICB0b29sX291dCA9IHNlbGYudG9vbF9sb29rdXBbdG9vbF0uX3J1bih0b29sX2lucHV0KQogICAgICAgICMgUGFyc2UgdGhlIHRvb2wgb3V0cHV0IGhlcmUgLi4uCiAgICAgICAgcmV0dXJuIE9ic2VydmF0aW9uKG91dHB1dD10b29sX291dCk=)类
    GaiaDriver():def __init__(self, question, tools, ...):# 初始化工具查找表self.tool_lookup
    = {x.name:x for x in tools}...def step(self, action):# 解析操作tool, tool_input =
    action.action_value.split(’:’)# 调用工具tool_out = self.tool_lookup[tool]._run(tool_input)#
    解析工具输出在这里 ...return Observation(output=tool_out)'
