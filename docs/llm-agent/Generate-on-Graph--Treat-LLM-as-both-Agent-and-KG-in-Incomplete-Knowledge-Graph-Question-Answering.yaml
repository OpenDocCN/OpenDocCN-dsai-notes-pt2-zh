- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14741](https://ar5iv.labs.arxiv.org/html/2404.14741)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yao Xu^(1,2), Shizhu He^(1,2), Jiabei Chen^(1,2), Zihao Wang³, Yangqiu Song³,
    Hanghang Tong⁴, Kang Liu^(1,2,5), Jun Zhao^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹ The Laboratory of Cognition and Decision Intelligence for Complex Systems,
  prefs: []
  type: TYPE_NORMAL
- en: Institute of Automation, Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ² School of Artificial Intelligence, University of Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ³ The Hong Kong University of Science and Technology
  prefs: []
  type: TYPE_NORMAL
- en: ⁴ University of Illinois Urbana-Champaign,   ⁵ Shanghai Artificial Intelligence
    Laboratory
  prefs: []
  type: TYPE_NORMAL
- en: '{yao.xu, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn, zwanggc@connect.ust.hk, yqsong@cse.ust.hk,
    htong@illinois.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To address the issue of insufficient knowledge and the tendency to generate
    hallucination in Large Language Models (LLMs), numerous studies have endeavored
    to integrate LLMs with Knowledge Graphs (KGs). However, all these methods are
    evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete
    KGs, where the factual triples involved in each question are entirely covered
    by the given KG. In this situation, LLM mainly acts as an agent to find answer
    entities by exploring the KG, rather than effectively integrating internal and
    external knowledge sources. However, in real-world scenarios, KGs are often incomplete
    to cover all the knowledge required to answer questions. To simulate real-world
    scenarios and evaluate the ability of LLMs to integrate internal and external
    knowledge, in this paper, we propose leveraging LLMs for QA under Incomplete Knowledge
    Graph (IKGQA), where the given KG doesn’t include all the factual triples involved
    in each question. To handle IKGQA, we propose a training-free method called Generate-on-Graph
    (GoG) that can generate new factual triples while exploring on KGs. Specifically,
    we propose a selecting-generating-answering framework, which not only treat the
    LLM as an agent to explore on KGs, but also treat it as a KG to generate new facts
    based on the explored subgraph and its inherent knowledge. Experimental results
    on two datasets demonstrate that our GoG can solve IKGQA to a certain extent,
    while almost all previous methods cannot perform well on IKGQA.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/949c9219b590af4d958105aa985efaa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example of Incomplete Knowledge Graph Question Answering (IKGQA),
    where yellow and red nodes represent topic and answer entity, respectively. Red
    dash line represents this triple (Cupertino, timezone, Pacific Standard Time)
    is missing in the KG.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) Brown et al. ([2020](#bib.bib3)); Bang et al. ([2023](#bib.bib1))
    have made great success in various natural language processing (NLP) tasks. Benefiting
    from extensive model parameters and vast amounts of pre-training corpus, LLMs
    can solve complex reasoning tasks through In-Context Learning (ICL) Dong et al.
    ([2023](#bib.bib4)), without fine-tuning for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, LLMs are still suffer from insufficient knowledge and the tendency
    to generate hallucinationHuang et al. ([2023](#bib.bib6)); Li et al. ([2023a](#bib.bib10)).
    To mitigate this issue, many methods that incorporate LLMs with Knowledge Graphs
    (KGs) Ji et al. ([2021](#bib.bib7)) has been proposed Pan et al. ([2023](#bib.bib17)),
    where KGs provide accurate and abundant factual knowledge in triple format while
    LLMs provide strong natural language processing ability. These works can be roughly
    divided into two categories: (1) Semantic Parsing (SP) methods Li et al. ([2023c](#bib.bib12));
    Nie et al. ([2023](#bib.bib16)), which use LLMs to generate logical forms with
    ICL, and then obtain answers by executing these logical queries on KGs. (2) Retrieval
    Augmented (RA) methods Li et al. ([2023d](#bib.bib13)), which retrieve information
    related to the question from KGs as external knowledge to help LLMs to obtain
    the answers. The retrieval format can be triples Li et al. ([2023d](#bib.bib13)),
    paths Sun et al. ([2023](#bib.bib20)); Luo et al. ([2023b](#bib.bib15)) and subgraphs
    Wang et al. ([2023a](#bib.bib24)).'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic parsing based methods exclusively treat LLMs as parser, not only do
    they ignore LLMs’ inherent knowledge and reasoning ability, but they also depend
    heavily on KGs’ quality and completeness Sun et al. ([2023](#bib.bib20)). Therefore,
    more attention is paid to retrieval augmented methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8cd6d21585c6e940743557850477189e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The performance of ToG Sun et al. ([2023](#bib.bib20)) and our GoG
    under complete/incomplete KG. Dash line indicates the performance of LLM with
    CoT Wei et al. ([2023](#bib.bib27)) prompting (without KG).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although these retrieval augmented methods claim to solve the drawbacks of
    semantic parsing methods and obtain good performance on conventional Knowledge
    Graph Question Answering (KGQA) Yih et al. ([2016a](#bib.bib29)), it is still
    hard to determine if they effectively integrate knowledge from KGs and LLMs, and
    how much reasoning ability is utilized. One crucial reason is that, in conventional
    KGQA tasks, the factual triplets involved in each question are entirely covered
    by the KG. Therefore, in this scenario, LLMs still play the role of parser which
    only needs to identify the relationship path starting from the topic entity to
    the answer entity. For example, for the question "What is the timezone of the
    area where Apple headquarters is located?" in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"), the LLM only needs to start from Apple headquarters,
    sequentially choose the relations located_in and timezone to find the answer.
    That means LLMs only need to ground the relationship appearing in the question
    to the specific relation in the KG to reach the answer entity Pacific Standard
    Time. Therefore, in conventional KGQA, LLM mainly acts as an agent to find answer
    entities by exploring the KG, rather than effectively integrating internal and
    external knowledge sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in real-world scenarios, KGs are often incomplete to cover all the
    knowledge required to answer questions. Besides, LLMs contain rich knowledge content
    and have powerful reasoning ability. Therefore, to evaluate the ability of LLMs
    to integrate internal and external knowledge, in this paper, we propose leveraging
    LLMs for QA under incomplete KG (IKGQA). The distinction between IKGQA and conventional
    KGQA is that IKGQA does not encompass all the factual triplets relevant to each
    question, rendering the KG in IKGQA incomplete. For example, as shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering"), for answering the same
    question "What is the timezone of the area where Apple headquarters is located?",
    the key triple (Cupertino, timezone, Pacific Standard Time) does not exist in
    the KG. This means that even if the correct SPARQL query is generated, it may
    not retrieve the final answer ¹¹1SP methods do not parse ”timezone” into two hop
    path ”located_in -¿ timezone” because in the training set, ”timezone” only corresponds
    to one hop path ”timezone”, more details can be found in Appendix [A](#A1 "Appendix
    A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering").. Compared to KGQA,
    IKGQA holds greater research significance for the following reasons: (1) it is
    closer to real-world scenarios where the given KG is incomplete to answer users’
    questions. (2) it evaluates LLMs’ reasoning ability and its capability to integrate
    inherent and external knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering"),
    although previous methods have achieved outstanding performance in KGQA, their
    performance has significantly declined in IKGQA, even is worse than that without
    KG. This discrepancy suggests that these methods might not fully integrate the
    external and inherent knowledge of LLMs as professed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the challenges in IKGQA, we propose a novel method called Generate-on-graph
    (GoG), which adopts a selecting-generating-answering framework consisting of three
    main steps: (1) Selecting, where LLMs select the relations most relevant to the
    current question and expanding the subgraph using these relations. (2) Generating,
    where LLMs use its inherent knowledge and reasoning abilities to generate new
    factual triples based on the explored subgraph. For example, if the KG already
    contains the triples (Cupertino, located_in, California), (California, timezone,
    Pacific Standard Time), LLMs can infer new triple (Cupertino, timezone, Pacific
    Standard Time). (3) Answering, where LLMs try to answer the question based on
    the retrieval and generated knowledge. If the information is still insufficient,
    the process is repeated by going back to Steps 1 and 2 until the LLMs can answer
    the question. The codes and data are available at [https://github.com/YaooXu/GoG](https://github.com/YaooXu/GoG).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose leveraging LLMs for QA under incomplete KG (IKGQA), which is closer
    to real-world scenarios and can evaluate LLMs’ ability better, and construct relevant
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose a method called Generate-on-Graph (GoG), which uses the selecting-generating-answering
    framework, to address IKGQA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experimental results on two datasets show the superiority of GoG, and demonstrate
    that an incomplete KG can still help LLMs answer complex questions by providing
    related structured information, even without directly providing the answers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6fc52947cbc60dfa9c3991fad5b7b8b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparison of four methods in solving IKGQA: (a) Standard CoT prompting
    (closed-book, without KG), (b) Semantic parsing based method (e.g., KB-BINDER Li
    et al. ([2023c](#bib.bib12))), (c) Path retrieval method (e.g., ToG Sun et al.
    ([2023](#bib.bib20))), (d) The proposed GoG with selecting-generating-answering
    framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Question Answering Under Incomplete KG. Some previous works Saxena et al. ([2020](#bib.bib18));
    Zan et al. ([2022](#bib.bib31)) attempt to train KG embeddings to predict answers
    by similarity scores under incomplete KG. Compared to these previous KGE-based
    works, we propose leveraging LLMs for QA under incomplete KG to study whether
    LLMs can integrate internal and external knowledge well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unifying KGs and LLMs for KGQA. Various methods have been proposed to unify
    KGs and LLMs to solve KGQA, these methods can be classified into two categories:
    Semantic Parsing (SP) methods Li et al. ([2023c](#bib.bib12)); Nie et al. ([2023](#bib.bib16))
    and Retrieval Augmented (RA) methods Luo et al. ([2023b](#bib.bib15)); Sun et al.
    ([2023](#bib.bib20)). SP methods transform the question into a structural query
    using LLMs. These queries can then be executed by a KG engine to derive answers
    based on KGs Sun et al. ([2020](#bib.bib21)). KB-BINDER Li et al. ([2023c](#bib.bib12))
    generates the drafts as preliminary logical forms first, and then binds the drafts
    to the executable ones with entity and relation binders. However, the effectiveness
    of these methods relies heavily on the quality of the generated queries and the
    completeness of KGs. RA methods retrieve related information from the KG to improve
    the reasoning performance Li et al. ([2023b](#bib.bib11)). ToG Sun et al. ([2023](#bib.bib20))
    treats the LLM as an agent to interactively explore relation paths step-by-step
    on KGs and perform reasoning based on the retrieved paths. RoG Luo et al. ([2023b](#bib.bib15))
    first generates relation paths as faithful plans, and then use them to retrieve
    valid reasoning paths from the KGs for LLMs to reason. Our GoG belongs to retrieval
    augmented methods, we also utilize the knowledge modeling ability of LLMs, as
    well as the semantic parsing ability and reasoning ability.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM reasoning with Prompting. Many works have been proposed to elicit the reasoning
    ability of LLMs to solve complex tasks through prompting Wei et al. ([2023](#bib.bib27));
    Khot et al. ([2023](#bib.bib9)). Complex CoT Fu et al. ([2023](#bib.bib5)) creates
    and refine rationale examples with more reasoning steps to elicit better reasoning
    in LLMs. Self-Consistency Wang et al. ([2023b](#bib.bib26)) fully explores various
    ways of reasoning to improve their performance on reasoning tasks. DecomP Khot
    et al. ([2023](#bib.bib9)) solves complex tasks by instead decomposing them into
    simpler sub-tasks and delegating these to sub-task specific LLMs. ReAct Yao et al.
    ([2023](#bib.bib28)) treats LLMs as agents that interact with the environment
    and make decisions to retrieve information from external source. GoG can be viewed
    as a fusion of ReAct and DecomP, thereby enabling a more comprehensive utilization
    of the diverse capabilities inherent in LLMs for addressing complex questions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we firstly introduce Knowledge Graph (KGs). Then, we use symbols
    of KGs to define relation path and Knowledge Graph Question Answering (KGQA).
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Graphs (KG) is a set of factual triples, i.e., $\mathcal{G}=\{(h,r,t)\in\mathcal{V}\times\mathcal{R}\times\mathcal{V}\}$
    represents the relation.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Graph Question Answering (KGQA) is a reasoning task that aims to predict
    answer entities $e_{a}\in\mathcal{A}_{q}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Incomplete Knowledge Graph Question Answering (IKGQA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Task Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IKGQA differs from KGQA in that, in IKGQA, $\exists i\in[1,l],\;(e_{i-1},r_{i},e_{i})\notin\mathcal{G}$.
    Therefore, models need to recall their inherent knowledge or reasoning from subgraph
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Datasets Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We construct two IKGQA datasets based on two widely used KGQA datasets: WebQuestionSP
    (WebQSP) Yih et al. ([2016b](#bib.bib30)) and Complex WebQuestion (CWQ) Talmor
    and Berant ([2018](#bib.bib22)). Both datasets use Freebase Bollacker et al. ([2008](#bib.bib2))
    as their background KG. To simulate incomplete KG, we delete some crucial triples,
    which appear in the gold relation path, of each question from the original KG.
    By doing this, simple semantic parsing methods almost fail to obtain the correct
    answers (except for some scenarios with multiple golden paths). In order to save
    computational cost, we selected the first 1,000 samples of these two datasets
    for constructing IKGQA questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we only consider one topic entity here. The process of generating
    crucial triples of a question is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the topic entity $e_{q}$ by LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the SPARQL question $s_{q}$, and get all involved triples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter property node (e.g., time, text, height), then convert the involved triples
    into a subgraph $g$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Utilize breadth first search (BFS) to find the shortest path $p$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample $k$ as the crucial triples in the question.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Generate-on-Graph (GoG)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce our method Generate-on-Graph (GoG), which can
    integrate the knowledge of KGs and LLMs, as well as utilize the reasoning ability
    of LLMs. The comparison between GoG and other previous methods is illustrated
    in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM
    as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM as Agent. Motivated by ReAct Yao et al. ([2023](#bib.bib28)), we consider
    the LLM as an agent interacting with an environment to solve tasks. As shown in
    Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering") (d), for
    each step $i$ is the action space, to search information by calling graph database
    API (Act 1, 2) or generate more information by reasoning and inherent knowledge
    (Act 4).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Space. GoG uses the selecting-generating-answering framework, which
    consists of three main actions: Search, Generate and Answer.'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Search[target entity], which aims to find the most relevant top K neighbors
    of the target entity $e$ linking to the target entity. This process is completed
    by pre-defined SPARQL queries. Then, we utilize the LLMs to select the top K relations
    that most related to the current sub-question (last thought). As shown in Act
    2 of Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM
    as both Agent and KG in Incomplete Knowledge Graph Question Answering") (d), given
    the target entity Cupertino, LLMs select the two relation {Located_in, Adjoin}
    from all neighbor relations. In the end, triples {(Cupertino, located_in, California),
    (Cupertino, adjoin, Palo Alto)} are appended to context as Obs 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Generate[sub-question], which make the agent generate new factual triples
    based on retrieval information and inherent knowledge. As shown in Act 4 of Figure
    [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering") (d), although there
    is no triple directly representing the timezone of Cupertino, the agent can still
    infer the timezone of Cupertino based on {(Cupertino, located_in, California),
    (California, timezone, Pacific Standard Time)}. This process is similar to knowledge
    graph completion (KGC) Wang et al. ([2017](#bib.bib25)). It is also possible for
    the agent directly generates an entity, which is not explored before, based on
    its inherent knowledge. Therefore, we have to link the entity to its corresponding
    MID in the KG. This entity linking process is divided into two steps: (1) We retrieve
    some similar entities and their corresponding types based BM25 scores. (2) We
    utilize the LLM to select the most relevant entity based on the types.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Finish[answer], indicates that the agent finishes the task with answer.
    It should be noticed that the agent would also generate "Finish[unknown]", which
    means that there is not enough information for the agent to answer the question.
    In this case, we would roll back and search one more hop neighbors of the last
    target entity.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | CWQ |  | WebQSP |  |'
  prefs: []
  type: TYPE_TB
- en: '| without external knowledge |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| IO prompt | 37.6 |  | 63.3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 38.8 |  | 62.2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| CoT+SC | 45.4 |  | 61.1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| RoG w/o planning | 43.0 |  | 66.9 |  |'
  prefs: []
  type: TYPE_TB
- en: '| with external knowledge |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | CKG | IKG-1 | CKG | IKG-1 |'
  prefs: []
  type: TYPE_TB
- en: '| w training |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| RoG | 64.5 | 51.1 | 88.7 | 70.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatKBQA | 76.5 | 35.0 | 78.1 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o training |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| KB-BINDER | - | - | 50.7 | 34.4 |'
  prefs: []
  type: TYPE_TB
- en: '| StructGPT | - | - | 76.4 | 53.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ToG | 47.2 | 36.7 | 76.9 | 60.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GoG w/GPT-3.5 (Ours) | 55.7 | 43.4 | 78.7 | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| GoG w/GPT-4 (Ours) | 75.2 | 61.0 | 84.4 | 74.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The Hits@1 scores of different models over two datasets in different
    settings (%). CKG and IKG-1 denote using complete and incomplete KG (dropping
    one crucial triple for each question), respectively. We use GPT-3.5 as backbone
    of KB-BINDER, StructGPT and ToG. Results of the other baselines were re-run by
    us, more details can be found in Appendix [C](#A3 "Appendix C Settings for Baselines
    ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").
    The boldface indicates the best result in the same setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Experiments Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following previous works Li et al. ([2023d](#bib.bib13)); Jiang et al. ([2023](#bib.bib8));
    Sun et al. ([2023](#bib.bib20)), we use Hits@1 as our evaluation metric, which
    measures the proportion of questions whose top-1 predicted answer is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The baselines we compare can be divided into two groups: (1) LLM only methods,
    including standard prompting (IO prompt) Brown et al. ([2020](#bib.bib3)), Chain-of-Thought
    (CoT) prompting Wei et al. ([2023](#bib.bib27)) and Self-Consistency (SC) Wang
    et al. ([2023b](#bib.bib26)). (2) Semantic Parsing (SP) methods, including KB-BINDER
    Li et al. ([2023c](#bib.bib12)) and ChatKBQA Luo et al. ([2023a](#bib.bib14)).
    (3) Retrieval Augmented (RA) methods, including StructGPT Jiang et al. ([2023](#bib.bib8)),
    RoG Luo et al. ([2023b](#bib.bib15)) and ToG Sun et al. ([2023](#bib.bib20)),
    where RoG is the SOTA among all models requiring fine-tuning. All these methods
    are evaluated in both complete incomplete KGs.'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use two LLMs as the backbone in our experiments: GPT-3.5 and GPT-4\. We
    do not use Llama-2-70b-chat-hf Touvron et al. ([2023](#bib.bib23)), as we found
    that Llama’s output sometimes did not follow the format we defined in few-shot.
    We use OpenAI API to call GPT-3.5 and GPT-4\. The maximum token length for each
    generation is set to 256\. The temperature parameter is set to 0.7 in all experiments.
    We use 3 shots in GoG prompts for all the datasets. The prompts we use are listed
    in Appendix [B](#A2 "Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods
    Details ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge
    Graph Question Answering"). The details of re-running other baselines can be found
    in Appendix [C](#A3 "Appendix C Settings for Baselines ‣ Appendix B Prompt List
    ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | IKG-1 | IKG-2 | IKG-3 | IKG-4 |'
  prefs: []
  type: TYPE_TB
- en: '| CWQ | 2.3 | 4.3 | 5.9 | 6.8 |'
  prefs: []
  type: TYPE_TB
- en: '| WebQSP | 2.1 | 3.6 | 4.6 | 5.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The average number of edges deleted under different incompleteness
    degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each dataset, we generate four incomplete KGs with varying degrees of completeness:
    IKG-1/2/3/4, representing randomly dropping 1/2/3/4 crucial triples in the KG.
    In addition to the crucial triples themselves, all relations between these two
    entities will also be deleted. The statistics of these incomplete KGs are shown
    in Table [2](#S6.T2 "Table 2 ‣ Experiment Details ‣ 6.1 Experiments Setup ‣ 6
    Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"). Besides, we also ensure that after deleting
    these crucial triples, the number of neighbor nodes of the topic entities will
    not be zero, more details can be found in Appendix [D](#A4 "Appendix D Statistics
    of Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt
    List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: '|       Method |       WebQSP |'
  prefs: []
  type: TYPE_TB
- en: '|       CKG |       IKG-1 |       IKG-2 |       IKG-3 |       IKG-4 |'
  prefs: []
  type: TYPE_TB
- en: '|       StructGPT |       76.4 |       52.5 |       49.1 |       47.9 |       46.4
    |'
  prefs: []
  type: TYPE_TB
- en: '|       ToG |       76.9 |       60.6 |       57.8 |       58.0 |       57.3
    |'
  prefs: []
  type: TYPE_TB
- en: '|       GoG |       78.7 |       62.2 |       60.9 |       59.6 |       57.4
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |       CWQ |'
  prefs: []
  type: TYPE_TB
- en: '|  |       CKG |       IKG-1 |       IKG-2 |       IKG-3 |       IKG-4 |'
  prefs: []
  type: TYPE_TB
- en: '|       ToG |       47.2 |       36.7 |       33.1 |       32.2 |       31.7
    |'
  prefs: []
  type: TYPE_TB
- en: '|       GoG |       55.7 |       41.6 |       37.2 |       36.7 |       36.5
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The Hits@1 scores of prompt based methods (w/ GPT-3.5) under different
    numbers of missing triples (%). CKG represent using the complete KG. IKG-1/2/3/4
    represent randomly dropping 1/2/3/4 crucial triples in the KG.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    shows the Hits@1 scores of GoG and all baselines on two datasets in different
    settings. From the table, we can find that, compared with other prompt based methods,
    GoG can achieve the state-of-the-art performance on CWQ and WebQSP in both complete
    and incomplete KG settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the CKG setting, the excellent performance of GoG mainly comes from its
    dynamic subgraph expansion strategy, which performs better than ToG in question
    involving compound value types (CVTs), an example of CVT is demonstrated in Figure
    [4](#S6.F4 "Figure 4 ‣ 6.2 Main Results ‣ 6 Experiments ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering"). ToG
    is likely to think CVT nodes are not worthy to further explore and ignore them,
    as they do not offer information directly. Our GoG can easily solve this problem
    by expanding subgraph dynamically, that means if there is not enough information
    provided by the current subgraph, GoG would search one more hop, so the neighbors
    of CVT nodes is taken into consideration in this way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the IKG setting, the performance of SP methods, such as ChatKBQA and KB-BINDER,
    significantly declines. This is expected, as these SP methods don’t interact with
    the KGs, which means they have no idea of the absence of some triples. The performance
    of RoG, StructGPT and ToG also drop significantly. The performance of ToG and
    StructGPT on IKG is even worse than that without KG (IO prompt and CoT). That
    means, these methods still play a role of parsing to find answers rather than
    effectively integrating internal and external knowledge sources. Besides, in the
    IKG setting, these methods are even likely to retrieve wrong or unrelated paths
    which disturb LLMs. Our GoG can alleviate the problem by using the generate action,
    which utilizes the LLM to generate new factual triples when no direct answer is
    found after multiple rounds of searches on the KGs. Detailed analysis of answers
    generated by GoG can be checked in Appendix [E](#A5 "Appendix E Result Analysis
    ‣ Appendix D Statistics of Topic Entities in IKGs ‣ Appendix C Settings for Baselines
    ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8021fa6694da753388ce7bf359a283b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An example of compound value types (CVTs) in Freebase dataset. Blue,
    green and orange nodes denote normal entities, CVT node and property node.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Performance under Different Numbers of Missing Triples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to explore the impact of different degrees of KG incompleteness on
    different methods, we evaluate the performance of methods (w/ GPT-3.5) under different
    numbers of missing triples, the results are demonstrated in Table [3](#S6.T3 "Table
    3 ‣ Datasets Details ‣ 6.1 Experiments Setup ‣ 6 Experiments ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: It can be found that our GoG outperforms other prompt based methods consistently
    in different numbers of missing triples. Especially on the CWQ dataset, our GoG
    has a significant improvement on Hits@1 score, achieving average 7.5% improvement
    under all settings. That emphasizes the importance of integrate the external and
    inherent knowledge of LLMs. On the contrary, the performance of ToG on IKG is
    even much lower than that without KG, which indicates the performance of ToG still
    depends heavily on the completeness of KGs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, even though the majority of questions in the WebQSP dataset are
    single-hop questions, GoG can still demonstrate its advantages and perform better
    than ToG and StructGPT. This is because GoG can leverage the neighboring information
    of the topic entities to predict the tail entities while other methods can not
    make full use these information. More details can be found in Appendix [F](#A6
    "Appendix F Case Study ‣ Appendix E Result Analysis ‣ Appendix D Statistics of
    Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt
    List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Performance with Different LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Method | WebQSP |'
  prefs: []
  type: TYPE_TB
- en: '| CKG | IKG-1 | NKG |'
  prefs: []
  type: TYPE_TB
- en: '| GoG w/GPT-3.5 | 78.7 | 64.9 | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GoG w/GPT-4 | 84.4 | 74.8 | 65.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CWQ |'
  prefs: []
  type: TYPE_TB
- en: '|  | CKG | IKG-1 | NKG |'
  prefs: []
  type: TYPE_TB
- en: '| GoG w/GPT-3.5 | 55.7 | 43.4 | 38.8 |'
  prefs: []
  type: TYPE_TB
- en: '| GoG w/GPT-4 | 75.2 | 61.0 | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The Hits@1 scores of GoG using different backbone models (%). CKG,
    IKG and NKG denote using complete, incomplete and no KG.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6294ffb1797fdb7c96c1b90f7a6e6ec5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The Hits@1 scores of GoG with different generation strategies on
    the CWQ (a) and WebQSP (b) (%), w subgraph and w/o subgraph represent generating
    new factual triples with and without explored subgraph as context in the Generate
    action, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73304816b46e9f74193d38ea7713090e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The Hits@1 scores of GoG with different number of related triples
    in the Generate action (%).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate how different backbone models affect GoG performance. Table [4](#S6.T4
    "Table 4 ‣ 6.4 Performance with Different LLMs ‣ 6 Experiments ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    demonstrates that the performance of GoG using GPT-4 as backbone improves significantly.
    Especially under complete KGs setting, GoG (w/GPT-4) achieves 84.4 and 75.2 Hits@1
    score on the WebQSP and CWQ datasets respectively, which achieve SOTA performance
    in prompt based methods and outperforms most fine-tuned methods.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we can also find that, the more powerful the backbone model, the
    better it utilizes the incomplete KGs. For example, on the WebQSP dataset, the
    performance of GoG (w/GPT-3.5) using incomplete KG is only 2.5% higher than that
    without using KG, while the improvement increases to 9.2% when using GPT-4 as
    backbone.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Effect of Explored Subgraphs
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the influence of explored subgraphs on the performance of GoG, we
    conduct experiments under different two Generate action (introduced in section
    [5](#S5 "5 Generate-on-Graph (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")) strategies: (1) Utilizing
    explored subgraphs, which is obtained in Search action, as context to generate
    new factual triples. (2) Generating new factual triples directly without explored
    subgraphs. As shown in Figure [5](#S6.F5 "Figure 5 ‣ 6.4 Performance with Different
    LLMs ‣ 6 Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"), GoG’s performance improves significantly
    with the help of explored subgraphs. This implies that, even incomplete KGs don’t
    provide answers directly, the related knowledge they provide is still helpful
    for models generating corresponding knowledge. There are two potential reasons:
    (1) Utilizing related subgraphs as context can activate LLMs’ memory corresponding
    to this knowledge. (2) LLMs can reason new factual triples based on the related
    subgraphs, as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    (d).'
  prefs: []
  type: TYPE_NORMAL
- en: The Effect of the Number of Related Triples
  prefs: []
  type: TYPE_NORMAL
- en: 'Aiming to find out how many related triples is required in the Generate action,
    we perform additional to find out how the number of related triples effect GoG’s
    performance. We select the most relevant k triples based on BM25. The results
    are shown in Figure [6](#S6.F6 "Figure 6 ‣ 6.4 Performance with Different LLMs
    ‣ 6 Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"). It can be observed that, GoG’s performance
    first increases and then decreases as the number of related triples increases.
    This is mainly because noisy and unrelated knowledge are introduced when the number
    of related triples is large. How to filter valuable knowledge from the explored
    subgraph could be a future direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose leveraging LLMs for QA under Incomplete Knowledge
    Graph (IKGQA), which is closer to real-world scenarios, and construct relevant
    datasets. We propose Generate-on-Graph (GoG), which can effectively integrate
    the external and inherent knowledge of LLMs. Experiments on two datasets show
    the superiority of GoG, and demonstrate that an incomplete KG can still help LLMs
    answer complex questions.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The limitations of our proposed GoG are as follows: (1) We only evaluate GoG
    on CWQ and WebQSP, which uses Freebase as their background KG. The experimental
    data may comes from some specific domains. (2) It is possible for LLM to hallucinate
    in the generation process, which is unavoidable for existing LLMs. (3) There is
    room for further improvement in performance, as GoG’s performance is lower than
    that with CoT prompt when KGs are very incomplete.'
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper proposes a method for complex question answering in incomplete knowledge
    graph, and the experiments are conducted on public available datasets. As a result,
    there is no data privacy concern. Meanwhile, this paper does not involve human
    annotations, and there are no related ethical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale Fung. 2023. [A Multitask, Multilingual, Multimodal Evaluation
    of ChatGPT on Reasoning, Hallucination, and Interactivity](https://doi.org/10.48550/arXiv.2302.04023).
    ArXiv:2302.04023 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
    Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database
    for structuring human knowledge. In *Proceedings of the 2008 ACM SIGMOD international
    conference on Management of data*, pages 1247–1250.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language Models are Few-Shot Learners](https://doi.org/10.48550/arXiv.2005.14165).
    ArXiv:2005.14165 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. [A Survey on In-context
    Learning](https://doi.org/10.48550/arXiv.2301.00234). ArXiv:2301.00234 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2023. [Complexity-Based Prompting for Multi-Step Reasoning](http://arxiv.org/abs/2210.00720).
    ArXiv:2210.00720 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and
    Ting Liu. 2023. [A Survey on Hallucination in Large Language Models: Principles,
    Taxonomy, Challenges, and Open Questions](http://arxiv.org/abs/2311.05232). ArXiv:2311.05232
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2021) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and
    Philip S. Yu. 2021. [A Survey on Knowledge Graphs: Representation, Acquisition
    and Applications](http://arxiv.org/abs/2002.00388). *arXiv:2002.00388 [cs]*. ArXiv:
    2002.00388.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023. [StructGPT: A General Framework for Large Language
    Model to Reason over Structured Data](http://arxiv.org/abs/2305.09645). In *EMNLP
    2023*. arXiv. ArXiv:2305.09645 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khot et al. (2023) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle
    Richardson, Peter Clark, and Ashish Sabharwal. 2023. [Decomposed Prompting: A
    Modular Approach for Solving Complex Tasks](http://arxiv.org/abs/2210.02406).
    In *NIPS 2023*. arXiv. ArXiv:2210.02406 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and
    Ji-Rong Wen. 2023a. [HaluEval: A Large-Scale Hallucination Evaluation Benchmark
    for Large Language Models](http://arxiv.org/abs/2305.11747). ArXiv:2305.11747
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li,
    Xifeng Yan, Chao Zhang, and Bing Yin. 2023b. [Graph Reasoning for Question Answering
    with Triplet Retrieval](http://arxiv.org/abs/2305.18742). ArXiv:2305.18742 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023c) Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu
    Chen. 2023c. [Few-shot In-context Learning on Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-long.385).
    In *ACL 2023*, pages 6966–6980, Toronto, Canada. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023d) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq
    Joty, Soujanya Poria, and Lidong Bing. 2023d. [Chain-of-Knowledge: Grounding Large
    Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](http://arxiv.org/abs/2305.13269).
    ArXiv:2305.13269 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023a) Haoran Luo, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai
    Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, et al. 2023a. Chatkbqa:
    A generate-then-retrieve framework for knowledge base question answering with
    fine-tuned large language models. *arXiv preprint arXiv:2310.08975*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023b) Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui
    Pan. 2023b. [Reasoning on Graphs: Faithful and Interpretable Large Language Model
    Reasoning](http://arxiv.org/abs/2310.01061). ArXiv:2310.01061 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2023) Zhijie Nie, Richong Zhang, Zhongyuan Wang, and Xudong Liu.
    2023. [Code-Style In-Context Learning for Knowledge-Based Question Answering](https://doi.org/10.48550/arXiv.2309.04695).
    ArXiv:2309.04695 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. 2023. [Unifying Large Language Models and Knowledge Graphs: A
    Roadmap](https://doi.org/10.48550/arXiv.2306.08302). ArXiv:2306.08302 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxena et al. (2020) Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020.
    Improving multi-hop question answering over knowledge graphs using knowledge base
    embeddings. In *Proceedings of the 58th annual meeting of the association for
    computational linguistics*, pages 4498–4507.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Haitian Sun, Tania Bedrax-Weiss, and William W Cohen. 2019.
    Pullnet: Open domain question answering with iterative retrieval on knowledge
    bases and text. *arXiv preprint arXiv:1904.09537*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2023. [Think-on-Graph:
    Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://doi.org/10.48550/arXiv.2307.07697).
    ArXiv:2307.07697 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Yawei Sun, Lingling Zhang, Gong Cheng, and Yuzhong Qu. 2020.
    Sparqa: skeleton-based semantic parsing for complex questions over knowledge bases.
    In *Proceedings of the AAAI conference on artificial intelligence*, volume 34,
    pages 8952–8959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. The web as a
    knowledge-base for answering complex questions. *arXiv preprint arXiv:1803.06643*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen,
    Xinrun Wang, Lei Feng, and Bo An. 2023a. [keqing: knowledge-based question answering
    is a nature chain-of-thought mentor of LLM](http://arxiv.org/abs/2401.00426).
    ArXiv:2401.00426 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge
    Graph Embedding: A Survey of Approaches and Applications. *IEEE TRANSACTIONS ON
    KNOWLEDGE AND DATA ENGINEERING*, 29(12):20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. [Self-Consistency Improves
    Chain of Thought Reasoning in Language Models](https://doi.org/10.48550/arXiv.2203.11171).
    ArXiv:2203.11171 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models](https://doi.org/10.48550/arXiv.2201.11903).
    ArXiv:2201.11903 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. [ReAct: Synergizing Reasoning and Acting
    in Language Models](http://arxiv.org/abs/2210.03629). ArXiv:2210.03629 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yih et al. (2016a) Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei
    Chang, and Jina Suh. 2016a. The value of semantic parse labeling for knowledge
    base question answering. In *Proceedings of the 54th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers)*, pages 201–206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yih et al. (2016b) Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei
    Chang, and Jina Suh. 2016b. The value of semantic parse labeling for knowledge
    base question answering. In *Proceedings of the 54th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers)*, pages 201–206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zan et al. (2022) Daoguang Zan, Sirui Wang, Hongzhi Zhang, Kun Zhou, Wei Wu,
    Wayne Xin Zhao, Bingchao Wu, Bei Guan, and Yongji Wang. 2022. Complex question
    answering over incomplete knowledge graph as n-ary link prediction. In *2022 International
    Joint Conference on Neural Networks (IJCNN)*, pages 1–8\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Semantic Parsing Methods Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training datasets for SP methods are constructed under the complete KGs,
    which means that "Time Zone" corresponds directly to the relation "ns:location.location.time_zones"
    rather than a two-hop path "ns:location.located_in -> ns:location.location.time_zones".
    An example in CWQ is shown in Table [A](#A1 "Appendix A Semantic Parsing Methods
    Details ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge
    Graph Question Answering"). This means SP models trained on CWQ will always output
    "?c ns:location.location.time_zones ?x" instead of "?c ns:location.located_in
    ?y . ?y ns:location.location.time_zones ?x". Therefore, these methods will fail
    under Incomplete KGs. In another word, semantic parsing methods don’t interact
    with the KGs, which means they have no idea of the absence of some triples.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | In the nation that spends the Bahamian dollar as currency, what
    time zone is used? |'
  prefs: []
  type: TYPE_TB
- en: '| SPARQL | PREFIX ns: <http://rdf.freebase.com/ns/> |'
  prefs: []
  type: TYPE_TB
- en: '| SELECT DISTINCT ?x WHERE { |  |'
  prefs: []
  type: TYPE_TB
- en: '| FILTER (?x != ?c) FILTER (!isLiteral(?x) OR lang(?x) = ” OR langMatches(lang(?x),
    ’en’)) |  |'
  prefs: []
  type: TYPE_TB
- en: '| ?c ns:location.country.currency_used ns:m.01l6dm . |  |'
  prefs: []
  type: TYPE_TB
- en: '| ?c ns:location.location.time_zones ?x . |  |'
  prefs: []
  type: TYPE_TB
- en: '| } |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: An example about "timezone" in the CWQ dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Prompt List
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prompts used in GoG are shown in Table [B](#A2 "Appendix B Prompt List
    ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks | Prompt |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GoG Instruction | Solve a question answering task with interleaving Thought,
    Action, Observation steps. Thought can reason about the current situation, and
    Action can be three types: |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (1) Search[entity1 &#124; entity2 &#124; …], which searches the exact entities
    on Freebase and returns their one-hop subgraphs. You should extract the all concrete
    entities appeared in your last thought without redundant words, and you should
    always select entities from topic entities in the first search. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (2) Generate[thought], which generate some new triples related to your last
    thought. These new triples may come from your inherent knowledge directly or reasoning
    from the given triples. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (3) Finish[answer1 &#124; answer2 &#124; …], which returns the answer and
    finishes the task. The answers should be complete entity label appeared in the
    triples. If you don’t know the answer, please output Finish[unknown]. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Entities and answers should be separated by tab. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Attention please, entities begin with "m." (e.g., m.01041p3) represent CVT
    (compound value type) node, and they shouldn’t be selected as the final answers.
    To find out those entities involved in these event, you could select them as the
    entities to be searched. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| You should generate each step without redundant words. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Here are some examples. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| In-Context Few-shot |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Question: {Question} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Topic Entity: {List of Topic Entities} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Thought 1: |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Filter Relations | Please select 3 relations that most relevant to the question
    and rank them. You should answer these relations in list format directly without
    redundant words. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Here are some examples. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| In-Context Few-shot |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Thought: {Thought} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Entity: {Entity} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Relation: {List of Relations} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Answer: |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generate Triples | Given the existing triples, please generate some new triples
    related to your current thought. These new triples may come from your inherent
    knowledge directly or reasoning from the given triples. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Here are some examples. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| In-Context Few-shot |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Thought: {Thought} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Known Triples: {Explored Triples} |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generated Triples: |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Prompts for different tasks used in GoG.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Settings for Baselines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following ToG, the Freebase dump is acquired from [https://developers.google.com/freebase?hl=en](https://developers.google.com/freebase?hl=en),
    we deploy Freebase with Virtuoso. GoG, RoG, KB-BINDER and ChatKBQA are evaluated
    on the same Freebase database.
  prefs: []
  type: TYPE_NORMAL
- en: 'RoG. We use the checkpoints and the default settings provided by the official
    repository: n_beam=3 in generating rule, max_new_tokens=512 in inferring answers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatKBQA. We use the predicted S-expression provided by the official repository,
    and convert them into SPARQL queries. To compare ChatKBQA with other models fairly,
    we execute these SPARQL queries under the Freebase database mention before instead
    the DB files provided by them. Therefore, the performance of ChatKBQA reported
    in Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    is slightly different from that in their original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'KB-BINDER. We use the official repository and use KB-BINDER (6)-R (with majority
    vote and retrieve the most similar exemplars) to infer answers. However, the code-davinci-002
    used in their original paper is not available, so we use GPT-3.5 instead. Besides,
    to reduce runtime, we decreased the number of candidate MID combinations (despite
    that, it still takes about 4 hours to answer 200 questions). Therefore, the performance
    of KB-BINDER reported in Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG)
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") is slightly different from that in their original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ToG. We use the official repository and their default settings for inferring
    answers: max_length=256, width=3, depth=3\. Since the official repository doesn’t
    provide the alias answers in the CWQ dataset, we evaluate ToG on the CWQ dataset
    without considering alias answers (the same strategy for all models). Therefore,
    the performance of ToG reported in Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph
    (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge
    Graph Question Answering") is slightly different from that in their original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: StructGPT. We use the official repository and running scripts to evaluate StructGPT
    on the WebQSP dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Statistics of Topic Entities in IKGs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The statistics of topic entities are shown in Table [8](#A4.T8 "Table 8 ‣ Appendix
    D Statistics of Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix
    B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering"),
    and we drop those samples which have isolated topic entities (topic entity without
    any neighbor node).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset |  | IKG-1 | IKG-2 | IKG-3 | IKG-4 |'
  prefs: []
  type: TYPE_TB
- en: '| CWQ | Median number of neighbor nodes | 27 | 26 | 27 | 27 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of isolated topic entities | 19 | 42 | 59 | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| WebQSP | Median number of neighbor nodes | 428 | 427 | 427 | 426 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Number of isolated topic entities | 1 | 2 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Statistics of topic nodes in Incomplete KGs. Isolated topic entity
    represent topic entity without any neighbor node.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Result Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [9](#A5.T9 "Table 9 ‣ Appendix E Result Analysis ‣ Appendix D Statistics
    of Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt
    List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering") illustrates
    the frequency of the Generate operation in different datasets alongside their
    corresponding Hits@1 scores. In the complete KGs setting, GoG still conducts the
    Generate operation when related relations are not correctly selected or when answers
    to sub-questions cannot be directly found via a one-hop relationship. In the incomplete
    KGs setting, the frequency of the Generate operation is higher, as GoG needs to
    generate new factual triples that are missing in the KGs. Hits@1 scores under
    both settings mean that most generation leading to correct results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We considered four types of errors: (1) Knowledge Missing. (2) Hallucination.
    (3) Error before Generate. (4) False Negative. The distribution is shown in Figure
    [7](#A5.F7 "Figure 7 ‣ Appendix E Result Analysis ‣ Appendix D Statistics of Topic
    Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt List
    ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering"). It can be
    found that there are indeed about 26% error are caused by hallucination. Besides,
    the Figure [6](#S6.F6 "Figure 6 ‣ 6.4 Performance with Different LLMs ‣ 6 Experiments
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") demonstrates that the performance of the model improves with
    the number of relevant triples introduced in the Generation step, which means
    introducing more related triples can alleviate hallucination problems to a certain
    extent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c37f6ba4f25c9311ee0c79a2bae1a741.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The error proportions of GoG under IKG-1 of CWQ.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Models | CWQ | WebQSP |'
  prefs: []
  type: TYPE_TB
- en: '| CKG | GPT-3.5 | 17.7% (55.9) | 17.1% (66.6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4 | 4.6% (54.3) | 9.7% (65.9) |'
  prefs: []
  type: TYPE_TB
- en: '| IKG-1 | GPT-3.5 | 23.7% (57.3) | 24.1% (63.4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4 | 8.8% (57.9) | 18.4% (70.6) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Times of Generate operation in different KG settings. Numbers in brackets
    represent corresponding Hits@1 score.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present a case analysis to evaluate the utility of GoG,
    as demonstrated in Table [F](#A6 "Appendix F Case Study ‣ Appendix E Result Analysis
    ‣ Appendix D Statistics of Topic Entities in IKGs ‣ Appendix C Settings for Baselines
    ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").
    In this case, GoG will first search the neighbors of "Appalachian Mountains" and
    obtain <Bald Eagle Mountain, mountain.mountain_range, Appalachian Mountains>,
    <Spaulding Mountain, mountain.mountain_range, Appalachian Mountains> and <Old
    Rag Mountain, mountain.mountain_range, Appalachian Mountains>, then GoG can infer
    that Appalachian Mountains are also located in North America simply by knowing
    information about the other three mountains. However, ToG fails on this question
    once the crucial triple <Appalachian Mountains, location.location.containedby,
    North America> is missing. In another word, for one-hop questions, GoG can use
    the neighbors information of the topic entities to predict the tail entities while
    other methods can not make full use of the neighbors information.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Where do the appalachian mountains run? |'
  prefs: []
  type: TYPE_TB
- en: '| Dropped Crucial Triple | Appalachian Mountains, location.location.containedby,
    North America |'
  prefs: []
  type: TYPE_TB
- en: '| ToG | Reasoning Chains: None |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction: I’m sorry, but I need more information to answer your question.
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| GoG | Thought 1: I need to search the Appalachian Mountains to find out where
    they run. |'
  prefs: []
  type: TYPE_TB
- en: '| Act 1: Search[Appalachian Mountains] |  |'
  prefs: []
  type: TYPE_TB
- en: '| Obs 1: Appalachian Mountains, location.contains, Brasstown Bald |  |'
  prefs: []
  type: TYPE_TB
- en: '| Appalachian Mountains, location.contains, Craggy Dome |  |'
  prefs: []
  type: TYPE_TB
- en: '| Appalachian Mountains, location.geolocation, m.02_qbv6 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Bald Eagle Mountain, mountain.mountain_range, Appalachian Mountains |  |'
  prefs: []
  type: TYPE_TB
- en: '| Spaulding Mountain, mountain.mountain_range, Appalachian Mountains |  |'
  prefs: []
  type: TYPE_TB
- en: '| Old Rag Mountain, mountain.mountain_range, Appalachian Mountains |  |'
  prefs: []
  type: TYPE_TB
- en: '| Thought 2: The Appalachian Mountains contain various locations and are part
    of geolocations. To find out where they run, I need to generate some new triples
    based on these given triples and my inherent knowledge. |  |'
  prefs: []
  type: TYPE_TB
- en: '| Act 2:  Generate[where do the Appalachian Mountains run] |  |'
  prefs: []
  type: TYPE_TB
- en: '| Obs 2: Appalachian Mountains, geographical_feature.contains_location, Blue
    Ridge Mountains |  |'
  prefs: []
  type: TYPE_TB
- en: '| Appalachian Mountains, geographical_feature.start_location, Alabama |  |'
  prefs: []
  type: TYPE_TB
- en: '| Appalachian Mountains, geographical_feature.start_location, North America
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Thought 3: The Appalachian Mountains start from North America |  |'
  prefs: []
  type: TYPE_TB
- en: '| Act 3: Finish[Alabama &#124; North America ] |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ground Truth | North America |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Comparison between ToG and GoG.'
  prefs: []
  type: TYPE_NORMAL
