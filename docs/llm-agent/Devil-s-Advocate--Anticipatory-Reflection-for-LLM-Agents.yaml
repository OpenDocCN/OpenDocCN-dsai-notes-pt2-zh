- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:48'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Devil’s Advocate: Anticipatory Reflection for LLM Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16334](https://ar5iv.labs.arxiv.org/html/2405.16334)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Haoyu Wang¹   Tao Li²  Zhiwei Deng²  Dan Roth¹   Yang Li² ¹UPenn   ²Google DeepMind
    {why16gzl, danroth}@seas.upenn.edu {tlinlp, zhiweideng, liyang}@google.com Work
    done during internship at Google DeepMind.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this work, we introduce a novel approach that equips LLM agents with introspection,
    enhancing consistency and adaptability in solving complex tasks. Our approach
    prompts LLM agents to decompose a given task into manageable subtasks (i.e., to
    make a plan), and to continuously introspect upon the suitability and results
    of their actions. We implement a three-fold introspective intervention: 1) anticipatory
    reflection on potential failures and alternative remedy before action execution,
    2) post-action alignment with subtask objectives and backtracking with remedy
    to ensure utmost effort in plan execution, and 3) comprehensive review upon plan
    completion for future strategy refinement. By deploying and experimenting with
    this methodology—a zero-shot approach—within WebArena for practical tasks in web
    environments, our agent demonstrates superior performance with a success rate
    of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest
    that our introspection-driven approach not only enhances the agent’s ability to
    navigate unanticipated challenges through a robust mechanism of plan execution,
    but also improves efficiency by reducing the number of trials and plan revisions
    by 45% needed to achieve a task.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two roads diverged in a yellow wood,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And sorry I could not travel both
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: $\cdots$
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then took the other, as just as fair,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And having perhaps the better claim
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Robert Frost
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The enduring appeal of Frost’s emblematic poem, “The Road Not Taken,” resides
    not just in its poetic elegance, but also in the profound lesson it imparts about
    decision-making. As we stand at the crossroads of a choice, it is a daunting challenge
    to assess probable outcomes and choose a course that best aligns with our objectives.
    This task becomes even more formidable when Large Language Model (LLM) agents
    [[9](#bib.bib9), [29](#bib.bib29), [18](#bib.bib18)] have to navigate complex
    scenarios unfolding in real time, e.g., solving tasks in web environments [[11](#bib.bib11),
    [27](#bib.bib27), [2](#bib.bib2), [32](#bib.bib32)], conducting simulated science
    experiments [[23](#bib.bib23)], and solving embodied household tasks [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, LLM agent decision-making has witnessed enhancement by post-hoc reflection
    and correction [[16](#bib.bib16), [19](#bib.bib19)], coupled with adaptive planning
    [[20](#bib.bib20), [15](#bib.bib15)], where the agents learn from past successes
    and failures while concurrently mapping out flexible strategies. However, frequent
    shifts in plans, albeit a mere inconvenience for humans, can lead to disorientation
    for AI agents. This may produce confusion, a standstill, or even an infinite loop
    of failure, which substantiates the importance of *thoroughly executing a set
    plan with utmost effort before resorting to a plan revision*. Therefore, this
    paper puts forward a methodology aimed at achieving an optimal balance between
    consistency and adaptability. This critical equilibrium mirrors the resilience
    and agility that is anticipated of a capable system that is prepared for curveballs
    but unwavering in the execution of its plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we introduce a novel approach that integrates introspection
    into the fabric of LLM agents. This approach enables agents to continuously reflect
    on their actions, thereby stimulating a learning process that dynamically optimizes
    exploration paths and enhances robust decision-making under uncertainty. Our introspective
    intervention focuses on three principal dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Anticipatory reflection before action execution (similar to a devil’s advocate);
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Post-action evaluation and backtracking with remedy when necessary, to ensure
    the outcome aligns with subtask objectives;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An extensive review upon plan completion to generate finer plans for subsequent
    trials.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We implement this introspective methodology within WebArena [[32](#bib.bib32)],
    a comprehensive web environment featuring 812 tasks in five scenarios: online
    shopping, e-commerce management, social discussion forums, maps, and software
    development platforms. Experimental results demonstrate that our approach, which
    is zero-shot, significantly outperforms existing zero-shot methods while improving
    efficiency, paving the way for a new paradigm of intelligent systems that are
    more consistent, adaptable, and effective at solving problems¹¹1Code to reproduce
    our results will be released..'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we develop and expand upon several key themes within the realm
    of natural language processing, with a specific focus on the integration of action
    generation, planning, and reflection in the construction of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Generation: LLMs have been employed in tasks requiring decision-making
    or action generation and have proven useful as agent-controlling policies in embodied
    environments [[9](#bib.bib9), [8](#bib.bib8), [3](#bib.bib3), [21](#bib.bib21),
    [33](#bib.bib33)]. They have also demonstrated effectiveness in text-based environments
    [[11](#bib.bib11), [17](#bib.bib17), [12](#bib.bib12)], where techniques like
    ReAct [[29](#bib.bib29)] have shown notable benefits. Despite its success, ReAct’s
    limitation lies in its inability to adjust to changes in the environment. Several
    improvements [[13](#bib.bib13), [16](#bib.bib16)] have been proposed to counter
    these limitations, advocating for self-reflection to enhance decision-making and
    reasoning. However, these techniques primarily aim to improve single plans or
    trajectories without considering alternative actions, which could modify the plan
    in a wrong direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Position Bias Mitigation: While comparing answer choices is generally effective,
    large language models used for action generation are not without flaws. They can
    exhibit bias, especially towards the first (or sometimes second) answer they see,
    regardless of its quality. This is known as position bias [[30](#bib.bib30), [22](#bib.bib22)].
    Our method mitigates this bias by asking follow-up questions that challenge its
    own answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Planning: Extensive research has explored the potential of LLMs in task planning
    [[4](#bib.bib4), [15](#bib.bib15), [20](#bib.bib20), [25](#bib.bib25), [5](#bib.bib5),
    [6](#bib.bib6)]. The concept of decoupling planning and execution in formulating
    LLM agents has been validated through numerous paradigms such as ReWOO [[26](#bib.bib26)],
    ADaPT [[15](#bib.bib15)], Structured Self-Reflection [[10](#bib.bib10)], and DEFS
    [[24](#bib.bib24)]. Nonetheless, these methods exhibit a deficiency in establishing
    a resilient mechanism for plan execution, with agents frequently revisiting and
    revising their plans following each instance of adverse environmental feedback,
    often due to inaccurately executed actions. Our approach, conversely, emphasizes
    executing a previously defined plan with unwavering effort before considering
    any modifications. This guarantees a more stable and consistent problem-solving
    process. To implement this, the factor of tree search becomes crucial for exploring
    the best solutions. Past approaches, including ToT [[28](#bib.bib28)], RAP [[7](#bib.bib7)],
    LATS [[31](#bib.bib31)], AdaPlanner [[20](#bib.bib20)], and ToolChain* [[34](#bib.bib34)],
    have incorporated tree search techniques in identifying the optimal route to the
    desired solution. However, our approach distinguishes itself by engaging the LLM
    in preparing alternate solutions in anticipation of impending failures, ensuring
    more comprehensive consideration in action generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflection and Self-refinement: Reflection and refinement techniques have advanced
    significantly through works such as Reflexion [[16](#bib.bib16)], AdaPlanner [[20](#bib.bib20)],
    and AutoEval [[14](#bib.bib14)]. Our methodology further enhances this by incorporating
    an anticipatory reflection mechanism that operates before each action rather than
    performing post-action reflection. This approach simplifies exploration by expediting
    remedial action and reducing extensive backtracking and serial plan revisions,
    thereby improving efficiency in the overall task handling process.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given a task $\mathcal{T}$ interacts, our objective is to enable the agent
    to systematically and adaptively complete the task through introspective methods.
    We first present how we decompose the task and generate action regarding each
    state in the environment in [section 3.1](#S3.SS1 "3.1 Task Decomposition and
    Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")
    and [section 3.2](#S3.SS2 "3.2 State and Action Representation ‣ 3 Method ‣ Devil’s
    Advocate: Anticipatory Reflection for LLM Agents"). Then we introduce the introspection
    mechanism in [section 3.3](#S3.SS3 "3.3 Introspective Mechanisms ‣ 3 Method ‣
    Devil’s Advocate: Anticipatory Reflection for LLM Agents").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Task Decomposition and Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step involves decomposing the task $\mathcal{T}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{P}\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, the plan $\mathcal{P}$ is parsed into a sequence of ordered subtasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{P}=(\tau_{1},\tau_{2},\ldots,\tau_{N}),$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $\tau_{i}$ is the number of subtasks. For instance, Fig. [1](#S3.F1 "Figure
    1 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory
    Reflection for LLM Agents") shows a plan with 5 subtasks for solving a task in
    WebArena. The distribution of WebArena tasks based on the number of subtasks within
    each task is illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Task Decomposition
    and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents").
    This also reflects the difficulty of the tasks in WebArena, where most tasks take
    4-8 steps to complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plan for task: What is the color configuration of the picture frame I bought
    in Nov 2022: 1. Click on the ‘My Account’ link to access your account details.
    2. Click on the ‘Order History’ link to view your past orders. 3. Scroll down
    the page until you find the order from November 2022. 4. Click on the order details
    link for the order from November 2022. 5. Scroll down to the product details section
    to find the color configuration of the picture frame.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: An example plan with 5 subtasks, generated by GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca8c33cce37d828cc62d7a7b22afdd25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Distribution of WebArena tasks based on the number of subtasks within
    each task.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 State and Action Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let $S_{t}\in\mathcal{S}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a_{t}\sim G_{\text{action}}(\tau_{i},S_{t},\mathcal{H}_{t-1}),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $G_{\text{action}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{H}_{t}=\{\hat{a}_{1},\hat{a}_{2},\ldots,\hat{a}_{t}\},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\hat{a}_{t}$ accepts as input the state before the action, the action
    itself, the state after the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{a}_{t}\sim G_{\text{describe}}(S_{t},a_{t},S_{t+1}).$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'When the state observation is too long to fit in the context window of an LLM,
    the state is first summarized by the LLM into a shorter description before being
    fed to $G_{\text{describe}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{C}_{\tau_{i}}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{C}_{\mathcal{T}}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $G_{\text{completed}}$ is finished.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Introspective Agent
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: task $\mathcal{T};$'
  prefs: []
  type: TYPE_NORMAL
- en: time $t=0;$
  prefs: []
  type: TYPE_NORMAL
- en: 1:while $\neg G_{\text{completed}}(\mathcal{T},\cdot)$
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Introspective Mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sequential action generation above can potentially execute the plan and
    solve the task already. Nevertheless, without proper introspection and adaptation,
    the agent might be stuck at a certain unsolvable subtask or go into a loop of
    failure when unexpected problems emerge. Thus, we introduce three introspective
    mechanisms to enhance our LLM agent’s problem-solving ability below.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Anticipatory Reflection (Devil’s Advocate)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first layer of introspection occurs before each action execution. The agent
    anticipates potential failures and comes up with $R$:'
  prefs: []
  type: TYPE_NORMAL
- en: '"If your answer above is not correct, instead, the next action should be:"'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use $G_{\text{remedy}}$ at first attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a_{t}^{r}$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'If later found necessary, the agent can go back to state $S_{t}$ is the last
    one to be pushed to the stack so it can be popped and executed first (see line
    18 in Alg. [1](#alg1 "Algorithm 1 ‣ 3.2 State and Action Representation ‣ 3 Method
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"))..'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Post-action Evaluation and Backtracking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The second introspective mechanism kicks in after the execution of each action.
    Here, the agent evaluates whether the action and the resulting state align with
    the subtask objective. This introspective function, denoted as $G_{\text{align}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta_{t}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: Here $\theta_{t}\in(0,1)$. When backtracking, we can easily navigate back to
    the URL. However, the element information on the URL might differ from the state
    we first encountered upon arriving at that page. To address this, we prompt the
    LLM to map the recorded element in the action to the new element with which we
    want to interact, if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7527811ae7f3eb980610c19ad51afbf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Screen observation at one step in solving the subtask: Click on the
    order details link for the order from November 2022. The agent might decide to
    click ($a_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Plan Revision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The third introspective mechanism occurs upon plan failure, i.e., when the
    stack is empty and $\mathcal{C}_{\mathcal{T}}=0$. Now the agent performs a thorough
    review of the actions executed and the notes taken, and refines its future plan
    based on identified problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{P}_{\text{new}}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathcal{P}_{\text{new}}$ is the new plan after reflecting on the past
    failed trials. The agent then re-enters the plan execution phase and starts a
    new episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through these three layers of introspection, our agent is more capable of navigating
    the complexities of unforeseen circumstances and addressing tasks, bringing us
    a significant stride closer to achieving truly autonomous, adaptable, and intelligent
    systems. By structuring the problem in this manner, we have established a clear
    framework for enabling LLM agents to perform tasks autonomously and adaptively
    through introspection. Alg. [1](#alg1 "Algorithm 1 ‣ 3.2 State and Action Representation
    ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents") shows
    a pseudo code demonstration of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/71f8a7b1151e2277b23f400a791de26b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Decision making process of our agent in solving the task: What is
    the color configuration of the picture frame that I bought in Sep 2022? Before
    execution of the predicted action, the agent asks a follow-up question to itself
    regarding its decision: what if the picture frame is not in order #179? what should
    be the alternative remedy? And after finding out that order #179 contains no picture
    frame at all, the agent backtracks to the previous state to view order #175 and
    continue.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we demonstrate how introspection enhances consistency and
    adaptability of LLM agents in solving complex tasks in web environments. We first
    introduce the experimental setup for evaluation ([section 4.1](#S4.SS1 "4.1 Experimental
    Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")),
    followed by evaluation results ([section 4.2](#S4.SS2 "4.2 Results ‣ 4 Experiments
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")). Detailed error
    analysis is provided in [section 4.3](#S4.SS3 "4.3 Error Analysis ‣ 4 Experiments
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"), which highlights
    the directions for future endeavor.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Live environments: We evaluate our proposed method in the simulated web environments
    of WebArena [[32](#bib.bib32)], a dataset of human-annotated web browsing tasks
    designed to evaluate the ability of LLMs to perform complex, real-world actions
    on the internet. The 812 tasks in WebArena involve five websites: an online shopping
    website, a software development website, a social forum platform, a map, and an
    e-commerce management platform; and these tasks can be categorized into three
    classes: information seeking tasks, site navigation and content & config tasks,
    and unachievable tasks. Though WebArena provides visual observation (screenshots),
    in this work we use the text observation only. The observation at each step is
    the accessibility tree of the webpage, and the elements in the accessibility tree
    are all within the current viewport of a 1280$\times$720 screen. The action space
    of our LLM agent includes actions that interact with environment: click, type,
    scroll, goto, go_back, go_forward, and also a note_down action that takes down
    useful snippet/summary for answering information-seeking questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines: We employ gpt-4-0613⁴⁴4[https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    [[1](#bib.bib1)] with a context window of 8k tokens to build the agents and compare
    our method with three other agent construction strategies: planning and sequential
    decision making (Plan + Act w/o reflexion), similar to ReWOO [[26](#bib.bib26)];
    planning and sequential decision making with reflection (Plan + Act), similar
    to AdaPlanner [[20](#bib.bib20)]; and tree search based planning, similar to LATS
    [[31](#bib.bib31)], but with reflection. In all methods, we set the upper limit
    on the number of actions to 30, i.e., after the agent executes 30 actions for
    a given task, it has to stop. In all three methods, we adopt the same prompts
    for action generation $G_{\text{action}}$ to ensure a fair comparison⁵⁵5Detailed
    prompts are shown in Appendix.. In our experiments, we set the LLM temperature
    to 1.0 and max_tokens to 512, and keep all other parameters as default.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics: We follow the evaluation metric Success Rate in [[32](#bib.bib32)],
    and count the number of actions per trial and the number of plan revisions per
    task. To determine whether a task is successfully completed, the exact_match metric
    is used for some site navigation and information seeking tasks. However, this
    can sometimes be overly stringent. For instance, consider the URLs below that
    display the same content (under ‘electronics’, the category id of ‘headphones’
    is 60): both of them are correct answers but only one exact match with the gold
    answer (the first one) is considered correct⁶⁶6URL string matching is used to
    determine if a task is finished.. To address this issue, we manually review the
    evaluation process and correct such cases in our results.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://localhost:7770/electronics/headphones.html](http://localhost:7770/electronics/headphones.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://localhost:7770/electronics.html?cat=60](http://localhost:7770/electronics.html?cat=60)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d728ed85de1214acf62000ce97f4d50d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Results of different agent construction strategies on WebArena. AR
    is short for our method, anticipatory reflection; LATS represents our in-house
    implementation of the approach proposed by Zhou et al. [[31](#bib.bib31)]; Plan
    + Act is a method of decomposition of task and execution of each subtask, similar
    to ReWOO [[26](#bib.bib26)]. All three methods are equipped with plan revision
    (post-failure reflection).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The experimental results, depicted in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"),
    demonstrate the efficacy of our introspection-driven approach in enhancing the
    consistency and adaptability of LLM agents in web environments. We compare the
    success rates of various agent construction strategies across multiple episodes.
    Our method, anticipatory reflection (AR), consistently outperforms the others,
    achieving a success rate of 23.5% after seven episodes, closely followed by LATS
    with 22.7%. In contrast, the Plan + Act method shows gradual improvement, reaching
    19.8%, but remains significantly lower than the tree-search-based AR and LATS
    methods. Taking a closer look at the last few rounds of LATS reveals marginal
    improvements due to the homogeneous generated actions through direct sampling.
    In comparison, AR benefits from the “devil’s advocate” approach, enabling more
    thorough planning and execution due to introspective follow-up questions. This
    trend underscores the importance of incorporating introspection mechanisms for
    both plan execution and revision, highlighting their critical role in achieving
    more consistent and efficient results.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | # of Actions | # of Plan Revisions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | First Trial | Last Trial |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Plan+Act | 4.01 | 4.47 | 2.03 |'
  prefs: []
  type: TYPE_TB
- en: '| LATS | 6.08 | 6.45 | 1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| AR | 6.39 | 7.07 | 0.64 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistics of the trajectory of different agents solving tasks on
    WebArena. We report the number of actions in the first and last trial, and also
    the number of plan revisions, i.e., trials.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further insights can be gleaned from [Table 1](#S4.T1 "In 4.2 Results ‣ 4 Experiments
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"), which compares the
    average number of actions in the first and last trials across different methods.
    Our AR method shows an increase in the average number of actions from 6.39 in
    the first trial to 7.07 in the last trial, indicating a robust learning and adaptation
    process. In comparison, the average number of actions in the first trial of the
    Plan+Act method is only 4.01, suggesting that it stops at an early stage without
    completing full plan execution. Thus, our method effectively leverages a greater
    number of actions to achieve better outcomes, thereby reducing the number of plan
    revisions by 45% and improving overall efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Error Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The subsequent sections shed light on an analysis of errors we observed from
    the agent’s behavior when executing tasks. Two key areas have been identified
    for detailed discussion: an agent’s occasional inability to fully learn from past
    failures, and inefficiencies in solving specific kinds of tasks due to a sequential
    planning scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Agent only takes partial lesson from past failures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One category of errors we notice is that the agent is not taking full lesson
    from past failure in generating a new plan. As illustrated in Fig. [6(a)](#S4.F6.sf1
    "Figure 6(a) ‣ Figure 6 ‣ 4.3.1 Agent only takes partial lesson from past failures
    ‣ 4.3 Error Analysis ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents"), the agent is at the final step of drafting a refund message
    for a Bluetooth speaker, after a series of steps taken to seek information for
    the order. From the screen, we know that the agent should consolidate all the
    gathered information and type one piece of text into the (only) box titled “What’s
    on your mind?”. However, as depicted in Fig. [6(b)](#S4.F6.sf2 "Figure 6(b) ‣
    Figure 6 ‣ 4.3.1 Agent only takes partial lesson from past failures ‣ 4.3 Error
    Analysis ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"),
    while some improvements were made by adding the date of purchase and a more detailed
    explanation in the revised plan, the agent still failed to optimize the input
    process, repeating the typing actions separately for fields that do not exist.
    This inefficiency in the agent’s behavior showcases the need for either an LLM
    with stronger reasoning ability or a better mechanism to solicit more comprehensive
    and accurate reflection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc9f7b43b60762d498bfc6f1662c52b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Screen observation at the last step to solve the task: Draft a refund message
    via their "contact us" form for the bluetooth speaker I bought Feb 2023\. It broke
    after three days of use. The shop requires the order id, the reason and the amount
    to refund in the message. Don’t submit yet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial Plan: • … • Type the order id into the appropriate field in the contact
    form. • Type "The bluetooth speaker I bought broke after three days of use" into
    the ‘Reason’ field of the contact form. • Type the amount to refund into the appropriate
    field in the contact form. Revised Plan: • … • Type the order id ‘000000161’ into
    the appropriate field in the contact form. • Type "The bluetooth speaker I bought
    in Feb 2023 broke after three days of use. I would like to request a refund."
    into the ‘Reason’ field of the contact form. • Type the amount to refund ‘$56.35’
    into the appropriate field in the contact form.'
  prefs: []
  type: TYPE_NORMAL
- en: (b) Comparison between the last few steps of the initial plan and the revised
    plan for the same task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Illustration of the first type of errors: the agent is not taking
    full lesson from past experience in plan revision. The agent does not capture
    the root cause of its failure, i.e., separately typing each piece of information
    into different fields that do not even exist.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Sequential planning cannot solve all tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our analysis, we observed a recurrent error pertaining to the design of
    the agent’s planning process. Currently, the proposed methodology structures a
    plan as a sequence of tasks that are executed in a specific order. This approach,
    effective in a decent amount of use cases, seems to falter when faced with tasks
    necessitating more sophisticated logic. Specifically, tasks that mandate implementing
    a function encapsulating several actions, employing a loop construct, or those
    executed repetitively, tend to challenge the model’s current configuration. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List out reviewers, if exist, who mention about average print quality
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give me the SKU of the products that have 1-3 units left.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like all submissions created by CameronKelsey in subreddit earthporn.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The ability to process these tasks effectively would necessitate the incorporation
    of additional cognitive constructs into the planning model—e.g., loops, repetitive
    actions, or encapsulation of a group of actions into callable functions. Though
    taking notes can help the agent eliminate wrong choices, these systemic extensions
    would add crucial capabilities to the web agent, significantly enhancing its navigation
    and problem-solving competence in realistic web environments. Moreover, while
    the current agent can succeed in the limited search space of simple tasks, it
    often struggles to review and introspect upon more extensive descriptive tasks
    requiring dynamic problem-solving. By addressing these limitations in future work,
    i.e., effectively converting textual description of a plan into robust execution
    of callable functions and loops, we believe that the reasoning capability of our
    agent can be substantially improved, leading to better outcomes in understanding
    and solving tasks that involve dynamic cognition in web environments.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we introduce a novel introspective methodology that significantly
    enhances the problem-solving capabilities of LLMs in complex environments, as
    demonstrated through comprehensive evaluations in the WebArena setting. Our approach
    strategically decomposes tasks into actionable subtasks and incorporates a three-tiered
    introspection process, which includes anticipatory reflection, robust post-action
    evaluation, and episode-level plan revision. This setup not only allows LLM agents
    to adapt their strategies in real time but also fosters long-term learning, reducing
    the need for frequent interventions as experience accumulates. The successful
    application of our introspective methodology in the WebArena suggests its potential
    transferability to other domains that demand dynamic decision-making such as autonomous
    driving, healthcare, and interactive customer services. By enabling LLM agents
    to proactively contemplate potential failures, evaluate actions post-execution,
    and continuously refine their strategy based on experiential insights, our approach
    equips AI systems with a human-like strategic thinking capability.
  prefs: []
  type: TYPE_NORMAL
- en: Broader Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking forward, the integration of multi-modal data inputs could further enhance
    the contextual understanding and decision-making accuracy of these agents. The
    principles and findings from our approach provide a robust foundation for future
    research in AI, particularly in aspects of autonomous decision-making, learning
    efficiency, and adaptability. As AI continues to integrate into diverse aspects
    of decision-making, embedding introspective capabilities will be essential to
    ensure these systems operate not only with precision but with an understanding
    akin to strategic human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the capabilities of LLM agents enhance and their deployment in real-world
    applications increases, it is crucial to address potential ethical concerns, particularly
    regarding data privacy, bias, and transparency. Our work focuses on improving
    agent introspection to enhance task performance and decision-making explanations,
    aiming to develop more transparent and trustworthy AI systems. We emphasize the
    importance of human oversight to monitor and mitigate unforeseen consequences
    and encourage the responsible use of this technology for societal benefit. By
    promoting continuous evaluation and fair practices, we seek to minimize biases
    and ensure that the deployment of these agents does not exacerbate social inequalities.
    Furthermore, we are committed to optimizing computational resources to reduce
    the environmental impact, advocating for sustainable AI practices.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite substantial progress made with our current design, limitations persist
    that inhibit optimal performance. Notably, the agent lacks a full learning mechanism
    to capitalize on past failures when generating a new plan, resulting in inefficient
    execution and recurring mistakes. Furthermore, while the sequential planning approach
    is effective for simpler tasks, it falls short for more sophisticated operations,
    such as those requiring encapsulated actions or loop constructs. Additionally,
    the agent struggles with tasks that expand beyond a simple search space, suggesting
    obstacles in handling dynamic problem-solving. Last but not least, our agent needs
    significant amounts of LLM generation (i.e., API calling), consequently requiring
    substantial time and computational resources, which dents its efficiency. Therefore,
    future work needs to concentrate on improving the agent’s ability to fully learn
    from prior shortcomings, adapt to handle complex tasks, enhance dynamic problem-solving
    capabilities, and optimize time and resource utilization with more efficient LLM
    calling.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2023] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2Web: Towards a Generalist Agent for
    the Web](http://arxiv.org/abs/2306.06070).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,
    Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
    Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal
    Language Model. In *arXiv preprint arXiv:2303.03378*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dror et al. [2023] Rotem Dror, Haoyu Wang, and Dan Roth. 2023. [Zero-Shot On-the-Fly
    Event Schema Induction](https://doi.org/10.18653/v1/2023.findings-eacl.53). In
    *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    705–725, Dubrovnik, Croatia. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guan et al. [2023] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging Pre-trained Large Language Models to Construct
    and Utilize World Models for Model-based Task Planning](http://arxiv.org/abs/2305.14909).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gur et al. [2024] Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. [A Real-World WebAgent
    with Planning, Long Context Understanding, and Program Synthesis](https://openreview.net/forum?id=9JQtrumvg8).
    In *The Twelfth International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with Language Model is Planning with World
    Model](https://doi.org/10.18653/v1/2023.emnlp-main.507). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022a] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
    Mordatch. 2022a. Language Models as Zero-Shot Planners: Extracting Actionable
    Knowledge for Embodied Agents. *arXiv preprint arXiv:2201.07207*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022b] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre
    Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,
    and Brian Ichter. 2022b. Inner Monologue: Embodied Reasoning through Planning
    with Language Models. In *arXiv preprint arXiv:2207.05608*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. 2023.
    [A Zero-Shot Language Agent for Computer Control with Structured Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.753).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    11261–11274, Singapore. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided
    exploration. *arXiv preprint arXiv:1802.08802*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs
    as Agents. *arXiv preprint arXiv: 2308.03688*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. [2023] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. [Self-Refine: Iterative Refinement with Self-Feedback](http://arxiv.org/abs/2303.17651).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. [2024] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, and Alane Suhr. 2024. [Autonomous Evaluation and Refinement of Digital
    Agents](http://arxiv.org/abs/2404.06474).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al. [2023] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter
    Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2023. ADaPT: As-Needed
    Decomposition and Planning with Language Models. *arXiv*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language Agents with Verbal
    Reinforcement Learning](http://arxiv.org/abs/2303.11366).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. [2021] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2021. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768).
    In *Proceedings of the International Conference on Learning Representations (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2023] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler,
    Wei-Lun Chao, and Yu Su. 2023. LLM-Planner: Few-Shot Grounded Planning for Embodied
    Agents with Large Language Models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2024] Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. 2024. Trial and Error: Exploration-Based Trajectory Optimization
    for LLM Agents. *arXiv preprint arXiv:2403.02502*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2023] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. 2023. [AdaPlanner: Adaptive Planning from Feedback with Language Models](http://arxiv.org/abs/2305.16653).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An Open-Ended
    Embodied Agent with Large Language Models. *arXiv preprint arXiv: Arxiv-2305.16291*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023b] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai
    Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. [Large Language Models
    are not Fair Evaluators](http://arxiv.org/abs/2305.17926).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. 2022. [ScienceWorld: Is your Agent Smarter than a 5th Grader?](https://doi.org/10.18653/v1/2022.emnlp-main.775)
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 11279–11298, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023c] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. 2023c. [Describe, Explain, Plan and Select: Interactive Planning
    with LLMs Enables Open-World Multi-Task Agents](https://openreview.net/forum?id=KtvPdGb31Z).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2023] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan.
    2023. Embodied Task Planning with Large Language Models. *arXiv preprint arXiv:2305.03716*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2023] Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee,
    Yuchen Liu, and Dongkuan Xu. 2023. [ReWOO: Decoupling Reasoning from Observations
    for Efficient Augmented Language Models](http://arxiv.org/abs/2305.18323).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [preprint] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    preprint. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language
    Agents. In *ArXiv*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a. [Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models](https://openreview.net/forum?id=5Xc1ecxO1h).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *International Conference on Learning Representations (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging LLM-as-a-Judge with MT-Bench
    and Chatbot Arena](https://openreview.net/forum?id=uccHPGDlao). In *Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2024a] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. 2024a. [Language Agent Tree Search Unifies Reasoning Acting
    and Planning in Language Models](https://openreview.net/forum?id=6LNTSrJjBe).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2024b] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon,
    and Graham Neubig. 2024b. [WebArena: A Realistic Web Environment for Building
    Autonomous Agents](https://openreview.net/forum?id=oKn9c6ytLx). In *The Twelfth
    International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. 2023. Ghost in the Minecraft: Generally Capable Agents for Open-World
    Environments via Large Language Models with Text-based Knowledge and Memory. *arXiv
    preprint arXiv:2305.17144*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. [2024] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. 2024. [ToolChain*: Efficient
    Action Space Navigation in Large Language Models with A* Search](https://openreview.net/forum?id=B6pQxqUcT8).
    In *The Twelfth International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt for Plan Generation ($G_{\text{plan}}$)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you are imitating humans doing a task on a website step by step.
    You can click an element with the mouse, scroll up or down, go to a certain URL
    or go back to previous page, or type some text with the keyboard (e.g., click(),
    scroll(), goto(), go_back(), and type() functions in playwright). One step means
    one operation within any of the mentioned actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are within a sandbox and only have access to the following websites to
    work with:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An online shopping website (OneStopShop): {webarena_root}:7770'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An e-commerce management website (Magento): {webarena_root}:7780/admin'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Reddit website (Postmill): {webarena_root}:9999'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A GitLab website: {webarena_root}:8023'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A map website (OpenStreetMap): [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Wikipedia website: [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Notes:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to use the search function, you don’t need to click on the search
    bar. You can directly use “type [element_id] [things_to_type]”, and generally
    afterwards, you don’t need to click the search button (by default, the command
    contains an ENTER at the end).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can assume that you have signed in to your account (we have set up the cookies,
    so login is not needed).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The website that you will be working with is:'
  prefs: []
  type: TYPE_NORMAL
- en: '{WEBSITE INTRO}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please follow these specific instructions to solve tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '{INSTRUCTION}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a more detailed description of the starting screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '{STARTING SCREEN DESCRIPTION}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, based on the information above, what should be the steps to achieve the
    following goal (please give me a list of textual description of playwright actions,
    starting with ‘List’):'
  prefs: []
  type: TYPE_NORMAL
- en: '{TASK}'
  prefs: []
  type: TYPE_NORMAL
- en: 'For your reference, here are some experiences from previous failed trials (please
    consider the following information to generate a better plan):'
  prefs: []
  type: TYPE_NORMAL
- en: '{FAILED PLAN}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Past experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '{HISTORY}'
  prefs: []
  type: TYPE_NORMAL
- en: To be successful in generating a new plan, you need to provide a list (1, 2,
    3, …), in which each item is a natural language description of one playwright
    action that is necessary to complete the task (e.g., click on the ‘Account’ button;
    scroll down; use the search bar to search for iPhone 13). You should use the information
    from the past experiences to save unnecessary steps!
  prefs: []
  type: TYPE_NORMAL
- en: Prompt for Action Generation ($G_{\text{action}}$)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I am in a sandbox and only have access to the following websites (i.e., no
    access to external website like www.reddit.com):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An online shopping website (OneStopShop): {webarena_root}:7770'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An e-commerce management website (Magento): {webarena_root}:7780/admin'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Reddit website (Postmill): {webarena_root}:9999'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A GitLab website: {webarena_root}:8023'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A map website (OpenStreetMap): [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Wikipedia website: [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now I’m trying to complete a task on a website.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task is:'
  prefs: []
  type: TYPE_NORMAL
- en: '{TASK}'
  prefs: []
  type: TYPE_NORMAL
- en: 'The plan to complete this task is:'
  prefs: []
  type: TYPE_NORMAL
- en: '{PLAN}'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have executed the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '{HISTORY}'
  prefs: []
  type: TYPE_NORMAL
- en: 'And now I’m at this step: {STEP}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the screen I am looking at:'
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS}'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have taken down the following notes:'
  prefs: []
  type: TYPE_NORMAL
- en: '{NOTES}'
  prefs: []
  type: TYPE_NORMAL
- en: What should be the next action to complete this step in my plan (only give one
    action)?
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the next action is to click, please indicate the element id in [] (format:
    click [element_id]).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the next action is to scroll, please indicate the direction in [] (format:
    scroll [up or down]).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you need to navigate to a URL, please indicate the URL in [] (format: goto
    [url]).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need to go back to the previous page, please use go_back.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the next action is to type, please indicate both element id and the things
    to type in [] (format: type [element_id] [things to type]).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to note down something, use this format: note_down [things to note
    down].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next action is:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt for Objective Alignment ($G_{\text{align}}$)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you are imitating humans doing a task on a website step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are currently working on this step:'
  prefs: []
  type: TYPE_NORMAL
- en: '{STEP}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The step above is one of the steps in the following plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '{PLAN}.'
  prefs: []
  type: TYPE_NORMAL
- en: From Screen 1, you executed an action and then arrived at Screen 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The action you executed was:'
  prefs: []
  type: TYPE_NORMAL
- en: '{ACTION}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Screen 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS1}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Screen 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS2}.'
  prefs: []
  type: TYPE_NORMAL
- en: Now describe what this action is about in one sentence, starting with ‘The action
    is to’.
  prefs: []
  type: TYPE_NORMAL
- en: Does this action align with the goal of the following step (i.e., are we moving
    towards the right direction; Answer YES or NO)?
  prefs: []
  type: TYPE_NORMAL
- en: '{STEP}'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt for Task / Subtask Completion Evaluation ($G_{\text{completed}}$)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you are imitating humans doing a task on a website step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are asked to solve the following task:'
  prefs: []
  type: TYPE_NORMAL
- en: '{TASK}'
  prefs: []
  type: TYPE_NORMAL
- en: 'You made the following plan to solve it:'
  prefs: []
  type: TYPE_NORMAL
- en: '{PLAN}'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reach the current screen, you have previously executed the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '{HISTORY}'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have taken down a few notes after each action as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '{NOTES}'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the accessibility tree of the current screen you are looking at:'
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS}'
  prefs: []
  type: TYPE_NORMAL
- en: Look at the screen, the task, and the actions you executed, and think thoroughly,
    is the task completed now?
  prefs: []
  type: TYPE_NORMAL
- en: If the task is completed, answer YES.
  prefs: []
  type: TYPE_NORMAL
- en: If the task is not yet completed (meaning further actions are yet to be executed),
    answer NO.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt for Answer Delivery ($G_{\text{answer}}$)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you are imitating humans doing a task on a website step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are asked to solve the following task:'
  prefs: []
  type: TYPE_NORMAL
- en: '{TASK}'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reach the current screen, you have previously executed the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '{HISTORY}'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have taken down the following notes (to help you answer the question eventually)
    after each action:'
  prefs: []
  type: TYPE_NORMAL
- en: '{NOTES}'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is the accessibility tree of the current screen you are looking at:'
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the above information, answer the question in the task (starting with
    ###Answer).'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt for Element Mapping ($G_{\text{map}}$)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I want to interact with an element with element id: {element_id} in the following
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS1}'
  prefs: []
  type: TYPE_NORMAL
- en: Now if I want to click on the same element in the following screen, what should
    be the element id now?
  prefs: []
  type: TYPE_NORMAL
- en: '{OBS2}'
  prefs: []
  type: TYPE_NORMAL
- en: 'New element id is:'
  prefs: []
  type: TYPE_NORMAL
