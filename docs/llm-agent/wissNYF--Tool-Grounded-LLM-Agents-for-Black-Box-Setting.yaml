- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'wissNYF: Tool Grounded LLM Agents for Black Box Setting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10051](https://ar5iv.labs.arxiv.org/html/2402.10051)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Somnath Sendhil Kumar¹  Dhruv Jain¹  Eshaan Agarwal¹  Raunak Pandey¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹ [Intelligence Group, IIT (BHU), Varanasi](https://cops-iitbhu.github.io/IG-website/)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While Large Language Models (LLMs) have demonstrated enhanced capabilities in
    function-calling, these advancements primarily rely on accessing the functions’
    responses. This methodology is practical for simpler APIs but faces scalability
    issues with irreversible APIs that significantly impact the system, such as a
    database deletion API. Similarly, processes requiring extensive time for each
    API call and those necessitating forward planning, like automated action pipelines,
    present complex challenges. Furthermore, scenarios often arise where a generalized
    approach is needed because algorithms lack direct access to the specific implementations
    of these functions or secrets to use them. Traditional tool planning methods are
    inadequate in these cases, compelling the need to operate within black-box environments.
    Unlike their performance in tool manipulation, LLMs excel in black-box tasks,
    such as program synthesis. Therefore, we harness the program synthesis capabilities
    of LLMs to strategize tool usage in black-box settings, ensuring solutions are
    verified prior to implementation. We introduce TOPGUN, an ingeniously crafted
    approach leveraging program synthesis for black box tool planning. Accompanied
    by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning
    and verification tasks, addressing the aforementioned challenges and enhancing
    the versatility and effectiveness of LLMs in complex API interactions. The public
    code for SwissNYF is available at  [https://github.com/iclr-dummy-user/SwissNYF](https://github.com/iclr-dummy-user/SwissNYF)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88919d8f983c79f36ab95072684db8d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of different settings that an LLMs may require to manipulate
    tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Significant advancements in Large Language Models (LLMs) like GPT (Radford et al.
    ([2018](#bib.bib26)); Radford et al. ([2019](#bib.bib27)); Brown et al. ([2020](#bib.bib4));
    Achiam et al. ([2023](#bib.bib1))) and PaLM (Chowdhery et al. ([2023](#bib.bib8));Anil
    et al. ([2023](#bib.bib2));) have demonstrated profound abilities in reasoning
    and following instructions over an extensive array of tasks Huang & Chang ([2023](#bib.bib14)).
    The recent shift towards leveraging LLMs to interact with external tools for addressing
    complex real-world challenges marks a significant area of interest (Hao et al.
    ([2023](#bib.bib13)); Zhang et al. ([2023a](#bib.bib36)); Zhuang et al. ([2023b](#bib.bib42));
    Yang et al. ([2023](#bib.bib33)); Schick et al. ([2023](#bib.bib28));Lu et al.
    ([2023a](#bib.bib17));). In addressing intricate problems, autonomous agents powered
    by LLMs employ an amalgamation of LLMs and various external tools (APIs), crafting
    solutions that necessitate a sequence of intermediate reasoning steps (Schick
    et al. ([2023](#bib.bib28));Lu et al. ([2023a](#bib.bib17));Lu et al. ([2023a](#bib.bib17));Patil
    et al. ([2023](#bib.bib24));Qin et al. ([2023](#bib.bib25))). When presented with
    a problem, These agents’ primary objective is to identify and execute a series
    of API function calls sequentially, leading to a coherent solution. These approaches
    are ineffective when queries lack transparency or when the APIs are irreversible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We coin the term "black-box" settings in the context of tool planning as scenarios
    where the outcomes of an API or tool are not observable. This framework is especially
    pertinent in systems where using certain APIs poses risks, such as those causing
    inconsistencies by deleting or updating database entries, canceling jobs, or performing
    similar operations. It’s also relevant where API experimentation incurs high costs
    or when APIs require considerable time to execute, ensuring clarity and comprehensive
    coverage without redundancy, making it challenging to interpret their outcomes.
    We present a taxonomy of such systems Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") into three branches:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'White Box Systems: In these settings, planners can invoke the API, receive
    responses, access the source code and understand its complex logic. This access
    enables the system to navigate complex inputs, intricacies and use cases efficiently.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gray Box Systems: Planners in these environments have descriptions of the tools
    at their disposal and the capability to call the API and receive responses. The
    system’s planning relies solely on the limited descriptions provided and the responses
    for each tool.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Black Box Systems: In the most challenging scenarios, planners are confined
    to tool descriptions without access to actual tool outputs. Here, the planner
    must decipher the dynamics of each tool based solely on its description, making
    it a particularly demanding task to formulate responses to queries.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Zhuang et al. ([2023a](#bib.bib41)) and Qin et al. ([2023](#bib.bib25))
    methods excel in straightforward scenarios where an agent can iterate over tools
    to identify the optimal path, yet they lack efficiency and necessitate extensive
    exploration. Approaches like Yao et al. ([2022](#bib.bib34)) and Parisi et al.
    ([2022](#bib.bib22)), subsets of this exploratory paradigm, offer enhanced efficiency
    yet frequently falter due to their constrained directionality in tool search,
    making them suitable predominantly for straightforward API challenges. In contrast,
    the Zhang et al. ([2023b](#bib.bib37)) approach is efficient regarding API execution
    costs by constraining the number of calls. However, it omits any form of verification
    for its proposed trajectory, diminishing its precision in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'These methodologies in tool application present a dichotomy between accuracy
    and computational overhead. While generally unsuitable for black-box settings,
    the Reverse chain approach exhibits potential for adaptation within such frameworks.
    On the other hand, program synthesis-based algorithms have been instrumental in
    exalting reasoning and decision-making capabilities within LLMs, offering a more
    naturally associative decision-making process than that afforded by mere text.
    Works like The Chain of Code Li et al. ([2023](#bib.bib15)) and Program-of-thoughts
    Chen et al. ([2022](#bib.bib7)) are great examples of using code generation to
    improve decision-making for answering general open-domain questions. To this end,
    few works also upheld the reasoning capability of LLMs using code like "TORA:
    A Tool-Integrated Reasoning Agent for Mathematical Problem Solving" Gou et al.
    ([2023](#bib.bib12)), "Solving challenging math word problems using gpt-4 code
    interpreter with code-based self-verification" Zhou et al. ([2023b](#bib.bib40))
    and "PAL: Program-aided Language Models" Gao et al. ([2023](#bib.bib11)) have
    exploited code interpreters for zero-shot verified solving, substantially surpassing
    few-shot learning benchmarks by enabling semi-verification of proposed solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: However, works like Paranjape et al. ([2023](#bib.bib21)), which employs code
    synthesis for tool usage, are restricted by their limited toolset and the scalability
    challenge posed by the need for extensive human feedback and interventions and
    the need for the human expert to be familiar with the whole toolset. Similarly,
    works such as Xu et al. ([2023](#bib.bib32)), which deploys language models for
    real-time code generation and command execution within controlled environments,
    are limited by their narrow tool range and a deficit in generalizability. The
    state-of-the-art approaches on HumanEval Chen et al. ([2021](#bib.bib6)) and HumanEval-X
    Zheng et al. ([2023](#bib.bib38)) datasets for code generation, like Reflexion
    Shinn et al. ([2023](#bib.bib29)) and LATS Zhou et al. ([2023a](#bib.bib39)),
    which iterate upon code based on interpreter outputs and reflect over them, these
    approaches have yet to be experimented with in other domains associated with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: To bridge these gaps, we introduce the TOPGUN (Tool Orchestration and Program
    synthesis for Generalizing over UNknown systems) framework, which unifies code
    generation, reasoning, and strategic tool planning designed for complex tasks.
    TOPGUN also verifies the execution plans and does so with exceptional efficiency
    in API cost, effectively addressing the limitations of preceding models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key contributions of our work are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To the best of our knowledge, we are the First to coin the term Black Box setting
    for API usage and developed a suite to encourage the development of algorithms
    for such scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We leverage the program synthesis capabilities of Large Language Models (LLMs)
    to augment their efficacy in tool usage substantially, showcasing a notable enhancement
    in performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present a robust and cost-efficient framework for scalable solutions across
    a wide array of open-domain queries, even when faced with limited knowledge of
    user data/tools. It is also publically hosted to demonstrate the same.¹¹1[https://swiss-nyf.azurewebsites.net/](https://swiss-nyf.azurewebsites.net/)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This paper details our methodology and its evaluation by first elucidating
    the background on Tool planning [2.1](#S2.SS1 "2.1 Problem Formulation ‣ 2 Preliminaries
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") and Code generation
    using LLM [2.2](#S2.SS2 "2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF: Tool
    Grounded LLM Agents for Black Box Setting") followed by detailing individual components
    of the pipeline [3](#S3 "3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting"). Our evaluation is bifurcated into two segments: initially, we undertake
    a gray box [4.1](#S4.SS1 "4.1 Gray Box Evaluation ‣ 4 Experiments ‣ wissNYF: Tool
    Grounded LLM Agents for Black Box Setting") across principal datasets, and subsequently,
    we delve into a black box setting [4.2](#S4.SS2 "4.2 Black Box Evaluation ‣ 4
    Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"). For the
    latter, we have curated a bespoke dataset employing Toolbench prompts, intentionally
    adjusting the dataset to include only limited documentation of widely used libraries.
    This adjustment aims to validate the generalizability of our approach. Additionally,
    we juxtapose our methodology with a tailored variant of the Reverse Chain method
    to scrutinize performance disparities.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8db2eeceb07f2b1d0578e2eac1f5554d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of SwissNYF pipeline for tool usage in Black Box setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Tool planning within the context of a Large Language Model (LLM), denoted as
    $\rho$. This process ensures a structured and coherent response strategy, aligning
    the tools’ capabilities with the query’s specific requirements for an effective
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Code Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The integration of Reflexion Shinn et al. ([2023](#bib.bib29)) with Large Language
    Models (LLM) $\rho$. This methodology enhances code quality and aligns with contemporary
    standards, marking a leap in automated code development and verification. This
    process of iterative code generation can be mathematically denoted as Eq. [1](#S2.E1
    "In 2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting")'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{gathered}c_{i}\leftarrow\rho(q,\textit{feedback}_{i-1},c_{i-1})\\
    \textit{output}\leftarrow\mathcal{I}(c_{i})\\'
  prefs: []
  type: TYPE_NORMAL
- en: \textit{feedback}_{i},\textit{verified}\leftarrow\mathcal{F}(output)\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{gathered}$$ |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: 3 SwissNYF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bca7402a57110e6ec95547c025bb4015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Detailed pipeline of our proposed approach with TOPGUN in SwissNYF'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we introduce SwissNYF, a suite that enables LLM-based agents
    to efficiently navigate the action space to identify a valid solution for problem-solving
    in a black box scenario. SwissNYF is composed of five major components i.e., Function
    Signature Generation $\mathcal{P}$ as in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Problem
    Formulation ‣ 2 Preliminaries ‣ wissNYF: Tool Grounded LLM Agents for Black Box
    Setting"). We explain individual components of the pipeline in the subsequent
    subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02f5a109af928406bf28955481ddf69c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Example output of CodeSynth $\mathcal{P}$ Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4625cf4859efc3c10db6f3a7df9e67ab.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Example output of TOPGUN $\mathcal{G}$ Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Illustration of pseudo function and tool planning generated by CodeSynth
    and TOPGUN, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Function Signature Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Function signatures, conceptualized as pseudo APIs, serve to emulate the behaviour
    of real API functions based on given tool descriptions. This emulation is crucial
    for two primary reasons in our tool planning methodology: firstly, they act as
    stand-ins for actual API calls, thereby enabling LLMs to plan and execute tasks
    with higher efficiency; secondly, they are treated as pre-defined functions, facilitating
    the transformation of tool augmentation into a task akin to code generation, using
    these pseudo functions. These function signatures are distinguished by their docstrings
    and an example return object that aligns with the tool description, equipping
    the planner with the necessary means to effectively address user queries. In the
    context of our SwissNYF implementation, we have adopted a straightforward yet
    effective method for generating these function signatures, termed CodeSynth. The
    efficacy of this approach is further analyzed in [4.3](#S4.SS3 "4.3 CodeSynth
    Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 CodeSynth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a given set of tool descriptions $t\in\mathcal{T}$. Our primary objective
    is to ensure that the arguments and return types of these pseudo-functions remain
    consistent with their descriptions. Additionally, we craft detailed docstrings
    for each pseudo-function to facilitate subsequent processes. A critical aspect
    of CodeSynth is the inclusion of an example return value, which is designed to
    mimic all potential operations the returned object might undergo during the verification
    process. The output generated by CodeSynth is illustrated in Fig. [4(a)](#S3.F4.sf1
    "In Figure 4 ‣ 3.1 Overview ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for
    Black Box Setting"). Moreover, the code generation facilitated by this block benefits
    from validation through Reflexion, as outlined in Eq. [1](#S2.E1 "In 2.2 Code
    Generation ‣ 2 Preliminaries ‣ wissNYF: Tool Grounded LLM Agents for Black Box
    Setting"). Ultimately, the methodologies applied within CodeSynth can be encapsulated
    in Algo. [1](#algorithm1 "In 3.2.1 CodeSynth ‣ 3.2 Function Signature Generation
    ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $\rho$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 1 $\mathcal{P}$: CodeSynth'
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilizing the Function Calling module alongside the Interpreter, we rigorously
    test the pseudo-functions against a wide range of real-world scenarios. This approach
    guarantees that the test cases are comprehensive and reflective of actual function
    usage, allowing us to gather detailed feedback on the pseudo-functions’ performance.
    Such feedback is vital for the iterative improvement of the pseudo-functions,
    significantly enhancing their reliability and applicability in practical settings.
    Prompts for CodeSynth can be documented in [A.1](#A1.SS1 "A.1 Prompts ‣ Appendix
    A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Corpus and Retriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The function signatures, crucial components of our methodology, are systematically
    stored within a corpus for future utilization by any planning system. This corpus
    facilitates the indexing of tool descriptions, enabling the precise retrieval
    of the most appropriate tool based on the index. Notably, the literature documents
    several advanced retrieval systems designed for this purpose, demonstrating exceptional
    accuracy. These include ToolBench IR Qin et al. ([2023](#bib.bib25)), APIRetriever
    Zan et al. ([2022](#bib.bib35)), Instructor-XL Su et al. ([2022](#bib.bib30)),
    and GEAR Lu et al. ([2023b](#bib.bib18)). Our framework incorporates these retrievers,
    with Instructor-XL set as the default option, owing to its proven efficacy. Furthermore,
    we are actively exploring the integration of AnyTool’s Hierarchical API Retriever
    Du et al. ([2024](#bib.bib9)), anticipating significant enhancements to our tool
    retrieval capabilities. This strategic inclusion of multiple retrievers ensures
    our system remains versatile and effective in identifying the most suitable tools
    for a given task, aligning with the latest advancements in retrieval technology.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Planner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have implemented two planning approaches in our framework. The first leverages
    a modified Reverse Chain Zhang et al. ([2023b](#bib.bib37)) to support multiple
    end function calls by decomposing tasks into subtasks and creating sub-trees with
    the original reverse chain technique. The second, TOPGUN, is our proposed code-driven
    planning algorithm, designed for speed, efficiency, consistency, and accuracy,
    especially in black box scenarios. TOPGUN offers a streamlined alternative to
    traditional planning methods, optimizing for complex system navigation and task
    execution with greater reliability and cost-effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 TOPGUN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Input: q: query; $\rho$ code for execution and evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 2 $\mathcal{G}$: TOPGUN'
  prefs: []
  type: TYPE_NORMAL
- en: 'TOPGUN, an acronym for Tool Orchestration and Program synthesis for Generalizing
    over UNknown systems, redefines the approach to addressing user queries $q$, effectively
    depicted in Fig. [4(b)](#S3.F4.sf2 "In Figure 4 ‣ 3.1 Overview ‣ 3 SwissNYF ‣
    wissNYF: Tool Grounded LLM Agents for Black Box Setting"). Leveraging Reflexion
    detailed in Eq.[1](#S2.E1 "In 2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting"), the framework iteratively refines
    responses to the query. The synthesis of these components into the comprehensive
    algorithm is presented in Algo. [2](#algorithm2 "In 3.4.1 TOPGUN ‣ 3.4 Planner
    ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") showcases
    TOPGUN’s capability to navigate through various solution paths. Unlike traditional
    traversal-based techniques, TOPGUN capitalizes on the inherent code-generation
    capabilities of LLMs, facilitating a more direct and efficient solution process.
    This distinction not only enhances efficacy by pinpointing issues with precision
    but also ensures adaptability in black box scenarios, simultaneously optimizing
    performance in gray box settings. A detailed pipeline overview with TOPGUN in
    place is given in Fig.[4(b)](#S3.F4.sf2 "In Figure 4 ‣ 3.1 Overview ‣ 3 SwissNYF
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"). With prompts documented
    in [A.1](#A1.SS1 "A.1 Prompts ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM
    Agents for Black Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f719685ba53074f6132e75958f1b1db0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Illustartion of Self-Reflection Mechanism in TOPGUN'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Verifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Verification is closely linked to the functionality of the Planner $\mathcal{G}$
    can use for subsequent iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our framework, we leverage Reflexion Shinn et al. ([2023](#bib.bib29)),
    detailed in Eq. [1](#S2.E1 "In 2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting") and depicted in Algo. [2](#algorithm2
    "In 3.4.1 TOPGUN ‣ 3.4 Planner ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting"), to seamlessly integrate verification and feedback within
    the TOPGUN methodology. This eliminates the requirement for an additional function
    call module, concentrating instead on directly executing code pertinent to the
    user query. This approach is illustrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4.1
    TOPGUN ‣ 3.4 Planner ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting"), providing a visual representation of the concept.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Parser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Parser $\mathcal{K}$. The process’s efficacy is markedly improved by the
    judicious reuse of elements from the individual trees during their amalgamation.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, for the TOPGUN methodology, we adopt the established Abstract Syntax
    Tree (AST) paradigm Fischer et al. ([2007](#bib.bib10)) to segment the program
    into fundamental function calls, alongside specifying their arguments and return
    values. This segmentation is instrumental in constructing a systematic series
    of tool invocations. This meticulously arranged series, denoted as $St$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire pipeline, as depicted in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Overview
    ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"), emerges
    from the integration of various components designed to effectively address user
    queries through the strategic orchestration of tools within the SwissNYF framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Win Rate of different Candidate and Reference model over G1 set'
  prefs: []
  type: TYPE_NORMAL
- en: '| Candidate | Reference | G1-Instruction | G1-Tool | G1-Category |'
  prefs: []
  type: TYPE_TB
- en: '| T.LLaMA ReACT | ChatGPT ReACT | 45.0 | 42.0 | 47.5 |'
  prefs: []
  type: TYPE_TB
- en: '| T.LLaMA DFSDT | ChatGPT ReACT | 55.0 | 55.3 | 54.5 |'
  prefs: []
  type: TYPE_TB
- en: '| T.LLaMA DFSDT+Ret | ChatGPT ReACT | 62.3 | 59.0 | 55.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT DFSDT | ChatGPT ReACT | 60.5 | 62.0 | 57.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 ReACT | ChatGPT ReACT | 60.0 | 58.8 | 63.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 DFSDT | ChatGPT ReACT | 67.5 | 67.8 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | ChatGPT ReACT | 88.192 | 87.46 | 87.15 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | ChatGPT DFSDT | 78.49 | 77.55 | 76.24 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | T.LLaMA ReACT | 86.72 | 82.94 | 80.80 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT | 81.75 | 75.51 | 73.81 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT+Ret | 80.35 | 77.11 | 75.39 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | GPT4 ReACT | 82.996 | 79.956 | 77.633 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | GPT4 DFSDT | 82.065 | 73.69 | 71.14 |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tool planning datasets, while diverse, often fall short in supporting multi-turn
    and multi-call dialogues, as seen in works by Schick et al. ([2023](#bib.bib28))
    and Tang et al. ([2023](#bib.bib31)), and lack precise evaluation metrics, complicating
    thorough assessments. Even comprehensive datasets like ToolBench by Qin et al.
    ([2023](#bib.bib25)) struggle with aligning to black-box settings, presenting
    significant challenges for evaluating tool planning in such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our evaluation employs the ToolBench benchmark Qin et al. ([2023](#bib.bib25))
    and a specially curated dataset for unchar codebases, assessed in both gray ([4.1](#S4.SS1
    "4.1 Gray Box Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for
    Black Box Setting")) and black box ([4.2](#S4.SS2 "4.2 Black Box Evaluation ‣
    4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting")) settings.
    We benchmark our TOPGUN approach against existing methods using win rate, token
    count, and success rate. Additionally, we scrutinize CodeSynth’s ($\mathcal{P}$)
    performance and independently evaluate its ability to generate effective function
    signatures, acting as pseudo functions, detailed in Section [4.3](#S4.SS3 "4.3
    CodeSynth Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Win Rate of different Candidate and Reference model over G2, G3 set
    and Average over all sets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Candidate | Reference | G2-Instruction | G2-Category | G3-Instruction | Average
    |'
  prefs: []
  type: TYPE_TB
- en: '| T.LLaMA ReACT | ChatGPT ReACT | 50.8 | 41.8 | 55.0 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: '| T.LLaMA DFSDT | ChatGPT ReACT | 68.5 | 58.0 | 69.0 | 60.0 |'
  prefs: []
  type: TYPE_TB
- en: '| T.LLaMA DFSDT+Ret | ChatGPT ReACT | 68.5 | 60.8 | 73.0 | 63.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT DFSDT | ChatGPT ReACT | 72.0 | 64.8 | 69.0 | 64.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 ReACT | ChatGPT ReACT | 65.8 | 60.3 | 78.0 | 64.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 DFSDT | ChatGPT ReACT | 73.3 | 63.3 | 84.0 | 70.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | ChatGPT ReACT | 87.59 | 78.78 | 90.05 | 86.54 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | ChatGPT DFSDT | 81.63 | 73.07 | 85.26 | 78.71 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | T.LLaMA ReACT | 86.24 | 77.71 | 93.23 | 84.61 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT | 78.31 | 71.80 | 89.47 | 78.44 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT+Ret | 83.07 | 72.92 | 87.82 | 79.44 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | GPT4 ReACT | 78.61 | 73.75 | 93.68 | 80.27 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4 TOPGUN | GPT4 DFSDT | 73.92 | 71.35 | 79.25 | 78.59 |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Gray Box Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess the performance of TOPGUN and compare it with other gray box methodologies
    such as ReACT and DFSDT, we maintain the integrity of our pipeline while adapting
    the evaluation process to incorporate actual functions in place of pseudo functions
    within the output solution trajectory. This approach effectively leaves our black
    box pipeline intact while converting it into a gray box evaluation framework.
    The necessity of responses and Final answers for evaluation purposes has led us
    to adopt this hybrid strategy. In practical scenarios, this mirrors the process
    where a generalist planner delivers a strategy to the client, who then substitutes
    pseudo-function implementations with their real functions for execution. For this
    evaluation, we employ ToolBench, as detailed by Qin et al. ([2023](#bib.bib25)),
    and conduct our analysis across all problem categories provided in the dataset.
    Further elaboration on the precise evaluation methodology and the application
    of ToolBench is documented in [A.2](#A1.SS2 "A.2 ToolBench for Gray Box Evaluation
    ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results : Win Rate comparisons for ToolLLaMa-ReACT, ToolLLaMA-DFSDT, ChatGPT-DFSDT,
    GPT4-DFSDT, and GPT4-TOPGUN against ChatGPT-ReACT and GPT4-TOPGUN are summarized,
    with averages taken from 7 runs per model pair, detailed in Tables [1](#S3.T1
    "Table 1 ‣ 3.6 Parser ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting") and [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting"). TOPGUN significantly surpassed ReAct and DFSDT
    in all categories, achieving win rates of 80.27% versus GPT4-ReACT, 78.59% against
    GPT4-DFSDT, and 86.54% against ChatGPT-ReACT, showing improvements of 22.54% and
    16.14% respectively. These results highlight TOPGUN’s superior ability to create
    tool plans that align with preference evaluation criteria across various conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Black Box Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/588d2add61cc8c679a8ad46bfb99a8d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Average Token Consumption of individual methodologies in Black Box
    setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilizing the Data Generation pipeline from Qin et al. ([2023](#bib.bib25)),
    we constructed a black-box scenario dataset featuring 36 LLaMa-Hub LlamaIndex
    ([2023](#bib.bib16)) tools and unique functions from private libraries. Following
    Zan et al. ([2022](#bib.bib35)), we converted Pandas and Numpy into Monkey and
    BeatNum packages, renaming all internal functions and structures to test planner
    generalizability without LLM prior knowledge. This dataset, detailed at [A.1](#A1.SS1
    "A.1 Prompts ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting"), focuses on accuracy of the solution trajectory, with each query
    designed for a single correct path. After manual annotation, it comprises 100
    queries and 162 tools, with samples and TOPGUN outcomes at [A.3.2](#A1.SS3.SSS2
    "A.3.2 Queries Example ‣ A.3 PrivateEval Dataset ‣ Appendix A Appendix ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting") and [A.5.2](#A1.SS5.SSS2 "A.5.2
    PrivateEval ‣ A.5 TOPGUN Examples ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results : The black-box evaluation, featuring TOPGUN and a revised Reverse
    Chain, utilizes $\mathcal{P}$ function signatures for a comprehensive black-box
    methodology. TOPGUN surpasses Reverse Chain and undergoes comparison with GPT4-DFSDT
    and GPT4-ReACT within gray box evaluations, emphasizing output trajectories. Success
    rates, derived from exact trajectory matches with the ground truth and averaged
    over ten iterations, are documented in Table [4](#S4.T4 "Table 4 ‣ 4.3 CodeSynth
    Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").
    Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Black Box Evaluation ‣ 4 Experiments ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting") details the Average Token usage
    for each algorithm per query, underscoring TOPGUN’s effectiveness and efficiency
    in generating precise and resourceful tool plans in black-box scenarios, demonstrating
    its adaptability across diverse datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: A black-box evaluation using ToolBench is infeasible, as ToolEval’s metrics,
    such as pass rate and win rate, rely on intermediate tool responses and the final
    answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 CodeSynth Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess the quality of function signatures produced by CodeSynth, we adopt
    neuro-symbolic representations, as proposed by Parisotto et al. ([2017](#bib.bib23))
    and Nye et al. ([2021](#bib.bib20)). These representations aim to capture the
    abstract semantic essence of a given program, aligning well with our objectives.
    Our evaluation spans the Python subset of HumanEval-X Zheng et al. ([2023](#bib.bib38))
    and MBPP Austin et al. ([2021](#bib.bib3)) dataset. Inspired by the semantic probing
    model introduced by Ma et al. ([2023](#bib.bib19)), we construct semantic representations
    of both synthesized pseudo functions and ground truth code. Utilizing the tree-sitter
    Brunsfeld et al. ([2024](#bib.bib5)) package, we form the Abstract Syntax Tree,
    focusing our computation of the F1 score exclusively on the Function Definition
    block while excluding the body block. Hence, the final metric is precisely representative
    of our objective with CodeSynth. The appendix [A.4.1](#A1.SS4.SSS1 "A.4.1 HumanEval-X
    ‣ A.4 CodeSynth Examples ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting") can be referred to for function signature examples synthesized
    with the HumanEval-X dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: We evaluate CodeSynth across multiple reflection cycles, tracking
    the F1 score for each cycle to illustrate consistent enhancements in function
    signature quality, as depicted in Table [4](#S4.T4 "Table 4 ‣ 4.3 CodeSynth Evaluation
    ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"). CodeSynth
    significantly improved F1-scores on both HumanEval-X and MBPP datasets, achieving
    a perfect score of 1.0 by the fifth iteration from initial scores of 0.844 and
    0.912, respectively. These findings highlight CodeSynth’s ability to produce function
    signatures closely resembling the semantics of the target function.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Success Rate |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-TOPGUN | 70.58 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-DFSDT | 61.45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-ReAct | 45.45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT4-ReverseChain | 43.75 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of methodologies in Black Box Setting'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | F1 Score for max Reflexion Iteration |'
  prefs: []
  type: TYPE_TB
- en: '| @1 | @2 | @3 | @4 | @5 |'
  prefs: []
  type: TYPE_TB
- en: '| HumanEval-X | 0.844 | 0.894 | 0.965 | 0.983 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MBPP | 0.912 | 0.963 | 0.994 | 1.00 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: CodeSynth Evaluation for analyzing Reflexions improvement on Function
    Signature’s AST'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we address the challenge of tool planning in black-box settings,
    where direct access to API calls and their implementations is not feasible, raising
    concerns about cost efficiency and privacy in API interactions. We introduce SwissNYF,
    a comprehensive framework designed to equip Large Language Models (LLMs) with
    the ability to navigate these scenarios effectively. Central to SwissNYF is the
    ingenious function signature generation that allows the planner to rely on tool
    descriptions, circumventing the need for actual API executions. We further introduce
    TOPGUN, a code-driven planning approach leveraging LLMs’ code generation capabilities
    to offer a robust solution for black-box environments. Our extensive evaluation
    across various toolsets and settings demonstrates the superior performance of
    our methodology against traditional tool planning strategies, validating its effectiveness
    and reliability. Through SwissNYF and TOPGUN, we establish an exciting and emerging
    paradigm in tool planning, We envision SwissNYF as a central hub for black-box
    tool usage, encouraging future advancements in developing strategies for black-box
    scenarios, thus making a significant leap towards efficient, privacy-conscious
    tool planning in the realm of LLM-enhanced applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, et al. Palm 2 technical report. *arXiv preprint arXiv:2305.10403*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brunsfeld et al. (2024) Max Brunsfeld, Andrew Hlynskyi, Amaan Qureshi, Patrick
    Thomson, Josh Vera, Phil Turnbull, Timothy Clem, Douglas Creager, Andrew Helwer,
    dundargoc, Rob Rix, Daumantas Kavolis, Hendrik van Antwerpen, Michael Davis, Ika,
    Tuan-Anh Nguyen, Amin Yahyaabadi, Stafford Brunk, Matt Massicotte, and George
    Fraser. tree-sitter/tree-sitter: v0.21.0-pre-release-1, 2024. URL [https://doi.org/10.5281/zenodo.10638807](https://doi.org/10.5281/zenodo.10638807).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks. *arXiv preprint arXiv:2211.12588*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 24(240):1–113, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2024) Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective,
    hierarchical agents for large-scale api calls. *arXiv preprint arXiv:2402.04253*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischer et al. (2007) Gregor Fischer, J Lusiardi, and J Wolff Von Gudenberg.
    Abstract syntax trees-and their role in model driven software development. In
    *International Conference on Software Engineering Advances (ICSEA 2007)*, pp. 
    38–38\. IEEE, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models.
    In *International Conference on Machine Learning*, pp.  10764–10799\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie
    Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for
    mathematical problem solving. *arXiv preprint arXiv:2309.17452*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2023) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt:
    Augmenting frozen language models with massive tools via tool embeddings. *arXiv
    preprint arXiv:2305.11554*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang & Chang (2023) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki
    Okazaki (eds.), *Findings of the Association for Computational Linguistics: ACL
    2023*, pp.  1049–1065, Toronto, Canada, July 2023\. Association for Computational
    Linguistics. doi: 10.18653/v1/2023.findings-acl.67. URL [https://aclanthology.org/2023.findings-acl.67](https://aclanthology.org/2023.findings-acl.67).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman,
    Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code:
    Reasoning with a language model-augmented code emulator. *arXiv preprint arXiv:2312.04474*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaIndex (2023) LlamaIndex. Llamahub, 2023. URL [https://web.archive.org/web/20231229215448/https://llamahub.ai/](https://web.archive.org/web/20231229215448/https://llamahub.ai/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023a) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. *arXiv preprint arXiv:2304.09842*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023b) Yining Lu, Haoping Yu, and Daniel Khashabi. Gear: Augmenting
    language models with generalizable and efficient tool resolution. *arXiv preprint
    arXiv:2307.08775*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023) Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu,
    Jie Zhang, Wenhan Wang, and Yang Liu. Are code pre-trained models powerful to
    learn code syntax and semantics?, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nye et al. (2021) Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B.
    Tenenbaum, and Armando Solar-Lezama. Representing partial programs with blended
    abstract semantics. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=mCtadqIxOJ](https://openreview.net/forum?id=mCtadqIxOJ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paranjape et al. (2023) Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh
    Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step
    reasoning and tool-use for large language models. *arXiv preprint arXiv:2303.09014*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parisi et al. (2022) Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented
    language models. *arXiv preprint arXiv:2205.12255*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parisotto et al. (2017) Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh,
    Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis.
    In *International Conference on Learning Representations*, 2017. URL [https://openreview.net/forum?id=rJ0JwFcex](https://openreview.net/forum?id=rJ0JwFcex).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. Gorilla: Large language model connected with massive apis. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. *arXiv preprint arXiv:2307.16789*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2022) Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu,
    Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder,
    any task: Instruction-finetuned text embeddings. *arXiv preprint arXiv:2212.09741*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2023) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao
    Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with
    3000 simulated cases. *arXiv preprint arXiv:2306.05301*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing
    natural language and code for language agents. *arXiv preprint arXiv:2310.06830*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zan et al. (2022) Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and
    Jian-Guang Lou. When language model meets private library. *arXiv preprint arXiv:2210.17236*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023a) Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing
    Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive
    math reasoning, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing
    Zheng. Reverse chain: A generic-rule for llms to master multi-api planning. *arXiv
    preprint arXiv:2310.04474*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. Codegeex: A pre-trained
    model for code generation with multilingual benchmarking on humaneval-x. In *Proceedings
    of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pp. 
    5673–5684, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023a) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning
    in language models. *arXiv preprint arXiv:2310.04406*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023b) Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng
    Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging
    math word problems using gpt-4 code interpreter with code-based self-verification.
    *arXiv preprint arXiv:2308.07921*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2023a) Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain*: Efficient
    action space navigation in large language models with a* search, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2023b) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao
    Zhang. Toolqa: A dataset for llm question answering with external tools, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CodeSynth prompt for function signature generation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: TOPGUN prompt for code-based plan generation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Function Call Prompt for verification
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Self-Reflection Prompt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: CodeSynth prompt for function signature generation on PrivateEval
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: TOPGUN prompt for code-based plan generation on ToolBench
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Prompt for query generation for PrivateEval
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A.2 ToolBench for Gray Box Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ToolBench is a diverse benchmark spanning over 16k APIs across 49 categories
    from RapidAPI Hub. It consists of three sets of instructions for tool augmentation
    evaluation: (1) Single-tool instruction (I1), (2) Intra-category multi-tool instruction
    (I2), and (3) Intra-collection multi-tool instructions. Such a rich set of APIs
    and instructions makes it a perfect ground to test our pipeline. ToolBench proposes
    ToolEval containing the evaluation procedure for this set of instructions. ToolEval
    designs two evaluation metrics using ChatGPT: (1) Pass Rate, calculated by the
    proportion of instructions completed within a limited budget; (2) Win Rate, measured
    by asking a ChatGPT evaluator to select its preference for two solution paths.
    We focus on Win Rate for the evaluation metric to draw comparisons between TOPGUN
    and other gray box approaches such as DFSDT and ReAct. ToolEval uses a tree-based
    representation of the responses to generate solution paths, which are then compared
    to calculate the win rate.'
  prefs: []
  type: TYPE_NORMAL
- en: ToolEval response representation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We ensure that the code plan generated by TOPGUN precisely aligns with this
    representation to harness ToolEval for win rate calculation. In our black-box
    inference phase, we lack the final answer and tool responses. However, we retrieve
    these values during gray-box evaluation involving actual API calls and populate
    the representation accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Black Box Inference output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Gray Box Evaluation output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We input the solution path representations from TOPGUN and other approaches
    into ToolEval’s preference test to compute the win rate for each query. These
    win rates are then averaged across different sets of instructions to determine
    the average win rate.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 PrivateEval Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we list some examples of tools and queries that we created for PrivateEval.
  prefs: []
  type: TYPE_NORMAL
- en: A.3.1 Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Moneky and BeatNum
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Llama Hub
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: A.3.2 Queries Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A.4 CodeSynth Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Examples of function signatures and calls generated by CodeSynth while evaluating
    with HumanEval-X and PrivateEval datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A.4.1 HumanEval-X
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  intersperse(numbers:  List[int],  delimeter:  int)  ->  List[int]:"""Args:numbers  (List[int]):  A  list  of  integersdelimeter  (int):  An  integer  to  be  inserted  between  everytwo  consecutive  elements  of  the  inputlist\parReturns:List[int]:  A  new  list  with  the  delimeter  inserted  betweenevery  two  consecutive  elements  of  the  inputlist"""return  [0]  #  Dummy  return  object\par\par#  Function  Call:from  typing  import  List\parnumbers  =  [1,  2,  3]delimeter  =  4intersperse(numbers,  delimeter)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  pairs_sum_to_zero(l:  List[int])  ->  bool:"""Args:l  (List[int]):  A  list  of  integers  as  an  input.\parReturns:bool:  True  if  there  are  two  distinct  elements  inthe  list  that  sum  to  zero,  and  False  otherwise."""return  False\par\par#  Function  Call:l  =  [2,  4,  -5,  3,  5,  7]pairs_sum_to_zero(l)\par'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  vowels_count(s:  str)  ->  int:"""Args:s  (str):  A  string  representing  a  word.\parReturns:int:  The  number  of  vowels  in  the  string."""return  0\par\par#  Function  Call:s  =  "example"vowels_count(s)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  prod_signs(arr:  List[int])  ->  Union[int,  None]:"""Args:arr  (List[int]):  An  array  of  integers.\parReturns:Union[int,  None]:  The  sum  of  magnitudes  of  integersmultiplied  by  the  product  of  all  signsof  each  number  in  the  array,  representedby  1,  -1  or  0.  Returns  None  for  empty  arr.\par"""return  0  #  Dummy  return  object\par\par#  Function  Call:from  typing  import  List,  Union\pararr  =  [1,  2,  2,  -4]prod_signs(arr)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (e)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  will_it_fly(q:  List[int],  w:  int)  ->  bool:"""Args:q  (List[int]):  A  list  of  integers  representing  theobject’s  weight  distribution.w  (int):  The  maximum  possible  weight  for  the  object  to  fly.\parReturns:bool:  True  if  the  object  will  fly,  False  otherwise."""return  True  #  Dummy  return\par\par#  Function  Call:from  typing  import  List\parq  =  [3,  2,  3]w  =  9will_it_fly(q,  w)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.4.2 PrivateEval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  stats_analysis(knowledgeframe):"""Performs  various  statistical  analyses  on  a  KnowledgeFrameand  returns  a  new  KnowledgeFrame  containing  the  results.\parArgs:knowledgeframe  (KnowledgeFrame):  The  KnowledgeFrameon  which  statistical  analysis  will  be  performed.\parReturns:KnowledgeFrame:  A  KnowledgeFrame  containing  thestatistical  analysis  results."""return  KnowledgeFrame()  #  Dummy  return  object\par\par#  Function  Call:from  monkey  import  KnowledgeFrame\par#  Dummy  data  for  the  KnowledgeFramedata  =  {’column1’:  [1,  2,  3],’column2’:  [4,  5,  6],’column3’:  [7,  8,  9]}\par#  Create  a  dummy  KnowledgeFrameknowledgeframe  =  KnowledgeFrame(data)\par#  Call  the  stats_analysis  function  with  the  dummy  KnowledgeFrameresult  =  stats_analysis(knowledgeframe)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  knowledge_summary(knowledgeframe,  columns,  stats_analysis):"""Summarizes  a  KnowledgeFrame  based  on  specified  columns  and  statistical  analysis  results.\parArgs:knowledgeframe  (KnowledgeFrame):  The  KnowledgeFrame  tobe  summarized.\parcolumns  (List[str]):  The  list  of  column  names  to  includein  the  summary.\parstats_analysis  (Dict[str,  Any]):  The  dictionary  containingstatistical  analysis  results  for  the  specified  columns.\parReturns:dict:  A  summary  dictionary  containing  information  aboutthe  specified  columns  and  their  statistical  analysis."""return  {"dummy_key":  "dummy_value"}\par\par#  Function  Call:\par#  Dummy  function  call  for  knowledge_summaryknowledgeframe  =  {"dummy_key":  "dummy_value"}columns  =  ["column1",  "column2"]stats_analysis  =  {"column1":  {"mean":  5,  "median":  4},  "column2":  {"mean":  10,  "median":  8}}\parknowledge_summary(knowledgeframe,  columns,  stats_analysis)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  to_grayscale(image_array):"""Grayscale  function  takes  an  image  array  as  input  andconverts  it  into  grayscale.\parArgs:image_array  (beatnum.bdnumset):  Input  image  array  tobe  converted  to  grayscale.\parReturns:beatnum.bdnumset:  Grayscale  image  array."""dummy_shape  =  (1,  1)  #  Dummy  shape  for  the  bdnumsetreturn  beatnum.bdnumset(dummy_shape)\par\par#  Function  Call:from  beatnum  import  bdnumset\par#  Dummy  image_arraydummy_shape  =  (1,  1)  #  Dummy  shape  for  the  bdnumsetimage_array  =  bdnumset(dummy_shape)\par\par#  Function  callto_grayscale(image_array)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  flip(image_array,  axis=1):"""Flip  function  takes  an  image  array  as  input  and  flipsit  along  the  specified  axis.\parArgs:image_array  (beatnum.bdnumset):  Input  image  array  to  beflipped.\paraxis  (int,  optional):  Axis  along  which  to  flip  the  imagearray.  Default  is  1  (horizontal  flip).\parReturns:beatnum.bdnumset:  Flipped  image  array."""dummy_shape  =  image_array.shapereturn  beatnum.bdnumset(shape=dummy_shape)\par#  Function  Call:import  beatnum  as  bn\parimage_array  =  bn.bdnumset(shape=(2,  2),  dtype=float,  order=’F’)axis  =  1\parflip(image_array,  axis)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (e)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def  translate(text:  str,  language:  str)  ->  str:"""Translates  text  from  one  language  to  another.  The  sourcelanguage  will  be  automatically  detected.You  need  to  specifythe  target  language  using  a  two  character  language  code.\parArgs:text  (str):  Text  to  be  translated\parlanguage  (str):  Target  translation  language.  One  ofaf,  sq,  am,  ar,  hy,  as,  az,  bn,  ba,  eu,  bs,  bg,  ca,hr,  cs,  da,  dv,  nl,  en,  et,  fo,  fj,  fi,  fr,  gl,  ka,de,  el,  gu,  ht,  he,  hi,  hu,  is,  id,  iu,  ga,  it,  ja,kn,  kk,  km,  ko,  ku,  ky,  lo,  lv,  lt,  mk,  mg,  ms,  ml,mt,  mi,  mr,  my,  ne,  nb,  or,  ps,  fa,  pl,  pt,  pa,  to,ru,  sm,  sk,  sl,  so,  es,  sw,  sv,  ty,  ta,  tt,  te,  the,bo,  ti,  to,  tr,  tk,  uk,  ur,  ug,  uz,  vi,  cy,  zu\parReturns:str:  Translated  text"""return  "dummy_translated_text"\par\par#  Function  Call:text  =  "Hello,  how  are  you?"language  =  "fr"translate(text,  language)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.5 TOPGUN Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Examples of code-based plans generated by our proposed planning approach TOPGUN,
    as evaluated on ToolBench and PrivateEval datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A.5.1 ToolBench
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '#  Import  the  required  librariesimport  requests\par#  Use  the  api_tracking  function  to  track  the  package  with#  the  Pack  &  Send  reference  numberreference_number  =  ’ReferenceNumberHere’package_tracking_info  =  api_tracking(reference_number)\par#  Use  the  latest  function  to  get  the  latest  status  of  the#  package  with  colis  IDcolis_id  =  ’CA107308006SI’latest_status  =  latest(colis_id)\par#  The  package_tracking_info  and  latest_status  variables#  contain  the  required  information'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'import  requests\par#  Get  the  latest  arrivals  from  different  platformsnew_arrivals_data  =  new_arrivals(region=’US’)\par#  Initialize  an  empty  list  to  store  the  movie  detailsmovie_details  =  []\par#  Iterate  through  the  new  arrivals  datafor  movie  in  new_arrivals_data.get(’result’,  []):#  Get  the  IMDb  ID  of  the  movieimdb_id  =  movie.get(’imdbid’,  ’’)\par#  Get  the  basic  information  of  the  movie  using  the  IMDb  IDtitle_data  =  title_details(imdbid=imdb_id)\par#  Extract  the  required  information  from  the  title  datamovie_title  =  title_data.get(’title’,  ’’)streaming_platforms  =  title_data.get(’platforms’,  {})genres  =  title_data.get(’genre’,  ’’)\par#  Append  the  movie  details  to  the  movie_details  listmovie_details.append({’title’:  movie_title,’streaming_platforms’:  streaming_platforms,’genres’:  genres})\par#  The  movie_details  list  now  contains  the  new  arrivals#  along  with  their  streaming  platforms  and  genres'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'import  requests\par#  Search  for  videos  related  to  ’action’  on  Vimeoaction_videos  =  searchvideos(format=’json’,  query=’action’,  sort=’relevant’)\par#  Fetch  the  related  people  in  the  ’movies’  categoryrelated_people  =  getrelatedpeople(category=’movies’,  format=’json’)\par#  Provide  a  streaming  link  for  a  YouTube  video  with  the  ID#  ’UxxajLWwzqY’youtube_streaming_link  =  download_stream(is_id=’UxxajLWwzqY’)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'import  requests\par#  Use  the  search  function  to  find  top-rated  fitness  trackers#  on  Amazonsearch_results  =  search(type=’search’,  search_term=’fitness  tracker’,  amazon_domain=’amazon.com’,  sort_by=’average_review’,  exclude_sponsored=True)\par#  Extract  the  top  5  fitness  trackers  from  the  search  resultstop_5_fitness_trackers  =  search_results.get(’results’,  [])[:5]\par#  Get  the  ASINs  of  the  top  5  fitness  trackerstop_5_asins  =  [tracker.get(’asin’,  ’’)  for  tracker  in  top_5_fitness_trackers]\par#  Retrieve  the  product  details  for  each  of  the  top  5  fitness#  trackerstop_5_product_details  =  [product(type=’product’,  asin=asin,  amazon_domain=’amazon.com’)  for  asin  in  top_5_asins]\par#  Extract  the  features  and  prices  of  the  top  5  fitness  trackerstop_5_features_and_prices  =  []for  product_detail  in  top_5_product_details:try:features  =  product_detail.get(’features’,  [])except:features  =  []price  =  product_detail.get(’price’,  {}).get(’value’,  ’N/A’)top_5_features_and_prices.append({’features’:  features,  ’price’:  price})\par#  The  top_5_features_and_prices  variable  contains  the  features#  and  prices  of  the  top  5  fitness  trackers  on  Amazon'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (e)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'import  requests\par#  Fetch  cryptocurrency  datacrypto_data  =  prices_and_up_and_down()\par#  Fetch  subregions  of  North  Americasubregions_data  =  get_sub_regions(’North  America’)\par#  Accessing  specific  cryptocurrency  databitcoin_data  =  crypto_data.get(’Bitcoin’,  {})ethereum_data  =  crypto_data.get(’Ethereum’,  {})stellar_data  =  crypto_data.get(’Stellar’,  {})\par#  Accessing  subregions  of  North  Americatry:north_america_subregions  =  subregions_data.get(’subregions’,  [])except:north_america_subregions  =  []\par#  You  can  now  analyze  the  cryptocurrency  data  and  plan  your#  trip  to  North  America  using  the  subregions  information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (f)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'import  requests\par#  Search  for  landscape  drawing  tutorial  videossearch_result  =  search(’landscape  drawing  tutorial’,  type=’video’,  safesearch=True)\par#  Find  the  most  viewed  videomost_viewed_video  =  Nonemax_views  =  0for  video  in  search_result.get(’items’,  []):views  =  int(video.get(’statistics’,  {}).get(’viewCount’,  ’0’))if  views  >  max_views:max_views  =  viewsmost_viewed_video  =  video\par#  Get  the  video  detailsvideo_id  =  most_viewed_video.get(’id’,  ’video_id_example’)video_details  =  video(video_id)\par#  Get  the  channel  detailschannel_id  =  most_viewed_video.get(’snippet’,  {}).get(’channelId’,  ’channel_id_example’)channel_details  =  video(channel_id)\par#  The  video_details  and  channel_details  variables  contain  the#  required  information'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.5.2 PrivateEval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '#  Load  the  multilingual  dataset  (assuming  it’s  a  CSV  file)csv_file  =  ’path/to/your/multilingual_dataset.csv’knowledgeframe  =  load_csv(csv_file)\par#  Extract  the  French  column  (assuming  it’s  named  ’french_column’)french_column  =  knowledgeframe(’french_column’)\par#  Translate  the  French  column  to  Englishtranslated_column  =  []for  text  in  french_column:translated_text  =  translate(text,  language=’en’)translated_column.append(translated_text)\par#  Add  the  translated  column  to  the  KnowledgeFrameknowledgeframe[’english_column’]  =  translated_column\par#  Perform  statistical  analysis  on  the  translated  columnanalysis_kf  =  stats_analysis(knowledgeframe)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '#  Load  the  CSV  file  containing  the  survey  datasurvey_data  =  load_csv(’baby_food_preferences.csv’)\par#  Perform  statistical  analysis  on  the  survey  dataanalysis_results  =  stats_analysis(survey_data)\par#  Generate  a  histogram  to  visualize  the  preferences  across  different  age  groupsbuild_hist(analysis_results)\par#  Summarize  the  most  preferred  food  items  in  a  reportcolumns_to_include  =  [’age_group’,  ’food_item’,  ’preference_score’]summary_report  =  knowledge_summary(survey_data,  columns_to_include,  analysis_results)\par#  The  summary_report  variable  now  contains  the  report  summarizing  the  most  preferred  food  items'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'from  monkey  import  KnowledgeFramefrom  typing  import  List,  Dict,  Union\par#  Search  for  "Chinchilla  LLM"  paper  on  arXivquery  =  "Chinchilla  LLM"papers  =  arxiv_query(query)\par#  Assuming  first  result  is  relevant  load  the  PDF  datapdf_data  =  load_pdf_data(papers[0])\par#  Search  the  image  of  the  table  in  the  "Chinchilla  LLM"  paperimage_url  =  bing_image_search(’Chinchilla  LLM  paper  table’)\par#  Process  the  imageprocessed_image  =  process_image(image_url,  features=[’objects’])\par#  Convert  the  processed  image  to  a  KnowledgeFrameknowledge_frame  =  read_txt(processed_image.get(’objects’,  ’dummy_objects_text’))\par#  Perform  statistical  analysis  on  the  KnowledgeFrameanalysis_kf  =  stats_analysis(knowledge_frame)\par#  Build  a  histogram  based  on  the  analysisbuild_hist(analysis_kf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '#  Import  necessary  librariesfrom  typing  import  List,  Dict,  Unionimport  beatnumfrom  PIL  import  Image\par#  Retrieve  images  of  dogsimage_urls  =  bing_image_search(query=’dogs’)\par#  Load  the  first  imageimage  =  process_image(image_urls[0])image_array  =  beatnum.bdnumset(image)\par#  Perform  data  augmentation  using  simple  image  processing  techniquesrotated_image_array  =  rotate(image_array,  direction=’clockwise’)flipped_image_array  =  flip(image_array,  axis=1)cropped_image_array  =  crop(image_array,  scale_factor=0.5)grayscale_image_array  =  to_grayscale(image_array)\par#  creating  image  object  of  above  arrayrotated_image_data  =  Image.fromarray(rotated_image_array)flipped_image_data  =  Image.fromarray(flipped_image_array)cropped_image_data  =  Image.fromarray(cropped_image_array)grayscale_image_data  =  Image.fromarray(grayscale_image_array)\par#  Save  the  augmented  imagesrotated_image_data.save(’rotated_dog_image.png’)flipped_image_data.save(’flipped_dog_image.png’)cropped_image_data.save(’cropped_dog_image.png’)grayscale_image_data.save(’grayscale_dog_image.png’)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
