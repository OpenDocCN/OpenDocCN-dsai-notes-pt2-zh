- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of
    Small LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10251](https://ar5iv.labs.arxiv.org/html/2406.10251)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mert Yazan [m.yazan@hva.nl](mailto:m.yazan@hva.nl) [0009-0004-3866-597X](https://orcid.org/0009-0004-3866-597X
    "ORCID identifier") Amsterdam University of Applied SciencesAmsterdamNetherlands
    ,  Suzan Verberne [s.verberne@liacs.leidenuniv.nl](mailto:s.verberne@liacs.leidenuniv.nl)
    [0000-0002-9609-9505](https://orcid.org/0000-0002-9609-9505 "ORCID identifier")
    University of LeidenLeidenNetherlands  and  Frederik Situmeang [f.b.i.situmeang@uva.nl](mailto:f.b.i.situmeang@uva.nl)
    [0000-0002-2156-2083](https://orcid.org/0000-0002-2156-2083 "ORCID identifier")
    Amsterdam University of Applied SciencesAmsterdamNetherlands(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Post-training quantization reduces the computational demand of Large Language
    Models (LLMs) but can weaken some of their capabilities. Since LLM abilities emerge
    with scale, smaller LLMs are more sensitive to quantization. In this paper, we
    explore how quantization affects smaller LLMs’ ability to perform retrieval-augmented
    generation (RAG), specifically in longer contexts. We chose personalization for
    evaluation because it is a challenging domain to perform using RAG as it requires
    long-context reasoning over multiple documents. We compare the original FP16 and
    the quantized INT4 performance of multiple 7B and 8B LLMs on two tasks while progressively
    increasing the number of retrieved documents to test how quantized models fare
    against longer contexts. To better understand the effect of retrieval, we evaluate
    three retrieval models in our experiments. Our findings reveal that if a 7B LLM
    performs the task well, quantization does not impair its performance and long-context
    reasoning capabilities. We conclude that it is possible to utilize RAG with quantized
    smaller LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieval Augmented Generation, Quantization, Efficiency, Large Language Models,
    Personalization^†^†copyright: none^†^†journalyear: 2024^†^†doi: XXXXXXX.XXXXXXX^†^†ccs:
    Information systems Personalization^†^†ccs: Computing methodologies Natural language
    generation'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Model (LLM) outputs can be enhanced by fetching relevant documents
    via a retriever and adding them as context for the prompt. The LLM can generate
    an output grounded with relevant information with the added context. This process
    is called Retrieval Augmented Generation (RAG). RAG has many benefits such as
    improving effectiveness in downstream tasks (Huang et al., [2023](#bib.bib5);
    Ma et al., [2023](#bib.bib13); Shi et al., [2023](#bib.bib21); Xu et al., [2023](#bib.bib25)),
    reducing hallucinations (Proser, [[n. d.]](#bib.bib17)), increasing factuality
    (Nakano et al., [2022](#bib.bib15)), by-passing knowledge cut-offs, and presenting
    proprietary data that is not available to the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance of RAG depends on the number, quality, and relevance of the
    retrieved documents (Gao et al., [2024](#bib.bib4)). To perform RAG, many tasks
    demand a lot of passages extracted from multiple, unstructured documents: For
    question-answering tasks, the answer might be scattered around many documents
    because of ambiguity or the time-series nature of the question (eg. price change
    of a stock). For more open-ended tasks like personalization, many documents from
    different sources might be needed to capture the characteristics of the individual.
    Therefore to handle RAG in these tasks, an LLM needs to look at multiple sources,
    identify the relevant parts, and compose the most plausible answer (Gao et al.,
    [2024](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs do not pay the same attention to their whole context windows, meaning the
    placement of documents in the prompt directly affects the final output (Liu et al.,
    [2023](#bib.bib12)). On top of that, some of the retrieved documents may be unrelated
    to the task, or they may contain contradictory information compared to the parametric
    knowledge of the LLM (Xu et al., [2024](#bib.bib26)). An LLM has to overcome these
    challenges to leverage RAG to its advantage. Xu et al. ([2023](#bib.bib25)) have
    shown that an open-source 70B LLM (Touvron et al., [2023](#bib.bib22)) equipped
    with RAG can beat proprietary models, meaning it is not necessary to use an LLM
    in the caliber of GPT-4 (OpenAI, [2023](#bib.bib16)) to implement RAG. Still,
    for many use cases, it might not be feasible to deploy a 70B LLM as it is computationally
    demanding. To decrease the computational demand of LLMs, post-training quantization
    can be used. Quantization drastically reduces the required amount of RAM to load
    a model and can increase the inference speed by more than 3 times (Dettmers et al.,
    [2023](#bib.bib2); Frantar et al., [2023](#bib.bib3)). Despite the benefits, quantization
    affects LLMs differently depending on their size (Li et al., [2024](#bib.bib9)).
    For capabilities that are important to RAG, such as long-context reasoning, smaller
    LLMs (¡13B) are found to be more sensitive to quantization (Li et al., [2024](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b848b618b2a9c0d984bdcbb14611d572.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Prompts used for both datasets. The ones on the top represent $k=0$
    settings (RAG). The green text is the model output. Line endings are not shown
    for space reasons.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: Prompts used during experiments. LaMP-3U prompts are on the left and LaMP-5U
    prompts are on the right
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we investigate the effectiveness of quantization on RAG-enhanced
    7B and 8B LLMs. We evaluate the full (FP16) and quantized (INT4) versions of multiple
    LLMs on two personalization tasks taken from the LaMP (Salemi et al., [2023](#bib.bib19))
    benchmark. To better study how quantized LLMs perform in longer contexts, we compared
    the performance gap between FP16 and INT4 models with an increasing number of
    retrieved documents. We chose personalization because it is a challenging task
    to perform with RAG as it demands long-context reasoning over many documents.
    Contrary to question-answering where the LLM has to find the correct answer from
    a couple of documents, personalization requires the LLM to carefully study a person’s
    style from all the provided documents. Our findings show that the effect of quantization
    depends on the model and the task: we find almost no drop in performance for OpenChat
    while LLaMA2 seems to be more sensitive. Our experiments show that quantized smaller
    LLMs can be good candidates for RAG pipelines, especially if efficiency is essential.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting with LLaMA2-7B (Chat-hf) (Touvron et al., [2023](#bib.bib22)) to have
    a baseline, we experiment with the following LLMs: LLaMA3-8B (Meta, [2024](#bib.bib14)),
    Zephyr (Beta) (Tunstall et al., [2023](#bib.bib23)), OpenChat (3.5) (Wang et al.,
    [2023](#bib.bib24)), and Starling (LM-alpha) (Zhu et al., [2023](#bib.bib28)).
    These models were chosen because they were the highest-ranked 7B and 8B LLMs in
    the Chatbot Arena Leaderboard (Zheng et al., [2023](#bib.bib27)) according to
    the Elo ratings at the time of writing. Since all models except LLaMA are finetuned
    variants of Mistral-7B (Jiang et al., [2023](#bib.bib7)), we add Mistral-7B (Instruct-0.1)¹¹1Although
    there is an updated v0.2 version of Mistral-7B, we used v0.1 to match the other
    LLMs that are finetuned on it to our experiments too. We use Activation-aware
    Weight Quantization (AWQ) as it outperforms other methods (Lin et al., [2023](#bib.bib11)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Tasks and Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the LaMP benchmark that offers 7 personalization datasets with either
    a classification or a generation task (Salemi et al., [2023](#bib.bib19)). To
    represent both types of tasks, we chose one dataset from each: LaMP-3 (“Personalized
    Product Rating”) and LaMP-5 (“Personalized Scholarly Title Generation”). LaMP-3
    is composed of product reviews and their corresponding scores. For each user,
    one of the review–score pairs is chosen as the target and other pairs become the
    user profile. The LLM’s task, in this case, is to predict the score given a review
    using the other review–score pairs of the same user. LaMP-5 aims to generate a
    title for an academic paper based on the abstract. In this case, the user profile
    consists of abstract–title pairs that demonstrate the writing style of the user
    (scholar). The task of the LLM is to generate a title for the given abstract by
    incorporating the writing style of the scholar. Those datasets were chosen because
    compared to the other ones, on average, they had more samples in their user profiles,
    and the samples were longer. Therefore, they represented a better opportunity
    to evaluate RAG effectiveness as the retrieval part would be trickier.'
  prefs: []
  type: TYPE_NORMAL
- en: We work with the user-based splits (LaMP-3U, LaMP-5U) where the user appears
    only in one of the data splits (Salemi et al., [2023](#bib.bib19)). The labels
    for the test sets are not publicly available (results can be obtained by submitting
    the predictions to the leaderboard) and since we did not fine-tune our models,
    we chose to use the validation sets for evaluation. For both datasets, we noticed
    that some samples do not fit in the context windows. After analyzing the overall
    length of the samples, we concluded that those cases only represent a tiny minority
    and removed data points that are not in the 0.995th percentile. For LaMP-5U, we
    also removed abstracts that consisted only of the text “no abstract available”.
    There are 2500 samples in the validation sets, and we have 2487 samples left after
    the preprocessing steps for both datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation. We used mean absolute error (MAE) for LaMP-3 and Rouge-L (Lin,
    [2004](#bib.bib10)) for LaMP-5, following the LaMP paper (Salemi et al., [2023](#bib.bib19)).
    Their experiments also include root mean square error (RMSE) and Rouge-1 scores,
    but we found that the correlation between MAE and RMSE is 0.94, and between Rouge-1
    and Rouge-L is 0.99\. Therefore, we do not include those metrics in our results.
    The prompts we use are shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of
    Small LLMs"). Even though the LLMs are instructed to output only the score or
    the title, we notice that some are prone to give lengthy answers such as “Sure,
    here is the title for the given abstract, Title: (generated title)”. We apply
    a post-processing step on the LLM outputs to extract only the score or the title
    before evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct the experiments with the following number of retrieved documents:
    $k\in\{0,1,3,5,max\_4K,max\_8K\}$ in LaMP-5U, depending on the average length
    of documents in the user profile. As retrievers, we evaluate BM25 (Robertson et al.,
    [1994](#bib.bib18)) (BM25 Okapi) ²²2[https://pypi.org/project/rank-bm25/](https://pypi.org/project/rank-bm25/),
    Contriever (Izacard et al., [2022](#bib.bib6)) (finetuned on MS-Marco), and DPR
    (Karpukhin et al., [2020](#bib.bib8)) (finetuned on Natural Questions)³³3[https://huggingface.co/facebook/dpr-question_encoder-single-nq-base](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base).
    Since we focus on efficiency by reducing the computational load, the retrievers
    are not finetuned on the datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA2 | OpenChat | Starling | Zephyr | Mistral | LLaMA3 |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | Metric | k | FP16 | INT4 | FP16 | INT4 | FP16 | INT4 | FP16 | INT4
    | FP16 | INT4 | FP16 | INT4 |'
  prefs: []
  type: TYPE_TB
- en: '| LaMP-3U | MAE $\downarrow$ | 0 | 0.684 | +2.9% | 0.440 | -7.8% | 1.603 |
    +45% | 0.435 | -14.7% | 0.569 | -2.5% | 0.481 | -5.9% |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.453 | -1.1% | 0.312 | +5.5% | 0.800 | +7.1% | 0.300 | +1.9% | 0.461
    | -9.3% | 0.364 | -10.8% |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.637 | -7.6% | 0.256 | +2.8% | 0.718 | -30.0% | 0.273 | +2.6% | 0.404
    | -8.0% | 0.320 | -9.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.724 | -23.3% | 0.238 | +1.8% | 0.797 | -32.0% | 0.266 | +0.8% | 0.380
    | -8.1% | 0.305 | -13.0% |'
  prefs: []
  type: TYPE_TB
- en: '| max_4K | 0.508 | -80.2% | 0.224 | +1.8% | 0.985 | -57.1% | 0.237 | -4.4%
    | 0.346 | -14.7% | 0.285 | -23.1% |'
  prefs: []
  type: TYPE_TB
- en: '| max_8K | - | - | 0.257 | -3.9% | 1.352 | -1.1% | 0.392 | -6.3% | 0.368 |
    -16.1% | 0.288 | -23.4% |'
  prefs: []
  type: TYPE_TB
- en: '| LaMP-5U | Rouge-L $\uparrow$ | 0 | 0.338 | -0.6% | 0.361 | -0.5% | 0.359
    | -2.3% | 0.335 | -0.4% | 0.361 | +1.2% | 0.384 | -2.5% |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.380 | -9.7% | 0.404 | -1.0% | 0.397 | -1.0% | 0.360 | +0.9% | 0.400
    | -0.9% | 0.402 | -5.6% |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.385 | -11.6% | 0.415 | 0.0% | 0.412 | -0.2% | 0.360 | -0.8% | 0.410
    | -0.5% | 0.404 | -5.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.374 | -10.3% | 0.422 | -0.7% | 0.419 | -1.4% | 0.365 | -2.1% | 0.415
    | -0.7% | 0.397 | -4.5% |'
  prefs: []
  type: TYPE_TB
- en: '| max_4K | 0.337 | -16.8% | 0.419 | -1.1% | 0.402 | -0.5% | 0.357 | -7.7% |
    0.410 | -1.1% | 0.376 | -2.6% |'
  prefs: []
  type: TYPE_TB
- en: '| max_8K | - | - | 0.395 | -1.0% | 0.379 | -1.9% | 0.326 | -18.7% | 0.387 |
    -0.8% | 0.384 | -7.2% |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. The absolute percentage change between FP16 and INT4 scores, using
    Contriever. More than a 5% drop in performance is highlighted in red. For MAE,
    the lower is better while the inverse is true for Rouge-L.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: Table showing the results of the experiments for Contriever. We list the difference
    in scores between the FP16 and INT4 variants of the models by how much the performance
    changes in percentages.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e6f52ab785e96d310d80d29636b0c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) MAE results for LaMP-3U, the lower the better
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95066074ec1f6856f62c5a8858778606.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Rouge-L results for LaMP-5U, the higher the better
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2\. Results for both datasets. The upper and lower borders of each colored
    area represent the quantized and not-quantized performances of the models, and
    the corresponding lines are the mean of both.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: Results for both datasets using 3 different retrievers. The left plot shows
    the MAE for the third dataset and the right plot shows the Rouge-L score. The
    x-axis is composed of the values of the number of retrieved documents. It shows
    the same results as Table 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs. The results are shown in Table [1](#S2.T1 "Table 1 ‣ 2.3\. Retrieval
    ‣ 2\. Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation:
    An Analysis of Small LLMs"). We see the dominance of OpenChat in both datasets.
    Zephyr performs very close to OpenChat in LaMP-3U but falls far behind in LaMP-5U.
    The same can be said for Starling but reversed. Mistral-7B performs stable in
    both datasets albeit being slightly behind OpenChat. Overall, LLaMA2 performs
    the worst as it is below average in both datasets. Despite being the dominant
    small LLM currently, LLaMA3 is not the best for both tasks, despite performing
    reasonably well in LaMP-3U. Interestingly, LLaMA3 has the best zero-shot score
    in LaMP-5U but it struggles to improve itself with retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization. How much an LLM gets affected by quantization seems to be related
    to how well it performs the task. OpenChat suffers almost no performance degradation
    from quantization. On the contrary, LLaMA2 seems very sensitive, especially when
    the number of retrieved documents is increased. Starling suffers no significant
    consequence from quantization in LaMP-5U where it performs well, but it does suffer
    in LaMP-3U. There also seems to be a disparity between the tasks as quantized
    LLMs perform much worse in LaMP-3U than in LaMP-5U.
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of retrieved documents. Figure [2b](#S3.F2.sf2 "In Figure 2 ‣ 3\. Results
    ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of
    Small LLMs") shows that LLM performance is saturated with a couple of documents,
    and the improvement obtained from more is marginal. In LaMP-5U, adding more than
    5 documents starts to hurt the performance: Figure [2b](#S3.F2.sf2 "In Figure
    2 ‣ 3\. Results ‣ The Impact of Quantization on Retrieval-Augmented Generation:
    An Analysis of Small LLMs") shows an inverse-U-shaped distribution for all LLMs
    except LLaMA3\. For some models, performance even drops below the zero-shot setting
    when all the available context window is filled with retrieved documents. The
    LaMP-3U results (Figure [2a](#S3.F2.sf1 "In Figure 2 ‣ 3\. Results ‣ The Impact
    of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"))
    continue to improve after adding more than 5 documents as can be observed from
    $max\_4K$.'
  prefs: []
  type: TYPE_NORMAL
- en: We analyze whether a longer context window hurts the quantized variants more
    and find that there seems to be a peculiar relationship. INT4 LLaMA2 suffers from
    longer contexts, while INT4 OpenChat performs well and acts almost the same as
    its FP16 counterpart. INT4 Mistral and LLaMA3 act very similar to their FP16 counterparts
    in LaMP-5U but in LaMP-3U, they get progressively worse with more documents. Overall,
    quantization can increase the risk of worsened long-context capabilities but there
    is not a direct relationship as it is highly task and context-dependent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrievers. Figure [2b](#S3.F2.sf2 "In Figure 2 ‣ 3\. Results ‣ The Impact
    of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs")
    shows that the three retrievers gave almost identical results, albeit BM25 being
    marginally behind the others. Also in LaMP-5U, the gap between the FP16 and INT4
    LLaMA2 varies slightly. Other than that, the retriever model does not have a noticeable
    impact on the personalization tasks we experimented with. The patterns we found
    regarding LLMs, quantization, and the number of retrieved documents are the same
    for all the retrievers.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LaMP-3U (MAE) $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT (Salemi et al., [2023](#bib.bib19)) | 0.658 | 0.336 | ? |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-XXL (Salemi et al., [2023](#bib.bib19)) | 0.282 | 0.424 | 43 GB |'
  prefs: []
  type: TYPE_TB
- en: '| OpenChat (FP16) | 0.238 | 0.423* | 28 GB |'
  prefs: []
  type: TYPE_TB
- en: '| OpenChat (INT4) | 0.234 | 0.419* | 4.2 GB |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Our results compared with LaMP. Results indicated with * are not significantly
    lower than the reported best result (FlanT5-XXL). A quantized 7B LLM can perform
    on par with a larger model while being much more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: Table showing the Rouge-L scores obtained from the experiments
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark comparison. Finally, we compared our findings with the RAG results
    from LaMP (Salemi et al., [2023](#bib.bib19)). Table [2](#S3.T2 "Table 2 ‣ 3\.
    Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis
    of Small LLMs") shows that OpenChat (Contriever, $k=5$ value of 0.29 shows a non-significant
    difference between the results. Moreover, the results from the LaMP paper are
    with finetuned retrievers while our results are with non-finetuned retrievers.
    This indicates that a quantized-7B LLM can compete and even outperform a bigger
    model on personalization with RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3\. Results ‣ The Impact of Quantization on Retrieval-Augmented
    Generation: An Analysis of Small LLMs") shows how much GPU VRAM is needed to deploy
    each model. With this comparison, the benefit of quantization becomes more pronounced:
    multiple high-level consumer GPUs or an A100 is necessary for running even a 7B
    LLM while a mid-level consumer GPU (eg. RTX 3060) would be enough to run it with
    quantization. According to the scores taken from LaMP, both FlanT5-XXL and OpenChat
    decisively beat ChatGPT, but the authors warn that the prompts used for ChatGPT
    may not be ideal and may contribute to a sub-optimal performance. Therefore, our
    results should not be used to make a comparison with ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our results show that some LLMs (in particular OpenChat) can be successful in
    RAG pipelines, even after quantization, but the performance is LLM- and task-dependent.
    The method of quantization affects LLMs differently (Li et al., [2024](#bib.bib9)).
    Thus, the relationship between quantization and RAG performance is not straightforward
    and can be studied more extensively. Still, our results indicate that when a small
    LLM performs the task well, its AWQ-quantized counterpart performs on par.
  prefs: []
  type: TYPE_NORMAL
- en: The differing performance of some LLMs between the datasets may be partly due
    to prompting. LLMs are sensitive to prompts, and a prompt that works for one LLM
    may not work for another one (Sclar et al., [2023](#bib.bib20)). The most peculiar
    result is the lackluster performance of LLaMA3 in LaMP-5U. LLaMA3 is a recently
    released model trained with an extensive pretraining corpus (Meta, [2024](#bib.bib14)).
    It has a higher chance of seeing the abstracts presented in the LaMP-5U in its
    pretraining data. This may explain its superior zero-shot performance. LLMs suffer
    from a knowledge conflict between their parametric information and the contextual
    information presented through retrieval (Xu et al., [2024](#bib.bib26)). If LLaMA3
    had already memorized some of the titles of the abstracts in LaMP-5U, it might
    result in a knowledge conflict when similar abstract-title pairs of the same author
    are presented. This may explain the reduced improvement in its performance with
    retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have been shown to struggle with too many retrieved documents (Liu et al.,
    [2023](#bib.bib12)), and our findings are in accordance. Our results indicate
    that more than <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="></math> settings,
    the less relevant documents are put on the top. This situation might hurt the
    LLM performance as it would focus on the most and the least related information
    in this case. That being said, state-of-the-art LLMs with more than 7B parameters
    also suffer from the same phenomenon even when not quantized (Liu et al., [2023](#bib.bib12)).
    Although quantization increases the risk of worsened long-context performance,
    we cannot conclude that it is the sole perpetrator, as this is an inherent problem
    for all LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have shown that quantized smaller LLMs can use RAG to perform complex tasks
    such as personalization. Even though quantization might decrease the ability of
    LLMs to analyze long contexts, it is task- and LLM-dependent. An LLM that performs
    well on a task does not lose much of its long-context abilities when quantized.
    Thus, we conclude that quantized 7B LLMs can be the backbones of RAG with long
    contexts. The reduced computational load obtained from quantization would make
    it possible to run RAG applications with more affordable and accessible hardware.
    For future work, more quantization methods can be included in the experiments
    to see if the findings can be replicated across different methods. We can also
    extend the number set of k, especially between $k=5$, and change the order of
    the documents to better understand how quantized LLMs use their context windows.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 [cs.LG]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained
    Transformers. arXiv:2210.17323 [cs.LG]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented
    Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, and Bryan Catanzaro. 2023. RAVEN: In-Context Learning with Retrieval Augmented
    Encoder-Decoder Language Models. arXiv:2308.07922 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised
    Dense Information Retrieval with Contrastive Learning. arXiv:2112.09118 [cs.IR]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage
    Retrieval for Open-Domain Question Answering. arXiv:2004.04906 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. Evaluating Quantized
    Large Language Models. arXiv:2402.18158 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. Association for Computational
    Linguistics, Barcelona, Spain, 74–81. [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    Chuang Gan, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration. arXiv:2306.00978 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle:
    How Language Models Use Long Contexts. arXiv:2307.03172 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023) Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan.
    2023. Query Rewriting for Retrieval-Augmented Large Language Models. arXiv:2305.14283 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta (2024) Meta. 2024. Introducing Meta Llama 3: The most capable openly available
    LLM to date. Retrieved March 18, 2024 from [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. WebGPT: Browser-assisted
    question-answering with human feedback. arXiv:2112.09332 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proser ([n. d.]) Zachary Proser. [n. d.]. Retrieval Augmented Generation (RAG):
    Reducing Hallucinations in GenAI Applications. [https://www.pinecone.io/learn/retrieval-augmented-generation/](https://www.pinecone.io/learn/retrieval-augmented-generation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robertson et al. (1994) Stephen Robertson, Steve Walker, Susan Jones, Micheline
    Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3\. 0–.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salemi et al. (2023) Alireza Salemi, Sheshera Mysore, Michael Bendersky, and
    Hamed Zamani. 2023. LaMP: When Large Language Models Meet Personalization. arXiv:2304.11406 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
    2023. Quantifying Language Models’ Sensitivity to Spurious Features in Prompt
    Design or: How I learned to start worrying about prompt formatting. arXiv:2310.11324 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrieval-Augmented
    Black-Box Language Models. arXiv:2301.12652 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. 2023. Zephyr: Direct Distillation of LM Alignment. arXiv:2310.16944 [cs.LG]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen
    Song, and Yang Liu. 2023. OpenChat: Advancing Open-source Language Models with
    Mixed-Quality Data. arXiv:2309.11235 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. 2023. Retrieval meets Long Context Large Language Models. arXiv:2310.03025 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2024) Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang,
    and Wei Xu. 2024. Knowledge Conflicts for LLMs: A Survey. arXiv:2403.08319 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench
    and Chatbot Arena. arXiv:2306.05685 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao
    Jiao. 2023. Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
