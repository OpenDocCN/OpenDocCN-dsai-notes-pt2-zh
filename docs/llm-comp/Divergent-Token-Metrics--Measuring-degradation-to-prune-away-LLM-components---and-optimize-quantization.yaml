- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.01544](https://ar5iv.labs.arxiv.org/html/2311.01544)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Björn Deiseroth^(1,2,3,∗) &Max Meuer¹ &Nikolas Gritsch^(1,4) &Constantin Eichenberg¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: \ANDPatrick Schramowski^(2,3,5) &Matthias Aßenmacher^(4,6) &Kristian Kersting^(2,3,5)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Aleph Alpha ² Technical University Darmstadt
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: ³ Hessian Center for Artificial Intelligence (hessian.AI)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: ⁵ German Center for Artificial Intelligence (DFKI)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: ⁴ Department of Statistics, LMU ⁶ Munich Center for Machine Learning (MCML)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have reshaped natural language processing with
    their impressive capabilities. Their ever-increasing size, however, raised concerns
    about their effective deployment and the need for LLM compressions. This study
    introduces the Divergent Token metrics (DTMs), a novel approach for assessing
    compressed LLMs, addressing the limitations of traditional perplexity or accuracy
    measures that fail to accurately reflect text generation quality. DTMs focus on
    token divergence, that allow deeper insights into the subtleties of model compression,
    i.p. when evaluating component’s impacts individually. Utilizing the *First Divergent
    Token* metric (FDTM) in model sparsification reveals that a quarter of all attention
    components can be pruned beyond 90% on the Llama-2 model family, still keeping
    SOTA performance. For quantization FDTM suggests that over 80% of parameters can
    naively be transformed to int8 without special outlier management. These evaluations
    indicate the necessity of choosing appropriate compressions for parameters individually—and
    that FDTM can identify those—while standard metrics result in deteriorated outcomes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Divergent Token Metrics: Measuring degradation to'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: prune away LLM components – and optimize quantization
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Björn Deiseroth^(1,2,3,∗)                        Max Meuer¹                       
    Nikolas Gritsch^(1,4)                        Constantin Eichenberg¹
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Patrick Schramowski^(2,3,5)                        Matthias Aßenmacher^(4,6)
                           Kristian Kersting^(2,3,5) ¹ Aleph Alpha ² Technical University
    Darmstadt ³ Hessian Center for Artificial Intelligence (hessian.AI) ⁵ German Center
    for Artificial Intelligence (DFKI) ⁴ Department of Statistics, LMU ⁶ Munich Center
    for Machine Learning (MCML)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Corresp.: bjoern.deiseroth@aleph-alpha.com'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Aleph-Alpha/Divergent_Tokens](https://github.com/Aleph-Alpha/Divergent_Tokens)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77ae6560d1265f4c9aef7712e4fd6968.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of a diverging generation process. Given the 3-token
    prefix as prompt, a base and its compressed model generate 8 subsequent tokens.
    Our proposed metric points to the first divergent token (FDT). The FDT may cause
    further divergence during the iterative generation process. Note how both models
    score the same perplexity value, as the actual sampling process is not reflected
    (c.f. Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization"), Sec. [4](#S4 "4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") for an empirical exploration).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Cutting-edge Large Language Models (LLMs) based on the transformer architecture
    have revolutionized Natural Language Processing with their exceptional performance,
    notably exemplified by the GPT-series (Radford et al., [2018](#bib.bib11), [2019](#bib.bib12);
    Brown et al., [2020](#bib.bib1); Bubeck et al., [2023](#bib.bib2); OpenAI, [2022](#bib.bib9))
    in text generation. However, these models have grown vastly massive, even exceeding
    half a trillion parameters Chowdhery et al. ([2022](#bib.bib3)). While their bountiful
    parameters aid early training convergence, their practical utility and true necessity
    remain unclear.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Compression strategies like sparsification and quantization can enhance parameter
    efficiency. Current metrics, however, are either averaging too coarsely, such
    as perplexity, or are by design too specific, such as standard NLP benchmarks.
    Either fail to capture the diverging performance nuances introduced by the compression
    early on. As they oversight the actual discontinous text generation process. This
    however is the main use of the final model, and we thus argue, that they are therefore
    insufficient measures for the performance of compressed model. This misalignment
    can lead to unwanted subtle discrepancies in generations, such as grammatical
    errors or a mismatch in numbers as we will see, even when overall metrics, such
    as perplexity, appear satisfactory (cf. Prop. [3.2](#S3.Thmthm2 "Proposition 3.2\.
    ‣ 3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization"),
    Sec. [4](#S4 "4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")).
    An example is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization").'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: To meet these challenges, we introduce the family of Divergent Token metrics
    (DTMs). These metrics are tailored to measure the *model divergence* of LLMs throughout
    the compression process and in relation to the actual generation procedure. We
    demonstrate that the First Divergent Token metric (FDTM) and the Share of Divergent
    Tokens metric (SDTM) offer a more nuanced evaluation compared to perplexity. They,
    moreover, enable an individual component evaluation to rank parts of the model
    best suited for compression, thus enabling meaningful compression while preserving
    text generation quality. Specifically, sparsification enhanced by FDTM indicates
    significant differences in component utilization across layers. For the first
    time, we show that almost twenty percent of the model components can be pruned
    beyond 90%, several even entirely removed, while preserving a single-digit perplexity.
    Consequently, one can employ a sparse matrix format to accelerate computational
    efficiency. Likewise for precision reduction we show that sorting components by
    FDTM coincidentally correlates to sorting by their induced number of outliers
    when being converted to int8\. FDTM identifies the optimal 80% of components that
    overall keep performance without specific outlier-handling. The observed decline
    in performance with more outliers, and the significant influence of specific components
    on those, suggests to reevaluate the applied normalization methods throughout
    the model. This level of precision goes beyond what standard perplexity and conventional
    NLP benchmarks can achieve. As the proposed Divergent Token metrics closely reflect
    the generation process, and as such, can be a measure to foster confidence of
    deployed compressed models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对这些挑战，我们引入了“分歧令牌度量标准（DTMs）”家族。这些度量标准专门用于衡量LLMs在压缩过程中以及与实际生成过程相关的*模型分歧*。我们证明，**第一次分歧令牌度量标准（FDTM）**和**分歧令牌比例度量标准（SDTM）**提供了比困惑度更为细致的评估。它们还允许对模型的各个组件进行单独评估，以排名最适合压缩的部分，从而实现有意义的压缩，同时保持文本生成质量。具体而言，通过FDTM增强的稀疏化显示了不同层之间组件利用的显著差异。我们首次展示了几乎20%的模型组件可以被修剪超过90%，其中一些甚至完全移除，同时保持个位数的困惑度。因此，可以使用稀疏矩阵格式来提高计算效率。对于精度降低，我们展示了按FDTM排序的组件与其在转换为int8时诱发的异常值数量的排序具有巧合的相关性。FDTM识别出总体性能保持良好的80%的最佳组件，无需特定的异常值处理。异常值的增加导致性能下降，并且特定组件对这些异常值的显著影响，建议重新评估整个模型中的归一化方法。这种精度超出了标准困惑度和传统NLP基准所能达到的水平。由于所提议的分歧令牌度量标准密切反映了生成过程，因此可以作为促进压缩模型信心的一个度量。
- en: We proceed as follows. We first briefly revisit common compression principles
    known from the literature. Afterwards we introduce our novel family of metrics.
    Before concluding, we present our exhaustive experimental evaluation of sparsification
    and quantization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的步骤如下。首先，我们简要回顾文献中已知的常见压缩原理。随后，我们介绍我们新颖的度量标准家族。在结束之前，我们展示了对稀疏化和量化的详尽实验评估。
- en: 2 Compression Principles
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 压缩原理
- en: Model compression aims to reduce the hardware resources needed to operate the
    model. Indeed, doing so may sacrifice model accuracy. To keep the regret as small
    as possible, a corrective measure is typically used. Here, we discuss the most
    commonly used concepts and state-of-the-art methods for sparsification and quantization
    of LLMs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩旨在减少操作模型所需的硬件资源。实际上，这样做可能会牺牲模型的准确性。为了将遗憾降到最低，通常会使用修正措施。在这里，我们讨论了用于LLMs稀疏化和量化的最常用概念和最先进的方法。
- en: Outlier and Hessians.
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值和海森矩阵。
- en: Most model compression methods rely either on the separation of outliers or
    the computation of a Hessian matrix. Outliers usually refer to significantly larger
    values in magnitude occurring either in the weight matrix directly or in the activations
    during a forward pass. As most computations are linear matrix multiplications,
    such outliers strongly influence the remaining entropy contained in consecutive
    computations. In the case of sparsification, outliers should be left intact, and
    the values with the least magnitude—which are consequently the least influential—should
    be masked instead. For quantization, it was suggested to be beneficial to separate
    outliers and compute parts in higher-bit formats. This is i.p. motivated due to
    rounding issues in lower precision formats Dettmers et al. ([2022](#bib.bib4)).
    On the other hand, after conversion, Hessian matrices can be applied. They can
    effectively be approximated by computing backpropagation gradients for a small
    number of samples and represent a second-order approximation to reconstruct the
    original model Frantar et al. ([2023](#bib.bib6)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Sparsification.
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of sparsification is a reduction of the overall number of weights and
    as such, a distillation of the relevant computation. Typically, this category
    is divided into “structured” and “unstructured” pruning. Structured-pruning aims
    to locate dynamics, such as the irrelevance of an entire layer or dimension for
    a given use case and prunes these entirely. Unstructured-pruning usually refers
    to the masking of weights, i.e., setting the irrelevant weights to 0\. High levels
    of sparse matrix computations could result in more efficient kernels and computations.
    Masks exceeding 90%, in particular, allow the transition to a specific sparse
    matrix format, which typically necessitates the additional storage of weight indices,
    but significantly enhances performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude pruning selects the masking of weights only based on their magnitudes.
    This is fast to compute but significantly degenerates model performance when pruning
    larger amounts simultaneously. To resolve this issue, wanda Sun et al. ([2023](#bib.bib14))
    proposes to sample a small amount of data and incorporate activations during the
    forward pass. It was shown that this generates more effective one-shot pruning
    masks. Finally, SparseGPT Frantar and Alistarh ([2023](#bib.bib5)) computes iterative
    Hessian approximations to select the lowest impact weights and correct the remaining.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Note that the incorporation of activations can to some extent be interpreted
    as a milder form of training. Moreover, despite these efforts, one-shot pruning
    has not yet produced directly usable models without further final fine-tunings.
    This is in particular the case for high sparsity levels beyond 70%, that we target.
    Finally, the components of a model have not yet been investigated individually,
    at all.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Quantization.
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Model quantization pertains to reducing the utilized precision in the employed
    numeric format. Usually, LLMs are trained in 16-bit floating-point (fp16) and
    converted to 8-bit integer (int8) representations. The naive solution of converting
    float matrices to integers is the AbsMax rounding. It divides a number by the
    maximum value occurring in the matrix and multiplies by the largest available
    integer—as such, it spans a uniform representation grid. The largest float value
    is stored and multiplied for dequantization. The most prominent methods to mitigate
    the introduced rounding errors are LLM.int8() and GPTQ.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化涉及到减少所使用的数值格式的精度。通常，LLMs 在 16 位浮点数 (fp16) 中训练，并转换为 8 位整数 (int8) 表示。将浮点矩阵转换为整数的简单方法是
    AbsMax 四舍五入。它将一个数字除以矩阵中出现的最大值，并乘以最大可用整数——因此，它涵盖了均匀的表示网格。最大浮点值被存储并乘以用于去量化。减小引入的四舍五入误差的最显著方法是LLM.int8()
    和 GPTQ。
- en: Dettmers et al. ([2022](#bib.bib4)) introduced LLM.int8(), which identifies
    vectors containing outliers and retains them in their original fp16 form during
    the matrix multiplication of a forward pass. The vectors lacking outliers are
    quantized fully. Their int8 weights and activations during the forward pass are
    subsequently multiplied and dequantized afterward. This allows them to be integrated
    with the fp16 representation of the outliers. Through empirical investigation
    optimizing the trade-off between degradation in perplexity and the number of outliers
    preserved in fp16, they fixed an absolute outlier threshold.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dettmers 等人 ([2022](#bib.bib4)) 提出了LLM.int8()，该方法识别包含异常值的向量，并在前向传递的矩阵乘法过程中保留其原始的
    fp16 形式。缺少异常值的向量则被完全量化。它们的 int8 权重和激活在前向传递过程中随后会被乘以并在之后去量化。这使得它们可以与异常值的 fp16 表示进行整合。通过优化困惑度降级与保留在
    fp16 中的异常值数量之间的权衡，他们确定了一个绝对的异常值阈值。
- en: The GPTQ framework offers a more robust quantization approach, i.p., to different
    integer bit-precision. It does not rely on any outlier detection mechanism or
    mixed precision computations—matrix multiplications with the weights are fully
    performed on integers. Frantar et al. ([2023](#bib.bib6)) introduce an efficient
    Hessian approximation and iteratively quantize the weights of the matrices while
    performing error corrections on the remaining weights.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 框架提供了更为强大的量化方法，即对不同的整数位精度。它不依赖于任何异常值检测机制或混合精度计算——权重的矩阵乘法完全在整数上进行。Frantar
    等人 ([2023](#bib.bib6)) 引入了一种高效的 Hessian 近似，并在量化矩阵权重的同时，对剩余权重进行误差修正。
- en: 3 Model Divergence Metrics
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 模型离散度指标
- en: 'Perplexity fails to identify minor variations in model degradation at an early
    stage. This behavior is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") and [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL)
    ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization") and discussed in Sec. [3.5](#S3.SS5
    "3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")
    and [4](#S4 "4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    in more detail. To assess model divergence and enhance the model compression process,
    we introduce token-based metrics specifically designed to detect those nuances
    occurring in early compression stages. We start by establishing our notation and
    presenting the perplexity metric (PPL). Subsequently, we introduce an enhanced
    variant of PPL and propose the Share of Divergent Tokens metric (SDTM) and First
    Divergent Token metric (FDTM). We conclude by discussing the advantages of each
    metric compared to traditional perplexity-based measures when assessing the degradation
    of the generative performance of compressed models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Basic notation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let $F$
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textstyle\mathcal{G}(F,y_{:n},$ |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textstyle=\operatorname*{arg\,max}_{j}F(\mathcal{G}(F,y_{:n},N)_{:i})_{ij}.$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 3.2 Perplexity (PPL)
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a ground truth sequence $y$ is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textstyle\operatorname{NLL}(y,F,$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textstyle=-\frac{1}{N-n}\sum_{i=n}^{N-1}\log\mathbb{P}(y_{i+1}&#124;y_{i},..,y_{1}),$
    |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: with $\mathbb{P}(y_{i+1}|y_{i},..,y_{1})=(\mathrm{softmax}\ F(y))_{iy_{i+1}}$.
    Then the perplexity is given by
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\operatorname{PPL}(y,F,n)=\exp(\operatorname{NLL}(y,F,n)).$
    |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: A common practice in the literature, e.g. Dettmers et al. ([2022](#bib.bib4)),
    is to measure model degradation as the increase in average perplexity over a given
    test dataset $\mathcal{D}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34a4a0b69ddad7cf49fe3beeed5faff0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: We test the metrics to distinguish between pruning lowest weights,
    and random weights. FDT is able to discriminate the cases. PPL exactly performs
    on the level of guessing. C.f. Sec. [4.2](#S4.SS2 "4.2 Sparsification reveals:
    Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Context aware model comparison
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we argue that standard evaluation does not reflect the typical generative
    model usage, i.e., there are no empty prompts, and as such, those positions should
    not be taken into account when evaluating the generative performance. Moreover,
    when comparing a compressed model $F^{\prime}$. This leads to the definition of
    the *divergent perplexity metric* as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle M_{\mathrm{DPPL}}(F,$ |  | (1) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\operatorname{PPL}(\mathcal{G}(F,y_{:n},N),F^{\prime},n)\;.$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: 'Finally, let $\mathcal{D}$, we define the *aggregated divergent perplexity
    metric* as the complete evaluation on the dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textstyle\mathcal{M}_{\mathrm{DPPL}}($ |  | (2) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textstyle\frac{1}{&#124;\mathcal{D}&#124;}\sum_{y\in\mathcal{D}}M_{\mathrm{DPPL}}(F,F^{\prime},y_{:n},N).$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: In the following, we will ease notation and omit $\mathcal{M}$, or the words
    aggregated and metric, when they become clear by the context. The as such denoted
    DPPL already substantially improves discriminative capabilities over PPL, as we
    will demonstrate in the empirical evaluation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Divergent Token Metrics
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SDT.
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To iterate on the expressiveness and interpretability of model divergence,
    we propose the *share of divergent tokens (SDT)* as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textstyle\operatorname{SDT}(y,$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\textstyle=&#124;\{i\geq n\colon\ \operatorname*{arg\,max}_{j}\
    F(y)_{ij}\neq y_{i+1}\}&#124;,$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: $\operatorname{SDT}(y,F,n)$ can be interpreted as the number of times the model
    would need to be corrected during decoding to match the ground truth after consuming
    the prefix. This metric provides a more direct interpretation of the errors occurring
    during actual token generation, as opposed to estimating prediction certainties
    as PPL does.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: FDT.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to SDT, we introduce the first divergent token (FDT) as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\textstyle\operatorname{FDT}(y,F,n)}$ |  | (3) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\textstyle=\min\{i\geq n\colon\operatorname*{arg\,max}_{j}\
    F(y)_{i,j}\neq y_{i+1}\}-n,}$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: with the convention that the minimum is equal to $N$ in the same fashion.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As an illustrative example, consider to compute $M_{\mathrm{FDT}}(F,F^{\prime},y_{:n},N)$,
    in contrast to PPL.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Token vs. Perplexity Metrics
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It turns out that divergent token metrics offer a superior criterion for analyzing
    model performance degradation compared to perplexity-based metrics, especially
    in the context of greedy decoding. The main reason for that is the fact that the
    greedy decoding operation $\mathcal{G}$ is a discontinuous function of the logits.
    To formalize this, let us discard the model itself and focus notation solely on
    the concept of logits.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3.1.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The operators and metrics from previous sections defined for models $F,F^{\prime}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: For example, $\mathcal{G}(l,y_{:n},N)_{i+1}=\operatorname*{arg\,max}_{j}l_{ij}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 3.2.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given any $y$ such that
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textstyle&#124;\operatorname{PPL}(y,l,1)-\operatorname{PPL}(y,l^{\prime},1)&#124;$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\textstyle M_{\mathrm{SDT}}(l,l^{\prime},y_{:1},N)$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: Proof.
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'C.f. App. [A](#A1 "Appendix A Proof of Propositions ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").
    ∎'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that even if the average perplexity of a compressed model matches
    the perplexity of the original model, the compressed model can produce a very
    different (and potentially worse) output when performing greedy decoding. Hence
    leading to a false positive. In practice, this is a severe issue since even a
    single diverging token can lead to a completely different subsequent output. It
    is illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")
    and [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics ‣
    Divergent Token Metrics: Measuring degradation to prune away LLM components –
    and optimize quantization") and discussed in Sec. [4.2](#S4.SS2 "4.2 Sparsification
    reveals: Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'As described before, another option is to compute the perplexity with respect
    to the generated completions of the original model. This metric relates more reasonably
    to the share of divergent tokens:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 3.3.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following upper bound holds:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\textstyle M_{\mathrm{SDT}}(l,l^{\prime},y_{:n},N)\leq\frac{N-n}{\log
    2}\log M_{\mathrm{DPPL}}(l,l^{\prime},y_{:n},N).}$ |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: Proof.
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'C.f. App. [A](#A1 "Appendix A Proof of Propositions ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").
    ∎'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: However, a comparable lower bound does not generally hold. In fact, in the case
    $l=l^{\prime}$ is a perfectly flat distribution at any sequence index. This could
    lead to a false negative signal for the generation process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, perplexity-based metrics suffer from false positives or false
    negatives when evaluating degradation of generative performance. The case for
    FDT and SDT is quite straightforward in that they both directly measure the difference
    between model outputs in a what-you-see-is-what-you-get manner.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Note that additional token-based metrics, such as the measurement of the width
    between erroneous predictions, can be readily formulated. These metrics may prove
    especially valuable when assessing potential benefits, for instance, in the context
    of correction-based inference strategies like speculative decoding Leviathan et al.
    ([2023](#bib.bib8)). We now empirically demonstrate the improvements of well-known
    compression methods using our metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 4 Token Metrics Improve Model Compression
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will demonstrate how the proposed metric provides novel insights into
    the efficiency of the architecture of LLMs and serve as a benchmark for model
    compression. We outperform PPL as a ranking metric throughout all experiments.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: More precisely, we apply component-wise probing on sparsification to determine
    individual sparsity rates. Interestingly, the model tends to remove components
    of the attention mechanism on certain layers entirely. In total 40 out of 280
    components are sparsed beyond 90% and 15 removed completely. For quantization,
    on the other hand, we show how component selection significantly influences the
    overall number of model outliers. For the first time, almost 10% of model components
    can be converted to 4-bit integer representations without significant model degradation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Protocol
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us start by clarifying the experimental setup.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Test environment.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All experiments were performed on the public Llama2-7B and 13B models Touvron
    et al. ([2023](#bib.bib15)). Note, however, that we observed similar behavior
    amongst other decoder transformer models. It remains, i.p., throughout upscaled
    sizes, and smaller variations on the architecture or the training procedure.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: For all experiments, we follow best practices of compression evaluations Sun
    et al. ([2023](#bib.bib14)) and randomly sampled from the C4 data Raffel et al.
    ([2019](#bib.bib13)) for training iterations. The final model evaluation utilizes
    the Wikitext2 dataset and standard NLP benchmarks Gao et al. ([2021](#bib.bib7)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Metrics.
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We apply our proposed metrics for performance evaluation as well as selection
    criteria.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'We employ FDT, SDT, and PPL as metrics to assess the overall model divergence.
    When it comes to model compression, we demonstrated that both PPL and our variant
    DPPL typically struggle to measure minor changes adequately (cf. Sec. [3.5](#S3.SS5
    "3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization"),[4.2](#S4.SS2
    "4.2 Sparsification reveals: Attention is not all you need! ‣ 4 Token Metrics
    Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation to
    prune away LLM components – and optimize quantization") and Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")).
    On the other hand, FDT is particularly suited to describe errors for subtle model
    changes. Consequently, we apply FDT for model compression. In the following paragraph,
    we describe the selected parameters for compression using FDT in more detail.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Divergent Token Parameters.
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We empirically selected hyperparameters as follows. Through preliminary sparsification
    experiments, we observed that the most variance is present in the 75%-quantile
    of FDT, as defined in Eq. [3](#S3.E3 "In FDT. ‣ 3.4 Divergent Token Metrics ‣
    3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation to
    prune away LLM components – and optimize quantization"). We denote this value
    by *FDT[75]*. It is in the following our compare-to value.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Next, we swept over the given *context prefix length* $n$ tokens, as it is most
    discriminative on average.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: We observed that most sparsification steps introduce an error in FDT[75] within
    a range of $500$.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d362b8f44cac75211b4568c85ef67a07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Hyperparameter selection of FDT. Visualized is the std in FDT[75]
    over all components when varying prefix length (y-axis) and applying different
    choices for sparsity-step increases (x-axis) as described in Sec. [4.1](#S4.SS1
    "4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") and [4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not
    all you need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3cc6375f53a060e40698e7ae54b781f7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: (a) Comparison of uniform and component-wise pruning using FDT as a metric for
    comparison.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fdf304dec9348977eaf571874ecedef4.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: (b) Converged component config with 75% average sparsity. Layers (x-axis), Component-sparsity
    (y-axis).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Depiction of the proposed sparsification process that converged to
    a 75% sparse Llama-2-13b. a) Model training performance throughout all rounds.
    Our FDT-based sparsification clearly outperforms uniform magnitude pruning. b)
    Converged sparsity values per component. One quarter of attention components are
    pruned beyond 90% sparsity. Significant outliers appear in first and last layers.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Pruning of LLMs.
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Sec. [4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not all you
    need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization"), we will
    show that FDT improves sparsification procedures to achieve high compression rates
    on LLama2-13b.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: We iterate small unstructured sparsification with continued training steps for
    the model to attune to the remaining weights and recover performance. Specifically,
    we apply eight iterations to increase the average model sparsity by $20,15,10,10,5,5,5,$
    percent. As such, the final model has 25% total parameters remaining.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: We run this experiment in two configurations, uniform and FDT-selective. Uniform
    sparsification applies the target increase of the current round to each component
    uniformly, pruning the lowest weights. For FDT, we determine individual component
    sparsification values to evenly distribute the induced error. Based on the previous
    sparsed model $F$ that optimize for
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\operatorname*{arg\,max}_{\{s_{i}\}}\min_{i}M_{\textrm{FDT}_{75}}(F,F^{c_{i}+s_{i}}),$
    |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: such that $\sum_{i}\tilde{s}_{i}=step$ in.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'We further follow the findings of AC/DC Peste et al. ([2021](#bib.bib10)) and
    alternate compressed and decompressed iterations as follows: Each round we train
    a total of $500$.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Note that throughout this experiment series, we only apply pure magnitude pruning
    per iteration. The probing strategy can be applied to other methods, such as Wanda,
    as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Quantization of LLMs.
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For model quantization, we compare the performance of the proposed metrics
    on the task of sorting the model’s components by their lowest introduced error.
    To this end, we build a search tree to find the best model configuration as follows:
    We construct a parallel depth-first search tree with a branching width of $10$
    performing nodes, with one more component naively quantized using AbsMax. From
    this newly identified set of nodes, we again select the best-performing for the
    next iteration. Starting with the unquantized base model Llama2-7b, each node
    contains exactly the number of quantized components respective to its depth, while
    the final node is a fully AbsMax quantized model. We further apply deduplication
    to prevent redundant computations.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Sparsification reveals:'
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Attention is not all you need!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'We applied step-wise uniform magnitude pruning, and our balanced component-wise
    pruning using FDT, to achieve 75% model sparsity. A summary of the results is
    shown in Fig. [4](#S4.F4 "Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental
    Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Attention almost erased.
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fig. [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental
    Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    visualizes the converged sparsity values when applying our balanced pruning using
    FDT.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Notably, the model favors pruning Attention over MLP. In total 40 out of 160
    attention components are sparsed beyond 90% and 15 even completely removed. In
    general the second half of the model appears to be more prunable than the first
    half. The value matrices are overall least pruned of the attention matrices. Finally,
    significant outliers appear at the first and last layers. This finding indicates
    that attention is not efficiently utilized throughout the entire model. In fact,
    only layers 3 to 20 and layer 40 appear to be of significant relevance for the
    models final prediction. This observation might be attributed to an evolving shift
    in distributions, and therewith the concepts processed in embeddings.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Notably, in the first layer Attention Dense and MLP Down remain significant
    dense, while all others are comparable sparse. This observation indicates an incomplete
    shift of token-embeddings.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: General Observations.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Fig. [4(a)](#S4.F4.sf1 "In Figure 4 ‣ Divergent Token Parameters.
    ‣ 4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization"), FDT based balanced pruning significantly lowers the introduced
    error between sparsification rounds. Uniform pruning, on the other hand, substantially
    diverged, and i.p. does not regain performance with the given amount of compute.
    Generally speaking, what is lost can hardly be recovered.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard evaluation of FDT and PPL on Wikitext2, is found in Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").
    The 75% compressed 13b model, with several components pruned away, scored PPL
    8.1, compared to PPL 4.8 of the base model. Note that no other model sparsed beyond
    70% has yet been reported i.p. achieving single-digit PPL. Uniform pruning achieved
    13.5\. Further note, that we almost doubled the mean FDT value when comparing
    to uniform pruning. However, as the generally low FDT value suggests, it already
    diverged quite far from the base model.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: FDT is more discriminative.
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In practice, FDT is more discriminative than PPL to subtle changes. We demonstrate
    this with a test as follows: On each component of the model, we prune 0.1% of
    the weights either randomly or from the lowest weights at a time. The resulting
    model is probed for $1000$ trials with all discussed metrics used to distinguish
    the cases. The results in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3
    Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation to prune
    away LLM components – and optimize quantization") clearly indicate that FDT is
    able to distinguish the cases, while they remain indifferent for PPL-based comparison.
    We therefore omit using PPL as a metric to determine step-sizes for the described
    sparsification experiment.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '在实践中，FDT 对细微变化的区分能力比 PPL 更强。我们通过以下测试演示了这一点：在模型的每个组件上，我们每次随机或从最低权重中修剪 0.1% 的权重。使用所有讨论的度量指标对得到的模型进行
    $1000$ 次试验以区分这些情况。图 [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence
    Metrics ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") 的结果清楚地表明 FDT 能够区分这些情况，而 PPL 基于的比较则无动于衷。因此，我们省略了使用
    PPL 作为确定步骤大小的度量指标，以进行描述的稀疏化实验。'
- en: 'In Fig. [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1
    Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") the converged sparsity rates for all components are displayed.
    A final discussion of the sparsification experiment is found in App. [G](#A7 "Appendix
    G Details on Sparsification, Sec. 4.2 ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental
    Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    中，显示了所有组件的收敛稀疏率。有关稀疏化实验的最终讨论见附录 [G](#A7 "Appendix G Details on Sparsification,
    Sec. 4.2 ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization")。'
- en: '4.3 Quantization: Outliers can be prevented'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 量化：可以防止异常值
- en: Finally, we demonstrate the impact of selecting the right components on quantization.
    We compare the proposed metrics PPL, DPPL, and FDT as a ranking criteria to showcase
    their discrimation capabilities.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示了选择正确组件对量化的影响。我们将提出的度量标准 PPL、DPPL 和 FDT 作为排序标准，展示它们的区分能力。
- en: '![Refer to caption](img/509f2300dc41c1565bd78c4d52d1b0c2.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/509f2300dc41c1565bd78c4d52d1b0c2.png)'
- en: (a) Performance of FDT vs PPL
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (a) FDT 与 PPL 的性能比较
- en: '![Refer to caption](img/91d033441dbaf17d3f6964b36642a93c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/91d033441dbaf17d3f6964b36642a93c.png)'
- en: (b) Selected Components FDT vs PPL
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 选择的组件 FDT 与 PPL 比较
- en: 'Figure 5: Evaluation of the Tree Search as described in text. a) Comparison
    of Tree Search based component-wise quantization. Different number of components
    (x-axis) lead to different token divergence scores (y-axis, normalized to $[0,1]$),
    and i.p. correlates early on to introduces ouliers (second y-axis). Throughout
    the entire search, FDT is able to rank components by their potential errors, and,
    coincidentally, outliers. b) Selected components at respective depth. A.Key and
    A.Value induce most error.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：如文本中所述的树搜索评估。a) 基于树搜索的组件量化比较。不同数量的组件（x 轴）会导致不同的令牌发散分数（y 轴，归一化到 $[0,1]$），并且
    i.p. 早期与引入异常值相关（第二个 y 轴）。在整个搜索过程中，FDT 能够根据潜在错误和偶然的异常值对组件进行排序。b) 各深度下的选择组件。A.Key
    和 A.Value 引发了最多的错误。
- en: Quantization without outlier-handling.
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无异常值处理的量化。
- en: 'Fig. [5](#S4.F5 "Figure 5 ‣ 4.3 Quantization: Outliers can be prevented ‣ 4
    Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization") shows the average performance
    of the top 10 nodes occuring in the respective search tree depth (x-axis). FDT
    constantly outperforms the other metrics on the Share-of-Divergent-Token metric
    (y-axis). Notably, this goes on par with the total number of occurring outliers
    for the respective configs (second y-axis). Certain components appear to significantly
    influence the decline observed in both measures. While DPPL enhances some aspects
    of performance, neither variant of PPL effectively distinguishes these components
    and tends to select those prematurely.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'With FDT, we can cast 80%, i.e. 150, of the model’s components directly to
    int8 using only naive AbsMax—and without further outlier handling—still outperforming
    full LLM.int8() conversion in model performance. Selecting those 150 components
    with DPPL and FDT leads to a close perplexity scores $5.490$ on Wikitext2, c.f.
    Tab. [1](#S4.T1 "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization:
    Outliers can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization"). However the resulting mean FDT improves by almost 50% when also
    selecting the components by this metric. The larger generation of the same sequences
    suggest a model closer to the original when choosing FDT as a selection criteria.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [5](#S4.F5 "Figure 5 ‣ 4.3 Quantization: Outliers can be prevented ‣ 4
    Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")b) shows the selected
    components to each depth respective of a). Most outliers occur when selecting
    A. Key early on. Notably, we observed in Sec. [4.2](#S4.SS2 "4.2 Sparsification
    reveals: Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization"), that this is one of the matrices most suitable
    to sparsify.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '| Sparsification |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '|  | Model | FDT $\uparrow$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '|  | Llama2-13b | - | 4.884 | 53.59 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| \hdashline | $\sim$ 60% sparse (unif.) | 004.7 | 09.244 | 46.32 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '|  | $\sim$ 60% sparse (our) | 007.9 | 6.242 | 48.89 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| \hdashline | $\sim$13.512 | 41.67 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '|  | $\sim$ 75% sparse (our) | 005.5 | 8.101 | 46.32 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| \hdashline | $\sim$ 80% sparse (our) | 005.2 | 9.531 | 45.66 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Quantization |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '|  | Model | FDT $\uparrow$ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '|  | Llama2-7b | - | 5.472 | 50.79 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| \hdashline int8 | LLM.int8()[all] | 036.1 | 5.505 | 50.81 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| AbsMax PPL[150] | 0$\bullet$46.3 | 5.500 | 50.72 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| AbsMax DPPL[150] | 054.1 | 5.490 | 50.75 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| AbsMax FDT[150] (our) | 071.7 | 5.489 | 50.75 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| \hdashline int4 | GPTQ[all] | 011.1 | 5.665 | 48.34 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| GPTQ PPL[16] | 45.0 | 5.511 | 49.91 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| GPTQ DPPL[16] | 0137.0 | 5.476 | 50.02 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| GPTQ FDT[16] (our) | 205.0 | 5.475 | 50.13 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Evals of Compressed Models. Even when evaluating the final model,
    standard NLP benchmarks don’t reflect the actual model degradation, as observed
    in AbsMax quantization. FDT, PPL are evaluated on Wikitext2. Subscript refers
    to best found $k$ quantized components. Bold denote best values.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c50dc5b7cd11709fe8aaff9b6f8a48e6.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: (a) Quantization methods evaluated on Components.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c27372d6927c8bbbe683056e75c7f3b9.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: (b) Top-selected GPTQ(4bit) components.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Evaluation of FDT performance. a) evaluates components separately
    on all quantization methods. Clear outliers in performance are A.Value and MLP.up.
    GPTQ(8bit) is able to evenly amortize the induced error. b) Selecting top-k components
    of GPTQ(4bit). FDT is suited to rank components one-shot.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 16 components in 4-bit.
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [6(a)](#S4.F6.sf1 "In Figure 6 ‣ Quantization without outlier-handling.
    ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics Improve Model
    Compression ‣ Divergent Token Metrics: Measuring degradation to prune away LLM
    components – and optimize quantization") presents a comprehensive assessment of
    the quantization techniques discussed. First, it is noticeable that the LLM.int8()
    method slightly lowers the lower quantile scores of FDT in comparison to AbsMax.
    Yet, GPTQ-8bit demonstrates superior performance, outshining both plain AbsMax
    and LLM.int8(). This method achieves a more balanced error distribution across
    all components (c.f. Fig. [15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization
    Sec. 4.3 ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization")). Conversely, GPTQ-4bit shows noticeable deviations
    in the generation process, with only a limited number of components achieving
    FDT scores above 300\. Despite this, the discriminative power of FDT enabled us
    to identify and merge the top 16 components that minimally compromised the model’s
    integrity, as illustrated in Fig. [6(b)](#S4.F6.sf2 "In Figure 6 ‣ Quantization
    without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'We conclude that PPL is not suitable for either, selecting components to compress,
    or for assessing the degradation of compressed models, as indicated in Table [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'In Appendix Fig. [15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization
    Sec. 4.3 ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") detailed FDT statistics on all components are displayed.
    A final discussion of the quantization experiment is found in App. [B](#A2 "Appendix
    B FDT compared to standard model evals ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '在附录图[15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization Sec. 4.3 ‣
    Divergent Token Metrics: Measuring degradation to prune away LLM components –
    and optimize quantization")中展示了所有组件的详细FDT统计数据。关于量化实验的最终讨论见附录[B](#A2 "Appendix
    B FDT compared to standard model evals ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")。'
- en: 5 Conclusion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We introduced the Divergent Token Metrics (DTMs), a tailored approach to evaluate
    the performance differences of compressed generative models. In particular, DTMs
    respect the usually applied greedy sampling procedure to generate predictions.
    We proved that DTMs achieve appropriate metric bounds and are not affected from
    catastrophic artefacts that perplexity-based metrics encounter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了发散标记度量（DTMs），这是一种评估压缩生成模型性能差异的定制方法。特别是，DTMs遵循通常应用的贪婪采样程序来生成预测。我们证明了DTMs能够达到适当的度量界限，并且不受困惑度基础度量所遇到的灾难性伪影影响。
- en: Due to our metrics’ discriminative capabilities, we are able to measure the
    subtle influence of model components individually, and in turn build a fine-grained
    selection for compression. With our sparsification experiments we achieved an
    outperforming 75% sparse version of the Llama2-13b model with a small amount of
    training steps and otherwise only applying magnitude pruning. Many (in particular
    attention) modules were entirely removed, which hints that attention is, after
    all, not always needed—during inference—in decoder models. Notably, the MLP components
    in the first and last layers are extremely sensitive, which hints at an incomplete
    shift of token-embedding distributions. For quantization, we were able to sort
    the influence of the quantized components individually. Interestingly sorting
    by FDT coincides with sorting by outliers. We successfully converted 80% of the
    LLama2-7b components naively to int8 using AbsMax, without severe degeneration
    of performance, and without any outlier-handling. Further, we concluded that GPTQ-8bit
    performs very well. The 4bit version, on the other hand, significantly degenerates.
    However, we were able to select the 16 out of 224 significantly outperforming
    components, that even combined, sustained substantial model performance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们度量标准的辨别能力，我们能够单独测量模型组件的微妙影响，从而建立一个精细的压缩选择。通过稀疏化实验，我们成功地将Llama2-13b模型压缩到75%的稀疏版本，只用了少量的训练步骤，并且只应用了幅度剪枝。许多（特别是注意力）模块被完全移除，这表明注意力在解码器模型中，在推理时并非总是必要的。值得注意的是，第一层和最后一层的MLP组件非常敏感，这表明token-embedding分布尚未完全转换。对于量化，我们能够单独排序量化组件的影响。有趣的是，按FDT排序与按异常值排序是一致的。我们成功地将80%的LLama2-7b组件通过AbsMax简单地转换为int8，没有严重的性能退化，也没有任何异常值处理。此外，我们得出结论，GPTQ-8bit表现非常好。而4bit版本则显著退化。然而，我们能够从224个组件中选择出16个表现显著的组件，这些组件即使结合在一起，也能保持模型的高性能。
- en: Building up on this work, one could investigate further variations of our metric
    for specific use-cases. The average distances between falsely predicted tokens,
    as an example, intuitively reflect the efficiency of speculative sampling algorithms.
    We envision fully heterogeneous model architectures, with components mixed in
    precision and varying levels of sparsities. Afterall, there is ample diversity
    throughout the model to be exploit.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，可以进一步研究我们度量的不同变体以适应特定用例。例如，错误预测的标记之间的平均距离，直观上反映了投机采样算法的效率。我们设想了完全异质的模型架构，其中组件的精度和稀疏度水平各不相同。毕竟，模型中有大量的多样性可以利用。
- en: Limitations
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: With the proposed DTMs, compression processes can be tailored to use cases—and
    we can measure its performance degeneration. We hinted with the sparsification
    experiments, that MLP and Attention can be ascribed varying levels of significance
    throughout the layers. These variations should further be exploited to optimize
    model architectures. In particular variations of specific datasets to probe or
    finetune on could lead to interesting variations.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: As a pruning strategy, we achieved outperforming results using only naive magnitude
    pruning. DTMs should directly be applicable to other masking strategies, such
    as Wanda Sun et al. ([2023](#bib.bib14)), which may further improve results. Finally,
    the generalizability of the metrics to other sampling strategies should be investigated.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We affirm that our research adheres to the [ACL Ethics Policy](https://www.aclweb.org/portal/content/acl-code-ethics).
    This work involves the use of publicly available data sets and does not involve
    human subjects or any personally identifiable information. We declare that we
    have no conflicts of interest that could potentially influence the outcomes, interpretations,
    or conclusions of this research. All funding sources supporting this study are
    acknowledged. We have made our best effort to document our methodology, experiments,
    and results accurately and are committed to sharing our code, data, and other
    relevant resources to foster reproducibility and further advancements in research.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG,
    German Research Foundation) as part of BERD@NFDI - grant number 460037581.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: We gratefully acknowledge support by the German Center for Artificial Intelligence
    (DFKI) project “SAINT”, the Hessian Ministry of Higher Education, and the Research
    and the Arts (HMWK) cluster projects “The Adaptive Mind” and “The Third Wave of
    AI”, and the HMWK and BMBF ATHENE project “AVSV”.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: We further thank Graphcore and Manuel Brack for the fruitful discussions throughout
    this work.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. [Llm.int8(): 8-bit matrix multiplication for transformers at scale](http://arxiv.org/abs/2208.07339).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. [Gptq: Accurate post-training quantization for generative pre-trained
    transformers](http://arxiv.org/abs/2210.17323).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.
    Fast inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pages 19274–19286\. PMLR.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) OpenAI. 2022. [Chatgpt: Optimizing language models for dialogue](https://openai.com/blog/chatgpt/).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peste et al. (2021) Alexandra Peste, Eugenia Iofinova, Adrian Vladu, and Dan
    Alistarh. 2021. Ac/dc: Alternating compressed/decompressed training of deep neural
    networks. *Advances in neural information processing systems*, 34:8557–8570.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://arxiv.org/abs/1910.10683).
    *arXiv e-prints*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. *arXiv preprint
    arXiv:2306.11695*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Proof of Propositions
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Proof of Proposition [3.2](#S3.Thmthm2 "Proposition 3.2\. ‣ 3.5 Token vs. Perplexity
    Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are many ways to construct sequences that satisfy the desired relation.
    One is as follows: Let $l\in{\mathbb{R}}^{N\times|\mathcal{V}|}$ by'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle l^{\prime}_{ia_{2}(l)_{i}}$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: and $l^{\prime}_{ij}=l_{ij}$. ∎
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof of Proposition [3.3](#S3.Thmthm3 "Proposition 3.3\. ‣ 3.5 Token vs. Perplexity
    Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $z=\mathcal{G}(l,y_{:n},N)$. Applying the definitions and elementary operations,
    we have
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{i=n}^{N}-\log p_{i}=(N-n)\log M_{\mathrm{DPPL}}(l,l^{\prime},y_{:n},N).$
    |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: Let $A=\{i\geq n\colon p_{i}\leq 1/2\}$. Then
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\sum_{i=n}^{N}-\log p_{i}$ |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\geq\sum_{i\in A}-\log p_{i}\geq&#124;A&#124;\log 2.$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: Here we first used that $\log p_{i}\leq 0$ is automatically the maximum value
    of the distribution, and the softmax operation is monotone. Putting everything
    together we arrive at the desired inequality. ∎
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3a1423bbb8c68a66dea48b9a842132ab.png)![Refer to caption](img/d273c5ebe55c045c8c1e986f3f89fbe1.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Pruning MLP and Attn only indeed compromises remaining model capabilities.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B FDT compared to standard model evals
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [10](#A5.F10 "Figure 10 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows a comparison of standard benchmarks (middle)
    to FDT (right) and PPL (left) when quantizing parts of a model. Often standard
    evals fail to distinguish between compressed models. Sometimes they even depict
    better performance—which may be true, when regarding compression as a fine-tuning
    method and considering the short required token predictions. FDT thoroughly gives
    discriminative statistics with resprect to the base model, on how much the compressed
    model equals the original. Note how the error seems to be upper bounded, which
    suggests that errors may average out throughout the model. Mean zeroshot accuracy
    denotes the average on the standard NLP-eval harness.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C True positives can be predicted
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [8](#A3.F8 "Figure 8 ‣ Appendix C True positives can be predicted ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") shows several metrics applied to the token-distributions, in order
    to estimate on whether the compressed and original model predictions are equal.
    Notably, L1 and L2 errors on the entire distribution seem to somewhat capture
    the discriminative capabilities of false predictions. The probability scores themselves
    are only marginally usable. Using top-2 uncertainty, i.e. the difference between
    the top-2 tokens as a measure, we obtain a reliable prediction of true positives.
    True negatives however still remain with a significant overlap.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed004c18300ce14b20e426527e157ba9.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Top-2 uncertainty is discriminative enough to give clear true-positives
    estimates on compressed models.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D MLP is for knowledge,
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Attention for relation
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we observed that when pruning only attention, prompt-extraction capabilities
    degenerate severely. When only pruning MLP components, on the other hand, it influences
    mostly world knowledge QA benchmarks, c.f. Fig. [7](#A1.F7 "Figure 7 ‣ Appendix
    A Proof of Propositions ‣ Divergent Token Metrics: Measuring degradation to prune
    away LLM components – and optimize quantization").'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix E Details on Search Tree, Sec. [4.3](#S4.SS3 "4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [9](#A5.F9 "Figure 9 ‣ Appendix E Details on Search Tree, Sec. 4.3 ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") shows the layers (y-axis) of which components are selected at each
    round (x-axis). While there seems to be a pattern on when using FDT as a criteria
    (top), selection by PPL (bottom) looks more random.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [13](#A5.F13 "Figure 13 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the comparison of search tree as described
    to greedy search on a single evaluation of all components. Until 150 components,
    FDT proves more stable over the PPL variants as seen in Fig. [13(a)](#A5.F13.sf1
    "In Figure 13 ‣ Appendix E Details on Search Tree, Sec. 4.3 ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1fa1849168639d7848b3205b2b91cec.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Layers selected in each round of the search tree. Top, when applying
    FDT, bottom, when applying PPL as a ranking metric.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1701b09ccf19bbdc1f5c1d849f7af700.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Comparison of the discrimination capabilities of FDT and PPL for
    different configurations when applying LLM.int8() conversion on Llama2-7b. Best
    and Worst mark a single component being converted, with most and least mean influence.
    First and Second half consecutively convert half of the model each. While significant
    changes can be observed using FDT, all configurations appear indifferent for PPL.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3ffba80f2a0054aacea49f2a0de983b.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: (a) 8-bit Quantization NLP benchmarks
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03bb44d07bc9b4bd079a2e914f5fb671.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: (b) 4-bit Quantization NLP benchmarks
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Detailed view on aggregated values of Tab. [1](#S4.T1 "Table 1 ‣
    Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented
    ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization") when selecting
    Llama2-7b components to quantize by metrics.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0d0c5ed15a38503c3a2351106a341a5.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Detailed view on aggregated values of Tab. [1](#S4.T1 "Table 1 ‣
    Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented
    ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization") when selecting
    Llama2-13b components to sparsify by metrics.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d266dcaf687a4d2349dad08665ef7f16.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: (a) FDT tree vs greedy
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e38fc03192fa19b5e0cbf00d4b01963.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: (b) DPPL tree vs greedy
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d7614092e0bee4506aea5883ffc60b12.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: (c) PPL tree vs greedy
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: Comparison of performance when selecting components by the tree-search
    as described to greedy selection of once evaluated components for all discussed
    metrics. Clearly, FDT is most stable until 150 components.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix F Details on Quantization Sec. [4.3](#S4.SS3 "4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows detailed component-wise evaluations aggregated
    in Fig. [6(a)](#S4.F6.sf1 "In Figure 6 ‣ Quantization without outlier-handling.
    ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics Improve Model
    Compression ‣ Divergent Token Metrics: Measuring degradation to prune away LLM
    components – and optimize quantization").'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [14](#A6.F14 "Figure 14 ‣ Appendix F Details on Quantization Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the final configurations as compared in Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [11](#A5.F11 "Figure 11 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the detailed nlp-eval scores of Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In total the entire search evaluation required 16 GPU-days with A100s to complete
    all metrics.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b358896a4a9240e0998b4b1be02b8f3.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: (a) The 150 selected components selected by metrics for 8-bit AbsMax conversion.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/00ff895da68a5c176586c27257f6301c.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: (b) The 16 selected components selected by metrics for 4-bit GPTQ conversion.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: Detailed view of the Llama2-7b components in Tab. [1](#S4.T1 "Table
    1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers can be
    prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    selected by metrics for lower precision conversion.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e83a01caf47640279447c6dbb4f56cc.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Full view of the influence of individual component-wise quantization
    measured by FDT.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix G Details on Sparsification, Sec. [4.2](#S4.SS2 "4.2 Sparsification
    reveals: Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization")'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [16](#A7.F16 "Figure 16 ‣ Appendix G Details on Sparsification, Sec. 4.2
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows a different aggregated perspective of Fig. [4(b)](#S4.F4.sf2
    "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental Protocol ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization"), to point out more
    direct the occuring variances.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4cff68c84b4cb611c28f3f925b45baf.png)![Refer to caption](img/fa199f6333999f9bd65f08c1a5b93f81.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Distribution of 75% average model sparsity. A. denotes Attention.
    Top: Aggregated by layers. The first and last layer have highest variance (MLP
    most important, c.f. Fig. [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters.
    ‣ 4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization")). Second half reaches sparsities close component removal. Bottom:
    Per component aggregation. In the second half of layers, the importance of attention
    drops drastically. MLP almost remains, with outliers to larger importance.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [17](#A7.F17 "Figure 17 ‣ Appendix G Details on Sparsification, Sec. 4.2
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the rank of lowest influence (measured by
    FDT) of components (x-axis) throughout various sparsity levels (y-axis). I.e.
    starting with a uniformly pruned model in 5% steps, we measured the rank when
    adding an additional 2.5% only to a single component. Interestingly, components
    seem to retain their importance throughout the various levels of sparsity.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [12](#A5.F12 "Figure 12 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the detailed nlp-eval scores of Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](#A5.F12 "图 12 ‣ 附录 E 关于搜索树的详细信息，第 4.3 节 ‣ 发散标记度量：测量退化以修剪 LLM 组件 - 以及优化量化")
    显示了表[1](#S4.T1 "表 1 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值 ‣ 4 标记度量改进模型压缩 ‣ 发散标记度量：测量退化以修剪
    LLM 组件 - 以及优化量化") 的详细 nlp-eval 评分。
- en: Note that, despite being often close in relative sparsity, the total number
    of parameters pruned for MLP is significantly larger than for Attention matrices
    (ratio 3:1).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管相对稀疏度通常接近，但 MLP 的总参数剪枝数量显著大于注意力矩阵（比例 3:1）。
- en: In total one sparsification training required 32 GPU-days with A100s for our
    experiment, and 29 GPU-days for uniform pruning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的实验中一次稀疏化训练需要 32 个 A100 GPU 天，而均匀剪枝需要 29 个 GPU 天。
- en: '![Refer to caption](img/9709d698129261b405730fbbae404925.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9709d698129261b405730fbbae404925.png)'
- en: 'Figure 17: Trends during sparsification. We plot the ranking of the components
    FDT value through various sparsity levels (y-axis) for all components (x-axis).
    Interestingly, there is a clear trend of components retaining “their importance”.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：稀疏化过程中的趋势。我们绘制了在不同稀疏度水平（y 轴）下所有组件的 FDT 值排名（x 轴）。有趣的是，组件保持“其重要性”的趋势非常明显。
