- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17415](https://ar5iv.labs.arxiv.org/html/2406.17415)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Razvan-Gabriel Dumitru^(1,2), Vikas Yadav¹, Rishabh Maheshwary¹, Paul-Ioan Clotan³,
  prefs: []
  type: TYPE_NORMAL
- en: Sathwik Tejaswi Madhusudhan¹, Mihai Surdeanu²
  prefs: []
  type: TYPE_NORMAL
- en: ¹ServiceNow Research, ²University of Arizona, ³Università di Bologna
  prefs: []
  type: TYPE_NORMAL
- en: 'Correspondence: [razvandumm@gmail.com](razvandumm@gmail.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We present a simple variable quantization approach that quantizes different
    layers of a large language model (LLM) at different bit levels. Specifically,
    we quantize the most important layers to higher bit precision and less important
    layers to lower bits to achieve floating point quantization levels. We propose
    two effective strategies to measure the importance of layers within LLMs: the
    first measures the importance of a layer based on how different its output embeddings
    are from the input embeddings (the higher the better); the second estimates the
    importance of a layer using the number of layer weights that are much larger than
    average (the smaller the better). We show that quantizing different layers at
    varying bits according to our importance scores results in minimal performance
    drop with a far more compressed model size. Finally, we present several practical
    key takeaways from our variable layer-wise quantization experiments: (a) LLM performance
    under variable quantization remains close to the original model until 25-50% of
    layers are moved in lower quantization using our proposed ordering but only until
    5-10% if moved using no specific ordering; (b) Quantizing LLMs to lower bits performs
    substantially better than pruning unless extreme quantization (2-bit) is used;
    and (c) Layer-wise quantization to lower bits works better in the case of larger
    LLMs with more layers compared to smaller LLMs with fewer layers. The code used
    to run the experiments is available at: https://github.com/RazvanDu/LayerwiseQuant.'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹footnotetext: The work was done while Razvan-Gabriel Dumitru interned at
    ServiceNow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels'
  prefs: []
  type: TYPE_NORMAL
- en: 'Razvan-Gabriel Dumitru^(1,2), Vikas Yadav¹, Rishabh Maheshwary¹, Paul-Ioan
    Clotan³, Sathwik Tejaswi Madhusudhan¹, Mihai Surdeanu² ¹ServiceNow Research, ²University
    of Arizona, ³Università di Bologna Correspondence: [razvandumm@gmail.com](razvandumm@gmail.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46f96d9dbfa4325d4bc74886cb39298d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overall intuition behind our approach. We first rank the layers in
    an LLM (e.g., LLaMa-2-13B) using an importance score (shown here is ranking based
    on our Layer Input Modification (LIM) score, see [section 3.1](#S3.SS1 "3.1 Layer
    Importance Scores ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels")). The color intensity in
    each layer represent their LIM importance score. Then, the 30 most important layers
    are quantized in 4 bit while the remaining 10 least important layers are quantized
    in 2 bits, resulting in 3.5 bits as the average bit size.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have achieved remarkable performance on a variety
    of tasks, especially when scaled to billions of parameters Jiang et al. ([2023](#bib.bib13),
    [2024](#bib.bib14)); Touvron et al. ([2023](#bib.bib29)); Zhang et al. ([2022](#bib.bib38));
    Team et al. ([2023](#bib.bib28)). The largest open-source models available can
    have an upwards of 400B parameters, such as LLaMa3-400B Touvron et al. ([2023](#bib.bib29)).
    Even small models such as LLaMa3-8B require as much as 20GB of VRAM to run on
    a GPU at the original precision, making them unusable for low resource settings.
    In such settings model compression techniques such as quantization are critical
    Zhu et al. ([2023b](#bib.bib40)); Wan et al. ([2023](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: Most prominent techniques for model compression broadly cover pruning Ma et al.
    ([2023](#bib.bib19)), knowledge distillation Gu et al. ([2023](#bib.bib11)), and
    quantization Zhu et al. ([2023b](#bib.bib40)). Pruning yields improvements in
    inference speed, but often results in substantial performance drop Frantar and
    Alistarh ([2023](#bib.bib7)); Men et al. ([2024](#bib.bib20)). On the other hand,
    quantization has proven to be a more robust solution for model size compression
    with comparatively much smaller performance drops Lin et al. ([2024](#bib.bib17)).
    In our work, we primarily focus on memory reduction through quantization. Further,
    quantization can be training specific or post-training Yao et al. ([2024](#bib.bib35)),
    where trained models are quantized without requiring any further training. We
    focus on post-training quantization due to its practicality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The majority of quantization techniques proposed recently (Frantar et al.,
    [2023](#bib.bib8); Xiao et al., [2023](#bib.bib33); Yao et al., [2022](#bib.bib36),
    inter alia) focus on the quantization of all the LLM layers to a single precision
    bit, or quantizing only specific weights of the network Lin et al. ([2024](#bib.bib17)).
    This has shown to be costly, their effectiveness is data dependent, and their
    implementations can be considered moderately challenging. In contrast, we propose
    a simple technique that quantizes different layers at different bit precision
    depending on their importance. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels") shows an example of our idea. We show that our approach
    is simple to implement, performs well, and provides greater flexibility to compress
    models in varying bit precision as per memory requirements. Further, our approach
    is complementary to other quantization techniques. For example, in our experiments
    we coupled our idea with two effective and prominent quantization techniques:
    GPT-Q and Quanto, but any other techinques can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The contributions of our work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4c5d9b032dd99f8d14373e5492ffdfdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Plots showing the effect of variable quantization for LLaMa2-13b
    and multiple datasets using Quanto. The leftmost point indicates LLM performance
    when all 40 layers of the LLM are represented in 4-bit; the rightmost point shows
    LLM performance when all layers are quantized to 2-bit. The dots on each curve
    (in each plot) show accuracy when the model is quantized to lower bits by converting
    less important layers to 2 bits one by one. Red and purple line indicate performance
    from 8bit and fp16 precision model (ceiling models). As shown, there is no considerable
    performance drop from fp16 or 8bit to 4bit precision. Hence, we focus our experiments
    on quantization below 4 bits. The vertical gray line indicates the quantization
    point that preserves 90% of the 4bit performance. The red line represents when
    layers are ordered randomly. We chose 3 random orders of the layers and quantized
    layers to 2 bits as per these orders. The standard deviation in performance from
    random orders are highlighted on the red curve. The curves are plotted on 2K evaluation
    data while results on full data is summarized in [table 1](#S3.T1 "In 3.2.1 Quanto
    ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64cc1d91c897604ff920334440c5fa57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Plots showing the effect of variable quantization for Mistral-7b
    and multiple datasets using Quanto. All notations are the same as in [Figure 2](#S1.F2
    "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a novel method for layer-wise quantization of LLMs at different
    bit levels to achieve flexible lower bit precision overall, enabling LLMs to fit
    into range of specified memory capacity. We accompany our method with a detailed
    study, which highlights various important findings such as importance of layer
    ranking for quantizing less important layers with lower bits and more important
    layers with higher bit precision. We show that models can be quantized significantly
    more from 4-bit while retaining 90% of performance when following our proposed
    order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose and study two layer importance scores for variable quantization.
    Our first score named layer input modification (LIM) is based on how much a layer
    changes its input representations into the output ones. This score is calculated
    with an unlabeled text calibration corpus. The second scoring method, called z-score
    distribution (ZD), measures the distribution of weight magnitudes within a layer
    to determine its importance. To our knowledge, we are the first to propose layer
    orderings for quantization. We validate these scores by empirically showing that
    when LLM layers are quantized to lower bits as per rankings from our two importance
    score, they retain performance much more strongly than several other layer ordering
    baselines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We evaluate the impact of our variable quantization method based on layer importance
    on five top performing LLMs from different model families and different sizes.
    We draw several important practical findings from these experiments: (a) LLM performance
    under variable quantization remains close to the original model when using our
    ordering until reaching the level of 3.0-3.25 bits on average; (b) Quantizing
    LLMs to lower bits performs substantially better than pruning; however, under
    extreme quantization settings (i.e., $<=2-bits$ bits) performs much better than
    three levels of quantization, suggesting that the interaction between layers with
    different quantization is complex and may need more investigation; and (d) Layer-wise
    quantization to lower bits works better in the case of larger LLMs with more layers
    compared to smaller LLMs with fewer layers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization techniques for large language models (LLMs) aim to reduce the precision
    of weights and activations to lower-bits without significantly compromising performance Zhu
    et al. ([2023b](#bib.bib40)). Popular approaches include Post-Training Quantization
    (PTQ) Banner et al. ([2019](#bib.bib2)) and Quantization-Aware Training (QAT)
    Liu et al. ([2023](#bib.bib18)). PTQ can be divided into static quantization,
    which uses a small dataset to calibrate scaling factors for weights and activations,
    and dynamic quantization, which quantizes activations on-the-fly during inference.
    Our study utilizes the static PTQ techniques such as GPT-Q in our experiments
    for practicality.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few works have highlighted 4 bit precision as a robust quantization limit
    for wide variety of NLP tasks Dettmers and Zettlemoyer ([2023](#bib.bib6)). Our
    results ([fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels") and [3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels")) also show similar findings
    that performance drop is very minute from bf16 to 8 bits, and then to 4 bits.
    Hence, our experiments focus more on quantizing LLMs below 4 bits to highlight
    effectiveness of layer-wise quantization based on layer importance. Specifically,
    our empirical results ([section 5](#S5 "5 Discussion of Results ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"))
    show that variable layer-wise quantization can retain 90% of the performance with
    a notable compression upto 2.85 bits overall. Concurrent (ArXiv preprint) work
    by Tai et al. ([2024](#bib.bib27)) also show effectiveness of variable quantization
    in vision language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A few previous works have studied layer importance in transformers Vaswani
    et al. ([2017](#bib.bib30)); Simoulin and Crabbé ([2021](#bib.bib24)). Some of
    the recent, (unpublished) concurrent works such as ShortGPT Men et al. ([2024](#bib.bib20))
    have also proposed utilizing layer importance but primarily focusing on pruning.
    Shen et al. ([2020](#bib.bib23)) had utilized hessian information from each layer
    as an importance measure to quantize specific weight matrices at different bits.
    Different from these, our work is focused more on utilizing layer importance for
    quantizing more important layers (fully) in higher bits and less important layers
    in lower bits. Importantly, our proposed approach of layer-wise quantization of
    LLMs based on their layer importance is a method that can be easily extended with
    any quantization techniques like GPT-Q and Quanto ([section 3.2](#S3.SS2 "3.2
    Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic and
    Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")). We also compare
    ShortGPT layer importance based pruning baseline with our layer-wise quantization
    approach in [section 6.1](#S6.SS1 "6.1 Pruning vs. Quantization ‣ 6 Analyses ‣
    Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our study, given a LLM with N layers denoted as $\{\mathbf{L_{1},L_{2},L_{3},....L_{N}}\}$,
    we first compute the importance of each layer, and then quantize them differently
    based on their importance. To achieve the former, we propose two scoring methods
    that allow us to rank the layers based on their respective importance scores.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Layer Importance Scores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We call our first layer importance score layer input modification (LIM). The
    intuition behind LIM is that the more a layer changes its received input embeddings
    the more important it must be. More formally, the LIM score for a specific layer
    $\mathbf{L_{i}}$ as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\vspace{-2mm}\text{LIM}(\mathbf{L_{i}})=-\frac{\mathbf{L_{i}^{I}}\cdot\mathbf{L_{i}^{O}}}{\&#124;\mathbf{L_{i}^{I}}\&#124;\&#124;\mathbf{L_{i}^{O}}\&#124;}\vspace{+1mm}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: The dot product $\mathbf{L_{i}^{I}}\cdot\mathbf{L_{i}^{O}}$, we use all 50 documents
    of pg19! Rae et al. ([2019](#bib.bib21)) (an unlabelled text corpus) as calibration
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Our second score, called z-score distribution (ZD), is based on the distribution
    of parameter values and does not require any calibration data. Intuitively, this
    score considers a layer as more important if it contains more weights that are
    much higher than average.
  prefs: []
  type: TYPE_NORMAL
- en: We examine the proportion of weights in a layer exhibiting a z-score greater
    than 1\. The z-score of a weight $w_{i}$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z_{i}=\frac{w_{i}-\mu}{\sigma},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where for layer $\mathbf{L_{i}}$, is expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\text{ZD}(L_{i})=\frac{&#124;Li_{Zscore}></math> |  |'
  prefs: []
  type: TYPE_TB
- en: quantifying the ratio of weights whose z-scores exceed 1 (<math id=$$ layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer ranking baselines:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To highlight the benefits of our layer importance scores, we compare them to
    two baselines. First, we randomly rank layers with different random seeds to highlight
    ordering of layer is important. Second, to specifically highlight the strengths
    of the LIM score, we consider a reverse LIM ordering as a baseline. Finally, we
    also considered the reverse index of layers, i.e., from layer 30 to layer 1, following
    Gromov et al. ([2024](#bib.bib10)) who has shown just removing layers from the
    top of the LLM to be an effective order for layer pruning. We implement the same
    pruning with specified reverse indexes from Gromov et al. ([2024](#bib.bib10))
    and show its comparison in [Figure 4(a)](#S5.F4.sf1 "In Figure 4 ‣ Item (3) ‣
    5 Discussion of Results ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Quantization Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our work, we employ two post-training quantization techniques—GPT-Q and Quanto—to
    our proposed layer-wise quantization study.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Quanto
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quanto is a fast-acting quantization method¹¹1[https://github.com/huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto)
    that simplifies the process of reducing precision after training. In practice,
    Quanto achieves quicker quantization by applying uniform scaling factors across
    all layers of a model, avoiding the need for detailed data-driven analysis of
    each layer’s distribution. Similarly to most techniques, it allows for int2, int4,
    int8, and fp8 quantization. In our implementation for variable quantization of
    different layers, we quantized the same model to two different bit levels (lets
    say int2 and int4). Then, based on the layer importance order, we selected less
    important layers from the int2 quantized layer sets and more important layers
    from int4 quantized sets.
  prefs: []
  type: TYPE_NORMAL
- en: Quanto Quantization Model Avg. Layers WNGD ARC PIQA HLSWG MMLU Avg.Acc. bits
    2 bits 4-2 bit LLaMa-7B 4.0 0 67.9 74.2 77.3 55.9 38.4 62.7 Mistral-7B 4.0 0 72.5
    78.8 79.8 59.3 55.5 69.2 LLaMa-13B 4.0 0 71.0 78.7 79.2 59.0 50.0 67.6 QWEN-7B
    4.0 0 68.7 79.0 79.2 57.7 66.3 70.2 \cdashline2-10 LIM Ordering LLaMa-7B 3.68
    5 65.6 68.7 74.6 53.7 36.6 59.8 3.37 10 65.3 61.9 70.6 49.8 34.3 56.4 3.06 15
    60.8 45.6 64.3 42.2 27.6 48.1 Mistral-7B 3.68 5 71.7 74.0 76.7 56.4 54.9 66.7
    3.37 10 69.3 61.8 70.0 50.1 51.7 60.6 3.06 15 59.4 43.6 61.2 37.6 26.4 45.6 LLama-13B
    3.75 5 70.2 76.6 78.0 57.7 48.9 66.3 3.50 10 69.1 72.9 76.3 55.7 47.4 64.3 3.25
    15 69.7 66.9 73.7 52.6 45.8 61.7 Qwen-2-7B 3.64 5 51.1 46.3 67.3 40.9 24.1 46.0
    3.28 10 51.7 31.0 57.4 29.8 23.4 38.6 2.92 15 48.2 25.7 53.1 26.1 24.5 35.5 Z-score
    Ordering LLama-7B 3.68 5 65.7 68.7 74.9 53.0 33.5 59.1 3.37 10 64.1 59.2 69.7
    48.7 31.2 54.6 3.06 15 55.4 43.8 61.4 36.4 24.5 44.3 Mistral-7B 3.68 5 70.7 74.2
    77.5 56.3 53.0 66.3 3.37 10 53.3 39.3 60.0 30.5 23.4 41.3 3.06 15 51.7 27.5 53.3
    27.2 23.5 36.6 LLama-13B 3.75 5 70.3 76.0 77.2 57.1 48.1 65.7 3.50 10 70.7 72.3
    75.8 54.6 47.0 64.1 3.25 15 68.9 66.8 72.1 51.9 47.0 61.3 Qwen-2-7B 3.64 5 63.1
    61.0 70.5 48.4 55.6 59.7 3.28 10 51.5 29.3 53.6 27.0 25.3 37.3 2.92 15 49.9 26.0
    52.5 26.0 25.0 35.9
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Accuracy on full evaluation datasets of different models quantized
    with Quanto. All layers start at 4 bits; we then quantize N number of layers in
    2 bits where N is mentioned in the “Layers 2 bits” column. We also show results
    for 8 to 4 bit quantization in Appendix in [table 4](#A1.T4 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). Average
    performances within 90% of the 4 bit model are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| GPT-Q Quantization |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Model | Avg. | Layers | WNGD | ARC | PIQA | HLSWG | MMLU | Avg.Acc |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | bits | 2 bits |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| LIM Ordering | LLama-7B | 3.68 | 5 | 67.1 | 73.4 | 76.2 | 54.9 | 37.5 | 61.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3.37 | 10 | 67.6 | 67.5 | 73.4 | 52.9 | 35.2 | 59.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.06 | 15 | 65.1 | 56.0 | 68.0 | 46.4 | 31.5 | 53.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mistral-7B | 3.68 | 5 | 73.1 | 77.4 | 78.0 | 59.1 | 55.4 | 68.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.37 | 10 | 70.6 | 74.8 | 76.9 | 56.8 | 54.9 | 66.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.06 | 15 | 66.1 | 65.3 | 72.9 | 50.7 | 42.0 | 59.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLama-13B | 3.75 | 5 | 71.4 | 77.5 | 78.5 | 59.0 | 49.3 | 67.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.50 | 10 | 71.8 | 76.4 | 77.7 | 58.2 | 48.2 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.25 | 15 | 72.6 | 73.7 | 75.7 | 56.7 | 47.3 | 65.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Qwen-2-7B | 3.64 | 5 | 62.0 | 67.2 | 77.0 | 52.0 | 56.6 | 63.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.28 | 10 | 56.7 | 53.1 | 71.4 | 45.5 | 29.4 | 51.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2.92 | 15 | 53.4 | 45.0 | 66.4 | 42.0 | 25.0 | 46.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Z-score Ordering | LLama-7B | 3.68 | 5 | 67.2 | 71.1 | 75.9 | 54.9 | 38.1
    | 61.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.37 | 10 | 68.0 | 66.9 | 73.1 | 52.1 | 33.7 | 58.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.06 | 15 | 62.9 | 56.7 | 67.8 | 46.2 | 28.5 | 52.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mistral-7B | 3.68 | 5 | 72.9 | 77.7 | 78.8 | 59.2 | 55.1 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.37 | 10 | 69.1 | 72.1 | 74.8 | 53.2 | 38.9 | 61.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.06 | 15 | 62.0 | 52.7 | 65.6 | 39.5 | 25.1 | 49.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLama-13B | 3.75 | 5 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.50 | 10 | 71.3 | 75.0 | 76.4 | 57.4 | 48.2 | 65.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.25 | 15 | 72.3 | 73.5 | 76.2 | 56.4 | 45.7 | 64.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Qwen-2-7B | 3.64 | 5 | 69.1 | 71.1 | 75.2 | 54.0 | 64.3 | 66.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3.28 | 10 | 65.6 | 65.2 | 71.6 | 48.2 | 47.5 | 59.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2.92 | 15 | 54.8 | 50.0 | 66.7 | 42.8 | 30.4 | 49.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison of different models and their performance across various
    tasks with GPT-Q quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 GPT-Q
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GPT-Q is a post-training quantization technique specifically designed for GPT
    models Frantar et al. ([2023](#bib.bib8)). Utilizing a dataset, it calculates
    the necessary scaling factors for quantization. After training, GPT-Q assesses
    the distribution of weights using this dataset to determine optimal scaling factors
    for converting floating-point representations to lower-bit formats such as int8
    or int4\. We only have several data points for GPT-Q and it adds significant execution
    time overhead to our experiments. Both Quanto and GPT-Q are available in HuggingFace
    library Wolf et al. ([2019](#bib.bib32)). We modified it for our use-case in the
    same way as described for Quanto.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Models and Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We studied quantization on 5 LLMs from different model families and different
    sizes – LLaMa-2-7b Touvron et al. ([2023](#bib.bib29)), LLaMa-2-13B, Mistral-7b
    Jiang et al. ([2023](#bib.bib13)), QWEN-1.8b, and QWEN-7b Bai et al. ([2023](#bib.bib1)).
    We evaluated these LLMs on 8 A100 GPUs with a batch size of 1 for inference using
    LLM harness library Gao et al. ([2023](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evaluation Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We select five diverse NLP tasks for evaluating the quantization effects: Winogrande
    Sakaguchi et al. ([2021](#bib.bib22)), ARC-easy Clark et al. ([2018](#bib.bib4)),
    PIQA Bisk et al. ([2020](#bib.bib3)), HellaSwag Zellers et al. ([2019](#bib.bib37)),
    and MMLU Hendrycks et al. ([2020](#bib.bib12)). We also evaluate our approach
    on two generation datasets to cover diverse tasks of reasoning and answer generation:
    GSM8K Cobbe et al. ([2021](#bib.bib5)), which contains math questions, and the
    Natural Questions (open) dataset Kwiatkowski et al. ([2019](#bib.bib16)), which
    consists of open-domain answer generation task.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion of Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our main results are shown in [Figure 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels") and [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    (please also see [Figure 8](#A1.F8 "In A.1 Quantizing Layers Using 3 Levels ‣
    Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels"), [9](#A1.F9 "Figure 9 ‣ A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"), and [10](#A1.F10
    "Figure 10 ‣ A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels") in Appendix). In both these figures, we ranked the layers in the
    respective LLMs in descending order of their importance score. The left most point
    in the plots indicates that all layers of Mistral-7b and LLaMa2-7B are quantized
    in 4-bits; as we move to the right on x-axis, we quantize the next least important
    layer to 2 bits. For example, the overall bit size of 3.75 mentioned on the x-axis
    represents 28 (most important) layers in 4-bit and 4 (least important) layers
    in 2-bit. The horizontal red dotted lines indicate the performance of the entire
    model represented in 8bit precision; as shown, this performs very similarly to
    the full model in 4 bit precision (the top left most point in [Figure 2](#S1.F2
    "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") and [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels")). Because the gap between the full model in 8 bit
    vs. 4 bit precision was less than 1% across the majority of the datasets, we focus
    mostly on quantizing below 4-bit precision in our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 to 2 bit 8 to 4 bit Models Layers GSM8K NQ_open GSM8K NQ_open 2 bits F1 F1
    LIM Ordering LLama-Ins 5 7.5 15.0 10.5 36.6 10 1.5 6.7 13.5 37.9 15 0.5 3.1 11.0
    35.4 Mistral-Ins 5 24.5 16.2 34.5 29.6 10 20.0 9.2 36.5 29.8 15 4.5 5.1 34.0 27.7
    Z Ordering LLama-Ins 5 6.0 11.7 12.5 37.3 10 3.5 5.3 12.5 36.9 15 1.0 1.8 10.0
    34.7 Mistral-Ins 5 25.0 15.4 37.0 28.5 10 10.5 8.7 34.5 30.1 15 1.0 2.2 34.0 29.3
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance comparison of LLaMa-instruct-7b and Mistral-instruct-7b
    across two generation tasks - GSM8K and Natural Questions open split.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We draw the following observations from our experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Variable quantization is useful: The first key finding of our work is that
    a fixed quantization technique can be extended to a variable number of bits by
    quantizing different layers at different bits according to their importance. This
    allows LLMs to be fit in the exact memory requirement of the user while retaining
    more of the original performance. Overall, our method of layer-wise quantization,
    guided by layer importance, proves to be an efficient strategy for attaining adaptable
    precision bit levels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Layer importance scoring is crucial: In the figures we compare layer ranking
    using our LIM and ZD importance scores with ranking using a reverse LIM and random
    ordering. As seen in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    and [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels") the quantization
    of least important layers from 4-bit to 2-bit as per the LIM score ranking shows
    strong performance retention. In contrast, quantizing based on the reverse of
    LIM score shows much worse performance when most important layers are quantized
    to 2-bit, highlighting the strength of meaningful layer ranking. Further, LIM
    and ZD ranking performs substantially better than random ordering of layers baseline
    where we quantize layers to lower bits randomly. Lastly, LIM performs better than
    ZD, but the differences are not large. We consider this a success for ZD, which
    is simpler and does not require calibration data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Layer-wise Quantization is useful until 3.0-3.25 bits: As shown in [fig. 2](#S1.F2
    "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") and [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels"), our first key observation is that quantization to
    8 bit barely affects performance (red vs. purple line in each plot). While there
    is marginal drop in performance when models are quantized to 4 bits, we observed
    really noticeable drops only after the models are dropped below 3.0-3.25 bits
    on average using Quanto. For example, the bit size for which performance drops
    below 90% on Winogrande for Mistral-7b, LLaMa-7b, QWEN-7b, and LLaMa2-13b are
    3.2, 3.1, 3.85, and 2.85 respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a) Quantizing from 8 to 4 bits against pruning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d96745458d1ad0a760637b7d3304c63.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) Quantizing from 4 to 2 bits against pruning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ca2cbea53bcc4b3e6c960c2815595e3.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4: We compare quantization against pruning as a method to reduce the
    memory requirement of a model (LLaMa-2-7b here). One increment means two layers
    moved to lower quantization for the blue line (quantization), and one layer removed
    for the red and orange lines (pruning), thus reducing the same amount of memory.
    We show the average accuracy over MMLU, Winogrande, PIQA, and Hellaswag.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8bf6f6a141e8b255b93ed2c3b4ffa1f1.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: Comparison of LLaMa2-7b quantized between 8 and 4 bits with LLaMa2-13b
    quantized between 4 and 2 bits to check when the performance intersects'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantization is more useful for larger LLMs: Variable quantization of larger
    models ([fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")) using our
    importance score shows much better retention of performance with lower quantization
    bit precision compared to moderately-sized LLMs such as LLaMa-7B ([fig. 8](#A1.F8
    "In A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    in appendix) and Mistral-7B ([fig. 3](#S1.F3 "In 1 Introduction ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")).
    Further, as we go down to even smaller LLMs such as QWEN-1.8B (see [fig. 10](#A1.F10
    "In A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    in Appendix) with only 20 layers, we observed layer importance ranking to be not
    as effective. This observation aligns with many other previous works that have
    shown quantization to be substantially more effective for larger LLMs when compared
    to their smaller counterparts Jin et al. ([2024](#bib.bib15)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our method is applicable to different quantization techniques: Tables [1](#S3.T1
    "Table 1 ‣ 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels") and [2](#S3.T2 "Table 2 ‣ 3.2.1 Quanto ‣ 3.2 Quantization Techniques
    ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing
    LLMs Beyond Integer Bit-Levels") summarize the overall results when quantizing
    individual layers with Quanto and GPT-Q, respectively. The tables show that our
    method can be coupled with any other quantization techniques. On average, GPT-Q
    leads to an average of 4% better accuracy than Quanto across all 5 tasks. Additionally,
    GPT-Q enables models such as LLaMa2-13B to be quantized down from 4 bits to 3.25
    bits with less than a 3% loss in average accuracy, as seen in Table [2](#S3.T2
    "Table 2 ‣ 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels"). Importantly, similar to Quanto and GPT-Q, our methodology of layer-wise
    quantization can be extended to any quantization technique.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (6)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Effect on generation tasks: — We also evaluate our approach of variable quantization
    of different layers on generation tasks. As shown in [table 3](#S5.T3 "In 5 Discussion
    of Results ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing
    LLMs Beyond Integer Bit-Levels"), we observe substantial drop in performance on
    both GSM8K and NQ_open generation tasks when quantizing more layers in 2 bits.
    Importantly, the performance drop in these generation tasks is more drastic when
    compared to the average performance drop in classification tasks ([table 1](#S3.T1
    "In 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")),
    emphasizing the need for more dedicated research in quantization for generation
    tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present several analyses to further spotlight on benefits from variable layer-wise
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Pruning vs. Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare our variable quantization against variable pruning (using the same
    layer importance ranking) as an alternative for the same goal of reducing model
    memory requirement. In [Figure 4(a)](#S5.F4.sf1 "In Figure 4 ‣ Item (3) ‣ 5 Discussion
    of Results ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing
    LLMs Beyond Integer Bit-Levels"), we show that for not extreme quantization levels,
    it is significantly better to move layers into lower quantization levels instead
    of removing them. For example, when 2 least important layers are removed resulting
    in remaining 30 layers (each in 8 bit) of LLaMa2-7b, the average performance drops
    to 62.7% (shown by red line denoting work by Gromov et al. ([2024](#bib.bib10))).
    But on the quantization counterpart with same memory i.e., when 4 layers are quantized
    to 4 bit and remaining 28 layers are in 8 bit (shown by blue line), performance
    remains intact close to 66.8% as shown in [fig. 4(a)](#S5.F4.sf1 "In Figure 4
    ‣ Item (3) ‣ 5 Discussion of Results ‣ Layer-Wise Quantization: A Pragmatic and
    Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). When 12 layers
    are removed, the performance drops around 53% on average while having 8 layers
    in 8 bits and 24 layers quantized to 4 bits (shown by blue curve) to maintain
    the same size, average performance still remains intact and close to 66%. This
    highlights the important finding that quantization until 4 bits overall is a substantially
    more effective strategy compared to pruning for model compression.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in case of extreme levels of quantization (i.e., $<4-bits$,
    pruning maybe the more effective strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Quantizing Larger vs. Smaller LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We further evaluate and compare feasibility of quantizing larger LLMs more drastically
    (i.e., $<4bits$) shows better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced a simple, flexible quantization approach that quantizes different
    layers at different bits based on their importance. We presented two layer importance
    scoring techniques which when used to select more important layers for quantizing
    them in 4 bits and less important layers in 2 bits lead to strong performance
    retention across several LLMs even until 2.85 overall bit size. Our work presents
    several key practical findings such as layer-wise quantization is more effective
    for larger LLMs (with more number of layers), in the same memory setting; quantization
    is better than pruning until a certain bit precision level, text generation tasks
    are affected more with quantization, etc. Overall, our work introduces layer-wise
    quantization and presents detailed empirical findings for motivating future research
    in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Limitations of our work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our experiments and results have focused more on quantization to lower bits
    i.e., $<4bits$ 1% on average) in performance between 8-bit and 4-bit quantized
    model (please see [table 4](#A1.T4 "In A.1 Quantizing Layers Using 3 Levels ‣
    Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") in appendix). We did not observe
    meaningful changes in performance from layer-wise quantization between 8-bit and
    4-bit because of such minute difference between their performances. Our study
    is limited to two level quantization i.e., more important layers in 4 bits and
    less important layers in 2 bits. One can also potentially apply three level of
    quantization i.e., most important layers in 8 bits, moderately important in 4
    bits and least important in 2 bits. In our experiments, we observed three level
    quantization to be always worse than two level quantization for similar memory
    sizes (please see [section A.1](#A1.SS1 "A.1 Quantizing Layers Using 3 Levels
    ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") in Appendix). .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have proposed LIM and ZD scores for rating importance of each layer in LLM.
    These scores are very simple to implement, and only LIM needs a calibration corpus
    which is also an (readily available) unlabelled corpus. As future works, there
    can be more sophisticated measures to find importance of layers. For example,
    supervised methods that can show layer-wise impact on specific annotated NLP tasks
    Zhu et al. ([2023a](#bib.bib39)), in depth model interpretability approaches Singh
    et al. ([2024](#bib.bib25)); Sun et al. ([2023](#bib.bib26)), analyses of block
    of layers and their interactionsYang et al. ([2024](#bib.bib34)), etc. In this
    work, we limited and focused our study on LIM and ZD score to emphasize more on
    the main contribution of the paper - quantizing different layers at different
    bits as per layer importance. To the best of our knowledge, our work is the first
    to introduce variable quantization layer wise as per their importance and our
    study can be easily extended with more layer importance measuring scores in future
    work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We presented comparison between pruning and quantization in [section 6.1](#S6.SS1
    "6.1 Pruning vs. Quantization ‣ 6 Analyses ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). A potential
    setup can also be combining pruning and quantization by removing few of the least
    important layers, quantizing remaining less important layers in lower 2 bits and
    keeping more important layers in 8 bits or above. Our work is primarily focused
    on introducing the idea of achieving variable quantization by quantizing different
    layers as per their importance. We leave exploration of combining quantization
    and pruning for future works.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9 Ethical Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study is focused on post training quantization (PTQ) of LLMs. We have not
    finetuned LLMs for any specific task or data and have selected well established
    LLMs such as LLaMa, Mistral, and QWEN in our experiments. Our study also uses
    widely used evaluation datsets such as MMLU, HellaSwag, ARC easy, etc. that contain
    safe test cases. Thus, we believe our experiments do not contain any harmful cases
    and to the best of our knowledge, we did not observe any unsafe outputs in our
    evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. *arXiv preprint arXiv:2309.16609*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banner et al. (2019) Ron Banner, Yury Nahshan, and Daniel Soudry. 2019. Post
    training 4-bit quantization of convolutional networks for rapid-deployment. *Advances
    in Neural Information Processing Systems*, 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. The
    case for 4-bit precision: k-bit inference scaling laws. In *International Conference
    on Machine Learning*, pages 7750–7774\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot. In *International
    Conference on Machine Learning*, pages 10323–10337\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. In *The Eleventh International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gromov et al. (2024) Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo
    Glorioso, and Daniel A Roberts. 2024. The unreasonable ineffectiveness of the
    deeper layers. *arXiv preprint arXiv:2403.17887*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Minillm:
    Knowledge distillation of large language models. In *The Twelfth International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2024) Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan,
    Bin Wang, and Deyi Xiong. 2024. A comprehensive evaluation of quantization strategies
    for large language models. *arXiv preprint arXiv:2402.16775*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question
    answering research. *Transactions of the Association for Computational Linguistics*,
    7:453–466.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024.
    Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
    *Proceedings of Machine Learning and Systems*, 6:87–100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner:
    On the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Men et al. (2024) Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin,
    Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language
    models are more redundant than you expect. *arXiv preprint arXiv:2403.03853*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier,
    and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence
    modelling. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based
    ultra low precision quantization of bert. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 34, pages 8815–8821.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simoulin and Crabbé (2021) Antoine Simoulin and Benoit Crabbé. 2021. How many
    layers and why? an analysis of the model depth in transformers. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing: Student
    Research Workshop*, pages 221–228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large
    language models. *arXiv preprint arXiv:2402.01761*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. In *The Twelfth
    International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tai et al. (2024) Yu-Shan Tai et al. 2024. Mptq-vit: Mixed-precisionpost-trainingquantizationforvisiontransformer.
    *arXiv preprint arXiv:2401.14895*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan et al. (2023) Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan
    Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. 2023. Efficient
    large language models: A survey. *arXiv preprint arXiv:2312.03863*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024) Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: Large
    language model pruning via layer collapse. *arXiv preprint arXiv:2402.11187*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2024) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong
    He. 2024. Exploring post-training quantization in llms from comprehensive study
    to low rank compensation. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 38, pages 19377–19385.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023a) Ligeng Zhu, Lanxiang Hu, Ji Lin, and Song Han. 2023a. Lift:
    Efficient layer-wise fine-tuning for large model models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2023b) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023b.
    A survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f448b05f1d9124578ec1976f5adda595.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: We first rank the layers in an LLM (e.g., LLaMa2-13B) using an importance
    score (shown here is ranking based on our Layer Input Modification (LIM) score,
    see [section 3.1](#S3.SS1 "3.1 Layer Importance Scores ‣ 3 Method ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels")).'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Quantizing Layers Using 3 Levels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In all of our experiments, we have focused on two level quantization, i.e.,
    either less important layers are quantized in 2 bits and more important layers
    in 4 bits or less important in 4 bits and more important in 8 bits. As a plausible
    variant, LLM layers can also be easily quantized using three levels, i.e., least
    important layers in 2 bits, moderately important in 4 bits, and the most important
    ones in 8 bits. In our study, we observed three level quantization almost always
    performs worse than two level quantization. We show three level quantization of
    LLaMa2-7b with fixed overall model bit size of 4bit. We first quantize all the
    32 layers in 4 bits as shown by the leftmost bar of [section A.1](#A1.SS1 "A.1
    Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").
    We then convert two least important layers to 2 bits each and one most important
    layer to 8 bit, thus maintaining the overall bit size of the model to 4 bit. This
    is represented by second bar from the left in [fig. 7](#A1.F7 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). We repeat
    the same process of converting two more layers in 2bits and a more important layer
    to 8bit represented by the consecutive bars in [fig. 7](#A1.F7 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). As observed,
    three level quantization always performs worse than one level quantization when
    the target bit-level is the same, thus we don’t propose the technique as a way
    to achieve better performance for a set quantization level, but as a way to achieve
    a variable level of quantization while retaining maximum performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e845cdb555a83ad7203017091e4b9e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: We compare different ways to achieve 4-bit quantization using three
    quantization levels. Each bar going from left to right represents adding one important
    layer in 8 bits and moving two less important layers to 2 bits, thus keeping an
    average of 4-bit quantization for all of the bars. Each bar having a value x on
    the x-axis represents the most important x layers in 8-bits, the least important
    2*x in 2-bits and the rest in 4-bits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c86635f436316f9d630a13dc43e2c489.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Similarly to the other bit plots the graphs showcase the accuracy
    on four distinct data sets when quantizing LLaMa2-7b from full 4-bit quantization
    to 2-bit by moving less important layers in 2-bit quantization'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19b0a71d47a001e8b6fd3b442146a90d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Qwen2 quantized with quanto between 4 and 2 bits. All notations are
    same as in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37b3363cb069ffbdaf1a5cfb8336229d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Qwen1.5-1.8b quantized with quanto between 4 and 2 bits. All notations
    are same as in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A
    Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").'
  prefs: []
  type: TYPE_NORMAL
- en: Models Layers WNGD ARC PIQA HLSWG MMLU Average low-bits Accuracy BI Ordering
    LLaMa2-7B 5 69.06 75.88 77.69 57.05 41.28 64.2 10 68.82 76.3 77.42 57.11 41.96
    64.3 15 68.82 76.3 77.42 57.11 41.96 64.3 Mistral-7B 5 73.87 81.01 80.9 61.13
    58.56 71.1 10 73.95 80.3 80.95 61.28 58.39 71.0 15 73.79 80.42 80.79 61.17 58.14
    70.9 LLaMa2-13B 5 72.29 79.33 79.16 60.15 50.49 68.3 10 71.74 79.08 79.32 59.96
    50.53 68.1 15 71.74 79.37 79.32 59.96 50.55 68.2 Qwen-2-7B 5 70.71 79.2 80.03
    58.66 68.06 71.3 10 70.95 79.2 79.97 58.44 67.68 71.2 15 68.82 78.32 79.81 58.26
    67.66 70.6 Z Ordering LLaMa2-7B 5 68.82 76.13 77.91 57.02 40.85 64.1 10 68.66
    75.92 77.63 57.09 40.6 64.0 15 68.27 75.54 77.42 57.09 39.7 63.6 Mistral-7B 5
    73.87 80.76 80.9 61.24 58.62 71.1 10 74.19 80.47 81.12 61.03 58.2 71.0 15 74.42
    80.47 80.84 60.89 58.31 71.0 LLaMa2-13B 5 72.29 79.54 78.94 60 50.54 68.3 10 72.21
    79.58 79.16 59.99 50.51 68.3 15 72.05 79.46 79.37 59.98 50.59 68.3 Qwen-2-7B 5
    71.58 79.2 79.76 59.2 69.44 71.8 10 71.5 79.08 80.35 58.82 69.01 71.8 15 71.58
    78.61 79.92 58.54 68.65 71.5
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Accuracy results of different models across various tasks for 8bit
    and 4bit quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 LLaMa2 and QWEN plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We show 4 bit to 2 bit variable layer-wise quantization for LLaMa2-7b and QWEN-7B
    in [fig. 8](#A1.F8 "In A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels") and [fig. 9](#A1.F9 "In A.1 Quantizing Layers Using
    3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels") respectively. All notations
    are same as in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A
    Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").
    These curves were also generated on 2K evaluation instances from each of the datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Commonality between layer importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, we are the first one to propose layer importance
    and utilize the importance order to quantize different layers at different bits.
    in [Figure 6](#A1.F6 "In Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"), we show
    the intensity bars below for each layer based on their importance for four different
    LLMs with different number of layers and sizes. As observed, there is a substantial
    pattern overlap of least important layers across multiple LLMs. We observe that
    first and the last layer are the most two important layers. Many of the important
    layers also tend to be the initial few set of layers. Lesser important layers
    (shown by block of layers with faded intensity) tend to be towards halfway of
    middle and end of the network. These observations suggest generalized patterns
    in layer importance across LLMs and pre-computed layer importance orders can be
    roughly utilized to quantize a wide variety of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that (unpublished) concurrent works like Gromov et al. ([2024](#bib.bib10))
    have presented an empirical finding that layers towards the end of the model can
    be removed except the last layer. The reverse order of layers indexes surprisingly
    has overlap with the importance order calculated with our LIM score but we believe
    our LIM score is more broadly applicable to any LLM where even the layers towards
    the end can be more important.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Results of 8bit to 4bit quantization on different datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We show results of quantizing models lower than 8 bit. Following our proposed
    methodology, the same techinque can be applied to have the more important layers
    in 8 bits and the least important ones in 4 bits. While this does still increase
    the performance that can be fit within a memory requirement, the results are not
    as major as the ones for the 4-2 bit range, thus we mainly focus on that range.
  prefs: []
  type: TYPE_NORMAL
