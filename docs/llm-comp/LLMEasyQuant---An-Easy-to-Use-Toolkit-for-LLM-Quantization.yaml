- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LLMEasyQuant - An Easy to Use Toolkit for LLM Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19657](https://ar5iv.labs.arxiv.org/html/2406.19657)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dong Liu¹ Meng Jiang² Kaiser Pister¹ dliu328@wisc.edu mjiang2@nd.edu kaiser@cs.wisc.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Currently, there are many quantization methods appeared for LLM quantization,
    yet few are user-friendly and easy to be deployed locally. Packages like TensorRT[[NVI24](#bib.bibx4)]
    and Quanto[[Fac24](#bib.bibx1)] have many underlying structures and self-invoking
    internal functions, which are not conducive to developers’ personalized development
    and learning for deployment. Therefore, we develop LLMEasyQuant, it is a package
    aiming to for easy quantization deployment which is user-friendly and suitable
    for beginners’ learning.
  prefs: []
  type: TYPE_NORMAL
- en: The code of LLMEasyQuant can be found at [code link](https://github.com/NoakLiu/LLMEasyQuant)¹¹1[https://github.com/NoakLiu/LLMEasyQuant](https://github.com/NoakLiu/LLMEasyQuant).
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Wisconsin-Madison,
  prefs: []
  type: TYPE_NORMAL
- en: ²University of Notre Dame
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization is the process of mapping a large set of input values to a smaller
    set of output values, often integers. It is a key technique in digital signal
    processing where continuous signals are mapped to discrete digital values, and
    it reduces the data’s precision to make storage and computation more efficient
    while attempting to retain essential information.
  prefs: []
  type: TYPE_NORMAL
- en: With the development of Large Language Models (LLMs), the models have grown
    extremely large, so the memory usage and inference speed are greatly limited by
    the size of the model. Consequently, as one of the most popular technique for
    model compression, quantization has many variants now used for LLM compression
    and inference acceleration. The goal of quantization in LLMs is to reduce their
    size while minimizing its influence on inference speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantization process can be described by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(x)=\text{clamp}\left(\left\lfloor\frac{x-\text{min}(X)}{\Delta}+0.5\right\rfloor+z,-128,127\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $x$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\Delta$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $z$ or calculated to shift the scale,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clamp function ensures values are kept within the $[-128,127]$ range.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMEasyQuant is a toolkit designed to simplify the process of deploying quantization
    on large language models (LLMs). Its functions are included as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'absmax: $\text{scale}=\frac{127}{\max(|X|)}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'zeropoint: Shifting the tensor values based on a computed zero-point'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'symquant: Symmetric quantization, a quantization method for scaling based on
    the absolute maximum value.[[FFBL18](#bib.bibx2)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'smoothquant: A quantization method to achieve efficient post-training quantizatio
    for LLMs.[[XLS^+24](#bib.bibx6)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'zeroquant: Adjust the numeric range of input data so that zero values in the
    original data can be represented exactly in the quantized format. [[YAZ^+22](#bib.bibx7)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'simquant: A quantization technique by KV Cache Quant. [[HKM^+24](#bib.bibx3)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3 Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3.1 ZeroQuant: Zero-point Quantization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 Definition and Concept
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zero-point quantization, or ZeroQuant [[YAZ^+22](#bib.bibx7)], focuses on adjusting
    the numeric range of data so that zero values are represented exactly by zero
    in the quantized format. This technique is particularly useful in optimizing quantization
    schemes where the preservation of zero values is crucial, such as in sparse datasets
    used in machine learning and signal processing.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Mathematical Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The quantization process using ZeroQuant can be described by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(x)=\text{clamp}\left(\left\lfloor\frac{x-\text{min}(X)}{\Delta}+z\right\rfloor,-128,127\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $x$ is the value to be quantized,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\text{min}(X)$ is the minimum value in the dataset,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\Delta$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $z$ maps to 0,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is clamped to the range $[-128,127]$, typical of 8-bit signed integers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Symmetric 8-bit Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1 Concept
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Symmetric 8-bit quantization [[FFBL18](#bib.bibx2)] involves mapping both positive
    and negative values of a dataset uniformly around zero, optimizing the quantization
    process for symmetric data distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Mathematical Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The symmetric quantization can be described with:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(x)=\text{clamp}\left(\text{round}\left(\frac{x}{s}\right),-128,127\right),\quad
    s=\frac{\text{max}(&#124;X&#124;)}{127.5}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $s$ is the scale factor calculated based on the maximum absolute value
    of the data, ensuring all quantized values fall within the 8-bit integer range
    of -128 to 127.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Layer-by-Layer Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.3.1 Concept
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Layer-by-layer quantization [[TOW^+23](#bib.bibx5)] is a technique applied in
    deep learning where each layer of a neural network is quantized independently
    to maintain accuracy while reducing the overall model size and computational demand.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Quantization Procedure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The process involves:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the quantization parameters for each layer separately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting each layer’s weights and biases according to the quantization rules
    defined for symmetric or asymmetric quantization methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally recalibrating layer parameters during or after the quantization to
    optimize performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3.3 Implementation Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is an example of applying layer-by-layer quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 3.4 Symmetric 8-bit and Layer-by-Layer Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.4.1 Layer-by-Layer ZeroQuant Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Applies quantization per layer in a model:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode input and compute unquantized outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each layer $i$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Freeze all but layer $i$.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update $i$-th layer’s parameters by quantization.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute loss and update using:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{loss}=\text{MSE}(\text{teacher\_outputs}[i],\text{quantized\_outputs}[i])$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize with gradient descent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.5 SimQuant: A Novel Approach by KV Cache Quant'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.5.1 Definition and Concept
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SimQuant [[HKM^+24](#bib.bibx3)], developed by KV Cache Quant, represents a
    groundbreaking advance in the field of data quantization. This technique is designed
    to optimize the efficiency of data representation while maintaining high accuracy,
    particularly in environments where computational resources and storage are limited.
    Unlike traditional quantization methods that apply fixed parameters across various
    datasets, SimQuant introduces a dynamic adjustment mechanism that tailors the
    quantization process to the specific statistical properties of the dataset in
    use.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Mathematical Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The core of the SimQuant technique is based on the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(x)=\text{round}\left(\frac{x-\min(X)}{\Delta}\right)+Z$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $x$ is the data point being quantized,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\min(X)$ is the minimum value in the dataset,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\Delta$ represents the quantization interval, which is adaptively calculated,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $Z$ is the zero-point adjustment to center the quantization range.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.5.3 Algorithmic Steps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Analysis: Initially, SimQuant analyzes the statistical distribution of
    the dataset to determine optimal quantization parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameter Adjustment: It then adjusts $\Delta$ dynamically during the quantization
    process to minimize information loss.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantization Application: Finally, the data is quantized using the calculated
    parameters, ensuring that the most critical information is retained with minimal
    resource usage.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.5.4 Quantization Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate range values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\text{vals}_{\text{min}}=\min(X_{\text{channel}}),\quad\text{vals}_{\text{max}}=\max(X_{\text{channel}})$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute scale $s$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $s=\frac{2^{\text{bits}}-1}{\text{vals}_{\text{max}}-\text{vals}_{\text{min}}},\quad
    z=-\text{vals}_{\text{min}}\cdot s$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply quantization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $X_{\text{quant}}=\text{clamp}(\lfloor X\cdot s+z+0.5\rfloor,0,2^{\text{bits}}-1)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 3.5.5 Dequantization Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '|  | $X_{\text{dequant}}=\frac{X_{\text{quant}}-z}{s}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 3.5.6 Pseudocode for SimQuant
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Algorithm 1 KVQuant Quantization
  prefs: []
  type: TYPE_NORMAL
- en: 1:function QuantizeKVCache(Key, Value, scaling_factor, zero_point)2:     for each
    layer $l$7:end function
  prefs: []
  type: TYPE_NORMAL
- en: '3.6 SmoothQuant: Smoothing Quantization Process'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.6.1 Definition and Concept
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SmoothQuant [[XLS^+24](#bib.bibx6)] is a post-training quantization (PTQ) method
    designed to facilitate accurate and efficient quantization of large language models
    (LLMs). It aims to achieve 8-bit quantization for both weights and activations
    without retraining, ensuring minimal loss in model accuracy. SmoothQuant addresses
    the challenge of activation quantization by redistributing the quantization difficulty
    from activations to weights, which are inherently easier to quantize.
  prefs: []
  type: TYPE_NORMAL
- en: The key concept behind SmoothQuant is the smoothing of activation outliers,
    which typically hinder effective quantization. By migrating the quantization difficulty
    from activations to weights, SmoothQuant ensures that both can be quantized to
    8-bit integers (INT8) effectively. This migration is achieved through a mathematically
    equivalent transformation that adjusts the scale of activations and weights.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Mathematical Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The quantization and smoothing process of the ‘SmoothQuantMatrix‘ can be mathematically
    described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{smoothed\_X}=X\cdot s,\quad\text{dequantized\_X}=\frac{\text{smoothed\_X}}{s}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the scale factor $s$ is calculated using the activity scales act_scales
    and the weight scales of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s=\left(\frac{\text{act\_scales}^{\alpha}}{\text{weight\_scales}^{1-\alpha}}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\alpha$ is a parameter that determines the balance between the activity
    and the weight scales, affecting the smoothness of the quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.3 SmoothQuant Methodology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SmoothQuant smooths activation outliers by migrating quantization difficulty
    from activations to weights. The following pseudocode outlines the main steps
    of the SmoothQuant process.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 SmoothQuant Process
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Activation tensor $X$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.4 Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ‘SmoothQuantMatrix‘ class computes the smoothing scales based on the provided
    activity scales and the inherent data characteristics, dynamically adjusting each
    feature’s scale. This customization allows the quantization process to be more
    adaptive and sensitive to the underlying data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 4 result
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/338681f2ebfc73eb30a4f4794f8e125b.png)![Refer to caption](img/1ea71519c9b53ccb88f848c093afd2cf.png)![Refer
    to caption](img/1ef9481844d88d567594f174640c8f2a.png)![Refer to caption](img/28e4e803c5fadaa145ef1bd728d6fff7.png)![Refer
    to caption](img/f72f183b613beb9d079eeab45e89d0f4.png)![Refer to caption](img/da7c03e748f25f82bfb3723e128a3df1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Quantized Weights Distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84666aacf31790d82254b97e73ed40ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Performance Comparison after Quantization on GPT'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Perplexity (ppl) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | 4.01 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 INT8 | 6.83 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 AbsMax Quantize | 9.32 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 ZeroPoint Quantize | 8.93 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Smooth Quant Apply | 6.31 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Sim Quantize | 7.16 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Sym Quantize 8bit | 7.01 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 Sym Quantize 8bit ZeroQuant Func | 7.37 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Perplexity Analysis of Quantization Models'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Simplification of Quantization Processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMEasyQuant provides a user-friendly interface that simplifies the application
    of quantization techniques, making it accessible to both novices and experienced
    users without requiring deep technical knowledge of the underlying algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Customization and Flexibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite its simplicity, the package offers extensive customization options that
    allow users to tailor the quantization process to their specific needs, balancing
    efficiency and performance according to the model’s deployment context.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Efficiency in Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimized for performance, LLMEasyQuant helps reduce the computational load
    and memory usage of models, facilitating their deployment on devices with limited
    resources such as mobile phones and embedded systems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Fac24] Hugging Face. Optimum-quanto. [https://github.com/huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FFBL18] Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip H. W.
    Leong. Syq: Learning symmetric quantization for efficient deep neural networks,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HKM^+24] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
    Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million
    context length llm inference with kv cache quantization, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[NVI24] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TOW^+23] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Yaowei Wang, Wen Ji,
    and Wenwu Zhu. Mixed-precision neural network quantization via learned layer-wise
    importance, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XLS^+24] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and
    Song Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[YAZ^+22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
