- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression
    Perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14124](https://ar5iv.labs.arxiv.org/html/2406.14124)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Minsang Kim
  prefs: []
  type: TYPE_NORMAL
- en: Korea University
  prefs: []
  type: TYPE_NORMAL
- en: Dept. of Computer Science and Engr.
  prefs: []
  type: TYPE_NORMAL
- en: South Korea
  prefs: []
  type: TYPE_NORMAL
- en: kmswin1@korea.ac.kr
  prefs: []
  type: TYPE_NORMAL
- en: '&Seungjun Baek'
  prefs: []
  type: TYPE_NORMAL
- en: Korea University
  prefs: []
  type: TYPE_NORMAL
- en: Dept. of Computer Science and Engr.
  prefs: []
  type: TYPE_NORMAL
- en: South Korea
  prefs: []
  type: TYPE_NORMAL
- en: sjbaek@korea.ac.kr
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Compute-efficient training of large language models (LLMs) has become an important
    research problem. In this work, we consider data pruning as a method of data-efficient
    training of LLMs, where we take a data compression view on data pruning. We argue
    that the amount of information of a sample, or the achievable compression on its
    description length, represents its sample importance. The key idea is that, less
    informative samples are likely to contain redundant information, and thus should
    be pruned first. We leverage log-likelihood function of trained models as a surrogate
    to measure information content of samples. Experiments reveal a surprising insight
    that information-based pruning can enhance the generalization capability of the
    model, improves upon language modeling and downstream tasks as compared to the
    model trained on the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction & Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training large language models (LLM) requires high computational costs due to
    their immense sizes and commensurately large datasets Kaplan et al. [[2020](#bib.bib1)],
    Hoffmann et al. [[2022](#bib.bib2)]. Recently, computationally efficient methods
    to train deep models based on *data pruning* have gained interest. Data pruning
    concerns deciding which samples are important for training, and remove unimportant
    samples from training datasets. Sorscher et al. [[2022](#bib.bib3)] proposed that
    neural scaling law Kaplan et al. [[2020](#bib.bib1)] may be overcome with data
    pruning. Their method uses distances between samples and their cluster centroids
    in the embedding space to measure the sample importance. Tan et al. [[2024](#bib.bib4)]
    proposed to assess the sample importance by efficiently measuring the change in
    empirical risk when the sample is removed from the training set. The aforementioned
    works apply data pruning to labeled data for supervised learning or focus on visual
    understanding tasks. By contrast, data pruning methods for LLMs have been largely
    underexplored. Instead, text deduplication methods have been proposed  Raffel
    et al. [[2020](#bib.bib5)], Abbas et al. [[2023](#bib.bib6)] to remove redundant
    data based on semantic similarity. However, they do not quantify the importance
    of samples for data pruning.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, connections between LLMs and data compression have been recently
    explored. Ge et al. [[2023](#bib.bib7)] proposed In-Context Autoencoder for context
    compression. Deletang et al. [[2023](#bib.bib8)] extensively study the view of
    LLM as a model for data compression. The predictive power of LLMs can be used
    for an optimally efficient expression of data based on the prediction probability,
    so as to compress various types of multi-modal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose to leverage the compression capability of LLMs for
    data pruning. If a model learns the distribution of corpus, it can estimate the
    amount of information of a sample, i.e., the optimal number of bits required to
    compress the sample. Importantly, if the amount of information of a sample is
    low, the sample likely contains information which is redundant or appear frequently
    in dataset. Less informative samples are regarded as less important and thus are
    removed from dataset. To estimate the information of samples, we first train called
    *data probe model* which is a small model trained with a subset of corpus. We
    use the log-likelihood output of data probe model as a surrogate to measure the
    sample information. Next, we prune the dataset based on the estimates by data
    probe model, and train a target model with the pruned dataset. Experimental results
    are quite surprising: in many cases, the proposed pruning actually *improves*
    the model performance. In some cases, the performance of generation and/or downstream
    tasks are maintained up to pruning 50% of the pretraining corpus. The key insight
    is that, the proposed pruning based on sample information helps removing irrelevant
    data, which promotes the generalization capability of language models.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Compression View to Data Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We take a data compression view to data pruning for language models as follows.
    Let $p(\cdot)$, consider
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H(W,p)=\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{p(w_{i}&#124;w_{<i})}$ |  |'
  prefs: []
  type: TYPE_TB
- en: We view $H(W,p)$ averaged over sequence length, which is our main metric for
    sample importance, is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H(W,q)=\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{q(w_{i}&#124;w_{<i})}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: From an information theoretic view, ([1](#S2.E1 "In 2.1 Compression View to
    Data Pruning ‣ 2 Method ‣ Measuring Sample Importance in Data Pruning for Training
    LLMs from a Data Compression Perspective")) represents the description length
    of words in $W$ as fixed, and ([1](#S2.E1 "In 2.1 Compression View to Data Pruning
    ‣ 2 Method ‣ Measuring Sample Importance in Data Pruning for Training LLMs from
    a Data Compression Perspective")) can be regarded as being proportional to the
    negative log-likelihood or log-loss.
  prefs: []
  type: TYPE_NORMAL
- en: We hypothesize that, if $q$ in an average sense. Indeed, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{H}(p)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{H}(p,q)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{H}(p)$, respectively. The cross-entropy is an upper bound of
    entropy Cover [[1999](#bib.bib13)], i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{H}(p)\leq\mathcal{H}(p,q)$ |  |'
  prefs: []
  type: TYPE_TB
- en: That is, we always need extra bits to encode words with distribution $p$ is
    used for encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if $q$ values from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Another view is that, one can show that the *perplexity* of $W$ results in low
    perplexity or “surprisal”, where the surprisal of a sample is a term used for
    the information content of the sample in an information theoretic context.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Pruning Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overview. We first train reference model $q$ from the dataset. The pruned dataset
    is used for training the target LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Data probe model. We train a data probe model denoted by $q$ provide a good
    bound on the description lengths. We heuristically determine the subsample size
    by hypothesizing that the model is sufficiently trained if the rate of decrease
    in the training loss saturates, similar to the early stopping method.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the subsample size is set to be about 12% of training dataset:
    see Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 Pruning Method ‣ 2 Method ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective").
    Later experiments show that the proposed level of training is sufficient for a
    good pruning performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning. Next, we perform pruning on the dataset $\mathcal{D}$. Use the pruned
    dataset to train the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24c45d70173767ee1e8b8c0bb661d52b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Loss curve of GPT-125M over 1 sweep of the training dataset. For
    data efficiency, we stopped training the probe model at the point where the decrease
    in loss saturates, i.e., at about 12% of the entire dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As pretraining corpus, we use a subset of c4 Raffel et al. [[2020](#bib.bib5)]
    via random sampling, which is a total of about 3 billion tokens from GPT-2 tokenizer Radford
    et al. [[2019](#bib.bib14)]. We use decoder-only transformers Vaswani et al. [[2017](#bib.bib15)]
    using GPT-2 tokenizer as the language model. As the data probe model, we use a
    125M-parameter model trained on 0.3 billion tokens randomly sampled from the total
    corpus. As the target model, we use both 125M- and 345M-parameter models. We evaluate
    the language modeling of target models on One billion words Chelba et al. [[2014](#bib.bib16)]
    and wikitext-103 Merity et al. [[2016](#bib.bib17)] where the models are pre-trained
    on different datasets. The downstream tasks for the target models are evaluated
    using glue benchmark Wang et al. [[2018](#bib.bib18)]. Detailed hyperparameters
    are in Appendix [A.1](#A1.SS1 "A.1 Hyperparameters ‣ Appendix A Appendix ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective").
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pretraining Loss with Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fig [2](#S3.F2 "Figure 2 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment
    ‣ Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression
    Perspective") shows the test loss of the 345M target model during pretraining.
    Interestingly, pruning 10% achieves a *lower* and pruning 20% obtains a similar
    test loss, as compared to no-pruning case. As we will see later, the lower loss
    indeed leads to improved performances in language modeling and downstream tasks
    with pruning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/208c428685b6f408db52db75945f31fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Test loss of 345M GPT per training token.'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning % Proposed Random High Entropy 0 90.23 10 89.32 90.95 98.47 20 86.88
    93.40 100.06 30 87.09 99.89 109.51 40 84.19 93.73 107.48 50 91.11 97.08 104.39
    60 91.24 101.32 103.30 70 102.93 104.65 152.94 80 101.70 110.39 181.16 90 147.18
    165.39 284.59
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Perplexity of 125M per pruning ratio on One Billion Words corpus.
    Bold denotes the best performance for each pruning ratio, and the underline indicates
    better performances than no-pruning case.'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning % Proposed Random High Entropy 0 52.23 10 50.59 51.56 54.71 20 51.71
    52.25 57.99 30 53.46 54.97 61.35 40 55.06 56.91 71.03 50 58.26 59.63 74.50 60
    61.97 64.29 85.91 70 62.66 70.22 97.87 80 71.46 85.42 126.69 90 115.03 135.34
    212.67
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Perplexity of 125M per pruning ratio on wikitext-103 corpus. Bold
    denotes the best performance for each pruning ratio, and the underline indicates
    better performances than no-pruning case.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Language Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate language modeling of target models using two test corpus. Table [1](#S3.T1
    "Table 1 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    shows the perplexity of each model per pruning ratio on One Billion Words corpus
    Chelba et al. [[2014](#bib.bib16)]. The proposed pruning significantly outperforms
    the random pruning. Surprisingly, the proposed pruning achieves *better* performance
    up to 40% pruning ratio, and on-par performance up to 60% ratio, as compared to
    no-pruning case. We observe a similar trend with different corpus: Table [2](#S3.T2
    "Table 2 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    shows the perplexity results of language modeling on wikitext-103 corpus Merity
    et al. [[2016](#bib.bib17)]. The results show that, our pruning can actually improve
    the generalization capability of the target model, which we explain as follows.
    Our pruning removes samples that are less informative or contain highly redundant
    information. Such redundancy in the dataset may cause overfitting such samples
    similar to duplication, i.e., the model may get biased towards such information,
    hampering generalization. Thus, pruning based on estimates of information content
    of samples may alleviate overfitting problems. For comparison, we tested removing
    samples with high $H(\cdot,q)$ first: see “High Entropy” columns in Table [1](#S3.T1
    "Table 1 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    and [2](#S3.T2 "Table 2 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective").
    Interestingly, it performs much worse than random pruning, which conversely shows
    the effectiveness of the proposed importance metric of samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Downstream tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ee76f8ff6318976ce1fdefddb546c86.png)![Refer to caption](img/a2255d5f50bf07299cd1a0281e82da7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Text classification results on CoLA dataset in glue benchmark. Left:
    125M, Right: 345M. The green line indicates the performance without pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2191298b2bdcfc89a684dd0c635e6052.png)![Refer to caption](img/d4037c76a3a25fd110736ca40693b1ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Textual similarity results on SST2 dataset in glue benchmark. Left:
    125M, Right: 345M. The green line indicates the performance without pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: We finetune and evaluate each model in downstream tasks such as text classification
    and textual similarity tasks from glue Wang et al. [[2018](#bib.bib18)]. The left
    of Fig. [3](#S3.F3 "Figure 3 ‣ 3.4 Downstream tasks ‣ 3 Experiment ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    shows the text classification results of 125M models on CoLA dataset, which compare
    between the proposed method and random pruning. The results show that, the accuracy
    of our method is always above that of the random pruning. Next, we scaled up the
    target model to 345M, and the right of Fig. [3](#S3.F3 "Figure 3 ‣ 3.4 Downstream
    tasks ‣ 3 Experiment ‣ Measuring Sample Importance in Data Pruning for Training
    LLMs from a Data Compression Perspective") shows that the proposed data pruning
    method achieves significant performance gains over random pruning. Notably, the
    proposed pruning outperforms no-pruning case with almost up to 50% pruning ratio,
    i.e., pruning helps improving the performance of language models.
  prefs: []
  type: TYPE_NORMAL
- en: Fig. [4](#S3.F4 "Figure 4 ‣ 3.4 Downstream tasks ‣ 3 Experiment ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    illustrates the results of the textual similarity task on SST2 dataset for 125M
    and 345M models. For both models, while random pruning continually decreases performances,
    the proposed method achieves similar or even better accuracy than no-pruning case
    up to 50% pruning ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, we make a similar observation to pretraining case: the proposed pruning
    not only reduces the training cost of language models, but also *improves* the
    performance of downstream tasks. We conclude that pruning based on the information
    content of samples can improve the generalization capabilities of the models on
    downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we took an information-theoretic view of compression for data
    pruning in training language models. We use the log-likelihood function of the
    model to estimate the compressed description length of a sample, or its informativeness.
    Pruning based on the proposed estimate of sample importance enables removing samples
    with redundant information, which not only reduces computational costs, but also,
    surprisingly, enhances the generalization capability of language models. Experiments
    with various corpus and tasks validated the effectiveness of the proposed pruning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sorscher et al. [2022] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
    Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling
    via data pruning. *Advances in Neural Information Processing Systems*, 35:19523–19536,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. [2024] Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan
    Wang, and Xiaojuan Qi. Data pruning via moving-one-sample-out. *Advances in Neural
    Information Processing Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abbas et al. [2023] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig,
    Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale
    through semantic deduplication. In *ICLR 2023 Workshop on Mathematical and Empirical
    Understanding of Foundation Models*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. [2023] Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu
    Wei. In-context autoencoder for context compression in a large language model.
    In *The Twelfth International Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deletang et al. [2023] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. In
    *The Twelfth International Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huffman [1952] David A Huffman. A method for the construction of minimum-redundancy
    codes. *Proceedings of the IRE*, 40(9):1098–1101, 1952.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rissanen [1976] Jorma J Rissanen. Generalized kraft inequality and arithmetic
    coding. *IBM Journal of research and development*, 20(3):198–203, 1976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cover [1999] Thomas M Cover. *Elements of information theory*. John Wiley &
    Sons, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chelba et al. [2014] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten
    Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring
    progress in statistical language modeling. *Interspeech 2014*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *International Conference on Learning
    Representations*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop
    BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353–355,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperparameter Value Model size 125M, 345M Pretrain learning rate 5e-4 Pretrain
    Learning rate scheduler Cosine Pretrain warmup ratio 0.01 % Pretrain weight decay
    0.1 Pretrain batch size 64 Sequence length 1024 Finetuning learning rate 2e-5
    Finetuning weight decay 0.01 Finetuning batch size 32 Optimizer Adam Kingma and
    Ba [[2014](#bib.bib19)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Detailed hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
