- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15734](https://ar5iv.labs.arxiv.org/html/2406.15734)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Changhai Zhou¹¹footnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: Fudan University
  prefs: []
  type: TYPE_NORMAL
- en: zhouch23@m.fudan.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Shijie Han'
  prefs: []
  type: TYPE_NORMAL
- en: Columbia University
  prefs: []
  type: TYPE_NORMAL
- en: sh4460@columbia.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Shiyang Zhang'
  prefs: []
  type: TYPE_NORMAL
- en: Columbia University
  prefs: []
  type: TYPE_NORMAL
- en: sz3209@columbia.edu
  prefs: []
  type: TYPE_NORMAL
- en: Shichao Weng
  prefs: []
  type: TYPE_NORMAL
- en: Fudan University
  prefs: []
  type: TYPE_NORMAL
- en: scweng23@m.fudan.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Zekai Liu'
  prefs: []
  type: TYPE_NORMAL
- en: Fudan University
  prefs: []
  type: TYPE_NORMAL
- en: zkliu23@m.fudan.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Cheng Jin'
  prefs: []
  type: TYPE_NORMAL
- en: Fudan University
  prefs: []
  type: TYPE_NORMAL
- en: jc@fudan.edu.cn Equal contribution
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The efficient compression of large language models (LLMs) is becoming increasingly
    popular. However, recovering the accuracy of compressed LLMs is still a major
    challenge. Structural pruning with standard Low-Rank Adaptation (LoRA) is a common
    technique in current LLMs compression. In structural pruning, the model architecture
    is modified unevenly, resulting in suboptimal performance in various downstream
    tasks via standard LoRA with fixed rank. To address this problem, we introduce
    RankAdaptor, an efficient fine-tuning method with hierarchical dynamic rank scheduling
    for pruned LLMs. An end-to-end automatic optimization flow is developed that utilizes
    a lightweight performance model to determine the different ranks during fine-tuning.
    Comprehensive experiments on popular benchmarks shows that RankAdaptor consistently
    outperforms standard LoRA with structural pruning over different pruning settings.
    Without increasing the trainable parameters, RankAdaptor further reduces the accuracy
    performance gap between the recovery of pruned model and the original model compared
    to standard LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, large language models (LLMs) have provided innovative solutions
    across various natural language processing (NLP) tasks, such as machine translation
    [[31](#bib.bib31), [23](#bib.bib23), [1](#bib.bib1)], sentiment analysis [[32](#bib.bib32),
    [5](#bib.bib5)], and speech recognition [[19](#bib.bib19), [7](#bib.bib7)]. However,
    the exceptional performance of LLMs comes at the cost of a massive number of parameters
    and high-end hardware resources. To manage these demands, popular compression
    techniques like pruning [[17](#bib.bib17), [29](#bib.bib29), [22](#bib.bib22),
    [8](#bib.bib8)], quantization [[24](#bib.bib24), [14](#bib.bib14)], and distillation
    [[10](#bib.bib10), [26](#bib.bib26)] have been introduced. Regardless of the compression
    approach, compressed LLMs typically require fine-tuning to recover their pre-compression
    performance. Therefore, designing an efficient fine-tuning framework for compressed
    LLMs, enabling them to regain their original performance, has become a meaningful
    research endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among compression techniques, pruning is the popular method, removing redundant
    weight connections to decrease model scale and computational demands. It primarily
    involves three stages: discovery, estimation, and recovery. While most existing
    research focuses on the first two stages, less attention is paid to the recovery
    phase [[17](#bib.bib17), [29](#bib.bib29)], specifically the fine-tuning method
    in this stage, which may directly affect models’ output accuracy. Few studies
    explore fine-tuning methods specific to compressed models, and researchers often
    apply the standard LoRA method directly [[12](#bib.bib12)]. However, although
    applying standard LoRA fine-tuning in the recovery stage can reduce the gap with
    unpruned accuracy, there is room for improvement. Standard LoRA applies same rank
    configuration to all layers of pruned models, but the pruned models lack structural
    regularity. Thus, a one-size-fits-all rank value may not optimally address the
    unique needs of each layer, potentially affecting downstream performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Although LoRA and its variants have demonstrated outstanding performance in
    fine-tuning tasks for current LLMs, their performance in the domain of model compression
    remains unexplored. Pruning leads to varying degrees of weight and connectivity
    reduction across the entire model network, so it is necessary to find a dynamic
    fine-tuning method to adapt to this imbalance. In the configuration of LoRA fine-tuning,
    directly related hyperparameter is the low-rank number of each layer. DyLoRA Li
    et al. [[15](#bib.bib15)] attempts to use dynamic ideas to select the best unified
    rank value for LoRA instead of setting ahead, which has been proven to be effective
    for unpruned models. But it still cannot adapt to the different hierarchical requirements
    of pruning layers with different pruning degrees. In addition, other fine-tuning
    methods like QLoRA [[6](#bib.bib6)] with quantization and LISA [[20](#bib.bib20)]
    with importance sampling on non-frozen layers enable efficient fine-tuning process.
    However, since they all disregard rank’s influence on LoRA fine-tuning for structurally
    pruned network, so they still remain unsuitable for pruned models.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach can effectively mitigate the above challenges, we introduce RankAdaptor,
    a hierarchical dynamic fine-tuning method tailored for structural pruned LLMs.
    This approach employs a performance model to allocate optimal rank values to the
    layers of the pruned LLMs, which have been pruned to varying degrees during the
    LoRA fine-tuning process. This allocation aims to enhance the recovered performance
    of the pruned LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contribution of this paper can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hierarchical Dynamic Rank Scheduling: We address the limitations of standard
    LoRA in recovering pruned LLMs by proposing a hierarchical dynamic rank scheduling
    approach. This method tailors the rank values for each layer based on its specific
    recovery needs, improving overall model performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Automated Performance Model: We develop an end-to-end automatic optimization
    flow utilizing a lightweight performance model. This model dynamically determines
    optimal rank values for each layer, providing solutions for finding the optimal
    rank value set of the pruned LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Superior Benchmark Performance: Our extensive experiments on benchmarks demonstrate
    that RankAdaptor consistently outperforms standard LoRA across various pruning
    settings and LLM architectures, achieving higher accuracy scores and reducing
    the performance gap between pruned and original models. For example, on the BoolQ
    task, after pruning LLaMA-7B by 20% and 30%, the RankAdaptor recovers 92.13% and
    90.59% of the original model’s accuracy, compared to 86.6% and 85.44% with LoRA.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83ed24b79ec7d763caf5c696cd7e0ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of the entire process of structural pruning and recovery.
    The baseline approach is detailed in Section [2.1](#S2.SS1 "2.1 Structural Pruning
    Framework ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs"), and the proposed method is described
    in Section [3.2](#S3.SS2 "3.2 RankAdaptor ‣ 3 Method ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Structural Pruning Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The structural pruning framework [[17](#bib.bib17)] is designed for task-agnostic
    compression of LLMs. It involves three stages: (1) Discovery stage, where it identifies
    clusters of interconnected structures within the LLM. (2) Estimation stage, assessing
    each cluster’s impact on the model’s performance to determine which to prune.
    (3) Recover stage, which focuses on mitigating the performance loss from pruning
    by applying LoRA fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discovery Stage. Pruning starts by establishing structural dependencies. Assume
    neuron $N_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ is dependent on }N_{i}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: This directional dependency means if the neuron $N_{j}$ must also be pruned
    due to its dependency. Starting with any neuron, it can trigger others that depend
    on it, leading to a cascade of activations. This process identifies and groups
    dependent neurons for pruning. The LLM-Pruner automatically identify and remove
    coupled structures in different LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Estimation Stage. After grouping all coupled structures within the model, weights
    within the same group should be pruned simultaneously. Consider a dataset $\mathcal{D}=\{x_{i},y_{i}\}_{i=1}^{N}$
    denotes the weights associated with a structure. To determine the least impactful
    group in model performance, the significance is assessed using the formula with
    Taylor expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $k$ is the loss for next-token predictions. At the end of this stage,
    the importance of each group is determined by aggregating the scores of its constituent
    weights in four way: Summation, Product, Max, and Last-Only. The groups are ranked
    according to their importance, and those with lower significance are pruned based
    on a predefined ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recover Stage. After pruning the LLM, fine-tuning is necessary to recover the
    model’s performance. The commonly used fine-tuning method is low-rank adaptation,
    LoRA. The objective of this stage is to minimize the performance discrepancy between
    the pruned model and the original model, and ultimately yielding a recovered LLM
    mentioned in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background and Motivation ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e13e0b3581ba1e64abd756734164d50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Schematic diagram of $\Delta W$ in all layers being decomposed into
    low-rank matrices with a fixed rank in LoRA fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Low-Rank Adaptation (LoRA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the context of structural pruning for LLMs, LoRA fine-tuning is employed
    to recover performance with minimal parameter updates. For a pruned LLM consisting
    of $n$, is updated. The forward computation can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(x)=(W+\Delta W)X+b=(WX+b)+(AB)X.$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Given the rank $r$. This optimization can reduce the trainable parameters during
    the learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 A Motivating Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the uneven distribution of importance within the internal architecture
    of LLMs [[34](#bib.bib34)], and the criteria for judgment during the discovery
    and estimation stages of structural pruning are the importance of the structure
    to model performance, pruning operations are conducted unevenly across different
    layers. Thus, the network of pruned LLM becomes highly complex and lacks a unified
    structure. While applying standard LoRA with a uniform rank value across all layers
    can achieve a certain degree of recovery, it fails to adequately meet the unique
    recovery needs of layers pruned to varying extents, resulting in suboptimal recovery
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some existing studies [[28](#bib.bib28), [35](#bib.bib35)] indicate that the
    later layers in LLMs tend to capture more complex and semantic information compared
    to the earlier layers, thereby rendering them more consequential. Therefore, we
    use LLaMA-7B pruned 20% as experiment model and maintain the rank of all layers
    to be the fixed values. Then, we allocate rank values in an incremental step-wise
    way from the bottom to the top layers. More details can be found in Figure [2.3](#S2.SS3
    "2.3 A Motivating Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs") and Table [2.3](#S2.SS3
    "2.3 A Motivating Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8121e8dbfe5ee05d0e15aba65cdc00e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overall performance of seven benchmarks for the different fine-tuning
    configurations. LoRA denotes using fixed ranks for different layers, whereas LoRA*
    indicates using different ranks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer | Fixed | Different |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1~8 | 8 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 9~16 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 17~24 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 25~32 | 12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Rank Value Configurations. Fixed refers to applying a constant rank
    value across multiple layers of the pruned model, while Different indicates assigning
    distinct rank values to different layers of the pruned model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preliminary exploration depicted in Figure [2.3](#S2.SS3 "2.3 A Motivating
    Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs") underscore the efficacy of using a different
    rank allocation for recovering pruned LLMs. The experiments compare three different
    fine-tuning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without Fine-Tuning: Represented by the blue color, this approach exhibits
    the lowest performance across all tasks. This indicates that if the pruned model
    is not fine-tuned, there is a significant performance degradation compared to
    the original model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-Tuning with Fixed Rank: The results depicted in red illustrate the model’s
    performance when a standard LoRA with fixed rank value is applied uniformly across
    all layers. While this approach enhances performance relative to the blue configuration,
    it fails to adequately address the distinct recovery requirements of layers pruned
    to varying degrees. Therefore, despite an overall improvement, the recovery process
    remains suboptimal.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-Tuning with Different Rank: Compared with the red configuration, the green
    color demonstrates the higher recovery performance which is achieved by using
    a different rank allocation strategy. This approach involves incrementally adjusting
    the rank values across layers, as detailed in Table [2.3](#S2.SS3 "2.3 A Motivating
    Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs"), thereby adapting to the specific requirements
    of each layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: According to the motivating example, we have experimentally demonstrated the
    effectiveness of using different rank values for fine-tuning pruned models across
    layers. Based on this finding, we describe our proposed method in the next section,
    which addresses the limitations of the standard LoRA and provides a more tailored
    recovery strategy for pruned LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is crucial for recovering the performance of pruned LLMs. However,
    there is currently a lack of efficient fine-tuning methods specifically designed
    for pruned models. To fully harness the potential of pruned models, we propose
    RankAdaptor, a hierarchical dynamic fine-tuning algorithm tailored for pruned
    LLMs. RankAdaptor utilizes a lightweight performance model to automatically identify
    the optimal rank values for each layer, thereby enhancing the performance of pruned
    models. In real-world scenarios, a complete fine-tuning process for a single rank
    configuration often takes several hours, but the performance model can predict
    the downstream task performance of billions of possible combinations in less than
    an hour.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The motivating example in Section [2.3](#S2.SS3 "2.3 A Motivating Example ‣
    2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs") demonstrates the effectiveness of hierarchical different
    LoRA in pruned model recovery. However, recovering the performance of pruned LLMs
    presents two significant challenges: i) The importance of each layer is difficult
    to evaluate for different tasks, making it challenging to determine the optimal
    hierarchical rank values. ii) The vast solution space created by different potential
    rank values for each layer makes exhaustive evaluation impractical.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb68eb81aa72f9f8c73dcf2265e32935.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Schematic diagram of $\Delta W$ being decomposed into low-rank matrices
    with different ranks in LoRA fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: In the original LLMs, the importance of each layer for downstream tasks varies.
    Structural pruning trims parameters based on their importance, further altering
    it and leading to an uneven distribution of importance across layers. In LoRA,
    the rank $r$ is the number of layers, which is astronomically large. Exhaustively
    evaluating all possible rank combinations is extremely time-consuming. This highlights
    the necessity of developing a more efficient method, and the performance model
    is key to achieving this goal.
  prefs: []
  type: TYPE_NORMAL
- en: To address these challenges and incorporate the hierarchical and dynamic nature
    of our approach, we propose an automated end-to-end process that leverages a lightweight
    performance model. Given a pruned model $PL$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our overall objective is to find the optimal rank set $R_{HD}^{*}$ of all possible
    rank value combinations, which can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R_{HD}^{*}=\arg\max_{R_{HD}\in S}\mathcal{Q}(R_{HD})$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Due to the vastness of the solution space, we do not search the entire space
    to expedite the solving process. Consequently, $R_{HD}^{*}$ represents a locally
    optimal solution rather than a globally optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8ce7c33e2ba8fbc599cf02e4c6f03b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: RankAdaptor Workflow: an end-to-end learning-based algorithm to optimize
    hierarchical dynamic rank values for pruned LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 RankAdaptor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We propose a learning-based algorithm in Figure [5](#S3.F5 "Figure 5 ‣ 3.1
    Problem Formulation ‣ 3 Method ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs") to solve this problem. We build a performance model
    to estimate the performance of the recovered model on given downstream tasks.
    Suppose $R_{HD}$ is fed back to the performance model as a training set, utilizing
    an incremental learning technique to update the weights. There are three important
    phases in our design.'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization Phase. It is necessary to train a lightweight performance model
    to find $R_{HD}^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental Learning Phase. In this phase, $R_{HD}$ is fed back into the performance
    model for incremental learning, enabling the model to continuously enhance its
    prediction accuracy over successive iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Converge Phase. When the gap between $\mathcal{Q}(R_{HD})$.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LLMs and Benchmarks. To demonstrate the effectiveness of RankAdaptor, we test
    it on three open source large language models: LLaMA-7B [[27](#bib.bib27)], LLaMA-13B
    [[27](#bib.bib27)] and Vicuna-7B [[36](#bib.bib36)], and specific version is in
    the Appendix [D](#A4 "Appendix D Version of LLMs ‣ RankAdaptor: Hierarchical Dynamic
    Low-Rank Adaptation for Structural Pruned LLMs"). We conduct these LLMs on zero-shot
    classification tests for commonsense reasoning datasets, including BoolQ [[3](#bib.bib3)],
    PIQA [[2](#bib.bib2)], HellaSwag [[30](#bib.bib30)], WinoGrande [[21](#bib.bib21)],
    ARC-easy [[4](#bib.bib4)], ARC-challenge [[4](#bib.bib4)], and OpenbookQA [[18](#bib.bib18)].'
  prefs: []
  type: TYPE_NORMAL
- en: Details of Performance Model. To validate the effectiveness of RankAdaptor,
    our performance model employs a basic MLP architecture. However, more suitable
    performance models tailored for RankAdaptor are expected to emerge, aiding in
    identifying superior rank sets. The Mixture of Experts (MoE) technique, where
    multiple sub-models are trained on different data subsets and a gating network
    determines which expert to consult for a given input, is a promising approach.
    In our experiment, we construct a performance model for the seven tasks, with
    an outer layer as a loss computation tool influenced by routing factors, and an
    inner layer consisting of seven identical three-layer MLPs. Each inner MLP receives
    a rank set as input and outputs an accuracy. Before the experiment, the model
    undergoes pre-training on a real data subset to achieve a certain predictive accuracy
    level, and the results from each iteration are used for further retraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementation Details. Our implementation utilizes the following software
    and hardware configurations: PyTorch version 2.1.2; Transformers library version
    4.41.0; PEFT (Parameter-Efficient Fine-Tuning) library version 0.6.0; CUDA version
    12.4; GPU: NVIDIA A800 with 80GB memory; Operating System: Ubuntu. We set the
    number of examples to 10, which is used for estimating the importance of each
    weight group. The estimation stage only uses the first-order Taylor expansion
    in Equation [2](#S2.E2 "In 2.1 Structural Pruning Framework ‣ 2 Background and
    Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs"). The MLP dimensions for the inner layers of the performance model
    are set to (32-32-32-1), meaning each inner MLP consists of three hidden layers
    with 32 neurons and an output layer with a single neuron. Micro-batch size is
    configured to 16, which specifies the number of examples processed in each step
    of model training.'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning Rate. Existing research indicates that specific layers of LLaMA-7B,
    Vicuna-7B, and LLaMA-13B are crucial to the models’ architecture and should remain
    unpruned [[17](#bib.bib17)]. Therefore, we prune only layers 5-30 of LLaMA-7B
    and Vicuna-7B, and layers 5-36 of LLaMA-13B to achieve the predefined global pruning
    rate. Specifically, we prune 25%, 32%, 38%, and 63.5% of the middle layers to
    achieve global pruning rates of 20%, 25%, 30%, and 50%. Regarding the unpruned
    layers, we also keep their rank values the same as the standard LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Size of the Solution Space. In conventional LoRA, setting fixed rank values
    within the range of 2 to 16 achieves favorable model recovery. To ensure that
    the trainable parameter count of RankAdaptor remains at the same level as conventional
    LoRA, the range of rank values in this experiment for Hierarchical Dynamic LoRA
    was set to $\left\{2,4,6,8,10,12\right\}$. Different models follow the same calculation
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Baseline and Configuration. We utilized the publicly available code from LLMPruner
    [[17](#bib.bib17)], applying LoRA, LoftQ, and Quantimize as fine-tuning methods
    during the recovery phase, and tested them across various benchmarks. Additionally,
    since neither LLaMA nor LLM-Pruner disclosed the relevant testing prompts in their
    papers, we employed Gao et al. [[9](#bib.bib9)] to create open prompts for the
    benchmarks shown in Section [4.1](#S4.SS1 "4.1 Experimental Setup ‣ 4 Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Results. We present the performance of the original LLM, pruned
    LLM and recovered LLM recovered by LoRA and RankAdaptor with the best results
    on each task in Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣
    RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs")
    below, and Table [4](#A1.T4 "Table 4 ‣ A.1 Performance in Vicuna-7B and LLaMA-13B.
    ‣ Appendix A More Results and Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs"), [5](#A1.T5 "Table 5 ‣ A.1 Performance
    in Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and Analysis ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs") in the appendix.
    The specific value of $R$ used in each experiment is shown Table [4](#A1.T4 "Table
    4 ‣ A.1 Performance in Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and
    Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs") in the appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Zero-shot performance of LLaMA-7B in unpruned, pruned, LoRA recovery,
    and RankAdaptor recovery. ‘Bold’ indicates the best performance at each pruning
    rate. Results are the optimal performance for each task. Specific $R$ configurations
    in Appendix [B](#A2 "Appendix B Specific 𝑅 configurations in LLaMA-7B Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs"). Reported in percentage (%).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 0% | - | 73.09 | 78.35 | 72.98 | 67.09 | 67.42 | 41.38 | 42.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 20% | w/o Tuning | 56.94 | 75.73 | 66.83 | 60.06 | 60.94 | 36.43 |
    39.80 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 63.30 | 76.82 | 68.68 | 63.38 | 63.76 | 37.11 | 40.60 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 67.34 | 77.31 | 69.07 | 64.17 | 65.36 | 37.80 | 41.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 25% | w/o Tuning | 59.94 | 73.23 | 62.35 | 58.80 | 55.81 | 34.90 |
    39.40 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 61.93 | 76.01 | 65.97 | 61.96 | 62.21 | 35.92 | 39.40 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 67.43 | 76.06 | 66.08 | 64.40 | 62.63 | 36.77 | 40.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 30% | w/o Tuning | 58.96 | 71.22 | 58.10 | 58.88 | 52.19 | 32.34 |
    38.40 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 62.45 | 74.37 | 63.14 | 61.96 | 59.22 | 33.70 | 39.60 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 66.21 | 75.19 | 63.61 | 63.14 | 60.10 | 34.64 | 40.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 50% | w/o Tuning | 57.98 | 60.94 | 34.35 | 52.25 | 31.82 | 27.30 |
    35.80 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 43.76 | 69.04 | 45.01 | 50.99 | 45.20 | 28.75 | 34.60 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 51.65 | 69.48 | 45.03 | 51.93 | 45.66 | 28.41 | 35.00 |'
  prefs: []
  type: TYPE_TB
- en: Different Tasks Analysis. Across various tasks, RankAdaptor consistently outperforms
    the LoRA, particularly for tasks like BoolQ, PIQA, HellaSwag, WinoGrande, and
    ARC-easy. For instance, on the BoolQ task with 20% pruning for LLaMA-7B, RankAdaptor
    achieves a 92.1% recovery rate compared to 86.6% for LoRA. This demonstrates RankAdaptor’s
    effectiveness in adapting the rank values to better suit the characteristics of
    different tasks and recover more of the original performance.
  prefs: []
  type: TYPE_NORMAL
- en: Different Pruning Rates Analysis. RankAdaptor performs better than LoRA at each
    level of pruning. For example, at a 25% pruning rate, LLaMA-7B recovered 92.26%
    of its original accuracy in the BoolQ task using RankAdaptor, compared to 84.73%
    with LoRA. Across the board, RankAdaptor shows a smaller degradation in performance
    with increasing pruning rates compared to LoRA. For example, in the Vicuna-7B
    model at a 30% pruning rate, RankAdaptor recovers 82.63% of the original performance
    in the HellaSwag task, compared to 80.51% with LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. The experimental results clearly indicate that RankAdaptor is a promising
    technique for recovering the performance of pruned LLMs, as evidenced by its consistently
    higher recovery percentages compared to the LoRA across various tasks and pruning
    rates. While the recovery rate gains vary, RankAdaptor consistently demonstrates
    its superiority, making it an efficient fine-tuning method for structural pruned
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generation Comparison. We present a comparison of pruned LLMs recovered by
    RankAdaptor and LoRA for generative tasks in Appendix [C](#A3 "Appendix C Generation
    Comparison. ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs"). Remarkably, the results produced by RankAdaptor are surprisingly
    compelling, further accentuating its effectiveness as an efficient model recovery
    technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we use LLaMA-7B with a 20% global pruning rate and the RankAdaptor
    recovery method to conduct all ablation experiments and all results are shown
    in Table [3](#S4.T3 "Table 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance comparison for the ablation study under the overall best
    $R$ configuration for seven tasks on LLaMA-7B. The results are reported in percentage
    (%).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | Sample Size | Element-wise Importance | Setting of Performance
    Model | Micro-batch Size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $N$=50 | Element¹ | Element² | Setting1 | Setting2 | Setting3 | Micro-4 |
    Micro-8 | Micro-16 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-e | 63.97 | 65.32 | 63.97 | 62.84 | 63.97 | 64.73 | 64.65 | 64.52 | 63.97
    | 65.24 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-c | 37.29 | 37.71 | 37.29 | 36.77 | 37.29 | 36.60 | 37.54 | 38.65 | 37.29
    | 37.54 |'
  prefs: []
  type: TYPE_TB
- en: '| WinoGrande | 63.61 | 63.14 | 63.61 | 63.22 | 63.61 | 63.46 | 63.06 | 62.04
    | 63.61 | 63.14 |'
  prefs: []
  type: TYPE_TB
- en: '| OBQA | 39.80 | 41.00 | 39.80 | 39.80 | 39.80 | 40.80 | 40.80 | 40.00 | 39.80
    | 40.80 |'
  prefs: []
  type: TYPE_TB
- en: '| BoolQ | 65.81 | 64.43 | 65.81 | 66.48 | 66.91 | 64.43 | 64.86 | 67.28 | 65.81
    | 66.91 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | 76.99 | 77.15 | 76.99 | 76.82 | 76.99 | 76.99 | 77.04 | 76.50 | 76.99
    | 76.93 |'
  prefs: []
  type: TYPE_TB
- en: '| HellaSwag | 68.56 | 68.52 | 68.56 | 67.88 | 68.56 | 68.75 | 69.00 | 68.08
    | 68.56 | 68.78 |'
  prefs: []
  type: TYPE_TB
- en: 'Sample Size. We conduct ablation experiments to assess the impact of sample
    size during the estimation phase [2.1](#S2.SS1 "2.1 Structural Pruning Framework
    ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs"), specifically comparing performance with $N=10$)
    yields competitive results, such as in WinoGrande. This underscores the need for
    careful selection of sample size based on the specific requirements of the task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Element-wise Importance.We further conduct tests on the proposed importance
    estimation techniques. The results compare the first-order (Element¹) and second-order
    (Element²) Taylor approximations for evaluating the importance of each parameter,
    as described in Equation [2](#S2.E2 "In 2.1 Structural Pruning Framework ‣ 2 Background
    and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs"). Our findings indicate that Element¹ provides better performance
    than Element² across the most benchmarks. While higher-order derivatives may theoretically
    offer more precise adjustments, their complexity may outweigh the marginal performance
    gains observed in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting of Performance Model. To investigate the impact of different inner MLP
    dimensions in the performance model, we test three configurations. The first setting
    consists of three hidden layers with 32 neurons each, followed by an output layer
    with a single neuron, abbreviated as 32-32-32-1\. The other two configurations
    are 32-64-32-1 and 32-16-32-1, following the same notation. The results illustrate
    that varying dimensions of inner MLP layers have nuanced impacts on performance
    across different benchmarks. For inner MLP dimensions, Setting1 provides the highest
    performance on tasks such as ARC-e and BoolQ, while Setting3 shows competitive
    performance on PIQA and HellaSwag.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-batch Sizes. We finally assess the impact of different micro-batch sizes
    (4, 8, and 16). The results indicate that larger micro-batch sizes can lead to
    better performance on certain tasks, though not universally across all benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We observe that in certain cases, the performance of the pruned model without
    any recovery method can surpass that of the models recovered through LoRA and
    RankAdaptor. As shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs"), when LLaMA-7B is pruned at a 50% rate, its performance on the BoolQ task
    is 57.98%. However, when recovered by LoRA and RankAdaptor, the performances decline
    to 43.76% and 51.65%, respectively. A potential explanation for this phenomenon
    is that the data distribution used for fine-tuning may differ from the BoolQ dataset,
    leading to a negative impact on the pruned model’s performance from LoRA and RankAdaptor.
    It is important to note that RankAdaptor consistently outperforms LoRA when recovery
    methods are employed.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Efficient Pruning of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structured Pruning. LLM-Pruner [[17](#bib.bib17)] employs structured pruning
    to remove non-essential interconnected structures by utilizing gradient information.
    This approach allows compressed models to restore good performance in multitask
    with basic fine-tuning. Xia et al. [[29](#bib.bib29)] introduces "Sheared LLaMA"
    to compress pre-trained LLMs. It employs dynamic batch loading to improve data
    efficiency during pruning and retraining. This approach retains high performance
    on various tasks with less computational effort than training from scratch. Santacroce
    et al. [[22](#bib.bib22)] presents Globally Unique Movement (GUM), a novel pruning
    technique that focuses on the sensitivity and uniqueness of LLMs’ network components.
    GUM selects models’ neurons that uniquely contribute to model output and are sensitive
    to loss changes to prune, thus maintaining high accuracy. This approach optimizes
    the trade-off between information retention and computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured Pruning. SparseGPT [[8](#bib.bib8)] is a pruning method that does
    not require retraining. SparseGPT [[8](#bib.bib8)] transforms the pruning process
    into a series of large-scale sparse regression problems, which can be quickly
    solved through Hessian matrix inversion. It efficiently prunes large models to
    high sparsity in a single step while maintaining high accuracy. Wanda [[25](#bib.bib25)]
    prunes LLMs by selectively removing weights based on their sizes and input activations.
    It adaptively adjusts sparsity levels to achieve a reduction of more than half
    without sacrificing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Parameter Efficient Fine-Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Houlsby et al. [[11](#bib.bib11)] introduce a transfer learning method that
    integrates adapter modules into pre-trained Transformer models. It can efficiently
    tackle various NLP tasks with few additional parameters and achieving performance
    similar to full fine-tuning. LLM-Adapters [[13](#bib.bib13)] is a method that
    integrates small adapters with few extra parameters to LLMs for efficient fine-tuning.
    This approach allows smaller models to perform as well as larger ones on specific
    reasoning tasks. While the adapter takes a serial approach to integrating trainable
    components into pre-trained Transformer models, low-rank adaptation (LoRA) [[12](#bib.bib12)]
    presents a parallel method of infusing rank decomposition matrices into each layer
    of the model’s architecture. Specifically, LoRA adds trainable matrices to each
    layer of the model and the pre-trained weights are kept the same. LoRA reduces
    the number of trainable parameters compared to fine-tuning the entire model, which
    makes model adaptation faster and less resource-intensive. LoRA-FA [[33](#bib.bib33)]
    freezes the projection-down weight of the low-rank adaptation (LoRA) layers and
    only updates the projection-up weight to reduce the memory requirements for fine-tuning.
    QLora [[6](#bib.bib6)] combines low-rank adapters and quantized 4-bit weights
    to efficient fine-tune LLMs. It significantly reduces the GPU memory requirements
    and achieves performance comparable to full 16-bit fine-tuning. LoftQ [[16](#bib.bib16)]
    applies quantization and low-rank approximation alternatively to obtain an good
    initialization for LoRA fine-tuning. It mitigates the discrepancy between quantized
    weights and pre-trained weights, enabling efficient fine-tuning of quantized models,
    especially in the challenging low-bit regimes.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce RankAdaptor, a novel fine-tuning algorithm tailored
    for recovering the performance of pruned LLMs. RankAdaptor employs a hierarchical
    dynamic fine-tuning strategy leveraging a lightweight performance model to adaptively
    adjust hierarchical rank values. This approach addresses the limitations of standard
    fixed-rank LoRA, which often yields suboptimal performance recovery due to the
    uneven architectural modifications induced by structural pruning. Comprehensive
    evaluations on multiple open-source LLMs and benchmark tasks demonstrate that
    RankAdaptor consistently outperforms standard LoRA across different pruning settings.
    RankAdaptor represents a significant advancement in fine-tuning pruned LLMs. Its
    adaptive rank scheduling and end-to-end optimization provide substantial improvements
    over standard techniques, making it a promising tool for enhancing the performance
    of pruned language models in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations. The first step of this work only focus on the designing efficient
    parameter fine-tuning for the LLM structural pruning. A general automatic flow
    with optimal performance model is considered for the various LLM compression tasks
    such as quantization or distillation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aycock and Bawden [2024] Seth Aycock and Rachel Bawden. Topic-guided example
    selection for domain adaptation in llm-based machine translation. In *Proceedings
    of the 18th Conference of the European Chapter of the Association for Computational
    Linguistics: Student Research Workshop*, pages 175–195, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. In *Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers)*, pages 2924–2936, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2023] Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner,
    and Michael Bendersky. What do llms know about financial markets? a case study
    on reddit market sentiment analysis. In *Companion Proceedings of the ACM Web
    Conference 2023*, pages 107–110, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fathullah et al. [2024] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng
    Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli,
    et al. Prompting large language models with speech recognition abilities. In *ICASSP
    2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pages 13351–13355\. IEEE, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model
    evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge
    distillation of large language models. In *The Twelfth International Conference
    on Learning Representations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International conference
    on machine learning*, pages 2790–2799\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2021] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language
    models. In *International Conference on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2023] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim,
    Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. Llm-adapters: An adapter family
    for parameter-efficient fine-tuning of large language models. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    5254–5276, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2023] Janghwan Lee, Minsoo Kim, Seungcheol Baek, Seok Hwang, Wonyong
    Sung, and Jungwook Choi. Enhancing computation efficiency in large language models
    through weight and activation quantization. In *Proceedings of the 2023 Conference
    on Empirical Methods in Natural Language Processing*, pages 14726–14739, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2020] Yinghui Li, Jing Yang, and Jiliang Wang. Dylora: Towards energy
    efficient dynamic lora transmission control. In *IEEE INFOCOM 2020-IEEE Conference
    on Computer Communications*, pages 2312–2320\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng
    He, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pages 2381–2391, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min and Wang [2023] Zeping Min and Jinbo Wang. Exploring the integration of
    large language models into automatic speech recognition systems: An empirical
    study. In *International Conference on Neural Information Processing*, pages 69–84\.
    Springer, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2024] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang,
    Chi Han, and Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient
    large language model fine-tuning. *arXiv preprint arXiv:2403.17919*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santacroce et al. [2023] Michael Santacroce, Zixin Wen, Yelong Shen, and Yuanzhi
    Li. What matters in the structured pruning of generative language models? *arXiv
    preprint arXiv:2302.03773*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sato et al. [2020] Shoetsu Sato, Jin Sakuma, Naoki Yoshinaga, Masashi Toyoda,
    and Masaru Kitsuregawa. Vocabulary adaptation for domain adaptation in neural
    machine translation. In *Findings of the Association for Computational Linguistics:
    EMNLP 2020*, pages 4269–4279, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. [2023] Shicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Shu
    Zhao, Peng Zhang, and Jie Tang. Gkd: A general knowledge distillation framework
    for large-scale pre-trained language model. In *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)*,
    pages 134–148, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. [2023] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared
    llama: Accelerating language model pre-training via structured pruning. In *The
    Twelfth International Conference on Learning Representations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023a] Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting
    large language model for machine translation: A case study. In *International
    Conference on Machine Learning*, pages 41092–41110\. PMLR, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023b] Boyu Zhang, Hongyang Yang, Tianyu Zhou, Muhammad Ali Babar,
    and Xiao-Yang Liu. Enhancing financial sentiment analysis via retrieval augmented
    large language models. In *Proceedings of the Fourth ACM International Conference
    on AI in Finance*, pages 349–356, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023c] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu,
    and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models
    fine-tuning. *arXiv preprint arXiv:2308.03303*, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng,
    Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient
    fine-tuning. In *The Eleventh International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2024] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng,
    Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large
    language models: A survey. *ACM Transactions on Intelligent Systems and Technology*,
    15(2):1–38, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A More Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Performance in Vicuna-7B and LLaMA-13B.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We list the performance of the configuration described in Section [4.1](#S4.SS1
    "4.1 Experimental Setup ‣ 4 Experiments ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs") for Vicuna-7B in Table [4](#A1.T4 "Table
    4 ‣ A.1 Performance in Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and
    Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs") and LLaMA-13B in Table [5](#A1.T5 "Table 5 ‣ A.1 Performance in
    Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and Analysis ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Zero-shot performance of Vicuna-7B in unpruned, pruned, pruned with
    LoRA recovery, and pruned with RankAdaptor recovery settings; ’Bold’ represents
    the best performance at the same pruning rate across the three pruned settings.
    The data in the table is from the optimal performance on each individual task.
    The results are reported in percentage (%).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 0% | - | 75.69 | 77.75 | 71.06 | 67.80 | 69.07 | 40.78 | 42.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 20% | W/O Tuning | 52.35 | 76.17 | 64.79 | 59.59 | 65.99 | 38.14 |
    40.00 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 57.77 | 77.58 | 67.16 | 63.14 | 67.30 | 37.71 | 40.40 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 61.19 | 77.15 | 67.32 | 63.85 | 67.68 | 38.05 | 41.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 25% | W/O Tuning | 43.98 | 74.76 | 61.58 | 57.06 | 63.72 | 37.20 |
    39.80 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 50.34 | 75.24 | 64.10 | 61.33 | 63.93 | 35.67 | 40.60 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 58.50 | 76.17 | 64.23 | 61.96 | 63.30 | 36.01 | 42.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 30% | W/O Tuning | 43.12 | 73.45 | 55.64 | 57.22 | 58.96 | 34.30 |
    37.80 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 58.81 | 74.37 | 60.70 | 60.62 | 59.01 | 33.79 | 38.80 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 57.58 | 75.57 | 61.63 | 60.22 | 60.94 | 34.81 | 39.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 50% | W/O Tuning | 62.29 | 60.28 | 33.91 | 54.54 | 35.14 | 28.16 |
    33.80 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 59.51 | 66.87 | 43.18 | 52.01 | 48.40 | 26.45 | 34.00 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 59.91 | 67.46 | 43.50 | 52.41 | 48.70 | 27.65 | 35.80 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Zero-shot performance of LLaMA-13B in unpruned, pruned, pruned with
    LoRA recovery, and pruned with RankAdaptor recovery settings; ’Bold’ represents
    the best performance at the same pruning rate across the three pruned settings.
    The data in the table is from the optimal performance on each individual task.
    The results are reported in percentage (%).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 0% | N/A | 68.50 | 79.11 | 76.21 | 70.09 | 74.58 | 44.54 | 42.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 50% | W/O Tuning | 58.90 | 66.32 | 42.24 | 52.25 | 39.06 | 29.18 |
    33.20 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 61.93 | 71.38 | 53.36 | 53.59 | 53.11 | 29.95 | 38.00 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 62.05 | 71.71 | 53.33 | 54.22 | 53.20 | 30.89 | 39.40 |'
  prefs: []
  type: TYPE_TB
- en: Performance Analysis of Vicuna-7B. At pruning rates of 20% and 25%, RankAdaptor
    shows a noticeable improvement over LoRA, achieving the best performance on most
    tasks. For instance, on the BoolQ task with a 20% pruning rate, RankAdaptor reaches
    61.19%, while LoRA only achieves 57.77%. As the pruning rate increases to 30%,
    the performance gap between RankAdaptor and LoRA narrows, but RankAdaptor still
    maintains a slight advantage on most tasks. When the pruning rate reaches 50%,
    RankAdaptor outperforms LoRA only on the PIQA and OpenbookQA tasks, while performing
    comparably or slightly worse on the other tasks. This suggests that at extreme
    pruning conditions, the advantage of RankAdaptor is not as prominent. Overall,
    RankAdaptor exhibits a certain advantage over LoRA, but this advantage tends to
    diminish as the pruning rate increases.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Analysis of LLaMA-7B. At the 50% pruning rate, RankAdaptor outperforms
    LoRA on the BoolQ, PIQA, WinoGrande, ARC-easy, ARC-challenge, and OpenbookQA tasks
    but falls slightly behind on the HellaSwag task. This indicates that even in extreme
    pruning conditions, RankAdaptor can maintain superior performance over LoRA on
    most tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Overall Optimal Performance across Multiple Tasks.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the experiment, we also use the performance model with RankAdaptor to achieve
    overall optimization on multiple tasks. Table [6](#A1.T6 "Table 6 ‣ A.2 Overall
    Optimal Performance across Multiple Tasks. ‣ Appendix A More Results and Analysis
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs") shows the performance of each task and average performance of seven tasks
    in the comprehensive optimization results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Zero-shot performance of LLaMA-7B in unpruned, pruned, pruned with
    LoRA recovery, and pruned with RankAdaptor recovery settings; ’Bold’ represents
    the best performance at the same pruning rate across the three pruned settings.
    The data in the table is from the overall performance across multiple tasks. The
    results are reported in percentage (%).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 0% | N/A | 73.09 | 78.35 | 72.98 | 67.09 | 67.42 | 41.38 | 42.40 |
    63.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 20% | W/O Tuning | 56.94 | 75.73 | 66.83 | 60.06 | 60.94 | 36.43 |
    39.80 | 56.96 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 63.30 | 76.82 | 68.68 | 63.38 | 63.76 | 37.11 | 40.60 | 59.52 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 66.91 | 76.93 | 68.78 | 63.14 | 65.24 | 37.54 | 40.80 | 59.90
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 25% | W/O Tuning | 59.94 | 73.23 | 62.35 | 58.80 | 55.81 | 34.90 |
    39.40 | 54.63 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 61.93 | 76.01 | 65.97 | 61.96 | 62.21 | 35.92 | 39.40 | 57.63 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 63.15 | 75.95 | 65.97 | 62.04 | 62.63 | 36.77 | 39.60 | 58.16
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 30% | W/O Tuning | 58.96 | 71.22 | 58.10 | 58.88 | 52.19 | 32.34 |
    38.40 | 52.58 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 62.45 | 74.37 | 63.14 | 61.96 | 59.22 | 33.70 | 39.60 | 56.63 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 62.72 | 74.39 | 63.61 | 63.14 | 60.10 | 33.62 | 40.20 | 56.97
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rate = 50% | W/O Tuning | 57.98 | 60.94 | 34.35 | 52.25 | 31.82 | 27.30 |
    35.80 | 42.92 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 43.76 | 69.04 | 45.01 | 50.99 | 45.20 | 28.75 | 34.60 | 45.62 |'
  prefs: []
  type: TYPE_TB
- en: '| RankAdaptor | 51.65 | 69.48 | 45.03 | 51.93 | 45.66 | 28.41 | 35.00 | 46.74
    |'
  prefs: []
  type: TYPE_TB
- en: 'The results in Table [6](#A1.T6 "Table 6 ‣ A.2 Overall Optimal Performance
    across Multiple Tasks. ‣ Appendix A More Results and Analysis ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs") has demonstrated RankAdaptor
    has outperformed the LoRA, showcasing its ability to effectively balance and optimize
    overall performance when considering multiple tasks simultaneously. While its
    advantage over LoRA has diminished at the extreme 50% pruning rate, RankAdaptor
    has still achieved a higher average accuracy. Notably, at the 20% pruning rate,
    it has recovered most of the original model’s performance with an average accuracy
    of 59.90%, only 3.2 percentage points lower than the unpruned model’s 63.10%.
    Throughout all pruning rates, RankAdaptor has significantly surpassed the unpruned
    models without tuning, highlighting its capability as an efficient model recovery
    method. Although it may not outperform LoRA on every individual task, RankAdaptor
    has demonstrated its ability to strike a better overall performance balance across
    multiple tasks, making it a promising solution for practical large language model
    deployment in resource-constrained environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Specific $R$ configurations in LLaMA-7B Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs"), we present
    the optimal performance achieved by RankAdaptor on each task. Table [7](#A2.T7
    "Table 7 ‣ Appendix B Specific 𝑅 configurations in LLaMA-7B Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs") displays
    the rank configurations corresponding to all reported performance results. We
    make these rank configurations publicly available to foster reproducibility and
    enable further research by other scholars.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Specific Composition of $R$ in LLaMA-7B Experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Rate | Tasks | Layers’ Rank Values (1~32/40) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 20% | BoolQ | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2,
    8, 2, 8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Hella | 8, 8, 8, 8, 2, 2, 4, 10, 10, 6, 10, 10, 10, 6, 6, 2, 2, 10, 2, 4,
    2, 10, 10, 10, 4, 10, 10, 6, 6, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Wino | 8, 8, 8, 8, 8, 10, 4, 10, 4, 6, 6, 2, 10, 8, 12, 12, 10, 12, 12, 10,
    6, 6, 8, 8, 10, 6, 6, 12, 2, 8, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-e | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-c | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| OBQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | BoolQ | 8, 8, 8, 8, 12, 2, 8, 2, 8, 12, 4, 2, 10, 12, 10, 4, 2, 2,
    12, 8, 10, 2, 12, 12, 8, 4, 4, 2, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | 8, 8, 8, 8, 4, 2, 2, 10, 10, 2, 10, 10, 10, 2, 2, 2, 4, 10, 4, 6,
    10, 2, 2, 6, 10, 2, 2, 10, 10, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Hella | 8, 8, 8, 8, 4, 10, 12, 12, 6, 10, 6, 6, 8, 2, 2, 12, 2, 12, 12, 6,
    4, 10, 6, 2, 2, 8, 4, 2, 2, 8, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Wino | 8, 8, 8, 8, 8, 4, 12, 8, 2, 2, 12, 2, 10, 12, 2, 12, 12, 10, 8, 12,
    4, 6, 6, 4, 10, 4, 2, 10, 10, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-e | 8, 8, 8, 8, 2, 12, 2, 6, 12, 6, 12, 10, 6, 4, 8, 8, 12, 2, 2, 6,
    8, 4, 12, 12, 2, 4, 2, 6, 6, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-c | 8, 8, 8, 8, 2, 12, 2, 6, 12, 6, 12, 10, 6, 4, 8, 8, 12, 2, 2, 6,
    8, 4, 12, 12, 2, 4, 2, 6, 6, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| OBQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 1, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2, 8,
    10, 12, 12, 10, 4, 4, 6, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 30% | BoolQ | 8, 8, 8, 8, 12, 2, 8, 2, 8, 12, 4, 2, 10, 12, 10, 4, 2, 2,
    12, 8, 10, 2, 12, 12, 8, 4, 4, 2, 2, 12, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | 8, 8, 8, 8, 12, 6, 10, 4, 2, 4, 2, 4, 12, 8, 2, 2, 2, 12, 12, 12,
    12, 2, 12, 4, 4, 2, 10, 2, 2, 8, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Hella | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Wino | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-e | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-c | 8, 8, 8, 8, 4, 10, 12, 12, 6, 10, 6, 6, 8, 2, 2, 12, 2, 12, 12, 6,
    4, 10, 6, 2, 2, 8, 4, 2, 2, 8, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| OBQA | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | BoolQ | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6,
    4, 2, 6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Hella | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Wino | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-e | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-c | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: '| OBQA | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Generation Comparison.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Complementing the evaluation of model performance on classification tasks in
    the experiments, we further investigate the generative capabilities of the recovered
    models. Notably, we conduct text generation tasks using LLaMA-7B and Vicuna-7B
    models recovered by LoRA and RankAdaptor at a 20% pruning rate. The results are
    remarkably promising. For article continuation, the models recovered by RankAdaptor
    demonstrate superior coherence in their generated sentences. Similarly, when tasked
    with step listing, RankAdaptor-recovered LLMs produce clearer and more logical
    step sequences. These compelling comparative results are illustrated in Figures
    1 and 2, showcasing the potential of RankAdaptor in preserving and enhancing generative
    abilities during model compression and recovery.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e9b22e59b2ce213996ff540ddd3a235.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Article continuation task comparison in LLaMA-7B'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/547aa92323e76c316ebc16fd616a88ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Step listing task comparison in Vicuna-7B'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Version of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We provide the Hugging Face link of LLMs used in the experiment: LLaMA-7B:
    [https://huggingface.co/baffo32/decapoda-research-llama-7B-hf](https://huggingface.co/baffo32/decapoda-research-llama-7B-hf);
    Vicuna-7B: [https://huggingface.co/yahma/llama-13b-hf](https://huggingface.co/yahma/llama-13b-hf);
    LLaMA-13B: [https://huggingface.co/lmsys/vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)'
  prefs: []
  type: TYPE_NORMAL
