- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:58:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17759](https://ar5iv.labs.arxiv.org/html/2403.17759)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹institutetext: University of Bologna, Bologna, Italy ²²institutetext: University
    of Tokyo, Tokyo, Japan ³³institutetext: National Institute of Informatics, Tokyo,
    JapanDavide Baldelli 1133 [0009-0006-4336-923X](https://orcid.org/0009-0006-4336-923X
    "ORCID identifier")    Junfeng Jiang 22 [0000-0002-3680-2465](https://orcid.org/0000-0002-3680-2465
    "ORCID identifier")    Akiko Aizawa 33 [0000-0001-6544-5076](https://orcid.org/0000-0001-6544-5076
    "ORCID identifier")    Paolo Torroni 11 [0000-0002-9253-8638](https://orcid.org/0000-0002-9253-8638
    "ORCID identifier")'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this paper, we present TWOLAR: a two-stage pipeline for passage reranking
    based on the distillation of knowledge from Large Language Models (LLM). TWOLAR
    introduces a new scoring strategy and a distillation process consisting in the
    creation of a novel and diverse training dataset. The dataset consists of 20K
    queries, each associated with a set of documents retrieved via four distinct retrieval
    methods to ensure diversity, and then reranked by exploiting the zero-shot reranking
    capabilities of an LLM. Our ablation studies demonstrate the contribution of each
    new component we introduced. Our experimental results show that TWOLAR significantly
    enhances the document reranking ability of the underlying model, matching and
    in some cases even outperforming state-of-the-art models with three orders of
    magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation
    benchmark BEIR. To facilitate future work we release our data set, finetuned models,
    and code¹¹1Code: [https://github.com/Dundalia/TWOLAR](https://github.com/Dundalia/TWOLAR);'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models and Dataset: [https://huggingface.co/Dundalia](https://huggingface.co/Dundalia).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Information Retrieval Reranking Knowledge distillation Large Language Model.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text ranking, a foundational task in search engines and question-answering systems,
    involves ordering textual documents based on their relevance to a given query
    or context.
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art text rerankers are traditional cross-encoders like monoBERT [[27](#bib.bib27)],
    monoT5 [[28](#bib.bib28), [35](#bib.bib35)], and RankT5 [[44](#bib.bib44)], and
    more recently rerankers based on Large Language Models (LLMs) like RankGPT [[36](#bib.bib36)]
    and PRP [[31](#bib.bib31)].
  prefs: []
  type: TYPE_NORMAL
- en: Cross-encoders are computationally efficient but rely on costly human-annotated
    labels for training, while LLM-based methods bypass the need of in-domain fine-tuning.
    However, a significant downside is their substantial size and computational footprint,
    which could render them unsuitable for real-time inference. However, the knowledge
    of an LLM could be distilled to produce a student model with performance comparable
    to the teacher model, but a size several orders of magnitude smaller.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we present TWOLAR, a TWO-step LLM-Augmented distillation method
    for passage Reranking. The distillation consists in exploiting the capabilities
    of an LLM as a reranker to produce high-quality annotations. The annotations are
    applied to a dataset of queries generated artificially, either as cropped sentences
    or again by a specialized language model. In this way, we obtain a compact model
    that ranks among top performing supervised, zero-shot, and LLM-based distillation
    methods in various popular benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper is structured as follows: Section [2](#S2 "2 Background ‣ TWOLAR:
    a TWO-step LLM-Augmented distillation method for passage Reranking") provides
    background on text ranking methods and benchmarks. Section [3](#S3 "3 Approach
    ‣ TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking")
    details our approach, subdivided into scoring and distillation strategies. Section
    [4](#S4 "4 Experimental setup ‣ TWOLAR: a TWO-step LLM-Augmented distillation
    method for passage Reranking") covers the experimental setup, including datasets,
    training, baselines, and results. Section [5](#S5 "5 Discussion ‣ TWOLAR: a TWO-step
    LLM-Augmented distillation method for passage Reranking") discusses the results
    and ablation studies. Section [6](#S6 "6 Conclusion ‣ TWOLAR: a TWO-step LLM-Augmented
    distillation method for passage Reranking") concludes the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Formally, given a query and a passage from a large text collection, text ranking
    requires returning a ranked list of the $n$ most relevant texts according to the
    relevance scores of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The early text ranking methods relied mainly on statistical lexical features,
    like BM25 [[34](#bib.bib34)] and TF-IDF, and used heuristic methods for retrieval [[42](#bib.bib42)].
    Based on this scheme, each query-document relevance score is computed according
    to the lexical similarity. Subsequently, statistical language modeling has been
    widely explored for text ranking [[41](#bib.bib41)]. With the development of machine
    learning, supervised approaches, which still rely on hand-crafted features as
    well as lexical features, have been proposed [[22](#bib.bib22), [19](#bib.bib19)].
    Further progress was made with the adoption of neural networks mapping pieces
    of text into low-dimensional vectors to obtain better representations [[13](#bib.bib13),
    [25](#bib.bib25), [14](#bib.bib14)]. Recently, a new paradigm emerged [[9](#bib.bib9),
    [20](#bib.bib20), [42](#bib.bib42), [30](#bib.bib30), [43](#bib.bib43)], consisting
    of multiple stages: using a first-stage retriever that aims to reduce the candidate
    space by retrieving a subset of relevant candidates, often numbering in the hundreds
    or thousands, and then refining these initial results with a second-stage reranker.'
  prefs: []
  type: TYPE_NORMAL
- en: The advent of pretrained language models (PLMs) [[8](#bib.bib8), [33](#bib.bib33),
    [32](#bib.bib32)] and large-scale human annotated datasets [[26](#bib.bib26),
    [18](#bib.bib18), [40](#bib.bib40)] marked a significant advancement in the field.
    Models such as DPR [[17](#bib.bib17)], SPLADE [[12](#bib.bib12), [10](#bib.bib10),
    [11](#bib.bib11)], and DRAGON [[21](#bib.bib21)] emerged as powerful first-stage
    retrievers. Complementing these retrievers, models like monoBERT [[27](#bib.bib27)],
    monoT5 [[28](#bib.bib28), [35](#bib.bib35)], and RankT5 [[44](#bib.bib44)] have
    been conceived as second-stage rerankers, designed explicitly to refine and optimize
    the results provided by the initial retrieval stage.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, large language models (LLMs) have begun to play an influential role
    in text reranking [[43](#bib.bib43), [24](#bib.bib24)]. The latest approaches
    in text reranking utilize LLMs in various ways. For instance, InPars [[2](#bib.bib2)]
    and InParsV2 [[16](#bib.bib16)] leverage GPT-3 Curie [[3](#bib.bib3)] and GPT-J [[39](#bib.bib39)]
    respectively, for data augmentation, generating synthetic queries to adapt a reranking
    model to different reranking tasks and domains. Other approaches instead consist
    in directly prompting the LLM to permute a set of documents given a query. In
    this vein, RankGPT [[36](#bib.bib36)], LRL [[24](#bib.bib24)], and PRP [[31](#bib.bib31)]
    have demonstrated the potential of LLMs as zero-shot rerankers. Moreover, RankGPT [[36](#bib.bib36)]
    demonstrates how the ranking abilities of ChatGPT could be distilled into a more
    efficient DeBERTa [[15](#bib.bib15)]. For a comprehensive survey on LLMs for Information
    Retrieval, the interested readers can refer to [[43](#bib.bib43)].
  prefs: []
  type: TYPE_NORMAL
- en: As for the benchmarks, The largest annotated dataset for information retrieval
    is the MS MARCO passage reranking dataset [[26](#bib.bib26)]. It contains around
    530K train queries and 6.8K ‘dev’ queries. The corpus is composed of more than
    8.8M passages. For each query, relevant passages are annotated as 1 and others
    are annotated as 0\. TREC-DL2019 [[7](#bib.bib7)] and TREC-DL2020 [[6](#bib.bib6)]
    are standard benchmarks derived from MS MARCO that provide dense human relevance
    annotations for each of their 43 and 54 queries. BEIR [[37](#bib.bib37)] is a
    heterogeneous benchmark containing 18 retrieval datasets, covering different retrieval
    tasks and domains, designed for zero-shot evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: We believe that the potential of LLM distillation for text ranking has not been
    fully exploited yet. Our contribution aims at bridging this gap by the methodological
    construction of a training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our reranking model is based on Flan-T5 [[5](#bib.bib5)], which is a text-to-text
    model developed as an instructed version of T5 [[33](#bib.bib33)]. For our task,
    we use the following input template:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query: [$Q$] Relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: where [$Q$] are the query and document texts, respectively, similar to the one
    adopted in monoT5 [[28](#bib.bib28), [35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Scoring Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flan-T5 can be straightforwardly applied to various tasks due to its text-to-text
    nature, such as summarization, translation, and classification. However, adapting
    to the ranking task is not trivial, because for each query-document pair, we usually
    ask models to answer with a score representing the degree of relevance. The state-of-the-art
    rerankers, monoT5 [[28](#bib.bib28), [35](#bib.bib35)] and RankT5 [[44](#bib.bib44)],
    which are specialized text-to-text models, suffer from this limitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'MonoT5 finetunes T5 on a binary classification task: given a query-document
    pair, the model is optimized to produce the words ‘true’ if the document is relevant
    to the query and ‘false’ otherwise. At inference time, the ranking score is obtained
    from the logits of the ‘true’ and ‘false’ tokens as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s=\frac{e^{z_{\texttt{true}}}}{e^{z_{\texttt{true}}}+e^{z_{\texttt{false}}}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $z_{\tt true},z_{\tt false}$ are the logits of ‘true’ and ‘false’, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: RankT5 directly learns to rank by optimizing a ranking-based loss function.
    This family of loss functions requires the model to directly output the ranking
    score for each query-document pair at training time, so that the unnormalized
    logit of a special unused token (‘extra_id_10’) in the vocabulary is used as ranking
    score.
  prefs: []
  type: TYPE_NORMAL
- en: 'On one hand, monoT5 is not directly finetuned as a ranking model, which may
    not optimize its ranking performance. On the other hand, RankT5 does not exploit
    the learned representation in the language modeling head. To overcome both limitations,
    we propose a new approach. Our idea consists of using the difference between the
    unnormalized logits corresponding to the ‘true’ and ‘false’ tokens. In this way,
    the model is able to output a score directly at training time, and since it is
    optimized on top of the learned representations of the two tokens, we can make
    full use of the knowledge from the PLMs. An illustration of these scoring strategies
    is shown in Fig. [1](#S3.F1 "Figure 1 ‣ 3.1 Scoring Strategy ‣ 3 Approach ‣ TWOLAR:
    a TWO-step LLM-Augmented distillation method for passage Reranking").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/115cef0851189c211a38ba06800c4c9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of the score strategies from monoT5, RankT5 and our
    proposed approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, we adopt the RankNet loss [[4](#bib.bib4)], a pairwise loss function
    that models the probability of one document being more relevant than another given
    a query. RankNet loss has shown compelling results in information retrieval [[44](#bib.bib44),
    [36](#bib.bib36)] and provided a solid foundation for our optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Given a query $q$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we optimize our model with the following loss function measuring
    the correctness of relative passage orders:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: It should be noted that the adoption of different ranking loss functions could
    potentially lead to alternative outcomes, but exploring their potential differences
    is not the main purpose of this paper. Thus, we utilize the RankNet Loss here
    and leave the comparison of different ranking loss functions as future work.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Distillation strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our distillation strategy aims to capture the reranking capability of LLMs,
    in our case ChatGPT, through constructing a query-document dataset. The core design
    principle is the synthesis of suitable artificial queries by query augmentation,
    and the subsequent use of multiple retrieval models and stages of distillation.
  prefs: []
  type: TYPE_NORMAL
- en: Query Augmentation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our query augmentation method is inspired by DRAGON [[21](#bib.bib21)], whereby
    two approaches are combined to amplify the size of training queries from a given
    corpus: sentence cropping and pseudo query generation. The former can readily
    scale up the size of query sets without any computationally expensive operation.
    The latter generates high-quality human-like queries using LLMs. The combination
    of the two strategies increases the diversity of the dataset and accordingly the
    challenge and complexity of the task.'
  prefs: []
  type: TYPE_NORMAL
- en: Following DRAGON, we randomly sampled 10K queries from DRAGON’s collection of
    cropped sentences, drawn from the MS MARCO corpus [[26](#bib.bib26)]. Simultaneously,
    we sampled an additional 10K queries from the query pool created by docT5query [[29](#bib.bib29)],
    a specialized T5 model that generates queries based on a given passage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1064abe4bb8ba06d9876fa37ebea3714.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of the method used to build the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First-stage distillation: retrieval.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The initial phase of our distillation process (see Fig. [2](#S3.F2 "Figure
    2 ‣ Query Augmentation. ‣ 3.2 Distillation strategy ‣ 3 Approach ‣ TWOLAR: a TWO-step
    LLM-Augmented distillation method for passage Reranking")) involves splitting
    each of the two sets of 10K queries, one set composed of cropped sentences and
    another set composed of docT5query-generated queries, into four subsets of 2.5K
    queries each. To retrieve documents for these queries, we chose four distinct
    retrieval models:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BM25 [[34](#bib.bib34)]: A state-of-the-art bag-of-words approach that relies
    primarily on word overlap to match documents to queries. Consequently, its hard
    negatives are expected to challenge the language model on lexical-level matches.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DRAGON [[21](#bib.bib21)]: A dense retrieval model designed to detect semantic
    similarity between queries and passages. It pushes the language model towards
    understanding deeper semantic relations and contexts. We have chosen the DRAGON+
    version.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SPLADE [[12](#bib.bib12), [11](#bib.bib11), [10](#bib.bib10)]: It serves as
    a kind of midpoint between BM25’s focus on word overlap and DRAGON’s emphasis
    on semantic similarity. It introduces a different level of complexity by considering
    interactions between the tokens of the document or query and all the tokens from
    the vocabulary. We have chosen the SPLADE++ version.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'monoT5 [[28](#bib.bib28), [35](#bib.bib35)]: A combination of BM25 and monoT5
    where the top-100 documents retrieved by BM25 are re-ranked using monoT5\. It
    introduces negatives that are influenced by the ranking capabilities of a cross-encoder.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In all cases, we retrieve the top 30 documents for each query.
  prefs: []
  type: TYPE_NORMAL
- en: This methodology is designed to provide high-quality results and to diversify
    the types of challenges and contexts presented to ChatGPT in the subsequent distillation
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S3.T1 "Table 1 ‣ First-stage distillation: retrieval. ‣ 3.2 Distillation
    strategy ‣ 3 Approach ‣ TWOLAR: a TWO-step LLM-Augmented distillation method for
    passage Reranking") gives a quantitative account of the diversity of the documents
    retrieved by the four distinct models by computing the intersection rate between
    the sets of documents obtained from any two sources of supervision.²²2The average
    intersection rates were then calculated to provide a comprehensive view of the
    overall overlap among the retrieved documents from all sources: $\displaystyle\frac{1}{|\mathcal{Q}|}\sum_{q\in\mathcal{Q}}\frac{|S^{1}_{q}\cap
    S^{2}_{q}|}{N}$. This process was carried out separately for both types of queries:
    the cropped sentence queries and the docT5query-generated queries. The low mean
    intersection rates between different sources provide a clear evidence of the diversity
    among the retrieved document sets for both types of queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Average intersection rate between each pair of sources. The upper
    triangular part of the table represents the intersection rate for cropped sentences
    and the lower triangular part represents the intersection rate for docT5query-generated
    queries.'
  prefs: []
  type: TYPE_NORMAL
- en: '| doct5query \ cropped sentence (%) | BM25 | SPLADE | DRAGON | monoT5 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | \ | 20.0 | 29.0 | 49.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SPLADE | 17.8 | \ | 35.8 | 26.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DRAGON | 25.0 | 41.0 | \ | 38.4 |'
  prefs: []
  type: TYPE_TB
- en: '| monoT5 | 46.4 | 27.2 | 38.5 | \ |'
  prefs: []
  type: TYPE_TB
- en: 'Second-stage distillation: reranking.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For reranking we used ChatGPT, in particular the checkpoint ‘gpt-3.5-turbo-16k-0613’.
    We prompted the model with the same prompt design proposed by RankGPT [[36](#bib.bib36)],
    including all the 30 documents per query to rerank in a single prompt.
  prefs: []
  type: TYPE_NORMAL
- en: We used the prompt made available by the RankGPT public repository. ³³3[https://github.com/sunnweiwei/RankGPT](https://github.com/sunnweiwei/RankGPT)
    We fed each of the 20K queries and their corresponding top 30 retrieved documents
    to ChatGPT, asking it to provide permutations of the indices of these documents,
    ordered according to their relevance to the associated query.
  prefs: []
  type: TYPE_NORMAL
- en: This approach requires significant computational resources due to the complexity
    of the task and the vast number of queries and documents involved. However, the
    total cost of this reranking operation using the ChatGPT API amounted to $212,
    demonstrating the feasible financial aspect of employing a large-scale language
    model in creating such a diverse and complex dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Train-Validation split.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We divided the dataset into training and validation splits. We included 1,000
    queries in the validation set: 500 queries generated by docT5query and the rest
    extracted as cropped sentences. The remaining 19,000 samples were allocated to
    the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate our approach using TREC-DL2019, TREC-DL2020 and BEIR for the zero-shot
    evaluation. All comparisons on TREC-DL2019 and TREC-DL2020 are based on the reranking
    of top-100 passages retrieved by BM25 [[34](#bib.bib34)] for each query. The evaluation
    on the BEIR benchmark is based on the reranking of the top-100 passages retrieved
    by three different retrievers: BM25, SPLADE++ [[11](#bib.bib11)], and DRAGON+ [[21](#bib.bib21)].
    The objective is to evaluate the adaptability of the rerankers to different retrievers.
    We also present the evaluation by reranking the top-1000 documents retrieved by
    BM25, to give a broad view of the performances in different settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We initialized our model with pretrained Flan-T5-xl checkpoint [[5](#bib.bib5)].
    We set the maximum input sequence length to 500 as monoT5\. The batch size is
    set to $32$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate our model on the TREC-DL2019 and TREC-DL2020 competitions against
    several baselines including three supervised methods: monoBERT [[27](#bib.bib27)],
    monoT5-3B [[28](#bib.bib28), [35](#bib.bib35)], RankT5 [[44](#bib.bib44)]; two
    zero-shot LLM-based methods: the listwise prompting based approach of RankGPT [[36](#bib.bib36)],
    using both gpt-3.5-turbo and gpt-4, and the sliding window approach performed
    only for 10 passes of PRP [[31](#bib.bib31)], using Flan-T5-xl (3B), Flan-T5-xxl
    (11B) and Flan-UL2 (20B); and a representative distilled model based on DeBERTav3
    proposed in [[36](#bib.bib36)] as the only other LLM distillation method other
    than ours. Regarding the zero-shot evaluation on the BEIR benchmark, we evaluate
    our models comparing with three different rerankers, including InParsV2 [[2](#bib.bib2),
    [16](#bib.bib16)], monoT5-3B [[28](#bib.bib28), [35](#bib.bib35)], and the distilled
    DeBERTav3 model proposed in [[36](#bib.bib36)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our results on the TREC-DL2019 and TREC-DL2020 benchmarks are summarized in
    Table [2](#S4.T2 "Table 2 ‣ 4.4 Results ‣ 4 Experimental setup ‣ TWOLAR: a TWO-step
    LLM-Augmented distillation method for passage Reranking"). Tables [3](#S4.T3 "Table
    3 ‣ 4.4 Results ‣ 4 Experimental setup ‣ TWOLAR: a TWO-step LLM-Augmented distillation
    method for passage Reranking") and [4](#S4.T4 "Table 4 ‣ 4.4 Results ‣ 4 Experimental
    setup ‣ TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking")
    instead summarize respectively the results on the BEIR benchmark by reranking
    the top-100 and the top-1000 documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Results on TREC-DL2019 and TREC-DL2020 datasets by reranking top 100
    documents retrieved by BM25\. The column titled ‘#Calls’ indicates the exact number
    of inference times of LLM when reranking the top 100 documents. The ‘Input Size’
    column uses the notation $|q|+n|d|$ documents. Best model is highlighted in boldface
    and the second best is underlined for each metric. All the reported results apart
    from the LLM distillation Methods are taken from the original papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | LLM | Size | #Calls | Input Size | TREC-DL2019 | TREC-DL2020 |'
  prefs: []
  type: TYPE_TB
- en: '|  | nDCG@1 | nDCG@5 | nDCG@10 | nDCG@1 | nDCG@5 | nDCG@10 |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | - | - | - | - | 54.26 | 52.78 | 50.58 | 57.72 | 50.67 | 47.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised Methods |'
  prefs: []
  type: TYPE_TB
- en: '| monoBERT | BERT | 340M | 100 | $\leavevmode\nobreak\ &#124;q&#124;+&#124;d&#124;$
    | 79.07 | 73.25 | 70.50 | 78.70 | 70.74 | 67.28 |'
  prefs: []
  type: TYPE_TB
- en: '| monoT5 | T5-xl | 3B | 100 | $\leavevmode\nobreak\ &#124;q&#124;+&#124;d&#124;$
    | 79.07 | 73.74 | 71.83 | 80.25 | 72.32 | 68.89 |'
  prefs: []
  type: TYPE_TB
- en: '| RankT5 | T5-xl | 3B | 100 | $\leavevmode\nobreak\ &#124;q&#124;+&#124;d&#124;$
    | 77.38 | 73.94 | 71.22 | \ul80.86 | 72.99 | 69.49 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM distillation Methods |'
  prefs: []
  type: TYPE_TB
- en: '| RankGPT | DeBertaV2 | 184M | 100 | $\leavevmode\nobreak\ &#124;q&#124;+&#124;d&#124;$
    | 78.68 | 69.77 | 66.56 | 59.26 | 59.83 | 59.43 |'
  prefs: []
  type: TYPE_TB
- en: '| TWOLAR-large | Flan-T5-large | 783M | 100 | $\leavevmode\nobreak\ &#124;q&#124;+&#124;d&#124;$
    | 79.84 | 75.94 | 72.82 | 79.94 | 71.35 | 67.61 |'
  prefs: []
  type: TYPE_TB
- en: '| TWOLAR-xl | Flan-T5-xl | 3B | 100 | $\leavevmode\nobreak\ &#124;q&#124;+&#124;d&#124;$
    | 78.29 | \ul76.71 | \ul73.51 | 80.25 | 73.73 | 70.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot LLM Methods |'
  prefs: []
  type: TYPE_TB
- en: '| RankGPT | gpt-3.5-turbo | 154B^∗ | 10 | $\leavevmode\nobreak\ &#124;q&#124;+20&#124;d&#124;$
    | \ul82.17 | 71.15 | 65.80 | 79.32 | 66.76 | 62.91 |'
  prefs: []
  type: TYPE_TB
- en: '| RankGPT | gpt-4 | 1T^∗ | 2^† | $\leavevmode\nobreak\ &#124;q&#124;+20&#124;d&#124;$
    | 82.56 | 79.16 | 75.59 | 78.40 | \ul74.11 | \ul70.56 |'
  prefs: []
  type: TYPE_TB
- en: '| PRP-Sliding-10 | Flan-T5-xl | 3B | 990 | $\leavevmode\nobreak\ &#124;q&#124;+2&#124;d&#124;$
    | 75.58 | 71.23 | 68.66 | 75.62 | 69.00 | 66.59 |'
  prefs: []
  type: TYPE_TB
- en: '| PRP-Sliding-10 | Flan-T5-xxl | 11B | 990 | $\leavevmode\nobreak\ &#124;q&#124;+2&#124;d&#124;$
    | 64.73 | 69.49 | 67.00 | 75.00 | 70.76 | 67.35 |'
  prefs: []
  type: TYPE_TB
- en: '| PRP-Sliding-10 | Flan-UL2 | 20B | 990 | $\leavevmode\nobreak\ &#124;q&#124;+2&#124;d&#124;$
    | 78.29 | 75.49 | 72.65 | 85.80 | 75.35 | 70.46 |'
  prefs: []
  type: TYPE_TB
- en: '| ^∗ OpenAI has not publicly released the amount of parameters and the numbers
    are based on public estimates [[38](#bib.bib38)] [[1](#bib.bib1)]. |'
  prefs: []
  type: TYPE_TB
- en: '| ^† In [[36](#bib.bib36)], gpt-4 reranks the top-30 passages reranked by gpt-3.5-turbo.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Results on the BEIR Benchmark by reranking the top 100 documents with
    different retrievers. Best model is in boldface and second best is underlined
    for each dataset. Evaluation for InPars on CQADupStack is missing due to its unavailability
    on the Hugging Face hub. We computed statistical tests comparing our biggest model
    against the baselines. The results revealed no significant difference compared
    to InPars $(p=0.477)$. It’s noteworthy that while our model operates in a zero-shot
    manner, the InPars models have been fine-tuned for each BEIR dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Retriever | BM25 | SPLADE | DRAGON |'
  prefs: []
  type: TYPE_TB
- en: '|  Reranker  | - |  MonoT5-3B  |  InPars  |  RankGPT-Deberta  |  TWOLAR-xl  |  TWOLAR-large  |
    - |  MonoT5-3B  |  InPars  |  RankGPT-Deberta  |  TWOLAR-xl  |  TWOLAR-large  |
    - |  MonoT5-3B  |  InPars  |  RankGPT-Deberta  |  TWOLAR-xl  |  TWOLAR-large  |'
  prefs: []
  type: TYPE_TB
- en: '| nDCG@10 |'
  prefs: []
  type: TYPE_TB
- en: '| TREC-COVID | 59.5 | 79.8 | 82.5 | 79.4 | 82.7 | 84.3 | 72.8 | 82.9 | 85.7
    | 80.1 | 85.2 | 86.9 | 75.8 | 82.8 | 84.8 | 82.6 | 84.6 | \ul86.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NFCorpus | 32.2 | 37.4 | 35.0 | 33.3 | 36.6 | 35.7 | 34.8 | 39.2 | 38.8 |
    33.2 | 37.3 | 35.5 | 33.9 | 39.7 | \ul39.3 | 33.2 | 37.9 | 35.7 |'
  prefs: []
  type: TYPE_TB
- en: '| FiQA-2018 | 23.6 | 46.1 | 46.2 | 32.7 | 41.9 | 41.1 | 34.8 | 50.0 | 50.0
    | 33.7 | 44.8 | 43.8 | 35.7 | 51.2 | \ul50.9 | 43.1 | 45.3 | 44.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ArguAna | 30.0 | 33.4 | 32.8 | 21.1 | 32.9 | 34.7 | 38.8 | 31.7 | 31.2 |
    18.6 | 32.9 | 34.6 | 46.9 | 41.5 | 40.9 | 25.7 | 42.8 | \ul45.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Tóuche-2020 | 44.2 | 31.6 | 29.6 | 37.7 | 37.1 | 33.4 | 24.6 | 29.8 | 28.7
    | 36.4 | 35.2 | 30.4 | 26.3 | 30.6 | 29.4 | \ul38.2 | 36.0 | 31.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Quora | 78.9 | 84.1 | 84.8 | 78.8 | 87.2 | 86.0 | 83.5 | 84.3 | 85.1 | 80.3
    | \ul87.4 | 86.0 | 87.5 | 83.5 | 84.4 | 78.7 | 87.2 | 85.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SCIDOCS | 14.9 | 19.0 | 19.2 | 16.1 | 19.5 | 18.3 | 15.9 | 19.9 | 20.9 |
    16.4 | 20.2 | 18.8 | 15.9 | 19.8 | \ul20.7 | 16.4 | 20.2 | 18.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SciFact | 67.9 | \ul76.4 | 73.5 | 70.5 | 76.5 | 75.6 | 70.2 | \ul76.4 | 76.0
    | 69.1 | 75.6 | 74.7 | 67.8 | 76.0 | 75.7 | 69.4 | 75.6 | 74.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NQ | 30.6 | 56.8 | 57.8 | 46.1 | 58.0 | 57.7 | 53.7 | 65.9 | 66.4 | 50.6
    | \ul66.8 | 65.8 | 53.8 | 65.1 | 66.6 | 50.6 | 66.9 | 66.2 |'
  prefs: []
  type: TYPE_TB
- en: '| HotpotQA | 63.3 | 74.2 | 76.5 | 69.9 | 76.7 | 75.9 | 68.7 | 74.1 | \ul77.1
    | 70.5 | 77.7 | 76.4 | 66.2 | 72.9 | 75.7 | 69.8 | 76.4 | 75.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | 31.8 | 44.8 | 44.0 | 41.9 | 48.0 | 47.8 | 43.6 | 48.2 | 51.1 |
    45.9 | 52.9 | 51.6 | 41.9 | 47.2 | 50.3 | 44.9 | \ul52.1 | 51.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FEVER | 65.1 | 83.2 | 85.5 | 80.2 | 84.9 | 83.4 | 79.3 | 85.0 | 88.0 | 81.8
    | \ul87.5 | 85.4 | 78.0 | 84.7 | 87.7 | 81.7 | 87.2 | 85.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Climate-FEVER | 16.5 | 27.4 | 30.1 | 24.2 | 26.9 | 26.1 | 22.9 | 28.7 | 32.8
    | 25.9 | 28.9 | 27.9 | 22.7 | 28.6 | \ul32.5 | 25.9 | 28.6 | 27.4 |'
  prefs: []
  type: TYPE_TB
- en: '| CQADupStack | 30.2 | 41.5 | - | 34.7 | 41.2 | 40.6 | 33.4 | 43.7 | - | 35.9
    | 43.6 | 42.7 | 35.4 | 44.4 | - | 36.0 | \ul44.2 | 43.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Robust04 | 40.8 | 56.6 | 58.7 | 52.8 | 57.9 | 58.3 | 46.7 | 62.1 | 64.3 |
    57.3 | 64.9 | 65.2 | 48.1 | 61.3 | \ul63.2 | 56.6 | 63.4 | 63.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Signal-1M | 33.1 | 32.2 | 32.9 | \ul33.4 | 33.8 | 33.9 | 30.0 | 29.4 | 30.3
    | 30.0 | 30.1 | 30.5 | 30.0 | 29.7 | 30.4 | 29.4 | 30.2 | 30.1 |'
  prefs: []
  type: TYPE_TB
- en: '| BioASQ | 52.3 | \ul57.2 | 59.8 | 53.0 | 56.2 | 56.0 | 49.7 | 54.1 | 57.2
    | 49.5 | 54.6 | 53.8 | 43.4 | 51.9 | 54.4 | 48.0 | 51.9 | 50.8 |'
  prefs: []
  type: TYPE_TB
- en: '| TREC-NEWS | 39.5 | 48.5 | 49.8 | 51.8 | 52.7 | 50.8 | 41.5 | 50.0 | 50.9
    | \ul53.4 | 53.3 | 50.7 | 44.4 | 49.5 | 50.8 | 52.1 | 53.8 | 50.0 |'
  prefs: []
  type: TYPE_TB
- en: '| avg nDCG@10 |'
  prefs: []
  type: TYPE_TB
- en: '| BEIR 18 | 41.9 | 51.7 | - | 47.6 | 52.8 | 52.2 | 46.9 | 53.0 | - | 48.3 |
    \ul54.4 | 53.4 | 47.4 | 53.4 | - | 48.5 | 54.7 | 53.7 |'
  prefs: []
  type: TYPE_TB
- en: '| BEIR 17 | 42.6 | 52.3 | 52.9 | 48.4 | 53.5 | 52.9 | 47.8 | 53.5 | 55.0 |
    49.0 | 55.0 | 54.0 | 48.1 | 53.9 | \ul55.2 | 49.2 | 55.3 | 54.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results on the BEIR Benchmark by reranking the top 1000 BM25 retrieved
    documents. The best model is highlighted in boldface, and the second best is underlined
    for each dataset. All results, apart from TWOLAR-xl, are from [[16](#bib.bib16)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BM25 | monoT5-3B | InPars-v2 | RankT5 | TWOLAR-xl |'
  prefs: []
  type: TYPE_TB
- en: '| nDCG@10 |'
  prefs: []
  type: TYPE_TB
- en: '| TREC-COVID | 59.5 | 80.1 | 84.6 | 82.3 | \ul84.3 |'
  prefs: []
  type: TYPE_TB
- en: '| NFCorpus | 32.2 | 38.3 | \ul38.5 | 39.9 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FiQA-2018 | 23.6 | 50.9 | \ul50.9 | 49.3 | 45.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ArguAna | 30.0 | \ul37.9 | 36.9 | 40.6 | 32.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Tóuche-2020 | \ul44.2 | 30.9 | 29.1 | 48.6 | 35.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Quora | 78.9 | 83.5 | \ul84.5 | 81.9 | 87.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SCIDOCS | 14.9 | 19.7 | 20.8 | 19.1 | \ul20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SciFact | 67.9 | 77.4 | \ul77.4 | 76.0 | 76.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NQ | 30.6 | 62.5 | 63.8 | 64.7 | \ul64.2 |'
  prefs: []
  type: TYPE_TB
- en: '| HotpotQA | 63.3 | 76.0 | \ul79.1 | 75.3 | 79.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | 31.8 | 47.2 | \ul49.8 | 45.9 | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| FEVER | 65.1 | 84.8 | 87.2 | 84.8 | \ul86.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Climate-FEVER | 16.5 | \ul28.8 | 32.3 | 27.5 | 27.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CQADupStack | 30.2 | 44.9 | \ul44.8 | - | 43.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Robust04 | 40.8 | 61.5 | \ul63.2 | - | 64.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Signal-1M | 33.1 | 30.2 | 30.8 | \ul31.9 | 31.5 |'
  prefs: []
  type: TYPE_TB
- en: '| BioASQ | 52.3 | 56.6 | 59.5 | \ul57.9 | 56.0 |'
  prefs: []
  type: TYPE_TB
- en: '| TREC-NEWS | 39.5 | 47.7 | \ul49.0 | - | 53.2 |'
  prefs: []
  type: TYPE_TB
- en: '| avg nDCG@10 |'
  prefs: []
  type: TYPE_TB
- en: '| BEIR 18 | 41.9 | 53.3 | 54.5 | - | \ul54.4 |'
  prefs: []
  type: TYPE_TB
- en: '| BEIR 15 | 42.9 | 53.7 | \ul54.9 | 55.0 | 54.5 |'
  prefs: []
  type: TYPE_TB
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, We will delve into a comprehensive discussion of our experimental
    results, and in the following section, we will explore the ablation study in detail.
  prefs: []
  type: TYPE_NORMAL
- en: On TREC-DL.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our evaluation on TREC-DL2019 and TREC-DL2020, our model demonstrated outstanding
    performance, consistently outperforming established supervised methods and LLM-distilled
    baselines. When set against zero-shot LLM baselines, our model either matches
    or exceeds their performance. Although the results are not directly comparable
    because we are using a specific checkpoint, we find it remarkable that our model
    outperforms even the teacher LLM used for the distillation process, i.e. gpt-3.5-turbo.
    We take it as an indication that our distillation strategy is well conceived.
    The sole model that distinctly outperformed ours was gpt-4. This performance difference
    suggests that leveraging a more advanced LLM for distillation within our methodology
    might lead to even superior outcomes. Importantly, this is achieved with significantly
    reduced computational overhead during inference since we distilled LLMs to obtain
    a much smaller task-specific model. It is worthwhile noticing the difference in
    size between the models used for comparison with TWOLAR. Remarkably the largest
    of the TWOLAR models is several orders of magnitude smaller than the largest RankGPT
    model.
  prefs: []
  type: TYPE_NORMAL
- en: On BEIR Benchmark.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our evaluations using the BEIR benchmark, TWOLAR consistently outperformed
    most existing baselines when reranking the top-100 documents, as shown in Table
    [3](#S4.T3 "Table 3 ‣ 4.4 Results ‣ 4 Experimental setup ‣ TWOLAR: a TWO-step
    LLM-Augmented distillation method for passage Reranking"). This is particularly
    significant when compared with the approach of models such as InPars. InPars employs
    a strategy of fine-tuning a monot5-3B on generated, topic-specific data tailored
    for each of the 18 datasets within the BEIR benchmark. This strategy means that,
    for each dataset, their model has been exposed to data related to the topic in
    question. In contrast, TWOLAR has never been exposed to any topic-specific data,
    making it genuinely zero-shot when facing new topics and tasks. Furthermore, our
    method does not require fine-tuning for different applications and is thus more
    economical.'
  prefs: []
  type: TYPE_NORMAL
- en: It is worthwhile noticing the performance variations across different datasets
    within BEIR. In datasets with a specific focus, such as BioASQ, InPars tends to
    perform better due to its targeted fine-tuning on artificial topic-specific data.
    However, in datasets where queries are centered around general knowledge, like
    DBpedia entity, TWOLAR demonstrates a clear advantage over InPars. This highlights
    the robustness of TWOLAR’s topic-agnostic approach and its applicability in a
    broad range of scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'When reranking the top-1000 documents (Table [4](#S4.T4 "Table 4 ‣ 4.4 Results
    ‣ 4 Experimental setup ‣ TWOLAR: a TWO-step LLM-Augmented distillation method
    for passage Reranking")), our model did not perform as well as when reranking
    the top-100 documents. However, the difference with the best performing model
    on BEIR 18 is minor (54.4 vs 54.5) and TWOLAR-xl outperforms every competitor
    in 5 out of 18 tasks. A possible explanation for this result lies in our model’s
    training setup. Since our method optimizes for reranking a subset of 30 documents,
    it seems plausible that it can easily scale up to 100 documents, less easily to
    1000.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: We perform ablation studies on the eight smallest datasets in BEIR
    benchmark. The reported scores are nDCG@10\. In comparing our scoring strategy
    against the RankT5 scoring strategy, the statistical tests yielded a p-value of
    $p=\text{0.268}$. *COV: TREC-COVID, SCI: SciFact, NFC: NFCorpus, TOU: Tóuche-2020,
    DBP: DBPedia, ROB: Robust04, SIG: Signal-1M, NEW: TREC-NEWS.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | COV | SCI | NFC | TOU | DBP | ROB | SIG | NEW | avg |'
  prefs: []
  type: TYPE_TB
- en: '| Score Strategy | effectiveness of score strategy - 19K train samples |'
  prefs: []
  type: TYPE_TB
- en: '| Difference | 74.0 | 67.9 | 31.9 | 35.7 | 38.8 | 47.4 | 32.5 | 43.7 | 46.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| RankT5 | 74.1 | 69.2 | 31.5 | 32.2 | 36.2 | 47.0 | 34.1 | 41.2 | 45.7 |'
  prefs: []
  type: TYPE_TB
- en: '| # documents | effectiveness of amount of documents - 19K train samples |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 74.0 | 67.9 | 31.9 | 35.7 | 38.8 | 47.4 | 32.5 | 43.7 | 46.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 73.0 | 69.8 | 32.5 | 31.7 | 37.9 | 47.5 | 31.9 | 40.8 | 46.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 72.3 | 65.6 | 29.6 | 28.4 | 34.2 | 43.2 | 30.4 | 37.9 | 42.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Not used source | effectiveness of first source of supervision - $\sim$14K
    train samples |'
  prefs: []
  type: TYPE_TB
- en: '| - BM25 | 72.7 | 70.3 | 31.8 | 31.8 | 37.7 | 47.0 | 31.9 | 41.5 | 45.6 |'
  prefs: []
  type: TYPE_TB
- en: '| - SPLADE | 73.9 | 70.9 | 33.6 | 32.7 | 38.6 | 48.8 | 32.4 | 42.5 | 46.3 |'
  prefs: []
  type: TYPE_TB
- en: '| - DRAGON | 74.0 | 67.7 | 32.9 | 33.9 | 37.6 | 47.7 | 33.1 | 43.2 | 46.2 |'
  prefs: []
  type: TYPE_TB
- en: '| - monoT5 | 73.9 | 69.4 | 31.8 | 33.0 | 36.3 | 46.6 | 32.5 | 43.6 | 45.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Type of query | effectiveness of type of query - 9.5K train samples |'
  prefs: []
  type: TYPE_TB
- en: '| Mixed | 75.5 | 67.3 | 30.4 | 34.0 | 37.2 | 46.2 | 31.8 | 41.6 | 45.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence | 67.2 | 67.9 | 31.4 | 32.7 | 32.2 | 44.8 | 31.7 | 39.7 | 43.4 |'
  prefs: []
  type: TYPE_TB
- en: '| docT5query | 74.6 | 59.4 | 31.2 | 33.4 | 37.8 | 44.9 | 28.1 | 44.0 | 44.2
    |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Ablation study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted an extensive ablation study in order to validate our design choices.
    Due to computational constraints, these experiments were performed using the smaller
    flan-t5-small checkpoint, with 77M parameters. Furthermore, we evaluated the models
    on reranking the top-100 documents retrieved by BM25 from a subset of 8 smallest
    datasets from the BEIR benchmark, including TREC-COVID, SciFact, NFCorpus, Tóuche-2020,
    DBPedia, Robust04, Signal-1M, and TREC-NEWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are summarized in Table [5](#S5.T5 "Table 5 ‣ On BEIR Benchmark.
    ‣ 5 Discussion ‣ TWOLAR: a TWO-step LLM-Augmented distillation method for passage
    Reranking").'
  prefs: []
  type: TYPE_NORMAL
- en: Scoring Strategy Effectiveness.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our scoring strategy achieved an average nDCG@10 of 46.5, showing superior
    performance compared to 45.7 for the RankT5 scoring approach, which indicates
    the importance of properly exploiting the knowledge from the language modeling
    head of PLMs. We did not make a direct comparison with the softmax method used
    in monoT5, due to the inherent differences in the pipeline structure: while our
    method and the RankT5 method allow for direct finetuning of the model to rank
    documents, the monoT5 approach operates on a fundamentally different mechanism,
    making a direct comparative analysis less feasible and potentially misleading
    in evaluating the distinct methodologies.'
  prefs: []
  type: TYPE_NORMAL
- en: Documents per training samples.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We trained models with varying numbers of documents per training sample. Our
    results suggest a clear advantage in using more than 10 documents per sample.
    The trade-off between 20 and 30 is less clear, with nDCG@10 scores of 46.3 and
    46.5 respectively, suggesting diminishing returns beyond 20 documents for the
    top-100 reranking task.
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of first source of supervision.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conducted four individual experiments by excluding each source of supervision
    (BM25, SPLADE, DRAGON, monoT5) from the training set and training the model on
    the residual data. This allowed us to evaluate the individual contribution of
    each retrieval strategy to the overall performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Our results demonstrate that BM25, even being a traditional bag-of-words method,
    still plays a critical role in the model’s performance. This result may be also
    due to the fact that, following standard practice, during the test the top-100
    documents have been retrieved using BM25 itself. Conversely, when SPLADE and DRAGON
    were excluded during training, the performance drop was not substantial, which
    suggests that the main contribution comes from blending lexical and semantic models
    rather than including multiple and possibly equivalent semantic models.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Query Type.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We also trained models exclusively on cropped sentences, docT5query generated
    queries, and a mixed subset of both types. The model trained only with docT5query
    generated queries, which are formulated as natural language questions, had a higher
    average performance than the model trained only on cropped sentences. This suggests
    that training with grammatically correct questions is more important.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, for datasets where the queries were predominantly formed as ‘what’
    or ‘how’ questions, such as TREC-COVID, the model trained on docT5query queries
    delivered a strongly superior performance. Conversely, the model trained with
    cropped sentences performed better in specific datasets where the queries are
    not expressed as a question. For example, the queries in SciFact are expert-written
    claims, aiming to find evidence in annotated abstracts. Here, the model trained
    with cropped sentences achieved an nDCG@10 score of 67.9, significantly outperforming
    the model trained with docT5query queries, which scored 59.4.
  prefs: []
  type: TYPE_NORMAL
- en: When we trained the model on a mixed subset comprising an equal proportion of
    both query types, it exhibited the best overall performance. This highlights the
    benefit of a diverse training regimen incorporating natural questions (docT5query)
    and sentences cropped directly from documents.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, these results of the ablation study underscore the value of our
    proposed scoring strategy, the importance of incorporating sufficient documents
    per training sample, the significant contribution of BM25 as a supervision source,
    and the advantages of a mixed query approach.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paradigm shift, enabled by LLMs, suggests that traditional methods relying
    heavily on handcrafted labeled data might no longer be the most effective or efficient
    approach for certain machine learning tasks. Indeed, as LLMs continue to showcase
    their prowess, there is a promising realization that they can be harnessed to
    provide the needed supervision, reducing the need for manual data labeling. However,
    tasks that demand efficiency, such as information retrieval, often cannot deploy
    LLMs directly due to their substantial computational overhead. In such scenarios,
    distillation enables the retention of the LLM’s capabilities in a more computationally
    amenable format.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we presented a novel two-step LLM-augmented distillation approach
    for passage reranking. Our method capitalizes on the strengths of LLMs to enable
    computationally efficient information retrieval systems, with performance comparable
    or even superior to that of state-of-the-art baselines and a reduction in size
    by several orders of magnitude. Our experiments, conducted across various benchmarks,
    demonstrate robustness and generality of our approach across domains. An ablation
    offers further insight about the crucial elements of our architectural design.
  prefs: []
  type: TYPE_NORMAL
- en: Looking forward, TWOLAR offers promising avenues for scalability. In the future,
    we plan to further our experimentation by substituting the 3B model with an 11B
    version, expanding the number of queries, increasing the sources of supervision,
    or even refining the quality of the LLM used for distillation, for example by
    experimenting with more powerful generative language models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Baktash, J.A., Dawodi, M.: Gpt-4: A review on advancements and opportunities
    in natural language processing (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Bonifacio, L., Abonizio, H., Fadaee, M., Nogueira, R.: InPars: Unsupervised
    dataset generation for information retrieval. In: Proceedings of the 45th International
    ACM SIGIR Conference on Research and Development in Information Retrieval. p.
    2387–2392\. SIGIR ’22, Association for Computing Machinery, New York, NY, USA
    (2022). https://doi.org/10.1145/3477495.3531863, [https://doi.org/10.1145/3477495.3531863](https://doi.org/10.1145/3477495.3531863)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models
    are few-shot learners. In: Proceedings of the 34th International Conference on
    Neural Information Processing Systems. NIPS’20, Curran Associates Inc., Red Hook,
    NY, USA (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N.,
    Hullender, G.: Learning to rank using gradient descent. In: Proceedings of the
    22nd International Conference on Machine Learning. p. 89–96\. ICML ’05, Association
    for Computing Machinery, New York, NY, USA (2005). https://doi.org/10.1145/1102351.1102363,
    [https://doi.org/10.1145/1102351.1102363](https://doi.org/10.1145/1102351.1102363)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y.,
    Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M.,
    Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D.,
    Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov,
    S., Chi, E.H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V., Wei, J.:
    Scaling instruction-finetuned language models (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Craswell, N., Mitra, B., Yilmaz, E., Campos, D.: Overview of the trec 2020
    deep learning track (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Craswell, N., Mitra, B., Yilmaz, E., Campos, D., Voorhees, E.M.: Overview
    of the trec 2019 deep learning track (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of
    deep bidirectional transformers for language understanding. In: Proceedings of
    the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp.
    4171–4186\. Association for Computational Linguistics, Minneapolis, Minnesota
    (Jun 2019). https://doi.org/10.18653/v1/N19-1423, [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Fan, Y., Xie, X., Cai, Y., Chen, J., Ma, X., Li, X., Zhang, R., Guo, J.,
    et al.: Pre-training methods in information retrieval. Foundations and Trends®
    in Information Retrieval 16(3), 178–317 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Formal, T., Lassance, C., Piwowarski, B., Clinchant, S.: Splade v2: Sparse
    lexical and expansion model for information retrieval. arXiv preprint arXiv:2109.10086
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Formal, T., Lassance, C., Piwowarski, B., Clinchant, S.: From distillation
    to hard negative sampling: Making sparse neural ir models more effective. In:
    Proceedings of the 45th International ACM SIGIR Conference on Research and Development
    in Information Retrieval. pp. 2353–2359 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Formal, T., Piwowarski, B., Clinchant, S.: Splade: Sparse lexical and
    expansion model for first stage ranking. In: Proceedings of the 44th International
    ACM SIGIR Conference on Research and Development in Information Retrieval. p.
    2288–2292\. SIGIR ’21, Association for Computing Machinery, New York, NY, USA
    (2021). https://doi.org/10.1145/3404835.3463098, [https://doi.org/10.1145/3404835.3463098](https://doi.org/10.1145/3404835.3463098)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Guo, J., Fan, Y., Ai, Q., Croft, W.B.: A deep relevance matching model
    for ad-hoc retrieval. In: Proceedings of the 25th ACM International on Conference
    on Information and Knowledge Management. ACM (oct 2016). https://doi.org/10.1145/2983323.2983769,
    [https://doi.org/10.1145%2F2983323.2983769](https://doi.org/10.1145%2F2983323.2983769)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Guo, J., Fan, Y., Pang, L., Yang, L., Ai, Q., Zamani, H., Wu, C., Croft,
    W.B., Cheng, X.: A deep look into neural ranking models for information retrieval.
    Information Processing & Management 57(6), 102067 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] He, P., Gao, J., Chen, W.: Debertav3: Improving deberta using electra-style
    pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Jeronymo, V., Bonifacio, L., Abonizio, H., Fadaee, M., Lotufo, R., Zavrel,
    J., Nogueira, R.: InPars-v2: Large language models as efficient dataset generators
    for information retrieval (2023). https://doi.org/10.48550/ARXIV.2301.01820, [https://arxiv.org/abs/2301.01820](https://arxiv.org/abs/2301.01820)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen,
    D., Yih, W.t.: Dense passage retrieval for open-domain question answering. In:
    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP). pp. 6769–6781\. Association for Computational Linguistics, Online (Nov
    2020). https://doi.org/10.18653/v1/2020.emnlp-main.550, [https://aclanthology.org/2020.emnlp-main.550](https://aclanthology.org/2020.emnlp-main.550)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A.,
    Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K.,
    Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov,
    S.: Natural questions: A benchmark for question answering research. Transactions
    of the Association for Computational Linguistics 7, 452–466 (2019), [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Li, H.: Learning to rank for information retrieval and natural language
    processing, second edition. Synthesis Lectures on Human Language Technologies
    7, 1–123 (01 2015). https://doi.org/10.2200/S00607ED2V01Y201410HLT026'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Lin, J., Nogueira, R., Yates, A.: Pretrained transformers for text ranking:
    Bert and beyond. Springer Nature (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Lin, S.C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.t.,
    Chen, X.: How to train your dragon: Diverse augmentation towards generalizable
    dense retrieval. arXiv preprint arXiv:2302.07452 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Liu, T.Y.: Learning to rank for information retrieval. Found. Trends Inf.
    Retr. 3(3), 225–331 (mar 2009). https://doi.org/10.1561/1500000016, [https://doi.org/10.1561/1500000016](https://doi.org/10.1561/1500000016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In:
    International Conference on Learning Representations (2019), [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Ma, X., Zhang, X., Pradeep, R., Lin, J.: Zero-shot listwise document reranking
    with a large language model. arXiv preprint arXiv:2305.02156 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Mitra, B., Craswell, N.: Neural models for information retrieval. arXiv
    preprint arXiv:1705.01509 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R.,
    Deng, L.: MS MARCO: A human-generated MAchine reading COmprehension dataset (2017),
    [https://openreview.net/forum?id=Hk1iOLcle](https://openreview.net/forum?id=Hk1iOLcle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Nogueira, R., Cho, K.: Passage re-ranking with bert (2019), [http://arxiv.org/abs/1901.04085](http://arxiv.org/abs/1901.04085)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Nogueira, R., Jiang, Z., Pradeep, R., Lin, J.: Document ranking with a
    pretrained sequence-to-sequence model. In: Findings of the Association for Computational
    Linguistics: EMNLP 2020\. pp. 708–718\. Association for Computational Linguistics,
    Online (Nov 2020). https://doi.org/10.18653/v1/2020.findings-emnlp.63, [https://aclanthology.org/2020.findings-emnlp.63](https://aclanthology.org/2020.findings-emnlp.63)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Nogueira, R., Lin, J., Epistemic, A.: From doc2query to doctttttquery.
    Online preprint 6,  2 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Nogueira, R., Yang, W., Cho, K., Lin, J.: Multi-stage document ranking
    with bert. arXiv preprint arXiv:1910.14424 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Qin, Z., Jagerman, R., Hui, K., Zhuang, H., Wu, J., Shen, J., Liu, T.,
    Liu, J., Metzler, D., Wang, X., et al.: Large language models are effective text
    rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.:
    Language models are unsupervised multitask learners. OpenAI blog 1(8),  9 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,
    Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
    unified text-to-text transformer. The Journal of Machine Learning Research 21(1),
    5485–5551 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Robertson, S.E., Walker, S., Jones, S., Hancock-Beaulieu, M.M., Gatford,
    M., et al.: Okapi at trec-3\. Nist Special Publication Sp 109,  109 (1995)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Rosa, G.M., Bonifacio, L., Jeronymo, V., Abonizio, H., Fadaee, M., Lotufo,
    R., Nogueira, R.: No parameter left behind: How distillation and model size affect
    zero-shot retrieval. arXiv preprint arXiv:2206.02873 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Sun, W., Yan, L., Ma, X., Ren, P., Yin, D., Ren, Z.: Is chatgpt good at
    search? investigating large language models as re-ranking agent. arXiv preprint
    arXiv:2304.09542 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: BEIR:
    A heterogeneous benchmark for zero-shot evaluation of information retrieval models.
    In: Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track (Round 2) (2021), [https://openreview.net/forum?id=wCu6T5xFjeJ](https://openreview.net/forum?id=wCu6T5xFjeJ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] VanBuskirk, A.: Gpt-3.5 turbo vs gpt-4: What’s the difference? (March
    2023), [https://blog.wordbot.io/ai-artificial-intelligence/gpt-3-5-turbo-vs-gpt-4-whats-the-difference/](https://blog.wordbot.io/ai-artificial-intelligence/gpt-3-5-turbo-vs-gpt-4-whats-the-difference/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Wang, B., Komatsuzaki, A.: GPT-J-6B: A 6 Billion Parameter Autoregressive
    Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
    (May 2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R.,
    Manning, C.D.: Hotpotqa: A dataset for diverse, explainable multi-hop question
    answering. arXiv preprint arXiv:1809.09600 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Zhai, C., et al.: Statistical language models for information retrieval
    a critical review. Foundations and Trends® in Information Retrieval 2(3), 137–213
    (2008)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Zhao, W.X., Liu, J., Ren, R., Wen, J.R.: Dense text retrieval based on
    pretrained language models: A survey. arXiv preprint arXiv:2211.14876 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Zhu, Y., Yuan, H., Wang, S., Liu, J., Liu, W., Deng, C., Dou, Z., Wen,
    J.R.: Large language models for information retrieval: A survey. arXiv preprint
    arXiv:2308.07107 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Zhuang, H., Qin, Z., Jagerman, R., Hui, K., Ma, J., Lu, J., Ni, J., Wang,
    X., Bendersky, M.: Rankt5: Fine-tuning t5 for text ranking with ranking losses.
    In: Proceedings of the 46th International ACM SIGIR Conference on Research and
    Development in Information Retrieval. pp. 2308–2313 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
