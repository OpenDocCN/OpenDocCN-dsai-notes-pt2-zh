- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ReALLM: A general framework for LLM compression and fine-tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \newaliascnt
  prefs: []
  type: TYPE_NORMAL
- en: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
  prefs: []
  type: TYPE_NORMAL
- en: Louis Leconte
  prefs: []
  type: TYPE_NORMAL
- en: Lisite, Isep, Sorbonne University
  prefs: []
  type: TYPE_NORMAL
- en: Math. and Algo. Sciences Lab, Huawei Tech
  prefs: []
  type: TYPE_NORMAL
- en: louis.leconte@ens-paris-saclay.fr
  prefs: []
  type: TYPE_NORMAL
- en: '&Lisa Bedin^∗'
  prefs: []
  type: TYPE_NORMAL
- en: CMAP, Ecole Polytechnique, France
  prefs: []
  type: TYPE_NORMAL
- en: lisa.bedin@polytechnique.edu
  prefs: []
  type: TYPE_NORMAL
- en: Van Minh Nguyen
  prefs: []
  type: TYPE_NORMAL
- en: Math. and Algo. Sciences Lab, Huawei Tech.
  prefs: []
  type: TYPE_NORMAL
- en: '&Eric Moulines'
  prefs: []
  type: TYPE_NORMAL
- en: CMAP, Ecole Polytechnique, France equal contribution
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We introduce ReALLM, a novel approach for compression and memory-efficient adaptation
    of pre-trained language models that encompasses most of the post-training quantization
    and fine-tuning methods for a budget of $<4$ bits, ReALLM achieves state-of-the
    art performance after fine-tuning on a small calibration dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) based on transformer architectures (Vaswani et al.,,
    [2017](#bib.bib48)) have attracted increasing interest, especially with the availability
    of high-quality, open-source LLMs such as LLaMA (Touvron et al.,, [2023](#bib.bib44)),
    Falcon (Almazrouei et al.,, [2023](#bib.bib1)) and Gemma (Team et al.,, [2024](#bib.bib43)).
    These open models offer the advantage that they can be used by end users for inference
    or local fine-tuning, provided their hardware has sufficient memory for the size
    of the models. However, “full fine-tuning” — a process that involves updating
    all previously trained parameters — is still prohibitively expensive for large
    models. For example, the standard 16-bits fine-tuning of the LLaMA-$65$ GB of
    GPU memory ([Dettmers et al., 2023a,](#bib.bib11) ). This high requirement is
    due to the need to store both the weights of the model and the states of the optimizer
    in GPU memory, a need that increases as the size of the LLMs increases.
  prefs: []
  type: TYPE_NORMAL
- en: A common method to mitigate memory constraints is to quantize the model weights,
    activations, and gradients — to a lower bit precision. Quantization-Aware Training
    (QAT) is often used in computer vision; see Courbariaux et al., ([2015](#bib.bib8));
    Liu et al., ([2020](#bib.bib33)); Gholami et al., ([2022](#bib.bib16)). However,
    training large language models (LLMs) from scratch is impractical due to high
    computational cost. Post-training quantization (PTQ) is an efficient compromise
    (Dettmers et al.,, [2022](#bib.bib10); Frantar et al.,, [2022](#bib.bib14)), which
    has recently attracted much attention ([Kim et al., 2023b,](#bib.bib24) ; [Dettmers
    et al., 2023b,](#bib.bib12) ; [Kim et al., 2023a,](#bib.bib23) ; Shao et al.,,
    [2023](#bib.bib39)). Although most research focuses on scalar quantization (SQ),
    a few studies investigate LLM compression using vector quantization (VQ) (Tseng
    et al.,, [2024](#bib.bib46); Egiazarian et al.,, [2024](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: In [Dettmers et al., 2023a](#bib.bib11) , quantization is effectively combined
    with the Parameter Efficient Fine-Tuning (PEFT) method, LoRA (Hu et al.,, [2021](#bib.bib21)),
    to improve efficiency and practicality in memory-constrained environments. Post-Training
    Quantization (PTQ) has the potential to be further improved to achieve sub-$3$
    bit quantization (Li et al.,, [2023](#bib.bib27); Guo et al.,, [2023](#bib.bib18)).
    However, it was found that the weights of the LLM often contain outliers — weights
    with significantly higher values than others ([Kim et al., 2023b,](#bib.bib24)
    ; [Dettmers et al., 2023b,](#bib.bib12) ). These outliers pose a considerable
    challenge for model compression with PTQ and lead to significant quantization
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper we present ReALLM - for Residual Autoencoder LLM - a general
    approach for LLM PTQ and fine-tuning. Pre-trained LLM matrices are decomposed
    into a $16$-bit remainder (low rank, sparse outliers, etc.) and a compressed part,
    which is fed into a VQ autoencoder (Van Den Oord et al.,, [2017](#bib.bib47)).
    In our experiments, we implement a low-rank and quantized decomposition of pre-trained
    LLM matrices. In this approach, only the low-rank components are fine-tuned (block-wise
    and end-to-end) while the quantized elements remain static. Our quantization strategy
    (i.e. the shape of the autoencoder) adapts to the matrix patterns: Our results
    suggest that some pre-trained LLM matrices exhibit “spatial” patterns (see [Figure 1](#S1.F1
    "In 1 Introduction ‣ ReALLM: A general framework for LLM compression and fine-tuning");
    left) that bear similarities to those in images/videos and allow for highly effective
    compression (see [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣
    ReALLM: A general framework for LLM compression and fine-tuning")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69769f0dcf311395c643505d6b507101.png)![Refer to caption](img/a5078b6b39a9315e48bf083e47a52b0b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c322daf7100a0c194d410694fbf1811e.png)![Refer to caption](img/1a3fe2b2cd3f9062207e0f1903a5d246.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Gemma-2B (Team et al.,, [2024](#bib.bib43))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Pre-trained matrix from the first block (left; with “structures”),
    and pre-trained matrix from the last block (right) for two different models. Stronger
    vertical patterns appear in the first blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present ReALLM, a method that uses a novel autoencoder and a residual pipeline
    to efficiently compress pre-trained LLM matrices;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We show that state-of-the-art PTQ approaches (Lin et al.,, [2023](#bib.bib29);
    Shao et al.,, [2023](#bib.bib39); Tseng et al.,, [2024](#bib.bib46); Egiazarian
    et al.,, [2024](#bib.bib13)) and fine-tuning methods (Hu et al.,, [2021](#bib.bib21);
    [Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18); Li et al.,,
    [2023](#bib.bib27); Liao and Monz,, [2024](#bib.bib28)) are all special cases
    of ReALLM;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a preprocessing step that includes scaling and column permutations
    of matrices to mitigate the quantization errors associated with outliers; We also
    propose to adapt the general autoencoder scheme to the type of pre-trained matrix
    patterns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our approach demonstrates that fine-tuning end-to-end with block-wise error
    reduction leads to the best results reported in the literature for 3 and 2-bit
    Post-Training Quantization (PTQ).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs adapters.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After the introduction of high-performance open-source LLMs and due to the impracticality
    of “full fine-tuning”, several methods of parameter-efficient fine-tuning (PEFT)
    have emerged, including prefix tuning (Li and Liang,, [2021](#bib.bib26)), selective
    fine-tuning (Guo et al.,, [2021](#bib.bib17)) and Low Rank Adapter (LoRA). LoRA,
    introduced in Hu et al., ([2021](#bib.bib21)), is a simple but effective fine-tuning
    method that retains the pre-trained matrices but adds a low-rank component. For
    a typical pre-trained matrix $W$ denotes the Euclidean norm of a matrix over each
    column. DoRA with the trainable size vector requires little computational effort,
    but can lead to significant performance improvements (Liu et al.,, [2024](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Current methods for compressing LLMs predominantly use quantization techniques.
    Early strategies, such as ZeroQuant (Yao et al.,, [2022](#bib.bib52)) and nuQmm
    (Park et al.,, [2022](#bib.bib36)), relied primarily on direct rounding of weights
    to the nearest quantization level. Later developments improved this approach by
    handling outliers through quantization to higher bitwidths (Xiao et al.,, [2023](#bib.bib51);
    Dettmers et al.,, [2022](#bib.bib10); [Kim et al., 2023b,](#bib.bib24) ; [Dettmers
    et al., 2023b,](#bib.bib12) ). Methods similar to ReALLM include those that combine
    quantization with a low-rank decomposition; see e.g. [Dettmers et al., 2023a](#bib.bib11)
    ; Guo et al., ([2023](#bib.bib18)); Li et al., ([2023](#bib.bib27)); Liao and
    Monz, ([2024](#bib.bib28)). QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) combined
    Parameter Efficient Fine-Tuning (PEFT) and quantization, but added zero-initialised
    low-rank adapters after quantization. In contrast, Loftq (Li et al.,, [2023](#bib.bib27))
    and LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) propose to minimize quantization
    errors by initializing LoRA components with an SVD of the pre-trained weights.
    As part of this integration, ApiQ (Liao and Monz,, [2024](#bib.bib28)) uses gradient
    descent to optimize both the LoRA components and the quantization parameters for
    the entire model rather than for each individual layer. Quantization of pre-trained
    weights facilitates efficient inference on devices with limited memory. To achieve
    significant computational and energy efficiency, recent studies have combined
    quantization of weights with activation quantization (Liu et al.,, [2023](#bib.bib30);
    Nrusimha et al.,, [2024](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: Block/Layer-Wise Tuning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GPTQ (Frantar et al.,, [2022](#bib.bib14)) introduced a higher accuracy strategy
    using an approximate large-scale solver to minimize the layer-wise quadratic error,
    which is crucial for low bit-width quantization, as highlighted in Tseng et al.,
    ([2024](#bib.bib46)); Egiazarian et al., ([2024](#bib.bib13)). Quip# (Tseng et al.,,
    [2024](#bib.bib46)) applies random rotations to the pre-trained matrices, segments
    the resulting matrix into vectors of dimension $d=8$ bits per parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1cdadff5c11726b871db407ed222c818.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: ReALLM; during the fine-tuning step only low-rank and scales are
    updated'
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank/sparse decomposition.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Starting from a pre-trained LLM matrix $W\in\mathbb{R}^{p\times q}$ that (approximately)
    solve the following problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{Q,L_{1},L_{2}}\&#124;W-(Q+L_{1}(L_{2})^{t})\&#124;.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: QLoRA [Dettmers et al., 2023a](#bib.bib11) provides a suboptimal solution for
    the previously described optimization problem by setting $L_{1}=0$. There is no
    guarantee that the initialization of the low-rank part to zero is optimal. It
    has been reported that QLoRA, Apiq and Loftq perform better than QLoRA in several
    language generation benchmarks (Guo et al.,, [2023](#bib.bib18); Liao and Monz,,
    [2024](#bib.bib28); Li et al.,, [2023](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Mixed-autoencoder configuration.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An autoencoder is the composition of an encoding function $\mathcal{E}$. LQ-LoRA
    (Guo et al.,, [2023](#bib.bib18)), Loftq (Li et al.,, [2023](#bib.bib27)), and
    ApiQ (Liao and Monz,, [2024](#bib.bib28)) are special cases of ReALLM where the
    encoder and the decoder are defined as the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach may not be optimal as some matrices are more challenging to quantize
    than others (Guo et al.,, [2023](#bib.bib18)). Specifically, [Figure 1](#S1.F1
    "In 1 Introduction ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    shows that pre-trained LLM matrices can display very different “spatial” patterns.
    ReALLM adapts the autoencoder to the type and shape of the matrix. When quantizing
    pre-trained matrices with strong coefficient dependencies, ReALLM is akin to image
    and video compression techniques that use the implicit neural representation (Chen
    et al.,, [2023](#bib.bib6); Kwan et al.,, [2024](#bib.bib25)). ReALLM extracts
    latent representations $\mathcal{E}_{\psi}(W)$ consisting of standard 2D convolutions,
    and a decoder combining 2D-convNeXt (Liu et al.,, [2022](#bib.bib32)) and PixelShuffle
    (Shi et al.,, [2016](#bib.bib40)).'
  prefs: []
  type: TYPE_NORMAL
- en: The decoding process is fast, as HNeRV requires only one network forward operation
    for decoding. ReALLM compression is a combination of a small (w.r.t. input signals)
    neural decoder model $\mathcal{D}_{\phi}$ bits per coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Quantization (VQ).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An efficient way to store the embedding $\mathcal{E}_{\psi}(W)$ bits). It should
    be noted that no separate gradient is defined for the quantization operator with
    the closest element (Van Den Oord et al.,, [2017](#bib.bib47)). Therefore, during
    the backward pass, we approximate the gradient similarly to the straight-through
    estimator (Bengio,, [2013](#bib.bib5)) and simply copy the gradients from the
    decoder input to the encoder output.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization pre-processing.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before using a tensor quantization method, it is important to perform an appropriate
    scaling. Several parameters (number of blocks, quantile bins, etc.) are chosen
    to correspond to a given compression ratio. But the presence of outliers ([Kim
    et al., 2023b,](#bib.bib24) ; [Dettmers et al., 2023b,](#bib.bib12) ) forces the
    scaling and quantization methods to have a poor compression ratio (Lin et al.,,
    [2023](#bib.bib29); Tseng et al.,, [2024](#bib.bib46); Ashkboos et al.,, [2024](#bib.bib4)).
    Incoherence processing uses random rotations as a pre-processing step. Although
    the main purpose of incoherence processing is to reduce the effects of outliers
    (Tseng et al.,, [2024](#bib.bib46); Ashkboos et al.,, [2024](#bib.bib4)), this
    technique has a detrimental effect on the structure of the pre-trained matrices
    within the initial blocks of the LLM (see [Figures 1](#S1.F1 "In 1 Introduction
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [3](#S3.F3
    "Figure 3 ‣ ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")). This is a serious bottleneck as quantization
    errors in these initial blocks can propagate throughout the model. As shown in
    [Figure 1](#S1.F1 "In 1 Introduction ‣ ReALLM: A general framework for LLM compression
    and fine-tuning"), some matrices have no specific patterns and resemble random
    Gaussian noise interspersed with randomly positioned outliers. To deal with outliers
    in the latent representation, we suggest rearranging the columns to create some
    spatial regularity. This strategy aims to find the most effective permutations
    that cluster outliers. Trukhanov and Soloveychik, ([2024](#bib.bib45)) has recently
    elaborated a row/column permutation strategy that summarizes vectors (i.e. sets
    of rows or columns) with similar norms. In contrast, for ReALLM we propose to
    permute columns such that neighboring columns are “similar” and not just on the
    same hypersphere. We develop a basic, yet efficient method for this: first we
    select a block of size $128\times q$ bits per coordinate.'
  prefs: []
  type: TYPE_NORMAL
- en: Input : Matrix $w$
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 permutation function
  prefs: []
  type: TYPE_NORMAL
- en: 'ReALLM: a new LLM format.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM standard formats represent LLM weights as a set of matrices encoded on $16$.
    This speeds up the decoding step compared to diffusion-based approaches (Wang
    et al.,, [2024](#bib.bib50); Soro et al.,, [2024](#bib.bib41)). Note that for
    ReALLM a decoder model has to be trained on LLM matrices, but this learning step
    is done once and for all. Additionally, the more we train and overfit, the better
    ReALLM becomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of hyper-parameters for ReALLM are: $r$ patches extracted from pre-trained
    LLM matrices, and we use the HNeRV Chen et al., ([2023](#bib.bib6)) autoencoder
    model. For more details on the practical aspect of decoder training, see [Section A.2](#A1.SS2
    "A.2 Autoencoder computational limitations ‣ Appendix A Appendix / supplemental
    material ‣ ReALLM: A general framework for LLM compression and fine-tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: We have experimentally discovered two sets of optimal combinations of hyperparameters
    that depend on the type and shape of the pre-trained matrix. Some pre-trained
    matrices, especially those closer to the input tokens, compress better with small
    latent representations ($e_{0}e_{1}e_{2}<1024$).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a3b8c73103df94f28a4dac4b780901b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56a4889e163c5d185e777e5271277b5e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Llama2-7B (Touvron et al.,, [2023](#bib.bib44))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks. Quip# (Tseng et al.,, [2024](#bib.bib46)) does not take advantage of the
    structures in the first blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general
    framework for LLM compression and fine-tuning") ReALLM achieves the lowest Frobenius
    norm quantization error. We perform ablation experiments with this metric to decouple
    the effects of VQ and permutation preprocessing of ReALLM on the final performance.
    For example, in block $8$. Quip# rotates the matrices randomly, causing all patterns
    in the initial blocks to be lost.'
  prefs: []
  type: TYPE_NORMAL
- en: Input : Number of end-to-end fine-tuning steps $T$ with gradient descent ;26      27
    end for
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Pseudo-code for ReALLM with block-wise and end-to-end fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We test ReALLM on the LLaMA-2 (Touvron et al.,, [2023](#bib.bib44)) family models
    (with $7$ hours for a LLaMA2-7B model.
  prefs: []
  type: TYPE_NORMAL
- en: Language Generation Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For continual language modeling, we train on a single partition of the C4 (Raffel
    et al.,, [2020](#bib.bib37)) dataset for half an epoch and use a sequence length
    of $4096$ for both WikiText-2 (Merity et al.,, [2016](#bib.bib34)) and C4 evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main baselines are LQ-LoRA (Guo et al.,, [2023](#bib.bib18)), Quip# (Tseng
    et al.,, [2024](#bib.bib46)), and AQLM (Egiazarian et al.,, [2024](#bib.bib13)).
    However, we also report the performance of popular quantization approaches GPTQ
    (Frantar et al.,, [2022](#bib.bib14)), AWQ (Lin et al.,, [2023](#bib.bib29)),
    Omniquant (Shao et al.,, [2023](#bib.bib39)), as well as the performance of recent
    work ApiQ (Liao and Monz,, [2024](#bib.bib28)) and QuaRot (Ashkboos et al.,, [2024](#bib.bib4)).
    In the results below, we present the target bits per parameter that takes into
    account quantized weights and include parameters kept in high precision (head
    layer, scales, codebooks, permutations in $16$ bits precision) similarly to the
    related work. The exact bit budget is detailed in [Table 5](#A1.T5 "In A.2 Autoencoder
    computational limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM:
    A general framework for LLM compression and fine-tuning") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: In our experiments, following [Dettmers et al., 2023a](#bib.bib11) ; Guo et al.,
    ([2023](#bib.bib18)), we take a DoRA (Liu et al.,, [2024](#bib.bib31)) rank of
    $r=64$. As far as we know, we have also developed the first VQ code (available
    in the supplementary material) that makes efficient use of PyTorch’s “torch dispatch”
    functionality (Ansel et al.,, [2024](#bib.bib2)), which is known to be as fast
    as dedicated CUDA kernels (Guo et al.,, [2023](#bib.bib18)). This allows us to
    overload PyTorch operations to perform just-in-time dequantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Tables 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [2](#S4.T2
    "Table 2 ‣ Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A
    general framework for LLM compression and fine-tuning") we evaluate the perplexity
    of ReALLM on the respective validation datasets of C4 and WikiText-2 for a single
    run. During fine-tuning (on a single partition of the C4 dataset), we only update
    the DoRA components (scales and low-rank matrices). For each dataset, we provide
    three sets of results in [Table 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"):
    Perplexity without any fine-tuning (only low-rank and VQ autoencoder decomposition),
    perplexity with only block-wise fine-tuning, and perplexities with end-to-end
    fine-tuning (in addition to the block-wise fine-tuning process).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Perplexity $(\downarrow)$'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | #bits | rank $r$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (no fine-tuning) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (block-wise) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (40% training) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (full training) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (no fine-tuning) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (block-wise) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (40% training) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (full training) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (no fine-tuning) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (block-wise 50 epochs) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (block-wise 200 epochs) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (40% training) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (full training) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (no fine-tuning) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (block-wise 50 epochs) | $2$ | 15.74 | 12.08 |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (40% training) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (full training) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: 'Our *data-free* version of ReALLM (no fine-tuning; see [Table 1](#S4.T1 "In
    Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")) achieves state-of-the-art metrics for $3$
    bits are needed to store the codebook). Additional results for other models are
    available in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Perplexity $(\downarrow)$'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Number of bits | C4 $(\downarrow)$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 7B | 13B | 7B | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2 (Touvron et al.,, [2023](#bib.bib44)) | $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ (Lin et al.,, [2023](#bib.bib29)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant (Shao et al.,, [2023](#bib.bib39)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ApiQ[PTQ] (Liao and Monz,, [2024](#bib.bib28)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot[A16W3] (Ashkboos et al.,, [2024](#bib.bib4)) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
  prefs: []
  type: TYPE_TB
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ApiQ (Liao and Monz,, [2024](#bib.bib28)) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
  prefs: []
  type: TYPE_TB
- en: 'In [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") we compare
    ReALLM with end-to-end fine-tuning, and the best performing PTQ approaches. All
    the methods cited in [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    also uses a calibration dataset. It is interesting to note that ReALLM with $2$
    bits precision) during the layer-wise fine-tuning. This does not only slow down
    the PTQ process (as gradients must be store for all weights in the given block),
    but it also means Quip# has to store learnable vectors and also quantized weights
    for *each* fine-tuning task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Accuracy $(\uparrow)$ in LM Eval (acc, not acc_norm).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Size | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | Average
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2 | 7B | $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 7B | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 7B | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2 | 13B | $16$ |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 13B | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 13B | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
  prefs: []
  type: TYPE_TB
- en: Zero-Shot Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following HuggingFace’s Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,
    and the literature (Frantar et al.,, [2022](#bib.bib14); Guo et al.,, [2023](#bib.bib18)),
    we also measure zero-shot accuracy on ARC (Clark et al.,, [2018](#bib.bib7)),
    PiQA (Tata and Patel,, [2003](#bib.bib42)), and Winogrande (Sakaguchi et al.,,
    [2021](#bib.bib38)), via the LM Evalaluation Harness (Gao et al.,, [2021](#bib.bib15)).
    We report results in [Table 3](#S4.T3 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"),
    and compute the average on the 4 mentioned tasks. For all LLM sizes, ReALLM provides
    a notable advantage (between $0.5$ bits) on the zero-shot tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present ReALLM, a weight-only PTQ method that achieves state-of-the-art results
    on LLMs at $2$ GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Large context sequence lengths result in large $KV$-cache quantization, and
    how to combine it with activation quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Societal impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of LLM compression
    and fine-tuning. There are many potential societal consequences of our work, in
    particular malicious usage of LLMs for spams or language generation on edge devices.
    However, this negative societal impact is not limited to ReALLM, but to the field
    of LLM in general.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almazrouei et al., (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic,
    Q., et al. (2023). The falcon series of open language models. arXiv preprint arXiv:2311.16867.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ansel et al., (2024) Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A.,
    Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia,
    A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J.,
    Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano,
    M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso,
    M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang,
    X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G.,
    Wu, P., and Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic
    Python Bytecode Transformation and Graph Compilation. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (ASPLOS ’24). ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arthur et al., (2007) Arthur, D., Vassilvitskii, S., et al. (2007). k-means++:
    The advantages of careful seeding. In Soda, volume 7, pages 1027–1035.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ashkboos et al., (2024) Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B.,
    Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. (2024). Quarot: Outlier-free
    4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio, (2013) Bengio, Y. (2013). Estimating or propagating gradients through
    stochastic neurons. arXiv preprint arXiv:1305.2982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al., (2023) Chen, H., Gwilliam, M., Lim, S.-N., and Shrivastava, A.
    (2023). Hnerv: A hybrid neural representation for videos. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10270–10279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Courbariaux et al., (2015) Courbariaux, M., Bengio, Y., and David, J.-P. (2015).
    Binaryconnect: Training deep neural networks with binary weights during propagations.
    Advances in neural information processing systems, 28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dao et al., (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., and Ré, C. (2019).
    Learning fast algorithms for linear transforms using butterfly factorizations.
    In International conference on machine learning, pages 1517–1527\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al., (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. (2022). Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale.
    Advances in Neural Information Processing Systems, 35:30318–30332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(11) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023a).
    Qlora: Efficient finetuning of quantized llms. Advances in Neural Information
    Processing Systems, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. (2023b). Spqr: A
    sparse-quantized representation for near-lossless llm weight compression. In The
    Twelfth International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egiazarian et al., (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., and Alistarh, D. (2024). Extreme compression of large language
    models via additive quantization. arXiv preprint arXiv:2401.06118.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al., (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. (2022). Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al., (2021) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for
    few-shot language model evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gholami et al., (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    and Keutzer, K. (2022). A survey of quantization methods for efficient neural
    network inference. In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al., (2021) Guo, D., Rush, A. M., and Kim, Y. (2021). Parameter-efficient
    transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al., (2023) Guo, H., Greengard, P., Xing, E., and Kim, Y. (2023). Lq-lora:
    Low-rank plus quantized matrix decomposition for efficient language model finetuning.
    In The Twelfth International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al., (2015) Han, S., Pool, J., Tran, J., and Dally, W. (2015). Learning
    both weights and connections for efficient neural network. Advances in neural
    information processing systems, 28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hooper et al., (2024) Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W.,
    Shao, Y. S., Keutzer, K., and Gholami, A. (2024). Kvquant: Towards 10 million
    context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al., (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., et al. (2021). Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al., (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. (2023). Mistral 7b. arXiv preprint arXiv:2310.06825.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., and Lee,
    D. (2023a). Memory-efficient fine-tuning of compressed large language models via
    sub-4-bit integer quantization. arXiv preprint arXiv:2305.14152.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., and Keutzer, K. (2023b). Squeezellm: Dense-and-sparse quantization. arXiv
    preprint arXiv:2306.07629.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwan et al., (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., and Bull, D.
    (2024). Hinerv: Video compression with hierarchical encoding-based neural representation.
    Advances in Neural Information Processing Systems, 36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang, (2021) Li, X. L. and Liang, P. (2021). Prefix-tuning: Optimizing
    continuous prompts for generation. In Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al., (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen,
    W., and Zhao, T. (2023). Loftq: Lora-fine-tuning-aware quantization for large
    language models. In The Twelfth International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao and Monz, (2024) Liao, B. and Monz, C. (2024). Apiq: Finetuning of 2-bit
    quantized large language model. arXiv preprint arXiv:2402.05147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. (2023). Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al., (2023) Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang,
    B. (2023). Qllm: Accurate and efficient low-bitwidth quantization for large language
    models. In The Twelfth International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al., (2024) Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., and Chen, M.-H. (2024). Dora: Weight-decomposed low-rank adaptation.
    arXiv preprint arXiv:2402.09353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al., (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell,
    T., and Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 11976–11986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al., (2020) Liu, Z., Shen, Z., Savvides, M., and Cheng, K.-T. (2020).
    Reactnet: Towards precise binary neural network with generalized activation functions.
    In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XIV 16, pages 143–159\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al., (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016).
    Pointer sentinel mixture models. In International Conference on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nrusimha et al., (2024) Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda,
    R., and Kim, Y. (2024). Mitigating the impact of outlier channels for language
    model quantization with activation regularization. arXiv preprint arXiv:2404.03605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al., (2022) Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B.,
    Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). Lut-gemm: Quantized matrix
    multiplication based on luts for efficient inference in large-scale generative
    language models. arXiv preprint arXiv:2206.09557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al., (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits
    of transfer learning with a unified text-to-text transformer. Journal of machine
    learning research, 21(140):1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al., (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2023). Omniquant: Omnidirectionally
    calibrated quantization for large language models. In The Twelfth International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al., (2016) Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P.,
    Bishop, R., Rueckert, D., and Wang, Z. (2016). Real-time single image and video
    super-resolution using an efficient sub-pixel convolutional neural network. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1874–1883.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soro et al., (2024) Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., and
    Hwang, S. J. (2024). Diffusion-based neural network weights generation. arXiv
    preprint arXiv:2402.18153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tata and Patel, (2003) Tata, S. and Patel, J. M. (2003). Piqa: An algebra for
    querying protein data sets. In 15th International Conference on Scientific and
    Statistical Database Management, 2003., pages 141–150\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al., (2024) Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju,
    S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024).
    Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trukhanov and Soloveychik, (2024) Trukhanov, N. and Soloveychik, I. (2024).
    Accurate block quantization in llms with outliers. arXiv preprint arXiv:2403.20137.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tseng et al., (2024) Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa,
    C. (2024). Quip#: Even better llm quantization with hadamard incoherence and lattice
    codebooks. arXiv preprint arXiv:2402.04396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Den Oord et al., (2017) Van Den Oord, A., Vinyals, O., et al. (2017). Neural
    discrete representation learning. Advances in neural information processing systems,
    30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al., (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all
    you need. Advances in neural information processing systems, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viazovska, (2017) Viazovska, M. S. (2017). The sphere packing problem in dimension
    8. Annals of mathematics, pages 991–1015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al., (2024) Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu,
    Z., and You, Y. (2024). Neural network diffusion. arXiv preprint arXiv:2402.13144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al., (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. (2023). Smoothquant: Accurate and efficient post-training quantization
    for large language models. In International Conference on Machine Learning, pages
    38087–38099\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al., (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C.,
    and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix / supplemental material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Structures in pre-trained matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interestingly, the blocks that show some visual structures in LLaMA and Mistral
    models are not the same for Gemma LLMs. For instance in [Figure 4](#A1.F4 "In
    A.1 Structures in pre-trained matrices ‣ Appendix A Appendix / supplemental material
    ‣ ReALLM: A general framework for LLM compression and fine-tuning"), we can see
    that Gemma2b (Team et al.,, [2024](#bib.bib43))’s matrices keep some internal
    patterns in all blocks, not only at the very first blocks. Note this has no negative
    impact on ReALLM, as the shape of the encoder is experimentally adapted to each
    block.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf9fe57040bc062c570f73b31d6925da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks of Gemma2b LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Autoencoder computational limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our GPU can not directly work on LLM pre-trained matrices with large sizes (typically
    of shape $4096\times 4096$ bits using straight through estimator Bengio, ([2013](#bib.bib5)).
    We also tested a post training quantization method where the weight of the decoder
    are quantized with a round to nearest (RTN) approache, at the end of the decoder
    training steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Reconstruction (Frobenius norm) error for layer of type “Q” inside
    the first block of Mistral-7b model, for patches of size $512\times 512$, and
    a varying quantization strategy (during the decoder training, i.e. QAT, or after
    the training, i.e. PTQ).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Error | # parameters c ($\times 10^{6}$ | bit budget | quantization |'
  prefs: []
  type: TYPE_TB
- en: '| $0.84$ | NF3(Guo et al.,, [2023](#bib.bib18)) |'
  prefs: []
  type: TYPE_TB
- en: '| $1.78$ | PTQ |'
  prefs: []
  type: TYPE_TB
- en: '| $1.19$ | PTQ |'
  prefs: []
  type: TYPE_TB
- en: '| $1.61$ | QAT |'
  prefs: []
  type: TYPE_TB
- en: '| $1.24$ | QAT |'
  prefs: []
  type: TYPE_TB
- en: '| $0.69$ | QAT |'
  prefs: []
  type: TYPE_TB
- en: We vary the number of parameters $c$), ReALLM yields a smaller quantization
    error compared to the scalar quantization NF3 ([Dettmers et al., 2023a,](#bib.bib11)
    ; Guo et al.,, [2023](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison of several LLM format for $m$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | LoRA | VQ only (like AQLM) | ReALLM |'
  prefs: []
  type: TYPE_TB
- en: '| Matrix representation | $(p\times q)\cdot 16$ |'
  prefs: []
  type: TYPE_TB
- en: '| Codebook | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Decoder | $-$ |'
  prefs: []
  type: TYPE_TB
- en: '| Low-rank | $(2\times r\times\min(p,q))\cdot 16$ |'
  prefs: []
  type: TYPE_TB
- en: '| Total bit cost | $16(pq+2r\min(p,q))\cdot m$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Quantization and fine-tuning approaches as particular case of ReALLM (with
    a rank $r$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | rank $r$ |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA (Hu et al.,, [2021](#bib.bib21)) | $64$ |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) | $64$ |'
  prefs: []
  type: TYPE_TB
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $64$ |'
  prefs: []
  type: TYPE_TB
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM | $64$ |'
  prefs: []
  type: TYPE_TB
- en: A.3 Permutations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In ReALLM, we compute permutations on sets of vectors in dimension $128$. We
    could work with smaller blocks, but it induces more memory dedicated to the permutation
    storage (one permutation for each block).
  prefs: []
  type: TYPE_NORMAL
- en: 'We start from the first vector (i.e. the first column of the initial matrix
    shrunk to a dimension $d=128$ vectors. The process is then iterated. Details are
    given in [Algorithm 1](#algorithm1 "In Quantization pre-processing. ‣ 3 Method
    ‣ ReALLM: A general framework for LLM compression and fine-tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Broader impacts and Safeguards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our computing unit seriously restricts the size of the decoder models we can
    train. We are not able to train one decoder model for a given LLM, but we work
    layer-wise and train a single decoder model for all patches extracted from the
    given layer. This layer-wise training forms the main limitation of ReALLM w.r.t. standard
    post-training quantization methods, such as round to nearest (RTN).
  prefs: []
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Perplexity $(\downarrow)$'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | #bits | rank $r$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (no fine-tuning) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (30% training) | $3$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (no fine-tuning) | $2$ |'
  prefs: []
  type: TYPE_TB
- en: '| ReALLM (10% training) | $2$ |'
  prefs: []
  type: TYPE_TB
