- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.17233](https://ar5iv.labs.arxiv.org/html/2405.17233)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Haoyu Wang
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai, China
  prefs: []
  type: TYPE_NORMAL
- en: fayuge@sjtu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Bei Liu^∗'
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai, China
  prefs: []
  type: TYPE_NORMAL
- en: beiliu@sjtu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Hang Shao'
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai, China
  prefs: []
  type: TYPE_NORMAL
- en: hangshao99@sjtu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Bo Xiao'
  prefs: []
  type: TYPE_NORMAL
- en: Meituan
  prefs: []
  type: TYPE_NORMAL
- en: Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: xiaobo09@meituan.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Ke Zeng'
  prefs: []
  type: TYPE_NORMAL
- en: Meituan
  prefs: []
  type: TYPE_NORMAL
- en: Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: zengke02@meituan.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Guanglu Wan'
  prefs: []
  type: TYPE_NORMAL
- en: Meituan
  prefs: []
  type: TYPE_NORMAL
- en: Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: wanguanglu@meituan.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Yanmin Qian'
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: Shanghai, China
  prefs: []
  type: TYPE_NORMAL
- en: yanminqian@sjtu.edu.cn Equal Contribution. This work was done when Haoyu Wang
    and Bei Liu were interns at Meituan.Corresponding Author.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Parameter quantization for Large Language Models (LLMs) has attracted increasing
    attentions recently in reducing memory costs and improving computational efficiency.
    Early approaches have been widely adopted. However, the existing methods suffer
    from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper,
    we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ)
    framework by introducing three different types of adaptive strategies for LLM
    quantization. Firstly, a K-Means clustering based algorithm is proposed that allows
    dynamic generation of quantization centroids for each column of a parameter matrix.
    Secondly, we design an outlier-guided adaptive precision search strategy which
    can dynamically assign varying bit-widths to different columns. Finally, a dynamic
    outlier reservation scheme is developed to retain some parameters in their original
    float point precision, in trade off of boosted model performance. Experiments
    on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate
    that our methods achieve the state-of-the-art results across different bit settings,
    especially in extremely low-bit scenarios. Code is available at [https://github.com/fayuge/CLAQ](https://github.com/fayuge/CLAQ).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advancements in Large Language Models (LLMs) have yielded impressive
    accomplishments across a variety of tasks. Building on the pioneering work of
    GPT-3 [gpt3](#bib.bib3) , a series of influential LLMs such as OPT [zhang2022opt](#bib.bib47)
    , BLOOM [le2023bloom](#bib.bib23) , LLaMA [touvron2023LLaMA](#bib.bib37) , Mixtral
    [jiang2024mixtral](#bib.bib21) and Yi [young2024yi](#bib.bib45) have consistently
    demonstrated that expanding model size predictably promotes modeling accuracy
    (a.k.a. the scaling law [henighan2020scaling](#bib.bib17) ). The pursue of large
    (e.g. hundreds of billions of parameters), and even larger language models is
    often considered as a promising avenue towards Artificial General Intelligence
    (AGI) [bubeck2023sparks](#bib.bib4) .
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the significant computational and memory demands of LLMs pose
    unprecedented challenges. For example, deploying the GPT-3 model requires a staggering
    350GB of memory merely to accommodate its parameters in half-precision format
    (FP16), with a minimum of five high-end A100-80G GPUs. Such immense memory demands,
    coupled with the enormous runtime communication overhead, hinder the integration
    of LLMs into practical applications. To tackle this problem, 8-bit Post-Training
    Quantization (PTQ) methods for both model weights and activations have been proposed
    [xiao2023smoothquant](#bib.bib40) ; [dettmers2022int8](#bib.bib8) , which have
    led to reduced model sizes in memory and negligible declines in accuracy. Unfortunately,
    when PTQ methods [chee2024quip](#bib.bib5) ; [lin2023awq](#bib.bib29) ; [kim2023squeezellm](#bib.bib22)
    are attempted in quantizing the parameters into 4 bits or less, notable decreases
    of performance have been observed, making further reduction of model sizes a challenging
    task. Additionally, several quantization-aware training (QAT) approaches [liu2023llmqat](#bib.bib30)
    ; [dettmers2024qlora](#bib.bib9) ; [li2023loftq](#bib.bib27) have been introduced
    to enhance the model performance. Nonetheless, these algorithms integrate a training
    phase within the quantization process, thus imposing substantial demands for computational
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we introduce a novel and effective training-free quantization
    framework for LLMs, namely Column-Level Adaptive weight Quantization (CLAQ), which
    utilizes three distinct column-level strategies to enhance the performance of
    low-bit (i.e., 2-bit and 3-bit) quantization. Specifically, we initially apply
    a K-Means clustering based quantization that enables the adaptive generation of
    quantization centroids for each column of a weight matrix. Then, a simple yet
    powerful quantization sensitivity metric (Outlier Order) is described, which aims
    to identify parameter columns that are highly sensitive to quantization. Leveraging
    this metric, we further proposed two strategies to enhance quantization performance,
    including Column-Level Adaptive Precision (AP) quantization and Column-Level Adaptive
    Outlier Reservation (OR). Both schemes can significantly boost the performance
    of the low-bit quantized LLMs with only a slight increase in memory footprint.
    Notably, our fusion model (AP+OR) yields the best results in extremely low-bit
    scenarios. Extensive experiments on mainstream public LLMs and benchmarks demonstrate
    that our method achieves the state-of-the-art performance across various quantization
    bit-widths.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of our work can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We introduce a novel centroid selection strategy for quantization: K-Means
    based weight quantization. The architecture of K-Means based quantization is illustrated
    in Figure [1](#S2.F1 "Figure 1 ‣ Quantization-Aware Training ‣ 2 LLM Quantization
    ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs"). Compared
    to existing methods, K-Means based quantization excels in more accurately approximating
    the parameter distribution of pre-trained models and dynamically generating quantization
    centroids. Our proposed quantization scheme surpasses all current 3-bit and 4-bit
    PTQ methods under equivalent or larger quantization group sizes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present a column-wise outlier ratio based quantization sensitivity metric,
    namely Outlier Order, to assess the importance of parameters in LLMs. Based on
    this metric, Adaptive Precision (AP) quantization and Outlier Reservation (OR)
    are proposed to allocate higher budget to columns identified as more sensitive
    to quantization errors. Notably, this sensitivity metric emerges as an extremely
    efficient guiding principle, which can be calculated once easily and directly
    applied to both AP and OR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We implement Adaptive Precision quantization with the guidance of Outlier Order.
    Different levels of precision are adaptively assigned within each weight matrix.
    By allocating higher quantization budget to columns with a greater concentration
    of outliers, we achieve a substantial reduction in quantization errors. Furthermore,
    adaptive precision quantization allows arbitrary equivalent bit-width from 2-bit
    to 4-bit, facilitating the deployment of LLMs across a variety of end-devices
    with different memory constraints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We develop a novel Outliers Reservation (OR) strategy to further enhance the
    performance with only a slight increase in memory footprint. Based on Outlier
    Order, OR involves reserving FP16 precision outliers for columns with a higher
    concentration of outliers to preserve model performance, whereas columns with
    a lower outlier ratio are allocated less reservation budget. Moreover, the OR
    strategy can be combined with AP quantization, which exhibits the best results
    in low-bit quantization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 LLM Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-Training Quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Post-training quantization methods [wang2020towards](#bib.bib38) ; [hubara2021accurate](#bib.bib20)
    ; [li2021brecq](#bib.bib28) ; [nagel2020up](#bib.bib32) quantize LLMs without
    fine-tuning the parameters, while compensating the quantization error through
    specially designed techniques. Previous work [xiao2023smoothquant](#bib.bib40)
    ; [yao2022zeroquant](#bib.bib43) ; [dettmers2022int8](#bib.bib8) ; [park2022qumm](#bib.bib33)
    ; [Dettmers2022tim](#bib.bib11) have utilized the vanilla Round To Nearest quantization.
    Based on the OBS approach [frantar2022OBQ](#bib.bib13) , GPTQ [frantar2022gptq](#bib.bib15)
    adjusted the remaining weights while quantizing, offering an effective PTQ paradigm.
    A series of work have [lin2023awq](#bib.bib29) ; [xiao2023smoothquant](#bib.bib40)
    ; [lee2024owq](#bib.bib24) ; [dettmers2023spqr](#bib.bib10) ; [yin2023owl](#bib.bib44)
    ; [heo2023rethinking](#bib.bib18) ; [wei2022outliersupp](#bib.bib39) noticed the
    significance of outlier treatment in LLM quantization. By preserving outliers,
    the performance of quantized models can be further improved. PTQ approach alleviates
    the computational burden associated with extensive re-training, thus facilitating
    the efficient deployment of LLMs in resource-constrained environments.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-Aware Training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Quantization-Aware Training (QAT) incorporates a training phase within the quantization
    workflow. Owing to the model’s ability to adapt to the constraints imposed by
    quantization, QAT typically leads to superior model performance. [li2023loftq](#bib.bib27)
    ; [dettmers2024qlora](#bib.bib9) ; [xu2023qa](#bib.bib41) integrated quantization
    with the basis of LoRA [hu2021lora](#bib.bib19) . [xu2024onebit](#bib.bib42) performed
    extreme quantization with 1-bit parameters. In [shao2023omniquant](#bib.bib36)
    , instead of training the quantized parameters, a learnable equivalent transformation
    is trained for quantization. By learning the quantization error, QAT can mitigate
    the information loss brought by quantization schemes. Nonetheless, the advantage
    of QAT comes with the inclusion of specialized training phases, which not only
    requires additional time but also increases the computational overhead and complexity
    of the quantization task. In practical applications, it may also couple with model
    fine-tuning processes that complicates the workflow. Therefore, in this work,
    we focus on PTQ approach first, and leave the study of QAT approach as a future
    work.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd7c45a35412c7f099ca19bd3d8de88e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The K-Means clustering based quantization. Elements in weight matrix
    column are input of K-Means clustering and the quantization centroids are derived
    as the output of clustering algorithm. Then the pre-trained weights are quantized
    to the nearest K-Means class center.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 CLAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 K-Means Based Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, prior quantization techniques on LLMs have typically adopted uniform
    quantization levels (i.e., quantization centroids are equally spaced), such as
    SmoothQuant [xiao2023smoothquant](#bib.bib40) and GPTQ [frantar2022gptq](#bib.bib15)
    . Alternative methods adopt strategies that sample quantization centroids from
    a normal distribution, as demonstrated by QLoRA [dettmers2024qlora](#bib.bib9)
    . Nonetheless, all of the above approaches fail to accurately capture the true
    distribution of a group of model parameters, given the heterogeneous nature of
    parameter magnitudes and distributions across various network layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this issue, we first introduce a straightforward yet effective strategy:
    employing K-Means clustering for the selection of quantization centroids (Figure
    [1](#S2.F1 "Figure 1 ‣ Quantization-Aware Training ‣ 2 LLM Quantization ‣ CLAQ:
    Pushing the Limits of Low-Bit Post-Training Quantization for LLMs")). The clustering
    samples (i.e., quantization groups) are the columns of a parameter matrix from
    either self-attention or MLP structures in the LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, when quantizing an parameter matrix $W$-th column is derived
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C_{W_{j}}=KMeans(W_{j})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Where $KMeans$ can be formalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Inspired by [frantar2022gptq](#bib.bib15) , our approach quantizes the parameter
    matrix $W$ per column through clustering and quantization. We adopt the same approach
    as GPTQ [frantar2022gptq](#bib.bib15) for updating the remaining parameters. The
    advantage of K-Means based quantization is three-fold: Firstly, the centroids
    generated by K-Means clustering adaptively conform to the distribution of the
    LLM’s parameters. The better matching with the underlying parameter distribution
    can minimize information loss and mitigate quantization errors. Secondly, our
    K-Means quantization reduces storage requirements by storing a single vector of
    K-Means centroids as codebook per column, yet not losing the performance. Previous
    approaches have employed smaller quantization groups to boost low-bit quantization
    performance. However, smaller groups incur additional overhead of codebook storage.
    We found that with equivalent or even smaller model sizes, our algorithm demonstrates
    superior performance compared to prior works, illustrating the effectiveness of
    K-Means based quantization. Lastly, our approach is simple and efficient. Previous
    studies such as OmniQuant [shao2023omniquant](#bib.bib36) and LLM-QAT [liu2023llmqat](#bib.bib30)
    trained auxiliary network components to ensure quantization efficacy. In comparison,
    our method avoids the necessity of additional training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d96b86e869bcdc9b73b1aa3e9696acf8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The overall structure of CLAQ: quantized models are obtained from
    different quantization approaches. The single-precision K-Means based quantization
    runs without sensitivity calculation. We leverage outlier ratio based quantization
    sensitivity metric (a) to provide the guidance of the column-level adaptive outlier
    reservation (OR, b) and column-level adaptive precision (AP, c). The AP and OR
    strategies are orthogonal to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Outlier Ratio Based Quantization Sensitivity Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous studies [xiao2023smoothquant](#bib.bib40) ; [lin2023awq](#bib.bib29)
    revealed that outliers in LLMs play a pivotal role in determining the performance
    of quantized models. Inspired by [yin2023owl](#bib.bib44) , we hypothesize that
    the number of outliers within the columns of the weight matrix can act as an indicator
    of the importance (or sensitivity) of each column for quantization. Therefore,
    we adopt outlier ratio as the key guiding metric for LLM quantization.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we introduce a novel, computationally efficient, and inference-agnostic
    adaptive precision guidance metric, termed Outlier Order. It can be utilized to
    allocate adaptive precision (AP) or apply Outlier Reservation (OR) according to
    the proportion of outliers within each column of the model weight. Columns with
    a high proportion of outliers, which are considered more important and sensitive
    to quantization, are allocated higher precision or more outlier reservation budget
    to preserve model capability, while columns with a lower outlier ratio are assigned
    lower precision or less reservation budget.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, for an $i\times j$ is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.E3.m1.3" class="ltx_Math" alttext="R_{j}=\frac{Card(&#124;W_{j}&#124;></math>
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $Card$ as Outlier Order.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Column-Level Adaptive Precision Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mixed Precision quantization (MP) involves the integration of multiple quantization
    levels with varying precisions to mitigate the performance degradation in highly
    compressed models. Mixed-precision has been applied to LLMs in previous studies
    [daillm](#bib.bib26) ; [li2023lijinhao](#bib.bib25) . [daillm](#bib.bib26) leveraged
    gradient magnitudes of parameters as a guiding principle for determining quantization
    precision levels. [li2023lijinhao](#bib.bib25) adopted a conventional criterion
    based on the relative magnitude of parameters concerning the input. Nonetheless,
    their metrics of precision allocation can not accurately reflect the importance
    of parameters and the performances of mixed-precision quantization were unsatisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further improve the performance of low-bit quantization, we present a Column-Level
    Adaptive Precision (AP) strategy directed by Outlier Order, which avoids the heavy
    computational overhead brought by metric calculation. Additionally, experimental
    results reveal the AP approach guided by Outlier Order outperforms previous methods
    with other mixed-precision metrics. To perform the adaptive precision quantization,
    firstly, the parameters of the matrix are sorted per column based on their outlier
    ratio as described in section 3.2\. Secondly, the bit allocation scheme is determined
    according to the target compression ratio. For example, a 2.2-bit quantized model
    is derived by allocating top 10% outlier-concentrated columns to 4-bit, and allocating
    2-bit to the rest. Through AP quantization, the columns with higher sensitivity
    to quantization are allocated higher precision. Let $B$ of the model is denoted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where the threshold $T_{AP}$. Detailed analysis on various bit-width candidates
    in adaptive precision schemes can be found in Appendix D. Experiments on assigning
    different bits to different parameter matrices are described in Appendix G.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Column-Level Adaptive Outlier Reservation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As [lin2023awq](#bib.bib29) ; [lee2024owq](#bib.bib24) ; [kim2023squeezellm](#bib.bib22)
    ; [ashkboos2023quik](#bib.bib1) have revealed, the preservation of an appropriate
    fraction of outliers can significantly improve the performance of quantized models.
    However, previous approaches have simply retained outliers based on the entire
    parameter matrix and resulted in sub-optimal performances.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we propose a Column-Level Adaptive Outlier Reservation (OR) strategy
    to adaptively preserve full-precision outliers across different columns. We leverage
    the outlier ratio defined in Sec 3.2 to guide their retention strategy. The analysis
    of outlier distribution in weight (see Appendix A) illustrates that 90% of outliers
    are concentrated within the top 10% of columns, leading us to adopt a retention
    policy that preserves a greater number of outliers in the initial 10% of columns,
    while conserving fewer outliers in the remaining 90%.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the OR strategy, firstly, we pick top 10% quantization-sensitive columns
    with Outlier Order metric. Then we preserve a higher proportion $o_{1}$-th column
    is presented below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $OR$ represents the threshold of choosing the top 10% columns of higher
    outlier ratios. Columns with higher outlier ratio will be assigned more FP16 outlier
    reservation. Same proportion of outliers is reserved in all matrices to control
    the size of the compressed model. The advantages of this approach are threefold:
    First, by retaining more full-precision parameters in quantization-susceptible
    columns, model performance is better preserved in quantization. Second, the metrics
    employed for outlier reservation align with those used for generating adaptive
    precision quantization, avoiding redundant computational overhead. Thirdly, the
    implementation of outlier reservation and adaptive precision quantization are
    capable of being employed concurrently. Our fusion model (AP+OR) outperforms other
    PTQ methods with similar memory footprints.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The experiments are conducted on popular public LLMs. We adopted LLaMA [touvron2023LLaMA](#bib.bib37)
    and Yi [young2024yi](#bib.bib45) as the baseline models, known for their superior
    performances in English and Chinese language tasks, respectively. The pre-trained
    base versions are employed in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We employ widely-used datasets C4 [raffel2020c4](#bib.bib34) and WiKiText2 [merity2016wiki2](#bib.bib31)
    as perplexity test sets. To evaluate downstream task capabilities, we tested on
    a diverse range of zero-shot benchmarks, including WinoGrande [sakaguchi2021winogrande](#bib.bib35)
    , PiQA [bisk2020piqa](#bib.bib2) , HellaSwag [zellers2019hellaswag](#bib.bib46)
    , ARC (comprising ARC-easy and ARC-challenge) [clark2018think](#bib.bib7) and
    BoolQ [clark2019boolq](#bib.bib6) . Additionally, C4 was chosen as the calibration
    dataset for tuning our quantization process. It is noteworthy to emphasize that
    the choice of calibration data might have an impact on experimental outcomes,
    as detailed in Appendix H.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Results of perplexity on WikiText2 [merity2016wiki2](#bib.bib31) and
    C4 [raffel2020c4](#bib.bib34) of our proposed CLAQ and other methods. CLAQ models
    with * are fusion models employing column-level adaptive precision and outlier
    reservation techniques. The detail of fusion model setups is described in Appendix
    F. $g=128$ means group size of quantization is 128, followed by the equivalent
    bit-width in italics. The results of LLaMA-2 series and Yi-34B models can be found
    in Appendix E.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Bits |'
  prefs: []
  type: TYPE_TB
- en: '&#124; LLaMA1-7B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LLaMA1-13B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LLaMA1-30B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LLaMA1-65B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| – | 16 | 5.63/7.08 | 5.02/6.61 | 4.04/5.97 | 3.49/5.61 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN[frantar2022gptq](#bib.bib15) | 4 | 6.43/7.93 | 5.55/6.98 | 4.57/6.34
    | 3.87/5.85 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ[frantar2022gptq](#bib.bib15) | 4 | 5.99/7.43 | 5.26/6.83 | 4.35/6.20
    | 3.77/5.79 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 10.9/7.4 | 9.6/6.5 | 7.3/6.5 | –
    |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ[lin2023awq](#bib.bib29) | 4 | 6.08/7.52 | 5.34/6.86 | 4.39/6.17 | 3.76/5.77
    |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 4 | 5.86/7.34 | 5.21/6.76 | 4.25/6.11
    | 3.71/5.73 |'
  prefs: []
  type: TYPE_TB
- en: '| SqueezeLLM[kim2023squeezellm](#bib.bib22) | 4.05 | 5.79/7.21 | 5.18/6.71
    | 4.22/6.06 | 3.76/5.69 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR[dettmers2023spqr](#bib.bib10) | 3.94/3.96/3.89/3.90 | 5.87/7.28 | 5.22/6.72
    | 4.25/6.08 | 3.68/5.70 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 4 | 5.78/7.21 | 5.15/6.70 | 4.17/6.06 | 3.62/5.69 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ[frantar2022gptq](#bib.bib15) | 3 | 8.0/9.54 | 6.54/8.15 | 5.59/7.29
    | 4.94/6.69 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ[lin2023awq](#bib.bib29) | 3 | 11.88/13.26 | 7.45/9.13 | 10.07/12.67 |
    5.21/7.11 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 3 | 6.49/8.19 | 5.68/7.32 | 4.74/6.57
    | 4.04/6.07 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | 6.47/7.87 | 5.61/7.14 | 4.79/6.49 | 4.11/6.00 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ[lin2023awq](#bib.bib29) | 3 ($g=128$,3.15) | 6.46/7.92 | 5.51/7.07 |
    4.63/6.37 | 3.99/5.94 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 3 ($g=128$,3.15) | 6.15/7.75 |
    5.44/7.05 | 4.56/6.37 | 3.94/5.93 |'
  prefs: []
  type: TYPE_TB
- en: '| SqueezeLLM[shao2023omniquant](#bib.bib36) | 3.24 | 6.13/7.56 | 5.45/6.92
    | 4.44/6.23 | 3.88/5.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 3.12 | 5.97/7.40 | 5.27/6.83 | 4.35/6.18 | 3.75/5.78 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 3.23 | 5.95/7.37 | 5.24/6.81 | 4.33/6.16 | 3.74/5.76 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ[frantar2022gptq](#bib.bib15) | 2 | 2148.66/691.25 | 8730.02/1467.84
    | 508.02/188.73 | 58.34/39.57 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | 27.64/24.37 | 21.05/19.69 | 15.37/13.43 | 97.16/48.95 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ[lin2023awq](#bib.bib29) | 2 ($g=64$,2.28) | 2.5e5/2.8e5 | 2.7e5/2.2e5
    | 2.3e5/2.3e5 | 7.4e4/7.4e4 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 2 ($g=64$,2.28) | 8.90/11.78 |
    7.34/9.75 | 6.59/8.65 | 5.65/7.60 |'
  prefs: []
  type: TYPE_TB
- en: '| decoupleQ[guo2024decoupleq](#bib.bib16) | 2 ($g=64$,2.28) | 8.18/- | 6.96/-
    | 5.81/- | 5.07/- |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.12 | 7.57/8.87 | 6.41/7.92 | 5.40/7.05 | 4.70/6.46 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.24 | 6.93/8.35 | 5.99/7.52 | 5.03/6.74 | 4.41/6.22 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The evaluation result of Zero-Shot accuracy of our proposed CLAQ and
    other methods. CLAQ models with * are fusion models employing column-level adaptive
    precision and outlier reservation techniques. The results of LLaMA-2 series, Yi-34B
    model and experiments on other bit-width can be found in Appendix E.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | # Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 78.3 | 70.0 | 45.0 | 75.5 | 74.0
    | 69.0 | 68.63 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 78.67 | 73.23 | 42.83 | 74.16
    | 75.07 | 70.48 | 69.07 |'
  prefs: []
  type: TYPE_TB
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 77.09 | 51.89 | 40.87
    | 72.53 | 71.61 | 65.03 | 63.17 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.63 | 78.45 | 67.13 | 38.23 | -
    | 56.01 | 67.48 | 61.46 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | CLAQ | 4 | 78.29 | 75.29 | 43.25 | 74.70 | 75.26 | 70.32 | 69.52
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 52.50 | 25.76 | 26.96 | 41.87
    | 25.77 | 52.49 | 37.56 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 2.12 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 | 64.29 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 79.4 | 72.8 | 52.0 | 77.7 | 77.7
    | 71.5 | 71.85 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 79.60 | 76.60 | 47.87 | 76.45
    | 77.89 | 71.90 | 71.72 |'
  prefs: []
  type: TYPE_TB
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 78.40 | 57.28 | 42.91
    | 67.00 | 75.82 | 68.27 | 64.95 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.63 | 78.94 | 74.37 | 43.17 | -
    | 59.02 | 69.77 | 65.05 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-13B | CLAQ | 4 | 79.65 | 76.94 | 48.04 | 77.74 | 78.58 | 72.85 | 72.30
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 52.18 | 25.88 | 28.41 | 41.35
    | 25.67 | 49.64 | 37.19 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 2.12 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14 | 66.94 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 81.0 | 79.4 | 56.8 | 81.8 | 81.8
    | 75.1 | 75.98 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 81.12 | 76.81 | 50.00 | 79.36
    | 81.43 | 74.66 | 73.90 |'
  prefs: []
  type: TYPE_TB
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 79.81 | 58.79 | 45.22
    | 68.38 | 78.95 | 72.21 | 67.23 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.69 | 81.01 | 76.05 | 47.18 | -
    | 62.50 | 72.93 | 67.93 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-30B | CLAQ | 4 | 81.18 | 80.39 | 54.10 | 82.17 | 81.91 | 75.85 | 75.93
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 53.16 | 27.74 | 26.79 | 43.46
    | 27.10 | 48.46 | 37.79 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 2.12 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64 | 72.26 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 82.48 | 80.89 | 54.01 | 83.55
    | 83.47 | 76.68 | 76.85 |'
  prefs: []
  type: TYPE_TB
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 81.01 | 58.12 | 46.33
    | 80.64 | 79.91 | 75.69 | 70.28 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.71 | 81.56 | 75.25 | 46.93 | -
    | 63.76 | 76.95 | 68.89 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-65B | CLAQ | 4 | 82.43 | 81.52 | 54.69 | 83.85 | 83.56 | 76.40 | 77.08
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 53.59 | 28.62 | 27.22 | 54.80
    | 30.09 | 51.62 | 40.99 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 2.12 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30 | 73.86 |'
  prefs: []
  type: TYPE_TB
- en: Implementation Details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To implement CLAQ, we leveraged the accelerated K-Means clustering algorithm
    provided by the scikit-learn-intelex library ¹¹1https://github.com/intel/scikit-learn-intelex
    (Apache-2.0 license). Our experiments were built upon the GPTQ framework ²²2https://github.com/IST-DASLab/gptq
    (Apache-2.0 license). For benchmarking purposes, the LM-Evaluation-Harness toolkit
    ³³3https://github.com/EleutherAI/lm-evaluation-harness (MIT License) served as
    the assessment platform. All experiments were conducted on a server with 8 NVIDIA
    A100 GPUs, each with 80GB capacity. Notably, a single NVIDIA A100 GPU is sufficient
    for the majority of our experiments except evaluating the LLaMA-65B model on zero-shot
    tasks, where two GPUs were used to accommodate its memory demands. Our contribution
    was mainly confined to the quantization of model weights, with activations left
    unquantized. Unless otherwise specified within the experimental results table,
    a 16-bit precision was maintained for activations. Comprehensive details regarding
    the hyper-parameters employed in our experiments can be found in Appendix F.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main results of our experiments are presented in Table [1](#S4.T1 "Table
    1 ‣ Datasets ‣ 4.1 Experimental Settings ‣ 4 Experiments and Results ‣ CLAQ: Pushing
    the Limits of Low-Bit Post-Training Quantization for LLMs") and Table LABEL:table:2.
    Table [1](#S4.T1 "Table 1 ‣ Datasets ‣ 4.1 Experimental Settings ‣ 4 Experiments
    and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for
    LLMs") shows the evaluations on perplexity, while the performance of zero-shot
    tasks is given in Table LABEL:table:2. It can be clearly observed that CLAQ consistently
    outperforms existing methods in various scenarios. Notably, in case of 3-bit and
    4-bit single-precision quantization with a comparable codebook size configuration,
    our method generally outperforms the baselines examined.'
  prefs: []
  type: TYPE_NORMAL
- en: For lower bit-width, we employ the fusion approach of AP and OR. Results show
    that with an additional 0.1-bit memory footprint, model accuracy is significantly
    boosted. Especially for compression around 2-bit (2.1 bits in our case), our method
    exhibits a consistent and substantial lead over all existing methods, achieving
    the state-of-the-art (SOTA) performance (significant difference at 0.01 confidence
    level).
  prefs: []
  type: TYPE_NORMAL
- en: Also, the zero-shot evaluation results demonstrate the superiority of our method.
    CLAQ outperforms other baselines in most cases. Specifically, in the case of 4-bit
    quantization, our solution closely matches the performance of full-precision models.
    For low-bit scenarios, our method significantly improve the accuracy of the quantized
    model in zero-shot tasks, and pushing the limits of large language model quantization
    under low-bit conditions. More zero-shot task results on LLaMA-2 series and Yi-34B
    models can be found in Appendix E.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.3.1 Adaptive Precision Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.3.2 Outlier Reservation ‣ 4.3 Ablation Study
    ‣ 4 Experiments and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") presents a comparison of different Adaptive Precision
    (AP) quantization methods across various precision levels. Inspired by [frantar2023sparsegpt](#bib.bib14)
    , we developed a magnitude-to-parameter method to guide the mixed-precision allocation.
    Experimental results demonstrate that our proposed column-level adaptive precision
    quantization strategy significantly outperforms other mixed-precision methods
    under comparable size. The improvements can be attributed to the superiority of
    our Outlier Order metric. According to the outlier distribution analysis (see
    Appendix A), a minority of columns exhibit higher sensitivity to quantization.
    Consequently, allocating additional precision on preserving these critical columns
    is an effective way to improve the performance of quantized models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Outlier Reservation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.3.2 Outlier Reservation ‣ 4.3 Ablation Study
    ‣ 4 Experiments and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") lists the results of the fixed and Outlier Reservation
    (OR) strategy, highlighting its superiority over static outlier preservation approaches.
    Remarkably, with an equal amount of increase in model sizes, the performance gain
    from retaining more outliers exceeds that from adaptive precision strategy. This
    result suggests the importance of outlier retention, indicating that directly
    preserving outliers at full precision is superior to quantizing them with a higher
    precision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Ablation study results for our proposed column-level adaptive precision
    quantization. † denotes a column-wise mixed-precision quantization with the metric
    of activation-to-weight ratio proposed in [frantar2023sparsegpt](#bib.bib14) .'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # Bits | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag
    | Winogrande | Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 |
    67.35 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
  prefs: []
  type: TYPE_TB
- en: '| +MP† | 2.5 | 8.63 | 9.98 | 75.35 | 65.15 | 36.51 | 69.08 | 62.96 | 64.71
    | 62.29 |'
  prefs: []
  type: TYPE_TB
- en: '| +AP(ours) | 2.5 | 8.43 | 9.78 | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40
    | 62.26 |'
  prefs: []
  type: TYPE_TB
- en: '| +MP† | 2.2 | 11.60 | 12.70 | 71.00 | 57.91 | 29.43 | 65.22 | 51.07 | 59.03
    | 55.61 |'
  prefs: []
  type: TYPE_TB
- en: '| +AP(ours) | 2.2 | 10.14 | 11.46 | 73.39 | 64.10 | 31.91 | 63.52 | 60.08 |
    62.83 | 59.31 |'
  prefs: []
  type: TYPE_TB
- en: '| +MP† | 2.1 | 16.42 | 16.57 | 66.86 | 44.02 | 26.53 | 62.56 | 44.03 | 53.59
    | 49.60 |'
  prefs: []
  type: TYPE_TB
- en: '| +AP(ours) | 2.1 | 11.06 | 12.34 | 72.42 | 63.17 | 31.06 | 64.92 | 58.33 |
    61.25 | 58.53 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Ablation study results for our proposed column-level adaptive outlier
    reservation. Outlier fix refers to keeping a fixed proportion of outliers in each
    column.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # Bits | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag
    | Winogrande | Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
  prefs: []
  type: TYPE_TB
- en: '| +Outlier fix | 2.28 | 7.49 | 8.87 | 74.70 | 68.56 | 37.88 | 71.52 | 67.03
    | 66.69 | 64.40 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR(ours) | 2.28 | 6.88 | 8.35 | 77.75 | 69.36 | 39.76 | 74.40 | 70.62 |
    67.09 | 66.50 |'
  prefs: []
  type: TYPE_TB
- en: '| +Outlier fix | 2.14 | 8.31 | 9.61 | 75.19 | 66.41 | 35.32 | 71.40 | 63.12
    | 66.14 | 62.93 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR(ours) | 2.14 | 7.22 | 8.64 | 76.28 | 69.02 | 37.88 | 73.70 | 68.14 |
    68.11 | 65.52 |'
  prefs: []
  type: TYPE_TB
- en: 5 Limitations and Future Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main limitations of our work lie in the simplification of outlier retention
    and adaptive precision search strategies. We adopt two-level precision in AP for
    the convenience of CUDA kernel development. However, when the assignable precision
    bit-width exceeds the equivalent of 0.5 bit (i.e., >2.5bit, >3.5bit), the adaptive
    precision strategy outlined in our method may not represent the optimal approach.
    The potential for generating superior adaptive precision configurations is discussed
    in Appendix G, where we propose a heuristic adaptive precision search algorithm.
    Our future work is twofold: (1) We will continue to investigate the accomplishment
    of superior adaptive precision quantization configurations through efficient search
    algorithms. (2) We are developing customized CUDA kernels to support efficient
    computation of the CLAQ approach on GPUs. Compared with the AP quantization, dynamically
    preserving outliers per column is more challenging in deployment. We are actively
    refining the underlying algorithms, and the code will be released soon in the
    future.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have introduced a novel Column-Level Adaptive weight Quantization
    (CLAQ) framework, which integrates column-level adaptive precision quantization
    and outlier reservation techniques. Guided by the column-wise outlier proportion
    metric, the CLAQ method achieves efficient compression of LLMs while maximizing
    the preservation of model accuracy. Extensive evaluations on public LLM models
    confirm the superiority of CLAQ in enhancing performance across varied bit-widths,
    notably in ultra-low bit cases. The results lend help to making LLMs more efficient
    and accessible in resource-constrained applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang,
    Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference
    on generative large language models. arXiv preprint arXiv:2310.09259, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(2) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (3) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
    Dhariwal, Arvind Neelakantan, et al. Language models are few-shot learners. In
    Advances in Neural Information Processing Systems, volume 33, pages 1877–1901,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(4) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(5) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(6) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (7) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(8) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3\.
    int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural
    Information Processing Systems, 35:30318–30332, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(9) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(10) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(11) Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. In International Conference on Machine Learning, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(12) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,
    and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural
    networks. Advances in neural information processing systems, 33:18518–18529, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(13) Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. Advances in Neural Information
    Processing Systems, 35:4475–4488, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(14) Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can
    be accurately pruned in one-shot. In International Conference on Machine Learning,
    pages 10323–10337\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(15) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(16) Yi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping
    Cai, Yang Zhang, and Shouda Liu. decoupleq: Towards 2-bit post-training uniform
    quantization via decoupling parameters into integer and floating points. arXiv
    preprint arXiv:2404.12759, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (17) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
    Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling
    laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (18) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon,
    and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit
    weight quantization of large language models. arXiv preprint arXiv:2309.15531,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(19) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (20) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
    Accurate post training quantization with small calibration sets. In International
    Conference on Machine Learning, pages 4466–4475\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (21) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(22) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(23) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(24) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
    Owq: Outlier-aware weight quantization for efficient fine-tuning and inference
    of large language models. In Proceedings of the AAAI Conference on Artificial
    Intelligence, volume 38, pages 13355–13364, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(25) Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang,
    and Guohao Dai. Enabling fast 2-bit llm on gpus: Memory alignment, sparse outlier,
    and asynchronous dequantization. arXiv preprint arXiv:2311.16442, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(26) Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li,
    Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization
    for efficient llm deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(27) Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
    Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language
    models. arXiv preprint arXiv:2310.08659, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(28) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(29) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(30) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (31) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (32) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen
    Blankevoort. Up or down? adaptive rounding for post-training quantization. In
    International Conference on Machine Learning, pages 7197–7206\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(33) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok
    Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized
    matrix multiplication based on luts for efficient inference in large-scale generative
    language models. arXiv preprint arXiv:2206.09557, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (34) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(35) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(36) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(37) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (38) Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate
    post-training network quantization via bit-split and stitching. In International
    Conference on Machine Learning, pages 9847–9856\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(39) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit
    of low-bit transformer language models. Advances in Neural Information Processing
    Systems, 35:17402–17414, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(40) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(41) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(42) Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu,
    Weidong Liu, and Wanxiang Che. Onebit: Towards extremely low-bit large language
    models. arXiv preprint arXiv:2402.11295, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(43) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(44) Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia,
    Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed
    layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity.
    arXiv preprint arXiv:2310.05175, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(45) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang,
    Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation
    models by 01\. ai. arXiv preprint arXiv:2403.04652, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(46) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(47) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Analysis on outlier ratio in LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section 3.2, we underscored the significance of outliers in the context
    of Large Language Model (LLM) compression. Here, we present concrete examples
    to demonstrate the efficacy of our approach. Initially, we observe notable variations
    in outlier prevalence among different columns within each parameter matrix. Figure
    [3](#A1.F3 "Figure 3 ‣ Appendix A Analysis on outlier ratio in LLM ‣ CLAQ: Pushing
    the Limits of Low-Bit Post-Training Quantization for LLMs") illustrates the statistics
    of outliers detected when applying an outlier standard deviation $S=7$ to the
    layer layers.0.self_attn.o_proj.weight. The findings highlight that in the LLaMA
    architecture, outliers are confined to a minority of columns, with a striking
    imbalance in their distribution across columns. Consequently, rationing the limited
    precision to those columns exhibiting a higher concentration of outliers is advocated
    as a strategy to maximize performance gains.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3c3d34b67c382fc7f9cba0ea235454f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The sorted outliers ratio in a self-attention matrix of LLaMA1-7B,
    most columns contain few outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Analysis on outlier ratio in LLM ‣
    CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs") displays
    the top 10% columns with higher outlier ratio in the same matrix, which are more
    evenly distributed with no apparent pattern. This intuitive evidence highlights
    the heterogeneity between these columns, further emphasizing the effectiveness
    for selective precision assignments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f37ac04740eb87a271afb22494a1c95e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The position of columns with higher outlier ratio in matrix. Columns
    with dark colour are the top 10% outlier concentrated columns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extending the analysis to the entire model, Figure [5](#A1.F5 "Figure 5 ‣ Appendix
    A Analysis on outlier ratio in LLM ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") summarizes the quantization outcomes for all 32 decoder
    layers in LLaMA1-7B. From a holistic model perspective, the initial layers exhibit
    a disproportionately high outlier incidences. Aligning with our hypothesis, this
    suggests that these layers hold increased significance. Considering the disparities
    between layers, we have explored a heuristic-based search strategy for identifying
    potentially improved adaptive precision allocation schemes, details of this approach
    are elaborated in Appendix G.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03f8f3776380e914fede861936afe6c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The overall outlier ratio of 32 layers in LLaMA1-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Ablation study on outlier definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The selection of the criterion for defining outliers is integral to the efficacy
    of the Outlier Order search process. To this end, we conducted an ablation study
    examining the impact of varying the outlier standard, denoted as $S$ emerges as
    the optimal setting, striking a balance where it neither lowers the threshold
    excessively to include an abundance of values nor sets the bar so high that certain
    columns lack any outliers. This configuration ensures accurate identification
    of meaningful outliers that significantly influence the performance of the model
    quantization strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Ablation study on the standard of outlier definition on LLaMA1-7B.
    CLAQ* denotes the column-level adaptive precision quantization is applied.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Bits | Outlier Standard | WikiText2↓ | C4↓ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=1$ | 16.39 | 16.47 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=3$ | 13.44 | 14.52 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=5$ | 10.76 | 12.07 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=7$ | 10.58 | 11.71 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=9$ | 10.37 | 11.64 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=11$ | 10.25 | 11.58 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=13$ | 10.14 | 11.46 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=15$ | 10.32 | 11.53 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.2 | $S=17$ | 10.34 | 11.51 |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Ablation study on hyper parameter in outlier reservation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Observations have indicated that the top 10% of columns harbor approximately
    90% of the outliers, prompting an investigation into the optimal strategy for
    allocating outlier retention within these columns with top outlier ratio versus
    the remaining 90%. We keep this proportion of columns to retain adaptively outliers
    in our experiments. The challenge lies in determining the most efficacious ratio
    for preserving outliers in these two segments. Various configurations were trialed
    in pursuit of the most suitable distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We did grid search on the choice of outlier reservation ratio. We found 3 promising
    configuration settings: Setting 1: 19% for high outlier ratio columns, 81% for
    90% low outlier ratio columns. Setting 2: 28% for high outlier ratio columns,
    72% for 90% low outlier ratio columns. Setting 3: 37% for high outlier ratio columns,
    63% for 90% low outlier ratio columns. Experimental results shows setting 3 outperforms
    in terms of PPL, while setting 2 obtained the higher accuracy on zero-shot tasks
    than other settings. The performance gap on PPL is more obvious than zero-shot
    accuracy. Finally, striking the balance between downstream task accuracy and perplexity,
    we decide to choose Setting 2 for our main experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Experimental results of ablation study conducted on different hyper-parameter
    settings about outlier reservation proportion.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # Bits | Exp. Config | WikiText2↓ | C4↓ | Zero-Shot Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | 16 | – | 5.63 | 7.08 | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| +OR | 2.28 | Setting1 | 7.01 | 8.47 | 65.98 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR | 2.28 | Setting2 | 6.88 | 8.35 | 66.50 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR | 2.28 | Setting3 | 6.77 | 8.31 | 65.21 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR | 2.14 | Setting1 | 7.55 | 8.93 | 64.31 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR | 2.14 | Setting2 | 7.22 | 8.64 | 65.52 |'
  prefs: []
  type: TYPE_TB
- en: '| +OR | 2.14 | Setting3 | 7.00 | 8.49 | 65.30 |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Ablation study on adaptive precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the objective of facilitating deployment at the hardware level, our approach
    adopts a dual-precision quantization scheme. A dilemma arises when targeting a
    quantization granularity between 2 to 3 bits: given an equivalent model size constraint,
    is it more beneficial to incorporate 2+3 mixture, or 2+4 mixture? To address this
    question, we designed a series of experiments under three distinct outlier standard
    conditions. The outcomes of which are summarized in Table LABEL:table:7. The outcomes
    consistently lead to a single, definitive conclusion: opting for fewer columns
    at 4 bits is preferable. This finding aligns with observations detailed in Appendix
    A, which underscores that columns with a high outlier count are relatively rare.
    Prioritizing higher precision for these selected columns proves crucial in achieving
    superior overall quantization efficacy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Experimental results of ablation study on LLaMA1-7B about adaptive
    precision candidate bit-width selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Bits | Bits in AP | Outlier Standard | WikiText2↓ | C4↓ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AP | 2.1 | 2&3 | $S=5$ | 12.35 | 13.77 |'
  prefs: []
  type: TYPE_TB
- en: '| AP | 2.1 | 2&4 | $S=5$ | 12.21 | 13.21 |'
  prefs: []
  type: TYPE_TB
- en: '| AP | 2.1 | 2&3 | $S=9$ | 11.89 | 13.07 |'
  prefs: []
  type: TYPE_TB
- en: '| AP | 2.1 | 2&4 | $S=9$ | 11.52 | 12.73 |'
  prefs: []
  type: TYPE_TB
- en: '| AP | 2.1 | 2&3 | $S=13$ | 11.67 | 12.78 |'
  prefs: []
  type: TYPE_TB
- en: '| AP | 2.1 | 2&4 | $S=13$ | 11.06 | 12.34 |'
  prefs: []
  type: TYPE_TB
- en: Appendix E More experimental results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More experiments on LLaMA2 and Yi are shown in the following tables. The results
    are consistent with the conclusions in the main results section. Our proposed
    CLAQ outperformed other quantization method in most settings; especially, CLAQ
    with AP and OR achieves SOTA performance under the low-bit condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Experimental results of perplexity in LLaMA-2-Base models. CLAQ models
    with * are fusion models employing column-level adaptive precision and outlier
    reservation techniques. $g=128$ means group size of quantization is 128, followed
    by the equivalent bit-width in italics.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Bits |'
  prefs: []
  type: TYPE_TB
- en: '&#124; LLaMA2-7B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LLaMA2-13B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| - | 16 | 5.46/6.97 | 4.88/6.46 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 4 | 5.82/7.36 | 5.13/6.70 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4 | 6.15/7.68 | 5.12/6.74 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4 ($g=128$,4.15) | 5.62/7.13 | 4.97/6.56 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 4 | 5.74/7.35 | 5.02/6.65 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 4 ($g=128$,4.15) | 5.58/7.12 | 4.95/6.56 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 4 | 5.62/7.13 | 5.00/6.56 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 3 | 8.49/9.92 | 6.40/8.00 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 3 | 24.00/23.85 | 10.45/13.07 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 3 | 6.58/8.65 | 5.58/7.44 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | 6.39/7.80 | 5.46/7.03 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 3 ($g=128$,3.15) | 6.24/7.84 | 5.32/6.94 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 3 ($g=128$,3.15) | 6.03/7.75 | 5.28/6.98 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 3.12 | 5.81/7.34 | 5.15/6.72 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 3.23 | 5.79/7.31 | 5.12/6.70 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 2 | Nan/2477.60 | 1832.06/349.17 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | 7579.07/1412.06 | 4853.96/3404.98 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 2 ($g=64$,2.28) | 2.1e5/1.6e5 | 1.2e5/9.5e4 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 2 ($g=64$,2.28) | 9.62/12.72 | 7.56/10.05 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.12 | 7.69/9.13 | 6.30/7.94 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.24 | 6.89/8.47 | 5.88/7.50 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Experimental results of perplexity in Yi-34B-Base model. CLAQ models
    with * are fusion models employing column-level adaptive precision and outlier
    reservation techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Bits |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Yi-34B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiText2↓/C4↓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| - | 16 | 24.73/29.57 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 4 | 32.10/40.55 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 4 | 26.38/31.38 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 3 | 59.16/69.45 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | 56.03/67.69 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 3.12 | 26.85/31.82 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 3.23 | 28.89/33.24 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 2 | 9577.46/9018.70 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | 199.86/247.80 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.12 | 46.04/51.31 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ* | 2.24 | 41.80/45.90 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Experimental results of zero-shot tasks in LLaMA-2-Base and Yi-34B-Base
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | # Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 79.11 | 76.30 | 46.42 | 77.77 | 75.99 | 69.06 | 70.78 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 4 | 79.05 | 74.71 | 45.14 | 77.58 | 74.33 | 69.30 | 70.02 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B | CLAQ | 4 | 78.45 | 75.76 | 44.71 | 77.25 | 75.17 | 68.03 | 69.90
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 80.52 | 79.46 | 49.15 | 80.58 | 79.38 | 72.22 | 73.55 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 4 | 80.41 | 78.58 | 47.18 | 79.27 | 78.08 | 71.51 | 72.51 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B | CLAQ | 4 | 78.51 | 79.21 | 50.43 | 78.60 | 79.76 | 72.22 | 73.12
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 82.75 | 84.34 | 62.12 | 88.35 | 83.64 | 79.24 | 80.07 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 4 | 81.66 | 81.48 | 59.04 | 86.15 | 81.95 | 76.16 | 77.74 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-34B | CLAQ | 4 | 82.64 | 83.29 | 60.15 | 88.59 | 83.26 | 77.19 | 79.19
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Experimental results of zero-shot accuracy in LLaMA-2-Base and Yi-34B-Base
    models. CLAQ models with* are fusion models employing column-level adaptive precision
    and outlier reservation techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | # Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 75.52 | 64.10 | 35.58 | 66.76 | 68.31 | 63.30 | 62.26 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 78.89 | 72.60 | 42.92 | 73.52 | 74.32 | 69.69 | 68.66 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 52.50 | 25.76 | 26.96 | 41.87 | 25.77 | 52.49 | 37.56 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | CLAQ* | 2.1 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 |
    64.29 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 77.69 | 73.40 | 42.75 | 68.56 | 73.14 | 66.54 | 67.01 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 79.92 | 76.43 | 45.90 | 76.27 | 78.15 | 71.35 | 71.34 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 52.18 | 25.88 | 28.41 | 41.35 | 25.67 | 49.64 | 37.19 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-13B | CLAQ* | 2.1 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14
    | 66.94 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 78.78 | 73.65 | 46.50 | 75.78 | 77.86 | 74.59 | 71.19 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 81.66 | 78.96 | 51.02 | 82.11 | 81.49 | 74.66 | 74.98 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 53.16 | 27.74 | 26.79 | 43.46 | 27.10 | 48.46 | 37.79 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-30B | CLAQ* | 2.1 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64
    | 72.26 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 80.67 | 78.07 | 51.45 | 80.15 | 80.96 | 74.35 | 74.28 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 81.88 | 80.51 | 55.72 | 84.43 | 83.47 | 77.11 | 77.19 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 53.59 | 28.62 | 27.22 | 54.80 | 30.09 | 51.62 | 40.99 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-65B | CLAQ* | 2.1 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30
    | 73.86 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 79.11 | 76.30 | 46.42 | 77.77 | 75.99 | 69.06 | 70.78 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 74.81 | 66.04 | 38.65 | 69.63 | 66.70 | 63.54 | 63.23 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 78.62 | 74.24 | 42.83 | 75.47 | 74.33 | 69.53 | 69.17 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 52.12 | 26.52 | 27.90 | 42.45 | 26.11 | 48.07 | 37.20 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B | CLAQ* | 2.1 | 75.03 | 70.50 | 38.23 | 72.81 | 66.82 | 66.77 |
    65.03 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 80.52 | 79.46 | 49.15 | 80.58 | 79.38 | 72.22 | 73.55 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 77.97 | 73.02 | 43.86 | 74.07 | 73.88 | 68.11 | 68.49 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 79.54 | 78.07 | 49.23 | 76.67 | 77.81 | 71.43 | 71.63 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 53.16 | 25.67 | 27.90 | 39.69 | 25.61 | 49.88 | 36.99 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B | CLAQ* | 2.1 | 77.53 | 74.75 | 43.43 | 76.88 | 72.79 | 70.80
    | 69.36 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16 | 82.75 | 84.34 | 62.12 | 88.35 | 83.64 | 79.24 | 80.07 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 3 | 75.30 | 64.23 | 38.23 | 64.98 | 67.93 | 64.72 | 62.57 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLAQ* | 3.1 | 83.30 | 83.33 | 58.79 | 87.86 | 82.56 | 77.35 | 78.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 2 | 51.90 | 25.04 | 27.30 | 40.52 | 25.97 | 48.07 | 36.47 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi-34B | CLAQ* | 2.1 | 81.07 | 80.43 | 54.27 | 79.36 | 77.49 | 74.90 | 74.59
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix F Experiment details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our experiments are conducted based on a branching off from the GPTQ[frantar2022gptq](#bib.bib15)
    implementation. The quantization group size is set to the entire column. The calibration
    data consists of 128 random 2048 token segments from the C4 dataset. The K-Means
    clustering is calculated with the scikit-learn-intelex CPU version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fusion models with AP+OR utilize two settings: (1) 2.12/3.12bit: 0.05 bit
    increment in adaptive precision with 2&4 bits, and 0.07 bit of full-precision
    outliers kept with Setting 2 in Appendix C. (2) 2.24/3.23bit: 0.1 bit increment
    in adaptive precision with 2&4 bits, and 0.13 bit of full-precision outliers kept
    with Setting 2 in Appendix C. The outlier standard is set to $S=13$ in all experiments.
    Based on our observations in Appendix A, we fixed the column proportion of outliers
    to be retained in our experiments. We always preserve more outliers in the most
    sensitive top 10% of columns, while retaining fewer outliers in the remaining
    90% of columns. The code for reproducing the results in Table [1](#S4.T1 "Table
    1 ‣ Datasets ‣ 4.1 Experimental Settings ‣ 4 Experiments and Results ‣ CLAQ: Pushing
    the Limits of Low-Bit Post-Training Quantization for LLMs"), Table LABEL:table:2
    and other tables in Appendix E is contained in supplemental materials. Check the
    ReadMe file for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Heuristic adaptive precision search algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our adaptive precision approach, detailed in Section 3.3, combines two precision
    levels. However, when dealing with expanded adaptive precision search landscapes,
    such as 2.5 bits instead of 2.1 bits, straightforward methodologies may not consistently
    discover the most advantageous configurations. To address this complexity, we
    propose a heuristic-based adaptive precision search algorithm. This algorithm,
    inspired by the HAWQ v2 [dong2020hawq](#bib.bib12) method, initially ranks weight
    matrices by their outlier ratios, ensuring matrices with higher outlier concentrations
    receive higher precision. This step transforms the search space into a combinatorial
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discretize the precision levels for each column to a defined set, thereby
    confining the search within a manageable subset of all possible combinations under
    the model size constraint. We traverse this reduced search space, exploring all
    feasible precision arrangements, and establish a precision scoring system to facilitate
    the selection of the most suitable configuration. In essence, this heuristic-driven
    adaptive precision search process defines discrete precision options for each
    column, enumerates all feasible combinations within the narrowed search space,
    and selects the combination with the highest precision score. This systematic
    approach optimizes the use of precision levels across matrices, thereby enhancing
    the overall quantization efficiency under practical constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in the case of an adaptive precision search involving 2-bit and
    3-bit quantization levels, we categorize each matrix’s potential precision into
    three classes: 2-bit, a combination of 2&3-bit, and a combination of 2&4-bit.
    We allocate a higher precision score $PS_{4}$ can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle P=\left\{p_{3},p_{4}\right\}\quad M=\left\{M_{3},M_{4}\right\}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle P,L=\mathop{\arg\max}_{q}\enspace PS_{total}$ |  | (8)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Where $P$ for experiments in Table [12](#A7.T12 "Table 12 ‣ Appendix G Heuristic
    adaptive precision search algorithm ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs"). The precision score for 2-bit columns are 0\. This heuristic
    search strategy realizes adaptive precision quantization by exhaustively enumerating
    all feasible combinations of precision allocations, subsequently computing a search
    criterion based on the weighted parameter precision scores, which are derived
    from the outlier ratios. The goal of adaptive precision search process is to find
    the combination with the highest precision score. Following this adaptive precision
    search strategy, in scenarios where the incremental bit-width is modest (i.e.,
    2.1 or 2.2 bits), the search results favor a configuration utilizing the maximum
    number of 2&4-bit matrices, which is similar to our previously described approach.
    In the case of a 2.5-bit search, 19 matrices allocated as 2&4-bit with 10% 4-bit
    columns, 205 2&3-bit matrices are allocated with 52.6% 3-bit columns. Nevertheless,
    the model consistently minimizes the proportion of parameters quantized solely
    at 2 bits. The results of the heuristic algorithm based adaptive precision quantization
    are detailed in Table [12](#A7.T12 "Table 12 ‣ Appendix G Heuristic adaptive precision
    search algorithm ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization
    for LLMs"), evidencing that heuristic-based searching can indeed refine model
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Results of heuristic adaptive precision search at 2.5-bit.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # Bits | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag
    | Winogrande | Avg↑ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 |
    67.35 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
  prefs: []
  type: TYPE_TB
- en: '| +AP(ours) | 2.5 | 8.43 | 9.78 | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40
    | 62.26 |'
  prefs: []
  type: TYPE_TB
- en: '| +AP(Heuristic AP search) | 2.5 | 7.88 | 9.40 | 75.95 | 68.73 | 36.01 | 72.39
    | 65.98 | 64.72 | 63.96 |'
  prefs: []
  type: TYPE_TB
- en: 'However, further analyses indicate that the stability of our heuristic adaptive
    precision search strategy is somewhat inconsistent, with results being heavily
    influenced by the precision score assignment. This highlights a key insight: developing
    an end-to-end, effective, and interpretable adaptive precision search strategy
    that consistently improves performance under varying conditions remains a significant
    challenge for future research. The pursuit of a robust and explainable algorithm
    capable of systematically navigating the extensive search-space of adaptive precision
    configurations, optimizing both model performance and computational efficiency,
    is a critical direction for advancing the field of model quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Ablation study on calibration data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We investigated the impact of calibration data on experimental outcomes, aligning
    with the data selection used in GPTQ [frantar2022gptq](#bib.bib15) . Calibration
    data consists of 128 random 2048 token segments from the C4 [raffel2020c4](#bib.bib34)
    dataset. We found the non-negligible effect of varying calibration datasets, emphasizing
    that tailoring the calibration data to specific datasets positively influences
    the perplexity (PPL) evaluation for those datasets. This highlights the importance
    of a calibrated and dataset-specific approach in enhancing quantization performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: Experimental results of CLAQ models calibrated on C4 [raffel2020c4](#bib.bib34)
    and Wikitext2 [merity2016wiki2](#bib.bib31) .'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # Bits | Calibration | WikiText2↓ | C4↓ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B | 16 | – | 5.63 | 7.08 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 4 | on wiki | 5.72 | 7.23 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 4 | on c4 | 5.78 | 7.21 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | on wiki | 6.04 | 7.89 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 3 | on c4 | 6.47 | 7.87 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | on wiki | 32.35 | 44.95 |'
  prefs: []
  type: TYPE_TB
- en: '| CLAQ | 2 | on c4 | 27.64 | 24.37 |'
  prefs: []
  type: TYPE_TB
- en: Appendix I Broader Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our proposed LLM compression approach has the potential to enhance the compression
    ratio of mainstream open-source LLMs by up to 8x, with nearly lossless compression
    enhancement reaching up to 4x. This expands the applicability of LLMs. However,
    it is beyond our scope to determine whether the compressed models maintain their
    intended behavior post-compression; we lack the capability to monitor the exact
    usage scenarios of our compressed models or ascertain whether they might be repurposed
    for unintended applications.
  prefs: []
  type: TYPE_NORMAL
