- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:05:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Outlier Weighed Layerwise Sparsity (OWL ): A Missing Secret Sauce for Pruning
    LLMs to High Sparsity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.05175](https://ar5iv.labs.arxiv.org/html/2310.05175)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lu Yin¹,  You Wu³,  Zhenyu Zhang²,  Cheng-Yu Hsieh⁴,  Yaqing Wang³,  Yiling
    Jia³
  prefs: []
  type: TYPE_NORMAL
- en: Mykola Pechenizkiy¹,  Yi Liang³,  Zhangyang Wang²,  Shiwei Liu^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Eindhoven University of Technology, ²University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: ³Google Research,NY, ⁴University of Washington Partial of this work have been
    done while Lu Yin worked as a Research Intern at Google Research, NY. Corresponding
    to Lu Yin (l.yin@tue.nl) and Shiwei Liu (s.liu3@tue.nl).
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs), renowned for their remarkable performance across
    diverse domains, present a challenge due to their colossal model size when it
    comes to practical deployment. In response to this challenge, efforts have been
    directed toward the application of traditional network pruning techniques to LLMs,
    uncovering a massive number of parameters can be pruned in one-shot without hurting
    performance. Building upon insights gained from pre-LLM models, particularly BERT-level
    language models, prevailing LLM pruning strategies have consistently adhered to
    the practice of uniformly pruning all layers at equivalent sparsity levels, resulting
    in robust performance. However, this observation stands in contrast to the prevailing
    trends observed in the field of vision models, where non-uniform layerwise sparsity
    typically yields substantially improved results. To elucidate the underlying reasons
    for this disparity, we conduct a comprehensive analysis of the distribution of
    token features within LLMs. In doing so, we discover a strong correlation with
    the emergence of outliers, defined as features exhibiting significantly greater
    magnitudes compared to their counterparts in feature dimensions. Inspired by this
    finding, we introduce a novel LLM pruning methodology that incorporates a tailored
    set of non-uniform layerwise sparsity ratios specifically designed for LLM pruning,
    termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL
    is directly proportional to the outlier ratio observed within each layer, facilitating
    a more effective alignment between layerwise weight sparsity and outlier ratios.
    Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning
    various benchmarks, demonstrates the distinct advantages offered by OWL over previous
    methods. For instance, our approach exhibits a remarkable performance gain, surpassing
    the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high
    sparsity level of 70%, respectively. Codes are available at [https://github.com/luuyin/OWL](https://github.com/luuyin/OWL).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remarkable performance exhibited by Large Language Models (LLMs) across
    a diverse spectrum of applications has ignited an unparalleled race among tech
    giants and academic institutions to build LLMs at the billion-parameter scale (Brown
    et al., [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib40); [b](#bib.bib41);
    Brown et al., [2020](#bib.bib2)). The compelling performance of Large Language
    Models (LLMs) demonstrated in various applications triggers an unprecedented competition
    of building billion-level LLMs among tech giants and academic institutions (Brown
    et al., [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib40); [b](#bib.bib41);
    Brown et al., [2020](#bib.bib2)). While their exceptional capabilities are undeniable,
    the colossal size and computational demands of these models have also raised substantial
    concerns, particularly in terms of financial expenditure and environment (Luccioni
    et al., [2022](#bib.bib25); Patterson et al., [2021](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: Network pruning (Mozer & Smolensky, [1989](#bib.bib33); Janowsky, [1989](#bib.bib16);
    LeCun et al., [1989](#bib.bib18); Han et al., [2015](#bib.bib13)), as a long-established
    model compression method, is expected to serve as an effective solution for reducing
    the size of LLMs. However, network pruning usually favors a certain time of fine-tuning
    or re-training to reacquire the original optimal performance. Given the extensive
    text corpus and model size associated with LLMs, conventional fine-tuning becomes
    exceedingly challenging and less desirable. Fortunately, recent endeavors have
    explored the possibility of LLM pruning without the need for fine-tuning, showcasing
    that LLMs contain a substantial number of parameters that can be removed in a
    single step with minimal performance degradation (Jaiswal et al., [2023](#bib.bib15);
    Frantar & Alistarh, [2023](#bib.bib10); Sun et al., [2023](#bib.bib39)). SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib10)) addresses the challenge of LLM pruning from the
    perspective of layerwise reconstruction problem. In this context, the primary
    goal is to minimize the output discrepancy in terms of the reconstruction error
    between dense and sparse LLMs. It adopts an iterative strategy to handle the computational
    hurdle posed by the row-Hessian problem. Specifically, it employs the Optimal
    Brain Surgeon (OBS) algorithm (Hassibi et al., [1993](#bib.bib14)) to selectively
    prune and update weights in a column-wise manner. Wanda (Sun et al., [2023](#bib.bib39)),
    on the other hand, introduces a novel pruning metric that takes into account both
    the weight magnitudes and their corresponding input activations. Remarkably, it
    achieves performance on par with SparseGPT without relying on computationally
    expensive second-order information. The effectiveness of Wanda stems from the
    emergence of the outlier features residing within large-scale LLMs. These outliers,
    which tend to be significantly larger than typical features, are nonetheless crucial
    for optimizing LLM performance (Dettmers et al., [2022](#bib.bib5)). In general,
    both SparseGPT and Wanda exhibit competitive performance, showcasing their ability
    to reduce model parameters by up to 50% while incurring only a modest increase
    of approximately 1 in perplexity (Sun et al., [2023](#bib.bib39)).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that SparseGPT and Wanda unanimously follow previous work
    on BERT pruning (Sanh et al., [2020](#bib.bib37); Kurtic et al., [2022](#bib.bib17))
    and choose to prune LLMs with a uniform sparsity ratio per layer, *i.e.,* each
    layer will be pruned at the same sparsity. Such choice is reasonable for LLMs,
    as the pruning process typically involves sorting the importance scores of weights.
    Conducting such sorting globally across layers could become a computational bottleneck,
    especially for models at the billion-parameter scale. Nevertheless, before it
    has been taken root that uniform layerwise sparsity is the default choice for
    LLMs, we raise a timely inquiry: are there any pivotal aspects that have been
    inadvertently omitted in the context of favorable layerwise sparsity ratios for
    LLM pruning?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three reasons behoove us to pose the above research question: First, it is
    widely acknowledged that within Transformer architectures, certain components
    hold greater significance than others, and thus, they merit distinct treatment
    during the pruning process (Wang & Tu, [2020](#bib.bib44); Bhojanapalli et al.,
    [2021](#bib.bib1)); Second, a consensus view has been reached in computer vision
    that non-uniform layerwise sparsity typically achieves stronger results than uniform
    sparsity (Liu et al., [2022](#bib.bib24); Lee et al., [2020](#bib.bib19)); More
    importantly, LLMs demonstrate astonishingly emergent behaviors (Dettmers et al.,
    [2022](#bib.bib5); Wei et al., [2022](#bib.bib45); Schaeffer et al., [2023](#bib.bib38))
    as model size continuously scales up, a phenomenon distinct from smaller-scale
    language models such as BERT (Devlin et al., [2018](#bib.bib6)). These emergent
    behaviors offer fresh insights into the domain of LLM pruning. For instance, Dettmers
    et al. ([2022](#bib.bib5)) revealed the existence of outlier features within LLMs,
    with magnitudes up to 20 times larger than others, exerting a profound influence
    across all Transformer layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions. Given the pivotal role that outliers play in the performance
    of LLMs, coupled with the demonstrated effectiveness of Wanda (Sun et al., [2023](#bib.bib39)),
    our initial investigation centers on a systematic examination of the impact of
    existing LLM pruning methodologies on outliers. To our astonishment, we uncover
    a compelling correlation between pruning efficacy and the retention ratio of outliers:
    contemporary state-of-the-art LLM pruning approaches, such as SparseGPT and Wanda,
    exhibit remarkable preservation of outliers, even though the former was not originally
    designed with this intent. Moreover, we conduct an in-depth analysis of the distribution
    of outliers across different layers and observe a notably non-uniform pattern.
    This non-uniform distribution emerges as a valuable indicator for the formulation
    of layerwise sparsity strategies tailored specifically for LLMs. Building upon
    this newfound insight, we introduce an LLM pruning paradigm characterized by a
    novel layerwise sparsity ratio, denoted as Outlier Weighed Layerwise sparsity
    (OWL). OWL inherently assigns greater emphasis to layers housing a higher prevalence
    of outliers, thereby facilitating more nuanced coordination between sparsity in
    weight matrices and the presence of outliers within the layer.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct extensive experiments to evaluate the performance OWL across a spectrum
    of large language models, including LLaMA-V1 family (Touvron et al., [2023a](#bib.bib40)),
    and OPT (Zhang et al., [2022](#bib.bib49)), from 7B to 65B. Our empirical results
    show that OWL consistently outperforms existing top-performing LLM pruning methods,
    particularly at high sparsity levels. For instance, we observe significant improvements
    achieved by OWL over Wanda with LLaMa-7B on WikiText (Merity et al., [2016a](#bib.bib27)),
    with perplexity reductions of more than 60 and 3300 perplexity at sparsity levels
    of 70% and 80%, respectively. Our research presents a compelling counter-argument
    to previous study by shedding light on the previously overlooked yet crucial role
    of layerwise sparsity ratios in the context of LLM pruning. This shift in perspective
    has allowed us to push the boundaries of achievable LLM pruning ratios to reach
    70% without the need of any weight updates or second-order Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pruning and LLM Pruning. Since the 1980s, network pruning has been a well-established
    technique for simplifying neural networks in various applications while maintaining
    accuracy (Mozer & Smolensky, [1989](#bib.bib33); Han et al., [2015](#bib.bib13);
    Mocanu et al., [2018](#bib.bib32); Wen et al., [2017](#bib.bib46); Lin et al.,
    [2019](#bib.bib22)). However, when it comes to pruning Large Language Models (LLMs),
    progress has been limited. Traditional pruning typically requires a round of re-training
    to restore performance, which can be challenging for LLMs. To address this challenge,
    researchers have developed pruning algorithms specifically tailored for LLM compression.
    For example, Ma et al. ([2023](#bib.bib26)) explored structured sparse LLMs using
    Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning (Ma
    et al., [2023](#bib.bib26)). Recent research has shifted toward unstructured pruning
    without the need for fine-tuning, showing substantial advancements. SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib10)) utilizes the Hessian inverse for pruning and with
    subsequent weight updates to reduce reconstruction error of dense and sparse weights,
    while Wanda (Sun et al., [2023](#bib.bib39)) produces a criterion incorporating
    weight magnitude with their input activations, aiming to preserve outlier features (Dettmers
    et al., [2022](#bib.bib5)). Our work for the first time probe and highlight the
    crucial role of non-uniform layerwise sparsity for LLM pruning, making a notable
    progress in this field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layerwise Sparsity for Pruning. While it is common to use uniform layerwise
    sparsity (Zhu & Gupta, [2017](#bib.bib50); Gale et al., [2019](#bib.bib11)) to
    prune language models (Sanh et al., [2020](#bib.bib37); Kurtic et al., [2022](#bib.bib17)),
    there is a well-established line of work that explore non-uniform layerwise sparsity
    in terms of pruning vision models. Mocanu et al. ([2016](#bib.bib31)) propose
    a non-uniform and scale-free topology inspired from graph theory, showing better
    performance than the dense counterpart when applied to restricted Boltzmann machines.
    Follow-up works significantly improve its scalability based on Erdős-Rényi graph (Erdős
    & Rényi, [1959](#bib.bib7)), extending to fully-connected layers (Mocanu et al.,
    [2018](#bib.bib32)) and convolutional layers (Evci et al., [2020](#bib.bib8);
    Liu et al., [2022](#bib.bib24)) as data-free and feedforward-free layerwise sparsity.
    Another group of work produces non-uniform sparsity by applying a global threshold
    on every layer (Frankle & Carbin, [2019](#bib.bib9); Lee et al., [2019](#bib.bib20);
    Wang et al., [2020](#bib.bib43); Lee et al., [2020](#bib.bib19); Liu et al., [2021](#bib.bib23)).
    However, global pruning becomes extremely expensive and inefficacy in the context
    of LLM pruning as shown in Table [2](#S3.T2 "Table 2 ‣ 3.2 Empirical Study ‣ 3
    Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity
    (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"). We also provide
    a comparison among most common layerwise sparsity for LLMs in Section [5](#S5
    "5 Analysis ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"), and all of them fail to perform on LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Outliers in LLMs. Unlike traditional vision or smaller-scale transformer models,
    recent studies have revealed certain emergent characteristics unique to language
    models at scale. Specifically, one intriguing trait of LLMs is the exhibition
    of outlier features, which are the features with significantly larger magnitudes
    than others (Dettmers et al., [2022](#bib.bib5)). While constituting only a very
    small portion of the entire feature dimensions, these outliers play an imperative
    role in models’ predictive performance. Building upon this observation, several
    recent works have developed techniques to effectively quantize LLMs with minimal
    performance drop (Dettmers et al., [2022](#bib.bib5); Xiao et al., [2023](#bib.bib47);
    Lin et al., [2023](#bib.bib21)). On the other hand, in the context of LLM pruning,
    this unique characteristic has scarcely been taken into account to the best of
    our knowledge (Sun et al., [2023](#bib.bib39)). Our work draws on the importance
    of the emergent outliers in LLMs, and provides a systematic study on its correlation
    to the effectiveness of model pruning, leading to a novel technique that leverages
    the distribution of outliers to guide layerwise LLM pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Outlier Weighed Layerwise Sparsity – OWL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will introduce Outlier-Weighted Layer-wise sparsity (OWL)
    step by step, from rationale, to empirical studies, and eventually to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Rationale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary of goal of network pruning is to discover the least important components,
    such as individual weights in the case of unstructured pruning, which have minimal
    impact on the model’s output. In the context of pre-LLMs with smaller scales,
    magnitude pruning has traditionally serves as the most basic yet effective technique,
    consistently delivering robust results across various scenarios (Han et al., [2015](#bib.bib13);
    Mocanu et al., [2018](#bib.bib32); Frankle & Carbin, [2019](#bib.bib9); Jaiswal
    et al., [2023](#bib.bib15)). The effectiveness of magnitude pruning in compressing
    pre-LLM models is closely intertwined with the feasibility of fine-tuning. It
    has been observed that even the random removal of components can ultimately restore
    the original performance through adequate fine-tuning (Liu et al., [2022](#bib.bib24);
    Mittal et al., [2019](#bib.bib30)). However, fine-tuning encounters significant
    challenges when applied to LLMs, rendering magnitude pruning less effective compared
    to more precise pruning metrics, such as second-order Hessian (Frantar & Alistarh,
    [2023](#bib.bib10)) and input activation (Sun et al., [2023](#bib.bib39)). Notably,
    Wanda (Sun et al., [2023](#bib.bib39)) achieves remarkable performance by augmenting
    input activation with weight magnitude, underscoring the critical importance of
    preserving outlier features in LLM pruning. Considering the vital role that outliers
    play in the context of LLMs (Dettmers et al., [2022](#bib.bib5)) and the success
    of Wanda, we conjecture that the performance of different pruning methods has
    a strong correlation with their ability to preserve outlier features. To assess
    our conjecture, we undertake several preliminary investigations outlined below
    based on Layerwise Outlier Distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Empirical Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Layerwise Outlier Distribution (LOD). Our preliminary studies are based on Layerwise
    Outlier Distribution (LOD), a concept used to measure how outlier features distribute
    and effect weights across layers. Since we focus on weight pruning in this paper,
    instead of measuring the outlier distribution of input features, We opt to prioritize
    the impact of outlier features on weights, which is quantified as the accumulation
    of all input features connected to the target weight, multiplied by the weight
    magnitude (Sun et al., [2023](#bib.bib39)). Our intuition here is that weights
    that are most affected by outliers also play a pivotal role in propagating and
    preserving these outlier features.
  prefs: []
  type: TYPE_NORMAL
- en: To formalize our approach, we consider the input of a layer as $\mathbf{X}$.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, after obtaining the impact of features for all weights $\mathbf{A}$-layer
    LLMs. Based on LOD, we conduct three empirical studies outlined below to better
    understand the effect of LOD on LLM pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirical Study I: Dense LLMs vs. LOD. To investigate whether sparsifying LLMs
    necessitates differential treatment of individual layers, we employ LOD to gauge
    the layerwise distribution of outliers within dense LLMs. If LOD in dense LLMs
    exhibits a relatively uniform pattern, it suggests that a non-uniform layerwise
    distribution may not be imperative, at least in terms of outlier features, and
    vice versa. We assess the LOD across various dense LLMs, including LLaMA-7B, 13B,
    and 30B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirical Study II: Pruning Metric vs. LOD. We further delve into the impact
    of different pruning metrics on LOD. The primary objective of this study is to
    explore whether there exists a robust correlation between the performance of various
    pruning methods and their ability to preserve outliers. To achieve this, we aggregate
    the LOD values across layers for various LLM pruning methods, such as magnitude,
    Wanda, and SparseGPT, and compare them with their dense counterparts. In the case
    of sparse LLMs, we calculate LOD by considering only non-zero weights. All sparse
    models are pruned with uniform sparsity. These experiments are conducted using
    LLaMA-13B at sparsity level of 60% and 70% with $\mathbf{M}=7$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirical Study III: Pruning Granularity vs. LOD. It is well-established that
    non-uniform or global layerwise sparsity often leads to more accurate sparser
    networks at high sparsity than the uniform layerwise sparsity for pre-LLM pruning.
    However, endeavors unanimously point out that uniform sparsity is more favorable
    for pruning LLMs. To provide more insights about these two seemingly countradictory
    arguments, we study the effect of various pruning granularities on LOD. Specifically,
    we study two sets of pruning granularities: (1) Across different layers, we compare
    the performance as well as the resulting LOD of uniform sparsity and global sparsity;
    (2) Within the same layer, we study the output-imbalanced sparsity used by SparseGPT
    against the output-balanced sparsity adopted by Wanda. Output-balanced sparsity
    eliminates the same amount of weights for all outputs. We conduct experiments
    with magnitude pruning and Wanda using LLaMA-7B at various sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: We present our findings from Study 1-3, in Figure [1](#S3.F1 "Figure
    1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity"), Table [1](#S3.T1 "Table 1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed
    Layerwise Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing
    Secret Sauce for Pruning LLMs to High Sparsity"), and Table [2](#S3.T2 "Table
    2 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity"), respectively. These results provide positive support for our conjecture,
    and we summarize the key observations below:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS2.p8.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'LOD of dense LLMs exhibits a highly non-uniform distribution across layers.
    In essence, the distribution of dense LLMs shown in Figure [1](#S3.F1 "Figure
    1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity") loosely follows a “U” shape, with notable proportions at both ends,
    while the central region displays a monotonic descending trend. This finding validates
    our conjecture that individual layers need unique consideration during the pruning
    procedure. Employing uniform pruning across all layers would inevitably disrupt
    the outlier structure in layers characterized by a large outlier ratio, such as
    those layers at the beginning or end of models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Effects of various pruning methods on Layerwise Outlier Distribution
    (LOD) and Perplexity with LLaMA-13B on WikiText. LOD is calculated as the summation
    across all layers with $\mathbf{M}=7$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sparsity | Method | LOD (%) $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dense | 5.432 | - | 5.090 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wanda | 5.716 | 0.284 | 55.900 |'
  prefs: []
  type: TYPE_TB
- en: '| 70% | SparseGPT | 6.645 | 1.213 | 19.235 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Magnitude | 5.322 | -0.110 | 84539.445 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wanda | 5.433 | 0.001 | 8.761 |'
  prefs: []
  type: TYPE_TB
- en: '| 60% | SparseGPT | 6.044 | 0.612 | 8.458 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Magnitude | 5.322 | -0.110 | 229.451 | <svg id="S3.SS2.p9.pic1" class="ltx_picture"
    height="13.38" overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of sparse pruning methods on LLMs is closely correlated with
    their ability to retain outlier features. Leading pruning techniques like Wanda
    and SparseGPT all excel in outlier, resulting in an overall increase in LOD. In
    contrast, the naive baseline of magnitude pruning performs no better than random
    selection at 70% sparsity, as evidenced by a negative change of -0.110 in LOD,
    indicating the removal of important outliers. It is interesting to see that despite
    SparseGPT not being explicitly designed for outlier preservation, it achieves
    the highest LOD as well as performance, providing further insight into the underlying
    reason for its success. A plausible reason is that the weight update involved
    within SparseGPT helps increase LOD.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1da47306c7be711abad3daa56ff0e449.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Layerwise Outlier Distribution (LOD) (%) of dense LLaMA-7B, 13B,
    and 30B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: WikiText perplexity with LLaMA-7B of various pruning granularity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Layerwise | Output | Sparsity |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform | Balanced | 10% | 20% | 30% | 40% | 50% | 60% | 70% |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | ✓ | ✓ | 5.697 | 5.817 | 5.999 | 6.388 | 7.260 | 10 | 86 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | ✓ | ✗ | 5.695 | 5.819 | 6.029 | 6.572 | 7.942 | 20 | 238 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | ✗ | ✗ | 14.117 | 3134 | 10293 | 10762 | 14848 | 17765 | 5147 |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | ✓ | ✓ | 5.803 | 6.018 | 6.622 | 8.041 | 13.349 | 152 | 25304
    |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | ✓ | ✗ | 5.806 | 6.020 | 6.669 | 8.601 | 17.287 | 559 | 48419
    |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | ✗ | ✗ | 5.821 | 6.111 | 7.012 | 9.825 | 48.627 | 38335 | 29283
    | <svg id="S3.SS2.p10.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning with coarser granularity results in diminished performance. In general,
    we observe a consistent trend of improved perplexity as the pruning granularity
    becomes finer, transitioning from global layerwise sparsity to uniform layerwise
    sparsity at the macro level, and from output imbalanced sparsity to output balanced
    sparsity at the micro level. These findings align with the conclusions presented
    by Sun et al. ([2023](#bib.bib39)). One plausible explanation for this trend is
    that coarser-grained pruning tends to eliminate outlier features to a more significant
    extent, particularly in certain layers or outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Outlier Weighed Layerwise Sparsity (OWL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The above empirical studies underscore the critical significance of preserving
    outliers in the context of LLM pruning. Consequently, it becomes imperative to
    implement layerwise pruning strategies that take into account the non-uniform
    distribution of outliers across different layers. However, global pruning can
    be costly and lead to collapse of outliers, resulting in significant performance
    degradation. On the other hand, uniform pruning does not adequately consider the
    highly non-uniform distribution of outlier features across various layers. This
    negligence inevitably disrupts the structure of outliers in layers characterized
    by a substantial outlier ratio, particularly at high sparsity levels. Therefore,
    there is a need of an ideal layerwise sparsity that aligns effectively with the
    layerwise outlier distribution while maintaining computational and memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this issue, we propose a novel layerwise sparsity ratio strategy,
    referred to as Outlier-Weighted Layer-wise sparsity (OWL) explicitly tailored
    for Large Language Models, which can better coordinate with the outlier distribution
    by taking the layerwise outlier ratio into consideration. Given a $l$ [3, 5, 7,
    10]. The visualization of our layerwise sparsity ratio is demonstrated in Figure [2](#S3.F2
    "Figure 2 ‣ 3.3 Outlier Weighed Layerwise Sparsity (OWL) ‣ 3 Outlier Weighed Layerwise
    Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"), where we can clearly see that the layerwise
    sparsity level of OWL nuancedly aligns with model’s LOD.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84c6fe1be86c392f70b6bae7dcb59490.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The demonstration of the OWL layerwise sparsity and Uniform layerwise
    sparsity at 70% sparsity. The bar chart in background corresponds to the Layerwise
    Outlier Distribution (LOD).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models and Dataset. We assess OWL’s performance across a range of LLMs, encompassing
    the LLaMA-V1 model family (Touvron et al., [2023b](#bib.bib41)) with parameter
    counts ranging from 7 billion to 65 billion, as well as OPT-6.7B (Zhang et al.,
    [2022](#bib.bib49)). Our evaluation protocol aligns with established LLM pruning
    methodologies (Frantar & Alistarh, [2023](#bib.bib10); Sun et al., [2023](#bib.bib39)),
    encompassing assessments of language modeling proficiency and zero-shot capabilities
    of sparse LLMs. Specifically, we measure the Perplexity metric on the WikiText (Merity
    et al., [2016b](#bib.bib28)) validation dataset for language modeling performance,
    and employ the Accuracy metric for zero-shot evaluations on seven common sense
    benchmarks, including BoolQ (Clark et al., [2019](#bib.bib3)), RTE (Wang et al.,
    [2018](#bib.bib42)), HellaSwag (Zellers et al., [2019](#bib.bib48)), WinoGrande (Sakaguchi
    et al., [2019](#bib.bib36)), ARC Easy and Challenge (Clark et al., [2018](#bib.bib4)),
    and OpenbookQA (Mihaylov et al., [2018](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. We choose the three current LLM-pruning baselines, including magnitude (Jaiswal
    et al., [2023](#bib.bib15)), SparseGPT (Frantar & Alistarh, [2023](#bib.bib10)),
    Wanda (Sun et al., [2023](#bib.bib39)). Magnitude pruning serves as a naive baseline
    for LLMs, with an expected sharp decline in performance at modest sparsity levels,
    typically ranging from 10% to 30%. SparseGPT and Wanda, on the other hand, are
    established baselines known for their ability to maintain reasonable performance
    even at relatively high sparsity levels, typically around 50% to 60%. Notably,
    in contrast to our approach, all baseline methods employ with uniform layerwise
    sparsity. We primarily focus on high sparsity levels, not falling below 50%, as
    regions with low sparsity pose challenges for existing sparse GPU kernels to outperform
    their dense counterparts (Gale et al., [2020](#bib.bib12)). To ensure equitable
    comparisons, we have employed the identical set of calibration data as utilized
    by SparseGPT and Wanda for model pruning, *i.e.,* comprising 128 sequences with
    2048 tokens for each, randomly sampled from the first shard of the C4 (Raffel
    et al., [2020](#bib.bib35)) dataset. We incorporate OWL directly into Wanda and
    SparseGPT, resulting in two variants: “OWL w. Wanda” and “OWL w. SparseGPT”. The
    only distinction between these variants lies in their layerwise sparsity ratios,
    with OWL providing a more tailored layerwise sparsity in this regard. Hyperparameters
    are shared in Table [4](#S4.F4 "Figure 4 ‣ 5.2 Pruning Efficiency ‣ 5 Analysis
    ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning
    LLMs to High Sparsity")-Right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: WikiText validation perplexity of pruning methods for LLaMA-V1 family
    and OPT-6.7B at 70% sparsity. The best performance method is indicated in bold,
    and the gain in perplexity achieved by OWL is highlighted in blue.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Layerwise | Weight | LLaMA-V1 | OPT |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sparsity | Update | 7B | 13B | 30B | 65B | 6.7B |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | - | - | 5.68 | 5.09 | 4.10 | 4.77 | 10.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | Uniform | ✗ | 48419.12 | 84539.45 | 977.73 | 46.89 | 290985.03
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | Uniform | ✗ | 85.77 | 55.90 | 17.37 | 15.23 | 162.92 |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. Wanda | Non-Uni | ✗ | 24.55 (-61.22) | 17.17 (-38.73) | 10.75 (-6.62)
    | 8.61 (-6.62) | 40.22 (-120.70) |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | Uniform | ✓ | 26.30 | 19.24 | 12.56 | 10.45 | 20.29 |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. SparseGPT | Non-Uni | ✓ | 19.49 (-6.81) | 14.55 (-4.69) | 10.28 (-2.28)
    | 8.28 (-0.64) | 22.48 (2.19) |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Language Modelling. We first report the performance of various LLM pruning
    methods on language modelling with WikiText. The results is presented in Table [3](#S4.T3
    "Table 3 ‣ 4 Experiments ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing
    Secret Sauce for Pruning LLMs to High Sparsity") and Figure LABEL:fig:LLaMA_7B.
    We summarize the key observation below:'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.SS1.p2.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'OWL demonstrates its versatility serving as a general layerwise sparsity method
    suitable for various scenarios. As illustrated in Table [3](#S4.T3 "Table 3 ‣
    4 Experiments ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"), OWL exhibits effectiveness across different
    pruning methods (such as Wanda and SparseGPT), architectural variants (including
    LLaMA-V1 and OPT), and diverse model sizes (ranging from LLaMA-V1 with 7B, 13B,
    30B, to 65B parameters), resulting in substantial reductions in perplexity scores.
    Notably, even when applied to SparseGPT, a strong pruning method incorporating
    second-order information, OWL still achieves significant perplexity reductions,
    exemplified by a reduction of 6.81 for LLaMA-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.SS1.p3.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of OWL increases as significantly model size decreases. There is
    a clear trend that the performance gain of OWL monotonically increases as LLaMA-V1
    scales down from 65B to 7B. While the performance improvement of OWL .w Wanda
    for LLaMA-65B is relatively small, at 6.62, it achieves a remarkable gain of 61.22
    for LLaMA-7B, resulting in a reasonable 24.55 perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-Shot Tasks. While perplexity is a widely used metric for language modeling,
    it primarily serves as a statistical measure of how confidently a language model
    predicts a text sample and does not necessarily align with the quality of the
    generated text. To draw more robust conclusions, we conducted experiments to evaluate
    the zero-shot ability of various sparse LLMs on diverse zero-shot downstream tasks
    with prompting. These experiments were performed using the LLaMA-V1 family at
    70% sparsity, and the results are presented in Table [4](#S4.T4 "Table 4 ‣ 4.1
    Experimental Results ‣ 4 Experiments ‣ Outlier Weighed Layerwise Sparsity (OWL):
    A Missing Secret Sauce for Pruning LLMs to High Sparsity"). It’s noteworthy that
    OWL consistently improves accuracy across nearly all settings, with very few exceptions
    on RTE data, which is . For example, OWL achieves an average perplexity gain of
    4.72 and 2.19 over 7 tasks and 4 model sizes compared to Wanda and SparseGPT alone,
    respectively. This result highlights the promise of OWL is still hold for more
    challenging zero-shot downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Accuracies (%) for 7 zero-shot tasks with 70% sparsity using LLaMA-V1
    family.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Params | Method | BoolQ | RTE | HellaSwag | WinoGrande | ARC-e | ARC-c |
    OBQA | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | Dense | $75.14$ |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | $38.29$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | $55.11$ |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. Wanda | 62.48 | 58.48 | 44.79 | 58.72 | 45.03 | 26.19 | 29.60 | 46.47
    |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 64.53 | 53.79 | 42.11 | $58.64$ | 43.06 | 24.57 | 27.80 | 44.93
    |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. SparseGPT | 67.13 | 53.43 | 48.56 | 62.03 | 45.41 | 27.65 | 32.00
    | 48.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | Dense | $77.86$ |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | $52.94$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | $61.71$ |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. Wanda | 62.69 | 52.71 | 51.03 | 63.14 | 49.54 | 28.67 | 34.40 | 48.88
    |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 66.94 | 52.71 | 47.91 | 62.90 | 45.03 | 27.99 | 35.20 | 48.38
    |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. SparseGPT | 64.95 | 53.07 | 54.39 | 66.54 | 48.86 | 30.12 | 38.00
    | 50.85 |'
  prefs: []
  type: TYPE_TB
- en: '| 30B | Dense | 82.69 | 66.79 | 81.19 | 75.85 | 73.48 | 50.77 | 44.60 | 67.91
    |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | 39.14 | 46.21 | 24.31 | 52.33 | 24.66 | 22.87 | 29.00 | 34.07
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | $66.12$ |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. Wanda | 66.42 | 52.35 | 62.94 | 69.30 | 61.83 | 35.84 | 40.00 | 55.53
    |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 66.51 | 63.90 | 60.38 | 69.85 | 58.54 | 33.70 | 40.60 | 55.78
    |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. SparseGPT | 67.58 | 58.48 | 64.88 | 70.72 | 60.82 | 35.07 | 42.20
    | 57.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 65B | Dense | 84.86 | 69.68 | 82.94 | 77.35 | 75.08 | 52.56 | 44.20 | 69.52
    |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | 52.17 | 54.87 | 49.87 | 56.67 | 49.71 | 30.63 | 38.80 | 47.53
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 76.30 | 56.68 | 61.26 | 70.48 | 63.47 | 35.67 | 39.40 | 57.61 |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. Wanda | 80.12 | 58.84 | 66.16 | 73.56 | 65.45 | 39.93 | 42.20 | 60.89
    |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 80.64 | 59.57 | 66.42 | 72.61 | 60.52 | 38.57 | 40.80 | 59.88
    |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. SparseGPT | 82.63 | 67.15 | 68.52 | 75.06 | 60.10 | 39.59 | 39.00
    | 61.72 |'
  prefs: []
  type: TYPE_TB
- en: 5 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Comparisons Among Various Layerwise Sparsity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare OWL layerwise sparsity with multiple commonly used layerwise sparsity,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global Frankle & Carbin ([2019](#bib.bib9)). A global threshold is uniformly
    applied to all layers to satisfy the overall sparsity requirement, and the specific
    layerwise sparsity is automatically adjusted based on this threshold.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uniform (Zhu & Gupta, [2017](#bib.bib50)). Every layer is pruned with the same
    target sparsity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erdős-Rényi Kernel (ERK) (Evci et al., [2020](#bib.bib8)). The sparsity of the
    convolutional layer is scaled proportional to $1-\frac{n^{l-1}+n^{l}+w^{l}+h^{l}}{n^{l-1}\times
    n^{l}\times w^{l}\times h^{l}}$ are the corresponding width and height. ERK is
    modified based on ER (Mocanu et al., [2018](#bib.bib32)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ERK-plus (Liu et al., [2022](#bib.bib24)). ERK-plus modifies ERK by forcing
    the last layer as dense if it is not, while keeping the overall parameter count
    the same.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OWL-inverse. OWL-inverse metric is the inverse variant of OWL, whose outlier
    ratio is $1-\texttt{LOD}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For this study, we apply Wanda to the LLaMA-7B model. The results are presented
    in Table [5](#S5.T5 "Table 5 ‣ 5.1 Comparisons Among Various Layerwise Sparsity
    ‣ 5 Analysis ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"). It is noteworthy that all approaches, except
    for the Global method, perform satisfactorily when the sparsity level is at or
    below 40%. This observation suggests that the region of low sparsity does not
    provide significant distinctions for performance comparison. However, as the sparsity
    level exceeds 50%, discrepancies between the various approaches become evident.
    Notably, the Uniform and OWL methods emerge as the top-performing approaches,
    with OWL consistently outperforming the former across all sparsity levels. On
    the other hand, the ERK family of methods appears to be less suitable for LLM
    pruning. It’s worth mentioning that the performance of OWL experiences a significant
    decline when we invert its outlier ratio, underscoring the effectiveness of LOD
    in identifying critical layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: WikiText validation perplexity of LLaMA-7B with various layerwise
    sparsity using Wanda.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sparsity/Perplexity | 10% | 20% | 30% | 40% | 50% | 60% | 70% | 80% |'
  prefs: []
  type: TYPE_TB
- en: '| Global | 14.11 | 3134 | 10293 | 10762 | 14848 | 17765 | 5147 | 39918.56 |'
  prefs: []
  type: TYPE_TB
- en: '| ERK-plus | 5.70 | 5.82 | 6.05 | 6.62 | 8.00 | 14.04 | 229.17 | 6013.91 |'
  prefs: []
  type: TYPE_TB
- en: '| ERK | 5.69 | 5.80 | 6.02 | 6.55 | 7.74 | 12.16 | 112.03 | 11151.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform | 5.69 | 5.81 | 5.99 | 6.38 | 7.26 | 10.70 | 85.77 | 3499.88 |'
  prefs: []
  type: TYPE_TB
- en: '| OWL-inverse | 5.72 | 5.83 | 6.04 | 6.51 | 8.03 | 26.05 | 822.23 | 9616.08
    |'
  prefs: []
  type: TYPE_TB
- en: '| OWL (ours) | 5.70 | 5.80 | 6.01 | 6.39 | 7.22 | 9.35 | 24.54 | 1002.87 |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Pruning Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | LLaMA |'
  prefs: []
  type: TYPE_TB
- en: '| Method | 7B | 13B | 30B | 65B |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 208 | 341 | 731 | 1297 |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. SparseGPT | 208 | 342 | 733 | 1301 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 0.3 | 0.6 | 1.1 | 1.8 |'
  prefs: []
  type: TYPE_TB
- en: '| OWL w. Wanda | 0.5 | 1.3 | 2.0 | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Model | $\mathbf{M}$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | 5 | 8% |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13B | 7 | 8% |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-30B | 5 | 8% |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-65B | 5 | 20% |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-6.7B | 10 | 8% |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4: Left: Comparison of time overhead (in seconds), excluding the shared
    forward pass process. Right: Hyperparameters used to reproduce the results in
    this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we utilize the pruning metric of Wanda to determine our layerwise sparsity,
    the theoretical computational complexity of OWL is comparable to that of Wanda,
    which is expected to be significantly lower than SparseGPT. To demonstrate this,
    we measure the total pruning time, excluding the forward pass process, following
    the methodology outlined by Sun et al. ([2023](#bib.bib39)). These results were
    obtained using NVIDIA A100 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results in Table [4](#S4.F4 "Figure 4 ‣ 5.2 Pruning Efficiency ‣ 5 Analysis
    ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning
    LLMs to High Sparsity") indicate that OWL introduces nearly negligible overhead
    when compared to SparseGPT. Conversely, OWL .w Wanda doubles the pruning time
    in comparison to Wanda alone, yet it efficiently prunes a 65B LLaMA model within
    only 4 seconds. This additional time overhead primarily arises from the computation
    of $\|\mathbf{X}_{\texttt{j}}\|_{2}\cdot|\mathbf{W}_{\texttt{ij}}|$ for the computation
    of Layerwise Outlier Distribution (LOD). However, as Wanda also employs this metric
    for pruning, we believe there is potential for solutions to mitigate this overhead.
    This aspect is left for future work and further optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we focus on a crucial aspect of LLM pruning that have been overlooked
    by previous works – layerwise sparsity ratios. Despite the prevailing practice
    of uniformly pruning all layers at equivalent sparsity levels, as observed in
    prominent LLM pruning papers, our investigation diverges from this trend by drawing
    inspiration from the emergence of outliers, characterized by features exhibiting
    significantly greater magnitudes compared to others. Leveraging this discovery,
    we introduced a novel layerwise sparsity ratio known as Outlier Weighed Layerwise
    sparsity (OWL). OWL employs tailored non-uniform layerwise sparsity ratios designed
    specifically for LLM pruning, aligning sparsity ratios with outlier ratios within
    each layer. Notably, our approach demonstrates substantial performance gains,
    surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity
    points, respectively, at a high sparsity level of 70%. Our findings offer fresh
    insights into the critical significance of layerwise sparsity in the context of
    LLM pruning. This work opens up new avenues for the development of specialized
    sparse algorithms that can further optimize the deployment of LLMs in practical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation. One limitation of this work is the lack of results with hardware-friendly
    sparsity pattern. We will explore the promising N:M sparsity for OWL as one of
    our future directions.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Part of this work used the Dutch national e-infrastructure with the support
    of the SURF Cooperative using grant no. NWO2021.060, EINF-2694 and EINF-2943/L1\.
    S. Liu and Z. Wang are in part supported by the NSF AI Institute for Foundations
    of Machine Learning (IFML).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bhojanapalli et al. (2021) Srinadh Bhojanapalli, Ayan Chakrabarti, Andreas Veit,
    Michal Lukasik, Himanshu Jain, Frederick Liu, Yin-Wen Chang, and Sanjiv Kumar.
    Leveraging redundancy in attention with reuse transformers. *arXiv preprint arXiv:2110.06821*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems (NeurIPs)*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems (NeurIPs)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erdős & Rényi (1959) Paul Erdős and Alfréd Rényi. On random graphs i. *Publicationes
    Mathematicae (Debrecen)*, 6:290–297, 1959.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning (ICML)*, pp.  2943–2952, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *International Conference
    on Learning Representations (ICLR)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Massive language models
    can be accurately pruned in one-shot. In *International Conference on Machine
    Learning (ICML)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gale et al. (2020) Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen.
    Sparse gpu kernels for deep learning. In *SC20: International Conference for High
    Performance Computing, Networking, Storage and Analysis*, pp.  1–14\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. Learning
    both weights and connections for efficient neural network. In *Advances in Neural
    Information Processing Systems (NeurIPS)*, pp.  1135–1143, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal
    brain surgeon and general network pruning. In *IEEE international conference on
    neural networks*, pp.  293–299\. IEEE, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Janowsky (1989) Steven A Janowsky. Pruning versus clipping in neural networks.
    *Physical Review A*, 39(12):6600, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. Optimal brain damage.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, pp.  598–605,
    1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020) Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo
    Shin. Layer-adaptive sparsity for the magnitude-based pruning. *arXiv preprint
    arXiv:2010.07611*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip:
    Single-shot network pruning based on connection sensitivity. In *International
    Conference on Learning Representations (ICLR)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2019) Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan
    Cao, Qixiang Ye, Feiyue Huang, and David Doermann. Towards optimal structured
    cnn pruning via generative adversarial learning. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, pp.  2790–2799, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi,
    Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin
    Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin
    Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness
    of random pruning: Return of the most naive baseline for sparse training. *arXiv
    preprint arXiv:2202.02643*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luccioni et al. (2022) Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure
    Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model.
    *arXiv preprint arXiv:2211.02001*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016a) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016b) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mittal et al. (2019) Deepak Mittal, Shweta Bhardwaj, Mitesh M Khapra, and Balaraman
    Ravindran. Studying the plasticity in deep convolutional neural networks using
    random pruning. *Machine Vision and Applications*, 30(2):203–216, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mocanu et al. (2016) Decebal Constantin Mocanu, Elena Mocanu, Phuong H. Nguyen,
    Madeleine Gibescu, and Antonio Liotta. A topological insight into restricted boltzmann
    machines. *Machine Learning*, 104(2):243–270, Sep 2016. ISSN 1573-0565. doi: 10.1007/s10994-016-5570-z.
    URL [https://doi.org/10.1007/s10994-016-5570-z](https://doi.org/10.1007/s10994-016-5570-z).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
    Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial
    neural networks with adaptive sparse connectivity inspired by network science.
    *Nature Communications*, 9:1–12, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mozer & Smolensky (1989) Michael C Mozer and Paul Smolensky. Skeletonization:
    A technique for trimming the fat from a network via relevance assessment. In *Advances
    in Neural Information Processing Systems (NeurIPS)*, pp.  107–115, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterson et al. (2021) David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang,
    Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
    Carbon emissions and large neural network training. *arXiv preprint arXiv:2104.10350*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *arXiv preprint arXiv:1907.10641*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement
    pruning: Adaptive sparsity by fine-tuning. *arXiv preprint arXiv:2005.07683*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaeffer et al. (2023) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are
    emergent abilities of large language models a mirage? *arXiv preprint arXiv:2304.15004*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning
    tickets before training by preserving gradient flow. In *International Conference
    on Learning Representations (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Tu (2020) Wenxuan Wang and Zhaopeng Tu. Rethinking the value of transformer
    components. *arXiv preprint arXiv:2011.03803*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2017) Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan
    Wang, Fang Liu, Bin Hu, Yiran Chen, and Hai Li. Learning intrinsic sparse structures
    within long short-term memory. *arXiv preprint arXiv:1709.05027*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning (ICML)*,
    pp.  38087–38099\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu & Gupta (2017) Michael Zhu and Suyog Gupta. To prune, or not to prune:
    exploring the efficacy of pruning for model compression. In *International Conference
    on Learning Representations Workshop (ICLRW)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
