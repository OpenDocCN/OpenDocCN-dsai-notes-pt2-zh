- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'QuIP # # \# : Even Better LLM Quantization with Hadamard Incoherence and Lattice
    Codebooks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04396](https://ar5iv.labs.arxiv.org/html/2402.04396)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Albert Tseng    Jerry Chee    Qingyao Sun    Volodymyr Kuleshov    Christopher
    De Sa
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing
    their weights to low-precision. In this work, we introduce QuIP$\#$ outperforms
    existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML, Quantization, Large Language Models, LLMs, Low Precision,
    Inference, Systems, Hardware, 2 bit, QuIP, Incoherence Processing, Lattice Codebooks,
    Vector Quantization
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51fdeabf9fc2b906f024787f6f223bb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: QuIP$\#$ 3-bit models also scale better than theoretically lossless
    4-bit models, a previously unseen result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5946710a724a1ff952a59572639ed51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: QuIP$\#$ performs incoherence processing with a Randomized Hadamard
    Transform and uses lattice codebooks to achieve state-of-the-art quantized models.'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have driven rapid advances across diverse fields
    such as natural language processing (Touvron et al., [2023b](#bib.bib30)), scientific
    modeling (Nguyen et al., [2023](#bib.bib24)), and program synthesis (Rozière et al.,
    [2024](#bib.bib25)). However, the massive size of these models poses significant
    challenges to their deployment. For example, the largest model in the Llama2 family
    has 70B parameters, and requires 140GB of GPU memory in native 16-bit precision
    (Touvron et al., [2023b](#bib.bib30)). This massive memory footprint motivates
    research into methods that can compress LLMs without sacrificing quality.
  prefs: []
  type: TYPE_NORMAL
- en: Post-training quantization (PTQ) reduces the memory requirements of large models
    by converting trained weights to a lower precision. For example, with 2-bit quantization,
    a 16-bit LLama2 model with 70B parameters fits on a single consumer-grade 24GB
    GPU and benefits from increased inference throughput (Cai et al., [2024](#bib.bib2)).
    However, 2-bit quantization also often reduces the quality of the model and pushes
    the limits of PTQ algorithms (Chee et al., [2023](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we introduce QuIP$\#$ includes an inter-layer fine-tuning algorithm
    that further improves quantization quality (Section [5](#S5 "5 Fine-Tuning During
    Quantization ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and
    Lattice Codebooks")).'
  prefs: []
  type: TYPE_NORMAL
- en: These developments allow QuIP$\#$ is also the first PTQ method where 3-bit models
    scale better than 4-bit models. This directly refutes Dettmers & Zettlemoyer ([2023](#bib.bib9))’s
    claim that 4-bit models are “optimal” and indicates that as the field of PTQ develops,
    2-bit models are likely to scale better than 3-bit models in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we note that QuIP$\#$ achieves over 50% of peak memory bandwidth on
    a NVIDIA RTX 4090, validating our design choices.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we introduce QuIP$\#$, a post-training quantization method that
    achieves state-of-the-art results by
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing incoherence processing with the Randomized Hadamard Transform, which
    has better incoherence properties and faster runtime than the Kronecker factorization
    in QuIP.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rounding incoherence-processed weight matrices with block adaptive rounding
    and codebooks based on the $E_{8}$ lattice, which achieves the highest 8-dimension
    unit ball packing density (kissing number).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing an inter-layer fine-tuning algorithm that further improves quantization
    quality.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Algorithm 1 QuIP$\#$-NoFT)
  prefs: []
  type: TYPE_NORMAL
- en: 0:  Weight $W\in\mathbb{R}^{m\times n}$
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 QuIP$\#$ Inference (for a Linear Layer)
  prefs: []
  type: TYPE_NORMAL
- en: 0:  $\hat{W}$
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background / Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Compressing LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A large body of work has focused on compressing LLMs, as doing so can directly
    benefit LLM inference at scale. Methods such as pruning, quantization aware training
    (QAT), and post-training quantization (PTQ) all focus on different areas of this
    problem and are not strictly orthogonal to each other. Pruning removes weights
    from models while preserving model quality and inference performance (Chee et al.,
    [2022](#bib.bib3); Sun et al., [2023](#bib.bib28)). QAT focuses on training models
    that are more “quantizable” but usually requires training models from scratch
    (Nagel et al., [2022](#bib.bib23)). PTQ, which QuIP$\#$ falls under, instead quantizes
    pre-trained models. PTQ generally requires much less compute than QAT and achieve
    competitive performance (Chee et al., [2023](#bib.bib4); Frantar et al., [2023](#bib.bib12);
    Shao et al., [2024](#bib.bib26); Egiazarian et al., [2024](#bib.bib10)). For the
    rest of this paper, we focus on the PTQ realm of LLM compression.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Quantization and Adaptive Rounding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In QuIP$\#$, we follow existing state-of-the-art PTQ methods and round weights
    to minimize the per-layer proxy loss, as formalized by Nagel et al. ([2020](#bib.bib22)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\ell(\hat{W})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\operatorname{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Here, $W\in\mathbb{R}^{m\times n}$ is to use adaptive rounding methods that
    iteratively round weight matrices by considering the current rounding error for
    that specific matrix. For example, the LDLQ¹¹1OPTQ(Frantar et al., [2023](#bib.bib12))
    and QuIP independently introduced alternative formulations of this rounding method,
    and QuIP showed them to be equivalent. LDLQ is the name given by QuIP. rounding
    algorithm iteratively rounds rows of model weights using linear feedback from
    quantization error of already rounded rows. LDLQ is optimal within the class of
    adaptive rounding methods with linear feedback and offers provably better error
    rates than nearest or stochastic rounding (Chee et al., [2023](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Incoherence Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiple works have observed that outliers in model activations and weights
    can hinder quantization quality, motivating methods that “suppress” outliers during
    quantization. For example, AWQ (Lin et al., [2023](#bib.bib20)) scales model weights
    by information from activations and OmniQuant (Shao et al., [2024](#bib.bib26))
    uses simple learnable model-preserving transformations. However, these heuristic-based
    approaches tend to fail at lower bitrates.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, in QuIP, Chee et al. ([2023](#bib.bib4)) proposed that *incoherence*
    is important for LLM quantization. Informally, incoherent matrices have concentrated
    entry magnitudes—ruling out outliers. In LLMs, incoherent weight and Hessian matrices
    mean that both the thing being rounded (weights) and important rounding directions
    (Hessians) are not too large in any coordinate. This enables quantization with
    *provably* bounded error.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.1  (Chee et al. ([2023](#bib.bib4))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A Hessian $H\in\mathbb{R}^{n\times n}$ has
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\max_{i,j}\;&#124;Q_{ij}&#124;=\max_{i,j}\;&#124;e_{i}^{T}Qe_{j}&#124;\leq\mu/\sqrt{n}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: A weight matrix $W\in\mathbb{R}^{m\times n}$-incoherent if
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{i,j}\;\textstyle&#124;W_{ij}&#124;=\max_{i,j}\;&#124;e_{i}^{T}We_{j}&#124;\leq\mu\&#124;W\&#124;_{F}/\sqrt{mn}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: To exploit incoherence, Chee et al. ([2023](#bib.bib4)) introduced *incoherence
    processing* as a part of their quantization method QuIP. QuIP’s incoherence processing
    works by conjugating $W$ to compute
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle U^{T}(\operatorname{quantized}(\tilde{W})(Vx))\approx U^{T}(\tilde{W}(Vx))=Wx.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: These structured orthogonal multiplies by a Kronecker product lead to a runtime
    overhead of $\Theta(n\sqrt{n}+m\sqrt{m})$.
  prefs: []
  type: TYPE_NORMAL
- en: Incoherence processing can be seen as a principled alternative to more complicated
    and heuristic methods for outlier suppression. Methods such as grouping require
    extra storage and can negatively impact performance. For example, using a 16 bit
    scale per group of 64 weights requires an extra 0.25 bits per weight. This increase
    is significant in extreme compression regimes, whereas incoherence processing
    allows more bits to be spent on actually quantizing model weights.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Vector Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prior PTQ works have focused on quantizing each scalar weight $W_{ij}$ with
    our rounding method BlockLDLQ in Section [4.1](#S4.SS1 "4.1 Adaptive Rounding
    for Vector Quantization ‣ 4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Incoherence Processing with the Randomized Hadamard Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 3 Incoherence Processing with RHT (IP-RHT)
  prefs: []
  type: TYPE_NORMAL
- en: 0:  $W\in\mathbb{R}^{m\times n},H\in\mathbb{R}^{n\times n}$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we propose a way of improving the incoherence processing of
    QuIP by replacing the 2-factor Kronecker product by a Randomized Hadamard Transformation
    (RHT) (Halko et al., [2011](#bib.bib15)). This change yields three advantages:
    (1) the theoretical bound on the incoherence parameter $\mu$. Additionally, we
    show in Section [6.4](#S6.SS4 "6.4 Ablations ‣ 6 Experiments ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks") that this change
    by itself improves the perplexity of quantized LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from section [2.3](#S2.SS3 "2.3 Incoherence Processing ‣ 2 Background
    / Related Work ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") that one way to efficiently perform incoherence processing
    is to conjugate $W$. We will temporarily assume that all dimensions are powers
    of 2. Later in the section we will explain 2 methods for incoherence processing
    when the dimension is not a power of 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '{restatable}'
  prefs: []
  type: TYPE_NORMAL
- en: lemmalemmahadincoh Let $H$, where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu_{H}=\sqrt{2\log\left(\frac{2n^{2}}{\delta}\right)}\;\;\text{ and
    }\;\;\mu_{W}=2\log\left(\frac{4mn}{\delta}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In QuIP (Chee et al., [2023](#bib.bib4)), the 2-factor Kronecker approach achieves
    $\mu_{W}^{Kron}=A^{2}\log\left(4Cmn/\delta\right)^{2}$’s RHT achieves superior
    incoherence via a log dependence on the matrix size rather that the Kronecker
    method’s log-squared dependence. All of QuIP’s theory analyzing the proxy loss
    in Eq. ([1](#S2.E1 "Equation 1 ‣ 2.2 Quantization and Adaptive Rounding ‣ 2 Background
    / Related Work ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks")) still holds with the RHT, with the improved incoherence
    rates propagating through.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, what about dimensions $n$ add less than 0.01 bits per weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the Hadamard conjecture states that $\exists H_{k}\forall k,4\mid k$ to
    new hardware. In practice, we find that the RFFT performs slightly worse than
    the RHT but still achieves strong results (Table [1](#S3.T1 "Table 1 ‣ 3 Incoherence
    Processing with the Randomized Hadamard Transform ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")). We describe the RFFT in detail
    in Section [A.2](#A1.SS2 "A.2 Incoherence Processing with the Randomized Fast
    Fourier Transform (RFFT) ‣ Appendix A Concentration Inequalities for the Randomized
    Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: RHT vs. RFFT incoherence processing using 2 Bit QuIP$\#$), context
    length 4096.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Incoherence | 2-7B | 2-13B | 2-70B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hadamard | 8.22 | 6.06 | 4.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Fourier | 8.30 | 6.08 | 4.17 |'
  prefs: []
  type: TYPE_TB
- en: 4 BlockLDLQ and Lattice Codebooks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The proof of Lemma [3](#alg3 "Algorithm 3 ‣ 3 Incoherence Processing with the
    Randomized Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks") tells us that the incoherence-processed weights
    follow a roughly ball-shaped sub-Gaussian distribution. However, rounding weights
    one at a time, as QuIP does with its LDLQ, ignores this shaping—producing a set
    of representable weight vectors that is shaped like a hypercube rather than a
    ball. Vector quantization (VQ) lets us shape codebooks to better match the source
    distribution. In Section [4.1](#S4.SS1 "4.1 Adaptive Rounding for Vector Quantization
    ‣ 4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks"), we introduce BlockLDLQ, which iteratively
    rounds blocks of weights with VQ. Within BlockLDLQ’s VQ step, QuIP$\#$ (Viazovska,
    [2017](#bib.bib31)). E8P achieves good shaping while admitting fast inference
    from only needing to lookup from a size 256 codebook.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Adaptive Rounding for Vector Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chee et al. ([2023](#bib.bib4)) formulated a class of adaptive rounding algorithms
    with linear feedback. These methods round columns one at a time with linear feedback
    $a_{k}$ acts elementwise.
  prefs: []
  type: TYPE_NORMAL
- en: The LDLQ algorithm sets U to be $L^{T}-I$. From QuIP, we know that LDLQ is optimal
    within adaptive rounding methods with linear feedback when rounding to the integers.
    However, LDLQ does not work with vector quantization, which rounds multiple columns
    together.
  prefs: []
  type: TYPE_NORMAL
- en: We propose to extend LDLQ to support vector quantization. Given a block size
    $g$ in a block-wise fashion via
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{W}_{k}=\mathbf{Q}(W_{k}+(W_{:(k-1)}-\hat{W}_{:(k-1)})\mathbf{A}_{k}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{A}_{k}\in\mathbb{R}^{n\times g}$. Then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq\frac{gm\mu^{2}\sigma^{2}}{n}\operatorname{tr}(H^{1/2})^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Observe that under the same conditions, just quantizing all blocks independently
    would yield $\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq gm\sigma^{2}\operatorname{tr}(H)$ 
    beyond Theorem [4.1](#S4.Ex5 "4.1 Adaptive Rounding for Vector Quantization ‣
    4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks"), so (if desired) they are left as an exercise
    for the reader.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 The E8P (“E8 Padded”) Codebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BlockLDLQ relies on an internal vector quantization (VQ) step $\mathbf{Q}$.
    Since the codebook size is exponential in both the vector dimension and bitrate,
    VQ quickly becomes intractable at high dimensions or bitrates.
  prefs: []
  type: TYPE_NORMAL
- en: For QuIP$\#$ whose sum is an even number, that is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle E_{8}=\left(\mathbb{Z}^{8}\cup\left(\mathbb{Z}^{8}+\frac{1}{2}\right)\right)\cap\left\{x\mid\mathbf{1}^{T}x\text{
    is even}\right\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: The construction of the E8P codebook starts with an equivalent way to write
    $E_{8}$ (keeping the same optimal packing density).
  prefs: []
  type: TYPE_NORMAL
- en: $\hat{D}_{8}$-entry lattice codebook “E8P.”
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e629a2444b3e422907b5af4ac9e28ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Minimum achievable elementwise MSE of quantizing a Gaussian to various
    codebooks. $E_{8}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.2 The E8P (“E8 Padded”) Codebook ‣ 4 BlockLDLQ
    and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") plots the elementwise MSE of quantizing a standard multivariate
    Gaussian to various $k$. This figure illustrates the importance of dimension for
    vector quantization. Increasing the vector dimension decreases the error for the
    half integer grid, as the resulting codebook is closer in shape to the source
    distribution. Finally, while K-means on the source distribution would achieve
    lower MSE (Lloyd, [1982](#bib.bib21)), there are a number of practical reasons
    why a K-means based codebook would be less practical, including worse end-to-end
    empirical performance. We discuss this more in Section [C.3](#A3.SS3 "C.3 Why
    not K-Means? ‣ Appendix C E8P details ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Scaling $E_{8}$ to Higher Bitrates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The $E_{8}$ with norm 4). One could also use more advanced multi-codebook quantization
    approaches other than RVQ, but we found that RVQ was sufficient to achieve strong
    quantization performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Fine-Tuning During Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent works have suggested that at extreme quantization levels (e.g. 2 bits),
    inter-layer interactions are a significant hurdle to lossless quantization (Shao
    et al., [2024](#bib.bib26); Egiazarian et al., [2024](#bib.bib10)). Here, we employ
    a simple fine-tuning algorithm that attempts to recover the original unquantized
    model during quantization. Our fine tuning method runs on a small development
    set and works by relaxing the sign vectors in the RHT to arbitrary real vectors
    after quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Our fine tuning method contains two steps. First, we fine-tune within each transformer
    block by quantizing the first linear layer, fine-tuning the remaining parameters
    to match the unquantized model’s output activations on the unquantized model’s
    input activations, quantizing the second linear layer, fine-tuning, and so on
    until all linear layers are quantized. This step attempts to minimize the activation
    error caused by an individual linear layer during quantization, and it is parallelizable
    across transformer blocks as the activation error does not consider the effect
    of quantizing preceding blocks. The idea of fine-tuning on the level of a transformer
    block was previously proposed in Egiazarian et al. ([2024](#bib.bib10)); our methodology
    differs in that we set a different set of parameters to be trainable. In the second
    step, after all linear layers in the model are quantized, the unquantized parameters
    (layernorms, sign vectors, language model head) are fine-tuned to minimize activation
    error over the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: 'By optimizing the sign vectors as real vectors instead of binary vectors in
    both steps, we allow the incoherence processing step to shape the weight matrix
    to the codebook. While this means we must store the sign vectors in FP16 instead
    of as bitvectors, the size of LLM matrices means that the sign vectors still add
    less than 0.01 bits per weight. We describe these steps in more detail in Section
    [D](#A4 "Appendix D Fine-Tuning During Quantization ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 2: Llama 1 & 2 Wikitext2 and C4 perplexity ($\downarrow$), context length
    2048.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Wikitext 2 | C4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Bits | 1-7 | 1-13 | 1-30 | 1-65 | 2-7 | 2-13 | 2-70 | 1-7 | 1-13
    | 1-30 | 1-65 | 2-7 | 2-13 | 2-70 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | 16 | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.32 | 7.08 | 6.61
    | 5.98 | 5.62 | 6.97 | 6.47 | 5.52 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4 | 6.08 | 5.34 | 4.39 | 3.76 | 6.15 | 5.12 | - | 7.52 | 6.86 | 6.17
    | 5.77 | 7.68 | 6.74 | - |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQ | 4 | 5.86 | 5.21 | 4.25 | 3.71 | 5.74 | 5.02 | 3.47 | 7.34 | 6.76
    | 6.11 | 5.73 | 7.35 | 6.65 | 5.65 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# no FT & no $E_{8}$ | 4 | 5.83 | 5.20 | 4.23 | 3.63 | 5.66 | 5.00 |
    3.42 | 7.25 | 6.70 | 6.06 | 5.68 | 7.17 | 6.59 | 5.59 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 4 | 5.76 | 5.17 | 4.18 | 3.60 | 5.56 | 4.95 | 3.38 | 7.18 | 6.67
    | 6.03 | 5.66 | 7.07 | 6.54 | 5.56 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 3 | 11.9 | 7.45 | 10.0 | 5.21 | 24.0 | 10.5 | - | 13.3 | 9.13 | 12.7
    | 7.11 | 23.9 | 13.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQ | 3 | 6.49 | 5.68 | 4.74 | 4.04 | 6.58 | 5.58 | 3.92 | 8.19 | 7.32
    | 6.57 | 6.07 | 8.65 | 7.44 | 6.06 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# no FT & no $E_{8}$ | 3 | 6.29 | 5.52 | 4.54 | 3.91 | 6.19 | 5.34 |
    3.71 | 7.82 | 6.98 | 6.29 | 5.86 | 7.85 | 6.98 | 5.78 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 3 | 5.98 | 5.31 | 4.36 | 3.78 | 5.79 | 5.10 | 3.56 | 7.39 | 6.83
    | 6.17 | 5.77 | 7.32 | 6.72 | 5.67 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQ | 2 | 15.5 | 13.2 | 8.71 | 7.58 | 37.4 | 17.2 | 7.81 | 24.9 | 18.3
    | 13.9 | 10.8 | 90.6 | 26.8 | 12.3 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# no FT & no $E_{8}$ | 2 | 9.95 | 7.18 | 5.80 | 5.02 | 12.3 | 7.60 |
    4.87 | 11.7 | 8.67 | 7.55 | 6.83 | 14.8 | 9.57 | 6.82 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2 | 6.86 | 5.97 | 5.02 | 4.36 | 6.66 | 5.74 | 4.16 | 8.36 | 7.48
    | 6.71 | 6.19 | 8.35 | 7.45 | 6.12 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Zeroshot Accuracy (acc in LM Eval, not acc_norm), Llama 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 2-70 | 2-13 | 2-7 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Bits | ArcC | ArcE | PiQA | Wino | Bits | ArcC | ArcE | PiQA | Wino
    | Bits | ArcC | ArcE | PiQA | Wino |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | 16 | 51.1 | 77.7 | 81.1 | 77.0 | 16 | 45.6 | 73.3 | 73.5 | 69.6 |
    16 | 40.0 | 69.3 | 78.5 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQ | 4 | 49.8 | 77.9 | 80.7 | 75.8 | 4 | 43.1 | 70.2 | 78.4 | 67.8 | 4
    | 37.9 | 67.8 | 77.1 | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 4 | 47.0 | 74.3 | 80.3 | 76.0 | 4 | 44.9 | 73.3 | 79.0 | 69.7 | 4
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 4.07 | 51.0 | 78.1 | 81.4 | 76.9 | 3.94 | 43.9 | 72.2 | 78.6 | 70.4
    | 4.04 | 40.3 | 68.9 | 77.7 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 4 | 50.6 | 78.1 | 81.4 | 77.1 | 4 | 45.5 | 73.9 | 78.9 | 69.9 | 4
    | 40.5 | 69.1 | 78.4 | 67.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQ | 3 | 47.6 | 75.7 | 79.7 | 73.5 | 3 | 42.0 | 69.0 | 77.7 | 65.9 | 3
    | 35.3 | 62.6 | 73.6 | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 3 | 46.3 | 73.2 | 80.0 | 74.6 | 3 | 41.5 | 70.4 | 76.9 | 69.9 | 3
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 3.01 | 50.0 | 77.6 | 81.3 | 77.2 | 3.03 | 43.6 | 73.5 | 77.8 | 67.6
    | 3.04 | 38.7 | 67.8 | 76.6 | 68.4 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 3 | 50.9 | 77.7 | 81.4 | 76.4 | 3 | 44.0 | 72.5 | 78.4 | 69.1 | 3
    | 39.2 | 68.4 | 77.3 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQ | 2 | 28.7 | 55.4 | 68.8 | 53.2 | 2 | 23.0 | 44.4 | 62.6 | 52.6 | 2
    | 21.6 | 35.2 | 57.5 | 51.5 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 2 | 34.0 | 62.2 | 74.8 | 67.5 | 2 | 23.5 | 45.2 | 62.0 | 52.8 | 2
    | 19.4 | 26.0 | 54.6 | 51.8 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.07 | 47.9 | 77.7 | 80.4 | 75.9 | 1.97 | 38.5 | 67.0 | 75.1 | 69.5
    | 2.02 | 33.6 | 62.8 | 73.5 | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2 | 48.7 | 77.3 | 80.3 | 75.9 | 2 | 39.5 | 69.3 | 77.3 | 67.7 | 2
    | 34.6 | 64.6 | 75.1 | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Wikitext2 and C4 perplexity ($\downarrow$), context length 4096.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | 2-7 |  |  | 2-13 |  |  | 2-70 |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Bits | W2 | C4 | Bits | W2 | C4 | Bits | W2 | C4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | 16 | 5.12 | 6.63 | 16 | 4.57 | 6.05 | 16 | 3.12 | 4.97 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 4 | 5.19 | 6.75 | 4 | 4.63 | 6.13 | 4 | 3.18 | 5.02 |'
  prefs: []
  type: TYPE_TB
- en: '| no FT | 4 | 5.22 | 6.79 | 4 | 4.65 | 6.15 | 4 | 3.18 | 5.02 |'
  prefs: []
  type: TYPE_TB
- en: '|   no $E_{8}$ | 4 | 5.29 | 6.86 | 4 | 4.68 | 6.20 | 4 | 3.22 | 5.05 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 4 | - | - | 4 | 4.76 | 6.29 | 4 | 3.58 | 5.38 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 4.04 | 5.21 | 6.74 | 3.94 | 4.64 | 6.14 | 4.07 | 3.17 | 5.01 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 3 | 5.41 | 7.04 | 3 | 4.78 | 6.35 | 3 | 3.35 | 5.15 |'
  prefs: []
  type: TYPE_TB
- en: '| no FT | 3 | 5.60 | 7.34 | 3 | 4.90 | 6.50 | 3 | 3.41 | 5.20 |'
  prefs: []
  type: TYPE_TB
- en: '|   no $E_{8}$ | 3 | 5.77 | 7.61 | 3 | 4.99 | 6.65 | 3 | 3.48 | 5.28 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 3 | - | - | 3 | 5.12 | 6.79 | 3 | 3.87 | 5.67 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 3.04 | 5.46 | 7.10 | 3.03 | 4.83 | 6.37 | 3.01 | 3.36 | 5.17 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2 | 6.19 | 8.16 | 2 | 5.35 | 7.20 | 2 | 3.91 | 5.71 |'
  prefs: []
  type: TYPE_TB
- en: '| no FT | 2 | 8.22 | 11.0 | 2 | 6.06 | 8.07 | 2 | 4.16 | 6.01 |'
  prefs: []
  type: TYPE_TB
- en: '|   no $E_{8}$ | 2 | 11.2 | 14.5 | 2 | 7.04 | 9.37 | 2 | 4.58 | 6.51 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 2 | - | - | 2 | 13.5 | 16.2 | 2 | 5.90 | 8.17 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.02 | 6.93 | 8.84 | 1.97 | 5.70 | 7.59 | 2.07 | 3.94 | 5.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: 2 and 4 bit QuIP$\#$ peak memory bandwidth (1008GB/s) during generation
    and is fast and scalable.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2 Bit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tok/s &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2 Bit % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mem BW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4 Bit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tok/s &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4 Bit % &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mem BW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7B | 170.50 | 29.60% | 117.73 | 40.87% |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13B | 104.83 | 33.80% | 71.09 | 45.84% |'
  prefs: []
  type: TYPE_TB
- en: '| 1-30B | 51.60 | 38.39% | 32.50 | 48.36% |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70B | 32.74 | 56.84% | OOM | OOM |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/58bc67c23ebcc80018db3cea8ce085a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: QuIP$\#$ 2 bit scales similarly to higher bitrates.'
  prefs: []
  type: TYPE_NORMAL
- en: Our main experiments show the performance of QuIP$\#$ on the Llama 1 (Touvron
    et al., [2023a](#bib.bib29)) and 2 (Touvron et al., [2023b](#bib.bib30)) family
    of models. These models range in size from 7 billion to 70 billion parameters
    and offer good performance, making them suitable for understanding how quantization
    methods perform and scale. Additional results for other models are available in
    the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [6.1](#S6.SS1 "6.1 QuIP# on Llama Models ‣ 6 Experiments ‣ QuIP#:
    Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks"),
    we compare QuIP$\#$.'
  prefs: []
  type: TYPE_NORMAL
- en: We report W$x$ numbers. Finally, we bold numbers in our tables when they are
    clearly better, such as a smaller model matching or outperforming a larger model
    or a similar sized model significantly outperforming another model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 QuIP$\#$ on Llama Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [2](#S6.T2 "Table 2 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows a comparison of QuIP$\#$ without
    fine-tuning or lattice codebooks significantly outperforms OmniQuant and AWQ,
    which both rely on heuristics to reduce model outliers during quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows a comparison of QuIP$\#$ 3
    and 4 bit results presented in this paper use residual vector quantization; one
    could potentially achieve better numbers with more advanced multi-codebook quantization
    approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S6.T3 "Table 3 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows zeroshot results for QuIP$\#$.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 QuIP$\#$ Bit Scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figures [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") (first page) and [4](#S6.F4
    "Figure 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks") show how QuIP$\#$ 3 bit outperforms a theoretical
    lossless 4 bit model (FP16 at 4 bits). To the best of our knowledge, this is the
    first time a 3 bit PTQ method has outperformed a theoretical lossless 4 bit model
    and also the first time a 2 bit PTQ method has offered similar scaling to higher
    bitrates.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Efficient Inference with QuIP$\#$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows 2 and 4 bit QuIP$\#$ inference
    is fast and scalable on modern GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Ablations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") also contains an ablation on
    the various components of QuIP$\#$’s RHT. The RHT offers stronger incoherence
    properties than the Kronecker factorization (Section [3](#S3 "3 Incoherence Processing
    with the Randomized Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks")), which improves performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present QuIP$\#$ is the first PTQ method to achieve superior scaling at 3
    bits over 4 bits and similar scaling at 2 bits to higher bitrates. Our results
    indicate that, in the near future, 2 bit models are likely to scale better than
    3 bit ones.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank, in no particular order, David Hou for helping with the QuIP$\#$, and
    Together AI for compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almazrouei et al. (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Étienne Goffinet, Hesslow, D., Launay, J., Malartic,
    Q., Mazzotta, D., Noune, B., Pannier, B., and Penedo, G. The falcon series of
    open language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2024) Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D.,
    and Dao, T. Medusa: Simple llm inference acceleration framework with multiple
    decoding heads. *arXiv preprint arXiv: 2401.10774*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chee et al. (2022) Chee, J., Renz, M., Damle, A., and Sa, C. D. Model preserving
    compression for neural networks. In Oh, A. H., Agarwal, A., Belgrave, D., and
    Cho, K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL
    [https://openreview.net/forum?id=gt-l9Hu2ndd](https://openreview.net/forum?id=gt-l9Hu2ndd).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chee et al. (2023) Chee, J., Cai, Y., Kuleshov, V., and Sa, C. D. QuIP: 2-bit
    quantization of large language models with guarantees. In *Thirty-seventh Conference
    on Neural Information Processing Systems*, 2023. URL [https://openreview.net/forum?id=xrk9g5vcXR](https://openreview.net/forum?id=xrk9g5vcXR).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cochran et al. (1967) Cochran, W., Cooley, J., Favin, D., Helms, H., Kaenel,
    R., Lang, W., Maling, G., Nelson, D., Rader, C., and Welch, P. What is the fast
    fourier transform? *Proceedings of the IEEE*, 55(10):1664–1674, 1967. doi: 10.1109/PROC.1967.5957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computer (2023) Computer, T. Redpajama: An open source recipe to reproduce
    llama training dataset, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao (2023) Dao, T. FlashAttention-2: Faster attention with better parallelism
    and work partitioning. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. FlashAttention:
    Fast and memory-efficient exact attention with IO-awareness. In *Advances in Neural
    Information Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for
    4-bit precision: k-bit inference scaling laws. In Krause, A., Brunskill, E., Cho,
    K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings of the 40th
    International Conference on Machine Learning*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  7750–7774\. PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/dettmers23a.html](https://proceedings.mlr.press/v202/dettmers23a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egiazarian et al. (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., and Alistarh, D. Extreme compression of large language models
    via additive quantization, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fino & Algazi (1976) Fino and Algazi. Unified matrix treatment of the fast
    walsh-hadamard transform. *IEEE Transactions on Computers*, C-25(11):1142–1146,
    1976. doi: 10.1109/TC.1976.1674569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. OPTQ: Accurate quantization for generative pre-trained transformers. In *The
    Eleventh International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gray (1984) Gray, R. Vector quantization. *IEEE ASSP Magazine*, 1(2):4–29,
    1984. doi: 10.1109/MASSP.1984.1162229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Halko et al. (2011) Halko, N., Martinsson, P.-G., and Tropp, J. A. Finding
    structure with randomness: Probabilistic algorithms for constructing approximate
    matrix decompositions. *SIAM review*, 53(2):217–288, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hedayat & Wallis (1978) Hedayat, A. and Wallis, W. D. Hadamard Matrices and
    Their Applications. *The Annals of Statistics*, 6(6):1184 – 1238, 1978. doi: 10.1214/aos/1176344370.
    URL [https://doi.org/10.1214/aos/1176344370](https://doi.org/10.1214/aos/1176344370).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F.,
    Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A.,
    Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril,
    T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Juang & Gray (1982) Juang, B.-H. and Gray, A. Multiple stage vector quantization
    for speech coding. In *ICASSP ’82\. IEEE International Conference on Acoustics,
    Speech, and Signal Processing*, volume 7, pp.  597–600, 1982. doi: 10.1109/ICASSP.1982.1171604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma & Ba (2017) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C.,
    and Han, S. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lloyd (1982) Lloyd, S. Least squares quantization in pcm. *IEEE Transactions
    on Information Theory*, 28(2):129–137, 1982. doi: 10.1109/TIT.1982.1056489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and
    Blankevoort, T. Up or down? Adaptive rounding for post-training quantization.
    In III, H. D. and Singh, A. (eds.), *Proceedings of the 37th International Conference
    on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  7197–7206\. PMLR, 13–18 Jul 2020. URL [https://proceedings.mlr.press/v119/nagel20a.html](https://proceedings.mlr.press/v119/nagel20a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2022) Nagel, M., Fournarakis, M., Bondarenko, Y., and Blankevoort,
    T. Overcoming oscillations in quantization-aware training. In Chaudhuri, K., Jegelka,
    S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), *Proceedings of
    the 39th International Conference on Machine Learning*, volume 162 of *Proceedings
    of Machine Learning Research*, pp.  16318–16330\. PMLR, 17–23 Jul 2022. URL [https://proceedings.mlr.press/v162/nagel22a.html](https://proceedings.mlr.press/v162/nagel22a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes,
    C., Wornow, M., Patel, A., Rabideau, C., Massaroli, S., Bengio, Y., Ermon, S.,
    Baccus, S. A., and Ré, C. Hyenadna: Long-range genomic sequence modeling at single
    nucleotide resolution. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2024) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov,
    A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong,
    W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N.,
    Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2024) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirectionally calibrated
    quantization for large language models. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=8Wuvhh0LYW](https://openreview.net/forum?id=8Wuvhh0LYW).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (27) Sloane, N. Hadamard Matrices — neilsloane.com. [http://neilsloane.com/hadamard/](http://neilsloane.com/hadamard/).
    [Accessed 02-02-2024].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and
    effective pruning approach for large language models. In *Workshop on Efficient
    Systems for Foundation Models @ ICML2023*, 2023. URL [https://openreview.net/forum?id=tz9JV2PRSv](https://openreview.net/forum?id=tz9JV2PRSv).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Viazovska (2017) Viazovska, M. The sphere packing problem in dimension $8$.
    *Annals of Mathematics*, 185(3), May 2017. ISSN 0003-486X. doi: 10.4007/annals.2017.185.3.7.
    URL [http://dx.doi.org/10.4007/annals.2017.185.3.7](http://dx.doi.org/10.4007/annals.2017.185.3.7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Concentration Inequalities for the Randomized Hadamard Transform
    and Fast Fourier Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Incoherence Processing with the Randomized Hadamard Transform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lemma A.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any non-negative real number $n$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We start with the following “standard” integral. For non-negative integer $m$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This means that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}x^{2m}(1-x^{2})^{n-1}\;dx$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{\Gamma\left(m+\frac{1}{2}\right)\Gamma\left(n\right)}{\Gamma\left(m+n+\frac{1}{2}\right)}\cdot\frac{\Gamma\left(n+\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(n\right)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{\Gamma\left(m+\frac{1}{2}\right)\Gamma\left(n+\frac{1}{2}\right)}{\sqrt{\pi}\cdot\Gamma\left(m+n+\frac{1}{2}\right)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Applying the Legendre duplication formula, for integer $m$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Gamma\left(m+\frac{1}{2}\right)=\frac{(2m)!\sqrt{\pi}}{4^{m}m!},$ |  |'
  prefs: []
  type: TYPE_TB
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}x^{2m}(1-x^{2})^{n-1}\;dx$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{(2m)!(2n)!(m+n)!}{m!n!(2m+2n)!}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: In particular, this means that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}\exp(tx)(1-x^{2})^{n-1}\;dx$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{(2n)!(m+n)!}{n!(2m+2n)!}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{1}{2^{m}}\prod_{k=1}^{m}\frac{1}{2k+2n-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{1}{2^{m}}\left(\frac{1}{2n+1}\right)^{m}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{1}{m!}\left(\frac{t^{2}}{4n+2}\right)^{m}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\exp\left(\frac{t^{2}}{4n+2}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: This proves the lemma
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma A.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Call $U\in\mathbb{R}^{nd\times nd}$ blocks are zero). Then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we let the $i$ dimensional space. Observe that this means that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{P}(z_{i})\propto(1-z_{i}^{2})^{\frac{d-1}{2}-1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: So,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{E}\left[\exp\left(tb^{T}USx\right)\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\prod_{i=1}^{n}\mathbf{E}\left[\exp\left(t\left\&#124;b\right\&#124;\left\&#124;x_{i}\right\&#124;n^{-1/2}z_{i}\right)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\prod_{i=1}^{n}\mathbf{E}\left[\exp\left(\frac{t^{2}\left\&#124;b\right\&#124;^{2}\left\&#124;x_{i}\right\&#124;^{2}}{2nd}\right)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbf{E}\left[\exp\left(\frac{t^{2}\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}{2nd}\right)\right],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the the last line follows from Lemma [A.1](#A1.Ex8 "Lemma A.1\. ‣ A.1
    Incoherence Processing with the Randomized Hadamard Transform ‣ Appendix A Concentration
    Inequalities for the Randomized Hadamard Transform and Fast Fourier Transform
    ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks").
    It follows from the standard application of Markov’s inequality that for any <math
    id="A1.SS1.2.p1.13.m1.1" class="ltx_Math" alttext="a></math>,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This is what we wanted to show. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma A.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $H\in\mathbb{R}^{n\times n}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{Prob}\left(\max_{i,j}\left&#124;e_{i}^{T}HSUe_{j}\right&#124;\geq\sqrt{\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)}\right)\leq\epsilon$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{Prob}\left(\max_{i,j}\left&#124;e_{i}^{T}FPUe_{j}\right&#124;\geq\sqrt{\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)}\right)\leq\epsilon.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: That is, with probability at least $1-\epsilon$-incoherent, where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu_{H}=\sqrt{2\log\left(\frac{2n^{2}}{\epsilon}\right)}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Setting $b=e_{i}$ in Lemma [A.2](#A1.Ex25 "Lemma A.2\. ‣ A.1 Incoherence Processing
    with the Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities
    for the Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even
    Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks"),'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{P}\left(\left&#124;e_{i}^{T}HSUe_{j}\right&#124;\geq a\right)\leq
    2\exp\left(-\frac{a^{2}nd}{2}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: By the union bound,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Setting
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $a^{2}=\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: proves the lemma. The FFT case is identical. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma A.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $H_{L}\in\mathbb{R}^{m\times m}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: That is, with probability at least $1-\epsilon$-incoherent, where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu_{W}=2\log\left(\frac{4mn}{\epsilon}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From Lemma [A.2](#A1.Ex25 "Lemma A.2\. ‣ A.1 Incoherence Processing with the
    Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities for the
    Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks"),'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq\left\&#124;b\right\&#124;\left\&#124;x\right\&#124;\sqrt{\frac{2}{n}\log\left(\frac{4mn}{\epsilon}\right)}\right)\leq\frac{\epsilon}{2mn}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: By applying this once on each side to the rows and columns respectively, and
    union bounding over the $mn$ entries, we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The proof in the FFT case is identical. ∎
  prefs: []
  type: TYPE_NORMAL
- en: \lemmahadincoh
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The incoherence of $H$ follows from the application of Lemma [A.4](#A1.Thmtheorem4
    "Lemma A.4\. ‣ A.1 Incoherence Processing with the Randomized Hadamard Transform
    ‣ Appendix A Concentration Inequalities for the Randomized Hadamard Transform
    and Fast Fourier Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks"). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Incoherence Processing with the Randomized Fast Fourier Transform (RFFT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithm 4 Incoherence Processing with RFFT (IP-RFFT)
  prefs: []
  type: TYPE_NORMAL
- en: 0:  $W\in\mathbb{R}^{m\times n},H\in\mathbb{R}^{n\times n}$
  prefs: []
  type: TYPE_NORMAL
- en: Here we described the Randomized Fast Fourier Transform (RFFT), $x\to VSx$,
    and interpreting the corresponding 2-tuples as a complex number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Incoherence processing via the RFFT achieves similar theoretical guarantees
    as the RHT, see Lemmas [A.3](#A1.Thmtheorem3 "Lemma A.3\. ‣ A.1 Incoherence Processing
    with the Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities
    for the Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even
    Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks") and [A.4](#A1.Thmtheorem4
    "Lemma A.4\. ‣ A.1 Incoherence Processing with the Randomized Hadamard Transform
    ‣ Appendix A Concentration Inequalities for the Randomized Hadamard Transform
    and Fast Fourier Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks"). Ultimately the choice of the orthogonal transformation
    is up to the user. A Fourier transform works almost as well as a Hamard transform
    in practice (Table [1](#S3.T1 "Table 1 ‣ 3 Incoherence Processing with the Randomized
    Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks")), so if a fast Hadamard implementation is not available,
    the FFT is a good option.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Block LDLQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lemma B.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $H\in\mathbb{R}^{nd\times nd}$. Then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{tr}\left(D\right)\leq\operatorname{tr}\left(H^{1/2}\right)\cdot\left\&#124;H^{1/2}\odot
    M_{D}\right\&#124;_{2},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $M_{D}=I\otimes\mathbf{1}_{d\times d}$ has
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;U_{ij}\&#124;\leq\frac{\mu}{\sqrt{nd}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{tr}\left(D\right)\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right)^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider the optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | minimize: | $\displaystyle\operatorname{tr}\left(R^{T}HR\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | subject to: | $\displaystyle R\text{ unit block lower diagonal}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Observe that the derivative of the loss is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla f(R)=HR.$ |  |'
  prefs: []
  type: TYPE_TB
- en: If $R=L^{-1}$.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let $M$. Observe that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left(I+\alpha M\odot H^{1/2}\right)^{T}\left(I+\alpha M\odot
    H^{1/2}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\succeq I+\alpha(M+M^{T})\odot H^{1/2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\succeq\alpha M_{D}\odot H^{1/2}+\alpha(M+M^{T})\odot
    H^{1/2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\succeq\alpha H^{1/2}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: It follows by inverting both sides that $RR^{T}\preceq\alpha^{-1}H^{-1/2}$.
  prefs: []
  type: TYPE_NORMAL
- en: So, for this $R$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{tr}\left(R^{T}HR\right)=\operatorname{tr}\left(HRR^{T}\right)\leq\alpha^{-1}\operatorname{tr}\left(H^{1/2}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This proves the first part of the lemma. For the second part, observe that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;H^{1/2}\odot M_{D}\right\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This proves the lemma. ∎
  prefs: []
  type: TYPE_NORMAL
- en: \thmLDLQ
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: First recall that from the description of block LDLQ,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{W}_{k}=\mathbf{Q}(W_{k}+(W_{:(k-1)}-\hat{W}_{:(k-1)})\mathbf{A}_{k}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We can also write this in matrix form in terms of the matrix $\mathbf{L}_{k}$
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{W}=\mathbf{Q}(W+(W-\hat{W})(\mathbf{L}^{T}-I)).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathbf{Q}$ denote the quantization error
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\eta=(W+(W-\hat{W})(\mathbf{L}^{T}-I))-\mathbf{Q}(W+(W-\hat{W})(\mathbf{L}^{T}-I)).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{W}=(W+(W-\hat{W})(\mathbf{L}^{T}-I))-\eta,$ |  |'
  prefs: []
  type: TYPE_TB
- en: which simplifies to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(W-\hat{W})\mathbf{L}^{T}=\eta.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This means that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: But by assumption, $\mathbf{E}[\eta\eta^{T}]\preceq m\sigma^{2}I$ rows), so
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq m\sigma^{2}\mathbf{E}[\operatorname{tr}(\mathbf{D})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Combining this with the result of Lemma [B.1](#A2.Thmtheorem1 "Lemma B.1\.
    ‣ Appendix B Block LDLQ ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") proves the theorem. ∎'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C E8P details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 Constructing $S$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the following 29 elements of $\hat{D}_{8}$ to 256 entries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: C.2 Example Decoding with E8P
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we give an example of decoding with E8P. In this example, the first 8
    bits of the codeword encode the entry in $S$. In this example, let that be the
    vector
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s=\left\{\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}\right\},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which is not in $\hat{D_{8}}$. Then, the next 7 bits 1001011 would indicate
    that we need to negate the 1st, 2nd, 4th, and 7th from right bits. Since we need
    an odd number of sign flips, the 8th from right bit is also a sign flip. The sign-decoded
    vector is then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\{-\frac{1}{2},-\frac{1}{2},\frac{1}{2},\frac{3}{2},-\frac{1}{2},\frac{1}{2},-\frac{1}{2},-\frac{1}{2}\right\},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which we can verify is in $E_{8}$, so the final decoded vector is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\{-\frac{1}{4},-\frac{3}{4},\frac{3}{4},\frac{7}{4},-\frac{1}{4},\frac{3}{4},-\frac{1}{4},-\frac{1}{4}\right\},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which is in $E_{8}+\frac{1}{4}$ as desired.
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Why not K-Means?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A significant motivating factor behind E8P is that post-incoherence processing,
    entries of $W$-dimensional Gaussian, we can get around this but sacrifice accuracy
    at the axis region. Second, using K-means requires storing a codebook in fp16,
    whereas the entries of E8P can be stored as 4 bit integers. This means that during
    inference, the source codebook for a 8 dimension K-means codebook will be 4 times
    larger than the source codebook of E8P, running the risk of a cache eviction.
    Finally, we observe that empirically, E8P actually outperforms K-means, which
    is somewhat interesting and suggests that allocating more information to the edge
    of the distribution, even after incoherence processing, is useful.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Fine-Tuning During Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Algorithm [5](#alg5 "Algorithm 5 ‣ Appendix D Fine-Tuning During Quantization
    ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks")
    we describe our fine tuning procedure for QuIP$\#$.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5 QuIP$\#$ with Fine-Tuning
  prefs: []
  type: TYPE_NORMAL
- en: 0:  Unquantized Model $M$ for early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Additional Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E.1 QuIP$\#$ on Mixtral 8x7B (Jiang et al., [2024](#bib.bib17)) and Falcon 180B
    (Almazrouei et al., [2023](#bib.bib1))
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 6: 2 bit QuIP$\#$ scales to different architectures without issue.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Bits | Wiki2 | C4 | ArcC | ArcE | BoolQ | PiQA | Wino |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | 16 | 3.45 | 6.85 | 0.56 | 0.74 | 0.85 | 0.84 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | 2 | 4.69 | 8.25 | 0.49 | 0.68 | 0.81 | 0.80 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-180B | 16 | 3.30 | 6.31 | 0.61 | 0.82 | 0.87 | 0.85 | 0.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-180B | 2 | 4.18 | 7.06 | 0.58 | 0.81 | 0.84 | 0.84 | 0.81 |'
  prefs: []
  type: TYPE_TB
- en: E.2 Zeroshot performance for ablation on lattice codebooks and fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 7: Ablation on lattice codebooks and fine-tuning. QuIP$\#$ uses lattice
    codebooks and performs fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Bits |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ArcC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (acc_norm) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ArcE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (acc_norm) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BoolQ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (acc) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PiQA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (acc_norm) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Wino &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (acc) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | Native | 16 | 48.0 | 59.7 | 76.6 | 80.9 | 76.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# no FT & no $E_{8}$ | 4 | 49.4 | 60.1 | 77.6 | 80.7 | 76.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# No FT | 4 | 48.3 | 60.1 | 78.4 | 80.6 | 76.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# | 4 | 48.3 | 59.4 | 77.4 | 80.7 | 77.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# no FT & no $E_{8}$ | 3 | 47.4 | 59.1 | 75.8 | 80.9 | 77.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# No FT | 3 | 47.9 | 59.9 | 78.8 | 79.9 | 77.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# | 3 | 48.4 | 59.5 | 74.8 | 80.3 | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# no FT & no $E_{8}$ | 2 | 43.5 | 56.2 | 75.1 | 78.1 | 76.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# No FT | 2 | 47.2 | 59.5 | 79.1 | 78.6 | 74.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-70 | QuIP# | 2 | 47.7 | 59.1 | 80.3 | 79.4 | 75.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | Native | 16 | 44.3 | 58.0 | 69.0 | 79.0 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# no FT & no $E_{8}$ | 4 | 43.7 | 58.6 | 70.1 | 78.7 | 69.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# No FT | 4 | 42.9 | 56.4 | 67.8 | 78.9 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# | 4 | 44.2 | 57.7 | 69.7 | 78.9 | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# no FT & no $E_{8}$ | 3 | 42.1 | 55.2 | 70.0 | 77.8 | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# No FT | 3 | 41.9 | 57.7 | 73.3 | 78.1 | 68.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# | 3 | 43.3 | 57.7 | 69.8 | 78.4 | 69.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# no FT & no $E_{8}$ | 2 | 36.3 | 50.8 | 67.4 | 73.4 | 63.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# No FT | 2 | 37.1 | 50.1 | 66.5 | 75.7 | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-13 | QuIP# | 2 | 41.3 | 55.1 | 68.3 | 77.4 | 67.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | Native | 16 | 40.6 | 53.5 | 71.0 | 76.9 | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# no FT & no $E_{8}$ | 4 | 39.5 | 51.9 | 71.3 | 76.6 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# No FT | 4 | 40.4 | 53.7 | 68.5 | 77.2 | 67.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# | 4 | 40.1 | 53.4 | 69.9 | 76.5 | 67.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# no FT & no $E_{8}$ | 3 | 38.1 | 52.6 | 65.2 | 76.1 | 65.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# No FT | 3 | 37.7 | 53.1 | 70.6 | 76.7 | 67.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# | 3 | 39.4 | 53.8 | 69.7 | 76.1 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# no FT & no $E_{8}$ | 2 | 29.2 | 42.5 | 63.3 | 68.0 | 59.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# No FT | 2 | 32.5 | 42.8 | 62.3 | 71.2 | 62.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2-7 | QuIP# | 2 | 36.1 | 50.5 | 68.3 | 74.9 | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: E.3 More Scaling Plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80de8e01bb1ca5194938bb72a20ea1a6.png)![Refer to caption](img/894b7e740a88b1a98978ea867154ad6e.png)![Refer
    to caption](img/7edf9e76a8e8f9c11e6a25a5d5b31b68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: QuIP$\#$ 2 and 3 bit scale better than AQLM 2 and 3 bit. (Top Right)
    Llama 2 C4 Perplexity. Context length 4096\. (Bottom) Llama 1 C4 Perplexity. Context
    length 2048.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section contains implementation details for our Llama experiments. These
    details also mostly apply to the Mixtral and Falcon numbers except we use the
    Falcon dataset (Almazrouei et al., [2023](#bib.bib1)) as it is publicly avaiable.
  prefs: []
  type: TYPE_NORMAL
- en: F.1 Hessian Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hessian matrices $H$ were generated with 6144 sequences of a model’s native
    context length (2048 for Llama 1, 4096 for Llama 2) from the RedPajama 1T (Computer,
    [2023](#bib.bib6)) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: F.2 Hadamard Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use Hadamard matrices available at Neil Sloane’s website ([Sloane,](#bib.bib27)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: F.3 Perplexity and Zeroshot Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the OPTQ (Frantar et al., [2023](#bib.bib12)) “Wiktext2” and “C4” (not
    “C4 New”) sampling functions to calculate perplexity for our experiments. We use
    LM Eval (Gao et al., [2023](#bib.bib13)) to calculate zeroshot numbers.
  prefs: []
  type: TYPE_NORMAL
- en: F.4 Fine Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the within-transformer block section of fine-tuning, we use the Adam optimizer
    (Kingma & Ba, [2017](#bib.bib19)), a learning rate of $5\times 10^{-5}$ for everything
    else as above) for both the within-block and end to end fine tuning stages.
  prefs: []
  type: TYPE_NORMAL
- en: F.5 Hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All experiments were run on NVIDIA A100 GPUs except for the timing numbers,
    which were measured on a NVIDIA RTX 4090 to see what was possible with the current
    state-of-the-art NVIDIA consumer GPU. We find that we can quantize Llama 2 70B
    without fine tuning in under 10 GPU-hours and with fine tuning in around 100 GPU-hours.
    Both numbers do not include Hessian generation, which can be done once for a model
    and reused across many different quantization experiments.
  prefs: []
  type: TYPE_NORMAL
- en: F.6 Code and Prequantized Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our code is available at [https://github.com/Cornell-RelaxML/quip-sharp](https://github.com/Cornell-RelaxML/quip-sharp)
    and prequantized QuIP$\#$ models are available at [https://huggingface.co/relaxml](https://huggingface.co/relaxml).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Example Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below are some example generations from Llama 2 70B chat quantized with QuIP$\#$ to
    2 bits, truncated to 256 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A7.p2.pic1" class="ltx_picture" height="205.9" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,205.9) matrix(1 0 0 -1 0 0)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69" height="178.34" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Prompt: How much wood could
    a woodchuck chuck if a woodchuck could chuck wood? QuIP$\#$: Here is a simple
    example of a matrix multiply using CUDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
