- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular
    Quantizers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16119](https://ar5iv.labs.arxiv.org/html/2309.16119)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Junjie Yin jyin27@jhu.edu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Johns Hopkins University Jiahao Dong jd787@cornell.edu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Cornell University and Cornell Tech Yingheng Wang yw2349@cornell.edu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Cornell University Christopher De Sa cdesa@cs.cornell.edu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Cornell University Volodymyr Kuleshov kuleshov@cornell.edu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Cornell University and Cornell Tech
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We propose a memory-efficient finetuning algorithm for large language models
    (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision
    on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA),
    integrates any user-specified weight quantizer with finetuning via low-rank adapters
    (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that
    adaptively materializes low-precision LLM weights from a custom black-box quantization
    module. This approach enables finetuning 3-bit LLMs for the first time—leveraging
    state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies
    on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains
    competitive performance on text classification, natural language infernece, and
    instruction following tasks using significantly less memory than existing approaches,
    and we also surpass the state-of-the-art ROUGE score on a popular summarization
    task. We release ModuLoRA together with a series of low-precision models—including
    the first family of 3-bit instruction following Alpaca LLMs—as part of LLMTools,
    a user-friendly library for quantizing, running, and finetuning LLMs on consumer
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) excel across diverse tasks such as code generation,
    instruction following, and reasoning (Brown et al., [2020](#bib.bib2); Scao et al.,
    [2023](#bib.bib31); Zhang et al., [2022](#bib.bib42)). However, the massive size
    of these models—often reaching into hundreds of billions of parameters—makes them
    challenging to deploy on downstream tasks and motivates research into efficient
    finetuning algorithms (Li & Liang, [2021](#bib.bib19); Hu et al., [2022](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we propose modular low-rank adaptation (ModuLoRA), a memory-efficient
    finetuning algorithm for large language models (LLMs) that runs on consumer-grade
    hardware. For example, ModuLoRA finetunes a LLAMA-30B model (Touvron et al., [2023](#bib.bib35))
    on one Nvidia RTX 3090 24GB GPU and a LLAMA-65B on one RTX A6000 48GB GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach adds high-precision low-rank adapters to the low-precision 3-bit
    or 4-bit weights of a frozen base LLM obtained via modern quantization algorithms
    (Hubara et al., [2021](#bib.bib16); Yao et al., [2021](#bib.bib38); Frantar et al.,
    [2023](#bib.bib12)). Crucially, ModuLoRA does not specify its own quantization
    procedure—rather, it integrates with user-defined quantizers via a simple quantization-agnostic
    backward pass. This backward pass adaptively materializes low-precision LLM weights
    obtained from a black-box quantizer and integrates them with high-precision low-rank
    adapters.
  prefs: []
  type: TYPE_NORMAL
- en: We release ModuLoRA as part of LLMTools, a user-friendly library that enables
    finetuning LLMs on consumer GPUs. When paired with the modern OPTQ quantizer (Frantar
    et al., [2023](#bib.bib12)), ModuLoRA enables finetuning 3-bit LLMs for the first
    time, often outperforming methods based on less sophisticated 4-bit and 8-bit
    quantization. Across tasks in classification, natural language inference, and
    instruction following, our 3-bit and 4-bit models achieve competitive performance
    using significantly less memory than existing approaches. On a popular summarization
    benchmark, we attain a new state-of-the-art ROUGE score using a quantized LLAMA-65B
    model. We open-source all our low-precision models, including the first 3-bit
    family of Alpaca models that feature strong instruction-following performance
    at multiple model sizes. Our findings reveal that high performance can be achieved
    using smaller quantized LLMs than previously thought.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions.  In summary, this paper makes the following contributions: (1)
    we propose ModuLoRA, a memory-efficient finetuning method that operates over low-precision
    weights obtained via a user-specified black-box quantization module; (2) we release
    LLMTools, a user-friendly Python library that features an implementation of ModuLoRA and
    that enables users to easily finetune the largest LLMs on consumer GPUs; (3) we
    provide empirical evidence that high performance on downstream tasks can be achieved
    with a smaller LLM than previously thought.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are interested in finetuning a pre-trained LLM for downstream tasks (Li &
    Liang, [2021](#bib.bib19); Lester et al., [2021](#bib.bib18); Houlsby et al.,
    [2019](#bib.bib14); Rebuffi et al., [2017](#bib.bib30)). LLMs use a transformer
    architecture where almost all of the learnable weights—and almost all of the memory
    used to store these weights—appear in linear layers.¹¹1These layers include the
    $K$ in full precision.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Large Language Model Finetuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because of the high memory requirements needed to fine-tune and store all the
    weights of a LLM, practitioners have developed a variety of *parameter-efficient
    fine tuning* methods that learn in a lower dimensional space. These methods include
    tuning only the output layer (Devlin et al., [2018](#bib.bib9)) and tuning the
    prompt or prefix passed as input to an LLM (Lester et al., [2021](#bib.bib18);
    Li & Liang, [2021](#bib.bib19); Liu et al., [2023a](#bib.bib22); [b](#bib.bib24)),
    as well as LoRA, which is the focus of this work.
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Adaptation (LoRA)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The LoRA algorithm (Hu et al., [2022](#bib.bib15)) decomposes the weights $\mathbf{W}$;
    the rectangular case is a straightforward generalization.:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{W}=\mathbf{W}_{0}+\mathbf{A}\mathbf{B}^{\top}.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: LoRA reduces the number of trained parameters by a factor of $2r/d$ in memory,
    which requires multiple high-end GPUs and precludes tuning large LLMs on commodity
    hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Low-Precision Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The computational requirements of modern machine learning models motivate a
    wide range of efficient machine learning algorithms (Li & Liang, [2021](#bib.bib19);
    Hu et al., [2022](#bib.bib15); Frantar et al., [2023](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Quantization methods for neural networks reduce the number of bits required
    to store model weights (Dong et al., [2019](#bib.bib10); [2020](#bib.bib11); Yao
    et al., [2022](#bib.bib39); Park et al., [2023](#bib.bib27)). A $b$-bit quantization
    method has the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle(\hat{\mathbf{W}}_{q},\mathbf{z},\mathbf{s})=\mathcal{Q}(\mathbf{W})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Here, the quantization algorithm $\mathcal{Q}$ are extended with numpy-style
    broadcasting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Frantar et al. ([2023](#bib.bib12)) proposed OPTQ, a quantization
    algorithm that scales to modern LLMs. The method iteretiavely runs two steps over
    the weight columns: (1) quantize with nearest rounding and compute the error,
    (2) update the remaining weights with a scaled error. Many of our experiments
    finetune LLMs quantized with OPTQ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In concurrent work, Dettmers et al. ([2023](#bib.bib8)) proposed QLoRA, an
    approach for tuning quantized LLMs based on LoRA. While our work seeks to integrate
    with any user-defined quantization module (such as OPTQ), QLoRA defines its own
    quantization scheme, which is simpler than, say, OPTQ. One advantage of our approach
    is support for 3-bit finetuning (and potentially 2-bit via new quantizers; Chee
    et al. ([2023](#bib.bib4))); QLoRA only supports 4-bit finetuning. We will also
    identify settings where using advanced quantizers yields performance gains over
    QLoRA. See Section [5.1](#S5.SS1 "5.1 Comparison to Related Work ‣ 5 Discussion
    ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular
    Quantizers") for details.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Low-Precision Low-Rank Adaptation with a Modular Quantizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe modular low-rank adaptation (ModuLoRA), a memory-efficient
    finetuning algorithm for large language models (LLMs) that leverages custom quantization
    algorithms and runs on consumer GPU hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '\par\parclass  ModuLoRALinear(Module):"""Linear  ModuLoRA  Layer"""\pardef  __init__(self,  …):self.hatWq_z_s  =  quantize(pretrained_W)(self.A,  self.B)  =  lora_init(…)\pardef  forward(self,  x):(hatWq,  z,  s)  =  self.hatWq_z_sreturn  LPLinear.apply(x,  hatWq,  z,  s)  \  +  (x  @  self.B)  @  self.A.t()  +  self.bias\par\par\parclass  LPLinear(Function):"""Low-Precision  Linear  Map"""@staticmethoddef  forward(ctx,  input,  hatWq,  z,  s):ctx.save_for_backward(hatWq,  z,  s)hatW  =  dequantize(hatWq,  z,  s)output  =  input  @  hatW.t()return  output  #  hatW  is  deallocated@staticmethoddef  backward(ctx,  grad_output):hatWq,  z,  s  =  ctx.saved_tensors#  we  recompute  hatWhatW  =  dequantize(hatWq,  z,  s)grad_input  =  grad_output  @  hatW#  here  hatW  can  be  deallocatedreturn  grad_input,  None,  None,  None\par'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: PyTorch pseudocode for ModuLoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Low-Rank Adaptation of Low-Precision Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step of our approach is quantization: we apply a black-box quantization
    algorithm $\mathcal{Q}$ as part of ModuLoRA—rather, we seek to support user-defined
    quantizers that are treated by our method is a black-box.'
  prefs: []
  type: TYPE_NORMAL
- en: The core of our efforts focuses on finetuning the base quantized model. Our
    method first modifies the network by replacing each linear layer—originally defined
    by the affine map $x\mapsto x(\mathbf{W}^{(i)})^{\top}+\mathbf{b}^{(i)}$—with
    the reparameterized low precision `ModuLoRALinear} layer in Figure \ref`fig:lplora_code,
    given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x\mapsto x(\hat{\mathbf{W}}^{(i)})^{\top}+x\mathbf{B}^{(i)}(\mathbf{A}^{(i)})^{\top}+\mathbf{b}^{(i)}.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Here $\mathbf{A}^{(i)},\mathbf{B}^{(i)}\in\mathbb{R}^{d\times r}$’s.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 The Structure of a Quantized Backward Pass
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We illustrate the technical challenges that arise in the design of a quantized
    backward pass in the context of a network of $n$ `ModuLoRALinear} layers. Each
    \mintinline`pythonModuLoRALinear is effectively a fully connected layer with reparameterized
    dense weights defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{W}^{(i)}_{l}=\hat{\mathbf{W}}^{(i)}+\mathbf{A}^{(i)}(\mathbf{B}^{(i)})^{\top},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: biases $\mathbf{b}^{(i)}$, where we overload the Leibniz notation for derivatives
    to also denote gradients. By the chain rule,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\mathrm{d}L}{\mathrm{d}\mathbf{A}^{(i)}}=\frac{\mathrm{d}L}{\mathrm{d}\bar{\mathbf{y}}_{i}}\cdot\frac{\mathrm{d}\bar{\mathbf{y}}_{i}}{\mathrm{d}\mathbf{A}^{(i)}}.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Because of the additive structure of the weights $\mathbf{W}^{(i)}_{l}$. The
    second term can be computed via the chain rule of calculus as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where ${\mathrm{d}\mathbf{y}_{i}}/{\mathrm{d}\bar{\mathbf{y}}_{i}}$. Performing
    this multiplication in a stable and efficient way is a challenge that we must
    address.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Efficient Mixed-Precision Computation of Forward and Backward Passes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we could precompute all dequantized weight matrices $(\hat{\mathbf{W}}^{(i)})^{\top}$
    from the forward pass to use them in the backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Mixed Precision Computation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our strategy is to *recompute* the high-precision materialization $\hat{\mathbf{W}}^{(i)}$
    and performs multiplication. Similarly, `backward} re-dequantizes $\tbWi$ and
    computes the gradient derived in Appendix \ref`sec:bwd_pass via dynamic programming.
    The
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'pythonLPLoRA module is small: all the intermediates are either the same size
    as the input $x$'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 LLMTools: A Library for Efficient LLM Finetuning Using ModuLoRA.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We implement ModuLoRA as part of LLMTools, a user friendly library that enables
    users to interact with the largest LLMs on consumer hardware. The LLMTools library
    enables finetuning LLMs in 3-bit and 4-bit precision using the ModuLoRA algorithm.
    It also provides an easy-to-use Python API for quantization, inference, and finetuning,
    as well as modular support for multiple quantizers, LLMs (including LLAMA1, LLAMA2,
    BLOOM, and OPT), and optimization algorithms (including all that are compatible
    with the Hugging Face Trainer class). Lastly, LLMTools supports easily loading
    datasets and sharing models via the HuggingFace Hub. Our code is available at:
    [https://github.com/kuleshov-group/llmtools](https://github.com/kuleshov-group/llmtools);
    our evaluation code to reproduce our results is available at: [https://github.com/kuleshov-group/MODULoRA-Experiment](https://github.com/kuleshov-group/MODULoRA-Experiment).
    A key quantization algorithm implemented in LLMTools is OPTQ (Frantar et al.,
    [2023](#bib.bib12)). In order to integrate OPTQ with LoRA-based finetuning, LLMTools provides
    efficient CUDA implementations of mixed-precision matrix-vector multiplication,
    including row and weight materialization. We provide CUDA kernels for both row
    and weight materialization in both the forward and backward passes. For maximum
    efficiency, we materialize elements of $\hat{\mathbf{W}}^{(i)}_{q}$ all stored
    as float16.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Models.   We evaluate ModuLoRA and LLMTools on the recent LLAMA (Touvron et al.,
    [2023](#bib.bib35)) family of models, as well as open-source BLOOM (Scao et al.,
    [2023](#bib.bib31)) and OPT models (Zhang et al., [2022](#bib.bib42)). We quantize
    the models to 3 bits and 4 bits using OPTQ as in Frantar et al. ([2023](#bib.bib12))
    with calibration 128 samples from C4 (Raffel et al., [2020](#bib.bib29)). Baseline.  
    We use LoRA (as implemented in the PEFT library (Mangrulkar et al., [2022](#bib.bib25)))
    to finetune models quantized in 8 bits using the BitsAndBytes library (Dettmers
    et al., [2022](#bib.bib7)); we also compare to full-precision results from the
    literature. In concurrent work, Dettmers et al. ([2023](#bib.bib8)) proposed QLoRA,
    a related 4-bit finetuning algorithm implemented in the BitsAndBytes library.
    Accordingly, we present an experimental comparison of QLoRA with our approach,
    along with an in-depth discussion. Training.   We finetune all models for $3$,
    and report results from 3 random seeds. See Appendix [D](#A4 "Appendix D Hyperparamters
    Used in Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating
    with Modular Quantizers") for details on the hyperparameters used for each of
    our experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Text Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data & Metrics.   We start with a simple text classification task where we seek
    to classify a short text snippet (up to 50 words) into its genre (e.g., fiction,
    telephone chat, etc.). We finetune 13B to 65B LLAMA models on 392,702 snippets
    from five genres and evaluate on 9,815 held out instances (Williams et al., [2018](#bib.bib36)),
    reporting accuracy. This yields a challenging classification task for LLMs of
    all sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '| LLAMA Tuning | 13B | 30B | 65B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLMTools (3-bit) | 93.5 $\pm$ 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LLMTools (4-bit) | 92.9 $\pm$ 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Bits&Bytes 8-bit (LLM.int8()) | 93.0 $\pm$ 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Text classification accuracy (%) for LLAMAs finetuned with LoRA &
    ModuLoRA in 3, 4, 8 bits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results.   We observe that classification accuracy consistently improves as
    we increase the number of parameters of the LLM. ModuLoRA combined with a 3-bit
    or a 4-bit LLM offers comparable performance to 8-bit finetuning in Bits&Bytes
    while using significantly less memory (Table [1](#S4.T1 "Table 1 ‣ 4.2 Text Classification
    ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating
    with Modular Quantizers")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Natural Language Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data & Metrics.   Next, we finetune LLMs on natural language inference tasks.
    The model is asked to predict a label from a small set (entailment, contradiction,
    or neutral) after being presented with a sentence pairing (a hypothesis and premise
    sentence pair). We finetune 7B to 65B LLAMA models on the Multi-Genre Natural
    Language Inference Corpus (MNLI)  (Williams et al., [2018](#bib.bib36)) and evaluate
    on the matched test sets (in-domain examples), reporting accuracy. Baselines from
    GPT-3 and T5 are included, as presented in Hu et al. (2022) and  Chung et al.
    ([2022](#bib.bib6)). Results.   Our 3-bit 65B LLAMA model matches the performance
    of a full-precision GPT-3+LoRA baseline. We also find that 3-bit and 4-bit models
    from LLMTools outperform 8-bit models from the Bits&Bytes library for the entire
    model size range. Both 3-bit and 4-bit ModuLoRA models either match or outperform
    their 4-bit QLoRA counterparts, often using less memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines Models Finetuning Adaptation Model Size # Trainable Parameters MNLI-m
    (accuracy) GPT-3 Full Finetuning 175B 175,255.8M 89.5 $\pm$ 0.1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Natural language inference on the MNLI-m dataset evaluated using classification
    accuracy (%). Our LLAMA-65B-3bit model approaches state-of-the-art scores using
    significantly less memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Abstractive Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data & Metrics.   We finetune 7B-65B LLAMA and 7B-13B OPT models on the SAMSum
    dataset (Gliwa et al., [2019](#bib.bib13)), consisting of 14,732 (text, summary)
    training pairs and 819 test pairs. Our methodology fully mirrors the evaluation
    of GPT-style models finetuned using LoRA (Hu et al., [2022](#bib.bib15)). We evaluate
    summarization quality using ROUGE-1/2/L; we include GPT-3 baselines from Hu et al.
    ([2022](#bib.bib15)). Results.   Our 4-bit 65B LLAMA models finetuned with ModuLoRA outperform
    the GPT-3 baseline and even reach new state-of-the-art performance on this dataset
    (Table [3](#S4.T3 "Table 3 ‣ 4.4 Abstractive Summarization ‣ 4 Experiments ‣ ModuLoRA:
    Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")).
    Importantly, ModuLoRA demonstrates performance improvements over the 4-bit QLoRA
    and the 8-bit BitsAndBytes methods. In the 7B to 65B model size range, ModuLoRA models
    (3-bit or 4-bit) outperform 8-bit LoRAs in BitsAndBytes and LLM.int8() and 4-bit
    LoRAs in BitsAndBytes and QLoRA. We argue that a data-driven lower precision quantization
    scheme can improve over a higher precision zero-shot quantizer like LLM.int8().
    Switching from 4-bit to 3-bit precision within ModuLoRA reduces ROUGE by only
    about 1%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines Models Finetuning Adaptation # Trainable Parameters SAMSum (Rouge
    1/2/L) GPT-3 Full Finetuning 175,255.8M 52.0 / 28.0 / 44.5 GPT-3 Adapter 40.1M
    53.2 / 29.0 / 45.1 GPT-3 LoRA 4.7M 53.8 / 29.8 / 45.9 Pegasus SliC 2B 54.4 / 29.9
    / 45.9'
  prefs: []
  type: TYPE_NORMAL
- en: LLAMA Finetuning 7B 13B 30B 65B LLMTools (3-bit) 51.2 / 28.2 / 44.0 52.4 / 29.6
    / 45.1 53.6 / 30.8 / 46.3 54.1 / 30.9 / 46.5 LLMTools (4-bit) 51.7 / 28.3 / 44.4
    53.2 / 30.2 / 46.1 53.9 / 31.2 / 46.9 55.9 / 32.7 / 49.0 Bits&Bytes 4-bit (QLoRA)
    51.6 / 28.3 / 44.5 51.3 / 28.1 / 44.1 53.0 / 30.2 / 45.7 53.8 / 30.5 / 45.9 Bits&Bytes 8-bit
    (LLM.int8()) 51.9 / 28.1 / 44.5 51.3 / 28.2 / 43.6 50.8 / 28.4 / 44.1 53.9 / 30.4
    / 46.3
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Abstractive summarization on the SAMSum dataset evaluated using ROUGE
    1/2/L. Our LLAMA-65B-3bit model obtains state-of-the-art ROUGE 1/2 scores. All
    metrics have $\pm 0.5$ confidence intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: Round-to-Nearest Quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We also perform an ablation where we replace the OPTQ quantizer with a rount-to-nearest
    (RTN) approach (Table [4](#S4.T4 "Table 4 ‣ Other Model Families ‣ 4.4 Abstractive
    Summarization ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs
    by Integrating with Modular Quantizers")); OPTQ performs better than RTN, highlighting
    the importance of advanced quantizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Model Families
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We also apply LLMTools to the OPT (Zhang et al., [2022](#bib.bib42)) families
    of models (Table [5](#S4.T5 "Table 5 ‣ Other Model Families ‣ 4.4 Abstractive
    Summarization ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs
    by Integrating with Modular Quantizers")). Although these models perform worse
    than LLAMA, ModuLoRA matches or outperforms more memory-intensive 4-bit and 8-bit
    finetuning, which is consistent with our results on LLAMA.'
  prefs: []
  type: TYPE_NORMAL
- en: SAMSum Performance Quantizer 7B 13B LLMTools (3-bit) OPTQ 51.2 / 28.2 / 44.0 / 44.2
    52.4 / 29.6 / 45.1 / 45.1 RTN 50.7 / 27.2 / 43.6 / 43.6 51.1 / 28.7 / 44.3 / 44.5
    LLMTools (4-bit) OPTQ 51.7 / 28.3 / 44.4 / 44.4 53.2 / 30.2 / 46.1 / 46.1 RTN
    51.2 / 28.5 / 44.2 / 44.2 52.5 / 29.9 / 45.5 / 45.5
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: OPTQ and RTN quantization with different LLaMA model sizes on the
    SAMSum dataset. The evaluation was done on ROUGE 1/2/L/LSum.'
  prefs: []
  type: TYPE_NORMAL
- en: OPT Finetuning 13B 30B LLMTools (3-bit) 48.8 / 26.7 / 41.9 49.9 / 27.1 / 42.5
    LLMTools (4-bit) 49.3 / 26.8 / 42.0 49.6 / 27.1 / 42.4 Bits&Bytes 4-bit (QLoRA)
    49.2 / 27.0 / 42.1 49.9 / 27.0 / 42.5 Bits&Bytes 8-bit (LLM.int8()) 48.8 / 26.5
    / 41.7 49.3 / 27.1 / 42.3
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Abstractive summarization with OPT models on the SAMSum dataset. ModuLoRA in
    3-bit and 4-bit precision matches ROUGE 1/2/L scores of 4-bit and 8-bit baselines.
    All metrics have $\pm 0.5$ confidence intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Instruction Following
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data & Metrics.   We finetune 7B-65B LLAMA models on the Alpaca dataset (Taori
    et al., [2023](#bib.bib34)), consisting 52,000 instructions, as well on the CodaAlpaca
    dataset (Chaudhary, [2023](#bib.bib3)), consisting of 20K code generation instructions
    (ses [8](#A3.T8 "In C.1 Additional Experiments on Code-Alpaca with LLaMA ‣ Appendix
    C Additional Empirical Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer
    GPUs by Integrating with Modular Quantizers")). We evaluate our Alpaca instruction-tuned
    models on the BigBenchHard (BBH) benchmark (Suzgun et al., [2022](#bib.bib33)),
    consisting of 23 challenging tasks on which LLMs do not exceed human performance.
    We evaluate 3-shot performance via "answer-only" prompting and use exact match
    accuracy as our measurement standard, testing on 6,511 samples ($\sim$ 1.5k tokens
    each). We include Flan and LLAMA baselines from Chia et al. ([2023](#bib.bib5)).Results.  
    We find that 3-bit and 4-bit performance drops only slightly relative to 8-bit
    and 16-bit. Crucially, 4-bit and 3-bit 65B models outperform 8-bit Beand 16-bit
    30B models, despite using fewer total bits. Furthermore, 4-bit ModuLoRA compares
    well to 4-bit QLoRA, and provides consistent performance improvements, especially
    at smaller model sizes, where sophisticated quantization ought to provide greater
    benefits. This further highlights the benefits of one-shot quantization methods.
    Appendix [C](#A3 "Appendix C Additional Empirical Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers") also reports
    experiments on the CodeAlpaca dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines Model Method BASE (250M) L (780M) XL (3B) XXL (11B) FLAN-T5 No Finetuning
    30.8 30.3 39.9 47.4
  prefs: []
  type: TYPE_NORMAL
- en: Model Methods 7B 13B 30B 65B LLAMA LLMTools (3-bit) 31.1 $\pm$ 0.4 No Finetuning
    30.9 37.1 39.3 42.6
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Instruction-tuned models evaluated on BigBench Hard (BBH). We finetune
    LLAMA models on the Alpaca dataset in 3 to 16 bits. We provide exact standard
    deviation here.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Comparison to Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comparison to QLoRA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In concurrent work, Dettmers et al. ([2023](#bib.bib8)) proposed QLoRA, a related
    approach for finetuning a quantized LLM. We highlight methodological and experimental
    differences below. From a methods perspective, ModuLoRA integrates with a user-specified
    black-box quantization module. In our experiments, we find that using a sophisticated
    data-driven quantizer like OPTQ improves performance over simpler zero-shot strategies,
    e.g., a round-to-nearest baseline. Unlike ModuLoRA, QLoRA defines a quantization
    approach similar to RTN, but also introduces a specialized packing routine, quantization
    of zeros and scales, and other innovations. From an experiments and capabilities
    perspective, integrating with OPTQ enables ModuLoRA to fintune models quantized
    in 3-bits, which QLoRA cannot do. We anticipate ModuLoRA will enable finetuning
    2-bit LLMs by integrating with new quantizers, such as Chee et al. ([2023](#bib.bib4)).
    Lastly, we identify settings where ModuLoRA yields LLMs with better performance
    than LLMs from QLoRA; this gap is likely due to the use of improved quantizers.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison to Other Parameter-Efficient Finetuning Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recent Parameter-Efficient Finetuning (PEFT) methods have encompassed a range
    of techniques such as prompt tuning (Lester et al., [2021](#bib.bib18); Li & Liang,
    [2021](#bib.bib19); Qin & Eisner, [2021](#bib.bib28); Liu et al., [2022b](#bib.bib23)),
    modification of the embedding layer inputs (An et al., [2022](#bib.bib1)) or hidden
    states (Liu et al., [2022a](#bib.bib21)), inclusion of full layers (Houlsby et al.,
    [2019](#bib.bib14)), only tuning biases (Zaken et al., [2021](#bib.bib41)), and
    others (Sung et al., [2021](#bib.bib32); Karimi Mahabadi et al., [2021](#bib.bib17)).
    An important shortcoming of these methods is the need to store in memory a significant
    amount of frozen base model parameters. This limits their ability to finetune
    the largest LLMs on consumer GPU, a limitation that we address.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Running LLMs on Consumer GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Efficient LLM Algorithms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The computational requirements of modern deep neural networks motivate a wide
    range of efficient machine learning algorithms. Quantization methods reduce the
    number of bits required to store weights (Dong et al., [2019](#bib.bib10); [2020](#bib.bib11);
    Hubara et al., [2021](#bib.bib16); Li et al., [2021](#bib.bib20); Yao et al.,
    [2021](#bib.bib38)), including via adaptive methods (Nagel et al., [2020](#bib.bib26)).
    SmoothQuant (Xiao et al., [2023](#bib.bib37)) rescales between activations and
    weights to remove outliers from the activations and make quantization overall
    easier. ZeroQuant (Yao et al., [2022](#bib.bib39)) proposes a per-layer knowledge
    distillation method. LLM.int8() (Dettmers et al., [2022](#bib.bib7)) decompose
    matrix multiplications into a majority of 8 bit and a minority of 16 bit operations.
    LUT-GEMM (Park et al., [2023](#bib.bib27)) designs kernels to accelerate quantized
    matrix multiplications. RPTQ (Yuan et al., [2023](#bib.bib40)) reorders activations
    and quantizes them in groups, reducing the impact of range differences between
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: Running LLMs on Consumer GPUs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our 3-bit and 4-bit methods enable finetuning a 65B LLM on on 48GB GPU and a
    30B LLM on on 24GB GPU, bringing LLM finetuning to consumer hardware. Moreover,
    fitting an entire LLM on GPU unlocks data parallelism, which is more efficient
    than model parallelism. Previous 8-bit quantization methods required a 96GB GPU
    to fully fit a 65B model. Finetuning GPUs on consumer hardware holds promise to
    accelerate model iteration and apply LLMs to a wider range of domains by a larger
    number of practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 What is a Good Base LLM for Finetuning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Models | Quantization | BBH | PPL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA (13B) | 3-bit | 35.3 | 6.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 4-bit | 36.2 | 5.36 |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA (65B) | 3-bit | 43.3 | 5.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 4-bit | 43.7 | 3.84 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: BBH vs. PPL'
  prefs: []
  type: TYPE_NORMAL
- en: 'The traditional measure of a base LLM is perplexity. In the adjacent table,
    we report LLAMA perplexity (PPL) on Wiki2 as well as finetuning performance on
    BBH. Interestingly, the correlation is not perfect: large gaps in PPL admit small
    gaps in BBH. This questions LLM evaluation when the goal is finetuning, and suggests
    exploring new training strategies. More generally, our results provide empirical
    evidence that high performance on downstream tasks can be achieved with a smaller
    quantized LLM than previously thought. While existing methods (e.g., LLM.int8()+LoRA;
    Dettmers et al. ([2022](#bib.bib7))) operate in 8 bits, we find that 3-bit or
    4-bit finetuning yields the best results for a fixed bit budget. For example,
    we find that 4-bit and 3-bit 65B models outperform 8-bit and 16-bit 30B models
    on instruction following tasks. Similarly, we find that 3-bit models are able
    to attain a new state-of-the-art ROUGE score on the SamSum summarization task.
    The high performance of these models hints at the possibility of further pursuing
    2-bit models.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An advantage of LoRA is that it has low inference overhead, since the low-rank
    adaptor can be added in to the full-precision weight matrix when deploying. One
    limitation of ModuLoRA is that it does not share this advantage relative to the
    black-box quantized model: the low-rank adaptor cannot be trivially added to the
    weight matrix because the weight matrix is quantized while the adaptor is not.
    So, the weight matrix and adaptor cannot be fused readily, and an implementation
    as in Figure [1](#S3.F1 "Figure 1 ‣ 3 Low-Precision Low-Rank Adaptation with a
    Modular Quantizer ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating
    with Modular Quantizers") is required at inference time. A second limitation of
    ModuLoRA is that making finetuning possible on widely available commodity hardware
    may make finetuning too easy, presenting potential problems related to LLM safety.
    Another limitation of ModuLoRA  is that the largest models in use today (e.g.
    GPT-4) can have up to 1 trillion parameters, and even at the minimum of 1 bit
    per parameter this still would take up 125 GB, which exceeds memory on commodity
    GPUs: thus a straightforward application of ModuLoRA will be unable to make these
    largest-scale models finetunable on commodity hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finetuning large language models typically requires substantial hardware and
    storage resources. Our method, ModuLoRA, enables 3-bit finetuning of 65B models
    on a single 48GB consumer GPU. At the core of our approach is a simple, quantization-agnostic
    backward pass that enables integrating low-rank adapters with frozen LLM weights
    obtained from a user-defined quantization module. By integrating with modern quantizers,
    ModuLoRA achieves state-of-the-art performance compared to both parameter-efficient
    and full fine-tuning techniques. We anticipate MODULORA will enable finetuning
    2-bit LLMs by integrating with new quantizers, such as Chee et al. ([2023](#bib.bib4)).
    ModuLoRA’s flexibility and competitive performance make fine-tuning more accessible
    and cost-effective in a resource-constrained setting. This assists open-source
    model development and facilitates scientific research. More broadly, we believe
    that ModuLoRA will help democratize access to large language models and make them
    available to a broader audience.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An et al. (2022) Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang
    Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-tuning: Adapting unfamiliar
    inputs to frozen pretrained models. *arXiv preprint arXiv:2203.03131*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, and et. al. Language models are few-shot learners. In *Conference on Neural
    Information Processing Systems*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhary (2023) Sahil Chaudhary. Code alpaca: An instruction-following llama
    model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chee et al. (2023) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De
    Sa. Quip: 2-bit quantization of large language models with guarantees, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chia et al. (2023) Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria.
    Instructeval: Towards holistic evaluation of instruction-tuned large language
    models. *arXiv preprint arXiv:2306.04757*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8(): 8-bit matrix multiplication for transformers at scale. In *Conference
    on Neural Information Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2019) Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney,
    and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision.
    In *International Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2020) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W.
    Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization
    of neural networks. In *Conference on Neural Information Processing Systems*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Optq: Accurate quantization for generative pre-trained transformers.
    In *International Conference on Learning Representations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander
    Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.
    In *Proceedings of the 2nd Workshop on New Frontiers in Summarization*, pp.  70–79,
    Hong Kong, China, November 2019\. Association for Computational Linguistics. doi:
    10.18653/v1/D19-5409. URL [https://aclanthology.org/D19-5409](https://aclanthology.org/D19-5409).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International Conference
    on Machine Learning*, pp.  2790–2799\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language
    models. In *International Conference on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2021) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In *International Conference on Machine Learning*. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karimi Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, and
    Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. *Advances
    in Neural Information Processing Systems*, 34:1022–1035, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pp.  3045–3059, Online and
    Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pp.  4582–4597,
    Online, August 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353.
    URL [https://aclanthology.org/2021.acl-long.353](https://aclanthology.org/2021.acl-long.353).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. In *International Conference on Learning Representations*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022a) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning
    is better and cheaper than in-context learning. *Advances in Neural Information
    Processing Systems*, 35:1950–1965, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning
    across scales and tasks. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers)*, pp.  61–68, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. Gpt understands, too. *AI Open*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, and Sayak Paul. Peft: State-of-the-art parameter-efficient fine-tuning
    methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pp.  7197–7206\.
    PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    Lut-gemm: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models. *arXiv preprint arXiv:2206.09557*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin & Eisner (2021) Guanghui Qin and Jason Eisner. Learning how to ask: Querying
    lms with mixtures of soft prompts. *arXiv preprint arXiv:2104.06599*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
    Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation
    learning. In *Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition*, pp.  2001–2010, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2023) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sung et al. (2021) Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural
    networks with fixed sparse masks. *Advances in Neural Information Processing Systems*,
    34:24193–24205, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve
    them. *arXiv preprint arXiv:2210.09261*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. A
    broad-coverage challenge corpus for sentence understanding through inference.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*,
    pp.  1112–1122\. Association for Computational Linguistics, 2018. URL [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2021) Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali
    Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, and Kurt
    Keutzer. Hawq-v3: Dyadic neural network quantization. In *International Conference
    on Machine Learning*. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In *Conference on Neural Information
    Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Luzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaken et al. (2021) Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:
    Simple parameter-efficient fine-tuning for transformer-based masked language-models.
    *arXiv preprint arXiv:2106.10199*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained
    transformer language models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Additional Method Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Low-Rank Adaptation in Low Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a pre-trained network, the first step of ModuLoRA is to apply a black-box
    quantization $\mathcal{Q}$, with the reparameterized LP-LoRA layer given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x\mapsto\hat{\mathbf{W}}^{(i)}x+\mathbf{A}^{(i)}((\mathbf{B}^{(i)})^{\top}x)+\mathbf{b}^{(i)}.$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Here $\mathbf{A}^{(i)},\mathbf{B}^{(i)}\in\mathbb{R}^{d\times r}$. This enables
    finetuning LLMs on consumer-grade GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '\parclass  LPLinear(Function):@staticmethoddef  forward(ctx,  input,  hatWq,  z,  s):ctx.save_for_backward(hatWq,  z,  s)hatW  =  dequantize(hatWq,  z,  s)  #  allocates  for  hatWoutput  =  input  @  hatW.t()#  here  hatW  goes  out  of  scope  and  can  be  deallocatedreturn  output\par@staticmethoddef  backward(ctx,  grad_output):hatWq,  z,  s  =  ctx.saved_tensorshatW  =  dequantize(hatWq,  z,  s)  #  recompute  hatWgrad_input  =  grad_output  @  hatW#  here  hatW  goes  out  of  scope  and  can  be  deallocatedreturn  grad_input,  None,  None,  None\par\par\parclass  LPLoRA(Module):def  __init__(self,  pretrained_linear_layer,  rank):super().__init__()(hatWq,  z,  s)  =  quantize(pretrained_linear_layer.weight)self.bias  =  pretrained_linear_layer.bias…(A,  B)  =  lora_init(…,  rank)self.A,  self.B  =  (Parameter(A),  Parameter(B))\pardef  forward(self,  x):return  LPLinear.apply(x,  self.hatWq,  self.z,  self.s)  \  +  (x  @  self.B)  @  self.A.t()  +  self.bias'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: PyTorch pseudocode for ModuLoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Efficient Mixed-Precision Computation of Forward and Backward Passes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first introduce our approach in the context of a network with $n$ to denote
    the loss.
  prefs: []
  type: TYPE_NORMAL
- en: The Structure of a Quantized Backward Pass
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The backward pass seeks to compute derivatives $\mathrm{d}L/\mathrm{d}\mathbf{A}^{(i)}$,
    where we overload standard Leibniz notation to denote gradients. By the chain
    rule,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\mathrm{d}L}{\mathrm{d}\mathbf{A}^{(i)}}=\frac{\mathrm{d}L}{\mathrm{d}\bar{\mathbf{y}}_{i}}\cdot\frac{\mathrm{d}\bar{\mathbf{y}}_{i}}{\mathrm{d}\mathbf{A}^{(i)}}.$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'Because of the additive form of ([3](#S3.E3 "In 3.1 Low-Rank Adaptation of
    Low-Precision Models ‣ 3 Low-Precision Low-Rank Adaptation with a Modular Quantizer
    ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular
    Quantizers")), $\mathrm{d}\mathbf{y}_{i}/\mathrm{d}\mathbf{A}^{(i)}$. The second
    term can be computed via'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where ${\mathrm{d}\mathbf{y}_{i}}/{\mathrm{d}\bar{\mathbf{y}}_{i}}$ with high-precision
    vectors in the forward and backward passes.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.1 Mixed-Precision Matrix-Vector Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Importantly, we do not need to maintain $\hat{\mathbf{W}}^{(i)}$ may not even
    be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Configurations for BBH Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate the BBH dataset using LoRA adapter weights from huggingface hub
    with different configurations. For the Bits&Bytes 8-bit (LLM.int8()) LoRA adapter
    weights, we utilized two sources: the Alpaca-7B one is obtained from the ’tloen/alpaca-lora-7b’
    repository, while the weights for Alpaca-13b and 30b were sourced from ’chansung/alpaca-lora-xxb’.
    In the case of Bits&Bytes 4-bit (QLoRA) adapter weights, all configurations (Alpaca-7B,
    13B, and 30B)—were uniformly accessed from ’timdettmers/qlora-alpaca-xxb’. Note
    that for the Bits&Bytes 4-bit (QLoRA) and Bits&Bytes 8-bit (LLM.int8()) adapter
    wights of the 65B model, we obtain them by finetuning the base 65B LLaMa model
    on Alpaca dataset using the same set of hyperparameters as ours.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Additional Empirical Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conducted additional experiment on Code-Alpaca ((Chaudhary, [2023](#bib.bib3))).
    The result is shown in [8](#A3.T8 "In C.1 Additional Experiments on Code-Alpaca
    with LLaMA ‣ Appendix C Additional Empirical Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers"). Consistent
    with our hypothesis, ModuLoRA performs better than or at least on par with the
    higher precision 8-bit models given the same number of trainable parameters and
    set up.'
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Additional Experiments on Code-Alpaca with LLaMA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Code Alpaca Performance | 7B | 13B | 30B | 65B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLMTools (3-bit) | 53.6 / 36.3 / 50.7 | 57.0 / 40.0 / 53.3 | 58.1 / 40.7 / 54.3
    | 60.0 / 44.1 / 58.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LLMTools (4-bit) | 54.6 / 37.2 / 51.4 | 57.4 / 40.6 / 54.3 | 59.0 / 41.4 / 57.5
    | 60.2 / 43.5 / 56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Bits&Bytes 8-bit (LLM.int8()) | 54.0 / 36.3 / 50.9 | 57.7 / 41.3 / 54.9 |
    60.6 / 43.5 / 57.5 | 61.1 / 44.1 / 58.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Instruction-tuned models evaluated using ROUGE 1/2/LSum on Code Alpaca
    in 3, 4, and 8 bits.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Hyperparamters Used in Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 LLaMA / OPT on SAMSum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We set up the training procedure following Hu et al. ([2022](#bib.bib15)),
    with slight variation to accomodate our particular language models. For a fair
    comparison with the concurrent work by Dettmers et al. ([2023](#bib.bib8)), we
    use the exact same hyperparameter set up as shown in [9](#A4.T9 "In D.1 LLaMA
    / OPT on SAMSum ‣ Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers") . We train
    using AdamW for 350 steps with a batch size of 128 samples. We report the results
    over 3 random seeds; the result for each run is taken from the training steps
    with the lowest validation loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Model LLaMA 7B / 13B / 30B / 65B OPT 7B/ 13B / 30B SAMSum Optimizer
    AdamW Warmup Ratio 0.06 Batch size 128 Evaluation Batch size 16 Evaluation Steps
    50 Total # Training Steps 350 Learning Rate Schedule Cosine Learning Rate 1e-3
    WeightDecay 0.0 LoRAConfig $r_{q}=r_{v}=8$ 32 Max Seq. Len 250'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Hyperparamters configuration for ModuLoRA, Q-LoRA on SAMSum'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 LLaMA on Code-Alpaca & Text-Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We again train using AdamW optimizer with a warmup ratio of 0.06\. We tune learning
    rate, batch size, training steps for each task. We report the results over 3 random
    seeds. The result for each run is taken from the training steps that yield the
    lowest validation loss.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | LLaMA Model | 13/30/65 B |'
  prefs: []
  type: TYPE_TB
- en: '| Text- Classification | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '| Warmup Ratio | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Batch size | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Steps | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Total # Training Steps | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate Schedule | Cosine |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate | 1e-3 |'
  prefs: []
  type: TYPE_TB
- en: '| WeightDecay | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LoRAConfig | $r_{q}=r_{v}=8$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LoRA $\alpha$ | 32 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Max Seq. Len | 128 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Hyperparamters configuration for ModuLoRA, Q-LoRA on Text-Classification'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | LLaMA Model | 7/13/30/65 B |'
  prefs: []
  type: TYPE_TB
- en: '| Code- Alpaca | Optimizer | AdamW |'
  prefs: []
  type: TYPE_TB
- en: '| Warmup Ratio | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 128 |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Batch size | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation Steps | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Total # Training Steps | 120 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate Schedule | Linear |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Rate | 1e-3 |'
  prefs: []
  type: TYPE_TB
- en: '| WeightDecay | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LoRAConfig | $r_{q}=r_{v}=8$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LoRA $\alpha$ | 32 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Max Seq. Len | 165 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Hyperparamters configuration for ModuLoRA, Q-LoRA on Alpaca-Code'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 LLaMA on MNLI-M
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training is conducted using the AdamW optimizer, with a warmup ratio set at
    0.06\. We tune the learning rate, batch size, and training steps. Results are
    reported over three random seeds, and for each run, the performance metric is
    derived from the training step with the lowest validation loss. See [12](#A4.T12
    "In D.3 LLaMA on MNLI-M ‣ Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA:
    Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")
    for more details on the hyperparameters used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Model LLaMA 7B / 13B / 30B / 65B MNLI-M Optimizer AdamW Warmup Ratio
    0.06 Batch size 128 Evaluation Batch size 64 Evaluation Steps 64 Total # Training
    Epoch 1.0 Learning Rate Schedule Cosine Learning Rate 1e-3 WeightDecay 0.0 LoRAConfig
    $r_{q}=r_{v}=8$ 32 Max Seq. Len 128'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Hyperparamters configuration for ModuLoRA, Q-LoRA on MNLI-M'
  prefs: []
  type: TYPE_NORMAL
- en: D.4 LLaMA on Alpaca for BBH Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training is conducted using the AdamW optimizer, with a warmup ratio set at
    0.06\. We tune the learning rate, batch size, and training steps. Results are
    reported over three random seeds. See [13](#A4.T13 "In D.4 LLaMA on Alpaca for
    BBH Evaluation ‣ Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers") for more
    details on the hyperparameters used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Model LLaMA 7B / 13B / 30B / 65B Alpaca Optimizer AdamW Warmup Ratio
    0.06 Batch size 128 Total # Training Epochs 3 Learning Rate Schedule Linear Learning
    Rate 1e-3 WeightDecay 0.0 LoRAConfig $r_{q}=r_{v}=8$ 16 Max Seq. Len 256'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: Hyperparamters configuration for ModuLoRA on Alpaca'
  prefs: []
  type: TYPE_NORMAL
