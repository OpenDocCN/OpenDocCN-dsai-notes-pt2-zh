- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:49'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14852](https://ar5iv.labs.arxiv.org/html/2405.14852)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vladimir Malinovskii^†
  prefs: []
  type: TYPE_NORMAL
- en: Yandex, HSE University
  prefs: []
  type: TYPE_NORMAL
- en: '&Denis Mazur^†'
  prefs: []
  type: TYPE_NORMAL
- en: MIPT^⋄, Researchcore
  prefs: []
  type: TYPE_NORMAL
- en: '&Ivan Ilin^†'
  prefs: []
  type: TYPE_NORMAL
- en: ​​AI Initiative, KAUST^∗​​
  prefs: []
  type: TYPE_NORMAL
- en: '&Denis Kuznedelev'
  prefs: []
  type: TYPE_NORMAL
- en: Yandex, Skoltech
  prefs: []
  type: TYPE_NORMAL
- en: '&Konstantin Burlachenko'
  prefs: []
  type: TYPE_NORMAL
- en: ​​AI Initiative, KAUST^∗​​
  prefs: []
  type: TYPE_NORMAL
- en: '&Kai Yi'
  prefs: []
  type: TYPE_NORMAL
- en: ​​AI Initiative, KAUST^∗​​
  prefs: []
  type: TYPE_NORMAL
- en: '&Dan Alistarh^‡'
  prefs: []
  type: TYPE_NORMAL
- en: IST Austria, NeuralMagic
  prefs: []
  type: TYPE_NORMAL
- en: '&Peter Richtarik^‡'
  prefs: []
  type: TYPE_NORMAL
- en: ​​AI Initiative, KAUST^∗​​
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There has been significant interest in “extreme” compression of large language
    models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be
    executed efficiently on resource-constrained devices. Existing work focused on
    improved one-shot quantization techniques and weight representations; yet, purely
    post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width
    trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include
    fine-tuning (part of) the compressed parameters over a limited amount of calibration
    data; however, such fine-tuning techniques over compressed weights often make
    exclusive use of *straight-through estimators (STE)*, whose performance is not
    well-understood in this setting. In this work, we question the use of STE for
    extreme LLM compression, showing that it can be sub-optimal, and perform a systematic
    study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning
    — a representation-agnostic framework that generalizes and improves upon existing
    fine-tuning strategies, and provides convergence guarantees in restricted cases.
    On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms
    prior techniques for highly-performant models such as Llama and Mistral. Using
    PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family
    models at 2 bits per parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '^($\dagger$)^($\dagger$)footnotetext: Equal contribution.  $\ddagger$ Moscow
    Institute of Physics and Technology, Russia^*^*footnotetext: King Abdullah University
    of Science and Technology, Saudi Arabia'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent years have seen the development of ever more capable large language models,
    attracting immense interest from both researchers and industry. One of the driving
    factors behind progress in this area is the availability of powerful open LLMs
    such as Llama [[63](#bib.bib63)], Mistral [[31](#bib.bib31), [32](#bib.bib32)],
    or Phi [[38](#bib.bib38)]. The main advantage of open LLMs is that they can be
    run and fine-tuned locally by end users; however, as state-of-the-art LLMs grow
    larger, they also become harder to run on commodity hardware. For instance, in
    order to fit the best available Llama-3 model on a consumer GPU, the model would
    have to be compressed to below 2 bits per parameter¹¹1At the time of writing,
    the best open model (Llama-3 70B) takes up 130GB in FP16, while most consumer
    GPUs have 8-24GiB DRAM, some of which must be reserved for the attention cache..
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve such “extreme” degrees of compression accurately, researchers have
    proposed a variety of techniques, which can be roughly categorized into i) better
    quantized weight representations and ii) better algorithms to learn these representations.
    The weight representations used for extreme quantization include group quantization [[20](#bib.bib20),
    [18](#bib.bib18)], sparse high-precision outliers [[15](#bib.bib15), [29](#bib.bib29)],
    incoherence processing of the weights [[8](#bib.bib8), [64](#bib.bib64)], or additive
    and residual quantization [[19](#bib.bib19), [66](#bib.bib66)]. In turn, the calibration
    algorithms also vary between data-free methods [[18](#bib.bib18)], layer-wise
    calibration [[20](#bib.bib20), [16](#bib.bib16)], block-wise or global fine-tuning [[19](#bib.bib19),
    [65](#bib.bib65)] or even quantization-aware training [[72](#bib.bib72), [69](#bib.bib69)].
    However, the weight representation and the fine-tuning algorithm are largely orthogonal:
    most popular quantized representations could be obtained layer-wise in one-shot,
    fine-tuned layer-wise to a variety of optimization objectives, or even trained
    entirely from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69eb4804436f0917cfa08d545a1f9b1b.png)![Refer to caption](img/1de3e9479540f40a459a704b1e8572b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right)
    of 2-bit quantized Llama 2 models as a function of model size (GiB). See detailed
    setup in Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Surprisingly, there is a clear disparity between the degree of interest shown
    to accurate one-shot quantization versus accurate fine-tuning. Specifically, one-shot
    quantization is very well-studied, to the extent that, as shown in Figure [2](#S4.F2
    "Figure 2 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    improvements in this direction are clearly saturating. At the same time, the impact
    of fine-tuning strategy is largely unknown: while many recent works use some form
    of fine-tuning [[58](#bib.bib58), [19](#bib.bib19), [65](#bib.bib65)], they typically
    consider a single fine-tuning regimen based on straight-through estimation (STE) [[5](#bib.bib5),
    [13](#bib.bib13)]. Thus, given the multitude of representations considered, it
    is not at all clear whether current fine-tuning strategies are optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we analyze the problem of fine-tuning over highly-compressed
    weights from the optimization perspective. We begin by analyzing popular fine-tuning
    strategies for extreme LLM quantization. The key challenge in this context is
    that the quantized representations may contain both continuous and discrete variables:
    while continuous parameters, such as learnable scales or codebooks, can be optimized
    by backpropagation, the discrete parameters (e.g., integer assignments for the
    weights) cannot. Existing fine-tuning techniques either do not optimize over discrete
    parameters at all [[65](#bib.bib65), [19](#bib.bib19)] or fine-tune them using
    heuristics such as STE or stochastic rounding [[2](#bib.bib2)]. Unfortunately,
    these methods are not well-justified for weight quantization from the point of
    view of optimization theory, and, as we show in Section [3](#S3 "3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"), can provide poor practical performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose an alternative solution: instead of following heuristic gradient
    estimates, our approach follows the actual gradient of the objective in a small
    subspace of optimized parameters where it can be meaningfully improved. Following
    this insight, we formulate the *PV-tuning framework* for fine-tuning arbitrary
    quantized representations. We update both discrete and continuous components to
    minimize a global objective function, such as the KL divergence relative to the
    original model predictions. Our results show that this strategy leads to significant
    improvements across weight representations, achieving new state-of-the-art in
    compression-accuracy trade-offs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of our work can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We analyze the problem for training discrete quantized representations for better
    understanding of the limitations of existing optimization algorithms. We then
    propose a novel algorithm inspired by compressed gradient methods that addresses
    these limitations. When compared to straight-through estimation and stochastic
    rounding, our approach 1) can be shown to converge to a stable solution; and 2)
    this solution is significantly more accurate in practice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generalize the proposed algorithm into the PV-Tuning framework²²2The official
    implementation is available at [https://github.com/Vahe1994/AQLM/tree/pv-tuning](https://github.com/Vahe1994/AQLM/tree/pv-tuning).,
    which can minimize a global objective function over a general quantized representation,
    by optimizing both continuous and discrete parameters via a variant of coordinate
    descent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We demonstrate that PV-tuning can improve quantized model accuracy for leading
    existing approaches, including GPTQ and AQLM, on popular LLMs including Llama-2
    & 3 and Mistral. Our procedure achieves state-of-the-art accuracy (measured through
    perplexity) in 1- and 2-bit quantization regimes while using the same amount of
    calibration data as the original algorithms. Importantly, the PV-tuned models
    use the same underlying weight representations, and are compatible with existing
    inference kernels. In terms of accuracy per model size, PV-tuning of vector quantization
    outperforms all prior techniques in the 1-3 bits/parameter range, and is the first
    to achieve Pareto-optimal quantization for Llama 2 models at around 2 bits per
    parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Post-Training LLM Quantization (PTQ).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There has been significant interest in PTQ methods [[45](#bib.bib45), [23](#bib.bib23)]
    that would scale to LLMs. Early work [[15](#bib.bib15), [72](#bib.bib72), [46](#bib.bib46)]
    used direct round-to-nearest (RTN) quantization over weight groups of well-chosen
    size. GPTQ [[20](#bib.bib20)] improved upon these results significantly via an
    accurate one-shot solver for minimizing layer-wise compression errors. Next, AWQ [[39](#bib.bib39)]
    improved upon these results by employing per-channel scaling to reduce the error
    on important weights while SqueezeLLM [[33](#bib.bib33)] implemented non-uniform
    quantization. QuIP [[8](#bib.bib8)] proposed a more accurate weight representation
    by leveraging incoherence matrices. Another line of works [[16](#bib.bib16), [36](#bib.bib36)]
    proposes an improved quantized weight representation, which saves a small fraction
    of outliers in full precision. Other recent works propose augmenting quantized
    representations with lowrank “adapters” that compensate quantization error [[26](#bib.bib26),
    [77](#bib.bib77)]. Recently, BiLLM [[29](#bib.bib29)] developed residual binarization
    that stores salient weights in progressively higher bitwidth, quantizing models
    to nearly 1 bit per parameter at non-catastrophic accuracy loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the state-of-the-art methods in terms of accuracy-vs-size are QuIP# [[65](#bib.bib65)]
    and AQLM [[19](#bib.bib19)]. Both methods work roughly by mapping weight groups
    to points on highly-dimensional lattices, which are either chosen to satisfy some
    optimality properties (for QuIP#) or are learned (for AQLM). Interestingly, AQLM
    showed that fine-tuning the continuous parameters (codebooks) can improve accuracy
    significantly relative to pure one-shot compression; a variant of this approach
    was also adopted by QuIP#. PV-Tuning is compatible with both methods: as we show,
    it can lead to state-of-the-art compression results for such representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning over Quantized Weights.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As mentioned above, the two SOTA quantization techniques apply fine-tuning,
    but only update *continuous* parameters, such as quantization scales. When optimizing
    over *discrete* parameter sets, a standard choice in deep learning is the Straight-Through
    Estimator (STE) [[5](#bib.bib5), [13](#bib.bib13), [67](#bib.bib67)]. Prior work
    on LLM compression proposed to update both continuous and discrete parameters,
    via STE, both for post-training quantization [[72](#bib.bib72), [58](#bib.bib58)]
    and for training quantized networks from scratch [[29](#bib.bib29)]. However,
    it was observed early on that STE leads to instability when fine-tuning heavily
    quantized LLMs [[72](#bib.bib72)]. While early results suggest that STE can perform
    well when training quantized models from scratch [[41](#bib.bib41)], this behavior
    is yet to be validated for highly-performant multi-billion-parameter models, which
    are the focus of our work.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the two standard approaches for fine-tuning quantized LLMs are 1)
    fine-tuning only over the continuous parameters, such as quantization scales,
    which heavily limits the number of trainable parameters; and 2) optimizing all
    parameters via the STE, which however is known to be quite noisy especially for
    extreme quantization. In this context, our work proposes alternative approaches
    in the post-training compression setting, which lead to state-of-the-art results
    relative to both options.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Fine-Tuning Quantized Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we study the problem of fine-tuning quantized models to minimize
    a global objective, such as cross-entropy. Section [3.1](#S3.SS1 "3.1 Problem
    description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") formulates this problem from an optimization
    perspective and introduces our notation. In Section [3.2](#S3.SS2 "3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"), we analyze several
    popular strategies for solving this problem and highlight some of their limitations.
    To circumvent these limitations, we propose an alternative optimization algorithm
    in Section [3.3](#S3.SS3 "3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    and discuss implementation details in Section [3.4](#S3.SS4 "3.4 Implementation
    details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem description
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the problem of minimizing objective (loss) $\phi$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{x\in\mathbb{R}^{d}_{c}}\phi(x),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\phi:\mathbb{R}^{d}\to\mathbb{R}$
  prefs: []
  type: TYPE_NORMAL
- en: Useful notation. A vector $x\in\mathbb{R}^{d}_{c}$ characterized by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{i}=x_{j}\quad\Leftrightarrow\quad\exists k\;:\;i\in P_{k}\text{ and
    }j\in P_{k}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Let’s denote $P(x):=\{P_{1}(x),\dots,P_{c}(x)\}$.
  prefs: []
  type: TYPE_NORMAL
- en: PV method. Following this notation, we define an optimization algorithm that
    alternates between optimizing $\phi$. From a practitioner’s point of view, these
    represent optimizing continuous parameters (scales, codebooks, zeros) and discrete
    codes (assignments), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: $\diamond$, consider the mapping
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M_{P}(x)=M_{P,\phi}(x):=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\,:\,P(y)\supseteq
    P(x)\}.$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Notice that, necessarily, $M_{P}(x)\in\mathbb{R}^{d}_{\leq c}$-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: $\diamond$, we define the mapping
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)\subseteq
    V(y)\}.$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Likewise, $M_{V}(y)\in\mathbb{R}^{d}_{\leq c}$).
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 PV algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Initialization: starting point $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (V step:
    discrete)5:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our key algorithmic idea, in its simplest form, is to optimize $\phi$ operators.
    (We will propose several more practically-useful approximations and variations
    later; see Sections [3.2](#S3.SS2 "3.2 Linearized V step & gradient-based discrete
    updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")–[3.3](#S3.SS3 "3.3 Linearized subspace
    V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression") and also Appendix [B](#A2 "Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression").) This resulting method, which we call the PV method, is formalized
    as Algorithm [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"). Our key guarantee for the PV method is formalized in the next result.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.1  (Convergence of the PV method).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume $\phi$ converges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof can be found in Appendix [A.1](#A1.SS1 "A.1 Proof of Theorem 3.1
    ‣ Appendix A Proofs ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression"). Note that we do not claim that the method converges
    to a minimizer of $\phi$; the optimization problem is too difficult for us to
    be able to guarantee this. However, as we shall see in the numerical results,
    we nevertheless obtain great empirical performance, especially when coupling the
    PV approach with some additional algorithmic tricks.'
  prefs: []
  type: TYPE_NORMAL
- en: This general approach is popular in “shallow” machine learning problems; for
    instance, if $\phi(x)=\|x-z\|^{2}$, the approach is related to the EM algorithm [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In turn, we apply the PV method to obtaining highly-accurate quantized LLMs.
    Applying the PV method “as is”, would be infeasible in practice: computing the
    P and V mappings requires solving difficult optimization problems especially due
    to LLM parameter scales. However, both mappings can be approximated. Computing
    the P step ($M_{P}(\cdot)$. We dedicate the next two sections to this task.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Linearized V step & gradient-based discrete updates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The V mapping ([3](#S3.E3 "Equation 3 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) can be approximated by solving a discrete least squares problem
    using an approximation of $\phi(x)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textstyle\phi(x)\approx\widetilde{\phi}_{y}(x):=\phi(y)+\left\langle\nabla\phi(y),x-y\right\rangle+\frac{L}{2}{\left\&#124;x-y\right\&#124;}^{2},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $$L></math>:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Our first lemma shows that we can replace $\widetilde{\phi}_{y}$, disregarding
    the constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 3.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any $y\in\mathbb{R}^{d}_{\leq c}$ where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'The proof can be found in Appendix [A.2](#A1.SS2 "A.2 Proof of Lemma 3.2 ‣
    Appendix A Proofs ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression"). To summarize, the V step of the PV method (Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")), i.e., $x=M_{V,\phi}(y),$
    can be approximated via the “linearized V step”'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x:=M_{V,\phi}(y)\approx M_{V,\widehat{\phi}_{y}}(y):=\hat{x}.$ |  | (6)
    |'
  prefs: []
  type: TYPE_TB
- en: Our next lemma says that the above approximation is in a certain sense natural
    reasonable provided that $\phi$, i.e., provided that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Lemma 3.3  (Monotonicity).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $y\in\mathbb{R}^{d}_{\leq c}$ by the linearized V step ([6](#S3.E6 "Equation
    6 ‣ 3.2 Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, the point $\hat{x}$. Of course, one hopes that the loss will strictly
    decrease so that the method makes progress. From a practical perspective, the
    key advantage of linearized V step is that it can be performed much faster compared
    to the vanilla V step. The proof of Lemma [3.3](#S3.Thmtheorem3 "Lemma 3.3 (Monotonicity).
    ‣ 3.2 Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    can be found in Appendix [A.3](#A1.SS3 "A.3 Proof of Lemma 3.3 ‣ Appendix A Proofs
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since $\widehat{\phi}_{y}(x)$ error (see Appendix [D](#A4 "Appendix
    D Efficient Linearized V Step for Vector Quantization ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key challenge. The main caveat with linearized V step is that it may be impossible
    to make small gradient-based updates to low-bitwidth discrete weights. More specifically,
    in ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized V step & gradient-based discrete updates
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")), one must update the discrete assignments to approximate
    $y^{k}-\frac{1}{L}\nabla\phi(y^{k})$ can be very large, or, from a practitioner’s
    point of view, where one needs a small learning rate. In practice, as we explore
    in Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    the lowest learning rate where the algorithm makes any updates at all is already
    too large for optimization, leading to divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many popular strategies for discrete fine-tuning can be seen as attempts to
    reconcile coarse low-precision weights with the need to make small updates. These
    include straight-through estimation, stochastic rounding, or adding regularizers
    that push the solution to ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized V step & gradient-based
    discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) away from $y^{k}$. We review straight-through
    estimation in Appendix [E](#A5 "Appendix E Straight-through Gradient Estimation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    and stochastic rounding in Appendix [F](#A6 "Appendix F Stochastic Rounding ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 2 PV-Tuning: Optimization'
  prefs: []
  type: TYPE_NORMAL
- en: 0:initial parameters $x^{0}\in\mathbb{R}^{d}_{c}$8:  end for
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 3 PV-Tuning: Implementation, one step'
  prefs: []
  type: TYPE_NORMAL
- en: '0:quantized model, subspace size tau1:  deq_model := dequantize_weights(model)2:  for $t=1,\dots,T$
    V step: choose a subspace s and update codes11:  update ​​=​​ adam(grad_phi) ​​​-​​
    deq_model.weight12:  s = choose_subspace(update, tau)13:  model.codes[s] = find_nearest(update[s])'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Linearized subspace V step
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here we ask the following question: Can we modify the PV method so as to force
    the V step to make a larger update? In other words, we need an optimization algorithm
    that updates quantized weights either by a sufficiently large increment, or not
    at all.'
  prefs: []
  type: TYPE_NORMAL
- en: A natural example of such an algorithm is coordinate descent (CD) [[40](#bib.bib40),
    [53](#bib.bib53)], or more generally, subspace descent [[24](#bib.bib24), [35](#bib.bib35)].
    Instead of updating all parameters by a small margin, CD in each iteration chooses
    a single parameter, and makes a large update instead. This strategy can be generalized
    to updating more parameters at the same time, which leads to subspace descent
    methods.⁵⁵5Very closely related methods include block coordinate descent and compressed
    gradient descent with sparsification operators such as RandK or TopK [[3](#bib.bib3),
    [6](#bib.bib6)]. The parameters to be updated can be chosen either greedily, (e.g.,
    several $i\in[d]$), or at random, or through a variety of other means.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\mathcal{S}^{k}\subset[d]$. We now formulate the linearized subspace V
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'and <math id=$$ estimates in Appendix [G](#A7 "Appendix G On 𝐿-smoothness of
    LLM Objective in Sparse Subspaces ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, necessarily, $x^{+}_{i}=y_{i}$, for example.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it means that *the algorithm can apply large updates to quantized
    LLM weights, with the caveat that should only update a fraction of them at a time*.
    This allows us to perform the linearized V step with sufficiently large “learning
    rate” to make non-trivial (i.e., $x^{k+1}\neq y^{k}$) improvements to quantized
    weights even without straight-through estimation or stochastic rounding.
  prefs: []
  type: TYPE_NORMAL
- en: 'We formulate the full procedure in Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"). The algorithm
    performs the P step by directly optimizing $V(x)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Implementation details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To speed up convergence, we use adaptive learning rates for both P and V steps.
    In Eq. [8](#S3.E8 "Equation 8 ‣ 3.3 Linearized subspace V step ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"), we replace $\nabla\phi(y)$ weights with the largest update norm
    within each weight matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This could be further improved through better techniques for choosing $\mathcal{S}^{k}$
    explored in Appendix [O](#A15 "Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").
    We also found that, despite the fact that PV-tuning by itself outperforms straight-through
    estimation, we could achieve slightly better accuracy by combining PV-tuning with
    straight-through estimation. We explore this in more detail in Section [4.2](#S4.SS2
    "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We describe our approach for preparing the calibration data in Appendix [H](#A8
    "Appendix H Calibration Data Matters ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). We found that the preprocessing used
    in several recent PTQ works introduce a small bias when sampling the calibration
    data, leading to somewhat worse fine-tuning accuracy. For fairness, we always
    compare representations (Section [4.1](#S4.SS1 "4.1 Evaluating quantized representations
    with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) and algorithms (Section [4.2](#S4.SS2 "4.2 Evaluating
    Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) using the same pre-processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning efficiency. The most compute-intensive part of PV tuning is computing
    the gradients $\nabla\phi(\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1 Evaluating quantized representations with finetuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before evaluating PV-tuning, we need to choose the quantized representation
    to be fine-tuned. We therefore compare popular weight representations from recent
    works on LLM quantization (see Section [2](#S2.SS0.SSS0.Px1 "Post-Training LLM
    Quantization (PTQ). ‣ 2 Background ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). To better isolate the effect of the weight representation,
    we evaluate them in three configurations: i) when quantizing a single LLM layer,
    in terms of MSE, ii) full model quantization in terms of perplexity without finetuning
    and iii) with finetuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We compare several recently proposed quantized representations (see details
    in Appendix [J](#A10 "Appendix J Additional Details for Section 4.1 ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")):'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GPTQ: scalar uniform quantization with channel-wise and block-wise scales [[20](#bib.bib20)],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SpQR: an extension of block-wise GPTQ with learned sparse outliers [[16](#bib.bib16)],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'VQ: basic vector quantization with a single codebook  [[66](#bib.bib66)] with
    multi-step training.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AQLM: additive vector quantization with multiple learned codebooks [[19](#bib.bib19)],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'QuIP#: vector quantization with lattices and incoherence processing [[65](#bib.bib65)],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'VQ/AQ + outliers: vector/additive quantization with sparse outliers via pruning [[61](#bib.bib61),
    [7](#bib.bib7)],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'VQ/AQ + lowrank: vector/additive quantization with Low-Rank Compensation (LoRC) [[73](#bib.bib73)],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/064ad80c11bbc5b9488313ad92e20fbc.png)![Refer to caption](img/ed91444e22fed637394a55efd0a2f12b.png)![Refer
    to caption](img/94dd797315fa09921daace201e4538e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: (left) L2 errors for 17th layer of Llama 2 7B with different representations.
    Full model perplexity on WikiText-2 is reported without finetuning (middle) and
    with fine-tuning (right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We run all three experiments on Llama 2 7B model [[63](#bib.bib63)], calibrating
    on the RedPajama [[11](#bib.bib11)] dataset that best approximates the original
    pre-training data. When evaluating single layer errors, we report the L2 error
    in attention query projection outputs of a fixed transformer block, with other
    blocks exhibiting similar behavior. For full model evaluation, we report quantized
    model perplexity on WikiText-2 [[42](#bib.bib42)] dataset. We use the same data
    splits and preprocessing as in most recent PTQ works [[20](#bib.bib20), [39](#bib.bib39),
    [16](#bib.bib16), [64](#bib.bib64), [19](#bib.bib19), [65](#bib.bib65)], including
    the biased preprocessing step that we mentioned in [3.4](#S3.SS4 "3.4 Implementation
    details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). For fine-tuning, we train continuous
    parameters only, using the approach from [[65](#bib.bib65)]. To compare these
    diverse representations, we evaluate their quantization errors as a function of
    average number of bits per parameter. To get a diverse set of bits per parameter,
    we vary the hyperparameters such as wbits, block size, codebook and group size
    for vector quantization and the frequency of outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Evaluating quantized representations with
    finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression") summarizes our findings. Overall, vector quantization
    methods (VQ, QuIP and AQLM) outperform their scalar counterparts. Outliers and
    low-rank compensation both reduce error, but this improvement comes at the cost
    of extra bits per parameter. Interestingly, the improvement from outliers is significantly
    smaller when both methods have access to fine-tuning. We attribute this to the
    fact fine-tuning can compensate the error in “important” weights by adjusting
    the rest of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Our main takeaway is that for sub 2 bits per parameter, the vector quantization
    (VQ) representation can achieve near-optimal quantization accuracy, whether or
    not it uses outliers, LoRC or incoherence processing. Naturally, this does not
    reduce the value of prior works since they were designed for different scenarios,
    typically with a higher number of bits per parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on
    Llama 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy
    on tasks from Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion ‣
    4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fine-tuning Method | GPTQ 2.14 bit/w | VQ, 1.58 bit/w | AQLM, 2.01 bit/w
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Calibration only (no global fine-tuning) | 3290 | 4125 | 29.0 | 20.26 | 20.09
    | 43.42 | 7.38 | 9.34 | 53.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Continuous params only [[65](#bib.bib65), [19](#bib.bib19)] | 16.77 | 17.53
    | 46.27 | 8.17 | 10.99 | 52.14 | 6.69 | 8.77 | 56.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Naive Linearized PV (no subspace) | 16.73 | 17.48 | 47.68 | 8.19 | 10.94
    | 52.08 | 6.68 | 8.75 | 56.51 |'
  prefs: []
  type: TYPE_TB
- en: '| Stochastic Rounding [[49](#bib.bib49)] (tuned) | 11.97 | 13.07 | 49.79 |
    8.02 | 10.64 | 52.31 | 6.56 | 8.39 | 56.68 |'
  prefs: []
  type: TYPE_TB
- en: '| Straight Through Estimation [[71](#bib.bib71)] | 8.79 | 11.04 | 50.61 | 7.76
    | 10.26 | 52.58 | 6.41 | 8.63 | 57.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Subspace Linearized PV (ours, $\tau{=}0.01$) | 8.49 | 10.78 | 52.17 | 7.38
    | 9.47 | 53.36 | 6.13 | 8.35 | 57.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Subspace Linearized PV+STE ($\tau{=}0.01$) | 8.43 | 10.82 | 51.90 | 7.32
    | 9.35 | 55.22 | 5.90 | 7.43 | 58.19 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Evaluating Fine-tuning Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, we compare different fine-tuning strategies and ablate our PV-tuning
    protocol. We design our protocol to be representation-agnostic, i.e. compatible
    with different quantized representations. To showcase this, we pick three methods
    from the previous section: GPTQ, VQ and AQLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These methods differ not only in their weight representations, but also in
    how they search for the optimal codes. Namely, GPTQ can scale the target weight
    and round it to nearest 2-bit integer. In turn, VQ quantizes weights as a group
    and must find the nearest vector from its codebook, and AQLM uses a multi-step
    beam search procedure to choose the best combination of codes from both codebooks.
    Our PV-Tuning implementation uses these search algorithms during the subspace
    linearized V step (find_nearest in Alg. [3](#alg3 "Algorithm 3 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")). We describe
    the full PV configuration for each method in Appendix [K](#A11 "Appendix K Additional
    Details for Section 4.2 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We compare PV tuning against several popular fine-tuning regimens found in
    the literature. Our first baseline is fine-tuning only continuous parameters,
    e.g., codebooks or input/output embeddings [[65](#bib.bib65), [68](#bib.bib68)].
    The second baseline is training with Straight Through Estimation (STE) [[69](#bib.bib69),
    [71](#bib.bib71)]. We also test stochastic rounding as described in Appendix [F](#A6
    "Appendix F Stochastic Rounding ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). Finally, we evaluate PV tuning combined
    with STE, but otherwise the same configuration. We set the subspace size $\tau$,
    also known as known as trust ratio [[75](#bib.bib75)]. The results in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    show that PV-Tuning consistently finds better quantized models, with STE coming
    consistently second. We explore this further by combining subspace updates with
    STE, which leads to slightly better perplexity and accuracy in most (but not all)
    setups.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Large-scale Evaluation & Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, we evaluate the resulting PV algorithm with a vector quantization backbone
    and KL objective on a range of popular LLM models. For this section, our goal
    is to evaluate our approach holistically for different models and target bit-widths,
    comparing against the best known baselines in common settings. To that end, we
    evaluate on Llama 2 & 3 [[63](#bib.bib63)], Mistral 7B [[31](#bib.bib31)] and
    Phi-3 Mini-4k-Instruct [[1](#bib.bib1)] at 1–2.5 bits per parameter.
  prefs: []
  type: TYPE_NORMAL
- en: We report perplexity on WikiText-2 [[42](#bib.bib42)] and C4 [[50](#bib.bib50)]
    validation sets, zero-shot accuracy on WinoGrande [[55](#bib.bib55)], PiQA [[62](#bib.bib62)],
    HellaSwag [[76](#bib.bib76)], ARC-easy and ARC-challenge [[10](#bib.bib10)] via
    the LM Eval Harness [[22](#bib.bib22)]. We follow the exact evaluation setup from
    GPTQ [[20](#bib.bib20)]. We compare against QuIP [[64](#bib.bib64)], BiLLM [[29](#bib.bib29)],
    PB-LLM [[57](#bib.bib57)], DB-LLM [[9](#bib.bib9)], AQLM [[19](#bib.bib19)], OneBit [[71](#bib.bib71)],
    QuIP# [[65](#bib.bib65)], the latter three using fine-tuning. For Llama 3, we
    use baselines from [[30](#bib.bib30)] and re-evaluate perplexity in our setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    summarizes our findings: PV-tuning with vector quantization outperforms all known
    methods for 1- and 2-bit per weight. The closest competitors on Llama 2 are QuIP#,
    AQLM and OneBit, all of which use fine-tuning. The improvements on Llama 3 are
    also remarkable, as this model is notoriously hard to compress [[30](#bib.bib30)].
    We report additional evaluations in Appendix [L](#A12 "Appendix L Additional Details
    and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pareto-optimality. A key practical question concerns obtaining optimal quality
    for the target model size, where a smaller model compressed to 3-4 bits often
    dominates a larger model compressed to 1-bit. The best known Pareto-optimal bit-width
    for Llama 2 is 2.5 [[19](#bib.bib19)]: compressing a larger model to less than
    2.5 bits per weight is inferior to a smaller model quantized to the same total
    number of bytes. From this perspective, PV-tuning pushes the Pareto-optimal frontier
    for Llama 2 to 2.0 bits. This is easiest to see in Table [8](#A12.T8 "Table 8
    ‣ Appendix L Additional Details and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"): a 2-bit 13B
    model outperforms any 7B quantization and is comparable with the 16-bit 7B model.
    The same holds for the 2-bit 70B model.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning efficiency. One limitation of our algorithm is that it requires
    more compute and memory during the fine-tuning procedure. The 7B models can be
    fine-tuned on a single GPU, our 70B runs require a server with $8{\times}A100$
    or rely on RAM offloading. PV-Tuning shares this drawback with prior methods based
    on STE [[19](#bib.bib19), [65](#bib.bib65)], as both methods need gradients w.r.t.
    dequantized weights. Our longest training run took 2 days on 8 GPUs to outperform
    all baselines and 8 days to fully converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference speed. One advantage of PV-tuning is that it does not change the
    underlying compressed representation, so we can just simply existing high-performance
    inference kernels, but providing higher accuracy. Specifically, VQ+PV can reuse
    efficient kernels from [[19](#bib.bib19), [65](#bib.bib65)], while GPTQ+PV can
    use ExLlamaV2 kernels [[12](#bib.bib12)]. We illustrate speedups in Appendix [M](#A13
    "Appendix M Inference Speed with Vector Quantization Kernels ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Quantized model perplexity on WikiText2$\downarrow$ mean higher /
    lower is better.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 model family |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | – | 16 | 5.12 | 6.63 | 64.80 |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.08 | 32.48 | 40.52 | 41.68 |'
  prefs: []
  type: TYPE_TB
- en: '| OneBit | 1.01 | 9.73 | 11.11 | 50.06 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.02 | 8.28 | 10.37 | 50.66 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.02 | 6.64 | 8.56 | 56.47 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2.01 | 6.19 | 8.16 | 57.51 |'
  prefs: []
  type: TYPE_TB
- en: '| DB-LLM | 2.01 | 7.23 | 9.62 | 55.12 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.02 | 5.84 | 7.62 | 61.35 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | – | 16 | 4.57 | 6.05 | 67.82 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 1.97 | 5.65 | 7.51 | 60.59 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2.01 | 5.35 | 7.20 | 61.45 |'
  prefs: []
  type: TYPE_TB
- en: '| DB-LLM | 2.01 | 6.19 | 8.38 | 59.41 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.97 | 5.12 | 6.83 | 64.92 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.19 | 5.05 | 6.74 | 66.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 70B | – | 16 | 3.12 | 4.97 | 72.40 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.07 | 3.94 | 5.72 | 68.75 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2.01 | 3.91 | 5.71 | 68.60 |'
  prefs: []
  type: TYPE_TB
- en: '| DB-LLM | 2.01 | 4.64 | 6.77 | 65.83 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.07 | 3.78 | 5.56 | 70.72 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.14 | 5.52 | 7.50 | 64.58 |'
  prefs: []
  type: TYPE_TB
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 model family |'
  prefs: []
  type: TYPE_TB
- en: '| 8B | – | 16 | 5.54 | 7.10 | 68.61 |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.1 | 28.8 | 257 | 37.90 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.01 | 11.17 | 11.63 | 50.01 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 2.01 | 76.95 | 98.47 | 36.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DB-LLM | 2.01 | 12.77 | 14.82 | 51.8 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.01 | 6.99 | 8.29 | 64.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 70B | – | 16 | 2.59 | 5.78 | 75.37 |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.1 | 15.26 | 65.07 | 44.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.01 | 8.67 | 9.68 | 51.47 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 2.00 | 11.63 | 18.54 | 48.71 |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 2.00 | 10.33 | 28.89 | 46.04 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.07 | 4.57 | 6.56 | 70.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7B v0.1 (A) and Phi 3 Mini-4k-Instruct (B) |'
  prefs: []
  type: TYPE_TB
- en: '| $\underset{(A)}{\text{7B}}$ | – | 16 | 4.78 | 5.71 | 69.38 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2.01 | 6.02 | 6.84 | 62.20 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.01 | 5.29 | 6.17 | 66.32 |'
  prefs: []
  type: TYPE_TB
- en: '| $\underset{(B)}{\text{3.8B}}$ | – | 16 | 5.83 | 9.35 | 70.5 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.03 | 8.85 | 12.19 | 60.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.03 | 6.88 | 10.08 | 65.70 |'
  prefs: []
  type: TYPE_TB
- en: 5 Conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Limitations. We focus our effort on evaluating PV-Tuning with multiple setups
    and models, but we spent relatively little effort tuning our algorithm for each
    specific setup. For instance, we always use constant learning rate and $\tau$
    with no schedule, and always train on the same data. While this shows robustness
    of PV-Tuning, it also means that our results may be improved with better hyperparameters.
    Furthermore, the algorithm could achieve better accuracy by simply training longer
    and on more data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Future work. This work opens several new research directions. The first is
    about how to choose $\mathcal{S}^{k}$: while we found that a greedy strategy works
    in practice, there may be fundamentally better ways. Another direction is applying
    PV-Tuning to other quantization niches: our evaluation focuses on extreme weight-only
    quantization, but the proposed algorithm can be used in weight + activation setting
    or quantize vision models. Overall, PV-Tuning shows how an insight from optimization
    theory can improve LLM quantization and we are excited to see how this develops
    in future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Acknowledgements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Authors would like to thank Vage Egiazarian, Andrei Panferov and Ruslan Svirschevski
    for their help and advice on AQLM codebase and running large-scale experiments.
    We also thank Philip Zmushko and Artem Fedorov for helpful discussions during
    the early stages of our research. Finally, we thank the open-source contributors
    from llamacpp⁶⁶6[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
    and the LocalLlama⁷⁷7[https://reddit.com/r/LocalLaMA](https://reddit.com/r/LocalLaMA)
    community for discussions and inspirations on practical use cases of quantized
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla,
    N. Bach, A. Bahree, A. Bakhtiari, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck,
    S. Bubeck, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno,
    G. de Rosa, M. Dixon, R. Eldan, D. Iter, A. Garg, A. Goswami, S. Gunasekar, E. Haider,
    J. Hao, R. J. Hewett, J. Huynh, M. Javaheripi, X. Jin, P. Kauffmann, N. Karampatziakis,
    D. Kim, M. Khademi, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, C. Liang, W. Liu,
    E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker,
    T. Portet, R. Pryzant, H. Qin, M. Radmilac, C. Rosset, S. Roy, O. Ruwase, O. Saarikivi,
    A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, X. Song, M. Tanaka,
    X. Wang, R. Ward, G. Wang, P. Witte, M. Wyatt, C. Xu, J. Xu, S. Yadav, F. Yang,
    Z. Yang, D. Yu, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang,
    Y. Zhang, and X. Zhou. Phi-3 technical report: A highly capable language model
    locally on your phone, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Randomized
    quantization for communication-efficient stochastic gradient descent. In Conference
    on Neural Information Processing Systems (NeurIPS), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and
    C. Renggli. The convergence of sparsified gradient methods. In Advances in Neural
    Information Processing Systems, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] A. Babenko and V. Lempitsky. Additive quantization for extreme vector compression.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 931–938, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients
    through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Beznosikov, S. Horváth, P. Richtárik, and M. Safaryan. On biased compression
    for distributed learning. arXiv:2002.12410, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] V. Boža. Fast and optimal weight update for pruned large language models,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. Quip: 2-bit quantization of
    large language models with guarantees, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Chen, C. Lv, L. Ding, H. Qin, X. Zhou, Y. Ding, X. Liu, M. Zhang, J. Guo,
    X. Liu, and D. Tao. Db-llm: Accurate dual-binarization for efficient llms, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
    challenge. arXiv preprint arXiv:1803.05457, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] T. Computer. Redpajama: an open dataset for training large language models,
    October 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] E. contributors. ExLlamaV2\. A fast inference library for running LLMs
    locally on modern consumer-class GPUs. Open-source library developed by turboderp
    and controbutors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Courbariaux, Y. Bengio, and J.-P. David. Training deep neural networks
    with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from
    incomplete data via the em algorithm. Journal of the royal statistical society:
    series B (methodological), 39(1):1–22, 1977.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit
    matrix multiplication for transformers at scale. Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh. Spqr: A sparse-quantized
    representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] T. Dettmers and T. von Koeller. Accessible large language models via k-bit
    quantization for pytorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference
    scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and
    D. Alistarh. Extreme compression of large language models via additive quantization.
    arXiv preprint arXiv:2401.06118, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training
    quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Freund and R. E. Schapire. Large margin classification using the perceptron
    algorithm. In Proceedings of the eleventh annual conference on Computational learning
    theory, pages 209–217, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding,
    J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation,
    Sept. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A
    survey of quantization methods for efficient neural network inference. arXiv preprint
    arXiv:2103.13630, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. M. Gower and P. Richtárik. Stochastic dual ascent for solving linear
    systems. arXiv:1512.06890, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Griewank and A. Walther. Algorithm 799: revolve: an implementation
    of checkpointing for the reverse or adjoint mode of computational differentiation.
    ACM Transactions on Mathematical Software (TOMS), 26(1):19–45, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] H. Guo, P. Greengard, E. P. Xing, and Y. Kim. Lq-lora: Low-rank plus quantized
    matrix decomposition for efficient language model finetuning, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] K. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng, and R. Nusselder.
    Latent weights do not exist: Rethinking binarized neural network optimization.
    Advances in neural information processing systems, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] G. Hinton. Neural networks for machine learning, coursera (video lectures).,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno, and X. Qi.
    Billm: Pushing the limit of post-training quantization for llms. arXiv preprint
    arXiv:2402.04291, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu,
    and M. Magno. How good are low-bit quantized llama3 models? an empirical study,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv
    preprint arXiv:2310.06825, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts.
    arXiv preprint arXiv:2401.04088, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney,
    and K. Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International
    Conference on Learning Representations (ICLR), 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] D. Kozak, S. Becker, A. Doostan, and L. Tenorio. Stochastic subspace descent.
    arXiv preprint arXiv:1904.01145, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park. Owq: Outlier-aware weight
    quantization for efficient fine-tuning and inference of large language models,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
    J. Smith, B. Vaughan, P. Damania, and S. Chintala. Pytorch distributed: Experiences
    on accelerating data parallel training, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee.
    Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware
    weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method
    for convex differentiable minimization. Journal of Optimization Theory and Applications,
    72(1):7–35, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue,
    and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture
    models. arXiv preprint arXiv:1609.07843, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen,
    D. Arfeen, R. Abhyankar, and Z. Jia. Specinfer: Accelerating generative llm serving
    with speculative inference and token tree verification, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. Mises and H. Pollaczek-Geiringer. Praktische verfahren der gleichungsauflösung.
    ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik
    und Mechanik, 9(1):58–77, 1929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort.
    Up or down? Adaptive rounding for post-training quantization. In International
    Conference on Machine Learning (ICML), 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuQmm: Quantized
    matmul for efficient inference of large-scale generative language models. arXiv
    preprint arXiv:2206.09557, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Park and E. K. Ryu. Exact optimal accelerated complexity for fixed-point
    iterations. In Proceedings of the 39th International Conference on Machine Learning,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,
    A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch:
    An imperative style, high-performance deep learning library. In Advances in Neural
    Information Processing Systems (NeurIPS). Neural Information Processing Systems
    Foundation, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation
    and quantization. arXiv preprint arXiv:1802.05668, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. Liu. Exploring the limits of transfer learning with a unified text-to-text
    transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations
    toward training trillion parameter models. In Proceedings of the International
    Conference for High Performance Computing, Networking, Storage and Analysis, SC
    ’20\. IEEE Press, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang,
    D. Li, and Y. He. Zero-offload: Democratizing billion-scale model training, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] P. Richtárik and M. Takáč. Parallel coordinate descent methods for big
    data optimization. Mathematical Programming, 156(1-2):433–484, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] F. Rosenblatt. The perceptron - a perceiving and recognizing automaton.
    Technical Report 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New York,
    January 1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: an
    adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné,
    A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access
    multilingual language model. arXiv preprint arXiv:2211.05100, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Y. Shang, Z. Yuan, Q. Wu, and Z. Dong. Pb-llm: Partially binarized large
    language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao,
    and P. Luo. Omniquant: Omnidirectionally calibrated quantization for large language
    models. arXiv preprint arXiv:2308.13137, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Shekhovtsov and V. Yanush. Reintroducing straight-through estimators
    as principled methods for stochastic binary networks. In DAGM German Conference
    on Pattern Recognition, pages 111–126\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Spallanzani, G. P. Leonardi, and L. Benini. Training quantised neural
    networks with ste variants: the additive noise annealing algorithm. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 470–479,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning
    approach for large language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Tata and J. M. Patel. PiQA: An algebra for querying protein data sets.
    In International Conference on Scientific and Statistical Database Management,
    2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation
    language models. arXiv preprint arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa. Quip#: Even better
    llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint
    arXiv:2402.04396, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. D. Sa. Quip#: Even better
    llm quantization with hadamard incoherence and lattice codebooks, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] M. van Baalen, A. Kuzmin, M. Nagel, P. Couperus, C. Bastoul, E. Mahurin,
    T. Blankevoort, and P. Whatmough. Gptvq: The blessing of dimensionality for llm
    quantization. arXiv preprint arXiv:2402.15319, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. van den Oord, O. Vinyals, and k. kavukcuoglu. Neural discrete representation
    learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30\.
    Curran Associates, Inc., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] H. Vanholder. Efficient inference with TensorRT. NVIDIA GTC On-Demand.
    Slides available at https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu,
    and F. Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv
    preprint arXiv:2310.11453, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] B. Widrow and M. A. Lehr. 30 years of adaptive neural networks: perceptron,
    madaline, and backpropagation. Proc. IEEE, 78:1415–1442, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Y. Xu, X. Han, Z. Yang, S. Wang, Q. Zhu, Z. Liu, W. Liu, and W. Che. Onebit:
    Towards extremely low-bit large language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant:
    Efficient and affordable post-training quantization for large-scale transformers.
    arXiv preprint arXiv:2206.01861, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. Exploring post-training quantization
    in llms from comprehensive study to low rank compensation. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 38, pages 19377–19385, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] P. Yin, J. Lyu, S. Zhang, S. Osher, Y. Qi, and J. Xin. Understanding straight-through
    estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning:
    Training bert in 76 minutes. In International Conference on Learning Representations
    (ICLR), 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag:
    Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Màrquez,
    editors, Proceedings of the 57th Conference of the Association for Computational
    Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long
    Papers, pages 4791–4800\. Association for Computational Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] C. Zhang, J. Cheng, G. A. Constantinides, and Y. Zhao. Lqer: Low-rank
    quantization error reconstruction for llms, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small
    language model, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models.
    arXiv preprint arXiv:2205.01068, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[1 Introduction](#S1 "In PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2 Background](#S2 "In PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3 Fine-Tuning Quantized Models](#S3 "In PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1 Problem description](#S3.SS1 "In 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2 Linearized V step & gradient-based discrete updates](#S3.SS2 "In 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.3 Linearized subspace V step](#S3.SS3 "In 3 Fine-Tuning Quantized Models
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.4 Implementation details](#S3.SS4 "In 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4 Experiments](#S4 "In PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1 Evaluating quantized representations with finetuning](#S4.SS1 "In 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2 Evaluating Fine-tuning Algorithms](#S4.SS2 "In 4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3 Large-scale Evaluation & Discussion](#S4.SS3 "In 4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5 Conclusions](#S5 "In PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6 Acknowledgements](#S6 "In PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A Proofs](#A1 "In Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.1 Proof of Theorem 3.1](#A1.SS1 "In Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.2 Proof of Lemma 3.2](#A1.SS2 "In Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A.3 Proof of Lemma 3.3](#A1.SS3 "In Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B Approximate PV Algorithm](#A2 "In Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B.1 Approximate V step, variant 1 (non-accelerated)](#A2.SS1 "In Appendix
    B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[B.2 Approximate V step, variant 2 (accelerated)](#A2.SS2 "In Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[C Generalization to Other Quantization Algorithms](#A3 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[D Efficient Linearized V Step for Vector Quantization](#A4 "In Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E Straight-through Gradient Estimation](#A5 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[F Stochastic Rounding](#A6 "In Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[G On $L$-smoothness of LLM Objective in Sparse Subspaces](#A7 "In Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H Calibration Data Matters](#A8 "In Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[I Additional Engineering Considerations](#A9 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[J Additional Details for Section 4.1](#A10 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[K Additional Details for Section 4.2](#A11 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[L Additional Details and Evaluations for Section 4.3](#A12 "In Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M Inference Speed with Vector Quantization Kernels](#A13 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[N The Choice of the Initial Point $x^{0}$](#A14 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[N.1 Clipping of $x^{\star}$](#A14.SS1 "In Appendix N The Choice of the Initial
    Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[N.2 Random $x^{0}\in\mathbb{R}^{d}_{c}$](#A14.SS2 "In Appendix N The Choice
    of the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[O Small-Scale Experiments and Interpretation](#A15 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[O.1 Objective function](#A15.SS1 "In Appendix O Small-Scale Experiments and
    Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[O.2 Tiny-scale experiments ($d=6$)](#A15.SS2 "In Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[O.3 Small-scale experiments ($d=100$)](#A15.SS3 "In Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[O.4 Linearized PV](#A15.SS4 "In Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[O.5 Linearized PV + sparse updates](#A15.SS5 "In Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[P PV^+ Algorithm](#A16 "In Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Q Broader Impact](#A17 "In Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appendix A Proofs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A.1 Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1 (Convergence of the
    PV method). ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Part (i): First, by assumption, we know that $x^{0}\in\mathbb{R}^{d}_{\leq
    c}$. The claim now follows by induction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part (ii): Since'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y^{k}=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\;:\;P(y)\supseteq P(x^{k})\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and because $y=x^{k}$. Further, since
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x^{k+1}=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\;:\;V(x)\subseteq V(y^{k})\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and because $x=y^{k}$. In summary,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Part (iii): In view of part (ii), the sequence $\{\phi(x^{k})\}_{k=0}^{\infty}$
    is non-increasing. By assumption, it is bounded below. Hence, it converges to
    its infimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lim_{k\to\infty}\phi(x^{k})=\inf_{k\in\{0,1,\dots\}}\phi(x^{k}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'A.2 Proof of Lemma [3.2](#S3.Thmtheorem2 "Lemma 3.2\. ‣ 3.2 Linearized V step
    & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '|  | $\displaystyle M_{V,\widetilde{\phi}_{y}}(y)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'A.3 Proof of Lemma [3.3](#S3.Thmtheorem3 "Lemma 3.3 (Monotonicity). ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, note that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\phi(\hat{x})$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{\text{Lemma}~{}\ref{lem:prox}}{=}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{\eqref{eq:09u0fd9hfd}}{=}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{\eqref{eq:09-98u98yfhd}}{=}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Since $y\in\mathbb{R}^{d}_{\leq c}$, we can bound the last expression in ([9](#A1.E9
    "Equation 9 ‣ A.3 Proof of Lemma 3.3 ‣ Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) from below via'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\phi(\hat{x})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\overset{\eqref{eq:09u0fd9hfd}}{=}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Finally, since $x=y$, we can upper bound the same expression via
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\phi(\hat{x})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Approximate PV Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now introduce the pseudocode of an approximate PV meta-algorithm; the idea
    is to replace the P and V steps with some approximate computations to be defined
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 Approximate PV Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Parameters: starting point $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (for example,
    we can use the method from Section [B.1](#A2.SS1 "B.1 Approximate V step, variant
    1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression") or the method
    from Section [B.2](#A2.SS2 "B.2 Approximate V step, variant 2 (accelerated) ‣
    Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"))5:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we describe two new approximations of the V step.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Approximate V step, variant 1 (non-accelerated)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now describe an algorithm computing an approximation to $M_{V,\phi}(y)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with some $y\in\mathbb{R}^{d}_{c}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set $z^{0}=y$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For $t=0,\dots,T-1$ iterate:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Define $\widehat{\phi}_{z^{t}}(\cdot):={\left\|\cdot-\left(z^{t}-\frac{1}{L}\nabla\phi(z^{t})\right)\right\|}^{2}$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set $z^{t+1}=M_{V,\widehat{\phi}_{z^{t}}}(z^{t})$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output: $z^{T}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The method is constructed so that $z^{T}\approx M_{V,\phi}(y)$ may be advantageous.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Approximate V step, variant 2 (accelerated)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We now describe a different algorithm for computing an approximation to $M_{V,\phi}(y)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with some $y\in\mathbb{R}^{d}_{c}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a suitable decreasing sequence of positive scalars $\{\alpha_{t}\}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set $z^{0}=y$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For $t=0,\dots,T-1$ iterate:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Define $\widehat{\phi}_{z^{t}}(\cdot):={\left\|\cdot-\left(z^{t}-\frac{1}{L}\nabla\phi(z^{t})\right)\right\|}^{2}$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set $z^{t+1}=\left(1-\alpha_{t}\right)M_{V,\widehat{\phi}_{z^{t}}}(z^{t})+\alpha_{t}z^{0}$
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output: $z^{T}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The method is constructed so that $z^{T}\approx M_{V,\phi}(y)$. This approach
    is based on Halpern acceleration of fixed point methods [[47](#bib.bib47)], and
    as such, may be sometimes effective.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Generalization to Other Quantization Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [3.1](#S3.SS1 "3.1 Problem description ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    we define $\mathbb{R}^{d}_{\leq c}$ centroids found by clustering the weights.
    Below, we show how this can be generalized to linear quantization, vector quantization,
    additive quantization, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear quantization is the most basic and widely used type of quantization where
    weights are stored as integers, possibly multiplied by a scale and added to a
    zero point. The simplest way to account for this quantization type is to declare
    that weight values are integers up to $c$. Both options lead to equivalent fine-tuning
    algorithms where the V step does not change and the P step has an additional condition.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us discuss vector quantization. Consider a quantization that splits
    $x$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the P and V steps for vector-quantized weights follow the same general
    principle: P-step can be approximated by backprop with slightly more trainable
    parameters. In turn, the V step can be done by trying all values in V(x) and selecting
    the one with the lowest $\widehat{\phi}(\cdot)$. A more compute-efficient version
    of the V step for this case is described in Appendix [D](#A4 "Appendix D Efficient
    Linearized V Step for Vector Quantization ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: RVQ and Additive Quantization can be treated as learning two separate sets of
    vector-quantized parameters. However, a more efficient way would be to run the
    V step to find the best combination of codes via beam search [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization with sparse outliers [[15](#bib.bib15), [16](#bib.bib16), [39](#bib.bib39)]
    can be seen as learning two distinct matrices with different definitions of $\mathbb{R}^{d}_{c}$:
    one is quantized and the other is sparse (for outliers). Similarly, quantized
    weights with low-rank adapters (e.g. [[26](#bib.bib26)]) can treat the adapter
    as an additional non-quantized parameter for the P step. This makes PV-tuning
    potentially extensible to neural network pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Efficient Linearized V Step for Vector Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we describe in Section [3.2](#S3.SS2 "3.2 Linearized V step & gradient-based
    discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"), the linearized V step minimizes the
    squared error between quantized weights and the updated “target” weights. Here,
    we explain how one can compute and minimize the squared error efficiently in practice.
    To simplify the notation for this section, we define the objective as $\|x-B\|^{2}$
    norm, this objective can be re-written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;x-B\&#124;^{2}=\&#124;x\&#124;^{2}-2\langle x,B\rangle+\&#124;B\&#124;^{2}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'Consider the first term: $\|x\|^{2}=\sum_{i=1}^{d}x_{i}^{2}$ unique terms.
    Abusing your notation, this can be rewritten as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;x\&#124;^{2}=\sum_{i}^{{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}c}}V_{i}(x)^{2}\cdot&#124;P_{i}(x)&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $V_{i}(x)$ is the number of such elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second term is also a sum of $c$ unique values:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The third term does not depend on $x$.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the objective for some given $x$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{N}_{1}(x):=\{\hat{x}\in\mathbb{R}^{d}_{\leq{c}}:V(\hat{x})=V(x),\,\&#124;x-\hat{x}\&#124;_{0}=1\}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: Consider one $\hat{x}\in\mathcal{N}_{1}(x)$. Then,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\phi(\hat{x})-\phi(x)={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\hat{x}_{k}^{2}-x_{k}^{2}}-{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}2\cdot(\hat{x}_{k}-x_{k})\cdot
    B_{k}}+{\color[rgb]{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5}0}\\
    $ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'Note that, for any $\forall\hat{x}\in\mathcal{N}_{1}(x)$. This allows for an
    efficient local search algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let $x^{0}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute and save all $2c^{2}$ possible red and blue values
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute $\phi_{0}=\phi(x^{0})$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'for t = 0, …:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'for $\hat{x}\in\mathcal{N}_{1}(x^{t})$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: find $k:\hat{x}_{k}\neq x^{t}_{k}$)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: compute $1$2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $x^{t+1}:=\underset{\hat{x}\in\mathcal{N}_{1}(x^{t})}{\arg\min}\phi(\hat{x})$
      (minimum from array of pre-computed values)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In practice, this can be extended from greedy (local) search to semi-greedy
    beam search. These practical algorithms are described in AQ, LSQ, and LSQ++. Algorithms
    for $\|A(x-B)\|^{2}$ are explained in AQLM and probably other works.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Straight-through Gradient Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Straight-through gradient estimation is a technique for training neural networks
    with discrete components that ignores these discrete components during backpropagation.
    Its usage goes back to the Perceptron introduced by Rosenblatt [[54](#bib.bib54)].
    There, the artificial neuron uses a step function as activation, but the training
    procedure treats this function as though it was identity. Subsequent works introduce
    variations of this idea, extend it to multi-layer networks [[28](#bib.bib28)],
    discuss its convergence properties [[70](#bib.bib70), [21](#bib.bib21), [74](#bib.bib74)].
    Other works use straight-through estimation or similar techniques to training
    neural network with quantized weights [[27](#bib.bib27), [59](#bib.bib59), [60](#bib.bib60)].
  prefs: []
  type: TYPE_NORMAL
- en: 'STE for LLM quantization. As we discuss in Section [2](#S2.SS0.SSS0.Px2 "Fine-tuning
    over Quantized Weights. ‣ 2 Background ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression"), straight-through estimation introduces an auxiliary
    non-quantized weight tensor that is updated using the gradients $\nabla\phi(y)$
    will no longer be the solution to Equation ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This strategy prevents the algorithm from stalling, but it does so at the cost
    of convergence guarantees [[74](#bib.bib74)]. When applied to extreme LLM quantization
    (Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    straight-through estimation initially improves $y^{k}$, but then stops improving
    and oscillates. We also tried several a variant of straight-through estimation [[60](#bib.bib60)]
    that introduce stochasticity to forward pass. When applied to extreme LLM quantization,
    this variant did not diverge like naive STE, but trained much slower and did not
    reach the same optimum as “deterministic” STE. We attribute this to the fact that
    adding noise during training can slow down convergence, which also applies to
    stochastic rounding (Appendix  [F](#A6 "Appendix F Stochastic Rounding ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Stochastic Rounding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stochastic (or probabilistic) rounding is one of the techniques that can circumvent
    stalling when training low-precision weights. To recall, the linearized V step
    ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized V step & gradient-based discrete updates
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) can be seen as rounding $y^{+}:=y-\frac{1}{L}\nabla\phi(y)$
    for left and right. The probability of rounding is inversely proportional to the
    rounding error (distance), or, in terms of the objective,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This way,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{p(\text{round to }x)}{E}x=y-\frac{1}{L}\nabla\phi(y).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The main drawback of stochastic rounding is t introduces noise, it changes
    the underlying optimization problem. Intuitively, if the optimal $x^{\star}$ is
    adjacent to a significantly worse solution, the method may oscillate between rounding
    to either side. This rounding noise increases further as we consider lower quantization
    width. In Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    we exploit his phenomenon for real-world LLMs and find that stochastic rounding
    converges find significantly worse solutions, presumably because at every step,
    some portion of LLM weights will be rounded poorly. On top of that, when used
    for vector quantization, stochastic rounding is either intractable or biased.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stochastic rounding for vector quantization. To recall, stochastic rounding
    for non-vector quantization needs to find two quantized values: the nearest neighbor
    above the solution, and the nearest neighbor below it. It will then round to either
    of the two values inversely proportionally to their rounding errors.'
  prefs: []
  type: TYPE_NORMAL
- en: However, this intuition no longer works if you consider more complex quantization
    schemes such as vector quantization, additive quantization, quantized low-rank
    adapters, and others. In vector quantization, a group of weights is encoded jointly
    as one vector from a fixed set (usually called codebook or lattice). For simplicity,
    let us consider the case where the weight group size equals 2, i.e. weights are
    quantized in pairs.
  prefs: []
  type: TYPE_NORMAL
- en: For a pair of two eights, we can no longer rely on the fact that they have one
    neighbor from above and one from below. Instead, they may have any number of adjacent
    "clusters" they can be rounded to. Intuitively, a pair of weights is a point in
    2-dimensional that can have neighbors from left, right, top, bottom, and any diagonals.
    Formally, to determine a full list of neighbors, we can run Delaunay triangulation
    on all vectors from the codebook (or lattice) plus the target vector that needs
    to be rounded, then find all points that share a triangle with the target vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this procedure can be very expensive, especially for higher-dimensional
    vectors. A popular practical approximation to stochastic rounding for vector quantizations
    is to find K (e.g. 2) nearest vectors from the codebook, then use the probability
    formula from scalar stochastic rounding:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(\text{round to }x_{i})=\widehat{\phi}(x_{i})^{-1/2}/(\sum^{K}_{j}\widehat{\phi}(x_{j})^{-1/2})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: However, unlike the original stochastic rounding, this approximation does not
    guarantee that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{p(\text{round to }x_{i})}{E}x_{i}=y-\frac{1}{L}\nabla\phi(y).$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: For a (counter)example, if there is a high density of codes on one side of the
    target, all K (e.g. 2) nearest neighbors will be on the same side. As a result,
    this approximate stochastic rounding is biased and further changes the optimization
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G On $L$-smoothness of LLM Objective in Sparse Subspaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The classical definition of $L$ represented by requirement
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;\nabla f(x)-\nabla f(y)\&#124;\leq L\&#124;x-y\&#124;,\qquad\forall
    x,y\in\mathbb{R}^{d}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: If function $f(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: If the domain of function $f(x)$ we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Sparse sub-spaces satisfy this requirement; therefore, this theoretical observation
    is valid in this circumstance.
  prefs: []
  type: TYPE_NORMAL
- en: Another observation is that the definition of $L$ space.
  prefs: []
  type: TYPE_NORMAL
- en: Below we demonstrate two approximate schemas for evaluating $L$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Schema I: Estimating $L$ along the trajectory of iterates without capturing
    local curvature.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After running GD for $10$ with approximated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Results are presented in Tables [3](#A7.T3 "Table 3 ‣ Schema I: Estimating
    𝐿 along the trajectory of iterates without capturing local curvature. ‣ Appendix
    G On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"), [4](#A7.T4 "Table
    4 ‣ Schema I: Estimating 𝐿 along the trajectory of iterates without capturing
    local curvature. ‣ Appendix G On 𝐿-smoothness of LLM Objective in Sparse Subspaces
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").
    From them, we can see that $\hat{L}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Estimated $L$ along the GD trajectory for LLama-160m (Schema I).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5% | 2.36M | 10.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 10% | 8.26M | 14.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 20% | 17.69M | 305.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 30% | 24.77M | 498.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 40% | 36.57M | 919.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 60% | 60.75M | 5454.79 |'
  prefs: []
  type: TYPE_TB
- en: '| 70% | 85.52M | 6915.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 85% | 102.04M | 7043.19 |'
  prefs: []
  type: TYPE_TB
- en: '| 100% | 113.25M | 7251.50 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Estimated $L$ along the GD trajectory for TinyLlama-1.1B (Schema I).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5% | 13.11M | 33.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 10% | 49.28M | 143.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 20% | 136.84M | 2159.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 30% | 242.75M | 2369.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 40% | 369.10M | 2638.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 60% | 582.48M | 5185.92 |'
  prefs: []
  type: TYPE_TB
- en: '| 70% | 684.20M | 5901.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 85% | 831.52M | 6091.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 100% | 968.88M | 9480.57 |'
  prefs: []
  type: TYPE_TB
- en: 'Schema II: Estimating $L$ along the sequence of iterates with capturing local
    curvature.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The previous schema used a fixed sequence of iterates $s=\{x_{1},x_{2},\dots,x_{10}\}$-smoothness
    constant as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{L}=\underset{x_{i}\in s}{\max}\left(\&#124;\nabla^{2}f(x_{i})\&#124;\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore this schema exploits a sequence of points $s$. Computing any spectral
    information for a matrix with big dimensions can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approximate schema that we have used to compute $\|\nabla^{2}f(x)\|$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla f(x+r)-\nabla f(x)=\nabla^{2}f(x)\cdot r+\mathcal{O}\left(\&#124;r\&#124;^{2}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The $K$. In fact Power Iteration does not converge in case of degeneracy such
    as a situation when the matrix has two maximum eigenvalues in absolute values
    but with opposite signs, and the convergence rate is determined by the absolute
    value of the ratio of the second-largest-magnitude eigenvalue to the first. We
    ignore these aspects in our heuristic approximate Algorithm [5](#alg5 "Algorithm
    5 ‣ Schema II: Estimating 𝐿 along the sequence of iterates with capturing local
    curvature. ‣ Appendix G On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5 Approximate Matrix-free Algorithm for Computing $\|\nabla f(x)\|$
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Parameters: Point $x\in\mathbb{R}^{d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results are presented in Tables [5](#A7.T5 "Table 5 ‣ Schema II: Estimating
    𝐿 along the sequence of iterates with capturing local curvature. ‣ Appendix G
    On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣ Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression"), [6](#A7.T6 "Table 6
    ‣ Schema II: Estimating 𝐿 along the sequence of iterates with capturing local
    curvature. ‣ Appendix G On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").
    From them, we can see that also this notion of $\hat{L}$-the smooth constant is
    non-increasing similar to previous estimation method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Estimated $L$ along the GD iterates for LLama-160m with local curvature
    (Schema II).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5% | 2.36M | 10.96 |'
  prefs: []
  type: TYPE_TB
- en: '| 10% | 8.26M | 791.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 20% | 17.69M | 878.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 30% | 24.77M | 1202.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 40% | 36.57M | 1918.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 60% | 60.75M | 5262.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 70% | 85.52M | 5325.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 85% | 102.04M | 5901.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 100% | 113.25M | 11522.45 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Estimated $L$ along the GD iterates for TinyLlama-1.1B with local
    curvature (Schema II).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5% | 13.11M | 40.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 10% | 49.28M | 146.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 20% | 136.84M | 4366.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 30% | 242.75M | 4487.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 40% | 369.10M | 6767.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 60% | 582.48M | 8983.85 |'
  prefs: []
  type: TYPE_TB
- en: '| 70% | 684.20M | 15445.54 |'
  prefs: []
  type: TYPE_TB
- en: '| 85% | 831.52M | 21167.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 100% | 968.88M | 28629.24 |'
  prefs: []
  type: TYPE_TB
- en: Appendix H Calibration Data Matters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a fair comparison, we run our algorithm using the same calibration data
    as the baseline algorithms, typically a sample from RedPajama [[11](#bib.bib11)].
    However, the way we handle this calibration data differs from most baselines [[19](#bib.bib19),
    [16](#bib.bib16), [65](#bib.bib65)].
  prefs: []
  type: TYPE_NORMAL
- en: When analyzing their codebase, we found that these algorithms resample calibration
    data by taking a random excerpt of a fixed length from a random document in the
    calibration data, both sampled uniformly. However, with this approach, the data
    from longer documents (e.g. books) are underrepresented compared to shorter ones
    (e.g. news articles), which biases the calibration data.
  prefs: []
  type: TYPE_NORMAL
- en: Upon further investigation, we believe that new methods blindly copied this
    code from each other, going back to GPTQ [[20](#bib.bib20)] and possibly further.
    This choice was harmless for GPTQ since it requires relatively little calibration
    data; however, full model fine-tuning like in QuIP# [[8](#bib.bib8)] and AQLM [[19](#bib.bib19)],
    works better on unbiased data.
  prefs: []
  type: TYPE_NORMAL
- en: To remove the bias, we use standard⁸⁸8from e.g. [https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)
    LM preprocessing that concatenates all documents, then splits them into evenly
    sized chunks that become training sequences. The benefits from debiasing range
    from insignificant to as large as 0.15 perplexity for some models. To compensate
    for that, we run experiments with the same preprocessing protocol unless explicitly
    stated otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Additional Engineering Considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When done naively, the longest operation is the discrete update ([8](#S3.E8
    "Equation 8 ‣ 3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    that runs discrete optimization on all LLM parameters. For scalar quantization,
    this step does simple rounding and runs nearly instantly. In turn, applying it
    to vector quantization requires solving a discrete optimization algorithm (e.g.
    beam search) for every group of weights. However, since equation ([8](#S3.E8 "Equation
    8 ‣ 3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) can only update
    weights within $S^{k}$, we can skip discrete optimization for any weight that
    was not among the chosen few. As a result, when training models with up to 70
    billion parameters, we search less than one billion times per step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next longest operation is computing the gradients $\nabla\phi(\cdot)$,
    needed both for P and V steps. This involves running LLM multiple forward and
    backward passes on batches of texts and accumulating the gradients. To reduce
    the overhead from gradient computation, we compute the gradients once using mixed
    precision, then reuse these gradients for one P and one V step, respectively.
    In other words, we switch from alternating P and V steps to performing these steps
    simultaneously. We also reuse these gradients to update any parameters not affected
    by quantization: input embeddings, normalization layers, biases, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: To limit VRAM usage, we use gradient checkpointing [[25](#bib.bib25)], batch
    accumulation. For larger models, we also use parameter sharding⁹⁹9We use PyTorch
    FullyShardedDataParallel [[48](#bib.bib48), [37](#bib.bib37)] and wrap discrete
    weights as in bitsandbytes [[17](#bib.bib17)]  [[51](#bib.bib51)] and optimizer
    offloading [[52](#bib.bib52)]. We need these techniques so that smaller 7B LLMs
    can be trained on a single GPU and larger ones with 70B parameters fit into a
    single machine with 8${\times}$ longer than tuning continuous parameters and uses
    more memory (during training) to hold the gradients w.r.t. dequantized weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix J Additional Details for Section [4.1](#S4.SS1 "4.1 Evaluating quantized
    representations with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we describe some of the implementation details we used to optimize different
    quantized representations. For every optimizations, we check that this optimization
    improves the algorithm in both MSE and perplexity and does not require additional
    resources that would result in unfair comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Quantziation. The original algorithm quantizes all weights a single pass
    over input channels. We found that it works slightly better if we make multiple
    such passes and, between passes, update codes by Adam to minimize the same objective [[66](#bib.bib66)].
    This is resembles QuIP# with no RHT & lattices or AQLM with no additive quantization.
    For simplicity, we also use a single shared codebook (e.g. instead of groupwise
    codebooks).
  prefs: []
  type: TYPE_NORMAL
- en: VQ+outliers To select outlier coordinates, we use [https://github.com/fmfi-compbio/admm-pruning](https://github.com/fmfi-compbio/admm-pruning)
    that outperforms the SpQR outlier criterion [[16](#bib.bib16)] in both L2 error
    and perplexity (when both criteria are used alongside vector quantization). We
    re-run the ADMM procedure multiple times during optimization, resulting in an
    EM-like algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'VQ+lowrank. We experimented with two different initializations for low-rank
    correction: a) quantizing weight matrix, then training LoRC on quantization error,
    as in [[73](#bib.bib73)] and b) initializing LoRC to approximate the reference
    weight matrix, the applying vector quantization to LoRC errors. Of these two approaches,
    we found that the latter one produces a (slightly) better solution in both MSE
    and perplexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix K Additional Details for Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning
    Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'VQ: vector quantization as a simple near-optimal algorithm. We use a single
    16-bit codebook with group size 16 (over input dimension) and per-channel trainable
    scales over output dimension. During P step, we update the codebook, scales and
    non-quantized model layers by backprop. During V step, we try every code in the
    codebook and choose the one that minimizes ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPTQ: scalar quantization with block-wise scales and zero points. We use 2-bit
    base codes and block size 128\. During P step, we update the scales and non-quantized
    parameters by backprop. In turn, V step performs simple rounding to nearest (scaled)
    integer.^(10)^(10)10We also found that we can perform V step by running the entire
    OPTQ calibration while using $y-\frac{1}{L}\nabla\phi(y)$ as target weights, showing
    the flexibility of the general PV framework. This variant converges to the same
    values, but is much slower due to having to re-accumulate the L2 error hessians
    for each V step..'
  prefs: []
  type: TYPE_NORMAL
- en: 'AQLM: we perform scalar quantization with two 8-bit codebooks and group size
    8 and channel-wise steps. This was originally published in [[19](#bib.bib19)]
    as the “speed-optimized” configuration capable of fast lookup-based inference.
    During P step, we update both codebooks, as well as scales an non-quantized parameters
    by backprop. On V step, we run beam search with beam size 8 with gradient-updated
    dequantized weight as target.'
  prefs: []
  type: TYPE_NORMAL
- en: Training. We minimize Kullback–Leibler divergence as our loss function for all
    three representations. More specifically, we fine-tune the quantized “student”
    model to approximate the predictions (logits) of a “teacher” — the same model
    prior to quantization. We fine-tune on the same RedPajama sample as in calibration.
    More specifically, we use the official one-billion-token sample^(11)^(11)11https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample
    provided by the dataset authors [[11](#bib.bib11)]. We use a batch size of $2^{20}$
    smaller batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning: we tune the hyperparameters for each method individually.
    For all algorithms, we tune learning rate on a logarithmic scale out of (1e-4,
    3e-4, 1e-3, 3e-3, 1e-2). For methods involving discrete optimization, we tune
    learning rate for P and V step separately. The optimal configuration for STE and
    stochastic rounding is to use learning rate 3e-4 for both codes and codebooks.
    The optimal configuration for subspace linearized PV and the same with STE is
    to use learning rate 3e-4 for P step and 3e-3 for codes. Curiously, the subspace
    methods are stable even with larger step sizes for codes, e.g. 1e-2, whereas unrestricted
    methods (e.g. pure STE) are not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For stochastic rounding, we found that the unbiased rounding [[49](#bib.bib49)]
    causes the model quality to quickly degrade, likely due to the fact that the algorithm
    makes too many weight changes due to rounding. The results we reported in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    use stochastic rounding with temperature 0.2\. In other words, we measure the
    distances to 2 nearest bins and round to each bin proportionally to $\text{distance}^{-5}$.
    We also tried gradually annealing the rounding temperature to 0, but achieved
    only insignificant improvements in accuracy and perplexity (<0.01). To simplify
    evaluation, we do not use annealing in Table[1](#S4.T1 "Table 1 ‣ 4.1 Evaluating
    quantized representations with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: For PV-tuning and PV-tuning with STE, we always set $\tau$. However, we didn’t
    see this in practice.
  prefs: []
  type: TYPE_NORMAL
- en: We also found that Lamb^(12)^(12)12Lamb is a variant of Adam that limits the
    norm of weight updates relative to the norm of weights. [[75](#bib.bib75)] is
    more stable when training with large batch sizes, but converges to approximately
    the same accuracy. We use $\beta_{1}{=}0.9$, same as in most LLM training configurations [[63](#bib.bib63),
    [79](#bib.bib79), [56](#bib.bib56)]. We do not use learning rate decay for simplicity.
    It is likely possibly to improve our results by annealing the learning rates during
    training or using a warmup. We intentionally avoid this to reduce the number of
    “moving parts” and simplify evaluation. In future, we plan to release PV-tuned
    models with a more careful choice of training hyperparameters. Overall, we found
    that PV-tuning is about as sensitive to hyperparameters as continuous-only LLM
    finetuning [[65](#bib.bib65), [19](#bib.bib19), [58](#bib.bib58)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix L Additional Details and Evaluations for Section [4.3](#S4.SS3 "4.3
    Large-scale Evaluation & Discussion ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we report additional results for Llama 2 & 3, Mistral and Phi-3
    and discuss baselines. In this section, we always evaluate PV-tuning for vector
    quantization, using 14-16 bits per codebook for a group of 8 or 16 weights, with
    each combination fitting a particualr niche. For instance, 16 bits per 8 weights
    is slightly over 2 bits per weight, whereas 14 bits per 16 weights is either at
    or below 1 bit per weight, depending on the model size.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Llama 2 models as our main benchmark as they are well studied in the
    PTQ community. Here, we gather the latest state-of-the-art algorithms at the time
    of publication and group them according to their target number of bits, roughly
    1-1.7 bits per weight (Table [7](#A12.T7 "Table 7 ‣ Appendix L Additional Details
    and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) and 2-2.5 bits per weight (Table [8](#A12.T8
    "Table 8 ‣ Appendix L Additional Details and Evaluations for Section 4.3 ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: Both our training runs and almost all baselines use the same sample of RedPajama
    data from previous sections^(13)^(13)13[https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample).
    The only exception to this is OneBit that uses a corpora of LLM outputs gathered
    specifically for that paper [[71](#bib.bib71)].
  prefs: []
  type: TYPE_NORMAL
- en: The source code for this method was unavailable until very recently. The official
    link [https://github.com/xuyuzhuang11/OneBit](https://github.com/xuyuzhuang11/OneBit)
    used to point to an empty repository until the code was released in a commit [https://github.com/xuyuzhuang11/OneBit/commit/380a6aedc3c060993056ff50b79065e893be99ae](https://github.com/xuyuzhuang11/OneBit/commit/380a6aedc3c060993056ff50b79065e893be99ae)
    on May 10th. Thus, unfortunately, we did not have time to make OneBit compatible
    with models except Llama 2 7B and 13B that were featured in the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: For Llama 3, we evaluate PV-tuning of vector quantization against the baselines
    introduced in [[30](#bib.bib30)]. Curiously, their paper seems to compute perplexity
    differently than our paper. Since our protocol matches with most prior works [[20](#bib.bib20),
    [16](#bib.bib16), [19](#bib.bib19), [65](#bib.bib65)], we chose to re-evaluate
    the results from [[30](#bib.bib30)] with our perplexity code and not the other
    way around. We calibrate using the official code ^(14)^(14)14[https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)
    and reuse published models where available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Evaluation of quantized Llama 2 models for 1-1.7 bits per weight,
    grouped by bitwidth. We report perplexity on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)]
    and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks.
    Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | – | 16.00 | 5.12 | 6.63 | 43.43 | 76.3 | 57.14 | 78.07 | 69.06 | 64.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.08 | 32.48 | 40.52 | 24.4 | 36.2 | 34.8 | 60.6 | 52.4 | 41.68 |'
  prefs: []
  type: TYPE_TB
- en: '| OneBit | 1.0 | 9.73 | 11.11 | 29.61 | 41.58 | 52.58 | 68.12 | 58.41 | 50.06
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.02 | 8.28 | 10.37 | 25.85 | 57.58 | 40.88 | 68.99 | 57.77 |
    50.21 |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 1.70 | 69.20 | 80.15 | 25.00 | 28.00 | 27.70 | 53.80 | 49.30 | 36.76
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.58 | 7.32 | 9.35 | 29.44 | 64.14 | 46.03 | 73.12 | 63.38 |
    55.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | – | 16.00 | 4.57 | 6.05 | 48.38 | 79.42 | 60.03 | 79.05 | 72.22 | 67.82
    |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.10 | 16.77 | 27.54 | 21.84 | 46.84 | 30.97 | 60.61 | 56.75 | 43.40
    |'
  prefs: []
  type: TYPE_TB
- en: '| OneBit | 1.00 | 8.76 | 10.15 | 33.62 | 43.10 | 56.43 | 70.13 | 61.72 | 53.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 0.97 | 7.23 | 9.31 | 30.8 | 63.09 | 47.03 | 72.25 | 62.35 | 55.10
    |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 1.70 | 151.09 | 144.59 | 21.89 | 35.08 | 24.82 | 54.17 | 52.76 |
    37.74 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.37 | 6.65 | 8.72 | 34.04 | 67.38 | 49.14 | 73.94 | 65.51 |
    58.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 70B | – | 16.00 | 3.12 | 4.97 | 54.35 | 82.74 | 64.79 | 82.15 | 77.98 | 72.40
    |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.08 | 8.41 | 15.19 | 38.91 | 67.3 | 45.71 | 69.7 | 67.64 | 57.85
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.01 | 6.09 | 8.20 | 38.31 | 71.80 | 53.98 | 75.24 | 68.43 |
    61.55 |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 1.70 | 28.37 | 32.63 | 39.89 | 49.50 | 36.62 | 61.43 | 62.80 | 50.05
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.14 | 5.52 | 7.50 | 42.66 | 74.96 | 56.42 | 77.37 | 71.51 |
    64.58 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Evaluation of quantized Llama 2 models for 2-2.3 bits per weight,
    grouped by bitwidth. We report perplexity on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)]
    and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks.
    Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | – | 16.00 | 5.12 | 6.63 | 43.43 | 76.30 | 57.14 | 78.07 | 69.06 | 64.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| QUIP# | 2.02 | 8.22 | 11.01 | 34.60 | 64.60 | 48.34 | 75.10 | 64.90 | 57.51
    |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.02 | 6.64 | 8.56 | 33.28 | 61.87 | 49.49 | 73.56 | 64.17 | 56.47
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.02 | 5.84 | 7.62 | 38.40 | 71.17 | 53.50 | 76.99 | 66.69 |
    61.35 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.29 | 6.29 | 8.11 | 34.90 | 66.50 | 50.88 | 74.92 | 65.67 | 58.57
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.29 | 5.84 | 7.62 | 38.91 | 72.90 | 53.94 | 77.37 | 67.72 |
    62.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 13B | – | 16.00 | 4.57 | 6.05 | 48.38 | 79.42 | 60.03 | 79.05 | 72.22 | 67.82
    |'
  prefs: []
  type: TYPE_TB
- en: '| QUIP# | 2.01 | 6.06 | 8.07 | 39.50 | 69.30 | 53.44 | 77.30 | 67.70 | 61.45
    |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 1.97 | 5.65 | 7.51 | 37.80 | 69.78 | 53.74 | 76.22 | 65.43 | 60.59
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.97 | 5.12 | 6.83 | 43.00 | 75.38 | 57.96 | 78.24 | 70.01 |
    64.92 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.19 | 5.41 | 7.21 | 41.98 | 75.04 | 55.49 | 76.99 | 69.53 | 63.81
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.19 | 5.05 | 6.74 | 45.65 | 77.57 | 58.00 | 78.07 | 70.96 |
    66.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 70B | – | 16.00 | 3.12 | 4.97 | 54.35 | 82.74 | 64.79 | 82.15 | 77.98 | 72.40
    |'
  prefs: []
  type: TYPE_TB
- en: '| QUIP# | 2.00 | 4.16 | 6.01 | 48.70 | 77.30 | 60.79 | 80.30 | 75.90 | 68.60
    |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.07 | 3.94 | 5.72 | 51.96 | 81.44 | 61.46 | 80.25 | 76.64 | 70.35
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.07 | 3.78 | 5.56 | 51.88 | 81.02 | 63.07 | 80.74 | 76.87 |
    70.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Evaluation of quantized Llama 3 models for 1-2.3 bits per weight,
    grouped by bitwidth. We report perplexity on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)]
    and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks.
    Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| 8B | – | 16.00 | 5.54 | 7.10 | 50.43 | 80.09 | 60.19 | 79.71 | 72.61 | 68.60
    |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.10 | 28.80 | 65.00 | 17.70 | 36.00 | 28.90 | 56.10 | 51.00 | 37.90
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.01 | 11.13 | 11.63 | 25.43 | 59.09 | 41.01 | 68.26 | 56.27
    | 50.01 |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 1.70 | 35.68 | 197.56 | 17.50 | 31.70 | 27.70 | 52.50 | 50.40 |
    36.00 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.54 | 9.43 | 10.26 | 32.68 | 65.78 | 46.66 | 72.63 | 64.40 |
    56.43 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 2.00 | 76.95 | 98.47 | 21.30 | 29.00 | 29.20 | 52.90 | 51.70 | 36.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 2.00 | 21.74 | 61.04 | 17.20 | 37.80 | 29.80 | 57.00 | 52.50 | 38.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| DB-LLM | 2.00 | 12.77 | 14.82 | 28.20 | 59.10 | 42.10 | 68.90 | 60.40 | 51.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.00 | 6.99 | 8.29 | 42.75 | 75.84 | 55.52 | 77.75 | 69.93 |
    64.36 |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.30 | 6.76 | 8.10 | 42.32 | 75.46 | 56.21 | 78.45 | 71.67 |
    64.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 70B | – | 16.00 | 2.59 | 5.78 | 60.41 | 86.7 | 66.34 | 82.48 | 80.9 | 75.366
    |'
  prefs: []
  type: TYPE_TB
- en: '| BiLLM | 1.10 | 15.26 | 65.07 | 25.11 | 46.42 | 37.48 | 58.21 | 53.63 | 44.17
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.00 | 8.67 | 9.68 | 25.51 | 54.34 | 48.71 | 65.56 | 63.22 |
    51.47 |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 1.70 | 16.27 | 54.03 | 25.8 | 49.90 | 34.90 | 56.5 | 53.10 | 44.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.14 | 7.76 | 8.93 | 33.28 | 63.89 | 53.39 | 69.86 | 69.61 |
    58.01 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP | 2.00 | 11.63 | 18.54 | 26.50 | 48.90 | 40.90 | 65.30 | 61.70 | 48.70
    |'
  prefs: []
  type: TYPE_TB
- en: '| PB-LLM | 2.00 | 10.33 | 28.89 | 25.10 | 40.60 | 42.70 | 65.20 | 56.40 | 46.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.07 | 4.55 | 6.54 | 50.77 | 80.22 | 63.85 | 79.22 | 78.06 |
    70.42 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Evaluation of quantized Mistral v0.1 7B (A) and Phi 3-Mini-4k-Instruct
    3.8B (B) models for 1-2.3 bits per weight, grouped by bitwidth. We report perplexity
    on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)] and zero-shot accuracy.
    The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2
    (PPL), C4 (PPL) and Average (Accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\underset{(A)}{\text{7B}}$ | – | 16.00 | 4.77 | 5.71 | 50.43 | 80.09 | 60.19
    | 79.71 | 72.61 | 68.60 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 1.01 | 70.88 | 34.67 | 19.11 | 27.36 | 26.3 | 52.99 | 48.38 | 34.83
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.01 | 7.58 | 8.14 | 27.73 | 60.82 | 44.33 | 69.97 | 60.38 |
    52.65 |'
  prefs: []
  type: TYPE_TB
- en: '| QuIP# | 2.01 | 6.02 | 6.84 | 39.76 | 72.14 | 52.95 | 76.71 | 69.30 | 62.20
    |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.01 | 6.19 | 6.90 | 30.8 | 49.87 | 25.63 | 56.53 | 57.06 | 43.98
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.01 | 5.29 | 6.17 | 44.20 | 77.36 | 58.21 | 79.05 | 72.77 |
    66.32 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.27 | 5.78 | 6.55 | 42.06 | 75.17 | 55.09 | 76.93 | 70.24 | 63.90
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.27 | 5.22 | 6.10 | 45.31 | 77.57 | 58.61 | 79.22 | 70.96 |
    66.33 |'
  prefs: []
  type: TYPE_TB
- en: '| $\underset{(B)}{\text{3.8B}}$ | – | 16.00 | 4.77 | 5.71 | 60.41 | 86.7 |
    66.34 | 82.48 | 80.90 | 75.37 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 1.03 | 102.54 | 85.20 | 18.86 | 28.16 | 27.13 | 53.54 | 49.80 | 35.50
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.03 | 11.71 | 14.59 | 21.50 | 49.87 | 34.62 | 65.67 | 54.70
    | 45.27 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 1.60 | 34.36 | 35.16 | 19.62 | 35.10 | 29.71 | 57.24 | 51.7 | 38.67
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 1.60 | 9.21 | 12.16 | 30.89 | 63.55 | 43.09 | 70.35 | 61.4 |
    53.86 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.03 | 8.85 | 12.19 | 41.04 | 74.49 | 47.25 | 72.36 | 66.93 | 60.41
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.03 | 6.88 | 10.08 | 46.84 | 78.24 | 53.75 | 78.67 | 71.03 |
    65.71 |'
  prefs: []
  type: TYPE_TB
- en: '| AQLM | 2.30 | 8.07 | 11.23 | 47.01 | 78.03 | 50.51 | 75.95 | 70.8 | 64.46
    |'
  prefs: []
  type: TYPE_TB
- en: '| PV-Tuning | 2.30 | 6.63 | 9.89 | 50.51 | 79.5 | 55.32 | 79.49 | 73.01 | 67.57
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix M Inference Speed with Vector Quantization Kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we demonstrate that PV-Tuning can achieve speedups by using
    fast inference kernels from the underlying quantized representation. Since our
    main experiments use vector quantization, we adopt simplified version of AQLM
    inference kernels with one codebook of size 16 for groups of 8 consecutive weights.
    This kernel is not written by us: it was added to the official AQLM implementation
    by an open-source contributor.'
  prefs: []
  type: TYPE_NORMAL
- en: We adapt this inference code to our codebase and switch it to using group size
    16 to support out 1.1-1.58 bit models. We evaluate inference speeds on a single
    Nvidia RTX 3090 GPU using transformers with cuda graphs^(15)^(15)15The specific
    version of each library can be found in ‘requirements.txt‘.
  prefs: []
  type: TYPE_NORMAL
- en: 'We were able to acheve 47.4 tokens per second for 7B model, 32.8 tokens per
    second for 13B model and 7.2 tokens per second for Llama 2 70B model. Compared
    to 16-bit inference code, this results in speedups of 14%, 22% and 28% respectively.
    Note that PV-Tuning does not make the model inherently faster than, for instance,
    AQLM. However, it can enable faster inference indirectly: by achieving sufficient
    quality with lower bit models and allowing practitioners to deploy smaller and
    faster models.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix N The Choice of the Initial Point $x^{0}$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression") converges from any initial point $x^{0}\in\mathbb{R}^{d}_{c}$. In
    this section, we discuss two possible variants of instantiating the initial point.'
  prefs: []
  type: TYPE_NORMAL
- en: N.1 Clipping of $x^{\star}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Notation: $[d]:=\{1,\cdots,d\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have the vector $x$ in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C(x)=\tilde{x}:\quad V(\tilde{x})\subseteq V(x)\quad\text{and}\quad&#124;V(\tilde{x})&#124;\leq
    c$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: We can come up with different variants of clipping operators $C(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: Example N.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If $x=\{1,1,3,5,9\}$.
  prefs: []
  type: TYPE_NORMAL
- en: N.2 Random $x^{0}\in\mathbb{R}^{d}_{c}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let us define the algorithm for generating random points from $\mathbb{R}^{d}_{c}$
    (Alg. [6](#alg6 "Algorithm 6 ‣ N.2 Random 𝑥⁰∈ℝ^𝑑_𝑐 ‣ Appendix N The Choice of
    the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 6 Generation of Random Point $x^{0}\in\mathbb{R}^{d}_{c}$
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Parameters: dimensionality $d$.'
  prefs: []
  type: TYPE_NORMAL
- en: Different random $x^{0}\in\mathbb{R}^{d}_{c}$ as the number of runs with different
    random initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix O Small-Scale Experiments and Interpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: O.1 Objective function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let $c\in[d]$ and consider the problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{x\in\mathbb{R}^{d}_{\leq c}}{\phi(x)},$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $\phi:\mathbb{R}^{d}\to\mathbb{R}$ distinct values. In other words, the
    cardinality of the set
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V(x):=\{x_{1},\cdots,x_{d}\}$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: is at most $c$. For small experiments, we aim to minimize the following objective
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where $a_{i}\in\mathbb{R}^{d}$ and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\Lambda=\text{diag}(a_{1},\cdots,a_{d})=\begin{bmatrix}a_{1}&amp;\cdots&amp;0\\
    \vdots&amp;\ddots&amp;0\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;0&amp;a_{d}\end{bmatrix}.$$ |  | (19) |
  prefs: []
  type: TYPE_NORMAL
- en: 'We set $a_{i}=\nicefrac{{i}}{{d}}$ denote the vector to which the Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    converges.'
  prefs: []
  type: TYPE_NORMAL
- en: O.2 Tiny-scale experiments ($d=6$)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We applied PV algorithm to the problem ([16](#A15.E16 "Equation 16 ‣ O.1 Objective
    function ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")) with
    $\phi(x)$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86a3345ebd7c17d2bb1641106ba6830c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PV algorithm with different values of $c\in[1,6]$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f898afad03643ed21a4a55c194e8eff.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The influence of P and V steps on the loss function $\phi(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣
    3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) applied on the very small dimensional ($d=6$ is
    chosen randomly using the ng algorithm ([6](#alg6 "Algorithm 6 ‣ N.2 Random 𝑥⁰∈ℝ^𝑑_𝑐
    ‣ Appendix N The Choice of the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: When $c=1$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that as we increase the maximum number of unique elements $c$, when the
    loss is zero (the plot [3(a)](#A15.F3.sf1 "Figure 3(a) ‣ Figure 3 ‣ O.2 Tiny-scale
    experiments (𝑑=6, 𝑐∈[1,6]) ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    is in logarithmic scale and that is why we cannot see the last line).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We run PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) with different random starting points $x^{0}\in\mathbb{R}^{d}_{c}$,
    we used this value for all further experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the two steps of the PV algorithm contributes to the convergence. To
    observe this we plotted the loss function over the iterates and explicitly marked
    the progress of both P and V steps (Fig. [3(b)](#A15.F3.sf2 "Figure 3(b) ‣ Figure
    3 ‣ O.2 Tiny-scale experiments (𝑑=6, 𝑐∈[1,6]) ‣ Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). We can see that we have progress during each of
    these steps and one single P and V step is not enough to obtain a solution even
    in this very small and simple case.'
  prefs: []
  type: TYPE_NORMAL
- en: These simple experiments demonstrate that
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: larger $c$ (smaller compress ratio) provides better final accuracy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: several P and V steps are needed to converge to the solution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: O.3 Small-scale experiments ($d=100$)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The problem of the algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) is that the V step requires the full parameter
    search which gives us the complexity $\mathcal{O}(c^{d})$. Even for small tasks,
    this becomes unpractical to solve.'
  prefs: []
  type: TYPE_NORMAL
- en: Example O.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let us take $d=100$ years to make a single V step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider special sets of function $\phi(x)$ that we will call separable
    functions. This class of functions should satisfy the assumption ([O.2](#A15.Thmtheorem2
    "Assumption O.2 (Separable function). ‣ O.3 Small-scale experiments (𝑑=100, 𝑐∈[1,100])
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption O.2  (Separable function).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The function $\phi(x):\mathbb{R}^{d}\to\mathbb{R}$.
  prefs: []
  type: TYPE_NORMAL
- en: Example O.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The objective function ([18](#A15.E18 "Equation 18 ‣ O.1 Objective function
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) is a sum of
    squares that can be written in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi_{i}(x_{i})=a_{i}(x_{i}-x^{\star}_{i})^{2}$. Hence, the objective
    ([18](#A15.E18 "Equation 18 ‣ O.1 Objective function ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) is a separable function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One can show that for separable functions ([O.2](#A15.Thmtheorem2 "Assumption
    O.2 (Separable function). ‣ O.3 Small-scale experiments (𝑑=100, 𝑐∈[1,100]) ‣ Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) the algorithm ([1](#alg1 "Algorithm
    1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")) can be written in the
    form ([7](#alg7 "Algorithm 7 ‣ O.3 Small-scale experiments (𝑑=100, 𝑐∈[1,100])
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")). Hence, for
    separable functions, we can compute the V step in $\mathcal{O}(c\cdot d)$), which
    makes the algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) practical to use.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 7 PV Algorithm with Optimized V step
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Initialization: starting point $x^{0}\in\mathbb{R}^{d}_{c}$ (Optimized
    V step)6:     end for7:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we ran the optimized PV algorithm ([7](#alg7 "Algorithm 7 ‣ O.3 Small-scale
    experiments (𝑑=100, 𝑐∈[1,100]) ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    on the quadratic objective ([18](#A15.E18 "Equation 18 ‣ O.1 Objective function
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")). The results
    are presented in (Figure [4](#A15.F4 "Figure 4 ‣ O.3 Small-scale experiments (𝑑=100,
    𝑐∈[1,100]) ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).
    Number of runs with different random initial points $r=50$.'
  prefs: []
  type: TYPE_NORMAL
- en: For these experiments, we see the same results as for tiny scale with $d=6$.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it was mentioned before, the algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem
    description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) is conceptual only and cannot be used
    in the raw form in practice. That is why we need to move to the experiments with
    linearized V step.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2aa185008f5a15c3872200e20feca913.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PV algorithm with different values of $c\in[10,100]$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/599f63a45396efe4ea84ce8ee8588144.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The influence of P and V steps on the loss function $\phi(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Optimized PV algorithm ([7](#alg7 "Algorithm 7 ‣ O.3 Small-scale
    experiments (𝑑=100, 𝑐∈[1,100]) ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    applied on the quadratic objective ([18](#A15.E18 "Equation 18 ‣ O.1 Objective
    function ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")),
    $d=100$.'
  prefs: []
  type: TYPE_NORMAL
- en: O.4 Linearized PV
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linearized V step ([B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) allows us to greatly reduce the cost
    of one V step. We ran Linearized PV on the quadratic objective ([18](#A15.E18
    "Equation 18 ‣ O.1 Objective function ‣ Appendix O Small-Scale Experiments and
    Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")) (Fig. [5](#A15.F5 "Figure 5 ‣ O.4 Linearized PV ‣ Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a99b9060b05905a92f9adf443cb8e81.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Linearized PV algorithm ([B.1](#A2.SS1 "B.1 Approximate V step, variant
    1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) with different
    $T\in[1,5]$ provides faster convergence rates, but the same final accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a61fe5caab4a6c3ca870b14aab93ecf4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The influence of P and V steps on the loss function $\phi(x)$ linearized
    V steps on each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Experiments with Linearized PV algorithm ([B.1](#A2.SS1 "B.1 Approximate
    V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the first experiment (Fig. [5(a)](#A15.F5.sf1 "Figure 5(a) ‣ Figure 5
    ‣ O.4 Linearized PV ‣ Appendix O Small-Scale Experiments and Interpretation ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    we can see that'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: increasing $T$ to save computations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linearized PV algorithm ([B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) converges to a worse accuracy than the
    exact PV ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linearized PV has to make more iterations than PV to converge
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second plot (Fig.[5(b)](#A15.F5.sf2 "Figure 5(b) ‣ Figure 5 ‣ O.4 Linearized
    PV ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) demonstrates
    the effect of multiple Linearized V step. We can see that the largest effect comes
    from the first V step.'
  prefs: []
  type: TYPE_NORMAL
- en: O.5 Linearized PV + sparse updates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In previous section ([O.4](#A15.SS4 "O.4 Linearized PV ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) we have seen that Linearized PV algorithm
    converges to a worse accuracy than the exact PV ([1](#alg1 "Algorithm 1 ‣ 3.1
    Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearized PV ([B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) with combination of sparse updates ([3.3](#S3.SS3
    "3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) is intended
    to mitigate this issue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On (Fig. [6(a)](#A15.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ O.5 Linearized PV + sparse
    updates ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) we can see comparison
    of three methods: exact PV ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣
    3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) – red line, Linearized PV ([B.1](#A2.SS1 "B.1 Approximate
    V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    – blue line and Linearized PV with sparse updates ([3.3](#S3.SS3 "3.3 Linearized
    subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) – green line.'
  prefs: []
  type: TYPE_NORMAL
- en: From this experiment we observe
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linearized PV with sparse updates converges to a better accuracy than Linearized
    PV
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: inearized PV + sparse updates has to make more iterations than Linearized PV
    and exact PV to converge
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hence, this approach helps us to converge to a better accuracy, but with a price
    of larger number of iterations to converge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b96b9ca4b4ccecc940bf57d95677391.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Influence of sparse updates
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/385223201fec832196535f08533a1901.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Different choise of sampling for choosing $\mathcal{S}^{k}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparison of the exact PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1
    Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) with Linearized PV ([B.1](#A2.SS1 "B.1
    Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    and Linearized PV + sparse updates ([3.3](#S3.SS3 "3.3 Linearized subspace V step
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use different rules for choosing the subspace $\mathcal{S}^{k}$ which
    produce different convergence rates and the final accuracy levels ([6(b)](#A15.F6.sf2
    "Figure 6(b) ‣ Figure 6 ‣ O.5 Linearized PV + sparse updates ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/640063e0b0e21e1dfc9f48fcf500cef7.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The behaviour of $L_{\mathcal{S}^{k}}$ in Linearized PV + sparce updates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b7e4031d81614f0784601761abc66d4.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Degradation of dimensionality of $V(x)$ for Linearized PV algorithm ([B.1](#A2.SS1
    "B.1 Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Comparison of the exact PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1
    Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) with Linearized PV ([B.1](#A2.SS1 "B.1
    Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    and Linearized PV + Sparse Updates ([3.3](#S3.SS3 "3.3 Linearized subspace V step
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In large scale experiments we used sparse updates with $\mathcal{S}^{k}$ sampling
    can be even more advanced providing better final accuracy at cost of larger number
    of iterations to converge – purple and orange lines (Fig. [6(b)](#A15.F6.sf2 "Figure
    6(b) ‣ Figure 6 ‣ O.5 Linearized PV + sparse updates ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that Linearized PV with sparse updates can converge to even a better
    accuracy than the exact PV algorithm. The problem of Linearized PV algorithm is
    that we come to the local minimum and cannot get out of that minima because of
    the small value of the gradient (we are near to a real solution) and small stepsize.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearized PV with sparse updates allows us to mitigate this problem by reducing
    the subspace from $\mathbb{R}^{d}$ changes for Linearized PV with different sparse
    updates sampling methods (Fig. [7(a)](#A15.F7.sf1 "Figure 7(a) ‣ Figure 7 ‣ O.5
    Linearized PV + sparse updates ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix P PV^+ Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can have degradation of $|V(x^{k})|$ in ([18](#A15.E18 "Equation 18 ‣ O.1
    Objective function ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    are equal to one).'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Degradation during the P step: Let $x^{\star}=\{0,2,1\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Degradation during the V step: Let $x^{\star}=\{2,10,0,11\}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In real experiments we observe decreasing of $|V(x)|$ during the first several
    iterations. We can use this gap to find even better solution with improved final
    accuracy and faster convergence rate.
  prefs: []
  type: TYPE_NORMAL
- en: To add additional unique element in $V(x)$ to become larger up to some upper
    bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\phi(\cdot)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W(x,x^{\star},c)=V(x^{\star})\setminus V(x):\quad&#124;W(x,x^{\star},c)&#124;=c-&#124;V(x)&#124;$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: Algorithm 8 PV^+ Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Parameters: starting point $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (Modified
    V step)5:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem P.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume $\phi$. Then the algorithm PV^+ has the following guarantees
  prefs: []
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $y^{k},x^{k}\in\mathbb{R}^{d}_{\leq\hat{c}}$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k})$, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (iii)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the sequence $\{\phi(x^{k})\}_{k\geq 0}$ converges to some value, which is smaller
    or equal to one produced by PV algorithm
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Part (ii): Since'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and because $x=y^{k}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the proof is identical to ([A.1](#A1.SS1 "A.1 Proof of Theorem
    3.1 ‣ Appendix A Proofs ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). ∎'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix Q Broader Impact
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main impact of our work, both positive and negative, is in the ability to
    deploy higher-quality LLMs to run on memory-limited devices like desktops, laptops,
    and phones. On the positive side, this would allow practitioners to develop offline
    LLM applications (e.g. translate service), lower-latency chat assistants that
    are not dependant on network latency, or privacy-sensitive LLM applications where
    the user’s private data never leaves their device. Furthermore, this can facilitate
    the creation of free open-source software based on LLMs by eliminating the need
    to maintain costly inference servers on the backend. Since phones are everywhere
    and LLMs are powerful general-purpose tools, PV-tuned models could significantly
    impact how the general population uses LLMs to complete tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, LLMs are still a dual-use technology with the potential for significant
    benefits and serious harm. Risks range from deliberate misuse (e.g. spam generation)
    and accidental misuse to negative economic side-effects. An upper bound on these
    risks is that PV tuning does not create new (potentially risky) LLM capabilities,
    merely making existing ones more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: NeurIPS Paper Checklist
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Claims
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Do the main claims made in the abstract and introduction accurately
    reflect the paper’s contributions and scope?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: Our main claims are that the PV-tuning algorithm 1) achievstate-of-the-artart
    quantized LLM quality for 1-2 bits per parameter (backed by Section [4.3](#S4.SS3
    "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")), 2) Paretoeto optimal at around 2 bits
    (also Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")),
    3) and is compatible with various methods (Section [4.2](#S4.SS2 "4.2 Evaluating
    Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). In the introduction, we also claim that the advanced
    quantized representations, such as those having sparse outliers, do not give significant
    benefit on top of simple vector quantization with finetuning: this part is backed
    by [4.1](#S4.SS1 "4.1 Evaluating quantized representations with finetuning ‣ 4
    Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the abstract and introduction do not include the claims
    made in the paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The abstract and/or introduction should clearly state the claims made, including
    the contributions made in the paper and important assumptions and limitations.
    A No or NA answer to this question will not be perceived well by the reviewers.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The claims made should match theoretical and experimental results, and reflect
    how much the results can be expected to generalize to other settings.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is fine to include aspirational goals as motivation as long as it is clear
    that these goals are not attained by the paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper discuss the limitations of the work performed by the
    authors?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We discuss the methodological limitations of our study near
    the end, after Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion
    ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"). We also explain limitations for practitioners in Section [3.4](#S3.SS4
    "3.4 Implementation details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression").'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper has no limitation while the answer No means
    that the paper has limitations, but those are not discussed in the paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors are encouraged to create a separate "Limitations" section in their
    paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper should point out any strong assumptions and how robust the results
    are to violations of these assumptions (e.g., independence assumptions, noiseless
    settings, model well-specification, asymptotic approximations only holding locally).
    The authors should reflect on how these assumptions might be violated in practice
    and what the implications would be.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should reflect on the scope of the claims made, e.g., if the approach
    was only tested on a few datasets or with a few runs. In general, empirical results
    often depend on implicit assumptions, which should be articulated.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should reflect on the factors that influence the performance of
    the approach. For example, a facial recognition algorithm may perform poorly when
    the image resolution is low or images are taken in low lighting. Or a speech-to-text
    system might not be used reliably to provide closed captions for online lectures
    because it fails to handle technical jargon.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should discuss the computational efficiency of the proposed algorithms
    and how they scale with dataset size.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If applicable, the authors should discuss possible limitations of their approach
    to addressing problems of privacy and fairness.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While the authors might fear that complete honesty about limitations might be
    used by reviewers as grounds for rejection, a worse outcome might be that reviewers
    discover limitations that aren’t acknowledged in the paper. The authors should
    use their best judgment and recognize that individual actions in favor of transparency
    play an important role in developing norms that preserve the integrity of the
    community. Reviewers will be specifically instructed to not penalize honesty concerning
    limitations.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Theory Assumptions and Proofs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: For each theoretical result, does the paper provide the full set
    of assumptions and a complete (and correct) proof?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We carefully introduced the assumptions (e.g. that $\phi(\cdot)$
    is L-smooth) and provided proofs in appendix. To the best of our knowledge, these
    proofs are both correct and complete.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not include theoretical results.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All assumptions should be clearly stated or referenced in the statement of any
    theorems.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The proofs can either appear in the main paper or the supplemental material,
    but if they appear in the supplemental material, the authors are encouraged to
    provide a short proof sketch to provide intuition.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Inversely, any informal proof provided in the core of the paper should be complemented
    by formal proofs provided the in appendix or supplemental material.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Theorems and Lemmas that the proof relies upon should be properly referenced.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experimental Result Reproducibility
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper fully disclose all the information needed to reproduce
    the main experimental results of the paper to the extent that it affects the main
    claims and/or conclusions of the paper (regardless of whether the code and data
    are provided or not)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We train open-access LLMs on open datasets and release our full
    training code. We do our best to provide instructions and hyperparameters in the
    code, though running our algorithm in different conditions may require basic tuning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not include experiments.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the paper includes experiments, a No answer to this question will not be
    perceived well by the reviewers: Making the paper reproducible is important, regardless
    of whether the code and data are provided or not.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the contribution is a dataset and/or model, the authors should describe the
    steps taken to make their results reproducible or verifiable.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the contribution, reproducibility can be accomplished in various
    ways. For example, if the contribution is a novel architecture, describing the
    architecture fully might suffice, or if the contribution is a specific model and
    empirical evaluation, it may be necessary to either make it possible for others
    to replicate the model with the same dataset, or provide access to the model.
    In general. releasing code and data is often one good way to accomplish this,
    but reproducibility can also be provided via detailed instructions for how to
    replicate the results, access to a hosted model (e.g., in the case of a large
    language model), releasing of a model checkpoint, or other means that are appropriate
    to the research performed.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While NeurIPS does not require releasing code, the conference does require all
    submissions to provide some reasonable avenue for reproducibility, which may depend
    on the nature of the contribution. For example
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the contribution is primarily a new algorithm, the paper should make it clear
    how to reproduce that algorithm.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the contribution is primarily a new model architecture, the paper should
    describe the architecture clearly and fully.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the contribution is a new model (e.g., a large language model), then there
    should either be a way to access this model for reproducing the results or a way
    to reproduce the model (e.g., with an open-source dataset or instructions for
    how to construct the dataset).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We recognize that reproducibility may be tricky in some cases, in which case
    authors are welcome to describe the particular way they provide for reproducibility.
    In the case of closed-source models, it may be that access to the model is limited
    in some way (e.g., to registered users), but it should be possible for other researchers
    to have some path to reproducing or verifying the results.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open access to data and code
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper provide open access to the data and code, with sufficient
    instructions to faithfully reproduce the main experimental results, as described
    in supplemental material?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: As we state above, we release the full implementation for the
    PV algorithm with requisite instructions. We do not introduce new datasets and
    use openly available ones. We also plan to release the main quantized models in
    the non-anonymized version of the paper, since it would be impractical to upload
    them with the supplementary zip archive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means the paper does not include experiments requiring code.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While we encourage the release of code and data, we understand that this might
    not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply
    for not including code, unless this is central to the contribution (e.g., for
    a new open-source benchmark).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The instructions should contain the exact command and environment needed to
    run to reproduce the results. See the NeurIPS code and data submission guidelines
    ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should provide instructions on data access and preparation, including
    how to access the raw data, preprocessed data, intermediate data generated data,
    etc.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should provide scripts to reproduce all experimental results for
    the new proposed method and baselines. If only a subset of experiments are reproducible,
    they should state which ones are omitted from the script and why.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At submission time, to preserve anonymity, the authors should release anonymized
    versions (if applicable).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing as much information as possible in supplemental material (appended
    to the paper) is recommended, but including URLs to data and code is permitted.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experimental Setting/Details
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper specify all the training and test details (e.g., data
    splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary
    to understand the results?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We describe our setup in Sections [3.4](#S3.SS4 "3.4 Implementation
    details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") and [4](#S4 "4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"), with additional
    hyperparameters baked into the supplementary code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not include experiments.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The experimental setting should be presented in the core of the paper to a level
    of detail that is necessary to appreciate the results and make sense of them.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The full details can be provided either with the code, the in appendix, or as
    supplemental material.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment Statistical Significance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper report error bars suitably and correctly defined or
    other appropriate information about the statistical significance of the experiments?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [No]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We report error bars for small-scale experiments [O](#A15 "Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). For full fine-tuning runs, we do not
    include error bars since running those would be prohibitively costly for us.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not include experiments.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should answer "Yes" if the results are accompanied by error bars,
    confidence intervals, or statistical significance tests, at least for the experiments
    that support the main claims of the paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The factors of variability that the error bars are capturing should be clearly
    stated (for example, train/test split, initialization, random drawing of some
    parameter, or overall run with given experimental conditions).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The method for calculating the error bars should be explained (closed form formula,
    call to a library function, bootstrap, etc.)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The assumptions made should be given (e.g., Normally distributed errors).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear whether the error bar is the standard deviation or the standard
    error of the mean.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is OK to report 1-sigma error bars, but one should state it. The authors
    should preferably report a 2-sigma error bar and then state that they have a 96%CI
    if the hypothesis of Normality of errors is not verified.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For asymmetric distributions, the authors should be careful not to show in tables
    or figures symmetric error bars that would yield results that are out of range
    (e.g. negative error rates).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If error bars are reported in tables or plots, The authors should explain in
    the text how they were calculated and reference the corresponding figures or tables
    in the text.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments Compute Resources
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: For each experiment, does the paper provide sufficient information
    on the computer resources (type computing workers, memory, time of execution)
    needed to reproduce the experiments?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We report the hardware setting, calibration setting, time, and
    memory requirements in Section [4](#S4 "4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"), which is sufficient for practitioners
    to reproduce our results. We omit some details, e.g. which runs were restarted
    due to unrelated server infrastructure issues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not include experiments.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper should indicate the type of compute worker CPU or GPU, internal cluster,
    or cloud provider, including relevant memory and storage.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper should provide the amount of compute required for each of the individual
    experimental runs as well as estimate the total compute.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper should disclose whether the full research project required mocomputingute
    than the experiments reported in the paper (e.g., preliminary or failed experiments
    that didn’t make it into the paper).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Of Ethics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the research conducted in the paper conform, in every respect,
    with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: Our research is focused on the base capability and accessibility
    of LLMs. While working on LLMs always has potential externalities, our specific
    work adheres to the ethics guidelines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the authors answer No, they should explain the special circumstances that
    require a deviation from the Code of Ethics.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should make sure to preserve anonymity (eg. if there is a special
    consideration due to laws or regulations in their jurisdiction).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Broader Impacts
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper discuss both potential positive societal impacts and
    negative societal impacts of the work performed?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: While our work is more concerned with fundamental matters of
    discrete optimization and LLM quantization, we provide a brief overview of its
    societal impacts in Appendix [Q](#A17 "Appendix Q Broader Impact ‣ Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that there is no societal impact on the work performed.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the authors answer NA or No, they should explain why their work has no societal
    impact or why the paper does not address societal impact.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of negative societal impacts include potential malicious or unintended
    uses (e.g., disinformation, generating fake profiles, surveillance), fairness
    considerations (e.g., deployment of technologies that could make decisions that
    unfairly impact specific groups), privacy considerations, and security considerations.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The conference expects that many papers will be foundational research and not
    tied to particular applications, let alone deployments. However, if there is a
    direct path to any negative applications, the authors should point it out. For
    example, it is legitimate to point out that an improvement in the quality of generative
    models could be used to generate deepfakes for disinformation. On the other hand,
    it is not needed to point out that a generic algorithm for optimizing neural networks
    could enable people to train models that generate Deepfakes faster.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should consider possible harms that could arise when the technology
    is being used as intended and functioning correctly harms that could arise when
    the technology is being used as intended but gives incorrect results, and harms
    following from (intentional or unintentional) misuse of the technology.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are negative societal impacts, the authors could also discuss possible
    mitigation strategies (e.g., gated release of models, providing defenses in addition
    to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system
    learns from feedback over time, improving the efficiency and accessibility of
    ML).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Safeguards
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper describe safeguards that have been put in place for
    responsible release of data or models that have a high risk for misuse (e.g.,
    pre-trained language models, image generators, or scraped datasets)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [N/A]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: Our work does not release any newer models, and quantizing existing
    models typically results in a less capable (and therefore less risky) model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper poses no such risks.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Released models that have a high risk for misuse or dual-use should be released
    with necessary safeguards to allow for controlled use of the model, for example
    by requiring that users adhere to usage guidelines or restrictions to access the
    model or implementing safety filters.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets that have been scraped from the Internet could pose safety risks. The
    authors should describe how they avoided releasing unsafe images.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We recognize that providing effective safeguards is challenging, and many papers
    do not require this, but we encourage authors to take this into account and make
    the best faith effort.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Licenses for existing assets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Are the creators or original owners of assets (e.g., code, data,
    models), used in the paper, properly credited, and are the license and terms of
    use explicitly mentioned and properly respected?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We use academically published artifacts (datasets, models, etc)
    and cite their respective authors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not use existing assets.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should cite the original paper that produced the code package or
    dataset.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors should state which version of the asset is used and, if possible,
    include a URL.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of the license (e.g., CC-BY 4.0) should be included for each asset.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For scraped data from a particular source (e.g., website), the copyright and
    terms of service of that source should be provided.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If assets are released, the license, copyright information, and terms of use
    in the package should be provided. For popular datasets, [paperswithcode.com/datasets](paperswithcode.com/datasets)
    has curated licenses for some datasets. Their licensing guide can help determine
    the license of a dataset.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For existing datasets that are re-packaged, both the original license and the
    license of the derived asset (if it has changed) should be provided.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If this information is not available online, the authors are encouraged to reach
    out to the asset’s creators.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: New Assets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Are new assets introduced in the paper well documented and is the
    documentation provided alongside the assets?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [Yes]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We release the code and provide documentation in the form of
    README and detailed docstrings. Both are included in the supplementary archive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not release new assets.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Researchers should communicate the details of the dataset/code/model as part
    of their submissions via structured templates. This includes details about training,
    license, limitations, etc.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper should discuss whether and how consent was obtained from people whose
    asset is used.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At submission time, remember to anonymize your assets (if applicable). You can
    either create an anonymized URL or include an anonymized zip file.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Crowdsourcing and Research with Human Subjects
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: For crowdsourcing experiments and research with human subjects, does
    the paper include the full text of instructions given to participants and screenshots,
    if applicable, as well as details about compensation (if any)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [N/A]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We do not use human subjects in our experiments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Including this information in the supplemental material is fine, but if the
    main contribution of the paper involves human subjects, then as much detail as
    possible should be included in the main paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the NeurIPS Code of Ethics, workers involved in data collection,
    curation, or other labor should be paid at least the minimum wage in the country
    of the data collector.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '15.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
    Subjects
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: Does the paper describe potential risks incurred by study participants,
    whether such risks were disclosed to the subjects, and whether Institutional Review
    Board (IRB) approvals (or an equivalent approval/review based on the requirements
    of your country or institution) were obtained?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: [N/A]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justification: We did not conduct any research on human subjects.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Guidelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the country in which research is conducted, IRB approval (or equivalent)
    may be required for any human subjects research. If you obtained IRB approval,
    you should clearly state this in the paper.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We recognize that the procedures for this may vary significantly between institutions
    and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and
    the guidelines for their institution.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For initial submissions, do not include any information that would break anonymity
    (if applicable), such as the institution conducting the review.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
