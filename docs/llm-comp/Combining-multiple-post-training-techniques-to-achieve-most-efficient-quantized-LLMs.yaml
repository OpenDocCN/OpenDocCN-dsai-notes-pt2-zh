- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple post-training techniques to achieve most efficient quantized
    LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07135](https://ar5iv.labs.arxiv.org/html/2405.07135)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{strip}Sayeh Sharify'
  prefs: []
  type: TYPE_NORMAL
- en: d-Matrix
  prefs: []
  type: TYPE_NORMAL
- en: Santa Clara, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: sayehs@d-matrix.ai
  prefs: []
  type: TYPE_NORMAL
- en: '&Zifei Xu'
  prefs: []
  type: TYPE_NORMAL
- en: d-Matrix Santa Clara, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: xuzifei@d-matrix.ai
  prefs: []
  type: TYPE_NORMAL
- en: '&Wanzin Yazar'
  prefs: []
  type: TYPE_NORMAL
- en: d-Matrix
  prefs: []
  type: TYPE_NORMAL
- en: Santa Clara, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: wyazar@d-matrix.ai
  prefs: []
  type: TYPE_NORMAL
- en: '&Xin Wang'
  prefs: []
  type: TYPE_NORMAL
- en: d-Matrix
  prefs: []
  type: TYPE_NORMAL
- en: Santa Clara, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: xwang@d-matrix.ai
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have distinguished themselves with outstanding
    performance in complex language modeling tasks, yet they come with significant
    computational and storage challenges. This paper explores the potential of quantization
    to mitigate these challenges. We systematically study the combined application
    of two well-known post-training techniques, SmoothQuant and GPTQ, and provide
    a comprehensive analysis of their interactions and implications for advancing
    LLM quantization. We enhance the versatility of both techniques by enabling quantization
    to microscaling (MX) formats, expanding their applicability beyond their initial
    fixed-point format targets. We show that by applying GPTQ and SmoothQuant, and
    employing MX formats for quantizing models, we can achieve a significant reduction
    in the size of OPT models by up to $4\times$.
  prefs: []
  type: TYPE_NORMAL
- en: '*Keywords* Microscaling Formats (MX), LLM Quantization, PTQ, GPTQ, SmoothQuant'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have emerged as extremely powerful tools to comprehend
    and generate natural language. However, their intensive computational demand and
    energy consumption make widespread adoption of these models in everyday tasks
    to be challenging. One way to address these challenges is post-training quantization,
    a technique that involves reducing the precision of model parameters and/or activations
    from the original bit-width to formats with fewer bits. Quantization can significantly
    reduce the memory footprint and computational requirements of these models, making
    them more accessible and deployable on a wider range of hardware, including mobile
    devices and edge devices. However, previous work has shown that the activations
    of LLMs with more than 3B parameters are difficult to quantize due to the emergence
    of outliers with large magnitude, which leads to significant quantization errors
    and accuracy degradation [[1](#bib.bib1)]. To address this issue, Xiao et al.
    proposed SmoothQuant, a post-training quantization technique that smooths out
    the activation outliers by migrating the quantization difficulty from activations
    to weights with a mathematically equivalent transformation [[2](#bib.bib2)]. Similarly,
    Frantar et al. proposed GPTQ, a scalable one-shot quantization method that utilizes
    approximate second-order information to quantize weights of LLMs with high efficiency
    and accuracy [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the quantization challenges in LLMs, we first examine the difficulty
    of activation quantization for LLMs by measuring the activation magnitude in a
    linear layer of DistilGPT2 [[4](#bib.bib4)], and demonstrate the presence of outliers
    across the activations, similar to the LLM.int8() work [[1](#bib.bib1)]. Then
    we systematically study the interaction between two widely adopted LLM quantization
    techniques, SmoothQuant and GPTQ, which have demonstrated excellent results in
    quantizing LLMs to the fixed-point formats [[2](#bib.bib2), [3](#bib.bib3)]. Furthermore,
    we enhance these two algorithms to support quantization to *microscaling (MX)*
    data formats [[5](#bib.bib5), [6](#bib.bib6)]. We assess the necessity of enabling
    SmoothQuant for matrix multiplications between activations and weights, as well
    as those between activations¹¹1In the transformer architecture, there are two
    matrix multiplications between activations: query state with the transpose of
    key state, and attention score with value state.. Finally, we analyze the effect
    of different quantization granularities (per-channel vs. per-tensor) and quantization
    ranges (symmetric vs. affine) for fixed-point quantization with SmoothQuant.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/634d32b44a1ef254594862ddaaaf8c19.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Activation (Original)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c1d60416b23d0a2809d2adfbd31ad64.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Activation (SmoothQuant)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0787cd9542768bb31796fbc430a7a2d.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Weight (Original)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19ad9a8fc2b5ea2127c5db6f0a2254a8.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Weight (SmoothQuant)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Magnitude of the input activations and weights of the “lm-head” linear
    layer in DistilGPT2 before and after SmoothQuant. (a) There are a few outlier
    channels in the original activation tensor. (c) The weight tensor has a flat and
    uniform distribution. (b and d) By migrating the quantization difficulty from
    activations to weights we can greatly smooth activation outliers, while still
    maintaining an easy-to-quantize weight tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: Microscaling format.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The microscaling (MX) format for deep neural net computation was proposed by
    prior work, first as MSFP [[7](#bib.bib7), [5](#bib.bib5)] and later subsumed
    by an emerging industry standard *microscaling formats* [[6](#bib.bib6)]. Specifically
    MXINT8, a microscaling format that enables high-accuracy inference using half
    the memory footprint and twice the throughput of FP16, is an emerging industry
    standard endorsed by Microsoft, AMD, Arm, Intel, Meta, NVIDIA, and Qualcomm [[6](#bib.bib6)],
    already seeing adoption in today’s hardware products, such as Qualcomm Cloud AI100
    Accelerator [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The MX format, as outlined in this paper, is characterized by three key components:
    1) the scale factor data type, 2) the data type and precision of individual elements,
    and 3) the scaling block size. The scale factor is applied uniformly across a
    block of individual elements. This paper specifically focuses on MX formats employing
    the *INT* data type for individual elements, thus termed *MXINT*.'
  prefs: []
  type: TYPE_NORMAL
- en: Notation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Throughout the paper we denote a microscaling (MX) format with scaling block
    size of $b$ integer bits and no fractional bits is denoted by INT-i.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We enhance SmoothQuant and GPTQ to support quantization to microscaling (MX)
    data formats, extending their compatibility beyond the initially targeted fixed-point
    formats in the proposed methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We examine the necessity of enabling SmoothQuant for matrix multiplications
    between activations and weights, and those between activations. We show that enabling
    SmoothQuant only for the former operations is sufficient to preserve the perplexity
    of the baseline models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We study the interaction of SmoothQuant and GPTQ and show that SmoothQuant and
    GPTQ are synergistic, an effect most prominent in smaller models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Activation Quantization Difficulty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previous work has shown that LLMs are difficult to quantize due to the presence
    of activation outliers [[1](#bib.bib1), [9](#bib.bib9), [10](#bib.bib10)]. We
    verify this by visualizing the input activations and the weights of a linear layer
    in DistilGPT2 [[4](#bib.bib4)]. Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction
    ‣ Combining multiple post-training techniques to achieve most efficient quantized
    LLMs") illustrates the magnitude of the input activations for the “lm-head” layer
    of DistilGPT2\. The existing activation outliers in some of the channels dominate
    the maximum magnitude measurement, leading to few effective bits for non-outlier
    values using the per-tensor quantization scheme. This makes it difficult to quantize
    the activation tensor. On the other hand, as shown in Figure [1(c)](#S1.F1.sf3
    "In Figure 1 ‣ 1 Introduction ‣ Combining multiple post-training techniques to
    achieve most efficient quantized LLMs") the weight distribution of the same layer
    is quite uniform and flat, making its quantization easier compared to quantizing
    the activations. SmoothQuant proposes a technique to migrate the quantization
    difficulty from activations to weights, such that the “smoothed” activations and
    the adjusted weights are both easy to quantize [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 SmoothQuant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SmoothQuant (SQ) is a quantization method that targets both activations and
    weights of a model [[2](#bib.bib2)]. In this approach, the activation of a linear
    layer is scaled by a per-channel smoothing factor $s\in R^{C_{i}}$ to minimize
    quantization errors. Simultaneously, the weight of the layer is adjusted in the
    opposite direction to maintain the mathematical equivalence of the linear layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{Y}=(\textbf{X}\texttt{diag}(s)^{-1})\cdot(\texttt{diag}(s)\textbf{W})=\hat{\textbf{X}}\hat{\textbf{W}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'In Equation [1](#S3.E1 "In 3 SmoothQuant ‣ Combining multiple post-training
    techniques to achieve most efficient quantized LLMs"), X is the original input
    activation with outliers, and $\hat{\textbf{X}}=\textbf{X}\texttt{diag}(s)^{-1}$
    is set to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{j}=\texttt{max}(&#124;\textbf{X}_{j}&#124;),\hskip 8.53581ptj={1,2,...,C_{i}}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Where $C_{i}$ calibration samples from the calibration dataset (see Section [5.1](#S5.SS1
    "5.1 Setups ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve
    most efficient quantized LLMs") for more details). By dividing the input activation
    by the the scaling factor of Equation [2](#S3.E2 "In 3 SmoothQuant ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs"),
    all channels of the scaled input activation would have the same range, making
    quantization of the scaled tensor to be very easy. However, this will migrate
    the difficulty of the quantization completely to the weight side of a linear layer.
    To address this issue, Xiao et al. proposed a scaling formula that balances the
    quantization difficulty of activations and weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{j}=\texttt{max}(&#124;\textbf{X}_{j}&#124;)^{\alpha}/\texttt{max}(&#124;\textbf{W}_{j}&#124;)^{1-\alpha},\hskip
    8.53581ptj={1,2,...,C_{i}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Where $\alpha$ is a hyper-parameter that controls how much quantization difficulty
    we want to migrate from activations to weights. For quantization to the MX format
    using SmoothQuant, we directly calculated the SmoothQuant scaling factors, skipping
    the additional calibration phase required for quantization to fixed-point formats.
    For more details on the SmoothQuant algorithm refer to Xiao et al.’s work [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm 1 Enhanced GPTQ: Quantize W given inverse Hessian $\textbf{H}^{-1}=(2\textbf{X}\textbf{X}^{T}+\lambda\textbf{I})^{-1}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: $W$end forReturn: Q'
  prefs: []
  type: TYPE_NORMAL
- en: 4 GPTQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GPTQ is a post-training quantization (PTQ) method that uses second-order Hessian
    information for weight quantization in LLMs [[3](#bib.bib3)]. It employs layer-wise
    quantization for each layer $l$. In other words, GPTQ aims to find [[3](#bib.bib3)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\texttt{argmin}_{\hat{\textbf{W}}_{l}}&#124;&#124;\textbf{W}_{l}\textbf{X}_{l}-\hat{\textbf{W}_{l}}\textbf{X}_{l}&#124;&#124;^{2}_{2}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'To solve equation [4](#S4.E4 "In 4 GPTQ ‣ Combining multiple post-training
    techniques to achieve most efficient quantized LLMs"), GPTQ quantizes each row
    of the weight matrix, W, independently, focusing on a single weight per row at
    a time. It consistently updates all not-yet-quantized weights to offset the error
    introduced by quantizing a single weight. Since the objective function in equation [4](#S4.E4
    "In 4 GPTQ ‣ Combining multiple post-training techniques to achieve most efficient
    quantized LLMs") is quadratic, its Hessian H can be calculated using the following
    formula, where $F$ denotes the set of remaining full-precision weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{H}_{F}=2\textbf{X}_{F}\textbf{X}_{F}^{T}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Given H, the next to be quantized weight, $w_{q}$ to the nearest quantized
    value  [[3](#bib.bib3)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: For all rows of W, GPTQ quantizes weights in the same order. This accelerates
    the process, as certain computations need to be performed only once for each column
    rather than once for each weight. Additionally, the vectorized implementation
    of GPTQ enables processing multiple rows of W simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: The GPTQ algorithm, as originally proposed, is designed for quantization to
    a fixed-point format. We have enhanced the algorithm to also support quantization
    to a *microscaling (MX) format*. Algorithm 1 provides pseudocode for the modified
    GPTQ, that enables MX quantization. Note that for quantizing W to a specific MX
    format, the micro-block size in the algorithm, $B_{2}$, should be a multiple of
    the block size of the MX format. Additionally, to reduce the GPU memory requirement,
    we have implemented the GPTQ quantization process on the actual layer inputs in
    the partially quantized model rather than on the layer inputs of the full precision
    model. For more details on the GPTQ algorithm refer to Frantar et al.’s work [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/708a5cdbe196c164a23867bcde04598c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Perplexity for OPT models with different sizes when quantized to
    INT-8, MXINT8-64, and MXINT4-128\. The Y-axis represents perplexity in logarithmic
    scale. The X-axis represents the OPT models with different sizes. The Per-tensor
    affine scheme is used for INT-8 quantization of both activations and weights.
    The figure shows that: a) For models larger than 6.7B parameters quantization
    to INT-8 significantly degrades model performance, this is shown by a cliff in
    the perplexity curve of INT-8 at 6.7B (the orange curve). b) There are no such
    cliffs with the MXINT quantizations. c) SmoothQuant mitigates the cliff, preserving
    the baseline (FP16) perplexity of OPT models across different scales when quantized
    to INT-8\. d) SmoothQuant is beneficial with more aggressive quantization to MXINT4,
    reducing the perplexity gap between the baseline model and the quantized models.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Setups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We benchmarked different quantization methods on DistilGPT2 [[4](#bib.bib4)],
    the OPT [[11](#bib.bib11)], and the LLaMA [[12](#bib.bib12)] families. DistilGPT2
    is a distilled version of GPT-2 with only 82 million parameters. It is challenging
    to quantize a parameter-efficient model like DistilGPT2 as the model is already
    designed to be compact, and further reducing precision during quantization may
    hurt the model performance significantly, requiring careful optimization to maintain
    the desired balance between model size and accuracy. LLaMA and OPT are two families
    of open-source LLMs that are widely accepted among the machine learning community
    due to their superior performance compared to other open-source LLMs [[1](#bib.bib1),
    [3](#bib.bib3), [2](#bib.bib2), [13](#bib.bib13)]. LLaMA is also considered to
    be the foundation of many popular open-source models such as Alpaca [[14](#bib.bib14)],
    Vicuna [[15](#bib.bib15)], Guanaco [[16](#bib.bib16)], and Stable Beluga [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Following previous work  [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [13](#bib.bib13),
    [18](#bib.bib18), [19](#bib.bib19)], we measured the perplexity of quantized language
    models on WikiText-2 [[20](#bib.bib20)] as perplexity can stably reflect the performance
    of LLMs [[18](#bib.bib18), [13](#bib.bib13)]. Unless otherwise stated, the test
    split of the dataset is used to evaluate the models.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization formats.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We evaluated models using different microscaling and fixed-point quantization
    formats; the same numerical format was applied for quantizing both activations
    and weights unless specified otherwise. For the fixed-point quantization, we calibrated
    the models using $128$ random input sentences from WikiText-2-train to estimate
    the dynamic range of activations. We utilized *MinMaxObserver* to find the range
    of activations, and calculated the zero-point and the scale parameters for the
    activations and weights in per-tensor granularity levels.
  prefs: []
  type: TYPE_NORMAL
- en: Activation smoothing.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We calculated the per-channel scaling factor for activations and weights using
    the formula stated in Equation [1](#S3.E1 "In 3 SmoothQuant ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs"). As in the
    previous work, we consistently use a migration strength ($\alpha$ random sentences
    from the WikiText-2-train dataset. Once we calculated the scaling factors, we
    used the same values to evaluate the models with different quantization formats.
  prefs: []
  type: TYPE_NORMAL
- en: Targeted layers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similar to the previous work [[2](#bib.bib2)], we apply smoothing on the input
    activation of the self-attention and the feed-forward layers of LLMs. Unless stated
    otherwise, we transform all Linear layers, HFConv1D layers, and Activation by
    Activation matrix multiplications to the specified quantization format while keeping
    the activation/weight in the original format for other layers including ReLU,
    GELU, Softmax, and LayerNorm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.2.1 SmoothQuant effect on quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we evaluate the impact of the SmoothQuant technique on the
    quantization of LLMs from different families: OPT, DistilGPT2, LLaMA, and the
    llama2 models. We employ various fixed-point and MX formats with different bit-widths
    for our assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: DistilGPT2 quantization results, with and without SmoothQuant. Act,
    Wgt, and PPL denote activation, weight, and perplexity, respectively. $\downarrow$:
    the lower the metric, the better the result. We used per-tensor affine quantization
    for the fixed-point formats. LR: Likelihood Ratio over SmoothQuant disabled.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Act & Wgt format | $\downarrow$PPL w/ SQ | LR |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (FP32) | 46.06 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-64 | 46.06 | 46.06 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-32 | 46.06 | 46.06 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-8 | 59.23 | 60.91 | 0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-128 | 46.49 | 46.27 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-16 | 46.22 | 46.10 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-6 | 2230.21 | 1875.86 | 1.19 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-128 | 54.09 | 51.91 | 1.04 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-16 | 48.42 | 47.70 | 1.02 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-4 | 11559.05 | 7367.77 | 1.57 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-128 | 1649.68 | 898.35 | 1.84 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-16 | 153.87 | 123.53 | 1.25 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Quantization results for the LLaMA models with different sizes on
    WikiText-2-test, with and without SmoothQuant. Act, Wgt, SQ, and PPL denote activation,
    weight, SmoothQuant, and perplexity, respectively. $\downarrow$: the lower the
    metric, the better the result. We used per-tensor affine quantization for the
    fixed-point formats. LR: Likelihood Ratio of perplexity with SmoothQuant enabled
    over perplexity with SmoothQuant disabled.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | LLaMA-7B | LLaMA-13B | LLaMA-30B |'
  prefs: []
  type: TYPE_TB
- en: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
  prefs: []
  type: TYPE_TB
- en: '| format | SQ | SQ | SQ | SQ | SQ | SQ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (FP16) | 5.67 | N/A | N/A | 5.09 | N/A | N/A | 4.10 | N/A | N/A
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-64 | 5.67 | 5.67 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-32 | 5.67 | 5.67 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| INT-8 | 19.01 | 19.19 | 0.99 | 28.90 | 43.72 | 0.66 | 17.72 | 19.88 | 0.89
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-128 | 5.68 | 5.69 | 1.00 | 5.10 | 5.09 | 1.00 | 4.11 | 4.11 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-16 | 5.68 | 5.68 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| INT-6 | 45202.57 | 58760.27 | 0.77 | 65911.20 | 42067.46 | 1.56 | 27764.73
    | 31869.35 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-128 | 5.83 | 5.83 | 1.00 | 5.20 | 5.17 | 1.00 | 4.29 | 4.24 | 1.01
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-16 | 5.72 | 5.72 | 1.00 | 5.12 | 5.12 | 1.00 | 4.17 | 4.14 | 1.01
    |'
  prefs: []
  type: TYPE_TB
- en: '| INT-4 | 185803.03 | 230625.14 | 0.81 | 158380.31 | 160879.97 | 0.98 | 152000.44
    | 173788.06 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-128 | 16.35 | 12.87 | 1.27 | 11.69 | 9.60 | 1.22 | 10.49 | 8.06 |
    1.30 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-16 | 7.43 | 7.12 | 1.04 | 6.22 | 6.05 | 1.03 | 5.66 | 5.22 | 1.08
    |'
  prefs: []
  type: TYPE_TB
- en: OPT family.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To explore the impact of SmoothQuant on the quantization of the OPT family [[11](#bib.bib11)],
    first we reproduce the observation of the *INT-8 quantization cliff phenomenon*
    without applying SmoothQuant [[1](#bib.bib1), [21](#bib.bib21)]. We measure the
    perplexity of OPT models with different sizes when quantized to INT-8 and show
    that with scaling up the model size beyond 6.7B parameters, performance of the
    quantized model catastrophically degrades. This is shown with a cliff in perplexity
    at the 6.7B point of the orange curve (Figure 2). We further verify that introducing
    SmoothQuant rectified this anomaly, resulting in perplexities monotonically decreasing
    with increasing model size [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: Next, we ask *whether quantization to MX formats, e.g., MXINT8 and MXINT4, results
    in a similar quantization cliff anomaly?* To answer this question, we quantize
    the same models to the MXINT8 and the MXINT4 formats and show that quantization
    of the models to either of these formats does not result in a similar anomaly;
    see Figure [2](#S4.F2 "Figure 2 ‣ 4 GPTQ ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs"). Moreover, we demonstrate that across
    different model sizes, quantization to MXINT8 maintains the model perplexity while
    more aggressive quantization to MXINT4 penalizes perplexity by 13-44%. Enabling
    SmoothQuant improves performance of the MXINT4 models narrowing the perplexity
    gap between the baseline models and the quantized models to 1-12%. Table [6](#A1.T6
    "Table 6 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and
    w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs") in the appendix section indicates the
    detailed quantization results for the OPT family.
  prefs: []
  type: TYPE_NORMAL
- en: DistilGPT2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table [1](#S5.T1 "Table 1 ‣ 5.2.1 SmoothQuant effect on quantization ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs") illustrates quantization results for DistilGPT2 with
    and without SmoothQuant using different quantization formats. We found that generally,
    aggressive quantization to fewer bits increases perplexity for both the fixed-point
    and the MX formats. However, for a given bit-width, using MX results in better
    perplexity compared to the fixed-point format. Remarkably, for almost all of the
    studied quantization bit-widths and formats, enabling SmoothQuant increases the
    quantization quality, leading to better perplexity. The advantage of enabling
    SmoothQuant is small with larger precisions. Under more restrictive bit-widths
    and larger block sizes, SmoothQuant becomes more advantageous.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA family.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table [2](#S5.T2 "Table 2 ‣ 5.2.1 SmoothQuant effect on quantization ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs") illustrates perplexity of the quantized *LLaMA* models [[12](#bib.bib12)]
    with three different sizes on WikiText-2-test using various MX and fixed-point
    formats. For all three models, aggressive quantization to small bit-widths significantly
    hurts the model performance, while quantizing to higher bit-widths has negligible
    effect on perplexity. For example, quantizing LLaMA-7B to MXINT16 preserves the
    baseline perplexity of the model regardless of the format block size, while quantizing
    to MXINT4-16 increases perplexity by $31\%$. Finally, for the studied models regardless
    of the quantization format and precision, the advantage of enabling SmoothQuant
    is marginal with larger models and higher quantization precisions and is especially
    prominent for smaller models with more restrictive bit-widths.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we assess the impact of SmoothQuant on the quantization of the *llama2*
    family [[22](#bib.bib22)] using various fixed-point and MX formats with different
    precisions. We observe similar trends to those identified in the LLaMA study.
    Detailed results of the experiment can be found in the Table [7](#A1.T7 "Table
    7 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and w/o
    SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs") of the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Activation by Activation SmoothQuant
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The previous work [[2](#bib.bib2)] enabled SmoothQuant for the input activations
    of all of the matrix multiplications for a given model, regardless of the type
    of their operands. In this section, we divide the multiplications to two categories
    based on their operand types, and study the necessity of enabling SmoothQuant
    for each category separately. The two categories are a) activation by activation
    multiplications which includes Query $\times$ Value operations of the self-attention
    blocks, and b) activation by weight multiplications which covers all Linear and
    HFConv1D layers of the networks. We refer to the former as *A-A SmoothQuant*,
    and the latter as *A-W SmoothQuant*, in the rest of the manuscript. In the aforementioned
    studies where SmoothQuant significantly improved the performance of the quantized
    models, *we ask whether the improvement was due to A-W or to A-A SmoothQuant,
    or their combination?*
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, we performed ablation studies on the two types of
    SmoothQuant, on three models from three different LLM families: DistilGPT2, OPT-6.7B,
    and LLaMA-7B evaluated on WikiText-2-test (Table [3](#S5.T3 "Table 3 ‣ 5.2.2 Activation
    by Activation SmoothQuant ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple post-training
    techniques to achieve most efficient quantized LLMs")). We found that for all
    three networks, A-W SmoothQuant is sufficient to lead to the observed accuracy
    improvement, and A-A SmoothQuant does not contribute to the improvement materially.
    Additionally, contrary to previous research [[2](#bib.bib2)], we found that further
    enabling A-A SmoothQuant on top of A-W SmoothQuant significantly degrades the
    performance of LLaMA-7B by $10\%$ when quantized to INT-8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Perplexity of the quantized models for DistilGPT2, OPT-6.7B, and LLaMA-7B
    on WikiText-2-test. For INT-8 quantization of activations and weights the per-tensor
    affine scheme is used. Act, Wgt, SQ, and PPL denote activation, weight, SmoothQuant,
    and perplexity, respectively. $\downarrow$: the lower the metric, the better the
    result. For all three models, only enabling A-W SmoothQuant is sufficient to reduce
    perplexity of the quantized models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 | A:MXINT8 |'
  prefs: []
  type: TYPE_TB
- en: '| W:INT-8 | W:MXINT8 | W:MXINT4 |'
  prefs: []
  type: TYPE_TB
- en: '| A-A SQ | A-W SQ | DistilGPT2 $\downarrow$PPL (FP32 PPL: 46.06) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 59.23 | 46.37 | 72.97 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 60.57 | 46.23 | 72.93 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 59.90 | 46.35 | 77.18 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 60.91 | 46.23 | 77.24 |'
  prefs: []
  type: TYPE_TB
- en: '| A-A SQ | A-W SQ | OPT-6.7B $\downarrow$PPL (FP16 PPL: 10.86) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 15.01 | 10.86 | 12.33 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 11.10 | 10.86 | 11.30 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 15.01 | 10.86 | 12.35 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 11.17 | 10.86 | 11.29 |'
  prefs: []
  type: TYPE_TB
- en: '| A-A SQ | A-W SQ | LLaMA-7B $\downarrow$PPL (FP16 PPL: 5.67) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 19.01 | 5.68 | 6.31 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 17.47 | 5.68 | 6.18 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 19.40 | 5.68 | 6.31 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 19.19 | 5.68 | 6.18 |'
  prefs: []
  type: TYPE_TB
- en: 5.2.3 SmoothQuant with different fixed-point quantization schemes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we study the effect of different *quantization granularity
    (per-tensor vs per-channel)* and *quantization range (symmetric vs affine)* for
    fixed-point quantization, with and without enabling SmoothQuant.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization granularity.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Quantization can be done in different granularities. The per-tensor quantization
    uses a single scale and zero-point values to quantize the entire tensor while
    the per-channel quantization enables finer-grained quantization by using different
    scales and zero-point parameters for values associated with each channel of a
    given tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization range.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Quantization can have a symmetric or asymmetric range. In symmetric quantization
    we assume that the given tensor has the same negative and positive ranges and
    is symmetric around 0, while in affine quantization a zero-point offset is used
    to shift the quantization levels according to the negative and positive ranges
    of the given tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Perplexity for DistilGPT2, OPT-6.7B, and LLaMA-7B on WikiText-2-test
    with different quantization schemes. Act, Wgt, SQ, PPL, and Quant. denote activation,
    weight, SmoothQuant, perplexity, and Quantization, respectively. Both activations
    and weights are quantized to INT-8\. Enabling A-W SmoothQuant is only beneficial
    for per-tensor affine calibrations. $\downarrow$: The lower the metric, the better
    the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Act & Wgt format | INT-8 |'
  prefs: []
  type: TYPE_TB
- en: '| $\downarrow$PPL w/o & w/ A-W SmoothQuant | w/o SQ | w/ SQ |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. granularity | Quant. range | DistilGPT2 |'
  prefs: []
  type: TYPE_TB
- en: '| per-tensor | symmetric | 92.91 | 228.26 |'
  prefs: []
  type: TYPE_TB
- en: '| per-tensor | affine | 59.23 | 60.57 |'
  prefs: []
  type: TYPE_TB
- en: '| per-channel | symmetric | 54.08 | 54.32 |'
  prefs: []
  type: TYPE_TB
- en: '| per-channel | affine | 48.13 | 48.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. granularity | Quant. range | OPT-6.7B |'
  prefs: []
  type: TYPE_TB
- en: '| per-tensor | symmetric | 93.19 | 13222.79 |'
  prefs: []
  type: TYPE_TB
- en: '| per-tensor | affine | 15.01 | 11.10 |'
  prefs: []
  type: TYPE_TB
- en: '| per-channel | symmetric | 12.91 | 13.17 |'
  prefs: []
  type: TYPE_TB
- en: '| per-channel | affine | 10.96 | 11.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. granularity | Quant. range | LLaMA-7B |'
  prefs: []
  type: TYPE_TB
- en: '| per-tensor | symmetric | 38.51 | 23.89 |'
  prefs: []
  type: TYPE_TB
- en: '| per-tensor | affine | 19.01 | 17.38 |'
  prefs: []
  type: TYPE_TB
- en: '| per-channel | symmetric | 6.63 | 6.64 |'
  prefs: []
  type: TYPE_TB
- en: '| per-channel | affine | 5.93 | 5.92 |'
  prefs: []
  type: TYPE_TB
- en: 'Table [4](#S5.T4 "Table 4 ‣ Quantization range. ‣ 5.2.3 SmoothQuant with different
    fixed-point quantization schemes ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") reports quantization
    perplexity for DistilGPT2 on WikiText-2-test with different calibration techniques
    enabling and disabling SmoothQuant. Calibration is done using 128 random input
    sentences from WikiText-2-train. The INT-8 quantization format is used for both
    activation and weights. Based on the results of Section [5.2.2](#S5.SS2.SSS2 "5.2.2
    Activation by Activation SmoothQuant ‣ 5.2 Results ‣ 5 Experiments ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs"),
    with INT-8 quantization only enabling A-W SmoothQuant provides smaller perplexity
    numbers compared to enabling both A-A SmoothQuant and A-W SmoothQuant. Thus, in
    this study only the latter is enabled. We found that: a) generally the affine
    calibration method results in better perplexity compared to its corresponding
    symmetric technique regardless of enabling or disabling SmoothQuant; b) for per-tensor
    affine calibrations, enabling SmoothQuant improves quantization results significantly
    c) in most cases, with per-channel calibrations, enabling SmoothQuant slightly
    degrades perplexity; accordingly, SmoothQuant is not required with the per-channel
    calibration scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Quantization results for the OPT models with different sizes on WikiText-2-test,
    with enabling/disabling GPTQ and A-W SmoothQuant. A, W, SQ, and PPL denote activation,
    weight, SmoothQuant, and perplexity, respectively. $\downarrow$: the lower the
    metric, the better the result. We used per-tensor affine quantization for the
    INT-8 format.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 |'
  prefs: []
  type: TYPE_TB
- en: '| W:INT-8 | W:MXINT4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | OPT-125M (FP16 $\downarrow$PPL: 27.66) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 37.75 | 39.52 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 36.13 | 35.86 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 36.97 | 33.63 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 35.36 | 31.66 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | OPT-1.3B (FP16 $\downarrow$PPL: 14.63) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 16.43 | 18.94 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 15.44 | 16.16 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 16.40 | 18.00 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 15.41 | 15.52 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | OPT-6.7B (FP16 $\downarrow$PPL: 10.86) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 14.92 | 12.19 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 11.09 | 11.20 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 14.73 | 11.12 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 11.09 | 11.03 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | OPT-13B (FP16 $\downarrow$PPL: 10.12) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 255.85 | 11.52 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 11.03 | 10.54 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 250.77 | 10.28 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 10.98 | 10.31 |'
  prefs: []
  type: TYPE_TB
- en: 5.2.4 SmoothQuant and GPTQ Interaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPTQ has been shown to improve quality of models whose weights are quantized
    to lower than 8-bit integer formats [[3](#bib.bib3)]. Since we know from the above
    results that SmoothQuant also improves model accuracies in these modes, a natural
    question arises: *how do SmoothQuant and GPTQ interact when they are applied jointly?*'
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we conduct experiments on quantization of the OPT,
    and the LLaMA families where SmoothQuant and GPTQ are applied individually, as
    well as jointly to assess their impact on the quantization quality. As shown in
    Section [5.2.1](#S5.SS2.SSS1.Px1 "OPT family. ‣ 5.2.1 SmoothQuant effect on quantization
    ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple post-training techniques to
    achieve most efficient quantized LLMs"), for both the OPT and LLaMA models quantization
    to MXINT8 maintains the model perplexity across different model sizes, thus there
    is no need to enable SmoothQuant or GPTQ with MXINT8 quantization. Accordingly,
    in this section we have considered the joint application of SmoothQuant and GPTQ
    only for quantization to MXINT4, and INT-8. Moreover, we showed that SmoothQuant
    is only beneficial with the per-tensor affine scheme (Section [5.2.3](#S5.SS2.SSS3
    "5.2.3 SmoothQuant with different fixed-point quantization schemes ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs")). Thus, in this section for INT-8 quantization of activations
    and weights this method is used.
  prefs: []
  type: TYPE_NORMAL
- en: Table [5](#S5.T5 "Table 5 ‣ Quantization range. ‣ 5.2.3 SmoothQuant with different
    fixed-point quantization schemes ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") illustrates
    our experiment results for the OPT family. We found that for large models with
    more than 6.7B parameters, enabling SmoothQuant is essential to preserve the perplexity
    of the baseline model, when quantizing to INT-8, and applying GPTQ on top of SmoothQuant
    only improves the performance of the quantized models slightly. With MX quantization,
    the advantageous of jointly enabling SmoothQuant and GPTQ over only applying SmoothQuant
    is small for large models. This becomes more advantageous with smaller models.
    For instance, perplexity of OPT-125M when quantized to MXINT4 is $39.52$.
  prefs: []
  type: TYPE_NORMAL
- en: We conducted a similar experiment with DistilGPT2, the LLaMA, and the llama2
    families. In contrast to the findings from the OPT study, for these cases, the
    best results were achieved, except when quantizing llama2-13B to INT-8, with only
    applying GPTQ. However, for the INT-8 quantization of llama2-13B, SmoothQuant
    proved to be the solution to improve the perplexity degradation of the quantized
    model. Additional details on these experiments can be found in the appendix (Tables [8](#A1.T8
    "Table 8 ‣ DistilGPT2\. ‣ A.2 SmoothQuant and GPTQ Interaction for quantization
    of DistilGPT2 and the LLaMA family ‣ Appendix A Appendix ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") and [9](#A1.T9
    "Table 9 ‣ LLaMA and llama2 family. ‣ A.2 SmoothQuant and GPTQ Interaction for
    quantization of DistilGPT2 and the LLaMA family ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/139548ac85ed521256c5e81a11090477.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Perplexity for the LLaMA and llama2 families when quantized to MXINT8,
    and MXINT4\. The Y-axis represents perplexity. The X-axis represents model parameter
    size including the additional scale parameters required by the SmoothQuant quantization
    method. Note that the GPTQ algorithm does not introduce any additional model parameters
    during the inference. A, W, and SQ denote activation, weight, and SmoothQuant.
    The points corresponding to the quantized models on Pareto frontiers are indicated
    by a gray circle.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.5 Pareto frontier Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The objective of a quantization method is to reduce the model size while preserving
    its accuracy. In the experiments conducted in this study, the concept of the *Pareto
    frontier* becomes relevant in determining the most suitable quantization method
    for each model under a size constraint. A model is deemed to be on the Pareto
    frontier if no other model exists with both a smaller size and lower perplexity.
    Figure [3](#S5.F3 "Figure 3 ‣ 5.2.4 SmoothQuant and GPTQ Interaction ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs") illustrates perplexity of the LLaMA and llama2 families
    on WikiText-2-test as a function of model parameter size. Points corresponding
    to the quantized models on Pareto frontiers are marked with a gray circle. We
    observe that, in general, for aggressive weight quantization to 4-bit (e.g., MXINT4),
    models quantized with GPTQ are positioned on Pareto frontiers while in the case
    of quantization to 6-bit (e.g., MXINT6), models quantized with SmoothQuant are
    found on Pareto frontiers. Lastly, for a more relaxed quantization to 8-bit (e.g.,
    MXINT8), neither GPTQ nor SmoothQuant is deemed necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we explored Pareto frontiers for the OPT family revealing similar
    trends to those observed for the LLaMA and llama2 families. However, for the quantization
    of small OPT models to MXINT4 (e.g., OPT-1.3B and OPT-6.7B), the best perplexity
    is achieved when both GPTQ and SQ are applied jointly. Further details on the
    Pareto frontiers study of the OPT family can be found in Figure [4](#A1.F4 "Figure
    4 ‣ A.3 Pareto frontier study for the OPT family ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs") in
    the appendix. Note that in none of the aforementioned studies, no fixed-point
    quantization points appear on the Pareto frontiers, indicating that, for the studied
    models and quantization ranges, the MX format is more suitable for quantizing
    the models compared to the fixed-point format with the same bit-width. Moreover,
    we studied the effects of block size granularity on quantization to microscaling
    and fixed-point formats, and found that regardless of the quantization granularity
    for quantization to 6-bit and 8-bit, the majority of points on the Pareto frontier
    are associated with MX data-types, more details is presented in Section [A.4](#A1.SS4
    "A.4 Block size granularity effect on quantization ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs") of
    appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model quantization methods.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Quantization is a technique that lowers the bit precision of deep learning
    models, effectively reducing model size and accelerating inference. There are
    two primary categories of quantization techniques: Quantization-Aware Training
    (QAT), which leverages backpropagation to update quantized weights [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)], and Post-Training Quantization
    (PTQ), which typically requires no additional training. Quantization-aware training
    methods cannot easily scale up to quantize giant LLMs. Consequently, PTQ methods
    are commonly employed for quantizing LLMs [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)].
    In this work, we studied the interaction of two PTQ methods, GPTQ [[3](#bib.bib3)]
    and SmoothQuant [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Model quantization.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: With the recent open-source releases of language models like OPT [[11](#bib.bib11)]
    and LLaMA [[12](#bib.bib12)], researchers are actively working on developing cost-effective
    methods to compress these large networks for inference. Various approaches have
    been suggested to tackle the challenges of quantizing LLMs. ZeroQuant [[19](#bib.bib19)]
    and nuQmm [[34](#bib.bib34)] employ per-token and group-wise quantization schemes
    for LLMs, requiring customized CUDA kernels. ZeroQuant further proposes layer-wise
    knowledge distillation, similar to AdaQuant [[31](#bib.bib31)], but the largest
    evaluated model by both ZeroQuant and nuQmm has 20B parameters. LLM.int8() identifies
    activation outliers in a few feature dimensions as a hindrance to the quantization
    of larger models. To address this issue, it proposes to preserve those dimensions
    in higher precision using a mixed INT8/FP16 decomposition [[1](#bib.bib1)]. However,
    this implementation results in significant latency overhead, sometimes even slower
    than FP16 inference. Similarly, SpQR [[35](#bib.bib35)] and OWQ [[36](#bib.bib36)]
    propose to retain outlier features that are difficult to quantize in full-precision,
    while AWQ [[13](#bib.bib13)] mitigates the quantization error for the outliers
    using grid-searched channel-wise scaling. Additionally, Outlier Suppression [[9](#bib.bib9)]
    tackles activation outliers by utilizing non-scaling LayerNorm and token-wise
    clipping. Despite its success with smaller language models such as BERT [[37](#bib.bib37)]
    and BART [[38](#bib.bib38)], it falls short in maintaining accuracy for larger
    LLMs, while SmoothQuant and GPTQ both preserve the performance of LLMs up to 175B
    parameters [[2](#bib.bib2), [3](#bib.bib3)]. Lee et al., explored the combined
    use of GPTQ and SmoothQuant for quantizing LLMs, focusing solely on fixed-point
    data types in their study [[39](#bib.bib39)]. Trukhanov et al., proposed a technique
    for quantizing KV-cache to low-precision Block Floating-Point (BFP) formats without
    compromising the resulting model accuracy [[40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To summarize, our study showed that the optimality of decisions on whether to
    combine the GPTQ and the SmoothQuant techniques depends on factors such as numerical
    precision, data type, model family, and model size. SmoothQuant can be effective
    for quantizing both activations and weights in large language models using different
    fixed-point and MX formats. This benefit is particularly pronounced within the
    quantization formats ranging from 6 to 8 bits. Conversely, GPTQ demonstrated superior
    effectiveness in scenarios involving more aggressive weight quantization, particularly
    to 4 bits and 6 bits. Notably, our findings indicated that for quantization to
    MXINT8, neither GPTQ nor SmoothQuant is necessary to preserve the baseline accuracy.
    We demonstrated that quantizations using different MX formats deliver better perplexity
    compared to fixed-point formats with the same bit-width when the per-tensor quantization
    scheme is employed.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, contrary to the results of prior research [[2](#bib.bib2)], we
    illustrated that when applying SmoothQuant, it suffices to apply only the A-W
    SmoothQuant, as opposed to the original findings that recommended both A-W and
    A-A approaches. Throughout the paper, we have shown that by utilizing GPTQ and
    A-W SmoothQuant, and quantizing models to MX formats, we can significantly reduce
    the size of OPT models by up to $4\times$ with negligible perplexity degradation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Llm. int8 (): 8-bit
    matrix multiplication for transformers at scale,” arXiv preprint arXiv:2208.07339,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant:
    Accurate and efficient post-training quantization for large language models,”
    in International Conference on Machine Learning, pp. 38087–38099, PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ: Accurate post-training
    quantization for generative pre-trained transformers,” arXiv preprint arXiv:2210.17323,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled version
    of bert: smaller, faster, cheaper and lighter,” in NeurIPS EMC Workshop, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] B. Darvish Rouhani, R. Zhao, V. Elango, R. Shafipour, M. Hall, M. Mesmakhosroshahi,
    A. More, L. Melnick, M. Golub, G. Varatkar, et al., “With shared microexponents,
    a little shifting goes a long way,” in Proceedings of the 50th Annual International
    Symposium on Computer Architecture, pp. 1–13, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary,
    M. Cornea, E. Dellinger, K. Denolf, et al., “Microscaling data formats for deep
    learning,” arXiv preprint arXiv:2310.10537, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A. Vinogradsky,
    S. Massengill, L. Yang, R. Bittner, et al., “Pushing the limits of narrow precision
    inferencing at cloud scale with microsoft floating point,” Advances in neural
    information processing systems, vol. 33, pp. 10271–10281, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Qualcomm, “Qualcomm Cloud AI 100 Accelerator.” [https://www.qualcomm.com/developer/blog/2024/01/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx](https://www.qualcomm.com/developer/blog/2024/01/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx),
    20234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, and X. Liu,
    “Outlier suppression: Pushing the limit of low-bit transformer language models,”
    Advances in Neural Information Processing Systems, vol. 35, pp. 17402–17414, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Bondarenko, M. Nagel, and T. Blankevoort, “Understanding and overcoming
    the challenges of efficient transformer quantization,” arXiv preprint arXiv:2109.12948,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, et al., “OPT: Open pre-trained transformer language
    models,” arXiv preprint arXiv:2205.01068, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., “LLaMA: Open and efficient
    foundation language models,” arXiv preprint arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “AWQ: Activation-aware
    weight quantization for llm compression and acceleration,” arXiv preprint arXiv:2306.00978,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    and T. B. Hashimoto, “Alpaca: A strong, replicable instruction-following model,”
    Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca.
    html, vol. 3, no. 6, p. 7, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
    Y. Zhuang, J. E. Gonzalez, et al., “Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality,” See https://vicuna. lmsys. org (accessed 14
    April 2023), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLoRA: Efficient
    finetuning of quantized llms,” arXiv preprint arXiv:2305.14314, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Stability AI, “Stable Beluga.” [https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Dettmers and L. Zettlemoyer, “The case for 4-bit precision: k-bit inference
    scaling laws,” in International Conference on Machine Learning, pp. 7750–7774,
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He, “Zeroquant:
    Efficient and affordable post-training quantization for large-scale transformers,”
    Advances in Neural Information Processing Systems, vol. 35, pp. 27168–27183, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel mixture
    models,” arXiv preprint arXiv:1609.07843, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, S. Gou, P. Blunsom, A. Üstün,
    and S. Hooker, “Intriguing properties of quantization at scale,” arXiv preprint
    arXiv:2305.19268, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients
    through stochastic neurons for conditional computation,” arXiv preprint arXiv:1308.3432,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, and
    K. Gopalakrishnan, “Pact: Parameterized clipping activation for quantized neural
    networks,” arXiv preprint arXiv:1805.06085, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, and
    T. Blankevoort, “A white paper on neural network quantization,” arXiv preprint
    arXiv:2106.08295, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A
    survey of quantization methods for efficient neural network inference,” in Low-Power
    Computer Vision, pp. 291–326, Chapman and Hall/CRC, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and
    D. Kalenichenko, “Quantization and training of neural networks for efficient integer-arithmetic-only
    inference,” in Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 2704–2713, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. Nagel, M. v. Baalen, T. Blankevoort, and M. Welling, “Data-free quantization
    through weight equalization and bias correction,” in Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 1325–1334, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort,
    “Up or down? adaptive rounding for post-training quantization,” in International
    Conference on Machine Learning, pp. 7197–7206, PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. Wang, Q. Chen, X. He, and J. Cheng, “Towards accurate post-training
    network quantization via bit-split and stitching,” in International Conference
    on Machine Learning, pp. 9847–9856, PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry, “Accurate
    post training quantization with small calibration sets,” in International Conference
    on Machine Learning, pp. 4466–4475, PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and
    S. Gu, “Brecq: Pushing the limit of post-training quantization by block reconstruction,”
    arXiv preprint arXiv:2102.05426, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Z. Deng, X. Wang, S. Sharify, and M. Orshansky, “Mixed-precision quantization
    with cross-layer dependencies,” arXiv preprint arXiv:2307.05657, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee, “nuqmm: Quantized
    matmul for efficient inference of large-scale generative language models,” arXiv
    preprint arXiv:2206.09557, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh, “SpQR: A sparse-quantized
    representation for near-lossless llm weight compression,” arXiv preprint arXiv:2306.03078,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park, “OWQ: Outlier-aware weight
    quantization for efficient fine-tuning and inference of large language models,”
    in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 13355–13364,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” arXiv preprint
    arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “BART: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” arXiv preprint arXiv:1910.13461,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Lee, M. Kim, S. Baek, S. Hwang, W. Sung, and J. Choi, “Enhancing computation
    efficiency in large language models through weight and activation quantization,”
    in Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing, pp. 14726–14739, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] N. Trukhanov and I. Soloveychik, “Accurate block quantization in LLMs
    with outliers,” arXiv preprint arXiv:2403.20137, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and w/o SmoothQuant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tables [6](#A1.T6 "Table 6 ‣ A.1 Detailed Quantization Results of the OPT and
    llama2 Family w/ and w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") and [7](#A1.T7
    "Table 7 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and
    w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs") illustrate the perplexity of the OPT
    and llama2 family on WikiText-2-test, when quantized to various fixed-point and
    MX formats. Based on the experimental results of both families, we observe that:
    a) Aggressive quantization to small bit-widths significantly impairs model performance,
    whereas quantizing to higher bit-widths has a negligible effect on perplexity.
    b) Quantization results using different MXINT formats yield better perplexity
    compared to fixed-point formats with the same bit-width. c) Among MXINT formats
    with the same precision, enabling SmoothQuant is more advantageous when quantizing
    to formats with larger block sizes than to formats with smaller block sizes. d)
    SmoothQuant is effective with lower than 8-bit MXINT formats but not with 8-bit
    MXINT formats. e) Finally, for the studied models regardless of the quantization
    format and precision, the benefits of enabling SmoothQuant are marginal for larger
    models and higher quantization precisions, but notably pronounced for smaller
    models with more constrained bit-widths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Quantization results for the OPT models with different sizes on WikiText-2-test,
    with and without SmoothQuant. Act, Wgt, SQ, and PPL denote activation, weight,
    SmoothQuant, and perplexity, respectively. $\downarrow$: The vocabulary size for
    WikiText-2 is around 50000; a perplexity number larger than 50000 is most likely
    due to a numerical issue. LR: Likelihood Ratio of PPL with SmoothQuant enabled
    over PPL with SmoothQuant disabled.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | OPT-1.3B | OPT-6.7B | OPT-30B |'
  prefs: []
  type: TYPE_TB
- en: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
  prefs: []
  type: TYPE_TB
- en: '| format | SQ | SQ | SQ | SQ | SQ | SQ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (FP16) | 14.63 | N/A | N/A | 10.86 | N/A | N/A | 9.56 | N/A | N/A
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-64 | 14.63 | 14.62 | 1.00 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-32 | 14.62 | 14.62 | 1.00 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-8 | 16.48 | 15.44 | 1.07 | 15.01 | 11.17 | 1.34 | 315.83 | 9.79 | 32.26
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-128 | 17.21 | 14.63 | 1.18 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-64 | 14.81 | 14.63 | 1.01 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-32 | 14.64 | 14.63 | 1.00 | 10.86 | 10.86 | 1.00 | 9.57 | 9.56 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-16 | 14.63 | 14.63 | 1.00 | 10.87 | 10.86 | 1.00 | 9.58 | 9.56 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| INT-6 | 4421.20 | 291.79 | 15.15 | 12392.76 | 121.52 | 101.98 | 11415.95
    | 1131.71 | 10.09 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-128 | 25.40 | 24.64 | 1.03 | 11.02 | 10.98 | 1.00 | 9.60 | 9.52 |
    1.01 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-16 | 15.07 | 14.92 | 1.01 | 10.94 | 10.90 | 1.00 | 9.58 | 9.56 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| INT-4 | 83368.28 | 37182.13 | 2.24 | 13645.72 | 12064.92 | 1.13 | 34580.63
    | 125530.61${}^{\texttt{+}}$ | 0.28 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-128 | 2862.62 | 55.25 | 51.81 | 32.85 | 17.27 | 1.90 | 17.28 | 11.07
    | 1.56 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-16 | 25.04 | 22.76 | 1.10 | 13.82 | 14.26 | 0.97 | 10.60 | 10.47 |
    1.01 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Quantization results for the llama2-7B and llama2-13B models on WikiText-2-test,
    with and without SmoothQuant. Act, Wgt, SQ, and PPL denote activation, weight,
    SmoothQuant, and perplexity, respectively. $\downarrow$: the lower the metric,
    the better the result. We used per-tensor affine quantization for the fixed-point
    formats. LR: Likelihood Ratio of PPL with SmoothQuant enabled over PPL with SmoothQuant
    disabled.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | llama2-7B | llama2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
  prefs: []
  type: TYPE_TB
- en: '| format | SQ | SQ | SQ | SQ |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline (FP16) | 5.11 | N/A | N/A | 4.57 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-64 | 5.11 | 5.11 | 1.00 | 4.57 | 4.57 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT16-32 | 5.11 | 5.11 | 1.00 | 4.57 | 4.57 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-8 | 237.40 | 498.39 | 0.48 | 186.04 | 61.65 | 3.02 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-128 | 5.13 | 5.12 | 1.00 | 4.58 | 4.58 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-64 | 5.12 | 5.12 | 1.00 | 4.58 | 4.58 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-32 | 5.12 | 5.12 | 1.00 | 4.58 | 4.57 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT8-16 | 5.12 | 5.12 | 1.00 | 4.57 | 4.57 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-6 | 53068.17 | 52256.23 | 1.02 | 25028.43 | 39820.93 | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-128 | 5.30 | 5.26 | 1.01 | 4.72 | 4.68 | 1.01 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT6-16 | 5.18 | 5.16 | 1.00 | 4.62 | 4.61 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| INT-4 | 45024.28 | 38934.63 | 1.16 | 140992.44 | 136368.02 | 1.03 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-128 | 39.15 | 25.49 | 1.54 | 23.52 | 13.73 | 1.71 |'
  prefs: []
  type: TYPE_TB
- en: '| MXINT4-16 | 7.01 | 6.62 | 1.06 | 6.21 | 5.78 | 1.07 |'
  prefs: []
  type: TYPE_TB
- en: A.2 SmoothQuant and GPTQ Interaction for quantization of DistilGPT2 and the
    LLaMA family
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DistilGPT2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To understand how SmoothQuant interacts with GPTQ when quantizing a small model
    like DistilGPT2, we quantized this model to INT-8 and MXINT4 using these two methods
    individually, as well as in combination. (Table [8](#A1.T8 "Table 8 ‣ DistilGPT2\.
    ‣ A.2 SmoothQuant and GPTQ Interaction for quantization of DistilGPT2 and the
    LLaMA family ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs")). We found that the best results for
    both quantization formats are achieved when only the GPTQ method is applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: DistilGPT2 perplexity results on WikiText-2-test, with enabling/disabling
    GPTQ and A-W SmoothQuant. A, W, SQ, and PPL denote activation, weight, SmoothQuant,
    and perplexity, respectively. $\downarrow$: the lower the metric, the better the
    result. We used per-tensor affine quantization for the INT-8 format.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| W:INT-8 | W:MXINT4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | A-W SQ | DistilGPT2 (FP32 $\downarrow$PPL: 46.07) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 59.23 | 72.97 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 60.57 | 72.93 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 49.82 | 53.93 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 52.12 | 56.65 |'
  prefs: []
  type: TYPE_TB
- en: LLaMA and llama2 family.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We conducted a similar experiment for the LLaMA and llama2 families, demonstrating
    that for all studied models and quantization formats, except when quantizing llama2-13B
    to INT-8, the best results were obtained when only GPTQ was applied. For the latter
    case, SmoothQuant was necessary to improve the perplexity degradation of the quantized
    model (Table [9](#A1.T9 "Table 9 ‣ LLaMA and llama2 family. ‣ A.2 SmoothQuant
    and GPTQ Interaction for quantization of DistilGPT2 and the LLaMA family ‣ Appendix
    A Appendix ‣ Combining multiple post-training techniques to achieve most efficient
    quantized LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Quantization results for the LLaMA and the llama2 families with different
    sizes on WikiText-2-test, with enabling/disabling GPTQ and A-W SmoothQuant. A,
    W, SQ, and PPL denote activation, weight, SmoothQuant, and perplexity, respectively.
    $\downarrow$: the lower the metric, the better the result. We used per-tensor
    affine quantization for the INT-8 format.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 |'
  prefs: []
  type: TYPE_TB
- en: '| W:INT-8 | W:MXINT4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | LLaMA-7B (FP16 $\downarrow$PPL: 5.67) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 19.01 | 6.31 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 17.47 | 6.18 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 17.44 | 7.23 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 24.27 | 6.24 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | LLaMA-13B (FP16 $\downarrow$PPL: 5.09) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 28.90 | 5.43 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 32.86 | 5.47 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 28.41 | 5.32 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 31.82 | 5.37 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | LLaMA-30B (FP16 $\downarrow$PPL: 4.10) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 17.72 | 4.46 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 21.39 | 4.49 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 17.39 | 4.37 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 34.13 | 4.43 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | llama2-7B (FP16 $\downarrow$PPL: 5.11) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 237.40 | 5.56 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 303.96 | 5.61 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 231.23 | 5.47 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 239.88 | 5.50 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | SQ | llama2-13B (FP16 $\downarrow$PPL: 4.57) |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | disabled | 186.04 | 4.82 |'
  prefs: []
  type: TYPE_TB
- en: '| disabled | enabled | 58.16 | 4.93 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | disabled | 181.67 | 4.76 |'
  prefs: []
  type: TYPE_TB
- en: '| enabled | enabled | 48.61 | 4.86 |'
  prefs: []
  type: TYPE_TB
- en: A.3 Pareto frontier study for the OPT family
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure 4 illustrates the perplexity of the OPT family on WikiText-2-test when
    quantized to different formats. Each point on the figure corresponds to a quantization
    configuration, indicating the activation and weight formats and whether SmoothQuant
    and GPTQ are enabled or disabled. The quantized models positioned on Pareto frontiers
    are marked with a gray circle. We observe that, generally, for medium and large-sized
    models (e.g, OPT-13B, and OPT-30B) with restrictive quantization to MXINT4, the
    points appearing on the Pareto frontiers are those with only GPTQ enabled. However,
    for smaller models (e.g., OPT-1.3B), both GPTQ and SQ should be applied jointly
    to effectively reduce perplexity when aggressively quantizing the models to MXINT4\.
    For less restrictive quantization to MXINT6, only SmoothQuant is sufficient to
    mitigate perplexity degradation. Finally, with quantization to MXINT8, none of
    these techniques is required.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb11eb5d8502f6dd0b7ead2349e0d765.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Perplexity for the OPT family with different sizes when quantized
    to INT-8, MXINT8, and MXINT4\. The Y-axis represents perplexity. The X-axis represents
    model parameter size including the additional scale parameters required by the
    SmoothQuant quantization method. Note that the GPTQ algorithm does not introduce
    any additional model parameters during the inference. The Per-tensor affine scheme
    is used for INT-8 quantization of both activations and weights. A, W, and SQ denote
    activation, weight, and SmoothQuant, respectively. The points corresponding to
    the quantized models on Pareto frontiers are indicated by a gray circle.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Block size granularity effect on quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To study the effects of block size granularity on quantization to fixed-point
    and microscaling formats, we conducted a comprehensive study on DistilGPT2 and
    GPT-xl, the smallest and the largest networks from the GPT2 family. We considered
    quantization with block sizes of $16$ bits (Figure [5](#A1.F5 "Figure 5 ‣ A.4
    Block size granularity effect on quantization ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs")).
    For integer formats, we applied an affine quantization scheme, incorporating zero
    points and scales in three different formats: FP8-e5m2, FP8-e4m3fn, and FP32.
    For MXINT, the scale data type is INT8, and zero point is not used. We found that
    (a) with extreme quantization to 4-bit (points on the left cluster), most points
    on the Pareto frontier align with INT8 quantization with zero points and scales
    in the FP8-e4m3fn format; additionally, for a quantization granularity of 16,
    the INT8 with FP32 scale/zero point format also appears on the Pareto frontier,
    and (b) for quantization to 6-bit and 8-bit, the middle and right clusters, the
    majority of points on the Pareto frontier are associated with MX data-types. Notably,
    for DistilGPT2, points corresponding to INT8-FP32 are also present on the Pareto
    frontiers when the quantization granularity is small (16 and 32).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3550ea52897e02f7be389353400560eb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) DistilGPT2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cdae513ac2dbbd4a08835c4200046194.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) GPT2-XL
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Perplexity for (a) DistilGPT2 and (b) GPT2-XL when quantized to 8,
    6, and 4 bit-widths with different quantization granularities. The Y-axis represents
    perplexity. The X-axis represents the model size in MB. The affine scheme is used
    for INT quantization of weights. The points corresponding to the quantized models
    on Pareto frontiers are indicated by a gray circle.'
  prefs: []
  type: TYPE_NORMAL
