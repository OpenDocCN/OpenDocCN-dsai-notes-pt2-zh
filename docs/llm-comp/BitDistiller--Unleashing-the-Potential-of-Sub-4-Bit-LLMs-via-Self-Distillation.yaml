- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:19'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10631](https://ar5iv.labs.arxiv.org/html/2402.10631)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dayou Du¹, Yijia Zhang², Shijie Cao³, Jiaqi Guo³, Ting Cao³, Xiaowen Chu¹, Ningyi
    Xu²
  prefs: []
  type: TYPE_NORMAL
- en: ¹The Hong Kong University of Science and Technology (Guangzhou)
  prefs: []
  type: TYPE_NORMAL
- en: ²Shanghai Jiao Tong University
  prefs: []
  type: TYPE_NORMAL
- en: ³Microsoft Research Asia
  prefs: []
  type: TYPE_NORMAL
- en: ddu487@connect.hkust-gz.edu.cn, {zhangyijia, xuningyi}@sjtu.edu.cn,
  prefs: []
  type: TYPE_NORMAL
- en: '{shijiecao, jiaqiguo, ting.cao}@microsoft.com, xwchu@ust.hk'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The upscaling of Large Language Models (LLMs) has yielded impressive advances
    in natural language processing, yet it also poses significant deployment challenges.
    Weight quantization has emerged as a widely embraced solution to reduce memory
    and computational demands. This paper introduces BitDistiller, a framework that
    synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD)
    to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically,
    BitDistiller first incorporates a tailored asymmetric quantization and clipping
    technique to maximally preserve the fidelity of quantized weights, and then proposes
    a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which
    is employed in a self-distillation manner to enable faster convergence and superior
    model performance. Empirical evaluations demonstrate that BitDistiller significantly
    surpasses existing methods in both 3-bit and 2-bit configurations on general language
    understanding and complex reasoning benchmarks. Notably, BitDistiller is shown
    to be more cost-effective, demanding fewer data and training resources. The code
    is available at [https://github.com/DD-DuDa/BitDistiller](https://github.com/DD-DuDa/BitDistiller).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling up model sizes has been pivotal to the success of large language models
    (LLMs), yielding unprecedented performance across diverse natural language processing
    tasks Brown et al. ([2020](#bib.bib4)); Touvron et al. ([2023](#bib.bib41)); Kaplan
    et al. ([2020](#bib.bib20)). However, such escalating model size poses significant
    challenges in deployment, particularly on resource-constrained devices, due to
    the substantial memory footprint and computational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Weight quantization has emerged as a popular strategy to enhance the efficiency
    and accessibility of LLMs by reducing model size with minimal performance loss Gholami
    et al. ([2022](#bib.bib15)). In practice, 4-bit quantization has been widely adopted,
    offering a balance between a considerable compression ratio and the preservation
    of LLM capabilities Lin et al. ([2023](#bib.bib26)); Frantar et al. ([2022](#bib.bib13));
    Liu et al. ([2023a](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd8c4cfebad674b30763c028df07249d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Bit-Level scaling laws for code generation performance for 3B to
    34B parameter coder models. BitDistiller outperforms existing QAT methods in both
    3-bit and 2-bit settings. Details in Table [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation
    on Reasoning Tasks ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of
    Sub-4-Bit LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, sub-4-bit quantization significantly degrades the fidelity of model
    weights, leading to deteriorated model performance, especially in smaller models
    or tasks requiring complex reasoning Dettmers and Zettlemoyer ([2023](#bib.bib12)).
    To address this, researchers have developed various Post-Training Quantization
    (PTQ) and Quantization-Aware Training (QAT) methods Chee et al. ([2023](#bib.bib5));
    Shao et al. ([2023](#bib.bib38)). PTQ, while appealing without retraining, struggles
    to preserve model performance at very low precisions. In contrast, QAT incorporates
    quantization into the training loop, enabling dynamic adaptation to reduced precision
    and thus maintaining higher accuracy Liu et al. ([2023b](#bib.bib28)); Kim et al.
    ([2023a](#bib.bib22)). Despite its early promise, two fundamental challenges are
    essential for achieving high model performance in extreme low-bit QAT: how to
    maximally preserve weight fidelity during quantization, and how to effectively
    learn low-bit representations during training.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we present BitDistiller, a novel framework that synergizes QAT
    with Knowledge Distillation (KD) to significantly boost the performance of sub-4-bit
    quantized LLMs. To minimize quantization error, BitDistiller employs a tailored
    asymmetric quantization and clipping strategy to maintain the capabilities of
    the full-precision model as much as possible, particularly at ultra-low-bit levels.
    For efficient and effective low-bit representation learning, BitDistiller leverages
    a simple yet effective self-distillation approach, wherein the full-precision
    model acts as its own teacher to refine the low-bit student model. Notably, BitDistiller
    innovates with a Confidence-Aware Kullback-Leibler divergence (CAKLD) objective
    that optimizes knowledge transferring efficacy, enabling faster convergence and
    enhanced model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our empirical evaluations, conducted on a diverse suite of general language
    understanding and complex reasoning tasks including mathematics and coding, demonstrate
    that BitDistiller significantly outperforms existing PTQ and QAT methods in the
    realm of sub-4-bit quantization. As illustrated in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation"), BitDistiller achieves the most favorable scaling law in both
    3-bit and 2-bit configurations on the code reasoning benchmark. Moreover, BitDistiller
    is demonstrated to be more cost-effective, requiring less training data and fewer
    training resources, thereby marking a significant advancement toward deploying
    robust Large Language Models on resource-constrained devices.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Weight Quantization for LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PTQ and QAT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PTQ is directly applied to pre-trained models without additional training. PTQ
    for LLMs typically employs techniques that either adjust quantization error Frantar
    et al. ([2022](#bib.bib13)); Chee et al. ([2023](#bib.bib5)) or prioritize salient
    weights Dettmers et al. ([2023b](#bib.bib11)); Lin et al. ([2023](#bib.bib26));
    Kim et al. ([2023b](#bib.bib23)). However, the lack of retraining with PTQ may
    cause notable decreases in model performance at extremely low precisions. In contrast,
    QAT integrates quantization into the training phase, enabling the model to learn
    better representations for low-bit weights, as demonstrated by approaches like
    LLM-QAT Liu et al. ([2023b](#bib.bib28)), OmniQuant Shao et al. ([2023](#bib.bib38)),
    PB-LLM Shang et al. ([2023](#bib.bib37)), and BitNet Wang et al. ([2023](#bib.bib42)).
    Despite improved model performance, QAT is still challenged by the need of extensive
    training and data, with significant potential for further optimization and enhancement.
    In this work, we harness the synergy of QAT and KD to enhance the performance
    of quantized LLMs, especially at sub-4-bit settings.
  prefs: []
  type: TYPE_NORMAL
- en: Granularity and Format Optimizations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extensive research indicates that adopting finer-grained quantization approaches,
    such as group-wise quantization, can achieve higher accuracy compared to layer-wise
    or channel-wise methods Shen et al. ([2020](#bib.bib39)); Frantar et al. ([2022](#bib.bib13)).
    Floating-point formats (FP8/FP4/NF4) have been demonstrated to deliver superior
    accuracy compared to integer formats (INT8/INT4) in LLM quantization Kuzmin et al.
    ([2022](#bib.bib24)); Dettmers and Zettlemoyer ([2023](#bib.bib12)); Zhang et al.
    ([2023b](#bib.bib49)). Notably, asymmetric quantization methods, particularly
    for floating-point formats, outperform their symmetric counterparts by better
    accommodating the distribution of model weights Zhang et al. ([2023a](#bib.bib48)).
    BitDistiller aligns with these insights, employing finer granularity and asymmetric
    techniques for quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Knowledge Distillation for LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the realm of LLMs, white-box knowledge distillation (KD) has become increasingly
    prevalent due to the accessible distribution of the teacher model, which facilitates
    the transmission of knowledge representations to the student model Hinton et al.
    ([2015](#bib.bib18)); Zhu et al. ([2023](#bib.bib51)). Notably, MINILLM Gu et al.
    ([2023](#bib.bib16)) utilizes the reverse KLD to ensure the accuracy and fidelity
    of language generation. GKD Agarwal et al. ([2023](#bib.bib1)) has explored alternative
    divergences called the generalized Jensen–Shannon divergence (JSD) and addressed
    the distribution mismatch by sampling outputs from the student model during training.
  prefs: []
  type: TYPE_NORMAL
- en: To attain exceedingly high compression ratios, a promising method is to combine
    KD with model quantization, where KD can be effectively used to mitigate the accuracy
    decline of quantized models Zhang et al. ([2020](#bib.bib47)); Kim et al. ([2022](#bib.bib21)).
    In cutting-edge research applying QAT-based KD for LLMs, TSLD Kim et al. ([2023a](#bib.bib22))
    considers risks of overfitting and conducts logit distillation with ground truth
    loss. Similarly, LLM-QAT leverages randomly teacher-generated data for data-free
    distillation. In distinction from TSLD and LLM-QAT, we achieve better performance
    and cost-efficiency in the extremely low-bit quantization level.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f4ce7d34a29c9a88806d355a560837c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Depiction of the QAT-based KD framework of BitDistiller.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we introduce BitDistiller, a QAT with self-distillation framework
    for LLMs, as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3 Methodology ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"). To maximally
    preserve weight fidelity during quantization, we first present an asymmetric quantization
    and clipping method (see Section [3.1](#S3.SS1 "3.1 Asymmetric Quantization and
    Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation")). Second, to counteract the performance degradation
    caused by precision reduction, we adopt Knowledge Distillation and propose a novel
    Confidence-Aware KL divergence (CAKLD) objective, in which the full-precision
    model acts as a teacher and the low-precision one plays a student (see Section [3.2](#S3.SS2
    "3.2 Self Distillation with CAKLD ‣ 3 Methodology ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Methodology ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation") outlines the process of
    BitDistiller. Given the full-precision weight w, BitDistiller adopts the asymmetric
    clipping to alleviate outliers in w (Line [4](#alg1.l4 "In Algorithm 1 ‣ 3 Methodology
    ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")),
    prior to the training loop. Then, in each training step, BitDistiller forwards
    the model with the quantized weights ($w^{t}_{Q}$), computes the loss with the
    proposed CAKLD objective (Line [8](#alg1.l8 "In Algorithm 1 ‣ 3 Methodology ‣
    BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")-[9](#alg1.l9
    "In Algorithm 1 ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation")), and updates the full-precision weights (Line [11](#alg1.l11
    "In Algorithm 1 ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation")-[12](#alg1.l12 "In Algorithm 1 ‣ 3 Methodology ‣
    BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")) Bengio
    et al. ([2013](#bib.bib2)). When the training finishes, BitDistiller returns the
    final quantized weights.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 BitDistiller
  prefs: []
  type: TYPE_NORMAL
- en: 1:Full-precision weight $w$;
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Asymmetric Quantization and Clipping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adoption of finer granularities, or smaller group sizes, in weight quantization
    of LLMs inherently leads to asymmetrical distributions and the presence of outliers
    in weight groups. Proper management of asymmetry is crucial to maintaining model
    performance in low-bit PTQ regimes. Our investigation reveals that the effects
    of asymmetry are more prominent in extremely low-bit QAT, such as 3-bit and 2-bit
    configurations, necessitating tailored strategies to address these challenges.
    Therefore, in BitDistiller, we adopt asymmetric quantization techniques coupled
    with asymmetric clipping strategies to enhance the representational fidelity of
    quantized weights and maximally preserve the capabilities of the full-precision
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Asymmetric Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Previous studies have shown that floating-point formats (e.g., FP, NF) often
    outperform integer formats (INT) in LLM quantization Dettmers et al. ([2023a](#bib.bib10));
    Liu et al. ([2023a](#bib.bib27)). However, as the quantization level falls to
    2-bit, we observed a notable decline in the effectiveness of FP/NF formats. This
    advantage of FP/NF formats is attributed to their non-uniform nature, which can
    capture a wider range of values. Such a non-uniform distribution aligns better
    with the natural distribution of weight tensors in LLMs. In 2-bit cases, the limited
    representational capacity, offering only four distinct values, undermines the
    benefits of non-uniform distribution and impedes the efficient utilization of
    each numerical value. In light of these findings, we employ NF formats for quantization
    above 2-bit, while opting for the INT format at the 2-bit level.
  prefs: []
  type: TYPE_NORMAL
- en: 'For NF formats (e.g., NF3), we adopt the AFPQ method Zhang et al. ([2023a](#bib.bib48))
    to enable asymmetric quantization, which establishes separate scales, $s_{pos}$,
    as shown in Equation [1](#S3.E1 "In Asymmetric Quantization ‣ 3.1 Asymmetric Quantization
    and Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation"). For INT formats (e.g., INT2), we utilize conventional
    asymmetric methods with a single scale and a designated zero point, as detailed
    in Equation [2](#S3.E2 "In Asymmetric Quantization ‣ 3.1 Asymmetric Quantization
    and Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $INT\text{-}Asym:Q(w)=\lfloor\frac{w-z}{s}\rceil$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Asymmetric Clipping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The strategy of clipping, which involves constraining the range of weight values,
    has been recognized for its contribution to maintaining high accuracy after quantization Sakr
    et al. ([2022](#bib.bib36)); Shao et al. ([2023](#bib.bib38)). However, naive
    clipping methods often fall short in effectiveness, while advanced clipping techniques
    come at a high computational cost which is prohibitive for practical QAT use Li
    et al. ([2019](#bib.bib25)); Jung et al. ([2019](#bib.bib19)). To circumvent these
    limitations, we propose the use of asymmetric clipping solely during the initial
    phase, prior to the commencement of QAT. Asymmetric clipping at initialization
    provides a good starting point that significantly contributes to the final overall
    quantized model accuracy without incurring the prohibitive costs associated with
    iterative clipping optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable asymmetric clipping for QAT initialization, given input features
    $X$, for each layer of the model. These values aim to minimize the output difference
    after quantization. Formally, the objective is to optimize the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\alpha^{*},\beta^{*}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle w_{c}=Clip(w,\alpha,\beta)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\begin{cases}\alpha\in[\min\_val,0)\\ \beta\in(0,\max\_val]\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'To demonstrate the efficacy of asymmetric quantization and clipping, we conduct
    a tensor-wise analysis. We selected a random weight tensor from the LLaMa-2-7B
    model and focused on a single output channel. As illustrated in Figure [3](#S3.F3
    "Figure 3 ‣ Asymmetric Clipping ‣ 3.1 Asymmetric Quantization and Clipping ‣ 3
    Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"),
    our approach to asymmetric quantization and clipping achieves higher fidelity
    preservation compared to symmetric quantization. A more detailed ablation study
    on the impact of asymmetric quantization and clipping on model performance is
    presented in Table [3](#S4.T3 "Table 3 ‣ Asymmetric Quantization and Clipping
    ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation") in Section [4.4](#S4.SS4 "4.4 Ablation
    Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa407fbb932d7c053a83677c5265e04f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: (Top) The original weight distribution of a single output channel
    in the final down projection layer of LLaMA-2-7B. (Middle&Bottom) The weight distribution
    after symmetric quantization and asymmetric quantization and clipping, both using
    3-bit quantization with the group size of 128.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Self Distillation with CAKLD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To better counteract the performance degradation caused by precision reduction,
    we propose to adopt Knowledge Distillation (KD) in QAT, where the full-precision
    model acts as a teacher and its quantized variant plays a student:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathcal{D}(P_{T}\parallel P_{S}),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{D}$ denote the full-precision and quantized model, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition for KD is two-fold. First, learning the token-level probability
    distributions potentially helps the quantized model better imitate its full-precision
    counterpart Hinton et al. ([2015](#bib.bib18)), thereby re-gaining the strong
    downstream performance. Second, owing to the generative nature of LLM, it is easy
    to scale up the data size for QAT with the full-precision model.
  prefs: []
  type: TYPE_NORMAL
- en: The divergence $\mathcal{D}$) on instruction tuning Chung et al. ([2022](#bib.bib7)),
    while Forward KL promotes mode-covering and is superior on general text generation
    tasks like summarization Narayan et al. ([2018](#bib.bib32)). To provide a general
    receipt for QAT, we aim to seek a way to trade off the mode-seeking and mode-covering
    behaviors automatically, instead of manual selection according to some empirical
    understanding of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75d031142eb9c9ae65736f2a7c89d024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of Reverse KL, Forward KL and CAKLD, when a Gaussian distribution
    tries to fit a Gaussian mixture (Teacher).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we propose a novel Confidence-Aware KL divergence, shorted as
    CAKLD. It blends the Reverse KL and Forward KL with a coefficient $\gamma$ estimated
    by the averaged token probability, so that the mode-seeking and mode-covering
    behaviors can be automatically traded off according to the full-precision model’s
    confidence on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\begin{split}\mathcal{D}_{CAKLD}(P_{T}\parallel P_{S})&amp;=\gamma\mathcal{D}_{KL}(P_{S}\parallel
    P_{T})\\ &amp;+(1-\gamma)\mathcal{D}_{KL}(P_{T}\parallel P_{S})\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}$$ |  | (5) |
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\displaystyle\begin{split}\mathcal{D}_{KL}(P_{T}\parallel P_{S})=&amp;\mathbb{E}_{(x,y)\sim\mathbb{D}}[\frac{1}{&#124;\{y\}&#124;}\sum^{&#124;\{y\}&#124;}_{i=1}\\
    &amp;\mathbb{E}_{c\sim P_{T}(\cdot&#124;x,y_{<i})}[\log{\frac{P_{T}(c&#124;x,y_{<i})}{P_{S}(c&#124;x,y_{<i})}}]]\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}$$ |  |
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Intuitively, when the full-precision model is confident on the training data,
    CAKLD will prefer more on the mode-seeking behaviors. Otherwise, CAKLD will advocate
    more on the mode-covering behaviors, as the full-precision model is not certain
    about the data and modeling its single mode is suboptimal. Figure [4](#S3.F4 "Figure
    4 ‣ 3.2 Self Distillation with CAKLD ‣ 3 Methodology ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation") visualizes the difference
    between Reverse KLD, Forward KLD and CAKLD when a Gaussian distribution tries
    to fit a Gaussian mixture. It is clear that CAKLD manages to trade off mode-seeking
    and mode-covering behaviors with the coefficient. For a detailed performance comparison
    and in-depth analysis, please refer to Figure [6](#S4.F6 "Figure 6 ‣ Distillation
    Objectives ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation") and Appendix [A.2](#A1.SS2
    "A.2 Implementation Details and Analysis of Confidence-Aware KLD ‣ Appendix A
    Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| LLaMA-2-7B | PPL $\downarrow$ | MMLU (5s) | PIQA | Hella. | Wino. | ARC-c
    | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BF16 | 5.47 | 46.45 | 77.86 | 57.14 | 68.35 | 43.34 | 58.63 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN | 6.65 | 38.65 | 75.24 | 53.70 | 67.32 | 38.56 | 54.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Bits | GPTQ | 6.38 | 39.57 | 75.46 | 51.68 | 67.16 | 38.39 | 54.45 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | AWQ | 6.71 | 39.68 | 76.27 | 55.14 | 67.56 | 40.61 | 55.85 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 6.10 | 41.22 | 77.47 | 54.41 | 67.09 | 39.08 | 55.85 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT | 6.02 | 41.32 | 77.26 | 54.74 | 68.35 | 40.61 | 56.46 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitDistiller (ours) | 5.97 | 43.65 | 76.99 | 55.38 | 68.35 | 41.21 | 57.12
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN | 3453 | 24.12 | 53.43 | 26.33 | 49.96 | 21.58 | 35.08 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | NaN | 23.12 | 49.51 | 25.04 | 49.57 | 22.69 | 33.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | AWQ | 2.2e5 | 25.38 | 52.39 | 25.70 | 50.12 | 21.33 | 34.98 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | OmniQuant | 12.84 | 25.42 | 58.92 | 29.20 | 50.83 | 19.45 | 36.76
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT | 9.30 | 23.62 | 70.08 | 43.79 | 61.64 | 29.09 | 45.64 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitDistiller (ours) | 8.08 | 29.25 | 73.61 | 48.70 | 61.09 | 33.27 | 49.18
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: General language task results of BitDistiller versus established PTQ
    and QAT methods on LLaMA-2-7B Model. Our method achieves leading performance in
    both 3-bit and 2-bit quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate BitDistiller on the LLaMA-2 Touvron et al. ([2023](#bib.bib41))
    families and domain-specific LLMs with sub-4–bit quantization. We have set up
    comparative experiments to demonstrate the proficiency of our method against existing
    PTQ and QAT methods. Our findings illustrate that BitDistiller substantially enhances
    both the general language performance and the accuracy of reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tasks and Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following Frantar et al. ([2022](#bib.bib13)); Lin et al. ([2023](#bib.bib26)),
    we benchmark LLaMA-2 Touvron et al. ([2023](#bib.bib41)) on general language tasks,
    including language modeling tasks (WikiText-2 Merity et al. ([2016](#bib.bib31))),
    common sense QA benchmarks (PIQA Bisk et al. ([2020](#bib.bib3)), HellaSwag Zellers
    et al. ([2019](#bib.bib46)), WinoGrande Sakaguchi et al. ([2021](#bib.bib35)),
    ARC Clark et al. ([2018](#bib.bib8))) and in-context learning ability (MMLU Hendrycks
    et al. ([2020](#bib.bib17))) under a few-shot setting. We also consider the complex
    reasoning tasks and evaluate various sizes of domain-specific LLMs, including
    WizardCoder Luo et al. ([2023](#bib.bib30)) on LLM-Humaneval-Benchmarks Chen et al.
    ([2021](#bib.bib6)) in the setting of greedy decode, and MetaMath Yu et al. ([2023](#bib.bib44))
    on GSM8K Cobbe et al. ([2021](#bib.bib9)). To evaluate the domain-specific LLMs
    of smaller sizes, we finetune OpenLLaMA-3B Geng and Liu ([2023](#bib.bib14)) with
    domain-specific datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PTQ baselines include vanilla round-to-nearest (RTN), GPTQ Frantar et al. ([2022](#bib.bib13)),
    AWQ Lin et al. ([2023](#bib.bib26)), Omniquant Shao et al. ([2023](#bib.bib38))
    and QuIP Chee et al. ([2023](#bib.bib5)). QAT baselines include LLM-QAT Liu et al.
    ([2023b](#bib.bib28)) and TSLD Kim et al. ([2023a](#bib.bib22)). Detailed PTQ
    and QAT settings can be found in appendix [A.1](#A1.SS1 "A.1 Details of PTQ and
    QAT Configuration ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization and Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We focus on 3-bit/2-bit group-wise quantization, with a group size of 128 (represented
    as ’g’) as the default setting except for the 3B models with a group size of 64
    because of the dimension constraint. Following Liu et al. ([2023b](#bib.bib28));
    Kim et al. ([2023a](#bib.bib22)), we utilize logits distillation. Prior to QAT,
    the coefficient $\gamma$. The implementation details and example analysis of CAKLD
    are available in Appendix [A.2](#A1.SS2 "A.2 Implementation Details and Analysis
    of Confidence-Aware KLD ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: Training Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use the instruction-tuning data from Alpaca Taori et al. ([2023](#bib.bib40))
    and the training set of WikiText-2 for general language tasks. For code understanding
    and generation, we use Evol-Instruct-Code Rosh ([2023](#bib.bib34)). For math
    reasoning we use MetaMathQA Yu et al. ([2023](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the instruction prompt $x$. (See Appendix [A.3](#A1.SS3 "A.3 Training
    Datasets Examples ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation") for more details of training datasets
    composition).'
  prefs: []
  type: TYPE_NORMAL
- en: Training Implementation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We leverage DeepSpeed Rasley et al. ([2020](#bib.bib33)) and HuggingFace repository Wolf
    et al. ([2020](#bib.bib43)) to devise a QAT-based KD framework enabling the distillation
    of models up to 34B. The model optimization is facilitated through the AdamW optimizer
    Loshchilov and Hutter ([2017](#bib.bib29)), applied with zero weight decay. We
    initialize the constant learning rate to 8e-6 and set the sequence length to 1024
    for the code-related task and 512 for others.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evaluation on Language Modeling Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation") presents a comparative analysis of BitDistiller’s
    performance against previous PTQ and QAT methods on general language tasks. BitDistiller
    surpasses competing methods in terms of WikiText-2 perplexity and MMLU (5-shot)
    accuracy. Furthermore, BitDistiller demonstrates consistent performance across
    various QA benchmarks. Notably, in 2-bit weight quantization, BitDistiller substantially
    increases the average accuracy by +3.54% over LLM-QAT Liu et al. ([2023b](#bib.bib28))
    and by +12.43% compared to the leading PTQ method Shao et al. ([2023](#bib.bib38)).
    Similar results on LLaMA-2-13B can be found in Table [9](#A1.T9 "Table 9 ‣ A.5
    Integration with AWQ For Quantization Strategies ‣ Appendix A Appendix ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation") in the Appendix
    [A.4](#A1.SS4 "A.4 Evaluation of General Language Tasks on LLaMA-2-13B ‣ Appendix
    A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation on Reasoning Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Domain-specific LLMs | HumanEval @WizardCoder | GSM8K @MetaMath |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 3B | 7B | 13B | 34B | 3B | 7B | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BF16 | 23.17 | 54.88 | 62.80 | 71.95 | 36.40 | 66.41 | 72.30 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN | 4.27 | 34.15 | 50.00 | 33.54 | 17.50 | 59.30 | 68.51 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 4.30 | 46.34 | 55.48 | 63.41 | 6.72 | 62.11 | 68.75 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Bits | AWQ | 16.46 | 45.73 | 53.04 | 67.07 | 21.87 | 62.34 | 68.67 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | OmniQuant | 10.36 | 44.51 | 54.88 | 68.90 | 23.67 | 61.70 | 68.28
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT | 18.29 | 48.78 | 57.92 | 66.46 | 26.25 | 60.78 | 66.62 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitDistiller (ours) | 20.73 | 53.66 | 63.41 | 69.51 | 32.50 | 64.38 |
    69.69 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN | 0.0 | 0.0 | 0.0 | 0.61 | 0.0 | 0.0 | 7.89 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 0.0 | 0.0 | 1.83 | 3.65 | 0.0 | 0.0 | 11.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | AWQ | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 7.89 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | OmniQuant | 0.0 | 0.0 | 20.12 | 26.83 | 0.0 | 0.0 | 9.45 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT | 0.0 | 14.63 | 15.21 | 29.27 | 6.56 | 23.13 | 36.64 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitDistiller (ours) | 7.31 | 36.59 | 42.07 | 46.34 | 16.09 | 51.02 | 61.33
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Reasoning task results of BitDistiller versus established PTQ and
    QAT methods on domain-specific LLMs. Our method achieves leading performance in
    both 3-bit and 2-bit quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation on Reasoning Tasks ‣ 4 Experiments
    ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")
    demonstrates the superior performance of BitDistiller on reasoning-based benchmarks,
    including HumanEval and GSM8K, across a range of domain-specific language model
    families. BitDistiller achieves improvements over other methods in both 3-bit
    and 2-bit quantization. Especially in 2-bit quantization, while other methods
    exhibit significant performance drops, BitDistiller maintains a commendable level
    of accuracy. Detailedly, our method outperforms LLM-QAT by a remarkable margin
    of 24.69%, achieving an accuracy of 61.33% on complex mathematical reasoning tasks.
    These outcomes bolster the potential for implementing ultra-low-precision inference
    deployment in practical reasoning tasks without substantially compromising performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Asymmetric Quantization and Clipping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| LLaMA-2-7B | PPL $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| (start $\mapsto$ end ) |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Bits | NF-Sym | 6.45 $\mapsto$ 39.27 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | $\rightarrow$ 42.61 |'
  prefs: []
  type: TYPE_TB
- en: '| $+$ 43.65 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | INT-Sym | 2.4e5 $\mapsto$ 26.03 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | $\rightarrow$ 24.82 |'
  prefs: []
  type: TYPE_TB
- en: '| $+$ 29.25 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablation study of asymmetric quantization and clipping on WikiText2
    perplexity and MMLU (5-shot). The "start $\mapsto$ end" notation denotes the metric
    values before and after training.'
  prefs: []
  type: TYPE_NORMAL
- en: In this ablation study, we evaluate the efficacy of quantization strategies
    on the LLaMA-2-7B model. Our approach examines the impact of asymmetric quantization
    and clipping techniques within QAT. We specifically assess the 3-bit and 2-bit
    quantization levels, reporting our findings in terms of Perplexity (PPL) and MMLU
    (5-shot).
  prefs: []
  type: TYPE_NORMAL
- en: 'As demonstrated in Table [3](#S4.T3 "Table 3 ‣ Asymmetric Quantization and
    Clipping ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation"), asymmetric quantization significantly
    enhances model performance. Notably, under a 2-bit configuration, PPL can be reduced
    from 3.4e2 to 16.94 in post-training. Furthermore, the application of asymmetric
    clipping during initialization yields additional performance gains upon training
    completion. See Appendix  [A.5](#A1.SS5 "A.5 Integration with AWQ For Quantization
    Strategies ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation") for integration with other PTQ methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/28340878193de2c22949d5511b09e50c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d98085dba963f0ca59ca540e18c3a5e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Comparative analysis of using various data generation methods on
    WizardCoder-7B. (a) shows the per-token cross-entropy loss. (b) presents the HumanEval
    Pass@1\. (‘QAT w.o. KD’ indicates the baseline where only the ground truth dataset
    is used for supervised fine-tuning, without knowledge distillation.)'
  prefs: []
  type: TYPE_NORMAL
- en: In our analysis, we meticulously evaluated the logit information of the teacher
    model by computing the cross-entropy loss (CELoss) for various outputs $y$.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation Objectives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Figure [6](#S4.F6 "Figure 6 ‣ Distillation Objectives ‣ 4.4 Ablation Studies
    ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation"), we demonstrate the effectiveness of our proposed Confidence-Aware
    KL Divergence (CAKLD) by showcasing performance indicators for reasoning tasks
    under different objective functions. Our findings show that CAKLD outperforms
    other objective functions. Though JSD also has a bounded coefficient for interpolation,
    in practice we observe that it has a weak ability to converge for QAT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/970b5ee32452d2e5dd447101a902b61e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Performance comparison between different objective functions on WizardCoder-7B
    and MetaMath-7B with domain-specific tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Analysis and Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comparison with QuIP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'QuIP enhances 2-bit PTQ for LLMs through incoherence processing. Its subsequent
    iteration, QuIP#¹¹1[https://cornell-relaxml.github.io/quip-sharp/](https://cornell-relaxml.github.io/quip-sharp/),
    refines this approach by shifting from scalar quantization to vector quantization
    via lattice codebooks, significantly narrowing the performance gap with 16-bit
    models. For a consistent comparison, we utilize the BF16 pretrained model and
    then apply Quip(#) and BitDistiller. As shown in Table [4](#S4.T4 "Table 4 ‣ Comparison
    with QuIP ‣ 4.5 Analysis and Discussion ‣ 4 Experiments ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation"), our BitDistiller surpasses
    QuIP across all benchmarks. In comparison with QuIP#, BitDistiller retains its
    superior performance in language modeling and programming, while QuIP# outperforms
    in mathematical reasoning. Being orthogonal to QAT with distillation, PTQ incorporating
    incoherence processing and vector quantization could potentially serve as an effective
    initialization method for BitDistiller. We intend to explore whether the integration
    of QuIP(#) into BitDistiller can further improve the performance of low-bit models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | LLaMA-2-7B | WizardCoder-7B | MetaMath-7B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PPL$\downarrow$ | MMLU (5s) | QA-avg | HumanEval | GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BF16 | 5.47 | 46.45 | 61.67 | 54.88 | 66.41 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | Quip | 728.15 | 24.30 | 38.19 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | Quip# | 8.97 | 30.90 | 52.40 | 12.96 | 60.00 |'
  prefs: []
  type: TYPE_TB
- en: '| BitDistiller | 8.08 | 29.25 | 54.17 | 36.58 | 51.02 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison of 2-bit quantized models using QuIP, QuIP#,
    and BitDistiller on LLaMA-2-7B, WizardCoder-7B, and MetaMath-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with TSLD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Prior work Kim et al. ([2023a](#bib.bib22)) introduced Token-Scaled Logit Distillation
    (TSLD) to alleviate overfitting during QAT. To facilitate a direct and fair comparison
    between TSLD and our CAKLD, we incorporate TSLD into the BitDistiller framework
    by replacing CAKLD with TSLD while keeping all other settings unchanged. As depicted
    in Figure [7](#S4.F7 "Figure 7 ‣ Comparison with TSLD ‣ 4.5 Analysis and Discussion
    ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation"), CAKLD not only converges more rapidly but also delivers superior
    overall performance compared to TSLD.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b483fc97a611cbacac720db02e970718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparison of TSLD and CAKLD on perplexity (left) and reasoning tasks
    performance (right).'
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of Self-Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [5](#S4.T5 "Table 5 ‣ Effectiveness of Self-Distillation ‣ 4.5 Analysis
    and Discussion ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation") compares 2-bit QAT performance using the LLaMA-2-7B
    or larger LLaMA-2-13B as the teacher model. Surprisingly, in practice the larger
    13B model didn’t improve accuracy, hinting that a teacher with the same model
    architecture as the student may enhance weight alignment and probability distribution
    matching, thereby improving model effectiveness. Further investigation and deeper
    analysis are needed in future work to fully understand the implications of different
    teacher-student sizes and architectures in QAT.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLaMA-2-7B | Quantized Student | Teacher | PPL $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | 7B | 13B | 8.12 | 28.27 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | 7B | 7B | 8.08 | 29.25 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Performance comparison of 2-bit quantized models using LLaMA-2-13B
    and LLaMA-2-7B as the teacher model.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [6](#S4.T6 "Table 6 ‣ Training Efficiency ‣ 4.5 Analysis and Discussion
    ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation") highlights the efficiency of BitDistiller compared to LLM-QAT
    Liu et al. ([2023b](#bib.bib28)) in quantizing the WizardCoder-7B model. The results
    demonstrate a dramatic reduction in the total time required for quantization:
    BitDistiller completes the process in approximately 3 hours on a single A100-80G
    GPU, as opposed to the hundreds of GPU hours required by LLM-QAT. (Original LLM-QAT
    uses 64 GPUs. For a direct and fair comparison, we evaluate the GPU hours needed
    for LLM-QAT on a single GPU.)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Devices | #Data | Time (Hours) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data Gen | Quant Init | QAT | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-QAT | 1 * A100 80G | 100K | 270 | 0 | 10.64 | 280.64 |'
  prefs: []
  type: TYPE_TB
- en: '| BitDistiller | 2K | 1.47 | 0.63 | 0.92 | 3.02 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Time required for LLM-QAT and BitDistiller to quantize WizardCoder-7B
    on a NVIDIA A100-80G.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BitDistiller leverages QAT with self-distillation to boost sub-4-bit LLM performance.
    The asymmetric quantization and clipping strategies, coupled with the innovative
    CAKLD objective, facilitate faster learning and superior performance. BitDistiller
    outperforms existing PTQ and QAT methods, achieving notable improvements in 3/2-bit
    settings across diverse language and reasoning tasks. Moreover, BitDistiller is
    more cost-efficient with fewer data and training resources required.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the promising results demonstrated by BitDistiller, it is important
    to acknowledge certain limitations and areas for future investigation.
  prefs: []
  type: TYPE_NORMAL
- en: A key limitation lies in the empirical nature of our findings. For instance,
    the reason behind the counterintuitive outcome where a 7B model outperforms a
    13B model as a teacher during the distillation of a 2-bit 7B student model. Having
    the same model architecture may be the reason but not detailed explained and understood.
    This highlights the need for a deeper investigation and theoretical exploration
    to complement our empirical observations.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead, we aim to extend BitDistiller to the realm of 1-bit (binary)
    quantization. While this presents a more challenging scenario, it also offers
    the potential for significant advancements in efficient LLM inference as binary
    weights enables computation with only additions and without multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the current iteration of BitDistiller applies exclusively to scalar
    quantization. As future work, we plan to explore the adaptation of BitDistiller
    to vector quantization. Preliminary research in this area indicates that vector
    quantization could yield substantial benefits, and incorporating it into our framework
    represents a natural and promising progression of our research.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank the HPC-AI-Integrated Intelligent Computing center of
    HKUST(GZ) for providing some of the hardware platforms in this project.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. 2023. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chee et al. (2023) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher
    De Sa. 2023. Quip: 2-bit quantization of large language models with guarantees.
    *arXiv preprint arXiv:2307.13304*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023a. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. 2023b. Spqr: A sparse-quantized representation for near-lossless
    llm weight compression. *arXiv preprint arXiv:2306.03078*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. The
    case for 4-bit precision: k-bit inference scaling laws. In *International Conference
    on Machine Learning*, pages 7750–7774\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng and Liu (2023) Xinyang Geng and Hao Liu. 2023. [Openllama: An open reproduction
    of llama](https://github.com/openlm-research/open_llama).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W
    Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient
    neural network inference. In *Low-Power Computer Vision*, pages 291–326\. Chapman
    and Hall/CRC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge
    distillation of large language models. *arXiv preprint arXiv:2306.08543*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jung et al. (2019) Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon
    Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. 2019. Learning to quantize
    deep networks by optimizing quantization intervals with task loss. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    4350–4359.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2022) Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, and Jungwook
    Choi. 2022. Understanding and improving knowledge distillation for quantization-aware
    training of large transformer encoders. *arXiv preprint arXiv:2211.11014*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2023a) Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong
    Chang, Wonyong Sung, and Jungwook Choi. 2023a. Token-scaled logit distillation
    for ternary weight generative language models. *arXiv preprint arXiv:2308.06744*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023b) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023b. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuzmin et al. (2022) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel,
    Jorn Peters, and Tijmen Blankevoort. 2022. Fp8 quantization: The power of the
    exponent. *Advances in Neural Information Processing Systems*, 35:14651–14662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan,
    and Rui Fan. 2019. Fully quantized network for object detection. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    2810–2819.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. 2023a. Llm-fp4: 4-bit floating-point quantized transformers.
    *arXiv preprint arXiv:2310.16836*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023b. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering
    code large language models with evol-instruct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.
    [Don’t give me the details, just the summary! topic-aware convolutional neural
    networks for extreme summarization](https://doi.org/10.18653/v1/D18-1206). In
    *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*,
    pages 1797–1807, Brussels, Belgium. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosh (2023) Nick Rosh. 2023. Evol-teacher: Recreating wizardcoder. [https://github.com/nickrosh/evol-teacher](https://github.com/nickrosh/evol-teacher).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sakr et al. (2022) Charbel Sakr, Steve Dai, Rangha Venkatesan, Brian Zimmer,
    William Dally, and Brucek Khailany. 2022. Optimal clipping and magnitude-aware
    differentiation for improved quantization-aware training. In *International Conference
    on Machine Learning*, pages 19123–19138\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. (2023) Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023.
    Pb-llm: Partially binarized large language models. *arXiv preprint arXiv:2310.00034*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. Omniquant:
    Omnidirectionally calibrated quantization for large language models. *arXiv preprint
    arXiv:2308.13137*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based
    ultra low precision quantization of bert. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 34, pages 8815–8821.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie
    Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet:
    Scaling 1-bit transformers for large language models. *arXiv preprint arXiv:2310.11453*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander Rush. 2020. [Transformers: State-of-the-art natural language processing](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 38–45, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, JamesT. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.
    Metamath: Bootstrap your own mathematical questions for large language models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi
    Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning
    with large language models. *arXiv preprint arXiv:2308.01825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv preprint arXiv:2009.12812*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Yijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu
    Wei, Ting Cao, and Ningyi Xu. 2023a. [Afpq: Asymmetric floating point quantization
    for llms](http://arxiv.org/abs/2311.01792).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023b) Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. 2023b. Integer or floating
    point? new outlooks for low-bit quantization on large language models. *arXiv
    preprint arXiv:2305.12356*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna
    Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal.
    2023. [Distillspec: Improving speculative decoding via knowledge distillation](http://arxiv.org/abs/2310.08461).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023.
    A survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Details of PTQ and QAT Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c6ce01f964e49e43f9daebd164c770d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparative Evaluation of PTQ Methods Using Various Calibration Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate PTQ methods by examining the impact of different calibration dataset
    distributions. Illustrated in Figure [8](#A1.F8 "Figure 8 ‣ A.1 Details of PTQ
    and QAT Configuration ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation"), calibrating with domain-specific data
    significantly enhances task-specific performance. For a fair comparison, all PTQ
    methods utilize the default calibration datasets for general language tasks and
    domain-specific calibration datasets Rosh ([2023](#bib.bib34)); Yu et al. ([2023](#bib.bib44))
    for reasoning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding QAT methods, it should be noted that the use of symmetric quantization
    in LLM-QAT results in degradation when grouped quantization is applied. To ensure
    a fair comparison, we replicate the approach with our setup and employ asymmetric
    uniform quantization.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Implementation Details and Analysis of Confidence-Aware KLD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0e4e27a88e81ac19cfe70a0b07f1bfe5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Text Generation Task
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afdd956b78fde7f3585d939656b3f85d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Reasoning Task
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Per-token confidence scores when teacher model (full-precision) conducting
    text generation task and reasoning task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a straightforward method in the pre-calculation of the coefficient $\gamma$.
    We utilize ten batches of training data to perform forward passes without updating
    parameters. Subsequently, we obtain the logits from the teacher model to compute
    the average token probability. In Figure [9](#A1.F9 "Figure 9 ‣ A.2 Implementation
    Details and Analysis of Confidence-Aware KLD ‣ Appendix A Appendix ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"), we have conducted
    analysis by examining the confidence scores of the teacher model in various tasks
    during next-word prediction. This analysis reveals that confidence levels can
    vary in text generation tasks, in contrast to reasoning tasks where each step
    is critical. Notably, in text generation tasks using LLMs, relying solely on the
    highest conditional probability through Greedy Search may result in local optima,
    overlooking more optimal sequences. These observations advocate for a mean-seeking
    Kullback-Leibler (KL) approach, encouraging the student model to encompass all
    potential modes of the teacher, thereby more effectively capturing the teacher’s
    general generative capabilities. In reasoning tasks, where the teacher model shows
    high confidence in next-word predictions, the student model should concentrate
    on learning the predominant mode from the teacher. Our proposed method, CAKLD,
    is designed to balance these two distinct modes effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Training Datasets Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For general language tasks, we mix token sequences from Alpaca and WikiText-2
    datasets with a ratio of 2:1\. Since WikiText-2 lacks explicit instructions, we
    utilize the first 128 tokens from the corpus as the input prompt for the teacher
    model’s generation process, setting the temperature to 0.7\. For tasks related
    to code understanding and generation, we employ the Evol-Instruct-Code dataset.
    For mathematical reasoning, we utilize MetaMathQA. Examples of the training data
    utilized are shown in Table [8](#A1.T8 "Table 8 ‣ A.5 Integration with AWQ For
    Quantization Strategies ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to highlight that our self-distillation process utilizes only
    a small portion of the involved datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Evaluation of General Language Tasks on LLaMA-2-13B
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Additional results of the General Language Tasks for LLaMA-2-13B are shown
    in Table [9](#A1.T9 "Table 9 ‣ A.5 Integration with AWQ For Quantization Strategies
    ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs
    via Self-Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Integration with AWQ For Quantization Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| LLaMA-2-7B | PPL $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| (start $\mapsto$ end ) |'
  prefs: []
  type: TYPE_TB
- en: '|  | INT-Asym | 6.65 $\mapsto$ 6.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Bits | AWQ | 6.48 $\mapsto$ 6.09 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | Clip-Asym | 6.21 $\mapsto$ 6.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AWQ + Clip-Asym | 6.18 $\mapsto$ 6.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | INT-Asym | 3.4e2 $\mapsto$ 16.94 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | AWQ | 2.2e5 $\mapsto$ Inf |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | Clip-Asym | 17.98 $\mapsto$ 8.08 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AWQ + Clip-Asym | 16.61 $\mapsto$ 8.13 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Results of quantization initialization for QAT combining with AWQ
    on PPL of WikiText-2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [7](#A1.T7 "Table 7 ‣ A.5 Integration with AWQ For Quantization
    Strategies ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation"), we explore the efficacy of combining asymmetric
    clipping with AWQ during the self-distillation process. Our results indicate that
    asymmetric clipping significantly enhances robustness in sub-4-bit quantization
    scenarios. For instance, at the 2-bit quantization level, both INT-Asym and AWQ
    methods are unable to complete the task. Conversely, Clip-Asym not only succeeds
    but also achieves a marked improvement in perplexity. It is also noteworthy that
    while integrating AWQ prior to QAT yields improvements initially, there is no
    additional performance gain after training. This suggests that a straightforward
    clipping approach is sufficiently effective for initializing QAT.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks | Resources | Number | Instruction | Teacher Response |'
  prefs: []
  type: TYPE_TB
- en: '| General Language Tasks | Alpaca | 4K | Find the five steps to meditate |
    1\. Find a quiet place to meditate$\backslash$n5\. When thoughts invade your mind,
    gently bring your attention back to your breath |'
  prefs: []
  type: TYPE_TB
- en: '| General Language Tasks | WikiText | 2K | In December 1856 , Rice brought
    forward two bills in Congress : an enabling act that would allow Minnesota to
    form a state cons | itution, and a bill that would allow Missouri to secede from
    the Union. The bills were defeated, and Rice was removed from the chairmanship
    of the House Judiciary Committee. …… |'
  prefs: []
  type: TYPE_TB
- en: '| Code-related Tasks | Evol-Instruct-Code | 2K | Create a nested loop to print
    every combination of numbers between 0-9, excluding any combination that contains
    the number 5\. Additionally, exclude any combination that contains a repeating
    digit. Implement the solution without using any built-in functions or libraries
    to check for repeating digits. | Here’s one way to implement the solution:$\textbackslash$n
    for i in range(10):…… |'
  prefs: []
  type: TYPE_TB
- en: '| Math-related Tasks | MetaMathQA | 2K | A store is comparing their profits
    throughout the year. They had profits of $1,500 in the first quarter of the year,
    $3,000 in the third quarter, and $2,000 in the fourth quarter. If their annual
    profits are $8,000, how much profit, in dollars, did they make in the second quarter?
    | $\backslash$nTo find x, we need to isolate it on one side of the equation…….
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: The Training Dataset examples for different tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLaMA-2-13B | PPL $\downarrow$ | MMLU (5s) | PIQA | Hella. | Wino. | ARC-c
    | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BF16 | 4.88 | 55.54 | 79.16 | 60.13 | 72.14 | 48.12 | 63.02 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN | 5.52 | 50.74 | 78.35 | 57.75 | 71.11 | 43.86 | 60.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 Bits | GPTQ | 5.41 | 50.63 | 77.26 | 56.84 | 70.72 | 42.83 | 59.66 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | AWQ | 5.47 | 49.64 | 77.09 | 57.52 | 70.32 | 43.86 | 59.69 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 5.48 | 48.97 | 77.64 | 57.08 | 70.88 | 44.28 | 59.77 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT | 5.32 | 51.60 | 78.29 | 58.45 | 70.56 | 44.62 | 60.70 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitDistiller (ours) | 5.20 | 53.21 | 78.67 | 58.66 | 71.59 | 46.67 | 61.76
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN | 109.21 | 24.74 | 57.56 | 32.56 | 50.75 | 21.84 | 37.49 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ | 15.08 | 23.70 | 56.04 | 30.99 | 51.22 | 19.28 | 36.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Bits | AWQ | 1.2e5 | 27.04 | 53.16 | 25.82 | 51.70 | 23.04 | 36.15 |'
  prefs: []
  type: TYPE_TB
- en: '| g128 | OmniQuant | 25.69 | 26.09 | 61.81 | 31.92 | 51.38 | 22.27 | 38.69
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLM-QAT | 7.80 | 29.37 | 74.10 | 49.49 | 63.14 | 33.87 | 49.99 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitDistiller (ours) | 6.78 | 37.50 | 75.84 | 51.30 | 65.90 | 37.46 | 53.60
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: General language task results of BitDistiller versus established PTQ
    and QAT Methods on LLaMA-2-13B Model. Our method achieves leading performance
    in both 3-bit and 2-bit quantization.'
  prefs: []
  type: TYPE_NORMAL
