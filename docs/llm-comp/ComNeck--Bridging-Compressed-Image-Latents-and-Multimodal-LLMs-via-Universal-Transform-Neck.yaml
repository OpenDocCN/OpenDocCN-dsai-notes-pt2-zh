- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19651](https://ar5iv.labs.arxiv.org/html/2407.19651)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chia-Hao Kao^(1,2)  Cheng Chien¹  Yu-Jen Tseng¹  Yi-Hsin Chen¹
  prefs: []
  type: TYPE_NORMAL
- en: Alessandro Gnutti²  Shao-Yuan Lo³  Wen-Hsiao Peng¹  Riccardo Leonardi²
  prefs: []
  type: TYPE_NORMAL
- en: ¹National Yang Ming Chiao Tung University, Taiwan
  prefs: []
  type: TYPE_NORMAL
- en: ²University of Brescia, Italy ³Honda Research Institute, USA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper presents the first-ever study of adapting compressed image latents
    to suit the needs of downstream vision tasks that adopt Multimodal Large Language
    Models (MLLMs). MLLMs have extended the success of large language models to modalities
    (e.g. images) beyond text, but their billion scale hinders deployment on resource-constrained
    end devices. While cloud-hosted MLLMs could be available, transmitting raw, uncompressed
    images captured by end devices to the cloud requires an efficient image compression
    system. To address this, we focus on emerging neural image compression and propose
    a novel framework with a lightweight transform-neck and a surrogate loss to adapt
    compressed image latents for MLLM-based vision tasks. The proposed framework is
    generic and applicable to multiple application scenarios, where the neural image
    codec can be (1) pre-trained for human perception without updating, (2) fully
    updated for joint human and machine perception, or (3) fully updated for only
    machine perception. The transform-neck trained with the surrogate loss is universal,
    for it can serve various downstream vision tasks enabled by a variety of MLLMs
    that share the same visual encoder. Our framework has the striking feature of
    excluding the downstream MLLMs from training the transform-neck, and potentially
    the neural image codec as well. This stands out from most existing coding for
    machine approaches that involve downstream networks in training and thus could
    be impractical when the networks are MLLMs. Extensive experiments on different
    neural image codecs and various MLLM-based vision tasks show that our method achieves
    great rate-accuracy performance with much less complexity, demonstrating its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]
    have demonstrated impressive abilities in various Natural Language Processing
    (NLP) tasks. The recent surge of research on Multimodal Large Language Models
    (MLLMs) extends LLM’s abilities to data beyond languages [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)], particularly
    images, opening up promising opportunities in various applications. MLLMs have
    shown surprising capability for many vision tasks such as classification [[10](#bib.bib10)],
    image captioning [[9](#bib.bib9), [11](#bib.bib11)], Visual Question Answering
    (VQA) [[5](#bib.bib5), [7](#bib.bib7)], and meme interpretation [[4](#bib.bib4)].
    These models excel in unseen tasks through instruction following or in-context
    learning, which is impossible for traditional vision networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, MLLM’s billion-scale size hinders deployment on resource-constrained
    end devices. While computation can be offloaded to the cloud, transmitting images
    to cloud-hosted MLLMs becomes necessary. Our study shows that directly feeding
    the decoded image, generated by a fixed neural image codec trained for human perception,
    into an MLLM (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck") (a))
    significantly degrades task performance, particularly when the image is coded
    at low rates. This highlights the need for efficient image compression that considers
    the requirements of downstream MLLM-based vision tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many prior works address image compression for machine vision, commonly referred
    to as coding for machines [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]. Two common approaches to coding for machines
    are image coding and feature coding. The image coding approaches [[12](#bib.bib12),
    [16](#bib.bib16)] optimize the image codec for specific downstream tasks and/or
    networks (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed
    Image Latents and Multimodal LLMs via Universal Transform-Neck") (b)), while the
    feature coding approaches [[17](#bib.bib17)] divide the task network into two
    parts and focus on compressing the intermediate features (Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (c)). However, both approaches face the same
    issue: they limit the trained system to be only suitable for one specific model
    or task, thus requiring separate parameters or models for different tasks. Additionally,
    while they may potentially yield high rate-accuracy performance, the training
    process becomes challenging when one needs to back-propagate a training objective
    through a massive MLLM to train the neural image codec. In practice, the billion-scale
    parameters of MLLMs make the existing coding for machine methods inapplicable.
    To the best of our knowledge, there have been no attempts to tackle compression
    for MLLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a172d030b06ad8e1630a62e8283db84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: On the left is inadequate frameworks for image compression for MLLMs,
    where the image codec is trained for (a) human perception, (b) the downstream
    task network, or (c) compressing the intermediate features of the task network.
    On the right is the proposed transform-neck and surrogate loss under three distinct
    scenarios, with the image codec (d1) pre-trained for human perception, (d2) updated
    for joint human and machine perception, or (d3) updated for machine perception.'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose the first neural image compression system for MLLM-based
    vision tasks that enables compressed image latents to suit the needs of downstream
    MLLMs. The proposed method involves a lightweight transform-neck and a surrogate
    loss. The transform-neck adapts compressed image latents to match the intermediate
    features of the CLIP visual encoder [[18](#bib.bib18)], a common component in
    many popular MLLMs. This approach avoids the image decoding process and reduces
    computational complexity. To address the massive size of MLLMs, the surrogate
    loss updates our system with the CLIP visual encoder, refraining from back-propagating
    the task loss through the heavy MLLM. The transform-neck trained with the surrogate
    loss is universal and readily applicable to various MLLMs that share the same
    CLIP visual encoder, without requiring re-training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed method is generic and applicable to different neural image codecs
    under various application scenarios. First, if thedownstream applications prioritize
    image reconstruction quality for human interaction, our method can work with an
    off-the-shelf image codec trained for human perception (Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (d1)). Without any modification or re-training
    of the codec, our method adapts the compressed image latents while maintaining
    the same image reconstruction quality. Second, when allowing image codecs to be
    updated, we propose a multi-task training strategy that optimizes the codec for
    both human and machine perception (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") (d2)). This significantly improves MLLM performance at the cost
    of a marginal drop in reconstruction quality. Finally, we consider an extreme
    setting in which the applications prioritize machine perception over image reconstruction.
    In this case, the encoder and the transform-neck are jointly optimized for the
    MLLM systems exclusively (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")
    (d3)). On top of that, our transform-neck is agnostic to the architecture of neural
    image codecs, being able to work with various models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The contributions of this work are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It marks the first exploration into the field of neural image coding for MLLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proposed transform-neck adapts the compressed image latents to downstream
    MLLMs, avoiding the need for image decoding and saving computational complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proposed surrogate loss leverages the CLIP visual encoder to update the
    system, avoiding back-propagating the task loss through the heavy MLLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our method is agnostic to the downstream MLLMs or tasks; without re-training,
    the resulting system is readily applicable to a wide variety of MLLMs sharing
    the same visual encoder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our framework is able to accommodate various application scenarios that involve
    human perception, machine perception, or both.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Multimodal Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, there has been a surge of interest in MLLMs following the impressive
    demonstration of LLM’s ability in the NLP field [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)]. Many have sought to extend the success of these models from text
    to other modalities, particularly images, and several works have shown their effectiveness
    on various tasks, such as image captioning [[11](#bib.bib11), [7](#bib.bib7),
    [9](#bib.bib9), [19](#bib.bib19), [20](#bib.bib20)], VQA [[5](#bib.bib5), [9](#bib.bib9),
    [20](#bib.bib20)], Referring Expression Comprehension (REC) [[6](#bib.bib6), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)], few-shot classification [[24](#bib.bib24),
    [10](#bib.bib10)], action anticipation [[25](#bib.bib25)], meme interpretation [[4](#bib.bib4),
    [26](#bib.bib26)], biomedical reasoning [[27](#bib.bib27)], OCR-free math reasoning [[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: Most existing MLLM approaches use a visual encoder to process the input image
    data, and then introduce a connector to bridge the image features to the tokens
    understandable by the LLM. Earlier works adopt simpler connector designs, such
    as linear projectors [[6](#bib.bib6), [19](#bib.bib19)], while subsequent works [[11](#bib.bib11),
    [5](#bib.bib5), [9](#bib.bib9)] have refined upon the design for both performance
    and complexity. Furthermore, the entire MLLM can be further fine-tuned to enhance
    its capabilities through instruction tuning [[19](#bib.bib19), [29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: A notable aspect of the MLLMs is their reliance on existing pre-trained visual
    encoders in their systems, with CLIP [[18](#bib.bib18)] visual encoder being a
    very common choice for a large number of methods [[5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [19](#bib.bib19),
    [23](#bib.bib23), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]. Trained
    on large image-text pair data, the CLIP visual encoder offers the feature space
    that combines language and image modalities in a sense, making it a desirable
    feature for MLLMs. Sharing the same visual encoder also gives us the opportunity
    to design a universal method for MLLMs. Notably, all the existing works on MLLMs
    do not consider the scenarios where image compression is present, which is a significant
    departure from our work.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Image Coding for Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural image compression systems have made significant progress in the past
    few years. As a matter of fact, several works [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)] have even outperformed the traditional codecs
    such as intra coding in VVC [[37](#bib.bib37)]. However, these methods primarily
    focus on the quality of reconstructed images for human perception. Coding for
    machines, in contrast, targets downstream machine vision instead of human perception,
    and it has attracted increasing attention recently.
  prefs: []
  type: TYPE_NORMAL
- en: A common approach simply involves training the compression system for a predefined
    target downstream computer vision task [[12](#bib.bib12), [16](#bib.bib16), [38](#bib.bib38)],
    enabling the reconstructed image to be suitable for machine vision, albeit potentially
    sacrificing perceptual quality. Conversely, Chamain et al. [[13](#bib.bib13)]
    tune the task network to better process the compressed images, while Chen et al. [[39](#bib.bib39)]
    leverage prompt-tuning method on Transformer-based codecs to boost performance
    on multiple tasks. Also, with the trend of the new JPEG AI learning-based image
    coding standard [[40](#bib.bib40)], some methods [[41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)] utilize the compressed image latents instead
    of the reconstructed image for recognition through bridging the latents to task
    network. On the other hand, Ding et al. [[17](#bib.bib17)] directly compress the
    intermediate features of recognition networks, while Feng et al. [[45](#bib.bib45)]
    learn the omnipotent features suitable for various tasks in a self-supervised
    manner and fine-tune each task network tail on such features.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to note that none of the coding for machine methods considers
    MLLMs at the receiver side. All the above-mentioned methods leverage back-propagation
    through recognition models to update the system, which is prohibitively expensive
    for MLLMs due to their huge scale. Therefore, the direct application of the same
    methods on MLLMs is almost infeasible. In addition, the use of a specific task
    loss restricts the resulting models to be optimized for a single task and recognition
    model, thus requiring re-training for each new task and incurring additional costs.
    We aim to be the first to propose a neural image compression system designed for
    MLLMs, achieved through a universal transform-neck and the adoption of a surrogate
    loss, which allows to bypass the necessity of involving the entire billion-scale
    MLLM in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Proposed Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3.1 Preliminaries: Neural Image Codecs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The high-level architecture of a neural image codec is depicted in the top
    central green box in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries: Neural Image
    Codecs ‣ 3 Proposed Method ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck"). In a typical hyperprior-based neural image
    compression system [[46](#bib.bib46)], the key components include the main encoder
    $g_{a}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d812c0ff2c9dee3e36d9052ac5fc63bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overall architecture of the proposed method.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Overall Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this work, we focus on the scenario where MLLMs are hosted on the server
    side, while users on end devices need to perform inference on the remote model
    using both text and images as inputs. Given the necessity of incorporating image
    compression to ensure efficient transmission, we propose a compression framework
    with the consideration of MLLMs as downstream application networks, aiming to
    mitigate the potential task performance drop caused by image compression. Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")
    illustrates our overall framework, which includes three major components: the
    neural image codec, our proposed transform-neck, and the MLLM. The depicted MLLM
    system adheres to a typical structure, consisting of a visual encoder, an LLM,
    and a connector component facilitating the transformation of features from the
    visual encoder to the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: During inference, an input image at the end device is passed through an encoder
    $g_{a}$ for transformation into a middle layer of the visual encoder of an MLLM.
    We opt to adapt the image latents rather than the reconstructed images because
    the image latents inherently contain the information needed for reconstructing
    the image, and potentially the semantic information for the downstream tasks (when
    the image encoder is guided properly). By skipping the image decoding process,
    our method offers reduced computational complexity while maintaining the task
    performance. The rest of the MLLM system operates without any changes to generate
    the desired output response.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experiments, we examine three distinct settings denoted as (d1), (d2)
    and (d3), as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck").
    Firstly, in (d1), we consider the practical scenario where a fixed off-the-shelf
    image codec pre-trained for human perception is directly used alongside our transform-neck.
    In this setting, our framework offers the option for users to decode the image
    latents $\hat{y}$ instead of the transform-neck. The quality of the decoded image
    is not affected by the introduction of our transform-neck, as the image codec
    is not updated in the present case. Then, we extend the analysis to scenarios
    (d2) and (d3) to examine the impact of jointly training the image codec and transform-neck.
    In (d2), the entire image codec undergoes re-training to accommodate both human
    and machine perception, while in (d3), the encoder is re-trained specifically
    for machine perception. Regardless of the context examined, we circumvent the
    difficulties of back-propagating the task loss through MLLMs by introducing a
    surrogate loss. We remark that the resulting system is readily applicable to a
    wide variety of MLLMs and tasks. It needs no re-training of the system when these
    MLLMs adopt the same visual encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Transform-neck
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our transform-neck is designed to be a lightweight module, consisting only
    of a linear projection, a self-attention mechanism, a feed-forward layer, and
    two layer norms, as shown in the central red box in Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"). Its
    purpose is to adapt the compressed image latents $\hat{y}$ to a form suitable
    for consumption by the downstream MLLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adapting the compressed image latents to existing MLLM systems is a non-trivial
    task, especially when aiming for a universal approach compatible with multiple
    MLLMs and tasks. To address this challenge, we observe that a large number of
    existing MLLM systems share the same pre-trained visual encoder, i.e. the CLIP
    visual encoder, as discussed in Section [2.1](#S2.SS1 "2.1 Multimodal Large Language
    Models ‣ 2 Related Works ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck"). Inspired by this observation, we propose
    to leverage the CLIP visual encoder, denoted by $C$.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the image encoder $g_{a}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Surrogate Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid involving huge MLLMs in the training process, thus bypassing back-propagation
    through them, we propose a surrogate loss, which is back-propagated through only
    the partial CLIP encoder $C^{\prime}$. To this end, we introduce the following
    distillation loss for training, aiming to minimize the Mean Squared Error (MSE)
    between the two output features:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{dist}=\text{MSE}(C^{\prime}(T(\hat{y})),C(x)).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: The surrogate loss enables the resulting transform-neck to be applicable to
    various MLLMs sharing the same visual encoder $C$ without re-training.
  prefs: []
  type: TYPE_NORMAL
- en: However, we notice that applying the distillation loss alone during the early
    training phase can make the training challenging and unstable, potentially due
    to the strict requirement of fitting the exact representation. To address this,
    we adopt a progressive training strategy, by including an additional cross-entropy
    loss at first, which provides a better update direction in the early training
    phase. Thus, using a classification dataset, we first compute the cosine similarity
    between the image and text embeddings produced from the fixed CLIP visual and
    text encoder, respectively, when provided with transformed image latents and ground
    truth class labels. Then, the probability distribution over different recognition
    classes is calculated by applying the softmax operation to the resulting cosine
    similarities, and the cross-entropy loss is evaluated with respect to the ground
    truth labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Training Objective Under Different Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Application scenarios for our method with corresponding training objective.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Application scenario |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Update image codec &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Human viewing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training objective |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (d1) Human perception | ✗ | ✓ | $\mathcal{L}_{dist}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (d2) Multi-task | ✓ | ✓ | $R+\lambda(\gamma d_{recon}(x,\hat{x})+\beta\mathcal{L}_{dist})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| (d3) Machine perception | ✓ | ✗ | $R+\lambda\mathcal{L}_{dist}$ |'
  prefs: []
  type: TYPE_TB
- en: 'To explore the capabilities of our method under the settings introduced in
    Section [3.2](#S3.SS2 "3.2 Overall Framework ‣ 3 Proposed Method ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"), we
    implement different training objectives, as summarized in Table [1](#S3.T1 "Table
    1 ‣ 3.5 Training Objective Under Different Settings ‣ 3 Proposed Method ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck").
    In (d1), since we consider a fixed off-the-shelf codec for retaining high quality
    reconstructed images for human perception, we train our transform-neck simply
    using distillation loss as the sole loss function, i.e. $\mathcal{L}_{d1}=\mathcal{L}_{dist}$
    .'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in (d2), referred to as multi-task, the image codec is allowed
    to be re-trained to accommodate both human and machine perception. As a result,
    it is trained jointly with the transform-neck on both the distillation loss and
    traditional rate-distortion loss, i.e. $\mathcal{L}_{d2}=R+\lambda(\gamma d_{recon}(x,\hat{x})+\beta\mathcal{L}_{dist})$
    weight the two losses.
  prefs: []
  type: TYPE_NORMAL
- en: In (d3), where the downstream applications do not require image reconstruction,
    the encoder and transform-neck are jointly optimized to minimize the trade-off
    cost between the rate $R$.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the transform-neck in better learning the transformation with
    the image latents, the training process for (d2) and (d3) is conducted in three
    stages. Initially, only $\mathcal{L}_{dist}$, while keeping the updated image
    codec frozen.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training Details and Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We utilize ELIC [[33](#bib.bib33)] as our image codec, which outputs image and
    hyperprior latents with $N=320$ leads to a good trade-off between human and machine
    perception.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c88678042d5f0b29f3b9a5c7b11eaeaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Rate-accuracy comparison using various MLLMs on several tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Evaluation tasks with corresponding dataset, MLLM, and metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Dataset | MLLM | Metric |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Captioning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; COCO &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Karpathy Test [[48](#bib.bib48)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LLaMA- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adapter [[9](#bib.bib9)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CIDEr |'
  prefs: []
  type: TYPE_TB
- en: '| VQA | SEED-Bench [[49](#bib.bib49)] | Honeybee [[5](#bib.bib5)] | Score |'
  prefs: []
  type: TYPE_TB
- en: '| REC | RefCOCO-val [[50](#bib.bib50)] | Shikra [[6](#bib.bib6)] | Accuracy
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Few-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ImageNet [[47](#bib.bib47)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; V2L- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tokenizer [[10](#bib.bib10)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/d77e898dfb742179af493032b76ae254.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Reconstruction performance comparison on Kodak [[51](#bib.bib51)].'
  prefs: []
  type: TYPE_NORMAL
- en: Targeted MLLM-based Vision Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To validate the generalization ability of our proposed method, we evaluate
    its performance on four different MLLM systems for four different tasks. Note
    that our method is independent of the downstream tasks, and thus the same set
    of transform-necks is used for all the evaluations. Additionally, all the MLLMs
    are employed off-the-shelf without any fine-tuning. The tasks, datasets, corresponding
    MLLMs, and metrics are listed in Table [4.1](#S4.SS1.SSS0.Px1 "Training Details
    and Datasets. ‣ 4.1 Experimental Setting ‣ 4 Experimental Results ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"). These
    configurations follow the settings outlined in their original papers and the accompanying
    code, except for the few-shot classification task due to the inaccessibility of
    the code. We thus design a 5-way 1-shot classification scenario to evaluate the
    performance with in-context learning; the detailed setting is described in supplementary
    material.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We introduce two baseline methods for comparison. The first one, denoted as
    Reconstruction, involves inputting the reconstructed image generated by ELIC to
    the MLLM system. The second one, denoted as Post-processing, adapts the reconstructed
    image to MLLMs through a U-Net [[52](#bib.bib52)] post-processing network, which
    is trained using the same surrogate loss as that adopted by our method. We remark
    that these image-domain baselines incur higher complexity than our lightweight
    transform-neck, as they involve decoding the image and potentially processing
    it further with the post-processing network. This aspect is discussed in detail
    in Section [4.4](#S4.SS4 "4.4 Complexity Analysis ‣ 4 Experimental Results ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ Training Details and Datasets. ‣ 4.1 Experimental
    Setting ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck") illustrates the performance
    of the baseline methods and our proposed scheme for the three examined scenarios
    with regards to two aspects: coding bit-rate, calculated as bits per pixel (bpp),
    and task performance. When comparing the baselines and our method in scenario
    (d1), where the original ELIC is trained solely for human perception, we make
    the following observations. (1) Straightforwardly using the reconstructed images
    generated by a codec trained for human perception leads to a significant performance
    drop across all the tasks (Reconstruction). Such performance decline is expected
    because the MLLMs are not trained with compressed images, thus hindering their
    recognition performance. This highlights the necessity of adapting image compression
    and/or image latents to MLLMs. (2) In contrast, our transform-neck method successfully
    boosts the performance using the same latent representations for reconstructing
    the image in Reconstruction, confirming the effectiveness of the proposed latent
    transformation without the decoding process. (3) Post-processing is able to reach
    comparable performance to our (d1), offering another viable solution to the problem.
    However, it is worth noting that Post-processing requires relatively higher computational
    complexity with respect to our transform-neck method, rendering our approach preferable
    (see Section [4.4](#S4.SS4 "4.4 Complexity Analysis ‣ 4 Experimental Results ‣
    ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"))
    .'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  Captioning'
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA-Adapter  &#124;
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/a0668bda023469c5ddf2c5171dffc577.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: A man is walking an elephant down a path. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Post-processing: A man feeding an elephant with his hand. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ours (d1): A man is petting an elephant on the head. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.0958  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  REC'
  prefs: []
  type: TYPE_NORMAL
- en: Shikra  &#124;
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; I’m trying to locate man with mask on in <img>. Can you determine its
    coordinates for me? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/ed41e02e45d5df1cef4d9548ea212d94.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.061  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on image captioning with LLaMA-Adapter and REC with Shikra.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we evaluate the effects of allowing the image codec to be re-trained.
    First, we observe that (d2) outperforms both (d1) and Post-processing. This indicates
    that fine-tuning the encoder indeed results in a more suitable latent representation
    that can be better adapted to MLLMs. When examining the extreme setting (d3),
    we see significant further improvement in the task performance, approaching the
    performance upper bound with uncompressed images. This improvement comes at the
    cost of the image reconstruction quality, which, however, is not relevant in (d3).
    Figure [4](#S4.F4 "Figure 4 ‣ Training Details and Datasets. ‣ 4.1 Experimental
    Setting ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck") illustrates the rate-visual
    quality curves associated with the three scenarios. Interestingly, (d2) exhibits
    only a marginal PSNR drop compared to (d1), while (d3) significantly compromises
    the quality of the decoded image. We stress that our framework (i.e. the surrogate
    loss and transform-neck) is able to accommodate different application scenarios,
    allowing for a variable trade-off between the task performance and the image reconstruction
    quality.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Visualization of the Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present the visualization of outcomes with downstream MLLM-based vision
    tasks in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Performance Comparison ‣ 4 Experimental
    Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck"). Our method (d1) is compared with the two baseline methods, Reconstruction
    and Post-processing, with particular focus on how these models work at low bitrates
    to reflect a bandwidth-limited scenario. In the second and third columns (from
    left to right), we visualize the reconstructed and post-processed images from
    the two baselines, respectively, which exhibit drastically different characteristics.
    The former (Reconstruction) produces blurry and smooth images, while the latter
    (Post-processing) introduces some artificial patterns into the post-processed
    images. Compared with the baselines, our method yields MLLM results closer to
    the ground truth across all the tasks. Due to the space constraint, the results
    of VQA and few-shot classification are visualized in the supplementary material.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparison of the kMACs/pixel and model size. The table omits the
    shared components of the two methods, i.e. the image encoder, the partial CLIP
    visual encoder, the connector, and the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Component | Params (M) | kMAC/pixel |'
  prefs: []
  type: TYPE_TB
- en: '| Ours (d1, d2, or d3) | Transform-neck | 13.19 | 52.795 |'
  prefs: []
  type: TYPE_TB
- en: '| Post-processing | Decoder | 7.34 | 64.16 (+386%) | 112.00 | 1017.96 (+1828%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Post-processing network | 31.04 | 835.72 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; First 2 layers of CLIP visual encoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 25.78 | 70.24 |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Complexity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.3 Visualization of the Results ‣ 4 Experimental
    Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") compares the computational complexity between our proposed method
    and Post-processing baseline in terms of model size and the kilo-multiply-accumulate-operations
    per pixel (kMACs/pixel). Note that our method in Table [3](#S4.T3 "Table 3 ‣ 4.3
    Visualization of the Results ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed
    Image Latents and Multimodal LLMs via Universal Transform-Neck") refers to any
    of (d1), (d2), and (d3), since they share the same computational complexity characteristics
    at inference time. Our method offers a lightweight solution with only 13 million
    parameters, as opposed to 64 million parameters with the post-processing approach.
    Moreover, in terms of kMAC/pixel, the difference stands out even more, considering
    that the post-processing network operates at the full image resolution while our
    method operates in the latent domain, where the image latents have a much smaller
    spatial resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/442cea3b76fc579d1d154c86a9c2f3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Partial CLIP visual encoder
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e40cbd6abbc871cd87e27325b28e38c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Training objectives
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3a97183c8b81e27985f91ab54fe3b4b6.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Different image codecs
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Rate-accuracy comparison for three ablation studies evaluated using
    LLaMA-adapter with image captioning on COCO Karpathy test split.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following ablation experiments are performed based on (d1) to justify our
    design choices.
  prefs: []
  type: TYPE_NORMAL
- en: Partial CLIP Visual Encoder.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This experiment investigates the proper number of Transformer layers to remove
    from the CLIP visual encoder in order to strike a good balance between complexity
    and performance. As shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Complexity Analysis
    ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (a), removing the first one or two layers
    achieves similar performance, whereas removing four or eight layers results in
    a noticeable performance drop. We thus remove the first two layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Objective.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") (b) presents the performance of our method when trained exclusively
    with the cross-entropy loss or distillation loss. It is observed that training
    with only the cross-entropy loss results in a significant performance drop. Although
    providing a good initial update direction, this strategy is unable to learn an
    effective transformation for MLLMs. Instead, training solely with the distillation
    loss fails to update the transform-neck properly and leads to far inferior performance.
    This is potentially due to the more stringent requirement of fitting the exact
    feature representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Different Image Codecs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") (c) presents the performance comparison between our method and
    Reconstruction when they are tested with ELIC and TIC [[34](#bib.bib34), [53](#bib.bib53)].
    TIC is a Transformer-based codec, whereas ELIC is a convolutional neural network-based
    codec. We see that our transform-neck still outperforms Reconstruction by a significant
    margin when the image codec is changed from ELIC to TIC. This indicates that our
    method is still effective on a different type of image codec.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper proposes the first image compression system tailored to Multimodal
    Large Language Models (MLLMs). It introduces a transform-neck that bridges the
    compressed image latents and the intermediate layer of the CLIP visual encoder,
    a common component in MLLMs. By using our proposed surrogate loss, we avoid involving
    the MLLM in the training process, making our transform-neck universally applicable.
    With lower computational complexity, our method has demonstrated effectiveness
    across a wide variety of tasks, MLLMs, and neual image codecs, outperforming other
    baselines in extensive experiments. One consideration is that it requires the
    same pre-trained CLIP visual encoder to leverage the universal transform-neck,
    which may limit compatibility with MLLMs that use custom visual encoders. Furthermore,
    this paper focuses solely on the image compression aspect of MLLMs, leaving the
    exploration of video or audio coding for future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., “Llama: Open and efficient
    foundation language models,” arXiv preprint arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al., “Mistral 7b,”
    arXiv preprint arXiv:2310.06825, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 technical report,” arXiv
    preprint arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Cha, W. Kang, J. Mun, and B. Roh, “Honeybee: Locality-enhanced projector
    for multimodal llm,” in Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), June 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, “Shikra: Unleashing
    multimodal llm’s referential dialogue magic,” arXiv preprint arXiv:2306.15195,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz,
    M. Shoeybi, and S. Han, “Vila: On pre-training for visual language models,” in
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable
    multimodal models,” arXiv preprint arXiv:2312.11805, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] R. Zhang, J. Han, C. Liu, P. Gao, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li,
    and Y. Qiao, “Llama-adapter: Efficient fine-tuning of language models with zero-init
    attention,” in International Conference on Learning Representations (ICLR), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] L. Zhu, F. Wei, and Y. Lu, “Beyond text: Frozen large language models
    in visual signal comprehension,” in Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR), June 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models,” in International
    conference on machine learning, pp. 19730–19742, PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] N. Le, H. Zhang, F. Cricri, R. Ghaznavi-Youvalari, H. R. Tavakoli, and
    E. Rahtu, “Learned image coding for machines: A content-adaptive approach,” in
    2021 IEEE International Conference on Multimedia and Expo (ICME), pp. 1–6, IEEE,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] L. D. Chamain, F. Racapé, J. Bégaint, A. Pushparaja, and S. Feltman, “End-to-end
    optimized image compression for machines, a study,” in 2021 Data Compression Conference
    (DCC), pp. 163–172, IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. Matsubara, R. Yang, M. Levorato, and S. Mandt, “Supervised compression
    for resource-constrained edge computing systems,” in Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, pp. 2685–2695, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Liu, H. Sun, and J. Katto, “Improving multiple machine vision tasks
    in the compressed domain,” in 2022 26th International Conference on Pattern Recognition
    (ICPR), pp. 331–337, IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] N. Le, H. Zhang, F. Cricri, R. Ghaznavi-Youvalari, and E. Rahtu, “Image
    coding for machines: an end-to-end learned approach,” in ICASSP 2021-2021 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pp. 1590–1594, IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] D. Ding, Z. Chen, Z. Liu, X. Xu, and S. Liu, “Hierarchical image feature
    compression for machines via feature sparsity learning,” IEEE Signal Processing
    Letters, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable visual models
    from natural language supervision,” in International conference on machine learning,
    pp. 8748–8763, PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” in Conference
    on Neural Information Processing Systems (NeurIPS), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi,
    V. Chandra, Y. Xiong, and M. Elhoseiny, “Minigpt-v2: large language model as a
    unified interface for vision-language multi-task learning,” arXiv preprint arXiv:2310.09478,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei, “Kosmos-2:
    Grounding multimodal large language models to the world,” in International Conference
    on Learning Representations (ICLR), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Zhang, Z. Ma, X. Gao, S. Shakiah, Q. Gao, and J. Chai, “Groundhog:
    Grounding large language models to holistic segmentation,” in Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang,
    and Y. Yang, “Ferret: Refer and ground anything anywhere at any granularity,”
    in International Conference on Learning Representations (ICLR), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Yu, Y. Cheng, Z. Wang, V. Kumar, W. Macherey, Y. Huang, D. Ross, I. Essa,
    Y. Bisk, M.-H. Yang, et al., “Spae: Semantic pyramid autoencoder for multimodal
    generation with frozen llms,” Advances in Neural Information Processing Systems,
    vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Mittal, N. Agarwal, S.-Y. Lo, and K. Lee, “Can’t make an omelette without
    breaking some eggs: Plausible action,” in Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,
    M. Zeng, and L. Wang, “Mm-react: Prompting chatgpt for multimodal reasoning and
    action,” arXiv preprint arXiv:2303.11381, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon,
    and J. Gao, “Llava-med: Training a large language-and-vision assistant for biomedicine
    in one day,” Advances in Neural Information Processing Systems, vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth,
    S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch,
    and P. Florence, “Palm-e: An embodied multimodal language model,” in arXiv preprint
    arXiv:2303.03378, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing
    vision-language understanding with advanced large language models,” in International
    Conference on Learning Representations (ICLR), 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung,
    and S. Hoi, “Instructblip: Towards general-purpose vision-language models with
    instruction tuning,” Advances in Neural Information Processing Systems, vol. 36,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Q. Ye, H. Xu, J. Ye, M. Yan, H. Liu, Q. Qian, J. Zhang, F. Huang, and
    J. Zhou, “mplug-owl2: Revolutionizing multi-modal large language model with modality
    collaboration,” arXiv preprint arXiv:2311.04257, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,
    A. Mensch, K. Millican, M. Reynolds, et al., “Flamingo: a visual language model
    for few-shot learning,” Advances in neural information processing systems, vol. 35,
    pp. 23716–23736, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, and Y. Wang, “Elic: Efficient
    learned image compression with unevenly grouped space-channel contextual adaptive
    coding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 5718–5727, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Lu, F. Chen, S. Pu, and Z. Ma, “High-efficiency lossy image coding
    through adaptive neighborhood information aggregation,” arXiv preprint arXiv:2204.11448,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Liu, H. Sun, and J. Katto, “Learned image compression with mixed transformer-cnn
    architectures,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 14388–14397, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Xie, K. L. Cheng, and Q. Chen, “Enhanced invertible encoding for learned
    image compression,” in Proceedings of the 29th ACM international conference on
    multimedia, pp. 162–170, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-R.
    Ohm, “Overview of the versatile video coding (vvc) standard and its applications,”
    IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 10,
    pp. 3736–3764, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. Wang, Z. Wang, S. Wang, and Y. Ye, “Deep image compression towards
    machine vision: A unified optimization framework,” IEEE Transactions on Circuits
    and Systems for Video Technology, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y.-H. Chen, Y.-C. Weng, C.-H. Kao, C. Chien, W.-C. Chiu, and W.-H. Peng,
    “Transtic: Transferring transformer-based image compression from human perception
    to machine perception,” in Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 23297–23307, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Ascenso, E. Alshina, and T. Ebrahimi, “The jpeg ai standard: Providing
    efficient human and machine visual data consumption,” Ieee Multimedia, vol. 30,
    no. 1, pp. 100–111, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Liu, H. Sun, and J. Katto, “Improving multiple machine vision tasks
    in the compressed domain,” in 2022 26th International Conference on Pattern Recognition
    (ICPR), pp. 331–337, IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Liu, H. Sun, and J. Katto, “Learning in compressed domain for faster
    machine vision tasks,” in 2021 International Conference on Visual Communications
    and Image Processing (VCIP), pp. 01–05, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Mei, F. Li, L. Li, and Z. Li, “Learn a compression for objection detection
    - vae with a bridge,” in 2021 International Conference on Visual Communications
    and Image Processing (VCIP), pp. 1–5, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ballé, A. Shrivastava, and
    G. Toderici, “End-to-end learning of compressible features,” in 2020 IEEE International
    Conference on Image Processing (ICIP), pp. 3349–3353, IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. Feng, X. Jin, Z. Guo, R. Feng, Y. Gao, T. He, Z. Zhang, S. Sun, and
    Z. Chen, “Image coding for machines with omnipotent feature learning,” in Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXXVII, pp. 510–528, Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Variational
    image compression with a scale hyperprior,” arXiv preprint arXiv:1802.01436, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in 2009 IEEE conference on computer
    vision and pattern recognition, pp. 248–255, Ieee, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
    image descriptions,” in Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 3128–3137, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, “Seed-bench: Benchmarking
    multimodal llms with generative comprehension,” arXiv preprint arXiv:2307.16125,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame: Referring
    to objects in photographs of natural scenes,” in Proceedings of the 2014 conference
    on empirical methods in natural language processing (EMNLP), pp. 787–798, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] E. Kodak, “Kodak lossless true color image suite (PhotoCD PCD0992).”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in Medical image computing and computer-assisted
    intervention–MICCAI 2015: 18th international conference, Munich, Germany, October
    5-9, 2015, proceedings, part III 18, pp. 234–241, Springer, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Lu, P. Guo, H. Shi, C. Cao, and Z. Ma, “Transformer-based image compression,”
    in Data Compression Conference, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
    Y. Zhuang, J. E. Gonzalez, et al., “Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality,” See https://vicuna. lmsys. org (accessed 14
    April 2023), vol. 2, no. 3, p. 6, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in Computer Vision–ECCV
    2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
    Part V 13, pp. 740–755, Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Supplementary Material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adopt a progressive training strategy incorporating both the cross-entropy
    and distillation losses, divided into three stages. (1) We train our system using
    only the cross-entropy loss with a learning rate of $10^{-4}$ for 20 epochs. (3)
    Lastly, we train our system using only the distillation loss for 20 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: We use the Adam optimizer, configured with $\beta_{1}$. Weight decay is disabled.
    The transform-neck for each rate point undergoes training on an RTX 4090 for approximately
    three days during the training stage.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For few-shot classification with V2L-Tokenizer [[10](#bib.bib10)], we design
    a 5-way 1-shot classification evaluation scenario. In particular, we generate
    5000 groups of images from ImageNet dataset, where each group consists of five
    randomly sampled images from different classes, serving as the sample images,
    and one new image from one of the classes as the query image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different MLLM is utilized for the evaluation of our proposed method on each
    task. In Table [4](#A1.T4 "Table 4 ‣ Evaluation. ‣ A.1 Implementation Details
    ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck"), we provide the detailed specifications
    of the MLLM used in our evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The specifications of the MLLM used in our tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LLM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Captioning | LLaMA-Adapter v1 [[9](#bib.bib9)] | LLaMA-7B [[2](#bib.bib2)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| VQA | Honeybee-C-7B-M144 [[5](#bib.bib5)] | Vicuna-7B [[54](#bib.bib54)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| REC | Shikra-7B [[6](#bib.bib6)] | LLaMA-7B [[2](#bib.bib2)] |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot classification | V2L-Tokenizer [[10](#bib.bib10)] | LLaMA2-7B [[1](#bib.bib1)]
    |'
  prefs: []
  type: TYPE_TB
- en: A.2 Comparison with VVC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ A.2 Comparison with VVC ‣ Appendix A Supplementary
    Material ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via
    Universal Transform-Neck") compares Reconstruction and our method in (d1) using
    ELIC, with the state-of-the-art traditional codec VVC (VTM 17.0 intra coding).
    We set the QPs to $[37,40,43,46,49]$ for VVC. It is observed that VVC performs
    worse than Reconstruction across all the tasks, which is potentially due to (1)
    the small spatial resolution (256x256) of input images that is not optimal for
    VVC, (2) its inferior rate-distortion performance compared to ELIC as reported
    in [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08d02c9688906b881006b0677bfd0899.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Rate-accuracy comparison using VTM on several tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 More Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present additional visualization results on four different evaluation tasks,
    including image captioning (Figure [8](#A1.F8 "Figure 8 ‣ A.4 License of Assets
    Used ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image
    Latents and Multimodal LLMs via Universal Transform-Neck")), visual question answering
    (VQA) (Figure [9](#A1.F9 "Figure 9 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary
    Material ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via
    Universal Transform-Neck")), referring expression comprehension (REC) (Figure [10](#A1.F10
    "Figure 10 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary Material ‣
    ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")),
    and few-shot classification (Figure [11](#A1.F11 "Figure 11 ‣ A.4 License of Assets
    Used ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image
    Latents and Multimodal LLMs via Universal Transform-Neck")).'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 License of Assets Used
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary
    Material ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via
    Universal Transform-Neck") summarizes the used assets in our work along with their
    license terms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: List of assets used in the paper with their corresponding license.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Assets |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Licenses &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet [[47](#bib.bib47)] | Custom license. Available at https://image-net.org/download.php
    |'
  prefs: []
  type: TYPE_TB
- en: '| COCO [[55](#bib.bib55)] | CC BY 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SEED-Bench [[49](#bib.bib49)] | Apache 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-Adapter [[9](#bib.bib9)] | GPL-3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Honeybee [[5](#bib.bib5)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Source code: Apache 2.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pretrained weights: CC BY-NC 4.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Shikra [[6](#bib.bib6)] | CC BY-NC 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| V2L-Tokenizer [[10](#bib.bib10)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; No license provided. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Code available at https://github.com/zh460045050/V2L-Tokenizer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Task: Captioning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model: LLaMA-Adapter &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/1287368d73fa781cea163cb5ab29b31b.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: A microwave and a computer sitting on a desk. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Post-processing: A microwave and a refrigerator sitting on top of a
    table. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ours (d1): A microwave and a toaster oven on a counter. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.0725  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/26643bffd427cece4de622385d679b9a.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: Two cats are standing on the ground near a bench. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Post-processing: A dog and a cat are standing on a sidewalk. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ours (d1): Two dogs are standing near a bicycle on a sidewalk. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.0928  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/37a612663327a1f1cd1d52c742b819e5.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: A blurry picture of a blender with a knife. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Post-processing: A close up of a blurry image of a bug. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ours (d1): A close up of a knife cutting into a pizza. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.0910  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/81f7ad1f75fa589f3b61693790e78556.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: A young boy in a red shirt and tie posing for a picture.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Post-processing: A young boy standing in front of a wall with a clock.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ours (d1): A young boy in a tie and a white shirt. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.0899  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on image captioning with LLaMA-Adapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Task: Visual question answering &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model: Honeybee &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; What is the dog doing in the image? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A. Standing still  B. Chasing after something  C. Lying down  D. Jumping
    in the air &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/e785c5947eefd976a021536ca408d27d.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: B Post-processing: B Ours (d1): A GT: A &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.082  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; How many people are on the field in this image? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A. Four  B. Nine  C. Twelve  D. Eleven &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/a5a6898281982db592c8704fad237e77.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: D Post-processing: B Ours (d1): A GT: A &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.097  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; What is the person in the blue jacket holding? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A. A phone  B. Nothing  C. A wallet  D. A clipboard &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/8a6e02beae8a7c22a1e13c9600d8526e.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: D Post-processing: D Ours (d1): B GT: B &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.160  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; How many people are lighting candles in this image? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A. Two B. One C. Three D. Four &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/36074ceefa695df1d6fb572b54a812e0.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: A Post-processing: A Ours (d1): C GT: C &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.066  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on VQA with Honeybee.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Task: Referring expression comprehension &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model: Shikra &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Guide me to the location of brown bear within the image <img> by providing
    its coordinates. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/f7f568aa4f05ef41a0f0c88294bdc504.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.040  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Point me to the location of wine glass far left in the picture <img>
    by providing its coordinates. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/14c57556464e6101b47b431c895eb064.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.115  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Can you assist me in locating right female cop in <img>, and then provide
    its coordinates? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/7f518129e57fdac4bea93dada5f32247.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.098  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; In the photograph <img>, could you pinpoint the location of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; person holding a snowboard and tell me its coordinates? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/12a635cedb7eb43d755497dccc16d9e6.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP:0.088  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on REC with Shikra.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Task: Few-shot classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model: V2L-tokenizer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  Query                Examples  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/1be1fa69645318ee73026428b12c25e5.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/5eb6160283c73ffa8c7e8ac09c899de1.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: ptarmigan Post-processing: ptarmigan Ours (d1): walking
    stick GT: walking stick &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.0786  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  Query                Examples  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/cc8688af7dd02192d26c51a119e04722.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/c5d1a71b5886175480e26de3048d630a.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: hippopotamus Post-processing: wall clock Ours (d1):
    otterhound GT: otterhound &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.1369  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  Query                Examples  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/560623d17b486542176020958e974351.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ![[Uncaptioned image]](img/6539f048119b210a7b2417526401a46e.png) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction: horned viper Post-processing: horned viper Ours (d1):
    hornbill GT: hornbill &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  BPP: 0.2329  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on few-shot classification with V2L-tokenizer.'
  prefs: []
  type: TYPE_NORMAL
