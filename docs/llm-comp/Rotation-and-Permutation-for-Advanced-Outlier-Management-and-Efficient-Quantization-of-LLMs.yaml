- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.01721](https://ar5iv.labs.arxiv.org/html/2406.01721)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Haokun Lin¹¹1Equal contribution. ^(1,3,4), Haobo Xu¹¹footnotemark: 1 ², Yichen
    Wu¹¹footnotemark: 1 ⁴, Jingzhi Cui², Yingtao Zhang²,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linzhan Mou⁵, Linqi Song⁴, Zhenan Sun²²2Corresponding authors. ^(1,3), Ying
    Wei²²footnotemark: 2 ⁶'
  prefs: []
  type: TYPE_NORMAL
- en: ¹ School of Artificial Intelligence, University of Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ² Tsinghua University  ³ NLPR & MAIS, Institute of Automation, Chinese Academy
    of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ⁴ City University of Hong Kong  ⁵ Zhejiang University  ⁶ Nanyang Technological
    University
  prefs: []
  type: TYPE_NORMAL
- en: haokun.lin@cripac.ia.ac.cn xuhb20@mails.tsinghua.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: wuyichen.am97@gmail.com znsun@nlpr.ia.ac.cn ying.wei@ntu.edu.sg
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quantizing large language models (LLMs) presents significant challenges, primarily
    due to outlier activations that compromise the efficiency of low-bit representation.
    Traditional approaches mainly focus on solving Normal Outliers—activations with
    consistently high magnitudes across all tokens. However, these techniques falter
    when dealing with Massive Outliers, which are significantly higher in value and
    often cause substantial performance losses during low-bit quantization. In this
    study, we propose DuQuant, an innovative quantization strategy employing rotation
    and permutation transformations to more effectively eliminate both types of outliers.
    Initially, DuQuant constructs rotation matrices informed by specific outlier dimensions,
    redistributing these outliers across adjacent channels within different rotation
    blocks. Subsequently, a zigzag permutation is applied to ensure a balanced distribution
    of outliers among blocks, minimizing block-wise variance. An additional rotation
    further enhances the smoothness of the activation landscape, thereby improving
    model performance. DuQuant streamlines the quantization process and demonstrates
    superior outlier management, achieving top-tier results in multiple tasks with
    various LLM architectures even under 4-bit weight-activation quantization. Our
    code is available at [https://github.com/Hsu1023/DuQuant](https://github.com/Hsu1023/DuQuant).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) [[45](#bib.bib45), [7](#bib.bib7), [57](#bib.bib57),
    [18](#bib.bib18)] have demonstrated exceptional performance across a wide range
    of natural language processing tasks. However, their billions of parameters present
    considerable deployment challenges on resource-constrained edge devices, particularly
    in terms of memory usage and inference speed [[21](#bib.bib21), [16](#bib.bib16)].
    In response to these challenges, network quantization methods [[19](#bib.bib19),
    [23](#bib.bib23)] have been extensively explored to minimize memory usage by converting
    floating-point parameters into low-bit formats [[17](#bib.bib17), [30](#bib.bib30),
    [8](#bib.bib8)], and to expedite inference by quantizing both activations and
    weights for accelerating the matrix multiplication process [[52](#bib.bib52),
    [31](#bib.bib31), [61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: Among LLM quantization methods, a primary issue is the presence of activation
    outliers, which enlarge the quantization step sizes and subsequently cause significant
    accuracy loss [[49](#bib.bib49)]. To mitigate this problem, current research has
    developed various methods to address Normal Outliers in activations, which are
    persistent in several channels across all tokens [[12](#bib.bib12), [52](#bib.bib52)].
    However, besides Normal Outliers, there exists another type of activation outlier [[43](#bib.bib43),
    [32](#bib.bib32)], termed Massive Outliers. These outliers are characterized by
    their exceedingly high values and limited occurrence in a subset of tokens, as
    depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")(b). Unfortunately,
    existing LLM quantization methods struggle to effectively address these Massive
    Outliers. For instance, SmoothQuant [[52](#bib.bib52)], despite using a smooth
    factor to shift some of the activation outliers to the weight part, still cannot
    effectively handle Massive Outliers with extremely large values, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs")(c)(d). OmniQuant [[42](#bib.bib42)] and AffineQuant [[36](#bib.bib36)],
    on the other hand, exhibit training instability issues [[31](#bib.bib31)] due
    to the presence of Massive Outliers. Consequently, there is a pressing need for
    an LLM quantization approach that effectively addresses both Normal and Massive
    Outliers.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this challenge, we propose the Dual transformations Quantization (DuQuant)
    method. Our motivation is to redistribute the activation outlier values across
    different channels, facilitating easier quantization. Specifically, we construct
    the orthogonal rotation matrix and the orthogonal permutation matrix. By multiplying
    these matrices with the activations, we can effectively perform column transformations
    on the activations, which in turn allows for the redistribution of outliers. For
    the rotation transformation aspect, we first identify specific dimensions of outliers
    as the prior knowledge and employ a greedy algorithm to construct the rotation
    matrix. To enhance the multiplication efficiency, we utilize diagonal block-wise
    rotation matrices, with each matrix responsible for a small portion of the activations.
    However, this approach may result in uneven outlier magnitudes across different
    blocks. Therefore, we propose the zigzag permutation for reordering the activation
    channels, which promotes a more uniform distribution across different blocks.
    Concretely, we distribute the channels with the highest activations across the
    blocks in a back-and-forth pattern. After establishing blocks with uniformly distributed
    outlier magnitudes, we employ another rotation transformation to further redistribute
    the outliers within each block. Note that we multiply the weight matrix with the
    transpose of the rotation and permutation matrices at the same time, preserving
    the linear layer equivalence and smoothing weights. Theoretical analysis confirms
    that the rotation and permutation transformations greatly mitigate quantization
    challenges induced by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extensive evaluations demonstrate that our DuQuant approach significantly outperforms
    existing 4-bit weight-activation quantization baselines across various benchmarks.
    Notably, DuQuant achieves a 5% improvement in Commonsense QA tasks across all
    LLaMA model sizes and a 10% increase in zero-shot MMLU benchmarks for the Vicuna-v1.5-13B.
    Moreover, in practical applications with the LLaMA2-7B model, DuQuant not only
    accelerates prefilling phase by up to 2.08$\times$, with minimal impact on performance:
    only a 0.61 increase in perplexity and a 2.71% drop in accuracy compared to the
    FP16 model. These results highlight the effectiveness of DuQuant in enhancing
    both the efficiency and capacity of quantized LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09cb2891b2bad4ca1a55efd94c2ff26b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Visualizations of Outliers in LLaMA2-7B. (a) Input activation of
    Layer1 attention key projection shows Normal Outliers with relatively high magnitudes
    across all token sequences. (b) Input activation of Layer1 FFN down projection
    reveals Massive Outliers, presenting extremely high magnitudes (around 1400) at
    very few tokens. (c) Application of SmoothQuant on FFN down projection, illustrating
    its struggle with massive outliers in the Activation matrix. (d) Corresponding
    weight changes with SmoothQuant, highlighting the emergence of new outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normal Outliers and Massive Outliers.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Previous works [[12](#bib.bib12), [60](#bib.bib60), [30](#bib.bib30)] have
    highlighted the challenge posed by activation outliers in LLMs for model compression.
    These outlier features consistently manifest large values across specific feature
    dimensions and are present in all token sequences [[52](#bib.bib52)], which we
    refer to as Normal Outliers. Recently, a distinct type of outlier [[43](#bib.bib43),
    [32](#bib.bib32)], termed Massive Outliers, also known as attention sinks [[53](#bib.bib53)],
    has been observed in LLMs. The primary distinctions between normal and massive
    outliers are: 1) Normal outliers persist across all token sequences, whereas massive
    outliers are confined to a limited number of tokens. 2) Massive outliers exhibit
    significantly larger magnitudes, often surpassing 100 and being approximately
    1000 times greater than the median of other activations [[43](#bib.bib43)]. In
    our study, we delve deeper into the impact of these two distinct types of outliers
    on quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: Massive Outliers Exist at the Second Linear Layer of FFN Module.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to previous studies [[43](#bib.bib43), [32](#bib.bib32)] that observe
    massive outliers at the output of Transformer blocks, we first discover that these
    extremely large activations exist at the input of the down-projection layer within
    the FFN module. As depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"), the input of the down-projection layer in the LLaMA2-7B model Layer
    1 contains a single activation of significant magnitude (approximately 1200).
    This activation is isolated to one token and therefore classified as one of massive
    activations. This phenomenon is consistently observed across different layers
    and sizes of models, as illustrated in Appendix [H](#A8 "Appendix H More Visualizations
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Massive Outliers Enlarge Quantization Difficulty.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although previous studies [[52](#bib.bib52), [42](#bib.bib42), [36](#bib.bib36),
    [1](#bib.bib1)] have proposed various approaches to eliminate outlier features,
    they still face challenges in effectively managing massive outliers. SmoothQuant [[52](#bib.bib52)],
    for instance, attempts to shift the quantization difficulty from activations to
    weights by dividing the activation by a per-channel smoothing factor and multiplying
    it to the weight matrix. Nevertheless, we observe that this transfer at the input
    of the down-projection layer can cause the weights of the down-projection to display
    noticeable outliers, as demonstrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs") . This issue arises because massive outliers cause the smoothing factor
    to become significantly large. Moreover, extremely large outliers can lead optimization-based
    methods to encounter problems with loss explosion. Both OmniQuant [[42](#bib.bib42)]
    and AffineQuant [[36](#bib.bib36)] have had to exclude their learnable parameters
    for the down projection layer due to unstable gradients. Given the poor accuracy
    observed with 4-bit quantization, QUIK [[1](#bib.bib1)] opts to use INT8 quantization
    for the down projection layer and Atom [[61](#bib.bib61)] applies INT8 quantization
    for 128 outlier channels. Consequently, massive outliers introduce new challenges
    to the quantization process that existing methods cannot fully address. This observation
    has motivated us to develop rotation and permutation transformation, which effectively
    handles both massive and normal outliers and achieves state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we delve into the distribution of outliers and introduce our
    proposed DuQuant method. The DuQuant method is built on two key components: 1)
    the block-diagonal rotation matrix, tasked with the local redistribution of feature
    outliers, and 2) the zigzag permutation, responsible for the global reordering
    of outliers across different blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Preliminaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As the common modules within each transformer block of LLMs, both Multi-head
    Self-Attention (MSA) and Feed-Forward Network (FFN) fundamentally consist of basic
    linear layers, which can be represented as, $\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}\in\mathbb{R}^{T\times
    C_{out}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{X}_{q}=\text{clamp}\left(\left\lfloor\frac{\mathbf{X}}{\Delta}\right\rceil\!\!+\!z,0,2^{b}-1\right),\textrm{where}~{}\Delta=\frac{\max(\mathbf{X})-\min(\mathbf{X})}{2^{b}-1},z=-\left\lfloor\frac{\min(\mathbf{X})}{\Delta}\right\rceil.~{}~{}~{}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: The notation $\left\lfloor\cdot\right\rceil$).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b91ac31f593e0ebab135e648496aff6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Transformation Steps for Activation Matrices after smooth technique.
    (a) Sequential transformations on Normal Outliers: ① initial rotation to reduce
    outliers within blocks, ② permutation to evenly distribute outliers across blocks,
    and ③ a second rotation for further smoothing. (b) Activation changes for Massive
    Outliers before and after DuQuant. (c) A sample matrix for highlighting the continual
    reduction of outliers through rotation and permutation, with outliers marked in
    dark blue.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The proposed DuQuant Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address the Normal Outliers issue stated in Section [2](#S2 "2 Motivation
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"), current quantization methods, such as SmoothQuant [[52](#bib.bib52)]
    and OmniQuant [[52](#bib.bib52)], usually adopt the smooth technique. Concretely,
    it involves the utilization of a per-channel smoothing diagonal matrix, denoted
    as $\mathbf{\Lambda}$, which in turn introduce new outliers in the weight matrix
    and result in significant performance declines in low-bit quantization scenarios,
    such as INT4 .
  prefs: []
  type: TYPE_NORMAL
- en: According to these findings, we propose the DuQuant method, which includes the
    Rotation and Permutation transformations based on the smooth technique. By combining
    rotation transformation and channel permutation, our DuQuant method aims to redistribute
    these features within the activation space, thereby mitigating the effects of
    both Normal and Massive Outliers.
  prefs: []
  type: TYPE_NORMAL
- en: The Rotation Transformation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to the smooth technique, our aim is to apply a rotation matrix for
    row or column transformations, mitigating the impact of both Normal and Massive
    outliers. The ideal rotation matrix, denoted as $\mathbf{R}$ involves the following
    steps,
  prefs: []
  type: TYPE_NORMAL
- en: $\circ$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the feature dimension $d^{(1)}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\circ$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the searched dimensions $d^{(1)}$, we construct the rotation matrix
    as follows,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbf{R^{1}}=\mathbf{E}_{d^{(1)}}\tilde{\mathbf{R}}\mathbf{Q}\mathbf{E}_{d^{(1)}},\qquad\mathbf{Q}=\small{\left[\begin{matrix}1&amp;\mathbf{O}\\
    \mathbf{O}&amp;\mathbf{Q}^{\prime}\end{matrix}\right]}.$ |  | (2) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Here, $\mathbf{E}_{d^{(1)}}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\circ$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let $N$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Through this construction manner, we can ensure that the approximated optimal
    rotation matrix $\hat{\mathbf{R}}$ in a block-wise manner,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\centering\hat{\mathbf{R}}=~{}\text{BlockDiag}(\hat{\mathbf{R}}_{b_{1}},...,\hat{\mathbf{R}}_{b_{K}}),\@add@centering$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{\mathbf{R}}_{b_{i}}\in\mathbb{R}^{2^{n}\times 2^{n}}$.
  prefs: []
  type: TYPE_NORMAL
- en: The Permutation Transformation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite adopting the block-diagonal rotation matrix $\hat{\mathbf{R}}$ for its
    time and storage efficiency, its focus on local information introduces a potential
    limitation in further reducing the outliers. This is because the rotation transformation,
    conducted within each small block, cannot integrate the information across different
    blocks to further minimize outliers. Consequently, one block may have relatively
    larger outliers while another block has smaller outliers, resulting in high variance
    among different blocks, as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries
    ‣ 3 Method ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"). This limitation explains that merely utilizing the block-diagonal
    rotation matrix is insufficient to effectively reduce the overall outliers.
  prefs: []
  type: TYPE_NORMAL
- en: To effectively mitigate the overall outliers, it is essential to balance the
    outliers’ magnitudes among various blocks. Specifically, within each small block,
    we denote the largest outlier in dimension $d_{j}$. Then the variance in activation
    magnitudes across various blocks can be expressed as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\centering\text{Var}([M_{b_{1}},M_{b_{2}},...,M_{b_{K}}]).\@add@centering$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: To minimize this variance and further reduce the overall outliers, we introduce
    the zigzag permutation. Concretely, we generate a zigzag sequence that starts
    by assigning channels with the highest activations to the first block. The process
    continues by assigning channels with the next highest activations to the subsequent
    blocks in descending order until the end of block $K$. By employing the zigzag
    permutation, we achieve a balanced distribution of outliers across different blocks.
    This allows us to use an additional rotation transformation to further smooth
    the outliers. Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries ‣ 3 Method ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") provides an illustration of outlier mitigation.
  prefs: []
  type: TYPE_NORMAL
- en: The Overall DuQuant Method.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To effectively mitigate both Normal and Massive Outliers, we first employ the
    smooth technique to shift the quantization challenge from activations to weights.
    Next, we introduce the block-diagonal rotation matrix $\hat{\mathbf{R}}$ to locally
    redistribute feature outliers within the activation space. We then propose the
    zigzag permutation matrix for globally balancing the outliers across different
    blocks, followed by another application of the block-diagonal rotation transformation.
    To sum up, the linear layers within the transformer can be rewrite as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\centering\small{\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}=[(\mathbf{X}\cdot\underbrace{\mathbf{\Lambda})\hat{\mathbf{R}}_{(1)}\cdot\mathbf{P}\cdot\hat{\mathbf{R}}_{(2)}}_{\mathbf{G}}]\cdot[\underbrace{\hat{\mathbf{R}}_{(2)}^{\top}\cdot\mathbf{P}^{\top}\cdot\hat{\mathbf{R}}_{(1)}^{\top}(\mathbf{\Lambda}^{-1}}_{\mathbf{G}^{-1}}\cdot\mathbf{W})],}\@add@centering$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where the notation $\mathbf{P}$ represent the first and second block-diagonal
    rotation matrix, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1. It is worth noting that the proposed RAP method can simultaneously
    smooth the weight matrix. While the commonly adopted smooth technique is effective,
    it can cause the weight matrix of the down-projection layer to exhibit pronounced
    outliers, leading to performance degradation. However, in the proposed RAP method,
    the rotation transformation we designed is applied to not only the activation
    input but also the weight matrix. As a result, the outliers induced by the smooth
    technique can be mitigated through our approximated rotation matrix $\hat{\mathbf{R}}$,
    yielding a smoother, more quantization-friendly weight matrix. Moreover, this
    approach eliminates the reliance on complex weight quantization techniques, such
    as GPTQ [[17](#bib.bib17)] used in Atom [[61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: Remark 2. To further decrease the computation and memory costs, we initially
    construct the $k$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further demonstrate the effectiveness of the proposed DuQuant method, we
    conduct a theoretical analysis of the rotation and permutation transformations.
    Theorem [1](#Thmtheorem1a "Theorem 1 (Rotation). ‣ Appendix B Proofs ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") shows that within each block, the constructed rotation matrix effectively
    mitigates the maximum outlier, thereby reducing the outlier magnitude through
    a greedy search. Theorem [2](#Thmtheorem2a "Theorem 2 (Zigzag Permutation). ‣
    Appendix B Proofs ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs") reveals that the employed zigzag permutation
    ensures a balanced upper bound shared among different blocks. This suggests that
    the zigzag permutation effectively reduces the variance shown in Eqn. ([4](#S3.E4
    "In The Permutation Transformation. ‣ 3.2 The proposed DuQuant Method ‣ 3 Method
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs")) and thus assists the rotation matrix in further decreasing the outliers.
    Please refer to Appendix [B](#A2 "Appendix B Proofs ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs") for detailed
    proofs.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1  (Rotation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$ within the
    input. Then, we can deduce that,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Theorem 2  (Zigzag Permutation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$-th block
    consistently satisfies,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M_{b_{i}}\leq O^{(1)}+\small{\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta},\qquad
    i=1,2,3,...,K.$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models and Evaluations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We apply our DuQuant on pre-trained LLMs: LLaMA (7B-65B) [[45](#bib.bib45)],
    LLaMA2 (7B-70B) [[46](#bib.bib46)], LLaMA3 (8B, 70B) and instruction-tuned LLMs:
    Vicuna-v1.5 (7B-13B) [[9](#bib.bib9)]. We evaluate quantized LLaMA models on language
    generation tasks and commonsense QA tasks. Specifically, we assess the perplexity
    on WikiText2 [[37](#bib.bib37)] and C4 [[39](#bib.bib39)] datasets, as well as
    the zero-shot accuracy on PIQA [[6](#bib.bib6)], ARC [[11](#bib.bib11)], BoolQ [[10](#bib.bib10)],
    HellaSwag [[56](#bib.bib56)], and WinoGrande [[40](#bib.bib40)] datasets. Moreover,
    we evaluate quantized Vicuna models on MMLU [[20](#bib.bib20)] and MT-Bench [[62](#bib.bib62)]
    benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In line with prior studies [[31](#bib.bib31), [42](#bib.bib42)], we apply per-token
    activation quantization and per-channel weight quantization. Given that W8A8 quantization
    has been established as lossless in precision by SmoothQuant [[52](#bib.bib52)],
    our primary evaluation in this paper focuses on 4-bit and 6-bit quantization for
    weights and activations. As for details, we quantize all intermediate activations,
    excluding the SoftMax output. Moreover, we have developed two types of quantized
    models, denoted as DuQuant and DuQuant+LWC . For DuQuant, we employ round-to-nearest
    quantization, using a clipping ratio of 0.9 for activations and 0.8 for weights.
    To improve weight matrix quantization, DuQuant+LWC integrates the learnable weight
    clipping (LWC) technique from OmniQuant. Concretely, LWC adjusts weights by training
    parameters $\gamma,\beta\in[0,1]$ in Eqn. ([1](#S3.E1 "In 3.1 Preliminaries ‣
    3 Method ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs")). Notably, the smoothing diagonal matrix and the learned
    weight clipping factor can be integrated into the quantized weights, introducing
    no additional computational or memory costs. More details and hyperparameters
    are left in Appendix [C](#A3 "Appendix C Additional Implementation Details ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We compare with state-of-the-art (SOTA) weight-activation PTQ methods, including
    SmoothQuant [[52](#bib.bib52)], Outlier Supression+ [[49](#bib.bib49)], OmniQuant [[42](#bib.bib42)],
    QLLM [[31](#bib.bib31)], AffineQuant [[36](#bib.bib36)], and Atom [[61](#bib.bib61)].
    For Atom, we reproduce the results with no group-wise asymmetric quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Perplexity ($\downarrow$) results under 4-bit weight-activation quantization.
    The results for W6A6 can be found in Table [D4](#A4.T4 "Table D4 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"). Atom and OmniQuant unprocessed
    group-query attention for LLaMA2-70B.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | #Bit | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 | FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A4 | SmoothQuant | 25.25 | 40.05 | 192.40 | 275.53 | 83.12 | 35.88 | 26.01
    |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 11.26 | 10.87 | 10.33 | 9.17 | 14.26 | 12.30 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 10.28 | 10.32 | 9.35 | - | 12.69 | 11.45 | - |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 9.65 | 8.41 | 8.37 | 6.87 | 11.75 | 9.09 | 7.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 8.15 | 7.43 | 6.52 | 5.14 | 8.40 | 6.96 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 6.40 | 5.65 | 4.72 | 4.13 | 6.28 | 5.42 | 3.79 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 6.18 | 5.47 | 4.55 | 3.93 | 6.08 | 5.33 | 3.76 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | FP16 |  | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A4 | SmoothQuant | 32.32 | 47.18 | 122.38 | 244.35 | 77.27 | 43.19 | 34.61
    |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 14.51 | 13.78 | 12.49 | 11.28 | 18.02 | 14.55 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 13.64 | 13.44 | 11.58 | - | 15.76 | 13.97 | - |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 12.29 | 10.58 | 11.51 | 8.98 | 13.26 | 11.13 | 8.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 10.34 | 9.57 | 8.56 | 8.17 | 10.96 | 9.12 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 7.84 | 7.16 | 6.45 | 6.03 | 7.90 | 7.05 | 5.87 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 7.73 | 7.07 | 6.37 | 5.93 | 7.79 | 7.02 | 5.85 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Zero-shot QA ($\uparrow$) results of LLaMA1 models under 4-bit weight-activation
    quantization. The results for LLaMA2 models and W6A6 quantization can be found
    in Table [D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs")  [D5](#A4.T5 "Table D5 ‣ W6A6 Quantization Results. ‣ Appendix D More Empirical
    Results ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"), and  [D6](#A4.T6 "Table D6 ‣ W6A6 Quantization Results.
    ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B W4A4 | FP16 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 49.80 | 30.40 | 25.80 | 49.10 | 27.40 | 48.00 | 38.41 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 62.73 | 39.98 | 30.29 | 60.21 | 44.39 | 52.96 | 48.43 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 68.77 | 45.20 | 31.14 | - | 57.43 | 56.67 | 51.84 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 71.44 | 47.74 | 35.49 | 67.71 | 63.89 | 55.01 | 56.88 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 76.44 | 50.04 | 38.99 | 70.98 | 69.39 | 64.72 | 61.76 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 76.22 | 50.04 | 38.31 | 70.09 | 69.82 | 62.59 | 61.18 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-13B W4A4 | FP16 | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 61.04 | 39.18 | 30.80 | 61.80 | 52.29 | 51.06 | 49.36 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 63.00 | 40.32 | 30.38 | 60.34 | 53.61 | 51.54 | 49.86 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 69.69 | 47.39 | 33.10 | 62.84 | 58.96 | 55.80 | 54.37 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 71.38 | 47.60 | 34.30 | - | 63.70 | 59.43 | 55.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 71.38 | 49.07 | 36.69 | 64.53 | 68.00 | 58.56 | 58.04 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 77.26 | 58.04 | 41.55 | 67.55 | 73.62 | 66.69 | 64.12 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 77.64 | 57.32 | 41.21 | 66.79 | 74.12 | 65.98 | 63.84 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-30B W4A4 | FP16 | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 58.65 | 35.53 | 27.73 | 60.42 | 35.56 | 48.06 | 44.83 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 67.63 | 46.17 | 34.40 | 60.70 | 54.32 | 52.64 | 52.62 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 71.21 | 49.45 | 34.47 | 65.33 | 64.65 | 59.19 | 56.63 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 70.84 | 49.41 | 37.12 | 70.12 | 65.53 | 58.64 | 58.61 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 73.83 | 50.67 | 38.40 | - | 67.91 | 58.56 | 57.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 71.98 | 49.07 | 40.02 | 66.85 | 70.45 | 58.64 | 59.50 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 78.56 | 56.99 | 42.32 | 66.73 | 76.70 | 69.61 | 65.15 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 78.73 | 56.52 | 43.17 | 68.84 | 77.53 | 70.96 | 65.96 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-65B W4A4 | FP16 | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 64.47 | 40.44 | 29.82 | 59.38 | 39.90 | 52.24 | 47.71 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 68.06 | 43.98 | 35.32 | 62.75 | 50.73 | 54.30 | 52.52 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 71.81 | 48.02 | 35.92 | 73.27 | 66.81 | 59.51 | 59.22 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 73.56 | 52.06 | 39.68 | - | 70.94 | 62.90 | 59.83 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 74.48 | 51.60 | 40.61 | 73.76 | 73.78 | 62.12 | 62.73 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 79.71 | 57.95 | 45.05 | 79.82 | 78.66 | 72.29 | 68.91 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DuQuant+LWC | 79.98 | 58.29 | 44.80 | 77.89 | 79.22 | 72.21 | 68.73 |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantization of LLaMA1 and LLaMA2 Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct a comprehensive comparison of our DuQuant with several SOTA baselines
    on LLaMA1 and LLaMA2 models. Results for W4A4 quantization are presented in this
    Section, while results for W6A6 quantization are provided in Appendix [D](#A4
    "Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs"). Table [1](#S4.T1 "Table 1 ‣ Baselines.
    ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs") indicates that our DuQuant quantized models notably
    outperform other baselines on both the WikiText2 and C4 datasets. Notably, LWC
    further enhances model capacity, with our DuQuant+LWC achieving comparable performance
    with FP16 models. Table [2](#S4.T2 "Table 2 ‣ Baselines. ‣ 4 Experiment ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") and Table [D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") showcase the zero-shot accuracy of W4A4 quantization on Commonsense QA
    tasks, where DuQuant significantly improves the average accuracy. Our method surpasses
    QLLM by +9%, and Atom by +5% for all model sizes. These results demonstrate the
    superiority of our rotation and permutation transformation, which establishes
    new SOTA performance by effectively eliminating outlier features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Zero-shot and five-shot results on the MMLU benchmark for Vicuna-v1.5-13B
    under 4-bit weight-activation quantization. The results for Vicuna-v1.5-7b can
    be found in Table [D2](#A4.T2 "Table D2 ‣ MMLU Results for 4-bit Vicuna-v1.5-7B.
    ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | MMLU (0 shot) $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| STEM | Hums | Social | Others | Avg. | STEM | Hums | Social | Others | Avg.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-v1.5-13B W4A4 | FP16 | 43.70 | 50.48 | 62.72 | 62.74 | 54.54 | 44.96
    | 51.97 | 65.26 | 62.40 | 55.78 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 21.70 | 24.29 | 22.13 | 23.16 | 22.82 | 25.31 | 24.97 | 26.00
    | 27.08 | 25.84 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 26.81 | 26.57 | 30.35 | 28.75 | 28.12 | 28.79 | 27.29 | 31.13
    | 28.99 | 29.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 32.54 | 39.60 | 46.02 | 46.11 | 41.07 | 35.35 | 39.21 | 59.72 | 45.77
    | 45.01 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 40.82 | 46.61 | 58.73 | 57.59 | 50.94 | 40.92 | 48.78 | 60.42 |
    57.71 | 51.96 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DuQuant+LWC | 40.13 | 47.48 | 58.86 | 57.83 | 51.08 | 41..42 | 48.52 |
    58.73 | 57.74 | 51.61 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Perplexity and QA results of LLaMA3-8B under 4-bit/6-bit weight-activation
    quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| #Bits | Method | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | - | 6.14 | 8.88 | 9.91 | 80.85 | 77.78 | 53.41 | 81.28 | 79.16 | 72.84
    | 74.22 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8B W6A6 | SmoothQuant | 7.07 | 9.57 | 11.69 | 78.94 | 75.88 | 49.49
    | 77.58 | 77.39 | 70.8 | 71.68 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 7.24 | 9.82 | 11.90 | 78.90 | 73.95 | 47.35 | 74.95 | 76.77 |
    70.56 | 70.41 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 7.35 | 9.99 | 12.30 | 78.73 | 73.32 | 46.08 | 74.59 | 77.08
    | 70.88 | 70.11 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 6.27 | 8.38 | 10.77 | 80.20 | 77.27 | 52.05 | 80.12 | 79.14 | 72.77
    | 73.59 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 6.27 | 8.38 | 10.78 | 79.71 | 77.57 | 53.07 | 80.00 | 78.70
    | 73.09 | 73.69 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8B W4A4 | SmoothQuant | 210.19 | 187.93 | 278.02 | 54.57 | 31.9 |
    24.23 | 52.72 | 31.26 | 51.14 | 40.97 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 3.64e3 | 2.80e3 | 3.09e3 | 50.22 | 26.94 | 24.57 | 37.98 | 26.55
    | 50.20 | 36.08 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 21.21e3 | 34.60e3 | 16.72e3 | 50.71 | 25.93 | 26.02 | 40.55
    | 26.07 | 48.46 | 36.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 22.14 | 31.83 | 40.04 | 62.95 | 49.45 | 30.12 | 60.31 | 53.75 | 56.04
    | 52.10 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 8.56 | 11.98 | 13.66 | 75.68 | 68.48 | 41.81 | 71.99 | 73.07 |
    66.22 | 66.21 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 8.06 | 11.29 | 13.19 | 76.22 | 70.41 | 43.69 | 74.34 | 73.87
    | 67.80 | 67.72 |'
  prefs: []
  type: TYPE_TB
- en: Quantization of Instruction-tuned Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We quantize Vicuna-v1.5 [[9](#bib.bib9)] models to assess the generalizability
    of our DuQuant. Table [3](#S4.T3 "Table 3 ‣ Quantization of LLaMA1 and LLaMA2
    Models. ‣ 4.1 Main Results ‣ 4 Experiment ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") illustrates that our quantized
    models surpass the baselines across all task categories on MMLU benchmark. For
    Vicuna-13B, our DuQuant+LWC surpasses Atom by 10.01% under zero-shot settings
    and 6.95% under five-shot settings. Moreover, we compare our DuQuant with Atom
    and OmniQuant using MT-Bench and utilize GPT-4 to evaluate the answers from quantized
    models. As shown in Figure [3](#S4.F3 "Figure 3 ‣ Table 6 ‣ Influence of Normal/Massive
    Outliers. ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"), DuQuant quantized models
    significantly outperform both Atom and OmniQuant in win rates. Specifically, for
    Vicuna-7B, DuQuant only lost 16 and 1 times to Atom and OmniQuant, respectively,
    while achieving 68 and 155 wins against them.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization of LLaMA3 Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLaMA3, known for its superior performance in various tasks, faces significant
    degradation in low-bit quantization [[24](#bib.bib24)]. To address this, we apply
    our DuQuant to quantize LLaMA3-8B. Table [4](#S4.T4 "Table 4 ‣ Quantization of
    LLaMA1 and LLaMA2 Models. ‣ 4.1 Main Results ‣ 4 Experiment ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs") displays
    the perplexity and zero-shot accuracy results. Notably, under W6A6 setting, our
    DuQuant achieves performance comparable to FP16 model. Furthermore, unlike other
    methods that show weaker results under W4A4 setting, our DuQuant maintains competitive
    performance, indicating its robustness with LLaMA3. We attribute this success
    to the advanced handling of outliers achieved through dual transformations, which
    is not restricted to specific models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Module-wise Impact.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our DuQuant includes three main components: smoothing operation, initial rotation,
    and permutation with a second rotation. We combine these components to quantize
    the LLaMA2-13B model and evaluate their effects on language generation tasks.
    Results in Table [5](#S4.T5 "Table 5 ‣ Module-wise Impact. ‣ 4.2 Ablation Study
    ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs") show that the smoothing operation plays a basic
    role in our DuQuant by shifting activation outliers to weight. The initial rotation
    significantly enhances model performance, yielding competitive PPL results. Finally,
    permutation combined with a second rotation further enhances the quantized model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Influence of different components in DuQuant under 4-bit weight-activation
    quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Modules | LLaMA2-7B | LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Smooth | Rotation 1 | Permutation | Rotation 2 | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\checkmark$ |  |  |  | NaN | 1379.46 | 160.30 | 203.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\checkmark$ |  |  | 8.48 | 10.63 | 14.32 | 21.73 |'
  prefs: []
  type: TYPE_TB
- en: '| $\checkmark$ |  |  | 7.92 | 10.64 | 5.96 | 7.94 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\checkmark$ | 6.79 | 8.51 | 6.06 | 8.03 |'
  prefs: []
  type: TYPE_TB
- en: '| $\checkmark$ | 6.28 | 7.90 | 5.42 | 7.05 |'
  prefs: []
  type: TYPE_TB
- en: Influence of Normal/Massive Outliers.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we comprehensively explore the influence of massive and normal
    outliers on quantization. Notably, we observe that massive outliers primarily
    occur at the down-projection of the FFN module. To isolate their effect, we remove
    the rotation and permutation transformations, applying only the smoothing technique
    to all down-projection inputs. The resulting perplexity for LLaMA2-7B and LLaMA-13B
    showed significant degradation, presented in Table [6](#S4.T6 "Table 6 ‣ Influence
    of Normal/Massive Outliers. ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs").
    Conversely, when we eliminate the rotation and permutation transformations for
    normal outliers, the performance decrease was noticeable but less severe compared
    to massive outliers. These findings indicate that: 1) massive outliers exert a
    more substantial impact on quantization, corroborating our claims in Section [2](#S2
    "2 Motivation ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"); 2) the smoothing technique alone struggles to fully mitigate
    the influence of outliers, particularly massive ones; and 3) our rotation and
    permutation methods prove highly effective against both types of outliers, leading
    to superior performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Outliers impact on quantization. We only apply the smooth technique
    on Normal and Massive outliers for W4A4 quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Outlier Type | LLaMA2-7B | LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Normal | Massive | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\checkmark$ | 18.16 | 26.42 | 10.51 | 16.01 |'
  prefs: []
  type: TYPE_TB
- en: '| $\checkmark$ |  | 10.88 | 13.89 | 7.87 | 10.52 |'
  prefs: []
  type: TYPE_TB
- en: '| $\checkmark$ | 6.28 | 7.90 | 5.42 | 7.05 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: GPT-4 evaluation on the MT-Bench.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6dff483fd70ab5a37f6eb194461da70.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison with QuaRot [[2](#bib.bib2)]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In light of the recent introduction of Hadamard rotations by QuaRot[[2](#bib.bib2)]
    to eliminate outlier features, we have undertaken a detailed analysis to highlight
    the key differences between our DuQuant and QuaRot. To ensure a balanced evaluation,
    we have re-implemented QuaRot in accordance with our quantization settings. The
    results demonstrate that 1) the rotation matrix constructed by DuQuant outperforms
    QuaRot’s approach of simply selecting a randomly initialized Hadamard matrix.
    As depicted in Figure [7](#S4.T7 "Table 7 ‣ Comparison with QuaRot [2] ‣ 4.2 Ablation
    Study ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"), our DuQuant more effectively smooths activations
    than QuaRot; 2) As demonstrated by the perplexity in Table [7](#S4.T7 "Table 7
    ‣ Comparison with QuaRot [2] ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"),
    QuaRot employs GPTQ for their weight quantization method, whereas our DuQuant,
    with its sophisticated outlier management, attains competitive results using RTN
    quantization. Additionally, the inclusion of the LWC component within DuQuant significantly
    enhances the performance of our quantized model. For a more comprehensive comparison,
    please refer to Appendix [F](#A6 "Appendix F Detailed Comparison with QuaRot ‣
    Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: PPL ($\downarrow$) comparison under W4A4 setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | 1-7B | 1-13B | 1-30B | 2-7B | 2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | 5.68 | 5.09 | 4.10 | 5.47 | 4.88 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-RTN | 7.08 | 6.57 | 5.44 | 9.66 | 6.73 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-GPTQ | 6.44 | 5.63 | 4.73 | 6.39 | 5.75 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 6.40 | 5.65 | 4.72 | 6.28 | 5.42 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 6.18 | 5.47 | 4.55 | 6.08 | 5.33 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4: LLaMA2-7B Attention key_proj.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd69fa930d8a85eba39b395d49e31fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 8: Quantization runtime on one NVIDIA A100.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Omni. Affine. QLLM Atom DuQuant LLaMA2-7B 2.0h 9.1h 1.1h 20min 50s LLaMA2-13B
    3.2h 16.0h 1.7h 36min 71s LLaMA2-70B 14.6h 18.6h 9.3h 3.5h 270s
  prefs: []
  type: TYPE_NORMAL
- en: Runtime.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our DuQuant stands out for its efficiency, surpassing other baselines [[42](#bib.bib42),
    [31](#bib.bib31), [36](#bib.bib36), [61](#bib.bib61)]. The block-wise rotation
    ensures fast multiplication between the rotation and activation matrices. Zigzag
    permutation, involving simple channel swaps, is much faster than complex algorithms
    like Simulated Annealing, as discussed in Appendix  [E.3](#A5.SS3 "E.3 Effects
    of Permutation Algorithm. ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Moreover,
    the advanced management of outliers makes DuQuant not rely on GPTQ or gradient-based
    training. Hence, DuQuant enables a rapid quantization process shown in Table [F17](#A6.T17
    "Table F17 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"), e.g., we
    successfully quantize LLaMA2-13B in just 71s with superior performance.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Speedup.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To assess the inference speedup delivered by our DuQuant, we adopt the measurement
    strategy and W4A4 kernel from [[2](#bib.bib2)]. We evaluate the layer-wise speedup
    of LLaMA2-7B and LLaMA2-13B on NVIDIA RTX 3090 GPUs, with results detailed in
    Table [9](#S4.T9 "Table 9 ‣ Inference Speedup. ‣ 4.2 Ablation Study ‣ 4 Experiment
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"). We can observe that during the prefill phase, DuQuant achieves a $2.08\times$
    speedup and the impressive performance demonstrated by DuQuant  these additional
    costs are deemed acceptable. Further permutations do not enhance performance and
    can negatively impact inference efficiency. More detailed results on inference
    speedup are available in Appendix [E.1](#A5.SS1 "E.1 End-to-end Time Speedup and
    Memory Saving ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for
    Advanced Outlier Management and Efficient Quantization of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Layer-wise speedup during prefilling phase for 4-bit weight-activation
    quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Batch Size | Speedup |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B | 1 | $1.95\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | $2.03\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | $2.08\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B | 1 | $2.15\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | $2.30\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | $2.34\times$ |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Computational overhead analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/447eba1a713693348507e9c75af0057e.png)'
  prefs: []
  type: TYPE_IMG
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, this paper presents DuQuant, an innovative quantization strategy
    for large language models (LLMs) that effectively addresses the challenge of outlier
    activations. By integrating rotation and permutation transformations, DuQuant effectively
    mitigates the impacts of both massive and normal outliers. This strategic redistribution
    of outliers not only simplifies the quantization process but also leads to substantial
    improvements in model performance. Consequently, DuQuant establishes new state-of-the-art
    results in 4-bit weight-activation quantization scenarios. This advancement enhances
    the deployment of efficient LLMs in resource-constrained environments.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ashkboos et al. [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end
    4-bit inference on generative large language models. *arXiv preprint arXiv:2310.09259*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ashkboos et al. [2024] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    Quarot: Outlier-free 4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. [2020] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of
    bert quantization. *arXiv preprint arXiv:2012.15701*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2022] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and
    Michael R Lyu. Towards efficient post-training quantization of pre-trained language
    models. *Advances in Neural Information Processing Systems*, 35:1405–1418, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. [2013] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating
    or propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. In *Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pages 2924–2936, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8(): 8-bit matrix multiplication for transformers at scale. In *Conference
    on Neural Information Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2024] Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. SpQR: A sparse-quantized representation for near-lossless LLM
    weight compression. In *The Twelfth International Conference on Learning Representations*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2024] Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen
    Chu, and Ningyi Xu. Bitdistiller: Unleashing the potential of sub-4-bit llms via
    self-distillation. *arXiv preprint arXiv:2402.10631*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duanmu et al. [2024] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan,
    Xingcheng Zhang, and Dahua Lin. Skvq: Sliding-window key and value cache quantization
    for large language models. *arXiv preprint arXiv:2405.06219*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2:
    Parameter-efficient visual instruction model. *arXiv preprint arXiv:2304.15010*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2015] Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *International Conference on Learning Representations*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heo et al. [2023] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim,
    Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers
    for low-bit weight quantization of large language models. *arXiv preprint arXiv:2309.15531*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou and Kwok [2018] Lu Hou and James T Kwok. Loss-aware weight quantization
    of deep networks. *arXiv preprint arXiv:1802.08635*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. [2016] Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization
    of deep networks. *arXiv preprint arXiv:1611.01600*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good
    are low-bit quantized llama3 models? an empirical study. *arXiv preprint arXiv:2404.14047*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 2704–2713, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2023] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2024a] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm
    tweaking: High-performance low-bit quantization of large language models. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 38, pages 18536–18544,
    2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2024b] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized
    large language models. *arXiv preprint arXiv:2402.18158*, 2024b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023a] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei
    Cai, and Bohan Zhuang. Qllm: Accurate and efficient low-bitwidth quantization
    for large language models. In *The Twelfth International Conference on Learning
    Representations*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2024a] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao,
    Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language
    model quantization by keeping pivot tokens intact. *arXiv preprint arXiv:2403.01241*,
    2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. Post-training quantization for vision transformer. *Advances in Neural
    Information Processing Systems*, 34:28092–28103, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2024b] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo
    Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric
    2bit quantization for kv cache. *arXiv preprint arXiv:2402.02750*, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2024] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao,
    Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. Affinequant: Affine transformation
    quantization for large language models. *arXiv preprint arXiv:2403.12544*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *International Conference on Learning
    Representations*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. [2024] Yuzhang Shang, Zhihang Yuan, and Zhen Dong. PB-LLM: Partially
    binarized large language models. In *The Twelfth International Conference on Learning
    Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2024] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive
    activations in large language models. *arXiv preprint arXiv:2402.17762*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. [2022] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun
    Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models
    via quantization. *arXiv preprint arXiv:2203.10705*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence
    and lattice codebooks. *arXiv preprint arXiv:2402.04396*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2022] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. In *Conference on Neural
    Information Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2023] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization
    of large language models by equivalent and effective shifting and scaling. In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 1648–1665, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap
    forward in llms post-training w4a8 quantization using floating-point formats.
    *arXiv preprint arXiv:2307.09782*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xi et al. [2023] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training
    transformers with 4-bit integers. *Advances in Neural Information Processing Systems*,
    36:49146–49168, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2023a] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien
    Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. [2023b] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks. *arXiv preprint
    arXiv:2309.17453*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In *Conference on Neural Information
    Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2023] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou,
    Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient
    fine-tuning of language models with zero-init attention. *arXiv preprint arXiv:2303.16199*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2024a] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,
    Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse:
    Does your multi-modal llm truly see the diagrams in visual math problems? *arXiv
    preprint arXiv:2403.14624*, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. *arXiv
    preprint arXiv:2009.12812*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2024b] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou,
    and Carlo Vittorio Cannistraci. Plug-and-play: An efficient post-training pruning
    method for large language models. In *The Twelfth International Conference on
    Learning Representations*, 2024b. URL [https://openreview.net/forum?id=Tr0lPx9woF](https://openreview.net/forum?id=Tr0lPx9woF).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom:
    Low-bit quantization for efficient and accurate llm serving. *arXiv preprint arXiv:2310.19102*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [A](#A1 "Appendix A Related Work ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"): Related work.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [B](#A2 "Appendix B Proofs ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"): Theory proofs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [C](#A3 "Appendix C Additional Implementation Details ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"):
    Additional implementation details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [D](#A4 "Appendix D More Empirical Results ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"): More empirical
    results.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [E](#A5 "Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"): More detailed
    ablation studies.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [F](#A6 "Appendix F Detailed Comparison with QuaRot ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"):
    Detailed comparison with QuaRot.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [G](#A7 "Appendix G Limitations and Broader Impacts ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"):
    Limitations and broader impacts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Section [H](#A8 "Appendix H More Visualizations ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"): More visualization
    examples.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix A Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Network Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Network quantization [[19](#bib.bib19), [23](#bib.bib23), [22](#bib.bib22),
    [51](#bib.bib51)] is a widely utilized technique in neural networks aimed at reducing
    model size and memory usage. Research in this area generally falls into two main
    categories: quantization-aware training (QAT) [[59](#bib.bib59), [3](#bib.bib3),
    [44](#bib.bib44)] and post-training quantization (PTQ) [[38](#bib.bib38), [34](#bib.bib34),
    [4](#bib.bib4), [29](#bib.bib29)]. QAT involves training quantized model weights
    using additional data, often with the assistance of a straight-through estimator
    (STE) [[5](#bib.bib5)]. However, the computational cost associated with QAT poses
    challenges, particularly for large language models (LLMs) with millions of parameters,
    which necessitate significant amounts of data for retraining [[33](#bib.bib33),
    [14](#bib.bib14)]. In contrast, PTQ has gained popularity for LLMs [[48](#bib.bib48),
    [21](#bib.bib21), [58](#bib.bib58), [55](#bib.bib55), [35](#bib.bib35)] due to
    its efficient approach, involving the training of quantized models using a small
    amount of data, known as calibration data [[17](#bib.bib17)]. However, PTQ often
    leads to significant performance degradation, especially when employing low-bit
    settings  [[17](#bib.bib17), [47](#bib.bib47), [41](#bib.bib41), [28](#bib.bib28)].
    Consequently, our work focuses on enhancing the performance of low-bit PTQ quantized
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Post Training Quantization of LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Post-training quantization for LLMs can be categorized into weight-only quantization [[30](#bib.bib30),
    [13](#bib.bib13), [26](#bib.bib26), [27](#bib.bib27)] and weight-activation quantization [[54](#bib.bib54),
    [52](#bib.bib52), [50](#bib.bib50)]. We focus on 4-bit weight-activation quantization
    due to the actual speedup it provides with low-bit quantization kernels [[1](#bib.bib1)].
    Quantizing LLMs faces challenges due to activation outlier features persisting
    across different tokens and layers [[12](#bib.bib12), [48](#bib.bib48)]. Some
    approaches [[12](#bib.bib12), [61](#bib.bib61)] retain a small portion of crucial
    outlier channels at high precision (e.g., INT8), which poses challenges to hardware
    compatibility and leads to additional memory footprint. Other methods [[52](#bib.bib52),
    [49](#bib.bib49), [42](#bib.bib42)] attempt to shift quantization difficulty from
    activation to weight channels. However, the learnable equivalent transformation
    in OmniQuant [[42](#bib.bib42)] and the affine transform matrix in AffineQuant [[36](#bib.bib36)]
    exhibit instability as discussed in Section [2](#S2 "2 Motivation ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs").
    The channel disassembly and assembly in QLLM [[31](#bib.bib31)], coupled with
    LoRA-tuning, incur significant time costs. Notably, these methods demonstrate
    poor performance under W4A4 quantization. We attribute this degradation to the
    ineffective handling of outlier features, especially massive outliers. Hence,
    we propose DuQuant to effectively eliminate outlier features through rotation
    matrices and channel permutation, achieving state-of-the-art performance. In contrast
    with QuaRot [[2](#bib.bib2)] also utilizing hadamard matrices to enhance weight-activation
    quantization, our approach uniquely incorporates knowledge about the actual outlier
    channels. Furthermore, unlike QuaRot, which relies on GPTQ [[17](#bib.bib17)]
    for weight quantization, our channel permutation has been proven helpful and efficient,
    facilitating a faster quantization process. The more detailed analysis and comparison
    with QuaRot are left in Appendix [F](#A6 "Appendix F Detailed Comparison with
    QuaRot ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"). In addition, unlike RPTQ [[55](#bib.bib55)] and SKVQ [[15](#bib.bib15)],
    which use channel reordering to cluster similar activations, our method employs
    Permutation transformations with a fundamentally different goal: to evenly distribute
    outliers across blocks. This balanced distribution is crucial for enabling effective
    secondary rotations, ultimately leading to smoother activations that facilitate
    easier quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Proofs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theorem 1  (Rotation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$ within the
    input. Then, we can deduce that,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the case of a specific block $b_{i}$. This can be formally defined as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Without loss of generality, let’s assume that $\delta_{i}\geq 0$. Consequently,
    we can obtain the following inequality,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{1\leq j\leq 2^{n}}{\max}~{}~{}O_{j}(\mathbf{X}_{b_{i}}\hat{\mathbf{R}}_{b_{i}})$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\underset{1\leq j\leq 2^{n}}{\max}~{}~{}O_{j}(\mathbf{X}_{b_{i}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The inequality (1) holds because the switch matrix $\mathbf{E}_{d^{(1)}}$.
  prefs: []
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2  (Zigzag Permutation).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$-th block
    consistently satisfies,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M_{b_{i}}\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta,\qquad
    i=1,2,3,...,K.$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: According to the zigzag permutation described in Section [3.2](#S3.SS2 "3.2
    The proposed DuQuant Method ‣ 3 Method ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"), and considering the reordered
    outliers $O^{(1)},O^{(2)},...,O^{(C_{in})}$-th block, it contains the following
    channels,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle b_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Since $\delta=\max\{|O^{(i+1)}-O^{(i)}|\},i=1,2,...,C_{in}\!\!-\!1$, then we
    can get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle M_{b_{1}}$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, we can deduce that all $M_{b_{i}}$) share the same upper bound after
    applying our zigzag permutation. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Additional Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, all experiments are done on NVIDIA RTX 3090 GPUs for small-scale
    models and NVIDIA A100 GPUs for large-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: For calibration data, following [[42](#bib.bib42), [36](#bib.bib36), [31](#bib.bib31)],
    we randomly select 128 sampled sequences from the WikiText2 dataset, with the
    sequence length of 2048. For rotation and permutation transformations, the rotation
    block size $2^{n}$ equals 256\. We adopt once permutation times for efficiency.
    We conduct detailed ablation studies in Appendix [E.2](#A5.SS2 "E.2 Effects of
    Rotation Matrix ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"), [E.3](#A5.SS3
    "E.3 Effects of Permutation Algorithm. ‣ Appendix E More Ablation Studies ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs"), [E.4](#A5.SS4 "E.4 Effects of Calibration Datasets ‣ Appendix E More Ablation
    Studies ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Regarding quantization details, for multiplications between activations in MSA,
    such as Query and Key, attention outputs and Value, we apply a Hadamard rotation
    matrix for rapid and straightforward processing. A Hadamard matrix is an orthogonal
    and symmetric matrix filled with elements $\pm 1/\sqrt{2^{n}}$, we set it to 0.6
    for DuQuant and 0.5 DuQuant+LWC. We clip the maximum activation values in all
    projection blocks, and the clipping ratio is set to 0.9\. For DuQuant  we also
    clip the maximum values in weight matrices, with a clipping ratio of 0.8\. For
    lwc, we keep the same default epoch numbers of 20, batch size as 1, learning rate
    as 5e-3, and zero weight decay, as [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D More Empirical Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table D1: Zero-shot common-sense QA ($\uparrow$) results of LLaMA2 models under
    4-bit WA quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B W4A4 | FP16 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96 | 67.25 | 63.72
    |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 60.17 | 35.23 | 27.13 | 57.92 | 37.08 | 49.57 | 44.52 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 63.11 | 39.10 | 28.84 | - | 51.30 | 45.93 | 45.66 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 65.61 | 44.28 | 30.38 | 62.66 | 53.51 | 51.85 | 51.38 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 67.36 | 44.23 | 31.91 | 62.75 | 54.38 | 55.18 | 52.64 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 67.68 | 44.40 | 30.89 | - | 58.45 | 56.59 | 51.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 69.75 | 47.35 | 34.22 | 62.42 | 63.21 | 56.51 | 55.58 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 75.24 | 51.89 | 36.77 | 67.86 | 69.54 | 62.12 | 60.57 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 75.68 | 50.00 | 37.46 | 69.24 | 69.74 | 63.93 | 61.01 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B W4A4 | FP16 | 79.05 | 57.91 | 44.20 | 69.02 | 76.60 | 69.69 |
    66.08 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 62.30 | 40.28 | 30.72 | 60.49 | 42.24 | 49.96 | 47.67 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 64.47 | 41.46 | 32.17 | - | 59.30 | 51.38 | 49.76 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 69.80 | 47.22 | 33.79 | 65.47 | 59.34 | 55.49 | 55.19 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 68.55 | 47.64 | 32.34 | 66.97 | 59.97 | 55.07 | 55.09 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 70.46 | 48.48 | 34.39 | - | 62.80 | 55.41 | 54.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 71.16 | 50.89 | 37.88 | 63.91 | 67.51 | 58.40 | 58.29 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 77.31 | 55.60 | 41.55 | 66.61 | 73.68 | 66.06 | 63.47 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 77.26 | 56.23 | 42.15 | 65.78 | 73.68 | 65.43 | 63.42 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-70B W4A4 | FP16 | 81.01 | 59.68 | 47.95 | 75.87 | 80.87 | 76.95 |
    70.39 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 64.09 | 41.84 | 32.00 | 58.56 | 54.21 | 51.07 | 50.30 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 66.16 | 42.72 | 34.90 | - | 56.93 | 52.96 | 50.73 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 74.27 | 50.59 | 37.20 | - | 71.62 | 59.43 | 58.62 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 79.27 | 58.16 | 46.07 | 70.46 | 79.21 | 74.19 | 67.89 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 79.82 | 59.76 | 46.76 | 73.12 | 79.38 | 74.11 | 68.83 |'
  prefs: []
  type: TYPE_TB
- en: Zero-shot QA Results for 4-bit LLaMA2 Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs")
    showcases the zero-shot commonsense QA results for INT4 quantized LLaMA2 models.
    Our DuQuant method excels across various model sizes and datasets, demonstrating
    state-of-the-art performance in commonsense reasoning tasks. For example, DuQuant outperforms
    Atom by 5.43% for the LLaMA2-7B model and by 5.18% for the LLaMA2-13B model. In
    contrast to Atom [[61](#bib.bib61)], which relies on GPTQ for weight quantization
    and maintains 128 channels at INT8, thereby increasing memory usage, our method
    offers a rapid and more efficient weight-activation quantization solution through
    Rotation and Permutation.
  prefs: []
  type: TYPE_NORMAL
- en: MMLU Results for 4-bit Vicuna-v1.5-7B.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vicuna-v1.5 models [[9](#bib.bib9)], fine-tuned from LLaMA-2 models using high-quality
    user-shared conversations, are considered state-of-the-art chatbots. Table [D2](#A4.T2
    "Table D2 ‣ MMLU Results for 4-bit Vicuna-v1.5-7B. ‣ Appendix D More Empirical
    Results ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs") displays the INT4 quantization results for Vicuna-v1.5-7B
    on the MMLU benchmarks. In comparison to SmoothQuant, OmniQuant, and Atom, our
    DuQuant method exhibits the smallest performance decline and maintains competitive
    capacities in both zero-shot and five-shot settings. These results demonstrate
    the effectiveness of DuQuant in generalizing to instruction-tuned models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table D2: Zero-shot and five-shot results on the MMLU benchmark for quantized
    Vicuna-v1.5-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | MMLU (0 shot) $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| STEM | Hums | Social | Others | Avg. | STEM | Hums | Social | Others | Avg.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-v1.5-7B W4A4 | FP16 | 38.70 | 45.42 | 56.13 | 56.01 | 49.07 | 39.56
    | 45.76 | 58.14 | 57.43 | 50.22 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 27.10 | 25.16 | 27.40 | 26.71 | 26.59 | 25.22 | 25.06 | 24.99
    | 26.68 | 25.49 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 27.20 | 24.00 | 27.14 | 25.08 | 25.86 | 29.39 | 24.95 | 27.30
    | 24.80 | 26.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Atom | 30.28 | 34.73 | 38.97 | 40.56 | 36.14 | 31.97 | 35.37 | 40.46 | 40.81
    | 37.15 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 35.85 | 42.66 | 52.03 | 51.23 | 45.44 | 38.90 | 42.57 | 51.80 |
    51.23 | 46.13 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DuQuant+LWC | 35.18 | 41.91 | 51.28 | 50.52 | 44.72 | 37.34 | 42.21 |
    53.07 | 51.76 | 46.10 |'
  prefs: []
  type: TYPE_TB
- en: Results for 4-bit LLaMA3-70B.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As LLaMA3 models have proven to be sensitive to quantization, we apply our DuQuant to
    the LLaMA3-70B and present the results in Table [D3](#A4.T3 "Table D3 ‣ Results
    for 4-bit LLaMA3-70B. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Due to time
    constraints, we do not add learnable weight clipping. The results demonstrate
    that our RAP-quantized models outperform SmoothQuant by 12.9% on Commonsense QA
    tasks and significantly reduce perplexity across the WikiText2, C4, and PTB datasets.
    These improvements underscore the robustness of our DuQuant method when applied
    to the LLaMA3-70B model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table D3: Perplexity and QA results of LLaMA3-70B under 4-bit weight-activation
    quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| #Bits | Method | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | - | 2.9 | 6.9 | 8.2 | 82.4 | 86.9 | 60.3 | 85.2 | 84.9 | 80.6 | 80.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-70B W4A4 | SmoothQuant | 9.6 | 16.9 | 17.7 | 76.9 | 75.8 | 43.5 |
    64.4 | 62.9 | 58.9 | 63.7 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 4.9 | 8.3 | 8.7 | 81.1 | 80.8 | 57.3 | 81.3 | 82.1 | 77.0 | 76.6
    |'
  prefs: []
  type: TYPE_TB
- en: W6A6 Quantization Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To thoroughly evaluate the effectiveness of our DuQuant models, we conduct comprehensive
    assessments under the W6A6 quantization setting. The perplexity results for language
    generation tasks are displayed in Table [D4](#A4.T4 "Table D4 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"), while the zero-shot accuracy
    for Commonsense QA tasks is detailed in Tables [D5](#A4.T5 "Table D5 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") and [D6](#A4.T6 "Table
    D6 ‣ W6A6 Quantization Results. ‣ Appendix D More Empirical Results ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs"). Our findings reveal that DuQuant not only surpasses other baselines but
    also achieves nearly lossless performance with FP16 models in these tasks. Interestingly,
    in several instances, DuQuant slightly outperforms DuQuant+LWC . This suggests
    that the Rotation and Permutation transformations alone are sufficient to create
    highly competitive quantized models under W6A6 settings, without the need for
    additional enhancements such as the learnable weight clipping (LWC) technique.
    These outcomes highlight the exceptional versatility and robustness of DuQuant across
    various quantization scenarios, confirming its potential as a leading solution
    in post-training quantization for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table D4: Preplexity ($\downarrow$) results on the WikiText2 and C4 datasets
    under 6-bit WA quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | #Bit | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 | FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
  prefs: []
  type: TYPE_TB
- en: '| W6A6 | SmoothQuant | 6.03 | 5.42 | 4.55 | 3.88 | 6.20 | 5.18 | 3.69 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 5.96 | 5.28 | 4.38 | 3.75 | 5.87 | 5.14 | 3.71 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 5.89 | 5.28 | 4.30 | 3.73 | 5.91 | 5.08 | 3.55 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 5.73 | 5.13 | 4.14 | 3.57 | 5.53 | 4.92 | 3.35 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 5.74 | 5.13 | 4.15 | 3.60 | 5.53 | 4.92 | 3.35 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | FP16 | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
  prefs: []
  type: TYPE_TB
- en: '| W6A6 | SmoothQuant | 7.47 | 6.97 | 6.34 | 5.99 | 7.76 | 6.76 | 5.88 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 7.43 | 6.84 | 6.22 | 5.82 | 7.48 | 6.74 | 5.91 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 7.34 | 6.82 | 6.17 | 5.80 | 7.31 | 6.71 | 5.76 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 7.12 | 6.64 | 6.00 | 5.64 | 7.03 | 6.50 | 5.54 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 7.13 | 6.64 | 6.01 | 5.64 | 7.03 | 6.50 | 5.54 |'
  prefs: []
  type: TYPE_TB
- en: 'Table D5: Zero-shot common-sense QA ($\uparrow$) results of LLaMA1 models under
    6-bit WA quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B W6A6 | FP16 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 76.82 | 51.35 | 41.13 | 72.08 | 71.42 | 65.98 | 61.13 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 76.60 | 52.29 | 40.63 | 72.65 | 71.29 | 63.85 | 62.89 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 77.26 | 52.02 | 41.04 | - | 71.40 | 65.19 | 61.38 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 77.53 | 51.47 | 41.13 | 72.78 | 72.76 | 66.69 | 63.73 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 77.42 | 52.65 | 40.53 | 71.53 | 72.64 | 67.72 | 63.75 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-13B W6A6 | FP16 | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 78.29 | 56.90 | 43.09 | 66.98 | 75.09 | 69.22 | 64.92 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 77.91 | 57.70 | 42.92 | - | 75.02 | 69.14 | 64.54 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 78.62 | 59.51 | 44.03 | 68.44 | 75.98 | 70.08 | 66.11 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 79.16 | 59.39 | 43.69 | 68.10 | 75.81 | 69.06 | 65.87 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-30B W6A6 | FP16 | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 77.14 | 57.61 | 42.91 | 65.56 | 78.07 | 69.92 | 65.20 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 80.14 | 58.92 | 45.05 | 68.02 | 77.96 | 71.98 | 67.01 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 79.81 | 58.79 | 45.22 | 68.38 | 78.95 | 72.21 | 67.23 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 79.65 | 58.08 | 44.11 | - | 78.38 | 73.24 | 66.69 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 79.43 | 59.34 | 44.54 | 70.15 | 78.89 | 72.77 | 67.52 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 80.09 | 57.95 | 45.05 | 68.72 | 79.17 | 73.09 | 67.35 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-65B W6A6 | FP16 | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 80.25 | 57.92 | 45.50 | 80.22 | 80.18 | 74.76 | 69.80 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 79.67 | 55.68 | 45.22 | 80.02 | 78.03 | 73.95 | 68.76 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 81.01 | 58.12 | 46.33 | 80.64 | 79.91 | 75.69 | 70.28 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 80.14 | 57.79 | 45.05 | - | 79.74 | 74.59 | 67.46 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 80.96 | 59.09 | 46.76 | 82.20 | 80.68 | 77.27 | 71.16 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 80.63 | 58.00 | 46.50 | 82.08 | 80.49 | 76.87 | 70.76 |'
  prefs: []
  type: TYPE_TB
- en: 'Table D6: Zero-shot common-sense QA ($\uparrow$) results of LLaMA2 models under
    6-bit WA quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B W6A6 | FP16 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96 | 67.25 | 63.72
    |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 75.57 | 53.62 | 39.93 | 69.54 | 71.76 | 66.14 | 62.76 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 76.22 | 52.74 | 40.70 | - | 71.89 | 65.19 | 61.35 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 76.55 | 53.83 | 40.96 | 68.75 | 55.89 | 65.59 | 60.26 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 77.48 | 52.99 | 39.33 | - | 71.38 | 65.98 | 61.43 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 76.99 | 52.99 | 40.87 | 70.40 | 72.49 | 67.32 | 63.51 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 76.88 | 52.31 | 40.44 | 69.72 | 72.60 | 66.93 | 63.15 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B W6A6 | FP16 | 79.05 | 57.91 | 44.20 | 69.02 | 76.60 | 69.69 |
    66.08 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 78.29 | 57.41 | 43.86 | 69.50 | 75.02 | 66.93 | 65.17 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 78.29 | 59.13 | 43.34 | - | 75.37 | 67.56 | 64.74 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 78.24 | 57.58 | 43.86 | 71.10 | 75.52 | 68.35 | 65.78 |'
  prefs: []
  type: TYPE_TB
- en: '| AffineQuant | 78.35 | 57.58 | 43.34 | 66.73 | 74.71 | 68.59 | 64.88 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 78.78 | 58.29 | 43.77 | - | 75.10 | 68.43 | 64.87 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 78.62 | 56.94 | 43.43 | 68.35 | 76.19 | 69.22 | 65.46 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 78.94 | 57.95 | 44.11 | 68.81 | 76.17 | 68.98 | 65.83 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-70B W6A6 | FP16 | 81.01 | 59.68 | 47.95 | 75.87 | 80.87 | 76.95 |
    70.39 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant | 79.87 | 57.32 | 45.65 | 77.13 | 79.01 | 74.03 | 68.84 |'
  prefs: []
  type: TYPE_TB
- en: '| OS+ | 79.33 | 59.09 | 47.18 | - | 79.46 | 75.06 | 68.02 |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 80.20 | 60.27 | 46.84 | - | 80.55 | 76.01 | 68.77 |'
  prefs: []
  type: TYPE_TB
- en: '| QLLM | 80.63 | 59.01 | 45.99 | - | 79.64 | 75.37 | 68.13 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 80.96 | 59.39 | 47.27 | 77.34 | 80.70 | 76.40 | 70.34 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 81.18 | 59.26 | 47.78 | 77.86 | 80.68 | 76.95 | 70.62 |'
  prefs: []
  type: TYPE_TB
- en: Appendix E More Ablation Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E.1 End-to-end Time Speedup and Memory Saving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Section [4.2](#S4.SS2 "4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"), we analyzed
    the layer-wise time speedup for the LLaMA2-7B and LLaMA2-13B models. Here, we
    present comprehensive end-to-end results for time speedup and memory savings achieved
    with the LLaMA2-7B model on a single NVIDIA RTX 3090 GPU. As shown in Table [E7](#A5.T7
    "Table E7 ‣ E.1 End-to-end Time Speedup and Memory Saving ‣ Appendix E More Ablation
    Studies ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"), DuQuant achieves a maximum speedup of $2.01\times$ through
    quantization. These results underscore the efficiency of DuQuant in optimizing
    resource utilization, highlighting its potential to enhance performance and reduce
    costs in deploying large language models, particularly in resource-constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E7: End-to-end prefilling speedup on LLaMA2-7B model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch Size | FP16 Time | DuQuant Time | Speedup |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 568ms | 294ms | 1.93$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1003ms | 509ms | 1.97$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1449ms | 720ms | 2.01$\times$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table E8: Peak memory usage during prefilling phase of LLaMA2-7B model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch Size | FP16 Mem. | DuQuant Mem. | Saving Factor |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 15.28GB | 4.79GB | 3.20$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 17.94GB | 5.94GB | 3.02$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20.56GB | 7.10GB | 2.90$\times$ |'
  prefs: []
  type: TYPE_TB
- en: E.2 Effects of Rotation Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ablation of Rotation Block Size.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To further explore the impact of rotation block size, we apply varying block
    sizes in the rotation matrices to both LLaMA2-7B and LLaMA2-13B models and evaluate
    the perplexity of the quantized models. The results, presented in Table [E9](#A5.T9
    "Table E9 ‣ Ablation of Rotation Block Size. ‣ E.2 Effects of Rotation Matrix
    ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs"), indicate that increasing block
    sizes generally improves model performance. This improvement occurs because larger
    block sizes allow outliers to be distributed across more channels, evening out
    values throughout the activation/weight matrix thereby enhancing quantization
    accuracy and performance. Additionally, quantization runtime decreases with larger
    block sizes, likely due to more efficient transformations during the reshaping
    of original activation/weight matrices. Consequently, we adopt 128 as our rotation
    block size for all experiments for efficiency and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E9: Impact of rotation block size.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Block Size | LLaMA2-7B | LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 $\downarrow$ | Time/s |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 18.69 | 26.48 | 64.4 | 8.81 | 13.03 | 97.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 10.77 | 15.04 | 53.8 | 7.02 | 9.68 | 80.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 8.69 | 11.46 | 48.2 | 6.12 | 8.12 | 75.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 6.96 | 8.85 | 48.3 | 5.61 | 7.35 | 76.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | 6.38 | 8.07 | 50.1 | 5.45 | 7.13 | 74.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | 6.28 | 7.90 | 48.6 | 5.42 | 7.05 | 74.0 |'
  prefs: []
  type: TYPE_TB
- en: Ablation of Rotation Times.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Identifying the optimal rotation matrix $\mathbf{R}$ for all our experiments,
    as it offers the optimal balance between model performance and time usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E10: Impact of rotation times.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA2-7B | LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation Times | WikiText2 $\downarrow$ | Time/s |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6.60 | 8.41 | 22.9 | 5.48 | 7.12 | 37.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 6.34 | 8.04 | 22.6 | 5.41 | 7.06 | 38.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 6.32 | 7.98 | 28.8 | 5.43 | 7.05 | 41.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | 6.34 | 7.98 | 29.0 | 5.43 | 7.06 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 256 | 6.28 | 7.90 | 48.6 | 5.42 | 7.05 | 74.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1024 | 6.31 | 8.01 | 129.7 | 5.46 | 7.12 | 179.8 |'
  prefs: []
  type: TYPE_TB
- en: E.3 Effects of Permutation Algorithm.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in Section [3.2](#S3.SS2 "3.2 The proposed DuQuant Method ‣ 3 Method
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"), rotation transformations within each block are limited and unable to
    redistribute outliers across different blocks. To address this, we introduce a
    permutation transformation aimed at balancing outliers more comprehensively. Our
    primary goal is to minimize the variance among different blocks, as outlined in
    Eqn. ([4](#S3.E4 "In The Permutation Transformation. ‣ 3.2 The proposed DuQuant
    Method ‣ 3 Method ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs")). We explore several optimization algorithms,
    with the results detailed in Table [E11](#A5.T11 "Table E11 ‣ E.3 Effects of Permutation
    Algorithm. ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"). Note that the variance
    values are measured on activation values of the query project in the first layer
    of each model, and the time in the table represents the runtime of calibration.
    The Zigzag permutation notably reduces the variance to 3.0e-4, achieving this
    with minimal time expenditure and yielding competitive perplexity results. While
    Simulated Annealing slightly outperforms Zigzag in terms of perplexity for the
    LLaMA2-7B model, it was significantly more time-consuming, and the marginal gains
    did not justify the additional complexity. Therefore, we select Zigzag permutation
    as our preferred method, leading to smoother outlier distribution and more effective
    quantized models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E11: Impact of channel permutation algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMA2-7B | LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Permutation Method | WikiText2 $\downarrow$ | Variance | Time/s |'
  prefs: []
  type: TYPE_TB
- en: '| w.o. Permutation | 7.92 | 10.64 | 3.9e-2 | 27.5 | 5.96 | 7.94 | 3.1e-2 |
    44.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 6.40 | 8.08 | 4.9e-3 | 89.5 | 5.43 | 7.07 | 3.9e-3 | 148.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Simulated Annealing | 6.26 | 7.89 | 1.7e-4 | 769.6 | 5.42 | 7.06 | 1.5e-4
    | 1257.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Zigzag | 6.28 | 7.90 | 3.0e-4 | 48.6 | 5.42 | 7.05 | 2.5e-4 | 74.0 |'
  prefs: []
  type: TYPE_TB
- en: E.4 Effects of Calibration Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ablation of Different Calibration Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We apply our DuQuant to quantize the LLaMA2-7B model using different calibration
    datasets, with results presented in Table [E12](#A5.T12 "Table E12 ‣ Ablation
    of Different Calibration Datasets. ‣ E.4 Effects of Calibration Datasets ‣ Appendix
    E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"). It can be observed that the selection of
    calibration datasets has a relatively minor impact on quantization performance.
    This is because our method uses the calibration data solely to identify outlier
    channels, rather than for gradient-based parameter learning as seen in methods
    like OmniQuant [[42](#bib.bib42)] and AffineQuant [[36](#bib.bib36)]. This ablation
    study underscores the robustness of our DuQuant method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E12: Ablation of calibration datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLaMA2-7B | Eval. |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Calib. | WikiText2 | 6.28 | 7.90 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | 6.25 | 7.87 |'
  prefs: []
  type: TYPE_TB
- en: Calibration-free Quantization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To further explore the robustness of DuQuant under varying calibration conditions,
    we generate random calibration data within the vocabulary range of the model,
    setting the sample count to 256. The results, shown in Table [E13](#A5.T13 "Table
    E13 ‣ Calibration-free Quantization. ‣ E.4 Effects of Calibration Datasets ‣ Appendix
    E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"), indicate that even in calibration-free settings,
    our method continues to perform well, achieving results that are competitively
    close to those obtained with actual calibration data. This demonstrates that DuQuant could
    provide a viable solution in real-world scenarios where obtaining specific calibration
    data is challenging or impossible. The ability to maintain high performance without
    traditional calibration data opens avenues for deploying quantized models in environments
    with strict privacy or data availability limitations, suggesting a promising direction
    for future research to enhance model adaptability and deployment flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E13: Calibration-free quantization, where we generate random data within
    vocabulary range.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLaMA2-7B | Eval. |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Calib. | Randomly Generated | 6.25 | 7.86 |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 | 6.25 | 7.87 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B | Eval. |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Calib. | Randomly Generated | 5.45 | 7.05 |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2 | 5.44 | 7.05 |'
  prefs: []
  type: TYPE_TB
- en: Ablation of Different Numbers of Calibration Samples.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We utilize our DuQuant to quantize the LLaMA2-7B model using varying numbers
    of calibration samples from the WikiText2 dataset, with results detailed in Table [E14](#A5.T14
    "Table E14 ‣ Ablation of Different Numbers of Calibration Samples. ‣ E.4 Effects
    of Calibration Datasets ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Interestingly,
    the quantization performance shows a low correlation with the number of samples,
    demonstrating the robustness of DuQuantṪhis stability arises because we utilize
    the mean activation values from these samples to construct our rotation matrices.
    Since we average the activations, the influence of any single, potentially non-representative
    sample is minimized, ensuring consistent performance. Notably, as we use mean
    values, the time cost of our quantization process remains constant regardless
    of the number of samples, enhancing the efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table E14: Ablation of different numbers in the calibration dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| # of Samples | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 6.29 | 7.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 6.31 | 7.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | 6.29 | 7.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | 6.28 | 7.90 |'
  prefs: []
  type: TYPE_TB
- en: '| 256 | 6.23 | 7.88 |'
  prefs: []
  type: TYPE_TB
- en: Appendix F Detailed Comparison with QuaRot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table F15: Evaluation results between QuaRot and DuQuant under W4A4 quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B W4A4 | FP16 | 5.68 | 7.08 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00
    | 67.07 | 64.09 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-RTN | 7.08 | 8.73 | 74.59 | 48.57 | 36.01 | 68.99 | 65.69 | 58.56
    | 46.03 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-GPTQ | 6.44 | 7.87 | 76.17 | 49.96 | 38.23 | 70.80 | 69.29 | 63.06
    | 61.25 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 6.40 | 7.84 | 76.44 | 50.04 | 38.99 | 70.98 | 69.39 | 64.72 | 61.76
    |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 6.18 | 7.73 | 76.22 | 50.04 | 38.31 | 70.09 | 69.82 | 62.59
    | 61.18 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B W4A4 | FP16 | 5.47 | 6.97 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96
    | 67.25 | 63.72 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-RTN | 9.66 | 11.98 | 69.48 | 46.25 | 32.76 | 64.80 | 60.75 | 56.67
    | 44.04 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-GPTQ | 6.39 | 8.15 | 75.15 | 49.15 | 36.68 | 67.89 | 68.87 | 61.33
    | 59.85 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 6.28 | 7.90 | 75.24 | 51.89 | 36.77 | 67.86 | 69.54 | 62.12 | 60.57
    |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 6.08 | 7.79 | 75.68 | 50.00 | 37.46 | 69.24 | 69.74 | 63.93
    | 61.01 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA3-8B W4A4 | FP16 | 6.14 | 8.88 | 80.85 | 77.78 | 53.41 | 81.28 | 79.16
    | 72.84 | 74.22 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-RTN | 13.89 | 17.59 | 69.64 | 57.58 | 34.56 | 66.76 | 63.46 | 62.75
    | 59.13 |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot-GPTQ | 8.69 | 12.40 | 74.54 | 67.38 | 40.61 | 70.43 | 70.47 | 65.11
    | 64.76 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 8.53 | 12.01 | 76.93 | 70.88 | 45.05 | 74.59 | 73.17 | 66.14 |
    67.79 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant+LWC | 8.06 | 11.29 | 76.22 | 70.41 | 43.69 | 74.34 | 73.87 | 67.80
    | 67.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table F16: Matrices comparison between DuQuant and QuaRot under W4A4 quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | LLaMA2-7B | LLaMA2-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot | 9.66 | 11.98 | 6.73 | 8.69 |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 7.92 | 10.64 | 5.96 | 7.94 |'
  prefs: []
  type: TYPE_TB
- en: 'Table F17: Quantization runtime comparison on a single NVIDIA A100 80G GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  prefs: []
  type: TYPE_TB
- en: '| QuaRot | 20min | 36min | 5.1h |'
  prefs: []
  type: TYPE_TB
- en: '| DuQuant | 50s | 71s | 270s |'
  prefs: []
  type: TYPE_TB
- en: 'In this section, we present a detailed comparison between our DuQuant and QuaRot [[2](#bib.bib2)].
    QuaRot employs Hadamard matrices to mitigate outliers in activations and utilizes
    the GPTQ algorithm for weight quantization to achieve competitive performance.
    However, our DuQuant method demonstrates several distinct advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Effective Use of Prior Knowledge: DuQuant leverages prior knowledge to accurately
    target and eliminate outliers through multiple rotations, achieving a smoother
    activation distribution compared to the Hadamard transformation, as demonstrated
    in Figure [7](#S4.T7 "Table 7 ‣ Comparison with QuaRot [2] ‣ 4.2 Ablation Study
    ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efficient Channel Permutation: Our channel permutation not only further smooths
    outlier features but also benefits from rapid implementation, enhancing overall
    performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simultaneous Weight Matrix Smoothing: Unlike QuaRot, DuQuant directly and efficiently
    smooths the weight matrix, avoiding the time-consuming GPTQ algorithm and accelerating
    the quantization process, as demonstrated high quantization efficiency in Table [F17](#A6.T17
    "Table F17 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Experimental results underscore the superiority of DuQuant over QuaRot. For
    a fair comparison, we reproduce the QuaRot under 4-bit per-channel weight and
    per-token activation asymmetric quantization. Table [F15](#A6.T15 "Table F15 ‣
    Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") displays the perplexity
    (PPL) and zero-shot accuracy for models LLaMA1-7B, LLaMA2-7B, and LLaMA3-8B. Our
    DuQuant method consistently outperforms QuaRot-RTN across all benchmarks, showcasing
    our advanced weight matrix management. Furthermore, compared to QuaRot-GPTQ, DuQuant and
    DuQuant+LWC achieve better average accuracy across six QA tasks and demonstrate
    superior performance on the WikiText and C4 datasets, particularly for LLaMA3-8B.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we assess the effectiveness of the rotation matrices utilized
    in DuQuant  which incorporate prior knowledge against the Hadamard matrices used
    in QuaRot. For a fair comparison, we omit the permutation step in DuQuant and
    directly contrast it with QuaRot-RTN. Results in Table [F16](#A6.T16 "Table F16
    ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") show that our DuQuant without
    permutation outperforms QuaRot by a clear margin, which confirms that our rotation
    transformation is more effective than Hadamard by leveraging prior knowledge.
    It is worth noting that because a Hadamard matrix is orthogonal and symmetric,
    it multiplies by itself to yield the identity matrix. In other words, the Hadamard
    matrix is not suitable for greedy searches aimed at finding smaller outliers.
    These findings differentiate DuQuant from QuaRot and highlight the effectiveness
    of our approach in managing outliers for post-training quantization of large language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Limitations and Broader Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Limitations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary limitation of our method is the lack of a specialized strategy for
    calibration data selection. We adhere to established practices [[42](#bib.bib42),
    [36](#bib.bib36), [31](#bib.bib31), [61](#bib.bib61), [2](#bib.bib2), [1](#bib.bib1)]
    by randomly selecting 128 samples from the WikiText2 dataset to compute the mean
    embeddings that inform our rotation matrix and zigzag permutation order. We also
    explore the possibility of calibration-free quantization and show some promising
    results. However, further investigating more tailored choices for calibration
    data can potentially enhance the performance of our quantized models. We leave
    this for future study.
  prefs: []
  type: TYPE_NORMAL
- en: Broader Impacts.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our work identifies the presence of massive outliers in down-projection layer
    of FFN modules, which significantly complicates low-bit weight-activation quantization.
    To address this challenge, we implement a combination of rotation matrices and
    permutations to effectively smooth both massive and uniform outliers, proving
    both fast and effective. Consequently, we establish a new state-of-the-art for
    INT4 weight-activation post-training quantization methods. Our approach aims to
    accelerate large language models and reduce memory usage during deployment, offering
    substantial benefits to the field of LLM research. These advancements could lead
    to more efficient and accessible LLM applications, facilitating broader usage
    and enabling more sustainable AI implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H More Visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We provide additional visualizations of normal and massive outliers in various
    models (LLaMA1, LLaMA2, Vicuna-v1.5) from Figure [H1](#A8.F1 "Figure H1 ‣ Appendix
    H More Visualizations ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs") to Figure [H8](#A8.F8 "Figure H8 ‣ Appendix
    H More Visualizations ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"). In each figure, the left side illustrates
    changes in normal outliers before and after applying our rotation and permutation
    transformations, while the right side shows the changes in massive outliers before
    and after transformations. It is evident that massive outliers consistently occur
    in the down-projection layer of the FFN module across all models, supporting our
    findings discussed in Section [2](#S2 "2 Motivation ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Conversely,
    normal outliers appear in different modules within the transformation block. For
    instance, Figure [H3](#A8.F3 "Figure H3 ‣ Appendix H More Visualizations ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") shows normal outliers at the up-projection layer of the FFN module in LLaMA1-13B.
    Significantly, both massive and normal outliers are reduced markedly after our
    rotation and permutation transformations, leading to easier quantization of activations.
    This underscores the effectiveness of our RAP in managing outlier features across
    diverse LLM models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d18b5ff06ce331f4b9ae241b22c488d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H1: Activation change with the use of our DuQuant for LLaMA1-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/236073a6cb59e1a849291f5e204403e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H2: Activation change with the use of our DuQuant for LLaMA1-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91c8c9036642b8ff7d759e0484574db9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H3: Activation change with the use of our DuQuant for LLaMA1-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b7b9419c20313e19fcd77ceb09daa4fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H4: Activation change with the use of our DuQuant for LLaMA1-65B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca2c8f6989056893e48aedf5e7c4c500.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H5: Activation change with the use of our DuQuant for LLaMA2-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6dc216fb245aa4139d9ff8317f503abf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H6: Activation change with the use of our DuQuant for LLaMA2-70B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b365f13b159f31fc2f7b8ffd7d7a0b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H7: Activation change with the use of our DuQuant for Vicuna-v1.5-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9d6af9f6a5a6496385f73510ee692a80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure H8: Activation change with the use of our DuQuant for Vicuna-v1.5-13B.'
  prefs: []
  type: TYPE_NORMAL
