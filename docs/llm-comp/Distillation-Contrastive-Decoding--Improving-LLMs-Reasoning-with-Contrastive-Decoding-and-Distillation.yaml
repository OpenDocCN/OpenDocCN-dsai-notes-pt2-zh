- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:10'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14874](https://ar5iv.labs.arxiv.org/html/2402.14874)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Phuc Phan^∗, Hieu Tran * Equal contribution    Long Phan
  prefs: []
  type: TYPE_NORMAL
- en: VietAI Research
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We propose a straightforward approach called Distillation Contrastive Decoding
    (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during
    inference. In contrast to previous approaches that relied on smaller amateur models
    or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought
    Prompting and advanced distillation techniques, including Dropout and Quantization.
    This approach effectively addresses the limitations of Contrastive Decoding (CD),
    which typically requires both an expert and an amateur model, thus increasing
    computational resource demands. By integrating contrastive prompts with distillation,
    DCD obviates the need for an amateur model and reduces memory usage. Our evaluations
    demonstrate that DCD significantly enhances LLM performance across a range of
    reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and
    StrategyQA datasets.¹¹1Code is available at [https://github.com/pphuc25/distil-cd](https://github.com/pphuc25/distil-cd)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e687aa3ff1c19514742f315c178488f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of Distillation Contrastive Decoding method. Valid CoT
    demonstrations as well as the query will be sent to an LLM, while invalid CoT
    demonstrations and the query will be sent into a distilled version of the model.
    We will then use this logit information to enhance the reasoning decoding process.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reasoning capabilities in large language models (LLMs) refer to the models’
    ability to analyze, understand, and infer information, mirroring human-like logical
    reasoning. Recently, the reasoning skills of LLMs have seen substantial advancements,
    showcasing their vast potential in various natural language processing applications
    Brown et al. ([2020a](#bib.bib3)). While some research focuses on enhancing models
    through advanced training techniques and architectures Touvron et al. ([2023a](#bib.bib18));
    Jiang et al. ([2023](#bib.bib11)); Bai et al. ([2023](#bib.bib1)), others aim
    to augment the models’ internal capabilities Zou et al. ([2023](#bib.bib22));
    Bricken et al. ([2023](#bib.bib2)). Beyond model training and augmentation, further
    research explores innovative methods to enhance LLM efficiency during inference
    Li et al. ([2023b](#bib.bib14), [a](#bib.bib13)); Chuang et al. ([2023](#bib.bib6)).
    In this work, we introduce Distillation Contrastive Decoding (DCD), a method designed
    to enhance the reasoning abilities of LLMs during inference by leveraging Contrastive
    Chain-of-thought prompts and distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distillation Contrastive Decoding (DCD) builds on recent advancements in enhancing
    the reasoning capabilities of LLMs through Contrastive Decoding (CD) O’Brien and
    Lewis ([2023](#bib.bib16)) and Contrastive Chain-of-thought Prompting (CP) Chia
    et al. ([2023](#bib.bib5)). These methods utilize contrasting elements to reduce
    reasoning errors in text generation, thereby improving task performance. The primary
    motivation behind DCD is addressing two common limitations of CD. Firstly, CD
    typically requires a smaller amateur LLM within the same family to evaluate the
    outputs of the primary LLM. This prerequisite poses a challenge, especially for
    small-sized models, as smaller models with identical vocabularies may not be available.
    This challenge is notably present in cases such as Mistral-7B Jiang et al. ([2023](#bib.bib11))
    and DeepSeek-7B DeepSeek-AI et al. ([2024](#bib.bib8)), where smaller models are
    unavailable. The second limitation with CD is the requirement to simultaneously
    load two models into memory: an expert and an amateur model, which significantly
    increases computational resource demands. An example of this is using Llama2-7B
    as the amateur model and Llama2-13B as the expert model, highlighting the resource-intensive
    nature of the CD approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Our findings demonstrate that DCD surpasses existing methodologies in enhancing
    Chain-of-thought reasoning within LLMs. Specifically, on the GSM8K benchmark,
    which comprises grade-school level word math problems, DCD elevates the performance
    of Llama2 models by as much as $3.79\%$. We observe marked improvements in both
    arithmetic and commonsense reasoning tasks when DCD is applied to Mistral-7B,
    known for its robust foundational knowledge and high scores on the MMLU benchmark
    Hendrycks et al. ([2020](#bib.bib10)), suggesting that DCD could bring such widespread
    improvements to much stronger models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are: (1) Introducing a straightforward approach
    that combines Contrastive Chain-of-thought Prompting, Contrastive Decoding, and
    Distillation to enhance LLM reasoning abilities, eliminating the need for smaller
    models and reducing memory usage. (2) Demonstrating significant performance improvements
    across multiple reasoning benchmarks compared to Contrastive Decoding and other
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chain-of-thought (CoT) is a significant development in enhancing text-generation
    models’ reasoning capabilities. This concept, as originally introduced by Wei
    et al. ([2023](#bib.bib21)), involves the model generating intermediate steps
    in its reasoning process, akin to human problem-solving methods. Furthermore,
    the work by Kojima et al. ([2023](#bib.bib12)) revealed that specific prompts,
    such as "Let’s think step-by-step", can spontaneously trigger CoT reasoning in
    LLMs. These developments are the foundation for research works on enhancing LLM’s
    reasoning abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, O’Brien and Lewis ([2023](#bib.bib16)) demonstrated that Contrastive
    Decoding (CD), a decoding method proposed by Li et al. ([2023b](#bib.bib14)),
    can enhance LLM performance across a range of reasoning tasks. Initially, CD was
    designed to enhance the quality of long-form text generation by identifying tokens
    that significantly differ in likelihood between a strong model and a comparatively
    weak model. The study by O’Brien and Lewis ([2023](#bib.bib16)) further revealed
    that incorporating a smaller amateur LLM in the CD process can effectively reduce
    reasoning errors in the larger expert model, thereby achieving high performance
    across multiple benchmarks. Another study by Chuang et al. ([2023](#bib.bib6))
    proposes an alternative approach by contrasting the differences in logits obtained
    from projecting the later layers versus earlier layers to the vocabulary space
    in an LLM. Chia et al. ([2023](#bib.bib5)) looks into improving downstream CoT
    reasoning by incorporating both positive and negative reasoning in the few-shot
    sequences to allow the model to learn from both positive and negative examples.
  prefs: []
  type: TYPE_NORMAL
- en: Besides decoding intervention methods, recent work by Zou et al. ([2023](#bib.bib22))
    has introduced a new research area known as Representation Engineering (RepE).
    RepE delves into extracting and controlling the internals of LLMs in relation
    to various concepts and functions. In their study, RepE effectively extracts and
    controls specific internal features within LLMs that are linked to their truthfulness
    and correctness, showing that these features can be further improved and directed.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c7602b542cc81c696ffc59aef968208.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison between 3 methods: (1) Contrastive Chain-of-thought Prompting,
    which relies on extensive prefixes incorporating Contrastive Chain-of-thought
    examples; (2) Contrastive Decoding, which necessitates the availability of a smaller
    amateur version of the LLM; and (3) Distillation Contrastive Decoding (Ours),
    conceived to overcome the constraints of the previous methods by incorporating
    the fundamental principles of both (1) and (2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach, Distillation Contrastive Decoding (DCD), builds upon the foundational
    work of Contrastive Decoding (CD) O’Brien and Lewis ([2023](#bib.bib16)) and Contrastive
    Chain-of-thought Prompting (CP) Chia et al. ([2023](#bib.bib5)). A principal motivation
    behind DCD is to overcome a significant limitation of CD: its reliance on a smaller
    model of the same architecture, often referred to as an amateur model. This dependency
    poses significant challenges, as an equivalent amateur model is not always available
    across different open-source architectures, a situation exemplified by Mistral
    Jiang et al. ([2023](#bib.bib11)). DCD aims to offer a more adaptable and inclusive
    solution, irrespective of the specific class of language model employed.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Contrastive Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Contrastive Decoding (CD) involves two models: a larger expert model, and a
    smaller amateur model. The method leverages a comparison between the predicted
    logits of an expert model, denoted as $s_{e}$ is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s=(1+\beta)\cdot s_{e}-\beta\cdot s_{a}$ |  |'
  prefs: []
  type: TYPE_TB
- en: By exploiting the differences in predictive confidence between the two models,
    this method improves the generation of text sequences in reasoning tasks. However,
    the work shows that while a 1B-parameter amateur helps improve reasoning capabilities,
    a 7B-parameter amateur harms it. This poses a significant drawback as not all
    model classes have a 1B-parameter model to act as an amateur model in the decoding
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Contrastive Chain-of-thought Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrastive Chain-of-thought Prompting (CP) integrates both correct and incorrect
    reasoning examples to direct the model through a step-by-step reasoning process,
    thereby minimizing logical errors. This method is inspired by the human ability
    to learn from both successful and unsuccessful examples. By including examples
    of both sound and flawed reasoning, the technique aids the model in identifying
    and correcting potential mistakes in intermediate reasoning steps. Such errors
    have been identified as significant obstacles to accurate reasoning processes
    Ling et al. ([2023](#bib.bib15)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, given a query $Q$. The method can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A_{j}=(Q_{j},E_{1+},E_{1-},...,E_{n+},E_{n-})$ |  |'
  prefs: []
  type: TYPE_TB
- en: However, the method tends to extend the length of input sequences significantly,
    necessitating increased computational resources. In our experiments, we have also
    observed that the inclusion of multiple shots of both valid and invalid demonstrations
    can lead to confusion in an unaligned LLM, consequently diminishing its reasoning
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Distillation Contrastive Decoding (Ours)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Distillation Contrastive Decoding (DCD) is designed to overcome existing drawbacks
    in both CD and CP. Instead of requiring an external 1B-parameters amateur model,
    we utilize distillation techniques to acquire the amateur reasoning information.
    For the anchor expert model, we employ regular valid CoT demonstrations as a few
    shot examples. For the distilled amateur model, we employ invalid CoT examples
    to enable the motivations in leveraging incorrect reasoning features in computing
    the next token weights. The DCD algorithm is shown in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.3 Distillation Contrastive Decoding (Ours) ‣ 3 Methodology ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Distillation Contrastive Decoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Query $Q$'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we found that distilling the model by enabling a higher dropout
    rate during the inference step works best in most cases. The final results comparing
    between DCD with Dropout and previous baselines are shown in Section [6](#S6 "6
    Results ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation"). Additionally, we explore other distillation methods
    such as Quantization, as well as a combined approach of applying both Dropout
    and Quantization to the model in Section [7](#S7 "7 Distillation Methods ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Contrastive Chain-of-thought Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compared to conventional prompting methods with in-context demonstrations Brown
    et al. ([2020b](#bib.bib4)), Chain-of-thought (CoT) prompting Wei et al. ([2023](#bib.bib21))
    enhances this approach by incorporating a rationale for each few-shot example.
    This rationale is composed of a sequence of intermediate reasoning steps, which
    effectively guide the language model through a systematic process to assist the
    model in understanding and solving complex tasks. Wang et al. ([2023](#bib.bib20))
    identifies two components of a CoT rationale:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridging objects are the symbolic items that the model saw during the traverse
    to the final answer. In arithmetic reasoning, these are numbers and equations,
    while in factual/commonsense reasoning, these are subject and object entities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language templates are the complementary parts of the bridging objects, which
    serve as textual hints and relations or predicates that guide the model to derive
    the correct bridging objects throughout the reasoning process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Building on previous research Chia et al. ([2023](#bib.bib5)) that explores
    Contrastive Chain-of-thought Prompting design, we identified three types of contrasting
    bridging objects and one type of contrasting both bridging objects and language
    templates in arithmetic reasoning tasks. In our experiments on contrasting bridging
    objects, we explored three settings: (1) number shuffle, (2) number shuffle plus
    equation error, and (3) number shuffle plus irrelevant object plus operation swapping.
    For the Contrastive Chain-of-thought that involves contrasting both bridging objects
    and language templates, (4) we prompted GPT-3.5 to generate contrastive synthetic
    demonstrations. An example of each contrastive demonstration is shown in Figure
    [4](#S4.F4 "Figure 4 ‣ 4 Contrastive Chain-of-thought Design ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").
    Figure [3](#S4.F3 "Figure 3 ‣ 4 Contrastive Chain-of-thought Design ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation")
    shows the accuracy of the four contrastive settings on the GSM8K dataset using
    the Llama2 model with our method (DCD). Each of the four contrasting designs demonstrates
    a different increase in score compared to baselines. These preliminary results
    suggest that the incorporation of both contrastive bridging objects and language
    templates is crucial for designing effective Contrastive Chain-of-thought demonstrations.
    Additionally, setting (4), which includes synthetic examples, shows a significant
    increase in score. This indicates that DCD can effectively utilize automatic synthetic
    contrastive prompting generation with an external LLM, such as GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4070d5987b5e94c84a7d231dc489dd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Performance of different Contrastive Chain-of-thought settings discussed
    in Section [4](#S4 "4 Contrastive Chain-of-thought Design ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").
    Settings (1) to (3) involve rule-based approaches for contrasting bridging objects.
    Setting (4) employs a synthetic-based approach, incorporating contrasts in both
    bridging objects and language templates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84f73b6293e9e23007038068bcd84b00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of discrepancies among invalid CoT prompts. For more
    details, see Appendix [C](#A3 "Appendix C Appendix: Full Prompts for Amateurs
    Model ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experimental Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To obtain results, we evaluated two domains of text generation: arithmetic
    reasoning and commonsense reasoning. For arithmetic reasoning, we utilized the
    GSM8K dataset Cobbe et al. ([2021](#bib.bib7)), and for commonsense reasoning,
    the StrategyQA dataset Geva et al. ([2021](#bib.bib9)) was employed.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Arithmetic Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GSM8K dataset Cobbe et al. ([2021](#bib.bib7)) is structured to facilitate
    question answering on fundamental mathematical problems that require multi-step
    reasoning for resolution. The solutions to these problems primarily involve performing
    a sequence of elementary calculations using basic arithmetic operations, including
    addition, subtraction, multiplication, and division. In our experimental setup,
    we employed the complete test set, which consisted of 1319 samples. We utilized
    an 8-shot for the expert model and a 3-shot (using synthetic demonstrations) for
    the amateur model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Commonsense Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The StrategyQA dataset Geva et al. ([2021](#bib.bib9)) is a question-answering
    benchmark focusing on open-domain questions, requiring implicit reasoning to infer
    the necessary steps from the question itself through a strategic approach. It
    is designed to evaluate the ability to perform implicit reasoning, necessary for
    answering questions that do not have direct or explicit answers within the text.
    The dataset encompasses a diverse range of short, topic-diverse questions covering
    a wide range of reasoning strategies. In our study, we employed the full test
    set, which consists of 2290 samples, employing a 6-shot for both expert and amateur
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare Distillation Contrastive Decoding (DCD) with three decoding intervention
    baselines: Contrastive Chain-of-thought Prompting (CP), Contrastive Decoding (CD),
    and DoLA Chuang et al. ([2023](#bib.bib6)). For each baseline, we adhered to the
    original setup’s hyperparameters. For CD, we set $\alpha$ with a step of 13B models,
    both with the step of 2\. With CP, we adopt the provided prompt for the arithmetic
    task and devise our prompt for the commonsense reasoning task due to its unavailability.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Models and Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted experiments with Distillation Contrastive Decoding (DCD) on the
    Llama 1&2 Touvron et al. ([2023a](#bib.bib18), [b](#bib.bib19)), Mistral Jiang
    et al. ([2023](#bib.bib11)), and DeepSeek DeepSeek-AI et al. ([2024](#bib.bib8))
    models. For the Llama models, we engaged both the 7B and 13B variants. Meanwhile,
    we utilized the 7B versions for both Mistral and DeepSeek.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experiments, we controlled four distinct parameters: $\alpha$, is described
    in Section [7.1](#S7.SS1 "7.1 Dropout Rate ‣ 7 Distillation Methods ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | GSM8K | StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-7B | Greedy | 14.32 | 60.04 |'
  prefs: []
  type: TYPE_TB
- en: '| CP | 14.25 | 59.91 |'
  prefs: []
  type: TYPE_TB
- en: '| CD | 15.39 | 61.62 |'
  prefs: []
  type: TYPE_TB
- en: '| DoLA | 14.03 | 64.02 |'
  prefs: []
  type: TYPE_TB
- en: '| CP + CD | 16.00 | 63.23 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout] (Ours) | 17.28 | 65.15 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Quantization] (Ours) | 16.00 | 63.18 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout + Quantization] (Ours) | 16.00 | 63.32 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSeek-7B | Greedy | 12.74 | 60.00 |'
  prefs: []
  type: TYPE_TB
- en: '| CP | 14.40 | 59.00 |'
  prefs: []
  type: TYPE_TB
- en: '| CD | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DoLA | 10.37 | 55.10 |'
  prefs: []
  type: TYPE_TB
- en: '| CP + CD | 15.47 | 62.40 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout] (Ours) | 15.47 | 62.40 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Quantization] (Ours) | 16.38 | 62.01 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout + Quantization] (Ours) | 16.38 | 62.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | Greedy | 42.23 | 69.04 |'
  prefs: []
  type: TYPE_TB
- en: '| CP | 38.90 | 67.73 |'
  prefs: []
  type: TYPE_TB
- en: '| CD | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DoLA | 43.60 | 70.74 |'
  prefs: []
  type: TYPE_TB
- en: '| CP + CD | 47.08 | 73.45 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout] (Ours) | 48.98 | 74.02 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Quantization] (Ours) | 47.20 | 72.71 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout + Quantization] (Ours) | 48.60 | 73.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama2-13B | Greedy | 29.42 | 65.20 |'
  prefs: []
  type: TYPE_TB
- en: '| CP | 25.78 | 66.10 |'
  prefs: []
  type: TYPE_TB
- en: '| CD | 32.83 | 69.90 |'
  prefs: []
  type: TYPE_TB
- en: '| DoLA | 28.81 | 68.47 |'
  prefs: []
  type: TYPE_TB
- en: '| CP + CD | 31.62 | 69.65 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout] (Ours) | 33.21 | 71.10 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Quantization] (Ours) | 31.30 | 70.60 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD[Dropout + Quantization] (Ours) | 32.20 | 70.90 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Reasoning scores comparison of Distillation Contrastive Decoding (DCD)
    with other existing methods: Contrastive Prompting (CP)Chia et al. ([2023](#bib.bib5)),
    Contrastive Decoding (CD) Li et al. ([2023b](#bib.bib14)), and DoLA Chuang et al.
    ([2023](#bib.bib6)). DCD outperforms the current baselines in improving the reasoning
    abilities of LLMs for both arithmetic and commonsense reasoning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0eb1dcf713106744cde6f75fe43c1d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Relationship between MMLU Score and Improvement on GSM8K. Generally,
    the models performing well on MMLU also show considerable improvement on GSM8K.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main results on Llama2, Mistral, and DeepSeek models are shown in Table
    [1](#S5.T1 "Table 1 ‣ 5.3 Models and Hyperparameters ‣ 5 Experimental Settings
    ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation"). We report the Llama1 results in Appendix [D](#A4
    "Appendix D Llama1 Results ‣ Distillation Contrastive Decoding: Improving LLMs
    Reasoning with Contrastive Decoding and Distillation") for reference. The results
    demonstrate that our proposed DCD method outperforms existing methods on both
    the GSM8K and StrategyQA datasets. On GSM8K, DCD outperforms CD by $1.89\%$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DCD with dropout consistency outperforms other distillation approaches like
    quantization and combined quantization with dropout. This finding contradicts
    previous findings that performance benefits from smaller amateur models Li et al.
    ([2023b](#bib.bib14)); O’Brien and Lewis ([2023](#bib.bib16)). We further study
    the effect of different quantization methods in Section [8](#S7.F8 "Figure 8 ‣
    7.2 Quantization Amateur Model ‣ 7 Distillation Methods ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, we observe that there is a correlation between the base knowledge
    of the model and DCD (Figure  [5](#S6.F5 "Figure 5 ‣ 6 Results ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation"))
    which does not apply to previous methods like CP. As the model achieves a higher
    MMLU score Hendrycks et al. ([2020](#bib.bib10)), DCD becomes more effective when
    employed. For example, there is a $+6.8\%$ on Llama1-7B in the arithmetic reasoning
    GSM8K task. This shows the adaptability of DCD to newer and stronger base models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also find that DCD usually leads to fewer generated tokens compared to CD
    and CP baselines in Figure [6](#S6.F6 "Figure 6 ‣ 6 Results ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").
    This supports the finding from Wei et al. ([2023](#bib.bib21)) that generating
    more CoT tokens can be subjected to error flaws in reasoning thus affecting the
    final results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b7f84de856a87d65f180595f9352aabb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison of average generate token of different methods on Llama2-7B
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Distillation Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore different distillation settings in Distillation
    Contrastive Decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Dropout Rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted experiments with varying dropout rates ranging from $0.1$ is optimal
    in most cases for both arithmetic and commonsense reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d472069cd81efd31514597cb821c094a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The performance of LLama2-7B across different dropout rates on both
    arithmetic and commonsense problems. Demonstrating the dropout peak instead of
    ascending. Notably, the arithmetic task imposes an amateur penalty of 0.3 with
    CoT instruction and the commonsense task imposes a penalty of 0.7 with CoT incoherent
    facts.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Quantization Amateur Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9bd2f41a302f0a65564bde1d00f02880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparision of different quantization methods applied to simulate
    amateur models on Llama2-7B with the arithmetic problem, demonstrating that smaller
    amateur models do not invariably enhance performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The premise that smaller-scale amateur models yield superior performance has
    been explored in CD Li et al. ([2023b](#bib.bib14)). In our study, we try to replicate
    this experiment while retaining the same model architecture by implementing different
    quantizations to simulate a smaller model with degraded capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe that simply reducing the bit size of the amateur model does not
    invariably enhance the decoding process. Figure [8](#S7.F8 "Figure 8 ‣ 7.2 Quantization
    Amateur Model ‣ 7 Distillation Methods ‣ Distillation Contrastive Decoding: Improving
    LLMs Reasoning with Contrastive Decoding and Distillation") shows that all of
    the tested quantization amateurs give a lower reasoning accuracy than the original
    amateur. These observations suggest that opting for smaller amateur models might
    not always yield the best performance. This insight underscores the motivation
    behind developing our Distillation Contrastive Prompting method to address the
    limitations posed by the need for an amateur model smaller than 7B in Contrastive
    Decoding Li et al. ([2023b](#bib.bib14)).'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we address the limitations associated with Contrastive Decoding,
    particularly its dependency on small amateur models within the same family as
    the expert models. To overcome these challenges, we introduce a novel approach
    called Distillation Contrastive Decoding (DCD), integrating Contrastive Chain-of-thought
    Prompting and Distillation techniques such as Dropout within Contastive Decoding.
    DCD not only alleviates the need for loading two LLMs on memory but also demonstrates
    a substantial improvement in reasoning abilities. Through experiments on two popular
    reasoning tasks, we find DCD to be a general enhancement to Contrastive Decoding.
    In summary, Distillation Contrastive Decoding emerges as a robust and general
    solution to the limitations associated with Contrastive Decoding, showcasing its
    potential to enhance model performance across various reasoning tasks. This research
    represents a significant stride forward in advancing the proficiency and logical
    reasoning prowess of LLMs, contributing to the ongoing efforts dedicated to enhancing
    the capabilities of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Limitation and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While our study has provided valuable insights into the effectiveness of Distillation
    Contrastive Decoding, it is crucial to acknowledge certain limitations that need
    to be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: First, our investigation mainly focuses on base models. Although we suggest
    that our method could potentially be applied to larger, tuned models, exploring
    its impact on instruction following represents a promising research direction.
    Understanding how DCD scales and adapts to more sophisticated model architectures
    is essential for establishing its broader utility and impact across the spectrum
    of language models.
  prefs: []
  type: TYPE_NORMAL
- en: Second, although our extensive experiments showcase the substantial improvements
    achieved by DCD across various settings, our exploration has not delved into more
    complex reasoning tasks. Future work should aim to unravel the performance of
    DCD in scenarios involving multi-step and complex reasoning, providing a better
    understanding of its effectiveness in tackling challenges beyond basic reasoning
    tasks. This expansion will contribute to a more comprehensive evaluation of the
    versatility and robustness of DCD in various reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. *arXiv preprint arXiv:2309.16609*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bricken et al. (2023) Trenton Bricken, Adly Templeton, Joshua Batson, Brian
    Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda
    Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell,
    Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
    Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.
    2023. Towards monosemanticity: Decomposing language models with dictionary learning.
    *Transformer Circuits Thread*. Https://transformer-circuits.pub/2023/monosemantic-features/index.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020a) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020a. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020b) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020b. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
    *CoRR*, abs/2005.14165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chia et al. (2023) Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria,
    and Lidong Bing. 2023. [Contrastive chain-of-thought prompting](http://arxiv.org/abs/2311.09277).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James
    Glass, and Pengcheng He. 2023. [Dola: Decoding by contrasting layers improves
    factuality in large language models](http://arxiv.org/abs/2309.03883).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepSeek-AI et al. (2024) DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen,
    Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe
    Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong
    Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei
    Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu,
    Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong
    Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui
    Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang
    Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui
    Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong,
    Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai
    Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua
    Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng
    Zhou, Qihao Zhu, and Yuheng Zou. 2024. [Deepseek llm: Scaling open-source language
    models with longtermism](http://arxiv.org/abs/2401.02954).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark
    with Implicit Reasoning Strategies. *Transactions of the Association for Computational
    Linguistics (TACL)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. [Measuring massive multitask
    language understanding](http://arxiv.org/abs/2009.03300). *CoRR*, abs/2009.03300.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2023. [Large language models are zero-shot reasoners](http://arxiv.org/abs/2205.11916).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister,
    and Martin Wattenberg. 2023a. [Inference-time intervention: Eliciting truthful
    answers from a language model](http://arxiv.org/abs/2306.03341).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason
    Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023b. [Contrastive
    decoding: Open-ended text generation as optimization](http://arxiv.org/abs/2210.15097).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2023) Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee,
    Roland Memisevic, and Hao Su. 2023. [Deductive verification of chain-of-thought
    reasoning](http://arxiv.org/abs/2306.03872).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O’Brien and Lewis (2023) Sean O’Brien and Mike Lewis. 2023. [Contrastive decoding
    improves reasoning in large language models](http://arxiv.org/abs/2309.09117).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A.
    Smith, and Mike Lewis. 2023. [Measuring and narrowing the compositionality gap
    in language models](http://arxiv.org/abs/2210.03350).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023a. [Llama: Open and efficient foundation language models](http://arxiv.org/abs/2302.13971).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b.
    [Llama 2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu,
    Luke Zettlemoyer, and Huan Sun. 2023. [Towards understanding chain-of-thought
    prompting: An empirical study of what matters](https://doi.org/10.18653/v1/2023.acl-long.153).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 2717–2739, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](http://arxiv.org/abs/2201.11903).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2023) Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip
    Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
    Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven
    Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.
    2023. [Representation engineering: A top-down approach to ai transparency](http://arxiv.org/abs/2310.01405).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5bae01c0ca7663a00ba3e005f4d43e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An example of arithmetic reasonings completions across 3 methods:
    CP, CD, and DCD (Ours).'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Components of a Chain-of-thought Demonstration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Wang et al. ([2023](#bib.bib20)) indicates that there are two main components
    of a CoT example:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bridging Objects: Essential elements required for successful predictions. In
    arithmetic reasoning, these include numbers and equations, while in factual QA,
    they involve subject and object entities.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language Templates: Textual hints and relational predicates that complement
    bridging objects, guiding the model in the reasoning process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6fae69cb1f4f71df73e3353b079b818.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Example of bridging objects and language templates components of
    a CoT demonstration. The examples are from Wang et al. ([2023](#bib.bib20)); Cobbe
    et al. ([2021](#bib.bib7)); Press et al. ([2023](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix B Appendix: Full Prompts for Experts Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 GSM8K
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg id="A2.SS1.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE0]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: B.2 StrategyQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg id="A2.SS2.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE1]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix C Appendix: Full Prompts for Amateurs Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 GSM8K
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.1.1 Rule-based Number Shuffle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <svg id="A3.SS1.SSS1.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE2]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: C.1.2 Rule-based Number Shuffle with Calculation Error
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <svg id="A3.SS1.SSS2.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE3]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: C.1.3 Rule-based Number Shuffle with Irrelerive objects and Exchange Sign
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <svg id="A3.SS1.SSS3.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE4]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: C.1.4 Synthetic Demonstration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <svg id="A3.SS1.SSS4.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE5]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: C.2 StrategyQA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.2.1 Synthetic Demonstration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <svg id="A3.SS2.SSS1.p2.pic1" class="ltx_picture" height="16.6" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,16.6) matrix(1 0 0 -1 0 0)
    translate(0,15.22)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 15.22 -15.22)"><foreignobject width="0" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[PRE6]</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Llama1 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Model | Method | GSM8K | StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama1-7B | - | 11.37 | 58.82 |'
  prefs: []
  type: TYPE_TB
- en: '| CP | 9.48 | 58.60 |'
  prefs: []
  type: TYPE_TB
- en: '| CD | 11.45 | 61.79 |'
  prefs: []
  type: TYPE_TB
- en: '| DoLA | 10.5 | 64.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD (Ours) | 12.1 | 63.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama1-13B | - | 17.13 | 65.46 |'
  prefs: []
  type: TYPE_TB
- en: '| CP | 17.66 | 61.62 |'
  prefs: []
  type: TYPE_TB
- en: '| CD | 19.79 | 62.67 |'
  prefs: []
  type: TYPE_TB
- en: '| DoLA | 18.0 | 67.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD (Ours) | 20.02 | 65.81 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Reasoning scores comparison of Distillation Contrastive Decoding (DCD)
    with other existing methods: Contrastive Prompting (CP)Chia et al. ([2023](#bib.bib5)),
    Contrastive Decoding (CD) Li et al. ([2023b](#bib.bib14)), and DoLA Chuang et al.
    ([2023](#bib.bib6)) on Llama1-7B and Llama1-13B models.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Exploring the Impact of Dropout Rates on Model Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5aad611ca3b94db4234b4dbf30d8c423.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Comparison of reasoning scores on the GSM8K and StrategyQA datasets
    utilizing Distillation Contrastive Decoding (DCD) with Llama2-7B. Our findings
    indicate that optimal accuracy is achieved by initially determining the appropriate
    value for $\beta$ (the dropout rate).'
  prefs: []
  type: TYPE_NORMAL
