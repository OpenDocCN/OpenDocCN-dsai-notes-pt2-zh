- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:59'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.05406](https://ar5iv.labs.arxiv.org/html/2402.05406)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lucio Dery    Steven Kolawole    Jean-François Kagy    Virginia Smith    Graham
    Neubig    Ameet Talwalkar
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given the generational gap in available hardware between lay practitioners and
    the most endowed institutions, LLMs are becoming increasingly inaccessible as
    they grow in size. Whilst many approaches have been proposed to compress LLMs
    to make their resource consumption manageable, these methods themselves tend to
    be resource intensive, putting them out of the reach of the very user groups they
    target. In this work, we explore the problem of structured pruning of LLMs using
    only forward passes. We seek to empower practitioners to prune models so large
    that their available hardware has just enough memory to run inference. We develop
    Bonsai, a gradient-free, perturbative pruning method capable of delivering small,
    fast, and accurate pruned models. We observe that Bonsai outputs pruned models
    that (i) outperform those generated by more expensive gradient-based structured
    pruning methods, and (ii) are twice as fast (with comparable accuracy) as those
    generated by semi-structured pruning methods requiring comparable resources as
    Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000
    that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open
    LLM leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: '[Code for Bonsai can be found here](https://github.com/ldery/Bonsai)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML\addauthor
  prefs: []
  type: TYPE_NORMAL
- en: gnmagenta \addauthoratblue
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As large language models (LLMs) (OpenAI et al., [2023](#bib.bib38); Touvron
    et al., [2023](#bib.bib45); Gemini Team et al., [2023](#bib.bib13)) continue to
    grow in size, the gap between models that achieve state-of-the-art performance
    and those that every-day machine learning (ML) practitioners can feasibly run
    on their available hardware continues to widen (Bender et al., [2021](#bib.bib5);
    Samsi et al., [2023](#bib.bib40)). With the goal of democratizing access to these
    powerful models, previous research has proposed approaches such as pruning (Xia
    et al., [2022](#bib.bib48); Sun et al., [2023](#bib.bib44)), distillation (Hinton
    et al., [2015](#bib.bib18); Gu et al., [2023](#bib.bib14)) and quantization (Xiao
    et al., [2023](#bib.bib49)) to create smaller models from larger pre-trained language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cb706285e7f938a089f99d3ee179841.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Perplexity versus inference speed-up of pruned models for methods
    that use only forward passes on the parent model. At any given target perplexity,
    Bonsai produces the fastest model, resulting in improved latency and throughput.
    Post-pruning adaptation is only possible for models below a certain size, given
    the available hardware. Circle sizes are proportional to the model’s memory footprint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Landscape of resource consumption (memory and compute) of different
    model compression methods at training time and the inference time resource consumption
    of the models they deliver. ✗  means the method incurs a prohibitive cost to the
    lay practitioner whilst ✓  denotes that it is a viable option with respect to
    that resource.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Regime | Resource | Approaches |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Quantization (Mixed Precision) | Distillation | Unstructured Pruning
    | Gradient-Based Structured Pruning | Bonsai (Ours) |'
  prefs: []
  type: TYPE_TB
- en: '| Train | Memory | ✓ | ✓ | ✓ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | ✓ | ✗ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Inference | Memory | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | ✗ | ✓ | ✗ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Unfortunately, so far, these methods have fallen short of the goal of truly
    democratizing access to LLMs. In the process of producing a smaller model out
    of an LLM, methods such as distillation and structured pruning are inaccessible
    to the everyday practitioner due to their prohibitive resource consumption at
    training time. Specifically, pure distillation-based techniques require running
    LLMs to generate large amounts of teacher data (Jiao et al., [2019](#bib.bib22);
    Hsieh et al., [2023](#bib.bib19)) whilst existing structured pruning approaches
    like LLM-Pruner (Ma et al., [2023](#bib.bib31)) and LoRAPrune (Zhang et al., [2023](#bib.bib52))
    require several times more memory than is needed to run inference on the model
    being pruned. Though unstructured pruning (Frantar & Alistarh, [2023](#bib.bib11);
    Sun et al., [2023](#bib.bib44)) and quantization are less restrictive at training
    time, the models they produce are not faster except in the presence of specialized
    hardware for the former (Mishra et al., [2021](#bib.bib36)), whilst the latter
    can actually slow down inference due to added overhead (Dettmers et al., [2022](#bib.bib9)).
    This limits the usefulness of these options for practitioners who are concerned
    with latency-critical applications. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") summarizes
    the landscape of existing methods and their limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We aim to empower ML practitioners to compress LLMs *by themselves* using their
    available resources whilst still producing accurate yet fast and compact models.
    To this end, we propose a novel memory-friendly structured pruning method. We
    observe that the significant memory overhead of prior structured pruning methods
    chiefly comes from having to perform gradient-based optimization: a backward pass
    requires $\gtrapprox 2\times$. To capture the widest range of memory budgets available
    to practitioners, we focus on developing an approach for the following concrete
    setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The practitioner only has enough memory on their hardware to run inference
    on the model to be pruned.¹¹1Specifically we assume a forward pass with a batch
    size of at least 1\. Given the hardware and open source LLM landscape as of January
    2024, for an Nvidia A6000 (48GB) GPU, the largest (full-precision) model this
    would be applicable to is LLaMA-7B*'
  prefs: []
  type: TYPE_NORMAL
- en: The above setting is evergreen. As state-of-the-art models become more compute
    intensive over time, the generational gap in hardware available to the lay practitioner
    versus the most resource endowed institutions is expected to persist or possibly
    widen.
  prefs: []
  type: TYPE_NORMAL
- en: In light of the proposed setting, we present Bonsai, a forward pass-only structured
    pruning approach that is capable of delivering fast, compact, and accurate pruned
    models under the memory limitations that are typical of consumer hardware. To
    decide which modules (attention head, rows in feedforward projection, etc.) of
    the LLM to prune, Bonsai estimates module importances perturbatively by generating
    sub-models and evaluating their performance (running inference). We make this
    approach tractable by contributing multiple techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we treat the problem of inferring each module’s importance from the
    performance of generated sub-models as an under-determined regression problem.
    This enables us to estimate the importance of a large number of modules by exploring
    a manageable number of random sub-models. This is unlike past perturbative approaches,
    which prohibitively require roughly as many sub-models as there are modules to
    select from (Ancona et al., [2020](#bib.bib1)), making them intractable for LLMs.
    Next, instead of instantiating sub-models by dropping modules with equal likelihood
    (Kang et al., [2023](#bib.bib23)), we use informative priors derived from work
    on unstructured pruning (Han et al., [2015](#bib.bib15); Sun et al., [2023](#bib.bib44)).
    We thus obtain better estimates of module relevance with fewer evaluated sub-models.
    Finally, unlike past gradient-free approaches that greedily make pruning decisions
    layer-by-layer (Dekhovich et al., [2021](#bib.bib8); Nova et al., [2023](#bib.bib37);
    Sun et al., [2023](#bib.bib44)), Bonsai takes a holistic view to preserve the
    accuracy of the pruned model: modules across layers are removed and evaluated
    together and relevance scores are computed globally to make pruning decisions.
    To the best of our knowledge, these ingredients taken together make Bonsai the
    first successful attempt at scaling gradient-free structured pruning to LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct several experiments that demonstrate Bonsai’s efficacy. Bonsai, which
    uses only forward passes for pruning, achieves comparable performance to 2:4 semi-structured
    sparsity with Wanda (Sun et al., [2023](#bib.bib44)) ($0.75\times$1.8B model that
    outperforms the best sub-2B parameter model on the Huggingface Open LLM leaderboard
    on 4 out of 6 tasks as of the time of submission. Based on these strong results
    and its usability under real-world memory constraints, we view Bonsai as a significant
    contribution to unlocking the power of LLMs for a broader spectrum of practitioners
    facing diverse hardware constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background on Pruning, Problem Definition and Notation Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are given an LLM, $\mathbf{M}_{\theta}$ on available hardware, pruning can
    be critical for achieving latency targets, reducing compute burden, or making
    the model small enough to adapt to new (out-of-domain) tasks by gradient-based
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured pruning approaches compress $\mathbf{M}_{\theta}$ from the model.
    This results in the updated model consisting of sparsified weight matrices with
    a smaller memory footprint. Unfortunately, the updated model does not enjoy inference
    speedups except when specialized hardware is available and thus poses a compute
    burden during inference. Whilst semi-structured variants – those that remove parameters
    in patterns like 2:4 or 4:8 (Mishra et al., [2021](#bib.bib36)) – achieve some
    speedup, these are modest compared to those achieved with structured pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structured pruning takes a more modular view of the units to be removed from
    $\mathbf{M}_{\theta}$, structured pruning can be cast as the following combinatorial
    optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathbf{m}^{\ast}&amp;=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad
    U\left(\mathbf{M}_{&#124;\bar{\mathbf{m}}}\right)\qquad\text{where}\\ \mathcal{F}_{p}&amp;=\bigg{\{}\bar{\mathbf{m}}\subseteq\mathbf{m}~{}\bigg{&#124;}~{}\bigg{(}\sum_{[j:m_{j}\in\bar{\mathbf{m}}]}s_{j}\bigg{)}\leq(1-p)D\bigg{\}}\end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: $\mathcal{F}_{p}$, it is also faster to run inference on it since it has fewer
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many structured pruning methods attempt to solve Equation [1](#S2.E1 "Equation
    1 ‣ 2 Background on Pruning, Problem Definition and Notation Setup ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") by gradient-guided
    optimization (or search) over the space of sub-models. However, since we are interested
    in the memory-constrained setting where computing gradients is not feasible, these
    methods cannot be used. We will thus focus on developing a memory-friendly structured
    pruning technique, but we will compare to semi-structured pruning methods since
    they also have minimal memory overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will develop a structured pruning algorithm that relies
    exclusively on forward passes through the parent model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Estimating module relevance with only forward passes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have motivated the setting of pruning a model that is so large (relative
    to the amount of memory available to the practitioner), such that we can only
    run forward passes through it. This means that we have to solve Equation [1](#S2.E1
    "Equation 1 ‣ 2 Background on Pruning, Problem Definition and Notation Setup ‣
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") by
    relying on only evaluations of $U$ subsets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose a computationally tractable approach where we first perform a small
    number, $n$ modules. We can generate an approximate solution to Equation [1](#S2.E1
    "Equation 1 ‣ 2 Background on Pruning, Problem Definition and Notation Setup ‣
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{m}^{\ast}\approx\mathbf{m}^{\mathrm{approx}}=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad\sum_{j\in\bar{\mathbf{m}}}\beta_{j}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'We note that Equation [2](#S3.E2 "Equation 2 ‣ 3.1 Estimating module relevance
    with only forward passes ‣ 3 Methodology ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") is straightforward to solve, as it simply requires
    sorting $\beta_{j}$ for our settings of interest, the difference is not significant).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Estimating $\mathbf{\beta}$:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To obtain estimates of the module relevance scores $\mathbf{\beta}=\{\beta_{i}\}_{i\in[N]}$
    as an under-specified regression problem :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{\bar{\mathbf{m}}_{k}}\in\mathbb{R}^{N}~{}\big{|}~{}\left(\alpha_{\bar{\mathbf{m}}_{k}}\right)_{i}=\mathbf{1}[i\in\bar{\mathbf{m}}_{k}]$,
    is the binary vector that has 0 at indices where modules have been dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a sub-model $\bar{\mathbf{m}}_{k}$ is key to practically realizing
    our approach. We never actually fully instantiate sub-models as this would be
    prohibitively expensive. Instead, we create sub-models *virtually* by zeroing
    out the outputs of the components to be pruned so they have no effect on the final
    model output.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Selecting sub-models for evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An as yet unexplored design choice is how to choose the $n$ are accurate and
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a module $m_{i}$ is left unpruned would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\rho_{i}\propto\hat{\mathbf{a}}_{i}=\frac{1}{B}\sum_{b}\bigg{&#124;}\sigma\bigg{(}\big{(}W^{T}[i,:]\big{)}x_{b}\bigg{)}\bigg{&#124;}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: $\sigma\text{ is the nonlinearity}$ which allows us to respect memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the efficiency of our method, instead of considering all active modules
    for pruning, in each layer we consider the bottom $2p$ fraction of entries). Covert
    & Lee ([2020](#bib.bib7)) show that this technique can help reduce the variance
    of the estimator obtained from regression with binary inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Bonsai
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Input: Model [$\mathbf{M}_{\theta}$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Iterated Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous work on gradient-based pruning (Anwar et al., [2017](#bib.bib2); Frankle
    & Carbin, [2018](#bib.bib10)) have shown that taking an iterated approach to pruning
    yields improved results over pruning directly to the target sparsity $p$ sub-models
    to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We combine the recipes developed in Sections [3.1](#S3.SS1 "3.1 Estimating
    module relevance with only forward passes ‣ 3 Methodology ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes"), [3.2](#S3.SS2 "3.2 Selecting
    sub-models for evaluation ‣ 3 Methodology ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") and [3.3](#S3.SS3 "3.3 Iterated Pruning ‣ 3
    Methodology ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") together to produce Bonsai⁴⁴4Structural pruning is a canonical way of
    giving a bonsai tree its shape hence the name., our gradient-free structural pruning
    algorithm. Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 Selecting sub-models for evaluation
    ‣ 3 Methodology ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") specifies Bonsai in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Post-pruning adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on the sparsity level $p$, it is possible to obtain a pruned model
    on which it is feasible to run a parameter-efficient finetuning method like LoRA
    (Hu et al., [2021](#bib.bib20)) with the available hardware memory. In this case,
    we can fine-tune the pruned model (result of Algorithm [1](#alg1 "Algorithm 1
    ‣ 3.2 Selecting sub-models for evaluation ‣ 3 Methodology ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes")) on the downstream task
    in order to recover some of the parent model performance that was degraded by
    pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like many past works (Sanh et al., [2020](#bib.bib42); Xia et al., [2022](#bib.bib48)),
    we combine pruning with distillation by incorporating a distillation loss in the
    training objective during fine-tuning of the pruned model. Let $\mathcal{L}_{\mathrm{task}}$
    to index the task data, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: $\mathcal{L}_{\mathrm{distill}}=\sum_{i}D_{\mathrm{KL}}\bigg{(}\mathrm{logits}^{i}\left(\mathbf{M}_{|\mathbf{m}^{\mathrm{approx}}}\right)~{}\|~{}\mathrm{logits}^{i}\left({\mathbf{M}}\right)\bigg{)}$
    instead of hosting the model in memory during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Wikitext-2 perplexity for 50% sparsity of LLaMA-2 7B with end-to-end
    latency speedups (relative to LLaMA-2 7B). We use Phi-2 (Li et al., [2023](#bib.bib28))
    at the target size as a strong model for comparison. For semi-structured (Wanda
    2:4) pruning, after fine-tuning, the learned LoRA weights cannot be merged with
    the primary model weights else the model reverts back to being dense; this leads
    to the reported slow downs. All methods use forward passes only on the parent
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | $\sim$Size | Fine-tune | PPL | Speedup |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2 | 3B | ✓ | $8.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2 7B Pruned |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda 2:4 | 3B | ✗ | $10.52$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ✓ | $8.34$ |'
  prefs: []
  type: TYPE_TB
- en: '| Bonsai | 3B | ✓ | $8.89$ |'
  prefs: []
  type: TYPE_TB
- en: 5 Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All experiments are conducted on a single NVIDIA A6000 (48Gb) GPU. In all experiments
    with Bonsai, we prune (1) the heads in the self-attention layers (2) the dimensions
    of the fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing Forward Pass Only Methods: We focus our first set of experiments
    on comparing methods that can be run without gradient-based optimization. We consider
    pruning the LLaMA-2 7B model (Touvron et al., [2023](#bib.bib45)) to 50% sparsity.
    We evaluate on the Wikitext-2 (Merity et al., [2016](#bib.bib34)) validation dataset
    and so our signal for pruning, $U$, is the language modelling performance on the
    training set. When measuring speedups, we consider *end-to-end latency* of running
    inference on model.sequence_length chunks of the Wikitext-2 validation set. See
    Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment
    Details ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes")
    for details about the hyper-parameters used for all methods in this experiment
    and for specifics about fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing Structured Pruning Approaches: We compare Bonsai to the following
    gradient-based structured pruning approaches: LLM-Pruner (Ma et al., [2023](#bib.bib31))
    and LoRA-Prune (Zhang et al., [2023](#bib.bib52)). We prune the LLaMA-1 7B model
    (Touvron et al., [2023](#bib.bib45)) to 50% sparsity since this was the model
    version available at time of release of the above methods. We compare these methods
    on Wikitext-2 and also on 6 tasks from the Eleuther LLM Evaluation Harness (Gao
    et al., [2023](#bib.bib12)). Pruning signal for Wikitext-2 task is the same as
    the above experiment. For the Eleuther Harness tasks, we use language modelling
    performance on the C4 (Raffel et al., [2020](#bib.bib39)) dataset as pruning signal.
    We also do parameter efficient finetuning on our pruned model with 30K 512-length
    sequences from this corpus. Specific hyper-parameters for Bonsai for this experiment
    can be found in Appendix [A.3](#A1.SS3 "A.3 Experiments comparing to Gradient
    based structured pruning ‣ Appendix A Main Experiment Details ‣ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Main Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Bonsai produces fast and performant models with only forward passes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [2](#S4.T2 "Table 2 ‣ 4 Post-pruning adaptation ‣ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes"), we explore options
    that are available to practitioners when they can only run forward passes on the
    parent model. Here, LLaMA-2 7B is compressed to 50% sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: We compare Bonsai to the semi-structured variant of Wanda (Sun et al., [2023](#bib.bib44)).
    Before fine-tuning, the model produced by Wanda 2:4 achieves a speedup over the
    parent model (1.14$\times$) than the model from Bonsai.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this memory-constrained setting, practitioners could alternatively opt for
    a pre-existing model of the target size instead of pruning a larger model. We
    compare the Bonsai-pruned model to Phi-2 (Li et al., [2023](#bib.bib28)), a strong
    representative pre-existing model of similar size. As can be seen in Table [2](#S4.T2
    "Table 2 ‣ 4 Post-pruning adaptation ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes"), Bonsai is able to generate a model that is
    as accurate (0.2 difference in ppl) yet significantly faster (1.58$\times$ speedup),
    thus making it a competitive option to consider even if a model already exists
    at the target size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: LLaMA-1 (50% sparsity) after post-pruning adaptation with LoRA. ^†
    indicate results as reported by Zhang et al. ([2023](#bib.bib52)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Foward-only | Wikitext-2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA1-7B (Touvron et al., [2023](#bib.bib45)) | - | 5.68 | 75.05 | 56.92
    | 69.93 | 75.34 | 41.89 | 63.83 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner^†  (Ma et al., [2023](#bib.bib31)) | ✗ | 16.41 | 60.28 | 47.06
    | 53.43 | 45.96 | 29.18 | 47.18 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRAPrune^†  (Zhang et al., [2023](#bib.bib52)) | ✗ | 11.60 | 61.88 | 47.86
    | 55.01 | 45.13 | 31.62 | 48.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Bonsai | ✓ | 10.92 | 67.22 | 43.09 | 61.64 | 54.92 | 26.28 | 50.63 |'
  prefs: []
  type: TYPE_TB
- en: 6.2 Bonsai outperforms other structured pruning approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We next investigate the setting where the practitioner has enough memory to
    run gradient-based structured pruning methods. We compare Bonsai to recent SoTA
    methods: LLM-Pruner (Ma et al., [2023](#bib.bib31)) and LoRA-Prune (Zhang et al.,
    [2023](#bib.bib52)). Since these approaches report their results for the LLaMA-1
    only, we prune LLaMA-1 7B to 50% sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As evinced by Table [3](#S6.T3 "Table 3 ‣ 6.1 Bonsai produces fast and performant
    models with only forward passes ‣ 6 Main Results ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"), Bonsai outperforms existing structured
    pruning methods for LLMs even though it exclusively uses forward passes in the
    pruning stage. We attribute the superior performance of Bonsai to the fact that
    its pruning decisions are informed by directly exploring the space of sub-models
    whilst the other approaches resort on inaccurate proxies of module relevance in
    order to reduce the memory overhead of a fully gradient-based optimization approach.
    Specifically, LoRA-Prune’s criterion for deciding which modules to prune uses
    gradient signals not from the modules directly but from a surrogate low-rank matrix.
    On the other hand, LLM-Pruner attempts to model the impact of removing a module
    on the model loss via a Taylor expansion and substitutes the LLM’s intractable
    (memory-wise) Hessian term with the Fisher information matrix which we posit leads
    to sub-optimal pruning decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Bonsai can produce compressed models with strong zero-shot abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Phi-2 pruned to 35% sparsity compared to the best sub-2B parameter
    models on the Hugging Face OpenLLM Leaderboard. At the time of submission, $\dagger$
    was the best sub-2B parameter model on the Leaderboard.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Generation | Multiple Choice (MC) |'
  prefs: []
  type: TYPE_TB
- en: '| Model | Size | GSM8k (5-shot) | ARC-c (25-shot) | Winogrande (5-shot) | Hellaswag
    (10-shot) | Truthful-QA (0-shot) | MMLU (5-shot) | MC Average $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2 (Li et al., [2023](#bib.bib28)) | 2.7B | 54.81 | 61.09 | 74.35 | 75.11
    | 44.47 | 58.11 | 62.63 |'
  prefs: []
  type: TYPE_TB
- en: '| [StableLM-2-1_6b](https://huggingface.co/stabilityai/stablelm-2-1_6b)$\dagger$
    | 36.78 | 38.95 | 50.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 1.8B | 6.37 | $\boldsymbol{47.44}$ |'
  prefs: []
  type: TYPE_TB
- en: Considerable amounts of compute and data, beyond what is available to lay practitioners,
    are needed to train LLMs with strong zero-shot capabilities (OpenAI et al., [2023](#bib.bib38);
    Gemini Team et al., [2023](#bib.bib13)). In this section, we demonstrate that
    Bonsai can empower everyday practitioners to produce strong and compact models
    with competitive zero-shot abilities by simply pruning bigger models on their
    available hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Bonsai to prune a $\approx$1.8B (35% sparsity). Values for the Bonsai hyper-parameters
    in this experiment are in Appendix [A.4](#A1.SS4 "A.4 Phi-2 pruning experiment
    details ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"). Since its relatively small, the 1.8B
    pruned model can be fully fine-tuned on 1 A6000 GPU over 100k sequences of 2,048
    tokens from the C4 dataset. As can be seen from Table [4](#S6.T4 "Table 4 ‣ 6.3
    Bonsai can produce compressed models with strong zero-shot abilities ‣ 6 Main
    Results ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes"),
    our pruned model achieves strong zero-shot performance compared to StableLM, the
    leading sub-2B parameter LLM on the Hugging Face OpenLLM leaderboard (Gao et al.,
    [2023](#bib.bib12)) — outperforming it on 4 out of 6 tasks as of January 2024\.
    Interestingly, one exception to the general trend of Bonsai’s superior performance
    is the GSM-8K dataset, which is a mathematical reasoning dataset that requires
    generation of a long reasoning chains. We posit that this is because currently,
    Bonsai prunes with respect to language modeling likelihood, as opposed to reasoning
    accuracy. An interesting avenue for future work is to prune to improve maintain
    reasoning ability.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we conduct various ablative experiments to understand how
    the methodological ingredients from Section [3](#S3 "3 Methodology ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") contribute to
    make Bonsai effective.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do we need both perturbative and regressive components of Bonsai? Figure [2](#S7.F2
    "Figure 2 ‣ 7 Analysis ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") shows that both components are key to obtaining a good pruned
    model. Removing the estimation of module importances via regression leads to a
    degradation in performance ($61.6$ as computed from the unperturbed parent model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ab05c979f32dafa13204333ec4b8613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: LLaMA-2 7B pruned to 50% sparsity. Perturbatively evaluating the
    impact of removing modules and then using regression to estimate module importances
    are both key in making Bonsai effective. Details of experiment configuration can
    be found in Appendix [B](#A2 "Appendix B Impact of regression and perturbation
    ablation details ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes"). No post-pruning adaptation is performed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Varying the number of perturbative evaluations. Wikitext-2 perplexity
    of LLaMA-2 7B pruned to 50% sparsity. See Appendix [G](#A7 "Appendix G How many
    perturbative samples are reasonable? ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") for details. No post-pruning adaptation is
    performed.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{ns}=50$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PPL ($\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: 'How many perturbative samples are sufficient? We investigate the number of
    perturbative samples required to obtain good estimates of module importances after
    performing regression as in Equation [3](#S3.E3 "Equation 3 ‣ Estimating 𝛽: ‣
    3.1 Estimating module relevance with only forward passes ‣ 3 Methodology ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"). Our results
    are shown in Table [5](#S7.T5 "Table 5 ‣ 7 Analysis ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"). As expected, performance improves
    as we increase the number of sub-models explored. We note that the number of samples
    we explore, $\mathrm{ns}$), nevertheless Bonsai is able to deliver a performant
    pruned model because of the recipes developed in Section [3](#S3 "3 Methodology
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'How much performance is recovered by post-pruning adaptation? During iterative
    pruning, Bonsai damages the parent model by removing modules but does not perform
    intermittent retraining to recover lost performance since even intermediate models
    may be too large for fine-tuning. Even so, as Table [6](#S7.T6 "Table 6 ‣ 7 Analysis
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") shows,
    the final model produced by Bonsai has reasonable performance without fine-tuning.
    We attribute this to the general robustness of LLMs, the redundancy of modules
    with respect to target end-tasks and Bonsai’s ability to identify good candidates
    for pruning. If the pruned model is small enough in size, we can perform either
    full fine-tuning or parameter-efficient fine-tunin to recover more performance,
    as can be seen from Table [6](#S7.T6 "Table 6 ‣ 7 Analysis ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Impact of post-pruning adaptation of LLaMA-2 7B pruned to 50% sparsity.
    See Appendix [E](#A5 "Appendix E Post-pruning adaptation ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Wikitext-2 PPL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| No Post-Pruning Adaptation | $19.47$ |'
  prefs: []
  type: TYPE_TB
- en: '| Post-Pruning Finetuning | $10.39$ |'
  prefs: []
  type: TYPE_TB
- en: '|    +  Distillation | $8.89$ |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/9c738ca6f41a278ee7be755755b9a999.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: LLaMA-2 7B pruned to 50% sparsity. See Appendix [F](#A6 "Appendix
    F Impact of prior ‣ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes") for details of experiment configuration and definitions of module-level
    analogues of Wanda and Activation Magnitude.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the impact of the choice of metric for the prior $\rho$. Figure [3](#S7.F3
    "Figure 3 ‣ 7 Analysis ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") shows that using the module-level analogue of Wanda (Sun
    et al., [2023](#bib.bib44)) yields the best performance, both before and after
    post-pruning adaptation. This indicates that Wanda is a strong signal for efficiently
    estimating the importance of model units (whether at the parameter- or module-level).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Should Bonsai prune iteratively? Table [7](#S7.T7 "Table 7 ‣ 7 Analysis ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") demonstrates
    the benefits of using Bonsai  in an iterative fashion. Pruning slowly ($p_{\mathrm{iter}}=0.05$
    persists even after post-pruning adaptation, indicating that slower pruning allows
    for more accurate estimates of module importance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Varying $p_{\mathrm{iter}}$. Wikitext-2 perplexity of LLaMA-2 7B pruned
    to 50% sparsity. See Appendix [C](#A3 "Appendix C Varying the pruning fraction
    per-iteration ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") for experiment details.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{\mathrm{iter}}=0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Adapt | $19.47$ |'
  prefs: []
  type: TYPE_TB
- en: '| w Adapt | $8.89$ |'
  prefs: []
  type: TYPE_TB
- en: 8 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 8.1 Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unstructured pruning:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whilst structured pruning approaches remove entire model components like layers
    (Xu et al., [2020](#bib.bib50); Xia et al., [2022](#bib.bib48)), dimensions of
    linear layers (Wang et al., [2019](#bib.bib47)) or attention heads (Michel et al.,
    [2019](#bib.bib35); Held & Yang, [2022](#bib.bib17)), unstructured pruning (Han
    et al., [2015](#bib.bib15); Frankle & Carbin, [2018](#bib.bib10); Benbaki et al.,
    [2023](#bib.bib4); Sun et al., [2023](#bib.bib44)) removes individual parameters
    of the model. These approaches achieve memory savings by inducing sparsity in
    the model weights, but they generally do not result in actual model speedups except
    when specialized hardware is available (Mishra et al., [2021](#bib.bib36)). Proposed
    semi-structured sparsity methods (Mishra et al., [2021](#bib.bib36)) such as 2:4
    and 4:8 patterns do result in faster inference, but the speedup gains they achieve
    are far from the idealized $2\times$. There are several gradient-free, unstructured
    pruning approaches. Since, as far as we know, there are no scalable gradient-free
    structured pruning alternatives outside of Bonsai, we compare it with Wanda (Sun
    et al., [2023](#bib.bib44)), a SoTA unstructured/semi-structured pruning technique
    for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structured pruning without backward passes:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To the best of our knowledge, all existing structured pruning techniques for
    large (over 1B scale) language models, like LoRAPrune (Zhang et al., [2023](#bib.bib52))
    and LLM-Pruner (Ma et al., [2023](#bib.bib31)), are gradient-based. Thus, under
    the memory setting we consider, these methods cannot be applied since their memory
    requirements well exceed the memory needed for inference on the model to be pruned.
    For smaller language models like BERT, Nova et al. ([2023](#bib.bib37)) recently
    proposed Kernelized Convex Masking (KCM) for gradient-free structured pruning.
    Unfortunately, to prune a fully connected layer with $K$ fraction within its layer
    will not be included in the pruned model — a clearly sub-optimal choice. Bonsai enables
    us to obtain globally meaningful estimates of module relevance that are comparable
    across layers.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Model compression beyond pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Distillation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Originally proposed by Hinton et al. ([2015](#bib.bib18)), Sanh et al. ([2019](#bib.bib41))
    have shown that it is possible to reach similar performances on many downstream
    tasks using much smaller language models pre-trained with knowledge distillation.
    The resulting models are lighter and faster at inference time. Unfortunately,
    as noted by Xia et al. ([2022](#bib.bib48)), distilling a pre-trained model to
    a student model that has been initialized from scratch requires lots of compute
    and data to recover reasonable levels of teacher performance. To reduce this resource
    burden, Sanh et al. ([2020](#bib.bib42)); Lagunas et al. ([2021](#bib.bib27));
    Xia et al. ([2022](#bib.bib48)) combine distillation with gradient-based structured
    pruning, so that the student model does not have to be initialized from scratch.
    Bonsai also leverages distillation, but only during the post-pruning adaptation
    phase. This maintains the constraint that the pruning process is gradient-free.
    And since the pruned model is smaller than the parent model, we can safely perform
    distillation and parameter efficient fine-tuning (Hu et al., [2021](#bib.bib20);
    He et al., [2021](#bib.bib16)) within the bounds of available memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Li et al. ([2020](#bib.bib29)) introduced a training strategy that showed that
    heavily compressed, large models achieved higher accuracy than lightly compressed,
    small models, providing both efficiency and high-accuracy results on NLP tasks
    at once. Dettmers et al. ([2022](#bib.bib9)) introduced a new method called ‘LLM.int8()‘,
    which allows loading large models with 16 or 32-bit weights and using them immediately
    for inference without any performance degradation by combining vector-wise quantization
    with mixed-precision decomposition. Xiao et al. ([2023](#bib.bib49)) enabled 8-bit
    weight, 8-bit activation (W8A8) quantization for LLMs with >100B parameters, which
    smoothed activation outliers by migrating the quantization difficulty from activations
    to weights with a mathematically equivalent transformation. Quantization tends
    to be complementary to pruning, allowing independent exploration and subsequent
    combinations of both techniques. Some quantization approaches, specifically mixed
    precision ones like Dettmers et al. ([2022](#bib.bib9)) (Appendix D), can produce
    resulting models that are *slower* than the parent model, though it has also been
    shown that it can result in more accurate models than pruning (Kuzmin et al.,
    [2023](#bib.bib26)). This paper focuses on gradient-free structured pruning. We
    leave combining Bonsai with quantization for future work.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion, Limitations, and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we have presented Bonsai, the first tractable gradient-free method
    for structured pruning of LLMs. Bonsai allows practitioners to prune any LLM as
    long as they have enough memory to run inferences on the model. Through a battery
    of experiments, we have shown that the models produced by Bonsai are small, fast,
    and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'A primary limitation of Bonsai is its runtime. As Section [7](#S7 "7 Analysis
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") shows,
    performance improves when we increase the number of sub-models explored, slow
    down pruning, and use more data samples; but this comes at the cost of increased
    runtime. Our result in Table [2](#S4.T2 "Table 2 ‣ 4 Post-pruning adaptation ‣
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") took
    $\sim 40$ hours to obtain, whilst Wanda-2:4 takes on the order of minutes. Since
    the model we produce is twice as fast, this trade-off is worthwhile when amortized
    over inference on the pruned models.'
  prefs: []
  type: TYPE_NORMAL
- en: Bonsai presents several avenues for expansion by future work. First, though
    sub-models are sampled from an informative prior, $\rho$, the sampling process
    is not adaptive. Bonsai could further be strengthened by dynamically exploring
    the space of sub-models. Next, because of memory constraints, Bonsai does not
    fine-tune the model during iterative pruning to recover degraded performance.
    However, forward-pass-only fine-tuning approaches like MeZO (Malladi et al., [2023](#bib.bib32))
    exist that can be used to dynamically update the model during pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Acknowlegements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by the National Science Foundation grants IIS1705121,
    IIS1838017, IIS2046613, IIS2112471, and funding from Meta, Morgan Stanley, Amazon,
    and Google. Any opinions, findings and conclusions or recommendations expressed
    in this material are those of the author(s) and do not necessarily reflect the
    views of any of these funding agencies. We are grateful for helpful feedback from
    Mingjie Sun, Victor Akinwande, Asher Trockman, Afshin Rostamizadeh and Daniel
    Glasner
  prefs: []
  type: TYPE_NORMAL
- en: 11 Broader Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network pruning is an effective compression strategy to reduce the inference
    costs of large language models. However, existing techniques for structured pruning
    impose expensive memory constraints that may render them out of reach in practice.
    By reducing such constraints, work aims to democratize pruning to provide practitioners
    the ability to produce their own pruned LLMs for real-world applications. While
    we ultimately view this democratization as a benefit, we note that a necessary
    outcome is that it may also make it more feasible for malicious actors to readily
    use LLMs in practice. Finally, we note that our work has primarily focused on
    the axes of efficiency and utility (e.g., perplexity) in assessing performance;
    recent work has shown that compression may also have outsized effects on issues
    such as model fairness and robustness, which would be interesting additional aspects
    to consider in future study.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ancona et al. (2020) Ancona, M., Öztireli, C. and Gross, M. Shapley value as
    principled metric for structured network pruning. *arXiv preprint arXiv:2006.01795*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anwar et al. (2017) Anwar, S., Hwang, K. and Sung, W. Structured pruning of
    deep convolutional neural networks. *ACM Journal on Emerging Technologies in Computing
    Systems (JETC)*, 13(3):1–18, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba et al. (2016) Ba, J.L., Kiros, J.R. and Hinton, G.E. Layer normalization.
    *arXiv preprint arXiv:1607.06450*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benbaki et al. (2023) Benbaki, R., Chen, W., Meng, X., Hazimeh, H., Ponomareva,
    N., Zhao, Z. and Mazumder, R. Fast as chita: Neural network pruning with combinatorial
    optimization. *arXiv preprint arXiv:2302.14623*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bender et al. (2021) Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell,
    S. On the dangers of stochastic parrots: Can language models be too big. In *Proceedings
    of the 2021 ACM conference on fairness, accountability, and transparency*, pp. 
    610–623, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridger (2023) Bridger, P. Pytorch memory tuning, Jul 2023. URL [https://paulbridger.com/posts/pytorch-memory-tuning/](https://paulbridger.com/posts/pytorch-memory-tuning/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Covert & Lee (2020) Covert, I. and Lee, S.I. Improving kernelshap: Practical
    shapley value estimation via linear regression. *arXiv preprint arXiv:2012.01536*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dekhovich et al. (2021) Dekhovich, A., Tax, D.M., Sluiter, M.H. and Bessa,
    M.A. Neural network relief: a pruning algorithm based on neural activity. *arXiv
    preprint arXiv:2109.10795*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y. and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. In *International Conference on Machine
    Learning*, pp.  10323–10337\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) Gao, L. et al. A framework for few-shot language model evaluation,
    12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemini Team et al. (2023) Gemini Team et al. Gemini: a family of highly capable
    multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2023) Gu, Y., Dong, L., Wei, F. and Huang, M. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015) Han, S., Mao, H. and Dally, W.J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2021) He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. and Neubig,
    G. Towards a unified view of parameter-efficient transfer learning. *arXiv preprint
    arXiv:2110.04366*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Held & Yang (2022) Held, W. and Yang, D. Shapley head pruning: Identifying
    and removing interference in multilingual transformers. *arXiv preprint arXiv:2210.05709*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Hinton, G.E., Vinyals, O. and Dean, J. Distilling the knowledge
    in a neural network. *ArXiv*, abs/1503.02531, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Hsieh, C.Y., Li, C.L., Yeh, C.K., Nakhost, H., Fujii, Y.,
    Ratner, A., Krishna, R., Lee, C.Y. and Pfister, T. Distilling step-by-step! outperforming
    larger language models with less training data and smaller model sizes. *arXiv
    preprint arXiv:2305.02301*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L. and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Jiang, A.Q. et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2019) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L.,
    Wang, F. and Liu, Q. Tinybert: Distilling bert for natural language understanding.
    *arXiv preprint arXiv:1909.10351*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2023) Kang, M., Li, L. and Li, B. Fashapley: Fast and approximated
    shapley based model pruning towards certifiably robust dnns. In *2023 IEEE Conference
    on Secure and Trustworthy Machine Learning (SaTML)*, pp.  575–592\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall (1948) Kendall, M.G. Rank correlation methods. 1948.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma & Ba (2014) Kingma, D.P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuzmin et al. (2023) Kuzmin, A., Nagel, M., Van Baalen, M., Behboodi, A. and
    Blankevoort, T. Pruning vs quantization: Which is better? *arXiv preprint arXiv:2307.02973*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lagunas et al. (2021) Lagunas, F., Charlaix, E., Sanh, V. and Rush, A.M. Block
    pruning for faster transformers. *arXiv preprint arXiv:2109.04838*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar,
    S. and Lee, Y.T. Textbooks are all you need ii: phi-1.5 technical report. *arXiv
    preprint arXiv:2309.05463*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein,
    D. and Gonzalez, J. Train big, then compress: Rethinking model size for efficient
    training and inference of transformers. In *International Conference on machine
    learning*, pp.  5958–5968\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled weight decay
    regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Ma, X., Fang, G. and Wang, X. Llm-pruner: On the structural
    pruning of large language models. *arXiv preprint arXiv:2305.11627*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malladi et al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D.,
    Chen, D. and Arora, S. Fine-tuning language models with just forward passes. *arXiv
    preprint arXiv:2305.17333*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McGrath et al. (2023) McGrath, T., Rahtz, M., Kramar, J., Mikulik, V. and Legg,
    S. The hydra effect: Emergent self-repair in language model computations. *arXiv
    preprint arXiv:2307.15771*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J. and Socher, R. Pointer
    sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michel et al. (2019) Michel, P., Levy, O. and Neubig, G. Are sixteen heads really
    better than one? *Advances in neural information processing systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra et al. (2021) Mishra, A., Latorre, J.A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C. and Micikevicius, P. Accelerating sparse deep neural
    networks. *arXiv preprint arXiv:2104.08378*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nova et al. (2023) Nova, A., Dai, H. and Schuurmans, D. Gradient-free structured
    pruning with unlabeled data, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2023) OpenAI et al. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W. and Liu, P.J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samsi et al. (2023) Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A.,
    Jones, M., Bergeron, W., Kepner, J., Tiwari, D. and Gadepally, V. From words to
    watts: Benchmarking the energy costs of large language model inference. In *2023
    IEEE High Performance Extreme Computing Conference (HPEC)*, pp.  1–9\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J. and Wolf, T. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. *ArXiv*, abs/1910.01108,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2020) Sanh, V., Wolf, T. and Rush, A. Movement pruning: Adaptive
    sparsity by fine-tuning. *Advances in Neural Information Processing Systems*,
    33:20378–20389, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan & Zisserman (2014) Simonyan, K. and Zisserman, A. Very deep convolutional
    networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A. and Kolter, J.Z. A simple and effective
    pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H. et al. Llama 2: Open foundation and fine-tuned
    chat models. *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vivek (2023) Vivek, S. The economics of large language models, Sep 2023. URL
    [https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c](https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang, Z., Wohlwend, J. and Lei, T. Structured pruning of
    large language models. *arXiv preprint arXiv:1910.04732*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2022) Xia, M., Zhong, Z. and Chen, D. Structured pruning learns
    compact and accurate models. *arXiv preprint arXiv:2204.00408*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. and Han,
    S. Smoothquant: Accurate and efficient post-training quantization for large language
    models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020) Xu, C., Zhou, W., Ge, T., Wei, F. and Zhou, M. Bert-of-theseus:
    Compressing bert by progressive module replacing. *arXiv preprint arXiv:2002.02925*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zagoruyko & Komodakis (2016) Zagoruyko, S. and Komodakis, N. Wide residual networks.
    *arXiv preprint arXiv:1605.07146*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.
    et al. Pruning meets low-rank parameter-efficient fine-tuning. *arXiv preprint
    arXiv:2305.18403*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Main Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Hyper-parameters for all Bonsai regression during pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using Bonsai, we estimate $\beta$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Bonsai hyper-parameters for regression. This applies to all experiments
    unless otherwise specified'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\gamma$(Regression Weight) | Learning rate | Batch Size | Epochs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| {100, 0, 1e-4} | {100, 10, 1, 0.1} | {32, 64, 128} | 50 |'
  prefs: []
  type: TYPE_TB
- en: During cross validation, we choose the model whose predictions have the best
    Kendall rank correlation co-efficient (Kendall, [1948](#bib.bib24)) with the target.
    We do this because we do not care about matching $U_{k}$ reasonably models relative
    module importances.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we use $\ell_{1}$-norm works better.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Forward Pass Only Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [10](#A1.T10 "Table 10 ‣ A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") show the Bonsai hyperparameters we used for the experiments
    in Section [6.1](#S6.SS1 "6.1 Bonsai produces fast and performant models with
    only forward passes ‣ 6 Main Results ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Bonsai hyper-params for forward only experiments'
  prefs: []
  type: TYPE_NORMAL
- en: '| $p_{\mathrm{iter}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | 200 | 32 (per-iter) | Wanda |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Bonsai  fine-tuning HP for pruned LLaMA family models'
  prefs: []
  type: TYPE_NORMAL
- en: '| LR | rank | LoRA-$\alpha$ (Distill Weight) | LoRA Modules |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1e-4 | 128 | 4$\times$rank | 0.01 | All Modules |'
  prefs: []
  type: TYPE_TB
- en: 'For Wanda(Sun et al., [2023](#bib.bib44)), we use the default hyper-parameters
    specified by [the paper repo here](https://github.com/locuslab/wanda/tree/main)
    for pruning. For fine-tuning, we use rank = 64\. We apply LoRA to only the q_proj
    and v_proj matrices in each layer of the pruned LLaMA model – this is unlike with
    Bonsai where we fine-tune all modules. We cannot do same because since the Wanda
    model just produces sparse matrices, the matrices instantiated during the backward
    pass are the same sizes as the sparsified matrices and thus occupy more memory
    (compared to our approach that actually makes the matrices smaller in dimension
    instead of sparsifying). We are also unable to perform distillation on the Wanda
    models due to this reason. For fine-tuning the Phi-2 model on Wikitext-2, we use
    the same hyper-parameters as Bonsai in Table [10](#A1.T10 "Table 10 ‣ A.2 Forward
    Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Experiments comparing to Gradient based structured pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compare to LoRA-Prune and LLM-Pruner. We take their performance results directly
    from the LoRA-Prune paper. Whilst we use 1 A6000 GPU (48G) for all experiments,
    LoRA-Prune uses A100 GPU (80G) for pruning LLaMA-1 7B.
  prefs: []
  type: TYPE_NORMAL
- en: 'All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward
    Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") except for $\mathrm{ns}_{\mathrm{sub-models}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Phi-2 pruning experiment details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the experiment in Section [6.3](#S6.SS3 "6.3 Bonsai can produce compressed
    models with strong zero-shot abilities ‣ 6 Main Results ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes"), All Bonsai hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except for the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathrm{ns}_{\mathrm{sub-models}}=2000$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $p_{\mathrm{iter}}=0.35$. We thus perform 1-shot pruning directly to the target
    sparsity of 35%. We find that this seems to work best for the Phi-2 model. We
    posit that this might be because the Phi-2 models use LayerNorm(Ba et al., [2016](#bib.bib3))
    whilst the other models we explore, LLaMA and Mistral use RMSNorm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to its relatively small size, the 1.8B pruned model can be fully fine-tuned
    on a single A6000 GPU over 100k sequences of length 2,048 tokens from the C4 dataset
    instead of using LoRA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix B Impact of regression and perturbation ablation details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the experiment in Section [3](#S7.F3 "Figure 3 ‣ 7 Analysis ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"), All Bonsai hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except $p_{\mathrm{iter}}=0.1$ to speed up pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple alternative to Bonsai is to leverage the prior $\rho$ for this experiment.
    Module level analogues of the unstructured pruning metrics we explore are defined
    in Appendix [F](#A6 "Appendix F Impact of prior ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: Experiment on linear regression to estimate module importances. Wikitext-2
    Perplexity. LLaMA-2 7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Linear Regression | Relative Speepdup | w/o Post-Pruning Adaptation | w Post-Pruning
    Adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| No | $2.06$ |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | $1.77$ |'
  prefs: []
  type: TYPE_TB
- en: Appendix C Varying the pruning fraction per-iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the experiment in Section [3](#S7.F3 "Figure 3 ‣ 7 Analysis ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"), All Bonsai hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except we vary $p_{\mathrm{iter}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: Varying the fraction pruned at a time. Wikitext-2 Perplexity. LLaMA-2
    7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prune Frac | Relative Speepdup | w/o Post-Pruning Adaptation | w Post-Pruning
    Adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| 0.05 | $1.58$ |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | $1.77$ |'
  prefs: []
  type: TYPE_TB
- en: '| 0.20 | $1.67$ |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Varying the number of calibration data points for pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward
    Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") except we vary $\mathrm{ns}_{\mathrm{data}}$
    to speed up pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: How many data-points to consider during forward passes. Wikitext-2
    Perplexity. Llama-2 7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\mathrm{ns}_{\mathrm{data}}$ | w/o Adapt | w Adapt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | $130.04$ |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | $61.63$ |'
  prefs: []
  type: TYPE_TB
- en: Appendix E Post-pruning adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this experiment, All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Impact of prior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this experiment, All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") except we vary
    $\rho$'
  prefs: []
  type: TYPE_NORMAL
- en: F.1 $\rho$ is Activation Magnitude
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MLP / Fully Connected Module: Let $d$ and then compute the following averaged
    activation magnitude :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\rho\in\mathbb{R}^{d}\right)\propto\hat{\mathbf{a}}=\frac{1}{B}\sum_{b}\mathrm{Mean}\bigg{(}\big{&#124;}\mathbf{a}_{b}\big{&#124;},\mathrm{axis=}0\bigg{)}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Self-Attention Module: For any data-sample sequence $b$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\rho\in\mathbb{R}^{h}\right)\propto\hat{\mathbf{a}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: F.2 $\rho$ is Wanda (Sun et al., [2023](#bib.bib44))
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MLP / Fully Connected Module: Let $d$ and then compute the following metric
    which is a module level analogue of Wanda:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Self-Attention Module: Let $W\in\mathbb{R}^{d\times o}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G How many perturbative samples are reasonable?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this experiment, All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") except $p_{\mathrm{iter}}=0.1$
    to speed up pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: Varying the number of sub-models generated. Wikitext-2 Perplexity.
    LLaMA-2 7B pruned to 50% sparsity'
  prefs: []
  type: TYPE_NORMAL
- en: '| Num Samples | w/o Post-Pruning Adaptation | w Post-Pruning Adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | $22.09$ |'
  prefs: []
  type: TYPE_TB
- en: '| 200 | $61.63$ |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | NaN | $9.24$ |'
  prefs: []
  type: TYPE_TB
- en: Using $\mathrm{ns}_{\mathrm{sub-models}}=50$ results in an model with NaN perplexity
    on the Wikitext validation set. We posit that this is because of the LLaMA models
    are half precision, and removing the wrong modules can result in activations going
    outside of the FP16 dynamic range for unique data-points. Note that we are able
    to recover good performance of the model after fine-tuning though (we do not observe
    NaNs with the Wikitext-2 training data). This indicates that Bonsai actually recovers
    good modules even using as few samples as 50 sub-models.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Mistral-7B Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the primary experiments on the LLaMA and Phi-2 models, supplementary
    experiments were performed on the Mistral-7B (Jiang et al., [2023](#bib.bib21))
    model in comparison with Wanda results on the stated model. We apply Bonsai with
    the same hardware and configuration settings as used for the LLaMA and Phi-2 experiments.
    We target different pruning fractions (0.05, 0.1, and 0.2) across different numbers
    of samples and masks per iteration to evaluate the method’s performance under
    varying sparsity conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mistral-7B model architecture differs from the LLaMA architecture in its
    use of group query attention and sliding window attention in lieu of the standard
    self-attention used in most transformer-based models like LLaMA (Jiang et al.,
    [2023](#bib.bib21)). We factor these differences into consideration in the implementation
    of Bonsai for Mistral. For the experiments that produced the results below, all
    Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass
    Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [15](#A8.T15 "Table 15 ‣ Appendix H Mistral-7B Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") presents the
    test perplexity results for Mistral-7B under different pruning methods. Considering
    the fully-structured sparsity nature of Bonsai, it achieves a test perplexity
    of 47.5 without post-pruning adaptation, with 1.66$\times$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 15: Test perplexity of Mistral-7B model on Wikitext-2 across fully-structured
    Bonsai and semi-structured Wanda methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Sparsity Level | Method | Test PPL |'
  prefs: []
  type: TYPE_TB
- en: '| Original, unpruned Mistral-7B | N/A | N/A | 5.245 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | semi-structured 2-4 | magnitude | 13.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 12.38 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 10.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Bonsai (w/o Adaptation) | structured 50% | magnitude | 67.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 47.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Bonsai (w/ Adaptation) | structured 50% | Wanda | 10.08 |'
  prefs: []
  type: TYPE_TB
- en: 'We further investigate the pruning habits of Bonsai by examining the pruned
    layers of Mistral, as shown in Figure [4](#A8.F4 "Figure 4 ‣ Appendix H Mistral-7B
    Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes"). We notice a recurring theme: when an attention layer is significantly
    altered, it leads to compensation in the next layers within the sequence. This
    adaptive behavior, termed the ”Hydra effect” by McGrath et al. ([2023](#bib.bib33)),
    implies that the layers within a language model interact in a way that changes
    in one layer prompt adjustments in another. McGrath et al. ([2023](#bib.bib33))
    specifically mentioned that when one attention layer was removed from a language
    model, the model was still able to self-repair and produce similar outputs; but
    it did so by relying more heavily on other layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/662bb7cfb2dc694c9131df9d6ae50772.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Mistral’s pruned attention layers. The heavily pruned layers are
    usually preceded by or sandwiched between lightly-pruned layers, exhibiting the
    self-repairing ”Hydra effect” (McGrath et al., [2023](#bib.bib33)).'
  prefs: []
  type: TYPE_NORMAL
