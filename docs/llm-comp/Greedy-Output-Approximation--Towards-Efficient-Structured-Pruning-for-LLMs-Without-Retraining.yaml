- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs
    Without Retraining'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19126](https://ar5iv.labs.arxiv.org/html/2407.19126)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Jianwei Li Department of Computer Science at North Carolina State University,
    Email: jli265@ncsu.edu    Yijun Dong Courant Institute of Mathematical Sciences
    at New York University, Email: yd1319@nyu.edu    Qi Lei Center for Data Science
    at New York University, Email: ql518@nyu.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To remove redundant components of large language models (LLMs) without incurring
    significant computational costs, this work focuses on single-shot pruning without
    a retraining phase. We simplify the pruning process for Transformer-based LLMs
    by identifying a depth-2 pruning structure that functions independently. Additionally,
    we propose two inference-aware pruning criteria derived from the optimization
    perspective of output approximation, which outperforms traditional training-aware
    metrics such as gradient and Hessian. We also introduce a two-step reconstruction
    technique to mitigate pruning errors without model retraining. Experimental results
    demonstrate that our approach significantly reduces computational costs and hardware
    requirements while maintaining superior performance across various datasets and
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the development of LLMs displaying emergent capabilities like sophisticated
    reasoning, the focus of the community has shifted to models with billions of parameters,
    for example, GPT-4 and Llama2 Achiam et al. ([2023](#bib.bib1)); Touvron et al.
    ([2023b](#bib.bib56)). This transition introduces unprecedented computational
    costs both in the training and the inference phases Touvron et al. ([2023a](#bib.bib55));
    Le Scao et al. ([2023](#bib.bib34)); Team et al. ([2023](#bib.bib52)); Banks and
    Warkentin ([2024](#bib.bib5)). To address this challenge, pruning plays a constructive
    role by removing redundant components from models, thereby reducing computational
    costs Gordon et al. ([2020](#bib.bib22)); Prasanna et al. ([2020](#bib.bib46));
    Wang et al. ([2020](#bib.bib59)); Li et al. ([2023a](#bib.bib36)). Notably, designing
    an optimal pruning strategy is an NP-hard problem (as it reduces to subset selection)
    and requires balancing accuracy, sparsity, generalizability, pruning costs, and
    hardware compatibility in practice Xu et al. ([2021](#bib.bib61)); Li et al. ([2023b](#bib.bib37));
    Du et al. ([2023](#bib.bib12)). Traditional pruning methods primarily focus on
    accuracy and sparsity, often neglecting other key factors. They typically involve
    model retraining and knowledge distillation to mitigate pruning errors. However,
    with current LLMs featuring billions of parameters, the training process is already
    a significant challenge, making the additional cost of model retraining even more
    unaffordable Frantar and Alistarh ([2022](#bib.bib18), [2023](#bib.bib19)). Given
    these challenges, there’s a pressing need for more efficient pruning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, some works have focused on structured pruning on pre-trained LLMs,
    directly addressing hardware compatibility and generalizability. This approach
    allows them to concentrate on the remaining trade-off factors: sparsity, accuracy,
    and pruning cost. For example, methods like LLM-Pruner, Shortened LLaMA, and Sheared
    LLaMA use a single-shot pruning strategy that requires only one round of retraining Kim
    et al. ([2024](#bib.bib32)); Xia et al. ([2023](#bib.bib60)); Ma et al. ([2023](#bib.bib40)).
    On the other hand, strategies such as FLAP, OPTIN, Sliced GPT, LLM Surgeon, Wanda,
    ZipLM, and KRP seek to eliminate the need for model retraining entirely An et al.
    ([2024](#bib.bib2)); Ashkboos et al. ([2024](#bib.bib4)); van der Ouderaa et al.
    ([2023](#bib.bib57)); Sun et al. ([2023](#bib.bib51)); Kurtić et al. ([2024](#bib.bib33));
    Khaki and Plataniotis ([2024](#bib.bib31)); Li et al. ([2023b](#bib.bib37)). However,
    these approaches have respective limitations, such as high computational costs
    from the calculation of higher-order information, a lack of fully structured pruning
    patterns Pool et al. ([2021](#bib.bib45)), or compromised performance in some
    cases. The development of these methods marks a crucial phase in the evolution
    of LLMs, aiming to enhance model capabilities while ensuring computational efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25c7d8614995ab36271d5cd331ec7ab6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Pruning metric analysis from the optimization perspective A: Function
    Approximation; B: Output Approximation; C: Objective Approximation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the existing challenges, we call for a pruning strategy that better trades
    off the accuracy, structure preservation, and computational costs. We delve into
    this kind of strategy by answering the following essential questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Question 1. Does an inherent pruning structure exist in Transformer-based language
    models?
  prefs: []
  type: TYPE_NORMAL
- en: We discovered depth-2 pruning modules within Transformer architecture by analyzing
    structured pruning from both input and output channels. These structures preserve
    feature knowledge while reducing pruning complexity from residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: Question 2. Is there an effective pruning criterion that does not require training
    awareness?
  prefs: []
  type: TYPE_NORMAL
- en: We identified two efficient and high-performing inference-aware pruning metrics
    based on output approximation for Transformer models. They are simpler and more
    computationally efficient than training-aware metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Question 3. Is there a low-computation performance recovery technique available?
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by layer-wise reconstruction Li et al. ([2023b](#bib.bib37)), we developed
    a two-step module reconstruction strategy. This method updates the weights of
    the depth-2 module without computing parameter gradients, effectively minimizing
    pruning errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answering the above questions altogether, this paper proposes an integrated
    and efficient pruning strategy with a focus on Transformer-based LLMs Vaswani
    et al. ([2017](#bib.bib58)). Specifically, we categorize all pruning metrics into
    three groups based on their implicit purpose: function (weights) approximation,
    output approximation, and objective approximation. Following the output approximation
    route, we introduce a similarity-based pruning strategy that exploits the redundancy
    in multi-head attention mechanisms by removing heads that extract similar information
    first rather than those with minimal impact. Additionally, we propose a second-moment-based
    pruning approach also under the output approximation category, which stands out
    for its ability to integrate information across multiple layers. We apply this
    metric for both attention and feed-forward modules to remove the elements with
    minimal impact on the model’s performance. Finally, we develop an optimization
    technique that eliminates the need for higher-order information by greedily reducing
    pruning error through weight reconstruction of the subsequent dense module. Our
    structured pruning experiments on pre-trained LLMs ranging from millions to billions
    of parameters demonstrate that our method ensures generalizability, hardware compatibility,
    and minimal pruning cost. Moreover, it outperforms or achieves comparative performance
    to other non-retraining methods and even some methods that require retraining.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pruning and Structured Pruning: Pruning is a technique used in machine learning
    to reduce the size of a model by eliminating unnecessary parameters, which can
    lead to reductions in storage requirements and computational complexity without
    significantly affecting the model’s performance Frantar and Alistarh ([2022](#bib.bib18));
    Frankle et al. ([2020](#bib.bib17)). This process involves identifying and removing
    the parts of the model that have the least impact on its output, such as weights
    in a neural network with small magnitudes. By doing so, pruning discovers a more
    efficient model that is faster to execute and easier to deploy on devices with
    limited resources. Structured pruning, a method that imposes more constraints,
    focuses on eliminating entire units or structures within the model, such as neurons,
    channels, or layers, instead of individual weights Anwar et al. ([2017](#bib.bib3));
    Fang et al. ([2023](#bib.bib15)). Being the focus of our paper, structured pruning
    is especially advantageous due to its compatibility with standard hardware, whereas
    unstructured pruning requires specially designed accelerators to deploy in practical
    scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data-free/dependent and Training/Inference-aware Metrics: When choosing which
    redundant components to remove from a model, the selection is typically guided
    by specific metrics Hoefler et al. ([2021](#bib.bib28)). These metrics can be
    broadly divided into data-free and data-dependent categories, depending on whether
    they rely on specific datasets. Additionally, they can be categorized as training-aware
    or inference-aware, based on whether they require model backpropagation. This
    paper focuses on inference-aware metrics and explores both data-free and data-dependent
    versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient and Low-Resource Pruning: As the number of parameters in LLMs increases,
    the quest for efficient pruning has become paramount. Methods such as LLM-Pruner,
    Sheared LLaMA, and Shortened LLaMA adopt a single-shot pruning strategy Kim et al.
    ([2024](#bib.bib32)); Xia et al. ([2023](#bib.bib60)); Ma et al. ([2023](#bib.bib40)).
    Yet, these approaches depend on computationally expensive metrics and still require
    retraining to minimize pruning-induced errors. In contrast, methods like OPTIN,
    Sliced GPT, LLM Surgeo, ZipLM, and KRP eliminate the need for retraining but still
    rely heavily on computationally expensive second-order Hessian information, which
    is a significant drawback for large-dimensional models Ashkboos et al. ([2024](#bib.bib4));
    Sun et al. ([2023](#bib.bib51)); Kurtić et al. ([2024](#bib.bib33)); Khaki and
    Plataniotis ([2024](#bib.bib31)); Li et al. ([2023b](#bib.bib37)). Meanwhile,
    FLAP and Wanda design specific pruning metrics that share similar ideas with the
    methods from the pre-deep learning age Engelbrecht et al. ([1999](#bib.bib14));
    Sietsma and Dow ([1988](#bib.bib49)); Engelbrecht and Cloete ([1996](#bib.bib13));
    Thimm and Fiesler ([1995b](#bib.bib54)), significantly reducing computational
    demand An et al. ([2024](#bib.bib2)); van der Ouderaa et al. ([2023](#bib.bib57)).
    This paper proposes a method that eliminates the need for both model retraining
    and computationally expensive metrics, offering superior or comparative performance
    compared to other non-retraining methods and even some methods that require retraining.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section outlines our structured pruning scheme, which consists of three
    key components: pruning structure recognition, pruning criteria definition, and
    post-pruning recovery strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Pruning Structure Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our approach involves single-shot pruning and targets structured components,
    such as entire rows or columns of weight matrices, rather than individual weights.
    We do not discuss layer or block pruning, as it disrupts inherent model correlations
    and requires retraining to restore layer dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Input or Output Channel Pruning for Sequential Layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To clarify structured pruning, it is important to understand that pruning neurons
    can be approached in two directions: input channels and output channels. Consider
    a linear function $f(X)=XW$, also known as feature selection. This paper focuses
    on a static approach to feature selection, where the same channels are removed
    for all samples, making feature selection equivalent to output channel pruning
    in the previous layer. An interesting phenomenon arises: in depth-2 sequential
    linear layers, pruning the input channels of the second layer simultaneously pruning
    the output channels of the first layer, using identical pruning indices. In contrast,
    pruning the output channels of the second layer does not affect the first layer.
    Both input and output channel pruning contribute to model compression, but they
    may have different impacts on performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b0cf654b6de0e0e6afc095a049a0c21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Pruning structure recognition. A: Two pruning strategies for the
    depth-2 module. B: Depth-2 modules identification in Transformer-based LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Input or Output Channel Pruning for Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Depth-2 Module Identification: In Transformer-based language models, the attention
    and feed-forward modules function as sequential layers with a depth of 2\. For
    the attention module, the first level includes the weight matrices $W_{Q}$ represent
    the weights for the query, key, value, and output in the attention block, respectively.
    The feed-forward module follows a similar structure: the upward projection and
    gated projection occur at the first level, while the downward projection occurs
    at the second level. These depth-2 modules have a unique characteristic: when
    pruning the input channels of layers at the second level, the output channel indices
    of the first-level layers must correspondingly match. This ensures that the structural
    integrity of the model is maintained while pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pruning Strategies for Depth-2 Modules: Given these depth-2 modules, we can
    employ two pruning strategies to achieve the same compression ratio. The first
    strategy involves pruning the output channels of the layers in the first level
    while concurrently pruning the input channels of the layers in the second level.
    This approach ensures that the dependencies outside the module remain invariant.
    The second strategy involves pruning the output channels and the initial input
    $X$ is effectively equivalent to pruning the output channels of a preceding module
    in the sequence. As this dependency propagates backward through the layers, it
    ultimately affects the model’s embedding layer, meaning we are directly pruning
    the channels of the token embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge of Residual Connection: Without considering the loss of tokens’ semantic
    information, the two pruning strategies described above should not differ significantly.
    However, residual connections impose substantial constraints on the second strategy.
    In the Transformer architecture, every depth-2 module determines residual connections.
    This means that the pruned channels must be strictly aligned across all modules.
    If the pruned indices of one of them do not align with others, it could lead to
    an unpredictable loss of information. This constraint severely limits the choice
    of channels for pruning and could significantly decrease performance. In contrast,
    the first strategy maintains a fixed number of output channels across these modules,
    avoiding this limitation. Each module can independently select which internal
    channels to prune based on its needs, resulting in a larger search space for optimization.
    This paper will focus on the first pruning strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional Structure for Attention Mechanism: The intricate topology of the
    attention block introduces an additional constraint: pruning must be conducted
    at the level of entire heads, encompassing continuous portions of the channels.
    Fortunately, given the design philosophy of multi-head attention—that each head
    is designed to capture correlations between tokens independently—this setup easily
    leads to redundancy, making it highly amenable to similarity analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pruning Criteria Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section begins by examining different pruning criteria from an optimization
    perspective. Then, we introduce two specific pruning metrics for the aforementioned
    depth-2 modules. Finally, an intuitive magnitude-based pruning method is employed
    to remove the least important channels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Pruning Metric Analysis from an Optimization Perspective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Previous work has categorized pruning metrics based on their relationship with
    data, as discussed in Section [2](#S2 "2 Related Work ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining"). Diverging
    from these approaches, we analyze these metrics from an optimization perspective
    and describe in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining"). Specifically,
    for a linear operation $f(X)=XW$. Objective approximation aims to directly approximate
    accuracy. This category encompasses metrics such as first-order or second-order
    information and regularization scores. However, this type of metric is computationally
    expensive as the optimization process involves backward propagation and calculation
    of the Hessian Matrix. By analyzing these strategies, this paper proposes two
    new metrics to guide the pruning of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Similarity-based Metric for Attention Block
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a4375ac106fd8d6d1751eacbc24dc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Similarity visualization of attention heads in A: block 4 and B:
    block 5 for Llama-7B. Heads with divergence less than $\tau=0.20$ are connected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous research on pruning attention heads typically involves removing heads
    with the lowest importance scores. Surprisingly, our experiments indicate that
    random pruning also yields competitive results compared to magnitude-based pruning,
    especially when the pruning ratio is below 50%. Further experimentation with different
    random seeds, leading to various head indices for pruning, consistently produces
    comparable results. Notably, nearly all heads have been selected for removal at
    some point during this process, suggesting a potential oversight in our initial
    understanding. Recall that different attention heads are intended to independently
    capture correlations between tokens. Thus, it’s common for similar information
    to be extracted across different heads. This observation prompted us to reconsider
    our strategy: we prioritize removing similar heads before eliminating those with
    the least importance score. By identifying and pruning heads that capture redundant
    information, we can optimize the model more effectively while preserving its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Previous studies have conducted similarity analysis between neurons Engelbrecht
    and Cloete ([1996](#bib.bib13)); Sietsma and Dow ([1988](#bib.bib49)); Engelbrecht
    et al. ([1999](#bib.bib14)), examining the output differences across multiple
    samples to identify similar components. The redundant neurons are then removed,
    and the remaining neurons scale their weights or biases to minimize the impact
    of this removal. However, these methods are primarily effective in smaller neural
    networks, as the scaling technique struggles to handle the accumulated error across
    numerous layers. Fortunately, due to the parallelism and independence of attention
    heads, removing redundant heads does not lead to significant information loss
    that affects subsequent layers, thus eliminating the need for costly remedial
    operations. Based on this observation, we define a pairwise head divergence matrix
    $D\in\mathbb{R}^{h\times h}$ is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $M_{ij}^{(n)}=\frac{1}{2}(P_{i}^{(n)}+Q_{j}^{(n)})$ via an edge, we can
    clearly illustrate the relationships between these heads. Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2.2 Similarity-based Metric for Attention Block ‣ 3.2 Pruning Criteria Selection
    ‣ 3 Methodology ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining") demonstrates that some heads fall into the same
    group, signaling information redundancy, whereas others stand alone, highlighting
    the uniqueness of their information. We also observe that specific layers form
    large groups, indicating higher redundancy. The details of our pruning strategy
    for the attention module are outlined in Algo [1](#alg1 "Algorithm 1 ‣ 3.2.2 Similarity-based
    Metric for Attention Block ‣ 3.2 Pruning Criteria Selection ‣ 3 Methodology ‣
    Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Attention Heads Pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Pairwise head divergence matrix $D\in\mathbb{R}^{h\times h}$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Pre-Pruning Recovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Depth-2 module $m_{i}$end procedure'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Second-moment-based Metric for Depth-2 Module
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To prune the identified depth-2 module, we follow the structure mentioned in
    Sec [3.1](#S3.SS1 "3.1 Pruning Structure Recognition ‣ 3 Methodology ‣ Greedy
    Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining"),
    namely pruning output channels in the first level and input channels in the second
    level. Since the pruned channel indexes from these two directions must match,
    we have to consider them together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper proposes a 2nd-moment-based pruning metric that is simple to calculate
    and incorporates information from multiple layers. To facilitate understanding,
    we use a basic feed-forward module as an example. Specifically, we describe the
    module as $f(X)=B\sigma(AX)$ can be derived as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $E[Y_{ij}^{2}]$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: When the activation function is ReLU, we add a coefficient of 1/2 to the above
    equation. For GeLU or SiLU, our observations indicate that only a small portion
    of values fall within the non-linear region; therefore, we treat them as equivalent
    to ReLU. This approach offers more valuable information from the covariance matrix
    compared to methods based on output energy (first moment) Hagiwara ([1993b](#bib.bib24),
    [1994](#bib.bib25)); Hu et al. ([2016](#bib.bib30)). Unlike some statistical methods
    that require calibration datasets to collect feature values and then calculate
    statistical properties, our method can flexibly integrate information from both
    input and output channels, whereas those methods are limited to focusing only
    on output channels. When there is no way to estimate $\Sigma_{X}$ for attention
    and the feed-forward modules can be found in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Pre-Pruning Recovery Without Retraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the selection of the pruning structure and criteria, this paper proposes
    a module-wise pruning approach. Similar to layer-wise pruning, we prune these
    depth-2 modules sequentially. To prune one of them, we calculate importance scores
    for its inner channels based on the module’s structure, weights, and inputs. Notably,
    due to errors introduced by pruning preceding modules, the input to the current
    module inevitably deviates from its dense version. Consequently, even without
    pruning the current module, a discrepancy between its output and the original
    output is unavoidable. Recall that our design philosophy is to approximate the
    output as closely as possible. Thus, it is crucial to reconstruct the weights
    of the current module before pruning. This reconstruction ensures that the output
    of this module can still align as closely as possible with the original, even
    with the new input. This way, the pruning criteria for each channel can be optimally
    up-to-date. Inspired by Li et al. ([2023b](#bib.bib37)), this paper presents a
    pre-pruning recovery technique in Algo [2](#alg2 "Algorithm 2 ‣ 3.2.2 Similarity-based
    Metric for Attention Block ‣ 3.2 Pruning Criteria Selection ‣ 3 Methodology ‣
    Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining").'
  prefs: []
  type: TYPE_NORMAL
- en: With this technique, we can mitigate the errors accumulated from previously
    pruned modules without requiring model retraining. Unlike previous work, which
    primarily focuses on a single layer, our approach targets more complex structures,
    including intricate layer dependencies. We provide more details on applying this
    method to the attention and feed-forward modules in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section initially presents the fundamental setup for our experiments. Subsequently,
    we demonstrate the results of experiments and provide an in-depth analysis from
    multiple perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: The zero-shot performance of the compressed LLaMA-7B (20% sparsity).
    Following the LLM-Pruner methodology Ma et al. ([2023](#bib.bib40)), we only prune
    the transformer blocks from the 4th to the 30th. The average performance is calculated
    across seven classification datasets. ’Bold’ indicates the best pruning-only performance,
    while ’underline’ represents the overall best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Methods | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dense Touvron et al. ([2023a](#bib.bib55)); Ma et al. ([2023](#bib.bib40))
    | 12.62 | 22.14 | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38 | 42.4 | 63.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data Free Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Random Hoefler et al. ([2021](#bib.bib28)) | 23.02 | 40.19 | 46.21 | 71.33
    | 59.35 | 56.51 | 47.97 | 32.0 | 36.30 | 49.95 |'
  prefs: []
  type: TYPE_TB
- en: '| L1 norm Hoefler et al. ([2021](#bib.bib28)) | 179.02 | 311.75 | 51.28 | 60.22
    | 43.14 | 52.01 | 36.53 | 27.89 | 30.8 | 43.12 |'
  prefs: []
  type: TYPE_TB
- en: '| L2 norm Hoefler et al. ([2021](#bib.bib28)) | 582.41 | 1022.17 | 60.18 |
    58.54 | 37.04 | 53.27 | 32.91 | 27.56 | 29.8 | 42.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours (Self-Gen) | 21.76 | 34.3 | 63.51 | 72.63 | 56.54 | 54.46 | 51.68 |
    33.79 | 36.4 | 52.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours SG w/ remedy | 20.32 | 33.42 | 64.17 | 72.67 | 58.43 | 57.29 | 53.32
    | 34.15 | 37.23 | 53.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Dependent Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Training-Aware Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner Vec Ma et al. ([2023](#bib.bib40)) | 22.28 | 41.78 | 61.44 | 71.71
    | 57.27 | 54.22 | 55.77 | 33.96 | 38.4 | 53.52 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner E1 Ma et al. ([2023](#bib.bib40)) | 19.09 | 34.21 | 57.06 | 75.68
    | 66.8 | 59.83 | 60.94 | 36.52 | 40.0 | 56.69 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner E2 Ma et al. ([2023](#bib.bib40)) | 19.77 | 36.66 | 59.39 | 75.57
    | 65.34 | 61.33 | 59.18 | 37.12 | 39.8 | 56.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Inference-Aware Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda-sp Sun et al. ([2023](#bib.bib51)) | 27.45 | 49.52 | 64.16 | 75.21
    | 68.62 | 62.27 | 59.68 | 36.68 | 39.2 | 57.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours (Calibration) | 17.48 | 30.04 | 66.48 | 75.78 | 67.73 | 62.27 | 61.4
    | 35.49 | 39.6 | 58.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours C w/ remedy | 17.90 | 31.23 | 70.12 | 76.86 | 68.55 | 65.76 | 64.23
    | 38.54 | 40.5 | 60.65 |'
  prefs: []
  type: TYPE_TB
- en: '| Retraining-required Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-P. LoRA Ma et al. ([2023](#bib.bib40)) | 17.37 | 30.39 | 69.54 | 76.44
    | 68.11 | 65.11 | 63.43 | 37.88 | 40.0 | 60.07 |'
  prefs: []
  type: TYPE_TB
- en: 'Baselines: This paper presents a comprehensive comparison of state-of-the-art
    pruning methods across multiple dimensions, aiming for fair evaluations and in-depth
    analyses to uncover the reasons behind the observed results. First, we compare
    our approach with data-free pruning methods, including random pruning and magnitude-based
    pruning (L1 and L2 norms) Hoefler et al. ([2021](#bib.bib28)). Next, we evaluate
    our methods against data-dependent pruning techniques, encompassing training-aware,
    inference-aware, and retraining-required methods. In the training-aware category,
    we compare with various configurations of LLM-Pruner Ma et al. ([2023](#bib.bib40)),
    such as Element1, Element2, and Vector-wise magnitude pruning. Within the inference-aware
    category, we compare with the structured version of Wanda Sun et al. ([2023](#bib.bib51))
    and FLAP An et al. ([2024](#bib.bib2)). Additionally, we extend our comparisons
    to include the LLM-Pruner method augmented with retraining. Such comprehensive
    evaluations will demonstrate the effectiveness of our pruning approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models: Our primary experiments are categorized into two series based on the
    model scale: LLaMA-7B with 7 billion parameters and GPT-2 with 110 million parameters Radford
    et al. ([2019](#bib.bib47)); Touvron et al. ([2023a](#bib.bib55)). This aligns
    with our study’s goal to assess pruning performance across different model sizes
    and ensure a thorough examination. Additionally, we extend our experiments to
    other models, including LLaMA-13B, Vicuna-7B Chiang et al. ([2023](#bib.bib7)).
    This comprehensive selection allows us to explore a broader spectrum of capabilities
    and sizes, enhancing our understanding of how different architectures perform
    under various computational constraints. Additional experiment results can be
    found in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation and Datasets: To evaluate performance, we adopt LLaMa’s approach
    by conducting zero-shot task classification on a range of common sense reasoning
    datasets: BoolQ Clark et al. ([2019](#bib.bib8)), PIQA Bisk et al. ([2020](#bib.bib6)),
    HellaSwag Zellers et al. ([2019](#bib.bib63)), WinoGrande Sakaguchi et al. ([2021](#bib.bib48)),
    ARC-easy Clark et al. ([2018](#bib.bib9)), ARC-challenge Clark et al. ([2018](#bib.bib9)),
    and OpenbookQA Mihaylov et al. ([2018](#bib.bib43)). Following the methodology
    in Gao et al. ([2021](#bib.bib21)), the model either ranks the options in multiple-choice
    tasks or generates answers in open-ended formats. Additionally, we enhance our
    evaluation by conducting a zero-shot perplexity (PPL) analysis on WikiText2 Merity
    et al. ([2016](#bib.bib42)) and the Penn Treebank (PTB) Marcus et al. ([1993](#bib.bib41)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementation: During the pruning phase, we randomly select 16 samples from
    Wikitext2 and Bookcorpus, truncated to a sequence length of 128 for LLaMA-7B and
    1024 for GPT-2\. These samples serve as calibration data for pruning metric calculation
    and covariance matrix extraction, respectively. During the recovery phase, we
    sample an additional 1,024 examples from the downstream dataset to guide optimization
    in the data-dependent comparison experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Similarity-based analysis for LLaMA-7B attention heads pruning (all
    blocks) with different $\tau$. ’Bold’ indicates the best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | # pruned heads | Wiki2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 0 | 12.62 | 22.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours ($\tau=0.16$) | 88 | 12.96 | 22.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 64 | 14.50 | 24.13 |'
  prefs: []
  type: TYPE_TB
- en: '| L2 Norm | 64 | 14.69 | 25.64 |'
  prefs: []
  type: TYPE_TB
- en: '| 1st+2nd order | 64 | 13.45 | 24.19 |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP | 88 | 12.90 | 22.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours ($\tau=0.19$) | 204 | 14.69 | 24.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 192 | 18.75 | 35.73 |'
  prefs: []
  type: TYPE_TB
- en: '| L2 Norm | 192 | 195.84 | 371.65 |'
  prefs: []
  type: TYPE_TB
- en: '| 1st+2nd order | 192 | 14.81 | 28.77 |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP | 204 | 13.22 | 24.42 |'
  prefs: []
  type: TYPE_TB
- en: 'We present the main results in Tab. [1](#S4.T1 "Table 1 ‣ 4.1 Setup ‣ 4 Experiment
    ‣ Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining"). For the data-free comparison experiments, we leverage the inherent
    ability of LLMs to generate sentences. Our pruning method uses these generated
    sentences as calibration data because, given that the LLMs are well-trained, these
    sentences naturally conform to the semantic and syntactic token distributions
    of the training data. Compared to traditional data-free metrics (L1 or L2), our
    data-free version, which relies solely on the model itself, achieves significant
    improvements in perplexity and up to a 20% enhancement in zero-shot evaluation
    for downstream tasks. Moreover, our method surpasses random pruning by at least
    6%, a significant improvement achieved without relying on existing datasets, while
    traditional metrics (L1 or L2) fail to outperform. These results demonstrate the
    superiority of our techniques in data-free pruning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5cad0261bd3b5a2653b5b1bdb841c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Performance of compressed A: LLaMA-7B (w/o Remediation) and B: GPT-2 (w/
    Remediation) concerning the number of calibration samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Our approach outperforms data-dependent pruning methods and the inference-only
    method Wanda-SP. Impressively, it also surpasses the state-of-the-art training-aware
    pruning method LLM-Pruner, which includes different configurations such as Element1,
    Element2, and Vector. Our approach consistently demonstrates better pruning results
    without requiring computationally intensive first-order and second-order information.
    Moreover, our method even achieves better results compared to LLM-Pruner with
    LoRA, despite the latter involving model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also compare our method with the state-of-the-art inference-only method
    FLAP and present the results in Tab. [3](#S4.T3 "Table 3 ‣ 4.2 Results and Analysis
    ‣ 4 Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining"). Our approach exhibits significantly better results
    on the GPT-2 model and achieves comparable performance with LLaMA-7B. Overall,
    our method demonstrates superior performance in both data-free and data-dependent
    pruning categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Perplexity of compressed GPT-2 and LLaMA-7B (25% and 50% sparsity)
    on Wikitext2 and PTB. We prune the 4th to 30th transformer blocks for LLaMA-7B
    and all blocks for GPT-2\. ’Bold’ indicates the best performance, while ’underline’
    represents the second-best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | GPT-2: [0-12) | LLama-7b: [4-30) |'
  prefs: []
  type: TYPE_TB
- en: '| Datasets: PPL | WikiText2: 29.95 Radford et al. ([2019](#bib.bib47)) $\downarrow$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsity | 25% | 50% | 25% | 50% | 25% | 50% | 25% | 50% |'
  prefs: []
  type: TYPE_TB
- en: '| Data Free Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Random Hoefler et al. ([2021](#bib.bib28)) | 189.73 | 1839.33 | 245.33 |
    2769.6 | 23.02 | 100.42 | 40.19 | 133.56 |'
  prefs: []
  type: TYPE_TB
- en: '| L1 norm Hoefler et al. ([2021](#bib.bib28)) | 338.3 | 1226.13 | 583.2 | 1290.45
    | 179.02 | 891.23 | 311.75 | 1034.69 |'
  prefs: []
  type: TYPE_TB
- en: '| L2 norm Hoefler et al. ([2021](#bib.bib28)) | 227.32 | 674.52 | 324.33 |
    800.14 | 582.41 | 14000.68 | 1022.17 | 28062.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours (Self-Generation w/ remedy) | 119.29 | 586.87 | 152.93 | 723.39 | 21.76
    | 58.61 | 34.3 | 64.24 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Dependent Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Training-Aware Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner Element1 Ma et al. ([2023](#bib.bib40)) | 9229.32 | 32453.23 |
    11993.24 | 8020.87 | 19.09 | 48.84 | 34.21 | 105.24 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner Element2 Ma et al. ([2023](#bib.bib40)) | 1897.32 | 14706.23 |
    2258.33 | 18598.33 | 19.77 | 72.89 | 36.66 | 138.33 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner Vector Ma et al. ([2023](#bib.bib40)) | 488.32 | 39025.12 | 6169.56
    | 18616.87 | 22.88 | 55.68 | 41.76 | 305.24 |'
  prefs: []
  type: TYPE_TB
- en: '| Inference-Aware Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda-Structured Pruning Sun et al. ([2023](#bib.bib51)) | 586.34 | 4147.32
    | 355.17 | 3246.79 | 27.45 | 69.02 | 49.52 | 132.52 |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP UL-UM w/o remed An et al. ([2024](#bib.bib2)) | 818.14 | 3636.23 | 554.32
    | 2758.37 | 17.15 | 36.08 | 34.96 | 85.22 |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP UL-UM w/ remedy An et al. ([2024](#bib.bib2)) | 2197.32 | 3043.35 |
    2199.24 | 3561.76 | 15.76 | 26.87 | 32.1 | 66.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours UL-UM (Calibration w/o remedy) | 81.96 | 317.37 | 186.68 | 936.57 |
    NA | NA | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP AL-AM w/o remedy An et al. ([2024](#bib.bib2)) | 126.57 | 5538.32 |
    135.07 | 10244.95 | 17.01 | 34.09 | 30.99 | 71.76 |'
  prefs: []
  type: TYPE_TB
- en: '| FLAP AL-AM w/ remedy An et al. ([2024](#bib.bib2)) | 1349.25 | 5382.14 |
    1769.56 | 7476.08 | 15.06 | 26.55 | 29.45 | 57.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours ML-MM (Calibration w/o remedy) | 79.4 | 251.34 | 130.54 | 756.33 | 17.48
    | 26.87 | 30.04 | 57.89 |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also explore our pruning metrics by exclusively pruning attention heads.
    The experimental results in Tab. [2](#S4.T2 "Table 2 ‣ 4.2 Results and Analysis
    ‣ 4 Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining") demonstrate that for colossal LLMs like LLaMA-7B,
    our similarity analysis effectively identifies redundant attention heads with
    minimal negative impact on model performance. Compared to inference-aware metrics
    such as the L2 norm, training-aware metrics using first- and second-order information,
    and random pruning, our similarity-based metric consistently outperforms. When
    compared to the specifically designed metric of FLAP, we achieve better or comparable
    performance. These results strongly indicate that we should prioritize pruning
    redundant information rather than heads with small importance scores.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we designed experiments to explore the influence of the number
    of calibration samples. Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Results and Analysis
    ‣ 4 Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining") shows that in LLaMA-7B pruning-only experiments,
    our method is insensitive to the number of calibration samples, achieving comparable
    results with as few as 8 samples and as many as 128 samples. Conversely, in GPT-2
    pruning with remediation experiments, performance improves with an increasing
    number of calibration samples. These findings demonstrate that our pruning method
    is robust regardless of the number of calibration samples, while our pre-pruning
    recovery method benefits from a higher number of calibration samples. However,
    this improvement gradually diminishes once the number of samples reaches a critical
    threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion, Limitation, and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implicit Motivation and Call: In the pre-deep learning era, various pruning
    metrics and structures were designed. For example, variance-based pruning and
    bias-based remedy methods similar to FLAP were proposed by researchers 30 years
    ago Engelbrecht et al. ([1999](#bib.bib14)); Sietsma and Dow ([1988](#bib.bib49));
    Engelbrecht and Cloete ([1996](#bib.bib13)); Thimm and Fiesler ([1995b](#bib.bib54)).
    These early researchers already recognized that feature information is at least
    as crucial as model weights in constructing pruning criteria. In the early stages
    of deep learning (before 2022), many researchers found that multi-round model
    retraining could easily recover the lost performance induced by pruning, even
    when based solely on weight magnitudes. As a result, the importance of pruning
    metrics and structure design was often overlooked, with reliance placed on retraining
    to validate methods. However, this paradigm shifted after 2022, when colossal
    LLMs became mainstream in the community. Training such models is prohibitively
    expensive, making pruning that relies on multi-round retraining impractical. Although
    parameter-efficient training methods like LoRA can reduce costs, they still require
    rigorous data selection Hu et al. ([2021](#bib.bib29)); Ma et al. ([2023](#bib.bib40)).
    Thus, we urge the community to return to designing metrics that better account
    for the influence of both weights and features, rather than focusing solely on
    dataset competition. Motivated by this, this paper focuses on inference-aware
    pruning metrics that do not require retraining.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Limitation: This work evaluates the compressed LLMs primarily on perplexity
    and downstream tasks. However, we do not assess the emergent abilities of colossal
    LLMs, such as mathematical reasoning, safety alignment, common sense reasoning,
    contextual understanding, and creativity in text generation. Future research will
    focus on evaluating and enhancing these emergent abilities to provide a more comprehensive
    understanding of the compressed LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conclusion: This paper introduces a novel approach to pruning large language
    models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware
    pruning criteria. These methods surpass traditional metrics and eliminate the
    need for computationally expensive retraining. Our two-step reconstruction technique
    further mitigates pruning errors, ensuring superior performance across various
    datasets and models. Overall, our approach significantly reduces computational
    costs and hardware requirements, offering an efficient solution for pruning colossal
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. [2024] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based
    adaptive structured pruning for large language models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 38, pages 10865–10873, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anwar et al. [2017] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured
    pruning of deep convolutional neural networks. *ACM Journal on Emerging Technologies
    in Computing Systems (JETC)*, 13(3):1–18, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ashkboos et al. [2024] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do
    Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language
    models by deleting rows and columns. *arXiv preprint arXiv:2401.15024*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banks and Warkentin [2024] Tris Warkentin Jeanine Banks and Tris Warkentin.
    Gemma: Introducing new state-of-the-art open models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2(3):6, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cuadros et al. [2020] Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff.
    Filter distillation for network compression. In *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 3140–3149, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. [2017] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune
    deep neural networks via layer-wise optimal brain surgeon. *Advances in Neural
    Information Processing Systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. [2023] Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi,
    Xia Hu, and Ahmed Hassan Awadallah. Robustness challenges in model distillation
    and pruning for natural language understanding. In *Proceedings of the 17th Conference
    of the European Chapter of the Association for Computational Linguistics*, pages
    1766–1778, Dubrovnik, Croatia, May 2023\. Association for Computational Linguistics.
    URL [https://aclanthology.org/2023.eacl-main.129](https://aclanthology.org/2023.eacl-main.129).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engelbrecht and Cloete [1996] Andries P Engelbrecht and Ian Cloete. A sensitivity
    analysis algorithm for pruning feedforward neural networks. In *Proceedings of
    International Conference on Neural Networks (ICNN’96)*, volume 2, pages 1274–1278\.
    IEEE, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engelbrecht et al. [1999] Andries P Engelbrecht, L Fletcher, and Ian Cloete.
    Variance analysis of sensitivity information for pruning multilayer feedforward
    neural networks. In *IJCNN’99\. International Joint Conference on Neural Networks.
    Proceedings (Cat. No. 99CH36339)*, volume 3, pages 1829–1833\. IEEE, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. [2023] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and
    Xinchao Wang. Depgraph: Towards any structural pruning. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 16091–16101,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle and Carbin [2019] Jonathan Frankle and Michael Carbin. The lottery
    ticket hypothesis: Finding sparse, trainable neural networks. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=rJl-b3RcF7](https://openreview.net/forum?id=rJl-b3RcF7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frankle et al. [2020] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy,
    and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.
    In *International Conference on Machine Learning*, pages 3259–3269\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2022] Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. [2019] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *ArXiv*, abs/1902.09574, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, page 8, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gordon et al. [2020] Mitchell Gordon, Kevin Duh, and Nicholas Andrews. Compressing
    BERT: Studying the effects of weight pruning on transfer learning. In *Proceedings
    of the 5th Workshop on Representation Learning for NLP*, pages 143–155, Online,
    July 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.repl4nlp-1.18.
    URL [https://aclanthology.org/2020.repl4nlp-1.18](https://aclanthology.org/2020.repl4nlp-1.18).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hagiwara [1993a] M. Hagiwara. Removal of hidden units and weights for back
    propagation networks. In *Proceedings of 1993 International Conference on Neural
    Networks (IJCNN-93-Nagoya, Japan)*, volume 1, pages 351–354 vol.1, 1993a. doi:
    10.1109/IJCNN.1993.713929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hagiwara [1993b] Masafumi Hagiwara. Removal of hidden units and weights for
    back propagation networks. In *Proceedings of 1993 International Conference on
    Neural Networks (IJCNN-93-Nagoya, Japan)*, volume 1, pages 351–354\. IEEE, 1993b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hagiwara [1994] Masafumi Hagiwara. A simple and effective method for removal
    of hidden units and weights. *Neurocomputing*, 6(2):207–218, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2015] Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2019] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter
    pruning via geometric median for deep convolutional neural networks acceleration.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 4340–4349, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoefler et al. [2021] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden,
    and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient
    inference and training in neural networks. *Journal of Machine Learning Research*,
    22(241):1–124, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2016] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network
    trimming: A data-driven neuron pruning approach towards efficient deep architectures.
    *arXiv preprint arXiv:1607.03250*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khaki and Plataniotis [2024] Samir Khaki and Konstantinos N Plataniotis. The
    need for speed: Pruning transformers with one recipe. *arXiv preprint arXiv:2403.17921*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2024] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells,
    Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened llama: A simple depth
    pruning for large language models. *arXiv preprint arXiv:2402.02834*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurtić et al. [2024] Eldar Kurtić, Elias Frantar, and Dan Alistarh. Ziplm:
    Inference-aware structured pruning of language models. *Advances in Neural Information
    Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le Scao et al. [2023] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2018] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr.
    Snip: Single-shot network pruning based on connection sensitivity. *arXiv preprint
    arXiv:1810.02340*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023a] Jianwei Li, Weizhi Gao, Qi Lei, and Dongkuan Xu. Breaking
    through deterministic barriers: Randomized pruning mask generation and selection.
    *arXiv preprint arXiv:2310.13183*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023b] Jianwei Li, Qi Lei, Wei Cheng, and Dongkuan Xu. Towards robust
    pruning: An adaptive knowledge-retention pruning strategy for language models.
    *arXiv preprint arXiv:2310.13191*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lis et al. [2019] Mieszko Lis, Maximilian Golub, and Guy Lemieux. Full deep
    neural network training on a pruned weight budget. *Proceedings of Machine Learning
    and Systems*, 1:252–263, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2017] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter
    level pruning method for deep neural network compression. In *Proceedings of the
    IEEE international conference on computer vision*, pages 5058–5066, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marcus et al. [1993] Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    Building a large annotated corpus of english: The penn treebank. *Computational
    linguistics*, 19(2):313–330, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molchanov et al. [2019] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
    and Jan Kautz. Importance estimation for neural network pruning. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    11264–11272, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pool et al. [2021] Jeff Pool, Abhishek Sawarkar, and Jay Rodge. Accelerating
    inference with sparsity using the nvidia ampere architecture and nvidia tensorrt.
    *NVIDIA Developer Technical Blog, https://developer. nvidia. com/blog/accelerating-inference-with-sparsityusing-ampere-and-tensorrt*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasanna et al. [2020] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When
    BERT Plays the Lottery, All Tickets Are Winning. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 3208–3229,
    Online, November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.259.
    URL [https://aclanthology.org/2020.emnlp-main.259](https://aclanthology.org/2020.emnlp-main.259).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sietsma and Dow [1988] Sietsma and Dow. Neural net pruning-why and how. In *IEEE
    1988 international conference on neural networks*, pages 325–333\. IEEE, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh and Alistarh [2020] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thimm and Fiesler [1995a] Georg Thimm and Emile Fiesler. Evaluating pruning
    methods. In *Proceedings of the International Symposium on Artificial neural networks*,
    pages 20–25, 1995a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thimm and Fiesler [1995b] Georg Thimm and Emile Fiesler. Evaluating pruning
    methods. In *Proceedings of the International Symposium on Artificial neural networks*,
    pages 20–25, 1995b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Ouderaa et al. [2023] Tycho FA van der Ouderaa, Markus Nagel, Mart Van Baalen,
    Yuki M Asano, and Tijmen Blankevoort. The llm surgeon. *arXiv preprint arXiv:2312.17244*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning
    of large language models. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 6151–6162, Online, November
    2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.496.
    URL [https://aclanthology.org/2020.emnlp-main.496](https://aclanthology.org/2020.emnlp-main.496).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. [2023] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared
    llama: Accelerating language model pre-training via structured pruning. *arXiv
    preprint arXiv:2310.06694*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021] Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley,
    and Furu Wei. Beyond preserved accuracy: Evaluating loyalty and robustness of
    BERT compression. In *Proceedings of the 2021 Conference on Empirical Methods
    in Natural Language Processing*, pages 10653–10659, Online and Punta Cana, Dominican
    Republic, November 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.832.
    URL [https://aclanthology.org/2021.emnlp-main.832](https://aclanthology.org/2021.emnlp-main.832).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2018] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu,
    Xintong Han, Mingfei Gao, Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks
    using neuron importance score propagation. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 9194–9203, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu and Gupta [2017] Michael Zhu and Suyog Gupta. To prune, or not to prune:
    exploring the efficacy of pruning for model compression. *arXiv preprint arXiv:1710.01878*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6 Appendix-A
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pruning is a promising method that can effectively reduce model inference costs.
    In this paper, we discuss general pruning methods and various classification philosophies.
    We summarize previous work and categorize pruning from multiple perspectives:
    structured and unstructured, data-free and data-dependent, training-aware and
    inference-aware, and retraining-free and retraining-dependent. We also propose
    an innovative optimization-oriented view of pruning, which includes: A: Function
    Approximation, B: Output Approximation, and C: Objective Approximation. Our pruning
    pattern is designed based on this perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we review more aspects of pruning, including the most popular
    techniques in the pre-deep learning era (before 2022), such as Iterative Magnitude
    Pruning and the comparison between random pruning and magnitude pruning. We also
    discussed the relationship between pruning and quantization. By considering these
    various dimensions and methodologies, we aim to provide a comprehensive understanding
    of pruning and its potential to enhance model efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Iterative Magnitude Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Iterative Magnitude Pruning (IMP) is the most renowned strategy for achieving
    state-of-the-art results, surpassing other methods such as Single-shot Network
    Pruning (SNIP) Frankle and Carbin [[2019](#bib.bib16)], Frankle et al. [[2020](#bib.bib17)],
    Frantar and Alistarh [[2022](#bib.bib18)], Lee et al. [[2018](#bib.bib35)]. This
    approach divides the pruning process into multiple stages by gradually increasing
    the sparsity. At each stage, the goal is to identify and remove redundant parameters
    or neurons. The most intuitive approach is to assign an importance score to each
    element and keep only the top-k elements, where the score can be based on the
    absolute value of weights, output sensitivity, gradients, or other fine-designed
    metrics Hagiwara [[1993a](#bib.bib23)], Gale et al. [[2019](#bib.bib20)], Thimm
    and Fiesler [[1995a](#bib.bib53)], Han et al. [[2015](#bib.bib26)], Zhu and Gupta
    [[2017](#bib.bib64)], Cuadros et al. [[2020](#bib.bib10)], Luo et al. [[2017](#bib.bib39)].
    Weight magnitude is the most straightforward and data-free method, while other
    metrics can be computationally expensive as they require training with data Yu
    et al. [[2018](#bib.bib62)], He et al. [[2019](#bib.bib27)], Lis et al. [[2019](#bib.bib38)],
    Molchanov et al. [[2019](#bib.bib44)], Singh and Alistarh [[2020](#bib.bib50)],
    Dong et al. [[2017](#bib.bib11)]. Moreover, IMP is accompanied by a retraining
    phase to restore performance, which can be computationally costly. Therefore,
    in the era of colossal LLMs, IMP and other methods that heavily depend on model
    retraining are no longer effective due to the immense costs involved.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Randomized Pruning v.s. Magnitude Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Excluding the influence of model retraining, we discovered an interesting phenomenon
    for model pruning. For colossal LLMs such as LLaMA-7B, randomized pruning surprisingly
    produced competitive results. Specifically, compared to traditional data-free
    pruning metrics like L1 and L2 norm values, randomized pruning achieved several
    times better results, even rivaling data-dependent pruning methods. However, this
    advantage only existed when the pruning ratio was less than 2x. As the pruning
    ratio increased, magnitude pruning gradually yielded better results. Initially,
    we attributed this phenomenon to the high redundancy of parameters in LLMs. However,
    our experiments with GPT-2 showed that randomized pruning was still weaker than
    magnitude pruning. Therefore, we speculate that for colossal LLMs like Llama-7B,
    feature information plays a more crucial role in model activations compared to
    smaller LLMs like GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude-based pruning methods aim to remove weights or neurons from a neural
    network that appear least influential, primarily determined by the value of their
    weights. The rationale behind these methods is to reduce overall model size and
    computational requirements without a drastic loss in performance. However, several
    challenges arise with this approach, and one major challenge is the lack of variety
    if the magnitude is based on data-free metrics (L1 or L2). This kind of metric
    focuses solely on the magnitude of the weights for pruning decisions, potentially
    missing smaller weights that play pivotal roles, especially in edge cases or rarer
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this more clearly, consider the following example. The output
    of a neural network can be represented as $y=\sum(w_{i}\cdot f_{i})$. This example
    indicates that the influence of feature information plays a significant role in
    identifying redundant elements.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/963426c77631a8105d6ddb4d2b85966d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Mean activation value of Llama-7B and GPT-2 on Wikitext2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the above observation, we speculate that LLaMA-7B’s feature information
    contributes more to the importance score of removed elements when the pruning
    ratio is less than 2x. As the pruning ratio gradually increases, the influence
    of the features on the activation values is no longer greater than that of the
    weights. Therefore, randomized pruning fails at larger pruning ratios. To validate
    our hypothesis, we conducted a statistical analysis on the feature values of LLaMA-7B,
    described in Figure [5](#S6.F5 "Figure 5 ‣ 6.2 Randomized Pruning v.s. Magnitude
    Pruning ‣ 6 Appendix-A ‣ Greedy Output Approximation: Towards Efficient Structured
    Pruning for LLMs Without Retraining"). Our results show that colossal LLMs like
    Llama-7B have larger activation values than smaller LLMs like GPT-2\. These findings
    further motivate us to design the pruning metrics that incorporate both feature
    and weight information instead of seeking dataset competition.'
  prefs: []
  type: TYPE_NORMAL
- en: '6.3 Pruning v.s. Quantization:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pruning, though considered less effective than quantization in the era of colossal
    LLMs, should not be underestimated. In practice, pruning and quantization can
    complement each other, yielding significant benefits when applied together Frantar
    and Alistarh [[2022](#bib.bib18)]. Even pruning a small percentage of parameters,
    such as 5%, can be valuable if it meets practical performance requirements. Therefore,
    integrating pruning into the optimization process is always worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Appendix-B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Input or Output Channel Pruning for
    Transformer ‣ 3.1 Pruning Structure Recognition ‣ 3 Methodology ‣ Greedy Output
    Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining"),
    we introduced the 2nd moment-based pruning metric for a standard depth-2 module.
    However, there are different variants of depth-2 modules, including the attention
    module and the gated feed-forward module. We describe the calculation of the metric
    for these variants in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notations: To better demonstrate our method, let us first establish the notations.
    We focus on the pruning of Transformer-based large language models, thus we refer
    to the attention mechanism as $\mathbf{Cat}_{i=1}^{h}[\sigma_{1}(\mathbf{X}\mathbf{W}_{i}^{K}\mathbf{W}_{i}^{Q}\mathbf{X}^{\top})\mathbf{X}\mathbf{W}_{i}^{V}]\mathbf{W}^{O}$
    refers to the activation function for all of them, which can be SoftMax, ReLU,
    GeLU, or SiLU function.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Calculation of 2nd-Moment-Based Metric for Attention Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the above notations, we can treat the entire output of $\sigma_{1}(\mathbf{X}\mathbf{W}{i}^{K}\mathbf{W}{i}^{Q}\mathbf{X}^{\top})\mathbf{X}$
    is the number of attention heads), as the attention heads operate independently.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Calculation of 2nd-Moment-Based Metric for Gate Feed-forward Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the gated feedforward module $\mathbf{W}^{D}(\mathbf{W}^{U}\mathbf{X}\cdot\sigma_{2}(\mathbf{W}^{G}\mathbf{X}))$,
    and calculate the 2nd-moment metric separately for them. Finally, we use the product
    of their own metric as the 2nd-moment metric for the entire module.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these approaches allow us to effectively prune channels in both attention
    and gated feedforward modules by leveraging the 2nd moment-based metric.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Pre-Pruning Recovery for Attention Module.
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Attention ModuleModel layers with weights $\{k\_proj,q\_proj,v\_proj,o\_proj\}$18:end procedure'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Appendix-C
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [3.3](#S3.SS3 "3.3 Pre-Pruning Recovery Without Retraining ‣ 3 Methodology
    ‣ Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining"), we only provide the pre-pruning recovery algorithm for the standard
    depth-2 module, thus we describe the details of the recovery process for the attention
    module and gated feed-forward module in Algo [3](#alg3 "Algorithm 3 ‣ 7.2 Calculation
    of 2nd-Moment-Based Metric for Gate Feed-forward Module ‣ 7 Appendix-B ‣ Greedy
    Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining")
    and Algo [4](#alg4 "Algorithm 4 ‣ 8 Appendix-C ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 Pre-Pruning Recovery for Gate Feed-forward Module.
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Gated Feed-forward Module with weights $\{up\_proj,gate\_proj,down\_proj\}$16:end procedure'
  prefs: []
  type: TYPE_NORMAL
- en: '9 Appendix-D: Broader Impact'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our proposed method efficiently prunes large language models with billions of
    parameters. Our proposal intends to mitigate AI risks from critical perspectives
    like economic inequality and concentration of power and further democratize the
    use of AI models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The zero-shot performance of the compressed Vicuna-7B (20% sparsity).
    Following the LLM-Pruner methodology Ma et al. [[2023](#bib.bib40)], we only prune
    the transformer blocks from the 4th to the 30th. The average performance is calculated
    across seven classification datasets. ’Bold’ indicates the best pruning-only performance,
    while ’underline’ represents the overall best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Methods | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dense Touvron et al. [[2023a](#bib.bib55)], Ma et al. [[2023](#bib.bib40)]
    | 16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 | 65.11 | 41.21 | 40.80 | 62.78
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data Free Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Random Hoefler et al. [[2021](#bib.bib28)] | 34.63 | 112.44 | 61.47 | 70.89
    | 54.67 | 56.27 | 55.60 | 31.74 | 34.60 | 52.18 |'
  prefs: []
  type: TYPE_TB
- en: '| L2 norm Hoefler et al. [[2021](#bib.bib28)] | 3339.98 | 5882.21 | 55.90 |
    56.15 | 32.37 | 51.85 | 30.01 | 28.41 | 28.20 | 40.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours SG w/o remedy | 28.45 | 92.3 | 62.51 | 72.63 | 56.54 | 57.46 | 58.68
    | 33.29 | 36.2 | 53.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Dependent Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner Vec Ma et al. [[2023](#bib.bib40)] | 27.03 | 92.51 | 62.17 | 71.44
    | 55.80 | 53.43 | 55.77 | 33.28 | 37.80 | 52.81 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner E2 Ma et al. [[2023](#bib.bib40)] | 24.70 | 94.34 | 62.87 | 75.41
    | 64.00 | 58.41 | 60.98 | 37.12 | 39.00 | 56.83 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner E1 Ma et al. [[2023](#bib.bib40)] | 25.74 | 92.88 | 61.70 | 75.30
    | 63.75 | 56.20 | 63.22 | 36.60 | 37.00 | 56.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours (C) w/o remedy | 19.88 | 90.04 | 62.48 | 75.68 | 65.23 | 61.27 | 63.4
    | 35.49 | 37.6 | 57.31 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Dependent Pruning w/ Retraining |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner LoRA Ma et al. [[2023](#bib.bib40)] | 18.97 | 76.78 | 60.40 |
    75.63 | 65.45 | 63.22 | 63.05 | 37.71 | 39.00 | 57.78 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The zero-shot performance of the compressed Llama-13B (20% sparsity).
    The average performance is calculated across seven classification datasets. ’Bold’
    indicates the best pruning-only performance, while ’underline’ represents the
    overall best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pruning Methods | WikiText2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dense Touvron et al. [[2023a](#bib.bib55)], Ma et al. [[2023](#bib.bib40)]
    | 11.58 | 20.24 | 68.47 | 78.89 | 76.24 | 70.09 | 74.58 | 44.54 | 42.00 | 64.97
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data Free Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| Random Hoefler et al. [[2021](#bib.bib28)] | 19.24 | 31.84 | 63.33 | 73.18
    | 63.54 | 60.85 | 64.44 | 36.26 | 38.00 | 57.09 |'
  prefs: []
  type: TYPE_TB
- en: '| L2 norm Hoefler et al. [[2021](#bib.bib28)] | 61.15 | 91.43 | 61.50 | 67.57
    | 52.90 | 57.54 | 50.13 | 31.14 | 36.80 | 51.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours SG w/o remedy | 18.47 | 29.87 | 66.51 | 74.63 | 68.54 | 61.35 | 66.80
    | 36.26 | 38.41 | 58.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Dependent Pruning |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner Channel Ma et al. [[2023](#bib.bib40)] | 49.03 | 106.48 | 62.39
    | 66.87 | 49.17 | 58.96 | 49.62 | 31.83 | 33.20 | 50.29 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner E1 Ma et al. [[2023](#bib.bib40)] | 16.01 | 29.28 | 67.68 | 77.15
    | 73.41 | 65.11 | 68.35 | 38.40 | 42.40 | 61.79 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours (C) w/o remedy | 15.90 | 28.33 | 68.48 | 77.78 | 74.73 | 65.01 | 68.90
    | 39.40 | 43.11 | 62.48 |'
  prefs: []
  type: TYPE_TB
- en: '| Data Dependent Pruning w/ Retraining |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Pruner LoRA Ma et al. [[2023](#bib.bib40)] | 15.18 | 28.08 | 70.31 |
    77.91 | 75.16 | 67.88 | 71.09 | 42.41 | 43.40 | 64.02 |'
  prefs: []
  type: TYPE_TB
