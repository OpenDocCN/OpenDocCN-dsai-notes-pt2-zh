- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:06:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10121](https://ar5iv.labs.arxiv.org/html/2405.10121)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, and Hongfei Lin Bo Zhang, Hui
    Ma, Jian Ding, Jian Wang, Bo Xu and Hongfei Lin are with the School of Computer
    Science and Technology, Dalian University of Technology, Dalian 116024, China.
    (e-mail: zhangbo1998@mail.dlut.edu.cn; huima_cumt@163.com; 91mr_ding@mail.dlut.edu.cn;
    wangjian@dlut.edu.cn; xubo@dlut.edu.cn; hflin@dlut.edu.cn).Corresponding author:
    Jian Wang.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Integrating multimodal knowledge into large language models (LLMs) represents
    a significant advancement in dialogue generation capabilities. However, the effective
    incorporation of such knowledge in zero-resource scenarios remains a substantial
    challenge due to the scarcity of diverse, high-quality dialogue datasets. To address
    this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF),
    an innovative approach aimed at enhancing LLMs for enriched dialogue generation
    in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises
    two main stages: knowledge distillation, using an Implicit Query Transformer to
    extract and encode visual implicit knowledge from image-text pairs into knowledge
    vectors; and knowledge integration, employing a novel Bidirectional Variational
    Information Fusion technique to seamlessly integrate these distilled vectors into
    LLMs. This enables the LLMs to generate dialogues that are not only coherent and
    engaging but also exhibit a deep understanding of the context through implicit
    multimodal cues, effectively overcoming the limitations of zero-resource scenarios.
    Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms
    existing state-of-the-art models in generating high-quality dialogues. The code
    will be publicly available following acceptance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models, Multimodal knowledge, Zero resource, Dialogue generation.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dialogue generation, a pivotal component of natural language processing, aims
    to create responses that are both natural and engaging within specific dialogue
    contexts. The emergence of large language models (LLMs), such as the Generative
    Pre-trained Transformer (GPT) series [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    has marked significant advancements in this domain. These models excel in identifying
    complex linguistic patterns and semantic details due to their training on extensive
    textual datasets. However, their effectiveness is limited to text-based contexts,
    overlooking the rich, multimodal aspects of human dialogue that incorporate visual,
    auditory, and other sensory inputs. This limitation highlights a crucial challenge:
    enabling LLMs to navigate the multimodal nature of human interactions, a capability
    that humans possess inherently.'
  prefs: []
  type: TYPE_NORMAL
- en: The integration of multimodal knowledge into dialogue systems signifies a major
    progression towards more nuanced and human-like communication capabilities. It
    enables these systems to understand and interpret the nuances of human communication
    that transcend beyond mere text, capturing the essence of multimodal interactions
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Building upon this concept,
    there has been an increase in research focused on augmenting LLMs with multimodal
    knowledge [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)].
    This involves processing and understanding information across different modalities,
    such as images, videos, and audio, thereby equipping LLMs to perform tasks that
    necessitate cross-modal comprehension. While these developments significantly
    expand the capabilities of LLMs in engaging with multimodal content, challenges
    persist in effectively applying multimodal knowledge in dialogue generation, necessitating
    further exploration and innovation in this area.
  prefs: []
  type: TYPE_NORMAL
- en: One pivotal challenge in augmenting LLMs with multimodal capabilities, crucial
    for advancing human-like dialogue generation, is the scarcity of high-quality,
    diverse multimodal dialogue datasets. This is particularly notable in domains
    that demand intricate interactions, such as image-grounded dialogues [[11](#bib.bib11)].
    Image-grounded dialogues involve conversations anchored on a shared image, necessitating
    visual reasoning and a wealth of common sense to elicit coherent and engaging
    responses. Current datasets [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)],
    while foundational, often fall short in capturing the breadth and depth of human
    multimodal communication, resulting in models that may not effectively generalize
    to varied real-world interactions. Moreover, existing frameworks like ZRIGF [[14](#bib.bib14)],
    which represent pioneering efforts in zero-resource image-grounded dialogue generation,
    do not seamlessly integrate into LLM architectures and depend on retrieving relevant
    images during inference to formulate responses. Thus, the challenge of enabling
    LLMs to generate multimodal dialogues in zero-resource scenarios, devoid of annotated
    multimodal dialogue datasets, remains unresolved.
  prefs: []
  type: TYPE_NORMAL
- en: To address the primary challenge of enabling LLMs to generate dialogues in zero-resource
    scenarios, we introduce the concept of implicit multimodal knowledge. Unlike existing
    approaches, which predominantly utilize explicit multimodal inputs such as images
    or sounds presented during interactions, we center on implicit multimodal knowledge.
    This form of knowledge, significantly distinct from the explicit forms commonly
    integrated in current models, refers to a mental imagery or conceptual understanding
    individuals have developed through their experiences [[15](#bib.bib15), [16](#bib.bib16)].
    Implicit multimodal knowledge encompasses a broad spectrum of sensory, emotional,
    and contextual understandings that, although not directly observable, profoundly
    influence the nature of dialogues [[17](#bib.bib17), [18](#bib.bib18)]. By leveraging
    readily available large-scale image-text pair corpora to learn how to utilize
    implicit multimodal knowledge, it is possible to circumvent the issue of scarcity
    in high-quality, diverse multimodal dialogue datasets. However, current multimodal
    LLMs primarily engage with explicit multimodal inputs, focusing on direct responses
    to visual or auditory stimuli within dialogues [[19](#bib.bib19)], resulting in
    a lack of capability to incorporate such implicit knowledge. Therefore, the challenge
    becomes how to effectively distill and integrate implicit multimodal knowledge
    into LLMs, thereby significantly enhancing their ability to generate nuanced dialogues
    in zero-resource scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9eb89b254bd3bb46173dd7fcfcf52c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Intuition of our proposed approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To navigate this refined challenge, we propose the Visual Implicit Knowledge
    Distillation Framework (VIKDF), a novel framework that reimagines the integration
    of multimodal knowledge into LLMs, focusing on the distillation and incorporation
    of visual implicit knowledge in a zero-resource scenario. VIKDF operates in two
    synergistic stages: knowledge distillation and knowledge integration, as illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Distilling Implicit Multimodal
    Knowledge into LLMs for Zero-Resource Dialogue Generation"). In the knowledge
    distillation stage, VIKDF utilizes a multimodal model that incorporates a text
    encoder, an image encoder, and a novel querying transformer termed the Implicit
    Query Transformer (IQ-Former). This transformer, an advancement of the Q-Former
    [[7](#bib.bib7)], is specially tailored for extracting implicit knowledge. It
    employs a set of learnable query vectors to distill visual implicit knowledge
    from extensive image-text pair corpora. These vectors serve as the representation
    of the visual implicit knowledge, which can be then effectively integrated into
    LLMs. During the knowledge integration stage, we introduce a pioneering technique
    to seamlessly integrate the distilled visual implicit knowledge into LLMs, named
    Bidirectional Variational Information Fusion (BVIF). BVIF leverages an instruction-aware
    dual-pathway approach to maximize the mutual information between textual context
    and distilled visual implicit knowledge, thereby capturing the essence of the
    visual implicit knowledge. This simultaneous optimization ensures coherent, context-rich
    dialogues and bridges the gap between explicit and implicit multimodal knowledge
    processing. Consequently, VIKDF enables LLMs to engage in complex dialogues without
    depending on annotated multimodal datasets, marking a significant step forward
    in zero-resource dialogue generation.'
  prefs: []
  type: TYPE_NORMAL
- en: To validate our framework’s efficacy, we conducted comprehensive experiments
    on the Image-Chat [[20](#bib.bib20)] and Reddit Conversation datasets [[4](#bib.bib4)],
    benchmarking our method against several state-of-the-art baselines such as ChatGPT
    and ZRIGF. Through both automatic and human evaluations, VIKDF showcased its exceptional
    ability to fluently incorporate visual implicit knowledge into dialogues, thereby
    generating contextually rich, engaging, and coherent conversations, and outperforming
    existing models in zero-resource scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel framework that distills and integrates visual implicit knowledge
    into LLMs, enabling them to generate more engaging dialogues without relying on
    any explicit images in zero-resource scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop the Implicit Query Transformer and Bidirectional Variational Information
    Fusion techniques, effectively distilling and integrating visual implicit knowledge
    into LLMs and enhancing their dialogue generation capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive evaluations across two datasets in diverse scenarios, demonstrating
    the superior performance and robust generalization capabilities of VIKDF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Multimodal Dialogue Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multimodal dialogue generation aims to produce responses that are natural and
    engaging, considering inputs from multiple modalities such as images, videos,
    or audio. This field requires models that have the ability to understand or generate
    content across these different modalities, leveraging this multimodal knowledge
    to enrich dialogues. Early research in this area [[21](#bib.bib21), [22](#bib.bib22)]
    was primarily focused on multimodal question-answering, where the goal was to
    respond to queries with inputs from various modalities. However, there has been
    a noticeable shift towards generating open-domain dialogues. In these cases, multimodal
    inputs serve to enrich conversations rather than strictly guide them, leading
    to two main streams of research.
  prefs: []
  type: TYPE_NORMAL
- en: The first stream focuses on dialogue generation based on multimodal information.
    In this approach, models utilize inputs such as images or videos to influence
    the dialogue generation process. These models typically employ multimodal encoders
    or attention mechanisms to integrate multimodal features with textual features,
    thus enhancing the relevance and diversity of the generated responses [[4](#bib.bib4),
    [6](#bib.bib6), [23](#bib.bib23)]. This approach mimics face-to-face interactions
    where non-verbal cues influence but do not solely dictate the conversation. The
    second stream involves models that not only interpret multimodal inputs but also
    generate outputs across multiple modalities. This approach is akin to network-based
    dialogues, where communication often includes and sometimes relies on multimodal
    elements such as emojis, images, or videos. These models require more advanced
    capabilities for handling cross-modal generation and alignment, as well as ensuring
    the coherence and consistency of multimodal outputs [[24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: Our research aligns with the first stream, aiming to enhance dialogue using
    multimodal inputs without generating multimodal outputs. However, most existing
    methodologies rely on annotated multimodal dialogue data, which is both scarce
    and expensive to obtain. In an effort to bridge this gap, Zhang et al. [[14](#bib.bib14)]
    introduced a zero-resource image-grounded framework that leverages images to enrich
    dialogues through a two-stage learning strategy. While innovative, this method
    requires access to relevant images during inference, which may not always be feasible.
    Our proposed approach differs by utilizing implicit multimodal knowledge, distilled
    from extensive collections of image-text pairs, to enhance dialogue generation.
    This strategy addresses the challenges of data scarcity and modality mismatch,
    enabling the generation of dialogues that are more natural, contextually rich,
    and authentically human-like.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Multimodal Knowledge Distillation and Integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multimodal knowledge distillation and integration are crucial for enabling LLMs
    to utilize multimodal information in dialogue generation. Distillation involves
    extracting and compressing information from a broad spectrum of multimodal data,
    such as image-text pairs. Integration, on the other hand, focuses on incorporating
    this distilled information into LLMs, thereby augmenting their capacity for understanding
    and generating multimodal dialogues.
  prefs: []
  type: TYPE_NORMAL
- en: Prior research on this topic has predominantly concentrated on explicit multimodal
    information, such as the direct fusion of image and textual features [[5](#bib.bib5),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)] or employing pre-trained
    multimodal models [[29](#bib.bib29)] to extract multimodal representations [[19](#bib.bib19),
    [30](#bib.bib30)]. However, these methods face limitations when applied to LLMs.
    Li et al. [[7](#bib.bib7)] proposed an innovative solution, the Q-Former, which
    uses learnable query vectors to distill multimodal knowledge and integrate it
    into LLMs. Although this method has shown promising results in improving the multimodal
    capabilities of LLMs, it still encounters challenges in handling implicit multimodal
    knowledge, especially for dialogue generation.
  prefs: []
  type: TYPE_NORMAL
- en: Our work seeks to address a critical gap in multimodal dialogue generation by
    leveraging implicit multimodal knowledge to enhance LLMs’ dialogue generation
    capabilities in zero-resource scenarios. This not only advances the technology
    of multimodal dialogue generation but also provides new insights into the complexities
    of human conversational interactions.
  prefs: []
  type: TYPE_NORMAL
- en: III Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Task Formalization and Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The task of dialogue generation based on multimodal information is defined
    as generating a response $R$. This leads to the model formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(R&#124;C)=P(R,K&#124;C)=P(R&#124;C,K)P(K&#124;C)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: This formulation is justified as $K$, explaining the rationale behind the first
    part of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we introduce a two-stage framework comprising knowledge distillation
    and integration, as shown in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Distilling
    Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation").
    The framework is anchored by three principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Text Encoder and Image Encoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The text encoder and image encoder are responsible for encoding the textual
    and visual information from a large corpus of image-text pairs. We adopt the CLIP
    model [[29](#bib.bib29)] as our text and image encoder, which is a pre-trained
    multimodal model that learns to align image and text features in a unified latent
    space. The text encoder transforms text input $T$. The parameters of both encoders
    are set to remain unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26c76345b72be27902c18d17885d19ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The architecture of IQ-Former.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Implicit Query Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The IQ-Former is a specially designed querying transformer that distills visual
    implicit knowledge from encoded image-text pairs. As illustrated in Figure [2](#S3.F2
    "Figure 2 ‣ III-A1 Text Encoder and Image Encoder ‣ III-A Task Formalization and
    Model Architecture ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge
    into LLMs for Zero-Resource Dialogue Generation"), the IQ-Former is structured
    as a transformer encoder equipped with a cross-attention mechanism. Uniquely,
    its inputs are a set of learnable query vectors $\mathbf{Q}={\mathbf{q}_{1},\mathbf{q}_{2},...,\mathbf{q}_{n}}$.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Large Language Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The LLM is tasked with generating dialogue responses based on the dialogue context
    and the distilled visual implicit knowledge. We employ the Llama-2 model [[33](#bib.bib33)]
    as our LLM, a pre-trained autoregressive language model renowned for its capability
    to produce natural and varied text. The LLM takes the dialogue context $C$ into
    the LLM, we introduce a novel technique termed Bidirectional Variational Information
    Fusion, detailed further in Section [III-C](#S3.SS3 "III-C Knowledge Integration
    Stage ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge into LLMs for
    Zero-Resource Dialogue Generation"). The LLM’s parameters are also frozen during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a2d74b6f201bef89cfe7a38529c152d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of the knowledge distillation stage. The central part illustrates
    Text-Image Matching. Below the dashed line lies Text-Assisted Masked Image Modeling,
    and above, Image-Assisted Masked Text Modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Knowledge Distillation Stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of the knowledge distillation stage is to develop a model, denoted
    as $P(K|C)$ by using three objectives.
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Text-Image Matching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The text-image matching objective ensures the alignment of knowledge vectors
    $\mathbf{K}_{T}$ for matching pairs and minimizing the cosine similarity for non-matching
    pairs. The loss for text-image matching is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{tim}=-\sum_{i=1}^{N}($ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle+$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is a learnable temperature parameter that controls the distribution’s
    concentration. By minimizing this loss, the IQ-Former learns to generate knowledge
    vectors that are semantically similar to the corresponding image features, thereby
    encapsulating visual implicit knowledge from the text input.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Text-Assisted Masked Image Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The objective of text-assisted masked image modeling is to reconstruct the
    masked portions of an image input, denoted as $\mathbf{I}$ in the masked regions,
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{tamim}=\frac{1}{N_{i}}\sum_{i=1}^{N_{i}}&#124;\mathbf{I}_{i}-\hat{\mathbf{I}}_{i}&#124;$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $N_{i}$ being the pixel values of the original and reconstructed images,
    respectively. Minimizing this loss enables the IQ-Former to create knowledge vectors
    that contain sufficient visual implicit knowledge to assist the image reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 Image-Assisted Masked Text Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal of image-assisted masked text modeling is to recover the masked tokens
    in a text input $T$ is fed into a linear layer followed by a softmax function
    to predict the vocabulary’s probability distribution for each masked token. The
    loss for image-assisted masked text modeling, calculated as the cross-entropy
    loss between the predicted distribution and the true masked tokens, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{iamtm}=-\frac{1}{N_{t}}\sum_{i=1}^{N_{t}}\log P(T_{i}&#124;\mathbf{O}_{T}^{\prime})$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $N_{t}$. By minimizing this loss, the IQ-Former is trained to produce
    output vectors that contain sufficient cross-modal knowledge to assist the text
    reconstruction, thus capturing cross-modal knowledge alignment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The comprehensive loss function for the knowledge distillation stage is the
    sum of the losses from the three objectives, weighted by their respective importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{kd}=\lambda_{1}\mathcal{L}_{tim}+\lambda_{2}\mathcal{L}_{tamim}+\lambda_{3}\mathcal{L}_{iamtm}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1}$ for the LLM during the knowledge integration stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2cbf09e2f7bc4a51d5cfc15d55270dc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Overview of the knowledge integration stage. Instruction-aware Contextual
    Inference on the left and Instruction-aware Knowledge Reconstruction on the right'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Knowledge Integration Stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The objective of the knowledge integration stage is to train a model, denoted
    as $P(R|C,K)$. As depicted in Figure [4](#S3.F4 "Figure 4 ‣ III-B3 Image-Assisted
    Masked Text Modeling ‣ III-B Knowledge Distillation Stage ‣ III Methodology ‣
    Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation"), the LLM integrates visual implicit knowledge through a pioneering
    technique named Bidirectional Variational Information Fusion. BVIF utilizes an
    instruction-aware dual-pathway approach, with each path providing a distinct yet
    complementary mechanism for knowledge fusion.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Instruction-aware Contextual Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The instruction-aware contextual inference pathway aims to enable the LLM to
    decode and integrate the textual intricacies contained within the distilled visual
    implicit knowledge. This pathway introduces distilled visual implicit knowledge
    $K$ by optimizing the likelihood of predicting each subsequent token, based on
    preceding tokens, the query embeddings, and the text prompts. The instruction-aware
    contextual inference loss is formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{iaci}=-\frac{1}{N_{t}}\sum_{i=1}^{N_{t}}\log P(T_{i}&#124;K,P,T_{<i})$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $N_{t}$. Minimizing this loss instructs the LLM to produce text reflective
    of the visual implicit knowledge distilled by the IQ-Former from the image-text
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Instruction-aware Knowledge Reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The instruction-aware knowledge reconstruction pathway is a crucial component
    of the BVIF technique, aimed at augmenting the LLM’s proficiency in interpreting
    and utilizing distilled visual implicit knowledge $K$, thereby establishing a
    bidirectional information flow between the text and the visual implicit knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'A primary challenge in this task is to ensure that the knowledge $K$ through
    a variational information maximization approach, as detailed in [[35](#bib.bib35)].
    This approach is mathematically represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I(K,T)\geq\mathbb{E}_{p(K)}\mathbb{E}_{p(T&#124;K)}\log q_{\phi}(K&#124;T)$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $q_{\phi}(K|T)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we use the LLM as the function $q_{\phi}$ across all queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{iakr}=\frac{1}{N_{q}}\sum_{i=1}^{N_{q}}(\mathbf{K}_{i}-\hat{\mathbf{K}}_{i})^{2}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $N_{q}$ representing the original and reconstructed knowledge vectors,
    respectively. Minimizing this loss enables the LLM to produce knowledge vectors
    consistent with the generated text, thus reinforcing a bidirectional flow of information
    between the text and the visual implicit knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall loss function for the knowledge integration stage is the weighted
    sum of the two objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{ki}=\lambda_{4}\mathcal{L}_{iaci}+\lambda_{5}\mathcal{L}_{iakr}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{4}$ are hyperparameters that control the relative importance
    of each objective. By minimizing this loss, the LLM is trained to generate dialogue
    responses that are coherent and engaging based on both the dialogue context and
    the distilled visual implicit knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Zero-Resource Learning Detail
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-D1 Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To train our framework, we use four large-scale image-text datasets, including
    COCO Captions [[36](#bib.bib36)], CC3M [[37](#bib.bib37)], CC12M [[38](#bib.bib38)],
    and SBU [[39](#bib.bib39)]. We follow the same data processing and augmentation
    methods as BLIP-2 [[7](#bib.bib7)], which generates synthetic captions for web
    images using a pre-trained captioning model and a CLIP model. These datasets contain
    tens of millions of image-text pairs that cover a wide range of topics and scenarios,
    providing a rich source of visual implicit knowledge. We concurrently train both
    stages of our framework, employing image-text pair data in accordance with Equations
    ([5](#S3.E5 "In III-B3 Image-Assisted Masked Text Modeling ‣ III-B Knowledge Distillation
    Stage ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge into LLMs for
    Zero-Resource Dialogue Generation")) and ([9](#S3.E9 "In III-C2 Instruction-aware
    Knowledge Reconstruction ‣ III-C Knowledge Integration Stage ‣ III Methodology
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation")). For fine-tuning on specific tasks, we minimize the negative log-likelihood
    loss to optimize the probability $P(R|C,K)$, as detailed in Section [III-D2](#S3.SS4.SSS2
    "III-D2 Inference ‣ III-D Zero-Resource Learning Detail ‣ III Methodology ‣ Distilling
    Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation").
  prefs: []
  type: TYPE_NORMAL
- en: III-D2 Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To perform zero-resource inference, we leverage the trained IQ-Former and the
    LLM to produce dialogue responses that enriched with the visual implicit knowledge
    distilled from the dialogue context. Given a dialogue context $C$ by sampling
    from the probability distribution over the vocabulary. Since the LLM has learned
    to integrate the visual implicit knowledge into the dialogue generation, it can
    produce natural and engaging responses that are consistent with the visual context,
    even in the absence of any explicit multimodal inputs.
  prefs: []
  type: TYPE_NORMAL
- en: IV Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate the performance of our proposed framework, VIKDF,
    on the task of zero-resource dialogue generation. We benchmark VIKDF against a
    range of baselines and ablation models, conducting both automatic and human evaluations.
    Additionally, we present qualitative examples to demonstrate the effectiveness
    of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We employ two datasets to evaluate our framework: Image-Chat [[20](#bib.bib20)]
    and Reddit Conversation [[4](#bib.bib4)]. The Image-Chat dataset, crucial for
    validating VIKDF’s efficacy in zero-resource scenarios, is a large-scale image-grounded
    dialogue dataset featuring 202,000 dialogues across 202,000 images. Each dialogue
    comprises a single turn of context and response, with the latter influenced by
    the corresponding image. This dataset is divided into 186,782 training dialogues,
    5,000 validation dialogues, and 9,997 testing dialogues. The Reddit Conversation
    dataset is served to assess the performance of visual implicit knowledge. This
    dataset, sourced from the widely-used online forum, encompasses an extensive variety
    of dialogue topics and styles. It has been preprocessed to include 1,000,000 training
    dialogues, with an additional 20,000 for validation and 20,000 for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to [[14](#bib.bib14)], we use the text-based dialogue dataset Reddit
    Conversation to train the model’s foundational dialogue capabilities, and the
    image-grounded dialogue dataset Image-Chat to validate the model’s zero-resource
    dialogue generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The VIKDF implementation utilizes the Hugging Face Transformers library [[20](#bib.bib20)].
    For the text and image encoders, we initialize them with the pre-trained CLIP
    model that employs a ViT-L/14 Transformer architecture. The IQ-Former is initialized
    with the pre-trained BERT-base model, while the chat version of Llama-2 with 7B
    parameters is used as the large language model. To ensure seamless integration
    and information flow across models of varying dimensionalities, the necessary
    linear transformations are applied, which are not mentioned in the methodology.
  prefs: []
  type: TYPE_NORMAL
- en: VIKDF adopts a simultaneous training regimen for both knowledge distillation
    and integration stages. This strategy is executed on four NVIDIA RTX 4090 GPUs,
    utilizing a batch size of 128 across 100,000 training steps. For optimization,
    we employ mixed-precision training with bfloat16 and utilize the AdamW optimizer
    [[40](#bib.bib40)] with a learning rate of $1e^{-4}$ patches at a 0.6 ratio. For
    the image-assisted masked text modeling, the strategy involves a 0.15 mask ratio.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Baseline Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare VIKDF with the following baseline models:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seq2Seq [[41](#bib.bib41)], a foundational architecture for sequence-to-sequence
    processing that includes an encoder and a decoder equipped with Long Short-Term
    Memory (LSTM) units.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BART [[42](#bib.bib42)], a pre-eminent sequence-to-sequence pre-training model
    that utilizes a Transformer architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ImgVAE [[4](#bib.bib4)], which employs variational autoencoder technology to
    integrate visual information into dialogue generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maria [[5](#bib.bib5)], a visual experience-powered conversational agent that
    enriches dialogues with experiences from the visual world through a large-scale
    image index.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Llama-2 [[33](#bib.bib33)], the LLM serving as our framework’s backbone. It
    is a pre-trained autoregressive model capable of generating natural and diverse
    text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT [[2](#bib.bib2)] by OpenAI, which leverages the GPT architecture to
    generate human-like text responses, incorporating a mix of supervised and reinforcement
    learning techniques for dialogue systems. We use the gpt-3.5-turbo version in
    this paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZRIGF [[14](#bib.bib14)], a state-of-the-art model for zero-resource image-grounded
    dialogue generation that combines multimodal learning with a two-stage strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Among them, ImgVAE, Maria, and ZRIGF are multimodal models, whereas the rest
    are unimodal text-based models. We prompt Llama-2 following the approach outlined
    in the Hugging Face blog¹¹1[https://huggingface.co/blog/llama2](https://huggingface.co/blog/llama2),
    and employ ChatGPT in accordance with the methodology described in [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In accordance with [[5](#bib.bib5)] and [[14](#bib.bib14)], we use both automatic
    and human evaluation metrics to assess the performance of VIKDF and the baseline
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'For automatic evaluation, we employ the following metrics: (1) Perplexity (PPL)
    measures the model’s fluency, with lower values indicating better performance.
    (2) BLEU-1 [[43](#bib.bib43)] and ROUGE-L [[44](#bib.bib44)] evaluate the alignment
    of generated responses with human references, focusing on word-level accuracy
    and sequence similarity, respectively. (3) For semantic analysis, we employ Average,
    Extrema and Greedy metrics [[45](#bib.bib45)] to measure the cosine similarity
    between word embeddings of generated and reference texts, capturing semantic coherence.
    (4) Dis-1 and Dis-2 metrics [[46](#bib.bib46)] quantify the diversity of the model’s
    output by calculating the uniqueness of unigrams and bigrams, respectively, ensuring
    the model’s capability to produce varied and engaging responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For human evaluation, we engage three evaluators to collect ratings from human
    annotators. We randomly sample 100 dialogues from the test set, and ask three
    evaluators to rate each dialogue on a scale of 1 to 5, based on the following
    criteria: (1) Relevance: How relevant and related is the generated response to
    the given context and image? (2) Informativeness: How much new and useful information
    does the generated response provide in the context of the dialogue? (3) Fluency:
    How natural, readable, and grammatically correct is the generated response? The
    final score for each criterion is computed as the average rating across all evaluators.
    To ensure the reliability of the evaluation process and measure the agreement
    among evaluators, Fleiss’ Kappa [[47](#bib.bib47)] statistic is applied to evaluate
    the concordance among evaluators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Assessment of automated metrics: ${\dagger}$ indicates a fully zero-resource
    scenario without any prior training on task-specific datasets. Bold font highlights
    the best performance in each column, and underlines signify the second-best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Methods | PPL | BLEU-1 | ROUGE-L | Average | Extrema | Greedy |     
    Dis-1 | Dis-2 |'
  prefs: []
  type: TYPE_TB
- en: '| Reddit Conversation | Seq2Seq | $77.27$ |'
  prefs: []
  type: TYPE_TB
- en: '| BART | $44.73$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2^‡ | $155.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT^‡ | - | $11.62$ | 38.63 |'
  prefs: []
  type: TYPE_TB
- en: '| ImgVAE | $72.06$ |'
  prefs: []
  type: TYPE_TB
- en: '| Maria | $56.23$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | ZRIGF | 36.21 | 16.06 | 14.51 | $82.27$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | VIKDF | 15.01 | 16.41 | 14.82 | 82.53 | 43.71 | 64.88 |     6.39 | 35.84
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image-Chat | Seq2Seq^† | $50.82$ |'
  prefs: []
  type: TYPE_TB
- en: '| BART^† | $37.26$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2^‡ | $193.20$ |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT^‡ | - | $10.77$ | 37.77 |'
  prefs: []
  type: TYPE_TB
- en: '| ImgVAE | $41.94$ |'
  prefs: []
  type: TYPE_TB
- en: '| Maria^† | $37.49$ |'
  prefs: []
  type: TYPE_TB
- en: '| Maria${}_{zero}^{\ddagger}$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZRIGF^† | $29.82$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZRIGF${}_{1/4}^{\dagger}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | ZRIGF${}_{zero}^{\ddagger}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | VIKDF^† | 12.56 | 17.92 | 17.33 | 86.47 | 50.83 | 68.45 | $4.61$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1/4 Data^† | 12.84 | 17.81 | $16.91$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1/8 Data^† | $13.12$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zero Data^‡ | $27.32$ |     6.17 | 32.93 |'
  prefs: []
  type: TYPE_TB
- en: V Results and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our evaluations differentiate between models operating in distinct zero-resource
    scenarios, as denoted by ${\dagger}$ denotes a fully zero-resource condition,
    in which models generate dialogues without any prior training on task-specific
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Automatic Evaluation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our proposed VIKDF demonstrates outstanding performance in zero-resource dialogue
    generation, outperforming both traditional and multimodal baseline models across
    various metrics on the Reddit Conversation and Image-Chat datasets. Table [I](#S4.T1
    "TABLE I ‣ IV-D Evaluation Metrics ‣ IV Experiments ‣ Distilling Implicit Multimodal
    Knowledge into LLMs for Zero-Resource Dialogue Generation") presents a comprehensive
    comparison of the automated evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the Reddit Conversation task, VIKDF achieves a significantly lower perplexity
    of 15.01, indicating superior fluency in generated dialogues compared to the nearest
    competitor, ZRIGF, which records a PPL of 36.21 in scenarios with explicit image
    guidance. Moreover, VIKDF outperforms all baselines in BLEU-1 and ROUGE-L scores,
    achieving 16.41 and 14.82, respectively, which highlights its ability to generate
    responses that are closely aligned with human references. Additionally, VIKDF
    surpasses other models in both Average and Greedy semantic similarity metrics,
    underscoring its enhanced ability to maintain semantic coherence in dialogues.
    Despite scoring slightly lower on the Extrema metric compared to ZRIGF, VIKDF
    remains competitive. Although it has a marginally lower Dis-2 score than ChatGPT,
    VIKDF’s strong performance in generating diverse dialogues is evident. This suggests
    that while maintaining high relevance and accuracy, VIKDF also generates a wide
    range of responses, contributing to more dynamic and engaging dialogues. The outstanding
    performance of VIKDF in the Reddit Conversation task highlights the substantial
    benefits of incorporating implicit knowledge into purely text-based dialogue systems,
    proving that a deep fusion of visual and contextual understanding significantly
    enhances the quality and engagement of the dialogues generated.
  prefs: []
  type: TYPE_NORMAL
- en: In the Image-Chat task, VIKDF’s capabilities are examined under various conditions,
    including limited data availability scenarios (1/4 and 1/8 of the full training
    set) and fully zero-resource scenarios. We can see that VIKDF showcases superior
    performance in various zero-resource scenarios, especially notable in the absence
    of annotated images (${\dagger}$), showcasing its exceptional capability to generate
    diverse, relevant, and engaging dialogues without task-specific training data.
    The achievements of VIKDF on the Image-Chat dataset affirm its leading position
    in zero-resource dialogue generation, illustrating unmatched adaptability and
    advanced integration of visual implicit knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Human evaluation outcomes for the Image-Chat dataset in a fully zero-resource
    scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Relevance | Informativeness | Fluency | Kappa |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2 | $2.94$ |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | $3.22$ |'
  prefs: []
  type: TYPE_TB
- en: '| ZRIGF |     3.55 | $3.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| VIKDF |     3.84 |     3.30 |     4.34 | $0.59$ |'
  prefs: []
  type: TYPE_TB
- en: V-B Human Evaluation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [II](#S5.T2 "TABLE II ‣ V-A Automatic Evaluation Results ‣ V Results and
    Discussion ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource
    Dialogue Generation") presents the human evaluation outcomes for the Image-Chat
    dataset within a fully zero-resource scenario (${\ddagger}$), comparing the performance
    of VIKDF against Llama-2, ChatGPT, and ZRIGF. Notably, VIKDF achieves the highest
    scores in both Relevance and Informativeness. This indicates its exceptional ability
    to generate dialogues that are not only closely related to the given context and
    images but also rich in informative content. Although ChatGPT receives the highest
    rating in Fluency, VIKDF remains competitive, with a score of 4.34, illustrating
    its capacity to produce responses that are natural, coherent, and grammatically
    correct. This demonstrates that VIKDF can generate dialogues which are both contextually
    relevant and engaging, with a high degree of linguistic quality. The Fleiss’ Kappa
    score, indicative of inter-rater agreement, falls within a moderate range across
    all models, ensuring that the evaluation process maintains a degree of reliability
    despite its inherently subjective nature.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, these results affirm VIKDF’s exemplary performance in zero-resource
    dialogue generation, particularly its adeptness at leveraging visual implicit
    knowledge to augment the relevance and informativeness of dialogues, while maintaining
    a high standard of fluency.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Ablation study. Zero Data means in a fully zero-resource scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Reddit Conversation | Image-Chat (Zero Data) |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU-1 | ROUGE-L | BLEU-1 | ROUGE-L |'
  prefs: []
  type: TYPE_TB
- en: '| VIKDF | 16.41 | 14.82 | 15.35 | 15.30 |'
  prefs: []
  type: TYPE_TB
- en: '| -TIM | $14.75$ |'
  prefs: []
  type: TYPE_TB
- en: '| -TAMIM | $16.03$ |'
  prefs: []
  type: TYPE_TB
- en: '| -IAMTM | $16.18$ |'
  prefs: []
  type: TYPE_TB
- en: '| -BVIF | $15.65$ |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/4ee9f9583ad2da47fc9f4a5fd96c48ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Case study on Image-Chat test set in a fully zero-resource scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the individual contributions of the components within VIKDF, we
    conduct an ablation study by sequentially removing each component and assessing
    the impact on performance metrics (BLEU-1 and ROUGE-L) across both Reddit Conversation
    and Image-Chat datasets. In the absence of BVIF, we adopt the vision-to-language
    generative learning as outlined in [[7](#bib.bib7)]. The results, detailed in
    Table [III](#S5.T3 "TABLE III ‣ V-B Human Evaluation Results ‣ V Results and Discussion
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation"), demonstrate a consistent decline in performance upon the exclusion
    of any component, underscoring their collective importance to the framework’s
    efficacy. Notably, the exclusion of TIM yields the most pronounced decline in
    performance metrics. This indicates TIM’s critical function in maintaining alignment
    between textual and visual modalities, a foundational aspect of generating coherent
    and contextually relevant dialogues. Additionally, omitting BVIF leads to a noticeable
    dip in performance metrics. This underscores BVIF’s importance in seamlessly integrating
    distilled visual knowledge into the dialogue generation process, further enhancing
    the model’s ability to produce contextually rich and engaging dialogues. The removal
    of TAMIM and IAMTM also leads to decreased performance. This result highlights
    their significance in enriching the model’s capability to infer and align multimodal
    knowledge, thereby facilitating a more nuanced dialogue generation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd94ef90f4248f48905163d829614a8b.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cbf3f3cf1d04eab8dc1da67d95c88b14.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Performance comparison with various numbers of queries.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d803cb508da47f12e8565e163ca5c561.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Example of visual implicit knowledge textualized by VIKDF.'
  prefs: []
  type: TYPE_NORMAL
- en: V-D Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further illustrate the superior capabilities of VIKDF, we conduct a case
    study contrasting VIKDF against key baseline models by examining a specific example
    from the Image-Chat test set in a fully zero-resource scenario, as shown in Figure
    [5](#S5.F5 "Figure 5 ‣ V-B Human Evaluation Results ‣ V Results and Discussion
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation"). In a dialogue context that evokes curiosity and apprehension towards
    the sky, VIKDF leverages distilled visual implicit knowledge to generate a response
    that captures both the awe of the sky’s vast beauty and the curiosity towards
    its unknown mysteries. In comparison, Llama-2 produces a relevant but less informative
    response. ChatGPT, while producing a context-aware response, misses the mark on
    integrating the emotive complexity of the conversation, focusing instead on extracting
    additional context from the user. This demonstrates the limitations of LLMs in
    multimodal dialog generation scenarios. In contrast, ZRIGF’s response, though
    creative, diverges from the contextual context due to its reliance on retrieved
    explicit images, whereas VIKDF overcomes this by distilling visual implicit knowledge
    from text.
  prefs: []
  type: TYPE_NORMAL
- en: To provide a more detailed analysis of visual implicit knowledge, we attempt
    to textualize the visual implicit knowledge through LLMs, following the process
    illustrated in Figure [7](#S5.F7 "Figure 7 ‣ V-C Ablation Study ‣ V Results and
    Discussion ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource
    Dialogue Generation"). The process involves transforming the dialogue context
    through IQ-Former that distills visual implicit knowledge, subsequently textualizing
    it via an LLM by a text prompt. We can see that the textual output illustrates
    the model’s capacity to encapsulate complex human emotions and curiosities about
    the sky. This demonstrates the model’s adeptness at integrating and expressing
    visual implicit knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis underscores VIKDF’s superior ability to synthesize and leverage
    visual implicit knowledge, enabling it to generate dialogues that are more engaging,
    visually grounded, and contextually appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Impact of Query Vector Quantity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploring the influence of the hyper-parameter $n$) could introduce unnecessary
    noise or dilute the relevance of the distilled knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment highlights the importance of a balanced query vector count in
    achieving effective dialogue generation. An optimal $n$ allows the IQ-Former to
    distill relevant visual implicit knowledge without overcomplicating the model,
    demonstrating the delicate balance between quantity and quality of distilled knowledge
    for enhancing dialogue generation.
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present VIKDF, an innovative methodology aimed at enhancing
    LLMs for dialogue generation in zero-resource scenarios through the distillation
    and integration of implicit multimodal knowledge. VIKDF utilizes an IQ-Former
    to extract visual implicit knowledge and a BVIF technique to incorporate this
    knowledge into LLMs, enabling the generation of dialogues that are coherent, engaging,
    and rich in contextual understanding. Our comprehensive experiments across diverse
    dialogue datasets have shown that VIKDF outperforms existing state-of-the-art
    models in zero-resource scenarios, illustrating its effectiveness in leveraging
    implicit multimodal knowledge even without explicit multimodal inputs or annotated
    datasets. The ablation study underscores the indispensable role of each component
    within VIKDF, and human evaluations have confirmed its success in generating dialogues
    that are relevant, informative, and naturally fluent, closely aligning human conversational
    standards. Consequently, VIKDF represents a significant advancement in the field
    of multimodal dialogue generation, highlighting the importance of implicit multimodal
    knowledge in enhancing LLMs capabilities in zero-resource scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed model utilizes only implicit multimodal information, which limits
    its applicability in tasks requiring explicit multimodal inputs, such as visual
    question answering, and multimodal outputs, such as text-to-image generation.
    In future work, we plan to integrate both explicit and implicit multimodal information
    to develop a dialogue generation system capable of supporting both multimodal
    inputs and outputs. This advancement will enable our model to engage more comprehensively
    with various types of content, potentially enhancing its performance and applicability
    in multimodal interaction scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is supported by the National Natural Science Foundation of China
    (No. 62006034).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
    R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
    M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems*, vol. 33, 2020, pp. 1877–1901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] OpenAI. (2022, nov) Introducing ChatGPT. [Online]. Available: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] ——, “GPT-4 technical report,” 2023, *arXiv:2303.08774*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Z. Yang, W. Wu, H. Hu, C. Xu, W. Wang, and Z. Li, “Open domain dialogue
    generation with latent images,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 35, no. 16, 2021, pp. 14 239–14 247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang, and D. Jiang,
    “Maria: A visual experience powered conversational agent,” in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 5596–5611.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Shen, H. Zhan, X. Shen, Y. Song, and X. Zhao, “Text is not enough: Integrating
    visual impressions into open-domain dialogue generation,” in *Proceedings of the
    29th ACM International Conference on Multimedia*, 2021, pp. 4287–4296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models,” in *Proceedings
    of the 40th International Conference on Machine Learning*, vol. 202, 2023, pp.
    19 730–19 742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour,
    R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi,
    “Photorealistic text-to-image diffusion models with deep language understanding,”
    in *Advances in Neural Information Processing Systems*, vol. 35, 2022, pp. 36 479–36 494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, “Video-chatgpt: Towards detailed
    video understanding via large vision and language models,” 2023, *arXiv:2306.05424*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. Ghosal, N. Majumder, A. Mehrish, and S. Poria, “Text-to-audio generation
    using instruction guided latent diffusion model,” in *Proceedings of the 31st
    ACM International Conference on Multimedia*, 2023, pp. 3590–3598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] N. Mostafazadeh, C. Brockett, B. Dolan, M. Galley, J. Gao, G. Spithourakis,
    and L. Vanderwende, “Image-grounded conversations: Multimodal context for natural
    question and response generation,” in *Proceedings of the Eighth International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, 2017,
    pp. 462–472.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] K. Shuster, S. Humeau, A. Bordes, and J. Weston, “Image-chat: Engaging
    grounded conversations,” in *Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics*, 2020, pp. 2414–2429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Zheng, G. Chen, X. Liu, and J. Sun, “MMChat: Multi-modal chat dataset
    on social media,” in *Proceedings of the Thirteenth Language Resources and Evaluation
    Conference*, 2022, pp. 5778–5786.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] B. Zhang, J. Wang, H. Ma, B. Xu, and H. Lin, “ZRIGF: An innovative multimodal
    framework for zero-resource image-grounded dialogue generation,” in *Proceedings
    of the 31st ACM International Conference on Multimedia*, 2023, pp. 5464–5473.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. Richardson, *Mental Imagery*.   Berlin, Heidelberg: Springer, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] R. Jackendoff, *Foundations of Language: Brain, Meaning, Grammar, Evolution*.   Oxford
    University Press, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. M. Kosslyn, W. L. Thompson, and G. Ganis, *The Case for Mental Imagery*.   Oxford
    University Press, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] D. Roy, K.-Y. Hsiao, and N. Mavridis, “Mental imagery for a conversational
    robot,” *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)*,
    vol. 34, no. 3, pp. 1374–1383, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Y. Koh, R. Salakhutdinov, and D. Fried, “Grounding language models
    to images for multimodal inputs and outputs,” in *Proceedings of the 40th International
    Conference on Machine Learning*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma,
    Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush,
    “Transformers: State-of-the-art natural language processing,” in *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, 2020, pp. 38–45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh,
    and D. Batra, “Visual dialog,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 326–335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Alamri, V. Cartillier, A. Das, J. Wang, A. Cherian, I. Essa, D. Batra,
    T. K. Marks, C. Hori, P. Anderson *et al.*, “Audio visual scene-aware dialog,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 7558–7567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang, and D. Jiang,
    “Maria: A visual experience powered conversational agent,” in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 5596–5611.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Feng, Q. Sun, C. Xu, P. Zhao, Y. Yang, C. Tao, D. Zhao, and Q. Lin,
    “Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal open-domain
    conversation,” 2022, *arXiv:2211.05719*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J. Y. Koh, R. Salakhutdinov, and D. Fried, “Grounding language models
    to images for multimodal inputs and outputs,” in *International Conference on
    Machine Learning*, 2023, pp. 17 283–17 300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Q. Sun, Y. Wang, C. Xu, K. Zheng, Y. Yang, H. Hu, F. Xu, J. Zhang, X. Geng,
    and D. Jiang, “Multimodal dialogue response generation,” in *Proceedings of the
    60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, 2022, pp. 2854–2866.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K.
    Mohammed, S. Singhal, S. Som, and F. Wei, “Image as a foreign language: Beit pretraining
    for vision and vision-language tasks,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2023, pp. 19 175–19 186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] X. Xue, C. Zhang, Z. Niu, and X. Wu, “Multi-level attention map network
    for multimodal sentiment analysis,” *IEEE Transactions on Knowledge and Data Engineering*,
    vol. 35, no. 5, pp. 5105–5118, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable
    visual models from natural language supervision,” in *Proceedings of the 38th
    International Conference on Machine Learning*, vol. 139, 2021, pp. 8748–8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] W. Dai, L. Hou, L. Shang, X. Jiang, Q. Liu, and P. Fung, “Enabling multimodal
    generation on CLIP via vision-language knowledge distillation,” in *Findings of
    the Association for Computational Linguistics: ACL 2022*, 2022, pp. 2383–2395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in neural
    information processing systems*, 2017, pp. 5998–6008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *International Conference
    on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” 2023, *arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, 2019,
    pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel,
    “Infogan: Interpretable representation learning by information maximizing generative
    adversarial nets,” in *Advances in Neural Information Processing Systems*, D. Lee,
    M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Computer Vision
    – ECCV 2014*, 2014, pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions:
    A cleaned, hypernymed, image alt-text dataset for automatic image captioning,”
    in *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*.   Melbourne, Australia: Association for
    Computational Linguistics, Jul. 2018, pp. 2556–2565.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m: Pushing
    web-scale image-text pre-training to recognize long-tail visual concepts,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2021, pp. 3558–3568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] V. Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using
    1 million captioned photographs,” in *Advances in Neural Information Processing
    Systems*, vol. 24, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
    in *International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Advances in Neural Information Processing Systems*,
    Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “BART: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” in *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*, 2020, pp. 7871–7880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
    evaluation of machine translation,” in *Proceedings of the 40th Annual Meeting
    of the Association for Computational Linguistics*, 2002, pp. 311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in
    *Text Summarization Branches Out*, 2004, pp. 74–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C.-W. Liu, R. Lowe, I. Serban, M. Noseworthy, L. Charlin, and J. Pineau,
    “How NOT to evaluate your dialogue system: An empirical study of unsupervised
    evaluation metrics for dialogue response generation,” in *Proceedings of the 2016
    Conference on Empirical Methods in Natural Language Processing*, 2016, pp. 2122–2132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity-promoting
    objective function for neural conversation models,” in *Proceedings of the 2016
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, 2016, pp. 110–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. L. Fleiss and J. Cohen, “The equivalence of weighted kappa and the
    intraclass correlation coefficient as measures of reliability,” *Educational and
    psychological measurement*, vol. 33, no. 3, pp. 613–619, 1973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
