- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:05:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint-based Progressive Chain-of-Thought Distillation for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16064](https://ar5iv.labs.arxiv.org/html/2405.16064)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kaituo Feng    Changsheng Li    Xiaolu Zhang    Jun Zhou    Ye Yuan    Guoren
    Wang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chain-of-thought distillation is a powerful technique for transferring reasoning
    abilities from large language models (LLMs) to smaller student models. Previous
    methods typically require the student to mimic the step-by-step rationale produced
    by LLMs, often facing the following challenges: (i) Tokens within a rationale
    vary in significance, and treating them equally may fail to accurately mimic keypoint
    tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently
    predicting all the steps in a rationale, which falls short in distinguishing the
    learning order of step generation. This diverges from the human cognitive progression
    of starting with easy tasks and advancing to harder ones, resulting in sub-optimal
    outcomes. To this end, we propose a unified framework, called KPOD, to address
    these issues. Specifically, we propose a token weighting module utilizing mask
    learning to encourage accurate mimicry of keypoint tokens by the student during
    distillation. Besides, we develop an in-rationale progressive distillation strategy,
    starting with training the student to generate the final reasoning steps and gradually
    extending to cover the entire rationale. To accomplish this, a weighted token
    generation loss is proposed to assess step reasoning difficulty, and a value function
    is devised to schedule the progressive distillation by considering both step difficulty
    and question diversity. Extensive experiments on four reasoning benchmarks illustrate
    our KPOD outperforms previous methods by a large margin.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated remarkable reasoning capabilities
    via chain-of-thought (CoT) prompting (e.g., “Let’s think step-by-step”), which
    prompts LLMs to generate a step-by-step rationale to help reasoning (Kojima et al.,
    [2022](#bib.bib23); Wei et al., [2022](#bib.bib44)). However, such abilities usually
    emerge in extremely large models, especially those with over 100 billion parameters
    (Fu et al., [2023](#bib.bib14); Hoffmann et al., [2022](#bib.bib17)) , such as
    175B GPT-3 (Brown et al., [2020](#bib.bib5)) and 540B PaLM (Chowdhery et al.,
    [2023](#bib.bib7)). The substantial amount of parameters unavoidably leads to
    high inference costs and makes it challenging to deploy LLMs in environments with
    limited computational resources (Hsieh et al., [2023](#bib.bib18)). To tackle
    with this, a recent surge of works, known as CoT distillation, has arisen as a
    promising avenue to distill reasoning capabilities of LLMs to smaller student
    models (Li et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib41); Fu et al.,
    [2023](#bib.bib14)). The core idea of these methods is to require the student
    model to mimic the step-by-step rationale generated by LLMs in response to a question.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, current CoT distillation methods often encounter the following two
    issues: First, in a rationale, each token carries different levels of importance
    in the reasoning process. Certain keypoint tokens play a pivotal role in reasoning,
    while other tokens are of less importance or even irrelevant to the reasoning
    process. For instance, consider a step in a rationale: “Next, we just need to
    simply add up the calories from the lettuce and cucumber: 30 + 80 = 110”. Here,
    terms like “just”, “simply” are reasoning-irrelevant, whereas the calculation
    “30 + 80 = 110” stands out as the keypoint for reasoning. The reasoning-irrelevant
    tokens can be replaced without negative effects, but even a slight deviation from
    the keypoint token could result in errors in reasoning. Therefore, it’s crucial
    for the student model to focus on the precise mimicry of these keypoint tokens.
    Nevertheless, previous CoT distillation methods usually treat all tokens equally
    during distillation (Li et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib41)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second issue stems from the fact that previous approaches usually demand
    the student model to consistently learn all the steps in a rationale throughout
    the distillation process, without distinguishing the learning order of step generation.
    This distillation strategy diverges from the human cognitive pattern that progresses
    from easier tasks to more challenging ones. This deviation might lead to sub-optimal
    outcomes. In the process of human or biological agent learning, ability acquisition
    doesn’t simply stem from random tasks (Molina & Jouen, [1998](#bib.bib32)). Instead,
    there is an organized progression from easy tasks to hard tasks for them to acquire
    capabilities, especially for complex skills such as reasoning (Peterson, [2004](#bib.bib36);
    Krueger & Dayan, [2009](#bib.bib25); Benoit et al., [2013](#bib.bib2)). In the
    field of machine learning, this ordered learning paradigm is regarded as curriculum
    learning (Bengio et al., [2009](#bib.bib1)). Inspired by this, we intend to develop
    a progressive CoT distillation strategy to facilitate the student model acquire
    reasoning ability from easy to hard. However, directly applying previous curriculum
    learning strategies to CoT distillation could be inferior because of the following
    two reasons: (i) They overlook the step-by-step reasoning nature where each reasoning
    step within a rationale may possess varying reasoning difficulty, resulting in
    sub-optimal difficulty assessment. (ii) As aforementioned, a step in the rationale
    might contain many tokens that are not crucial to the reasoning process. When
    assessing the difficulty of step generation, it may be dominated by these inessential
    tokens, thereby inaccurately reflecting the challenge of obtaining the expected
    outcome for a reasoning step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose Keypoint-based Progressive CoT Distillation for LLMs
    dubbed KPOD, with the goal of addressing the above two issues in a unified framework.
    First, we propose a rationale token weighting module to determine the token significance
    for distillation. It learns to generate masks for inessential tokens to the reasoning
    process via two distinctive loss functions: An answer prediction loss is introduced
    to encourage the module to utilize the question with the masked rationale to derive
    the answer, while a mask ratio loss is designed to maximize the ratio of masked
    tokens in the rationale. By doing so, the obtained probability of not masking
    a token can serve as an indicator of its significance weight. Second, we develop
    an in-rationale progressive distillation strategy that orders the learning sequence
    from easy reasoning to hard reasoning within the rationale of a question. This
    strategy begins by training the student model to generate the last few reasoning
    steps of the rationale, given the question with preceding steps of this rationale
    as input. Subsequently, it progressively extends to generate the entire rationale
    using only the question as input. To precisely assess each step’s reasoning difficulty,
    we propose a token generation loss based on the derived token significance, aiming
    to eliminate the negative effects of reasoning-irrelevant tokens. Finally, we
    design a value function to dynamically determine the number of steps taken as
    input at each stage, thereby automatically adjusting their learning difficulty.
    Meanwhile, we leverage the value function to select diverse questions, so as to
    prevent over-fitting (Jiang et al., [2014](#bib.bib22); Liang et al., [2021](#bib.bib28)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized as: 1) We propose a general and principled
    framework for CoT distillation, which simultaneously considers token significance
    and reasoning difficulty within a rationale during distillation. 2) We design
    a rationale token weighting module through mask learning to determine the token
    significance for reasoning. This allows the student to concentrate more on keypoint
    tokens. 3) We devise an in-rationale progressive CoT distillation strategy to
    schedule the learning order of reasoning steps within a rationale. This enables
    the student to progressively acquire reasoning abilities in an easy-to-hard manner.
    4) Extensive experiments on four reasoning benchmarks validate the effectiveness
    of our KPOD, showcasing significant performance improvements compared to baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chain-of-Thought Reasoning. The concept of employing step-by-step language rationales
    to aid in solving reasoning problems can be traced back to pioneering works (Ling
    et al., [2017](#bib.bib29)). Inspired by this, chain-of-thought prompting (Wei
    et al., [2022](#bib.bib44)) has been proposed to enable LLMs to generate intermediate
    reasoning steps that contribute to the final answer via few-shot CoT demonstrations.
    This prompting approach has illustrated remarkable performance gain for LLMs in
    reasoning related tasks (Zhang et al., [2022](#bib.bib47); Wang et al., [2023a](#bib.bib40)).
    In addition, researchers find that LLMs can also obtain impressive reasoning performance
    by zero-shot CoT (Kojima et al., [2022](#bib.bib23)) without task-related demonstrations.
    This is achieved by only using a single sentence “Let’s think step by step” for
    prompting. Recently, a number of CoT prompting methods have demonstrated effectiveness
    in enhancing the reasoning performance of LLMs (Diao et al., [2023](#bib.bib10);
    Yang et al., [2023](#bib.bib45)), such as SC-CoT (Wang et al., [2022](#bib.bib42)),
    Auto-CoT (Zhang et al., [2022](#bib.bib47)), Multimodal-CoT (Zhang et al., [2023](#bib.bib48)),
    etc. However, the emergence of CoT reasoning capabilities in LLMs typically requires
    models with more than 100 billion parameters (Wei et al., [2022](#bib.bib44);
    Fu et al., [2023](#bib.bib14)), making it resource-consuming for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'CoT Distillation. Knowledge distillation has been widely studied for model
    compression across various fields (Magister et al., [2023](#bib.bib30); Feng et al.,
    [2024](#bib.bib13)). Recently, CoT Distillation has emerged as a promising avenue
    to transfer the step-by-step reasoning capabilities of LLMs to smaller student
    models (Hsieh et al., [2023](#bib.bib18); Ho et al., [2023](#bib.bib16)). The
    key idea of CoT distillation is to make the student model mimic the step-by-step
    rationale generated by LLMs in response to a question. In this context, the rationale
    can be interpreted as the LLMs’ explanation of how to derive the final answer
    of a question, akin to the soft label used in conventional knowledge distillation
    (Hinton et al., [2015](#bib.bib15); Feng et al., [2022](#bib.bib12)). The representative
    works of CoT distillation include: SCoTD (Li et al., [2023](#bib.bib26)) introduces
    a symbolic CoT distillation method that enables smaller models to self-rationalize
    for reasoning via learning rationales from LLMs. Specialized KD (Fu et al., [2023](#bib.bib14))
    is proposed to train a small language model specialized for reasoning in four
    distinct in-context scenarios. MCC-KD (Chen et al., [2023](#bib.bib6)) adopts
    diverse rationales for distillation and attempts to ensure their consistency.
    SCOTT (Wang et al., [2023b](#bib.bib41)) designs a faithful CoT distillation strategy
    to make the student reason faithfully via counterfactual training. However, these
    methods fail to consider the reasonable learning order of the reasoning steps
    within a rationale, leading to sub-optimal performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum Learning. Early researches in cognitive science emphasize the significance
    of the easy-to-hard learning pattern to acquire knowledge (Elman, [1993](#bib.bib11)).
    Inspired by this, the pioneer work (Bengio et al., [2009](#bib.bib1)) introduces
    the concept of curriculum learning (CL) to the machine learning field by gradually
    including samples from easy to hard for training. In recent years, a variety of
    CL methods have been proposed to enhance the model performance (Kong et al., [2021](#bib.bib24);
    Wang et al., [2021](#bib.bib43)). For instance, Adaptive CL (Kong et al., [2021](#bib.bib24))
    proposes to utilize the loss of the model to dynamically adjust the difficulty
    score of each sample. SPL (Wan et al., [2020](#bib.bib39)) introduces the curriculum
    learning to the neural machine translation domain via introducing the token-level
    and sentence-level confidence score. ICL (Jia et al., [2023](#bib.bib21)) devises
    a curriculum learning method that organizes the curriculum within the token sequence
    of a sample for natural language generation tasks. However, as aforementioned,
    applying these CL methods directly to CoT distillation could yield inferior performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Proposed Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78ba807db61bed67d4bb4cbae9dab17c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of our KPOD framework. KPOD first determines the
    keypoint tokens for distillation through designing a rationale token weighting
    module based on mask learning. Then, an in-rationale progressive distillation
    strategy is devised to organize the learning order within rationale, so as to
    enable the student to acquire the reasoning capabilities in an easy-to-hard manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Preliminaries and Problem Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of CoT distillation is to transfer the reasoning capability of large
    language models (LLMs) to smaller student models via distilling the rationales
    produced by LLMs. We denote the dataset as $\mathcal{D}=\{(x^{(i)},y^{(i)})\}$
    as input. The standard negative log-likelihood loss for training the student model
    can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\!$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $r_{j}^{(i)}$ denotes the parameters of the student model. The first term
    of Eq.([3.1](#S3.Ex1 "3.1 Preliminaries and Problem Setting ‣ 3 Proposed Method
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs")) enables
    the student to mimic the rationale produced by LLMs, while the second term aims
    to train the student to output the final answer based on the rationale. By minimizing
    this loss, the student model can learn to generate the step-by-step rationale
    for deriving the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Framework Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As aforementioned, there are two key issues for CoT distillation methods: (i)
    Equally treating each token for distillation may make the student fail to mimic
    keypoint tokens accurately, leading to reasoning errors. (ii) Distilling the steps
    within a rationale without explicitly considering the learning order of step generation
    might lead to sub-optimal outcomes. To tackle these two issues, we propose a new
    CoT distillation framework KPOD, as illustrated in Figure [1](#S3.F1 "Figure 1
    ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs"). Our framework mainly consists of two components: a rationale token
    weighting component based on mask learning is proposed to determine the token
    significance for distillation. This encourages the student to faithfully replicate
    the crucial keypoint tokens; A progressive distillation component within the rationale
    is designed to establish a structured learning order for the reasoning steps.
    This guides the student model to progressively develop its reasoning abilities
    from simpler to more complex tasks, aligning with the proficiency of teacher LLMs.
    It’s worth noting that the obtained token significance weight fulfills two distinct
    functions in our framework: firstly, it encourages precise mimicry of keypoint
    tokens during distillation, and secondly, it mitigates the negative effects of
    inessential tokens when assessing step difficulty. Next, we will primarily delve
    into the detailed introduction of the two components in our framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Rationale Token Weighting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our rationale token weighting module, which determines
    the significance of each token via learning to mask reasoning-irrelevant token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight Generation. First, we intend to generate distinct significance weights
    for different tokens by leveraging their embeddings. This facilitates the estimation
    of their importance according to their characteristics. To achieve this, we feed
    the rationale tokens into a pre-trained input embedding layer, followed by a self-attention
    layer to encode in-context information. This process is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $e^{(i)}=\mathrm{Att}(\mathrm{Emb}(r^{(i)})),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathrm{Emb}$ of each token is fed into a weight generator, producing
    the significance weight as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{j}^{(i)}=\sigma(f_{w}(e_{j}^{(i)})),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $w_{j}^{(i)}$ is the sigmoid activation function (Narayan, [1997](#bib.bib33)).
    In this paper, we employ a simple two-layer MLP as the weight generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasoning-irrelevant Mask Learning. To optimize the weight generator, we formulate
    two loss functions: an answer prediction loss that encourages the module to utilize
    the question with the masked rationale for answer derivation, and a mask ratio
    loss aiming to maximize the ratio of masked tokens in the rationale. This allows
    the weight generator to generate low values of $w_{j}^{(i)}$ for tokens irrelevant
    to reasoning and high values for keypoint tokens. Next, we will introduce these
    two losses in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, considering that sampling the discrete mask policy $m_{j}^{(i)}\in\{0,1\}$
    is non-differentiable, we adopt the Gumbel-Softmax sampling (Jang et al., [2016](#bib.bib20))
    to avoid this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m_{j}^{(i)}=\mathrm{GumbelSoftmax}(w_{j}^{(i)}),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathrm{GumbelSoftmax}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we input ${r[m]}^{(i)}$ can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $y^{(i)}$, the transformer can be used to predict the answer by taking
    as input the question and the prefix of the masked rationale. Meanwhile, the weight
    generator is encouraged to generate large weights for the keypoint tokens, preventing
    them from being masked to facilitate the answer prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, to eliminate redundant tokens for reasoning, a mask ratio loss $\mathcal{L}_{m}$
    is presented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{m}=\sum_{j}m_{j}^{(i)}.$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: By optimizing $\mathcal{L}_{m}$, we enable the weight generator to identify
    the insignificant tokens in the reasoning process and generate lower weights for
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the overall loss function for training this module can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{k}=\mathcal{L}_{p}+\alpha\mathcal{L}_{m},$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ for each token within a rationale.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 In-rationale Progressive Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we elaborate our proposed in-rationale progressive distillation
    strategy, which schedules the learning order within a rationale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step Difficulty Assessment. Firstly, we assess the difficulty of each reasoning
    step in the rationale, so as to facilitate the learning order scheduling. In this
    work, we utilize the symbol “.” to separate steps in a rationale. As mentioned
    above, there could exist many reasoning-irrelevant tokens, and it is crucial to
    ensure that the difficulty evaluation is not influenced by them. Therefore, we
    propose a weighted token generation loss to calculate the difficulty value $d_{k}^{(i)}$
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{k}$-th step. In this way, the obtained step difficulty can be more
    concentrated on the difficulty of generating keypoint tokens, providing a more
    faithful reflection of the difficulty in deriving the correct outcome of each
    reasoning step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Progressive Distillation. Based on the step difficulty scores, we devise an
    in-rationale progressive distillation strategy to guide the student model learning
    each rationale in an easy-to-hard fashion. This strategy initiates with training
    the student model to generate the final few reasoning steps of the rationale using
    previous steps combined with the question as input, and progressively expands
    to produce the complete rationales. Supposed that we schedule the student model
    to output the last $n_{i}-c_{i}(t)$ of generating these steps can be formulated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h_{i}(S(t))=\sum_{j=c_{i}(t)+1}^{n_{i}}d_{j}^{(i)},$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $n_{i}$, which will be introduced later. In this paper, we treat each
    training epoch as a stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate selecting diverse questions to increase difficulty at each stage,
    we configure an overall learning difficulty $D(t)$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D(t)=\frac{ut^{p+1}}{p+1}+C_{0},$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $C_{0}$ are the pre-defined hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'When entering stage $t$ for the selected questions as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $c_{i}(t)$ is the ceiling magnitude for the increased difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in order to determine whether a question should increase difficulty,
    we design a value function $F$ is designed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$ is the selected question set. By using the square root operation,
    our aim is to promote a balanced distribution of questions within each cluster
    in the selected question set. This approach ensures that the diversity of the
    chosen question set is maintained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization of $F(S(t))$ can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{S(t)}F(S(t)),\ \ s.t.\Delta H(S(t))\leq\Delta D(t).$ |  | (13)
    |'
  prefs: []
  type: TYPE_TB
- en: By maximizing $F(S(t))$ satisfies the condition of monotone and submodular.
    Therefore, it can be approximately solved by a submodular maximization algorithm
    FTGP (Li et al., [2022](#bib.bib27)) in linear time with an approximation ratio
    guarantee, as formulated in Proposition [3.1](#S3.Thmtheorem1 "Proposition 3.1\.
    ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs"). The proof of Proposition
    [3.1](#S3.Thmtheorem1 "Proposition 3.1\. ‣ 3.4 In-rationale Progressive Distillation
    ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs") can be found in Appendix [D](#A4 "Appendix D Proof ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 3.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The optimization of $\max_{S(t)}F(S(t))$ represents the scale of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After obtaining the scheduled input step $c_{i}(t)$ can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{c_{i}(t)+1}$. In this way, the student model could learn the rationale
    of each question in an easy-to-hard manner.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Training Procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train our whole framework, we first optimize the rationale token weighting
    module by Eq.([7](#S3.E7 "Equation 7 ‣ 3.3 Rationale Token Weighting ‣ 3 Proposed
    Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs"))
    to determine the token significance. Then, we assess the step difficulty and derive
    the progressive distillation strategy by solving Eq.([13](#S3.E13 "Equation 13
    ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs")). Finally, by integrating
    these two modules, the overall loss for distilling the rationale at stage $t$
    can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: By optimizing $\mathcal{L}_{o}(t)$ (referring to the second term in Eq. ([3.1](#S3.Ex1
    "3.1 Preliminaries and Problem Setting ‣ 3 Proposed Method ‣ Keypoint-based Progressive
    Chain-of-Thought Distillation for LLMs"))), for the sake of clarity, as it remains
    constant. The pseudo-code of our training procedure is listed in Appendix [B](#A2
    "Appendix B Training Pseudo-code ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Performance comparison of our method and baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | # Params. | Distillation Methods | Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | ASDiv | SVAMP | CommonsenseQA |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo | unknown | - | 73.98 | 79.64 | 75.14 | 74.35 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | 7B | - | 11.00 | 40.20 | 32.80 | 33.90 |'
  prefs: []
  type: TYPE_TB
- en: '| SCoTD | 38.54 | 63.38 | 62.67 | 71.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Specialized KD | 39.15 | 64.01 | 63.33 | 72.32 |'
  prefs: []
  type: TYPE_TB
- en: '| SCOTT | 40.97 | 62.74 | 61.33 | 74.45 |'
  prefs: []
  type: TYPE_TB
- en: '| MCC-KD | 41.58 | 65.76 | 64.67 | 76.41 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD (ours) | 46.74 | 71.02 | 68.67 | 77.89 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-XL | 3B | - | 13.50 | 20.70 | 17.70 | 72.70 |'
  prefs: []
  type: TYPE_TB
- en: '| SCoTD | 21.85 | 25.16 | 26.67 | 79.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Specialized KD | 23.22 | 28.03 | 25.33 | 81.16 |'
  prefs: []
  type: TYPE_TB
- en: '| SCOTT | 21.09 | 25.48 | 24.67 | 83.62 |'
  prefs: []
  type: TYPE_TB
- en: '| MCC-KD | 24.28 | 31.35 | 30.00 | 82.88 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD (ours) | 25.19 | 33.76 | 34.67 | 88.04 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-Large | 760M | - | 6.90 | 10.10 | 6.80 | 67.60 |'
  prefs: []
  type: TYPE_TB
- en: '| SCoTD | 19.42 | 20.06 | 19.33 | 76.58 |'
  prefs: []
  type: TYPE_TB
- en: '| Specialized KD | 20.03 | 23.25 | 20.67 | 77.23 |'
  prefs: []
  type: TYPE_TB
- en: '| SCOTT | 18.21 | 21.66 | 18.67 | 77.48 |'
  prefs: []
  type: TYPE_TB
- en: '| MCC-KD | 18.36 | 23.89 | 21.33 | 78.13 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD (ours) | 22.46 | 27.39 | 25.33 | 81.41 |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we introduce our experiment settings. The implementation details
    can be found in Appendix [A](#A1 "Appendix A Implementation Details ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets. We evaluate our method on both mathematical reasoning tasks and commonsense
    reasoning tasks, following (Hsieh et al., [2023](#bib.bib18); Fu et al., [2023](#bib.bib14)).
    For mathematical reasoning, we adopt three benchmark datasets for evaluation:
    GSM8K (Cobbe et al., [2021](#bib.bib9)), ASDiv (Patel et al., [2021](#bib.bib34))
    and SVAMP (Miao et al., [2021](#bib.bib31)). For commonsense reasoning, CommonsenseQA
    benchmark (Talmor et al., [2019](#bib.bib37)) is employed to evaluate our method.
    Additionally, we conduct out-of-distribution (OOD) evaluation via training our
    method on GSM8K while testing it on ASDiv and SVAMP, following (Fu et al., [2023](#bib.bib14)).
    The dataset splits can be found in Appendix [A](#A1 "Appendix A Implementation
    Details ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models and Baselines. We adopt GPT-3.5-Turbo (Ye et al., [2023](#bib.bib46))
    as the teacher model to generate the rationale for each question in the dataset
    via zero-shot CoT prompting (Kojima et al., [2022](#bib.bib23)), following (Chen
    et al., [2023](#bib.bib6)). This is accessed via the OpenAI’s public API for ChatGPT.
    As for the student model, we adopt three widely-used pretrained language models
    of different architectures: LLaMA-7B (Touvron et al., [2023](#bib.bib38)), FlanT5-XL
    (Chung et al., [2022](#bib.bib8)) and FlanT5-Large (Chung et al., [2022](#bib.bib8)),
    similar to (Fu et al., [2023](#bib.bib14); Chen et al., [2023](#bib.bib6)). The
    parameter counts of LLaMA-7B, FlanT5-XL, FlanT5-Large are 7B, 3B, 760M respectively.
    As for baselines, we employ four state-of-the-art CoT distillation methods for
    comparison: Specialized KD (Fu et al., [2023](#bib.bib14)), SCOTT (Wang et al.,
    [2023b](#bib.bib41)), SCoTD (Li et al., [2023](#bib.bib26)), MCC-KD (Chen et al.,
    [2023](#bib.bib6)). Following previous works (Fu et al., [2023](#bib.bib14)),
    we use the accuracy (%) metric for evaluating the performance of our method and
    baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Overall Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we evaluate the overall performance of our method. We compare
    our method with four recent state-of-the-art CoT distillation methods as mentioned
    before. The GPT-3.5-Turbo serves as the teacher model. Table [1](#S4.T1 "Table
    1 ‣ 4 Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation for
    LLMs") illustrates the results. The symbol “-” denotes the model without using
    CoT distillation methods. First, we can observe that CoT distillation methods
    consistently boost the performance of smaller student models on reasoning tasks,
    underscoring the effectiveness of distilling rationales. In addition, it’s evident
    that our proposed KPOD outperforms previous methods by a large margin. For example,
    compared to MCC-KD, achieving the second best results when using LLaMA-7B as the
    student model, our approach achieves $5.16\%$ performance gains on the GSM8K,
    ASDiv, SVAMP, CommonsenseQA datasets, respectively. This highlights the effectiveness
    of promoting precise mimicry of keypoint tokens and implementing a learning schedule
    that progresses from easy to challenging tasks. Such an approach facilitates the
    acquisition of reasoning capabilities by the student model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Ablation study of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Settings | Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | CommonsenseQA |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | KPOD-w.o.-sig | 42.64 | 75.18 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-w.o.-sig-dif | 44.01 | 76.49 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-w.o.-prog | 43.25 | 74.61 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-w.o.-div | 44.16 | 75.76 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-ACL | 43.55 | 75.51 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-SPL | 42.94 | 75.84 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-ICL | 43.85 | 75.35 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD | 46.74 | 77.89 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-XL | KPOD-w.o.-sig | 22.46 | 85.26 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-w.o.-sig-dif | 23.82 | 86.08 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-w.o.-prog | 23.22 | 84.28 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-w.o.-div | 23.98 | 86.73 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-ACL | 23.52 | 86.40 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-SPL | 22.76 | 85.59 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD-ICL | 22.91 | 85.83 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD | 25.19 | 88.04 |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct ablation study to verify the effectiveness of the components in
    our proposed method. Specifically, we design several variants of our proposed
    KPOD: KPOD-w.o.-sig denotes our method wherein each token is treated equally,
    without incorporating the token significance weight for distillation. KPOD-w.o.-sig-dif
    represents our method without using the token significance weight for calculating
    the step difficulty. KPOD-w.o.-prog means our method without using the proposed
    progressive distillation strategy. KPOD-w.o.-div denotes our method without using
    the diversity term in the value function to select the question set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, we compare our method with three representative curriculum learning
    methods: Adaptive CL (Kong et al., [2021](#bib.bib24)), SPL (Wan et al., [2020](#bib.bib39))
    and ICL (Jia et al., [2023](#bib.bib21)). We design three variants of our method:
    KPOD-ACL, KPOD-SPL, KPOD-ICL respectively denote replacing our in-rationale progressive
    distillation strategy by Adaptive CL, SPL and ICL. The results are listed in Table
    [2](#S4.T2 "Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs"), KPOD-w.o.-sig
    obtains inferior performance than KPOD, illustrating the effectiveness of emphasizing
    the precise mimicry of keypoint tokens in our method. Besides, KPOD outperforms
    KPOD-w.o.-sig-dif. This shows that it’s essential to utilizing the token significance
    weight for the step difficulty calculation. The performance of KPOD-w.o.-prog
    is worse than KPOD, illustrating the effectiveness of scheduling an easy-to-hard
    learning order for CoT distillation. Moreover, KPOD obtains better performance
    than KPOD-w.o.-div. This demonstrates that ensuring a diverse question set to
    increase difficulty is effective. Finally, we can find that KPOD surpasses KPOD-ACL,
    KPOD-SPL and KPOD-ICL, showing the superiority of our in-rationale progressive
    distillation strategy compared to previous curriculum learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 OOD Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following (Fu et al., [2023](#bib.bib14)), we examine the out-of-distribution
    (OOD) generalization ability of the student model trained by our method and baselines.
    We use the in-distribution mathematical dataset GSM8K for training and adopt OOD
    mathematical datasets ASDiv, SVAMP for testing, similar to (Fu et al., [2023](#bib.bib14);
    Chen et al., [2023](#bib.bib6)). As shown in Table [3](#S4.T3 "Table 3 ‣ 4.4 OOD
    Performance ‣ 4 Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs"), our proposed KPOD consistently obtains superior performance compared
    to the baselines, indicating that the student model trained by our method has
    stronger OOD generalization capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: OOD performance of our method and baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Methods | In-distribution | OOD |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | ASDiv | SVAMP |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | SCoTD | 38.54 | 55.09 | 45.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Specialized KD | 39.15 | 53.82 | 38.67 |'
  prefs: []
  type: TYPE_TB
- en: '| SCOTT | 40.97 | 53.50 | 42.00 |'
  prefs: []
  type: TYPE_TB
- en: '| MCC-KD | 41.58 | 57.64 | 41.00 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD (ours) | 46.74 | 57.96 | 47.33 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-XL | SCoTD | 21.85 | 25.48 | 22.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Specialized KD | 23.22 | 26.11 | 24.67 |'
  prefs: []
  type: TYPE_TB
- en: '| SCOTT | 21.09 | 25.20 | 25.33 |'
  prefs: []
  type: TYPE_TB
- en: '| MCC-KD | 24.28 | 28.98 | 26.67 |'
  prefs: []
  type: TYPE_TB
- en: '| KPOD (ours) | 25.19 | 32.48 | 29.33 |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Visualizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we visualize the token significance weight $w_{j}^{(i)}$ generated
    by the weight generator, to intuitively show the effectiveness of the rationale
    token weighting module. Figure [2](#S4.F2 "Figure 2 ‣ 4.5 Visualizations ‣ 4 Experiments
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs") illustrates
    the visualization results on the GSM8K dataset. First, we can find that the digit
    tokens and operation tokens obtain the highest weights. This is because these
    tokens are usually of vital importance in the reasoning process, where even a
    slight deviation could cause errors. Additionally, several tokens that contribute
    significantly to the reasoning also exhibit relatively high weights. Tokens such
    as “twice”, “total”, “adding”, and “dividing” provide instructional cues for the
    reasoning steps. Besides, meaningful subjects like “Mark” and “Jennifer” can play
    a crucial role in reasoning, as their relationships should be considered during
    the reasoning process. Furthermore, it could be observed that some tokens of less
    importance for the reasoning are given low weights, such as “can”, “say”, “fit”,
    “received”, “got”, etc. These visualizations demonstrate our rationale token weighting
    module can effectively determine the significance of rationale tokens, thereby
    facilitating the student to accurately mimic crucial keypoint tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d62a69ef441b3d81366d94d25f2d61f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Visualizations of token significance weights produced by the weight
    generator. The intensity of red corresponds to the significance weight assigned
    to each token, with a deeper red indicating higher weight.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/004f701ab416e38ee7c1133f0c43b17a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) performance with varying $\alpha$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f1b6a85e54094102d845e0cc0f91cad8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) performance with varying $\beta$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Parameter sensitivity study of $\alpha$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Parameter Sensitivity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We perform experiments to analyze the effect of two important hyper-parameters
    $\alpha$ in a relatively large range. Thus it’s easy to set them in practice.
    We analyze the sensitivity of other hyper-parameters in Appendix [C](#A3 "Appendix
    C Additional Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we proposed a keypoint-based progressive chain-of-thought distillation
    framework for LLMs. Specifically, we devised a rationale token weighting module
    to encourage the student model to accurately mimic keypoint tokens during the
    distillation process. Besides, we proposed an in-rationale progressive distillation
    strategy to enable the student model to acquire reasoning capabilities from the
    teacher LLMs in an easy-to-hard manner. Extensive experiments validated the effectiveness
    of our proposed method.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the NSFC under Grants 62122013, U2001211\. This work
    was also supported by the Innovative Development Joint Fund Key Projects of Shandong
    NSF under Grants ZR2022LZH007.
  prefs: []
  type: TYPE_NORMAL
- en: Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of machine learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bengio et al. (2009) Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
    Curriculum learning. In *International Conference on Machine Learning*, pp.  41–48,
    2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benoit et al. (2013) Benoit, L., Lehalle, H., Molina, M., Tijus, C., and Jouen,
    F. Young children’s mapping between arrays, number words, and digits. *Cognition*,
    129(1):95–101, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bradley et al. (2000) Bradley, P. S., Bennett, K. P., and Demiriz, A. Constrained
    k-means clustering. *Microsoft Research, Redmond*, 20, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridle (1989) Bridle, J. Training stochastic model recognition algorithms as
    networks can lead to maximum mutual information estimation of parameters. *Annual
    Conference on Neural Information Processing Systems*, 2, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Annual Conference on Neural Information Processing
    Systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Chen, H., Wu, S., Quan, X., Wang, R., Yan, M., and Zhang,
    J. MCC-KD: Multi-CoT consistent knowledge distillation. In *Findings of the Association
    for Computational Linguistics: EMNLP 2023*, pp.  6805–6820\. Association for Computational
    Linguistics, December 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm:
    Scaling language modeling with pathways. *Journal of Machine Learning Research*,
    24(240):1–113, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,
    W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned
    language models. *arXiv preprint arXiv:2210.11416*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diao et al. (2023) Diao, S., Wang, P., Lin, Y., and Zhang, T. Active prompting
    with chain-of-thought for large language models. *arXiv preprint arXiv:2302.12246*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elman (1993) Elman, J. L. Learning and development in neural networks: The
    importance of starting small. *Cognition*, 48(1):71–99, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2022) Feng, K., Li, C., Yuan, Y., and Wang, G. Freekd: Free-direction
    knowledge distillation for graph neural networks. In *Proceedings of the 28th
    ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pp.  357–366, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2024) Feng, K., Li, C., Ren, D., Yuan, Y., and Wang, G. On the
    road to portability: Compressing end-to-end motion planner for autonomous driving.
    *arXiv preprint arXiv:2403.01238*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Specializing
    smaller language models towards multi-step reasoning. *International Conference
    on Machine Learning*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge
    in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ho et al. (2023) Ho, N., Schmid, L., and Yun, S.-Y. Large language models are
    reasoning teachers. In *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  14852–14882\. Association
    for Computational Linguistics, July 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya,
    E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark,
    A., et al. Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsieh et al. (2023) Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii,
    Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step!
    outperforming larger language models with less training data and smaller model
    sizes. In *Findings of the Association for Computational Linguistics: ACL 2023*,
    pp.  8003–8017\. Association for Computational Linguistics, July 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jang et al. (2016) Jang, E., Gu, S., and Poole, B. Categorical reparameterization
    with gumbel-softmax. *arXiv preprint arXiv:1611.01144*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. (2023) Jia, Q., Liu, Y., Tang, H., and Zhu, K. In-sample curriculum
    learning by sequence completion for natural language generation. In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pp.  11937–11950\. Association for Computational Linguistics,
    July 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2014) Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann,
    A. Self-paced learning with diversity. *Annual Conference on Neural Information
    Processing Systems*, 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
    Y. Large language models are zero-shot reasoners. *Annual Conference on Neural
    Information Processing Systems*, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. (2021) Kong, Y., Liu, L., Wang, J., and Tao, D. Adaptive curriculum
    learning. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pp.  5067–5076, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krueger & Dayan (2009) Krueger, K. A. and Dayan, P. Flexible shaping: How learning
    in small steps helps. *Cognition*, 110(3):380–394, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Li, L. H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., and
    Choi, Y. Symbolic chain-of-thought distillation: Small models can also “think”
    step-by-step. In *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pp.  2665–2679\. Association
    for Computational Linguistics, July 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Li, W., Feldman, M., Kazemi, E., and Karbasi, A. Submodular
    maximization in clean linear time. *Annual Conference on Neural Information Processing
    Systems*, 35:17473–17487, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Liang, C., Jiang, H., Liu, X., He, P., Chen, W., Gao, J.,
    and Zhao, T. Token-wise curriculum learning for neural machine translation. In
    *Findings of the Association for Computational Linguistics: EMNLP 2021*, pp. 
    3658–3670, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2017) Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program
    induction by rationale generation: Learning to solve and explain algebraic word
    problems. *arXiv preprint arXiv:1705.04146*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Magister et al. (2023) Magister, L. C., Mallinson, J., Adamek, J., Malmi, E.,
    and Severyn, A. Teaching small language models to reason. In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 2:
    Short Papers)*, pp.  1773–1781\. Association for Computational Linguistics, July
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. (2021) Miao, S.-Y., Liang, C.-C., and Su, K.-Y. A diverse corpus
    for evaluating and developing english math word problem solvers. *arXiv preprint
    arXiv:2106.15772*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molina & Jouen (1998) Molina, M. and Jouen, F. Modulation of the palmar grasp
    behavior in neonates according to texture property. *Infant Behavior and Development*,
    21(4):659–666, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narayan (1997) Narayan, S. The generalized sigmoid activation function: Competitive
    supervised learning. *Information Sciences*, 99(1-2):69–82, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patel et al. (2021) Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models
    really able to solve simple math word problems? *arXiv preprint arXiv:2103.07191*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Pennington, J., Socher, R., and Manning, C. D. Glove:
    Global vectors for word representation. In *Conference on Empirical Methods in
    Natural Language Processing*, pp.  1532–1543, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peterson (2004) Peterson, G. B. A day of great illumination: Bf skinner’s discovery
    of shaping. *Journal of the experimental analysis of behavior*, 82(3):317–328,
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talmor et al. (2019) Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa:
    A question answering challenge targeting commonsense knowledge. In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 
    4149–4158, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2020) Wan, Y., Yang, B., Wong, D. F., Zhou, Y., Chao, L. S., Zhang,
    H., and Chen, B. Self-paced learning for neural machine translation. In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pp.  1074–1080\. Association for Computational Linguistics, November 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Wang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang,
    B., Xu, R., and Wong, K.-F. Cue-cot: Chain-of-thought prompting for responding
    to in-depth dialogue questions with llms. In *Findings of the Association for
    Computational Linguistics: EMNLP 2023*, pp.  12047–12064, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., and Ren,
    X. SCOTT: Self-consistent chain-of-thought distillation. In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pp.  5546–5558\. Association for Computational Linguistics, July
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning
    in language models. *arXiv preprint arXiv:2203.11171*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Wang, Y., Wang, W., Liang, Y., Cai, Y., and Hooi, B. Curgraph:
    Curriculum learning for graph classification. In *Proceedings of the Web Conference
    2021*, pp.  1238–1248, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in
    large language models. *Annual Conference on Neural Information Processing Systems*,
    35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D.,
    and Chen, X. Large language models as optimizers. *arXiv preprint arXiv:2309.03409*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2023) Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y.,
    Zhou, Z., Gong, C., Shen, Y., et al. A comprehensive capability analysis of gpt-3
    and gpt-3.5 series models. *arXiv preprint arXiv:2303.10420*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain
    of thought prompting in large language models. *arXiv preprint arXiv:2210.03493*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and
    Smola, A. Multimodal chain-of-thought reasoning in language models. *arXiv preprint
    arXiv:2302.00923*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We perform our experiments using GeForce RTX 3090 GPUs. In order to accelerate
    training, we employ LoRA (Hu et al., [2021](#bib.bib19)) to train the student
    model. Following previous CoT distillation works (Chen et al., [2023](#bib.bib6)),
    the rank of LoRA is set to 64 for LLaMA-7B and 128 for FlanT5-XL. We use Adam
    optimizer for optimization with a learning rate of $1\times 10^{-5}$ for clustering
    the question.
  prefs: []
  type: TYPE_NORMAL
- en: We follow previous CoT distillation works to split the datasets (Chen et al.,
    [2023](#bib.bib6); Fu et al., [2023](#bib.bib14)), the datasets statistics are
    summarized in Table [4](#A1.T4 "Table 4 ‣ Appendix A Implementation Details ‣
    Keypoint-based Progressive Chain-of-Thought Distillation for LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Dataset statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Train Size | Validation Size | Test Size |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | 7473 | 660 | 659 |'
  prefs: []
  type: TYPE_TB
- en: '| ASDiv | 1462 | 313 | 314 |'
  prefs: []
  type: TYPE_TB
- en: '| SVAMP | 700 | 150 | 150 |'
  prefs: []
  type: TYPE_TB
- en: '| CommonsenseQA | 8520 | 1221 | 1221 |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Training Pseudo-code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 outlines the training procedure of our KPOD. Initially, we employ
    the CoT prompt (Kojima et al., [2022](#bib.bib23)) to instruct the teacher LLM
    to generate step-by-step rationales for each question in the dataset. Subsequently,
    the rationale token weighting module receives these rationales as input and is
    trained to determine the significance weights for each token. Following this,
    we compute the difficulty of each step in the rationale based on these weights.
    We then utilize the FTGP algorithm (Li et al., [2022](#bib.bib27)) to maximize
    Eq.([13](#S3.E13 "Equation 13 ‣ 3.4 In-rationale Progressive Distillation ‣ 3
    Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation for
    LLMs")) to schedule the question set for increasing difficulty at each stage.
    Once scheduled, we train the student model using Eq.([15](#S3.E15 "Equation 15
    ‣ 3.5 Training Procedure ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")) based on the established learning order and token significance
    weights. Before epoch $T$, the student is trained to generate the complete rationale
    for each question. This approach allows the student model to precisely mimic the
    keypoint tokens while progressively acquiring reasoning capabilities in an easy-to-hard
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 The training procedure of KPOD
  prefs: []
  type: TYPE_NORMAL
- en: a teacher LLM, dataset $\mathcal{D}=\{(x^{(i)},y^{(i)})\}$;end for
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Additional Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24a3182dc832970c98066da05507b9b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) performance with varying $p$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/05eb6aa2ed235154d1207400a72d6987.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) performance with varying $K$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/44fa4039e898b42ac55506700debfb2e.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) performance with varying $C_{0}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Parameter sensitivity study of $p$ on GSM8K.'
  prefs: []
  type: TYPE_NORMAL
- en: We additionally analyze the sensitivity of three hyper-parameters of $p$. Our
    method is still not sensitive to this hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Proof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we prove the Proposition [3.1](#S3.Thmtheorem1 "Proposition
    3.1\. ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs"). First, we introduce Theorem
    [D.1](#A4.Thmtheorem1 "Theorem D.1\. ‣ Appendix D Proof ‣ Keypoint-based Progressive
    Chain-of-Thought Distillation for LLMs") proposed in FTGP algorithm (Li et al.,
    [2022](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Theorem D.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a function $f:2^{N}\rightarrow\mathbb{R}$ holds.
  prefs: []
  type: TYPE_NORMAL
- en: According to Theorem [D.1](#A4.Thmtheorem1 "Theorem D.1\. ‣ Appendix D Proof
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs"), if we could
    prove that our value function $F$ satisfies these two conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1. (Monotonicity) A function $f:2^{N}\rightarrow\mathbb{R}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 1. Our value function $F$ in Eq.([13](#S3.E13 "Equation 13 ‣ 3.4 In-rationale
    Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")) is monotone.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We define two question sets $A(t),B(t)$. We have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\geq\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap B(t)&#124;}-\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap
    A(t)&#124;}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\beta\sum_{k=1}^{K}(\sqrt{&#124;C_{k}\cap B(t)&#124;}-\sqrt{&#124;C_{k}\cap
    A(t)&#124;})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\geq 0$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\Rightarrow F(A)\leq F(B).$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2. (Submodularity) A function $f:2^{N}\rightarrow\mathbb{R}$.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 2. Our value function $F$ in Eq.([13](#S3.E13 "Equation 13 ‣ 3.4 In-rationale
    Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")) is submodular.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We define two triad sets $A,B$. Then we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\Delta H(\{x\})-\Delta H(\{x\})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'Given that $x\in N\backslash B$. Then, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\ \ \ \ -(\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap B(t)&#124;+&#124;C_{k}\cap\{x\})&#124;}-\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap
    B(t)&#124;}).$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 'For convenience, we denote $x_{k}=|C_{k}\cap A(t)|$. Then, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Since $A\subseteq B$. Therefore, we conclude:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Then, we can derive:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\Rightarrow F(A\cup\{x\})-F(A)\geq F(B\cup\{x\})-F(B).$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
