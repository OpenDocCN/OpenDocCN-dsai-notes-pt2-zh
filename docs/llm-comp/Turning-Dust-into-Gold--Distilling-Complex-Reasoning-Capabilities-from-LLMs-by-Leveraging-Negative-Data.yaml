- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs
    by Leveraging Negative Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.12832](https://ar5iv.labs.arxiv.org/html/2312.12832)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yiwei Li¹¹¹1Equal contributions., Peiwen Yuan¹¹¹1Equal contributions., Shaoxiong
    Feng², Boyuan Pan², Bin Sun¹, Xinglin Wang¹, Heda Wang², Kan Li¹²²2Corresponding
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have performed well on various reasoning tasks,
    but their inaccessibility and numerous parameters hinder wide application in practice.
    One promising way is distilling the reasoning ability from LLMs to small models
    by the generated chain-of-thought reasoning paths. In some cases, however, LLMs
    may produce incorrect reasoning chains, especially when facing complex mathematical
    problems. Previous studies only transfer knowledge from positive samples and drop
    the synthesized data with wrong answers. In this work, we illustrate the merit
    of negative data and propose a model specialization framework to distill LLMs
    with negative samples besides positive ones. The framework consists of three progressive
    steps, covering from training to inference stages, to absorb knowledge from negative
    data. We conduct extensive experiments across arithmetic reasoning tasks to demonstrate
    the role of negative data in distillation from LLM¹¹1Equal contributions..
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹footnotetext: Our code and data have been released on https://github.com/Yiwei98/TDG.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nowadays, owing to chain-of-thought (CoT) prompting (Wei et al. [2022b](#bib.bib35)),
    large language models (LLMs) exhibit strong reasoning capabilities (Bubeck et al.
    [2023a](#bib.bib2)), especially when it comes to complex mathematical problems
    (Hendrycks et al. [2021](#bib.bib9)). Unfortunately, CoT has been demonstrated
    to be an emergent property of models with more than 100B parameters, but not of
    smaller models (Wei et al. [2022a](#bib.bib34)). The burdensome computational
    requirements and high inference costs of these models hinder their development
    in real-world scenarios with limited resources (Ho, Schmid, and Yun [2023](#bib.bib11)).
    Thus, the goal of our research is to enable complex arithmetic reasoning in small
    models for deploying at scale.
  prefs: []
  type: TYPE_NORMAL
- en: '| MATH Dataset | Intersection | Pos | Neg | IoU |'
  prefs: []
  type: TYPE_TB
- en: '| InterAlgebra | 4 | 35 | 21 | 0.077 |'
  prefs: []
  type: TYPE_TB
- en: '| Prealgebra | 9 | 72 | 43 | 0.085 |'
  prefs: []
  type: TYPE_TB
- en: '| Geometry | 1 | 21 | 10 | 0.033 |'
  prefs: []
  type: TYPE_TB
- en: '| NumberTheory | 1 | 29 | 17 | 0.022 |'
  prefs: []
  type: TYPE_TB
- en: '| Precalculus | 2 | 25 | 16 | 0.051 |'
  prefs: []
  type: TYPE_TB
- en: '| Probability | 4 | 19 | 16 | 0.129 |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | 8 | 52 | 43 | 0.062 |'
  prefs: []
  type: TYPE_TB
- en: '| Overall | 29 | 253 | 166 | 0.074 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The distribution of correct answers in MATH test set. Pos and Neg
    refer to models trained on positive and negative samples respectively. Intersection
    over Union (IoU) exhibits a remarkably low value across all subsets, which confirms
    the value of negative samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowledge distillation (Hinton, Vinyals, and Dean [2015](#bib.bib10)) offers
    a promising way to transfer specific capabilities from LLMs into smaller models.
    This process is also referred to as model specialization enforcing compact models
    to focus on certain skills. Prior works (Magister et al. [2023](#bib.bib23); Fu
    et al. [2023](#bib.bib7); Hsieh et al. [2023](#bib.bib12)) employed LLMs with
    in-context learning (ICL) (Brown et al. [2020](#bib.bib1)) to generate reasoning
    paths (rationales) of math problems, which are more beneficial for small models
    to acquire complex reasoning ability than reference reasoning paths. Table [1](#Sx1.T1
    "Table 1 ‣ Introduction ‣ Turning Dust into Gold: Distilling Complex Reasoning
    Capabilities from LLMs by Leveraging Negative Data") shows an intriguing phenomenon:
    models trained on positive and negative data separately have an extremely small
    overlap (intersection) in their correct answers on the MATH test set. Although
    the negative model has a lower accuracy, it can address some questions that the
    positive model is unable to provide correct answers, which confirms the valuable
    knowledge contained in negative data. Additionally, the undesirable behaviors
    within negative data are also useful when preventing the model from committing
    similar issues. Another reason that we should exploit negative data is the token-based
    pricing strategy of OpenAI. Even for GPT-4, the accuracy on MATH dataset is less
    than 50% (Bubeck et al. [2023b](#bib.bib3)), meaning that all tokens of negative
    data are charged for nothing. Therefore, instead of discarding negative samples,
    we extract and utilize valuable knowledge from negative samples to boost the model
    specialization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The conventional process of model specialization can be summarized as three
    steps (Zhu et al. [2023](#bib.bib41)): The first step is chain-of-thought distillation,
    training small models with reasoning chains generated from LLMs. The second step
    can be regarded as self-enhancement, conducting self-distillation (Mobahi, Farajtabar,
    and Bartlett [2020](#bib.bib25)) or self-augmentation to further optimize the
    models. Besides, self-consistency (Wang et al. [2023](#bib.bib33)) is widely used
    as an effective decoding strategy to boost the model performance in reasoning
    tasks. In this work, we propose a novel model specialization framework (shown
    in Figure [1](#Sx3.F1 "Figure 1 ‣ Methodology ‣ Turning Dust into Gold: Distilling
    Complex Reasoning Capabilities from LLMs by Leveraging Negative Data")) that can
    exploit negative data to enhance the distillation of the complex reasoning abilities
    from LLMs. Specifically, we first develop the negative assistant training (NAT)
    approach, where dual LoRA (Hu et al. [2022](#bib.bib13)) structure is designed
    to capture knowledge from both positive and negative sides. As an auxiliary module,
    the knowledge of negative LoRA can be dynamically integrated into the training
    process of positive LoRA through a corrected attention mechanism. For self-enhancement,
    we devise negative calibrated enhancement (NCE), which regards the negative output
    as a baseline to strengthen the distillation of critical positive rationales.
    In addition to the training stage, we also leverage the negative information during
    inference. Traditional self-consistency allocates equal or probability-based weights
    to all candidate outputs, leading to some fallible answers being voted up. To
    alleviate this issue, adaptive self-consistency (ASC) is proposed to conduct ranking
    before voting, where the ranking model is trained on both positive and negative
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform comprehensive experiments and detailed analyses across arithmetic
    reasoning tasks with LLaMA-7b (Touvron et al. [2023](#bib.bib32)) as the student
    model. Previous model specialization work only validated on ordinary datasets
    (e.g., GSM8K, ASDiv, etc.), while we are the first to focus on the challenging
    competition mathematical problems – MATH dataset (Hendrycks et al. [2021](#bib.bib9)).
    Experiments show that: (1) Negative assistant training can provide a more comprehensive
    way to absorb the knowledge from negative data. (2) Negative calibrated enhancement
    can make the process of self-distillation more targeted on crucial knowledge.
    (3) Ranking model trained on both positive and negative rationales can assign
    appropriate weights for answer aggregation. In summary, key contributions of this
    work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We illustrate that negative samples with incorrect answers can also provide
    a valuable resource besides positive data for distilling knowledge from LLMs in
    complex arithmetic reasoning tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To fully leverage the negative data, we propose a model specialization framework
    consisting of three progressive steps, spanning from training to inference stages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive evaluations on challenging arithmetic reasoning dataset demonstrate
    that the proposed framework can effectively exploit the negative information and
    outperform baselines by a large margin.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chain-of-Thought Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The approach of solving complex reasoning problems by generating chain-of-thought
    (rationales) has been proven to be an effective method (Wei et al. [2022b](#bib.bib35)).
    By following the pattern of gradually solving sub-problems, both few-shot CoT
    (Fu et al. [2023](#bib.bib7)) and zero-shot CoT (Kojima et al. [2022](#bib.bib16))
    can stimulate the potential reasoning ability of LLMs. On this basis, Least-to-most
    prompting (Zhou et al. [2023](#bib.bib40)) suggests explicitly splitting the problem
    and solving them step by step. Self-Consistency (Wang et al. [2023](#bib.bib33))
    further improves accuracy by conducting vote between multiple diverse rationales.
    PHP proposes (Zheng et al. [2023](#bib.bib39)) iteratively generating answers
    and adding the historically generated answers as hints to the context to achieve
    the final convergence on the answer. Both correct and incorrect answers generated
    during this iteration process serve as hints to provide effective information.
    We also think that responses with incorrect answers from LLMs can provide valuable
    information, but the differences lie in: (1) We believe that not only the generated
    answers, but also the rationales contain valuable knowledge. (2) We consider utilizing
    these negative samples in the process of transferring knowledge from LLMs to smaller
    models instead of only inference stage.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation from Large Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge distillation (Hinton, Vinyals, and Dean [2015](#bib.bib10); Sanh et al.
    [2019](#bib.bib29)) has proven effective for transferring knowledge from a large
    model to a smaller one. This process is usually achieved by optimizing the parameters
    of smaller models so that their outputs (distributions (Feng et al. [2021](#bib.bib6)),
    hidden states (Jiao et al. [2020](#bib.bib15)), attentions (Tang et al. [2019](#bib.bib31)))
    can be closer to that of large models. However, the black-box nature of current
    mainstream LLMs (e.g., GPT4) hinders the application of these methods. Thus, many
    studies (Ho, Schmid, and Yun [2023](#bib.bib11); Fu et al. [2023](#bib.bib7);
    Zhu et al. [2023](#bib.bib41)) have attempted to conduct hard distillation by
    fine-tuning smaller models directly on the LLMs generated responses with correct
    answers. However, as previously mentioned, responses generated by LLMs that contain
    incorrect answers also contain valuable knowledge. Discarding this portion of
    data directly would be a pity, especially considering that a significant portion
    of responses generated by LLMs in complex reasoning tasks end with incorrect answers.
    To this end, we propose multiple methods to fully utilize these abandoned knowledge
    in the process of transferring reasoning abilities of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Learning From Negative Views
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Samples that reflect some particular undesirable behavior are called negative
    data, which has been studied to help model correct such behavior (He and Glass
    [2020](#bib.bib8); Welleck et al. [2020](#bib.bib36); Lagutin, Gavrilov, and Kalaidin
    [2021](#bib.bib17)). He and Glass ([2020](#bib.bib8)) conducts negative updates
    with training signals provided by negative samples to avoid model generating such
    data. Welleck et al. ([2020](#bib.bib36)); Li et al. ([2020](#bib.bib19)) penalizes
    the model for outputting words with certain characteristics by introducing an
    unlikelihood loss term. Li et al. ([2022](#bib.bib20), [2023](#bib.bib21)) suggests
    maximizing the distance between the predictions of the negative teacher and student.
    These methods only consider the use of negative training signals in negative samples.
    But in fact, negative data can also provide valuable positive knowledge. In this
    work, we investigated how to comprehensively utilize knowledge of negative data
    from both positive and negative perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5fe938f6462b53e26a93e7a181b6dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The overview of proposed framework. Step 1: Training Neg-LoRA on
    negative samples to assist in the learning of reasoning on positive data through
    Integrate Unit. Step 2: Utilizing Neg-LoRA as baseline to calibrate the process
    of self-enhancement. Step 3: Training a ranking model on both positive and negative
    samples. Then weighting the candidates adaptively during inference according to
    scores from it.'
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chain-of-Thought Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rajani et al. ([2019](#bib.bib27)) demonstrated that training a language model
    on a dataset with explicit rationales preceding the answer could improve the ability
    to generate the final answer. Thus, chain-of-thought distillation is proposed
    to maximize the manifestation of the reasoning abilities of the LLMs on smaller
    models. Denote $\mathcal{D}=\{(x_{i},y_{i})\}^{N}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{(x,\hat{r},\hat{y})\sim S_{pos}}\mathrm{log}P(\hat{y},\hat{r}&#124;x;\theta).$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Self-Enhancement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the idea of human self reflection to achieve progress, various methods
    (Huang et al. [2022](#bib.bib14); Xu et al. [2021](#bib.bib37); Mobahi, Farajtabar,
    and Bartlett [2020](#bib.bib25)) have been proposed to strengthen language models
    based on their own knowledge, which we collectively refer to as self-enhancement.
    It consists two common methods: one is self-augmentation (Xu et al. [2021](#bib.bib37)),
    where the model first generates data with diversity and then trains on them to
    achieve better generalization (Huang et al. [2022](#bib.bib14)). The other is
    self-distillation (Zhu et al. [2023](#bib.bib41)), which involves using the model
    itself as teacher to complete iterative distillation, thereby utilizing dark knowledge
    to further improve the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Self-consistency (Wang et al. [2023](#bib.bib33)) capitalizes on the notion
    that a intricate problem requiring logical thinking usually offers several distinct
    approaches that all lead to the same accurate answer. Based on this, multiple
    candidates $\{(\hat{r}^{l},\hat{y}^{l})\}^{L}$ is selected as the final prediction
    through a voting process:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}=\arg\max_{i}\sum_{l=1}^{L}\mathbb{I}(\hat{y}^{l}=i)$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{I}(\hat{y}^{l}=i)$, and 0 otherwise).
  prefs: []
  type: TYPE_NORMAL
- en: Negative Assistant Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Table [1](#Sx1.T1 "Table 1 ‣ Introduction ‣ Turning Dust into Gold:
    Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data"),
    negative samples also contains valuable knowledge, which can even serve as a good
    complement to positive data. However, there is an increased risk of inference
    errors for $\hat{r}$. Extracting useful knowledge from negative samples without
    being affected by undesirable behaviors is therefore a challenging task. To address
    this, we propose a two-stage Negative Assistant Training (NAT) Paradigm (Step
    1.1 and 1.2 in Figure [1](#Sx3.F1 "Figure 1 ‣ Methodology ‣ Turning Dust into
    Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative
    Data")).'
  prefs: []
  type: TYPE_NORMAL
- en: Absorbing Negative Knowledge
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, we acquire the $(x,\hat{r},\hat{y})$ by maximizing the following expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{(x,\hat{r},\hat{y})\sim\mathcal{D}_{neg}}\mathrm{log}P(\hat{y},\hat{r}&#124;x;\theta_{neg}).$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: During this process, the parameters of LLaMA remain frozen, while the knowledge
    of $\mathcal{D}_{neg}$.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Integration Unit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since it is impossible to pre-determine which mathematical problems $\theta_{neg}$.
    We propose a corrected attention mechanism to achieve this vision as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha=W_{Q}(h_{input})W_{K}([h_{pos};h_{neg}])^{T}+[0.5;-0.5]$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $h_{output}=\alpha\cdot W_{V}([h_{pos};h_{neg}]))$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $W_{Q},W_{K}\in\mathbb{R}^{d\times w}$ and the LLaMA layer outputs forms
    the output of the Dynamic Integrate Unit.
  prefs: []
  type: TYPE_NORMAL
- en: By employing NAT, $\mathcal{M}_{NAT}$ can inherit LLM’s knowledge more comprehensively
    in both dimensions of diversity (more samples) and type (both positive and negative
    data), leading to improved complex reasoning abilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d618747d636630eaebb46febc77ff2ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The workflow of Integrate Unit. The outputs of both Neg-LoRA and
    Pos-LoRA are fused through a corrected attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: Negative Calibrated Enhancement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further strengthen the reasoning ability of the model, we propose Negative
    Calibrated Enhancement (NCE) that use negative knowledge to aid with the self-enhancement
    process (Zhu et al. [2023](#bib.bib41); Huang et al. [2022](#bib.bib14)). We first
    use $\mathcal{M}_{NAT}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\beta=\mathrm{Tanh}(f_{\text{KL}}(\theta_{neg},\theta_{NAT}))$ |  | (7)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: A larger $\beta$.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Self-Consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-Consistency (SC) technique (Wang et al. [2023](#bib.bib33)) is effective
    for further improving the performance of models in complex reasoning (Zhu et al.
    [2023](#bib.bib41)). However, current methods either naively assign equal weights
    to each candidate or simply assign weights based on generation probabilities.
    These strategies fail to adjust the weights of candidates based on the quality
    of ($\hat{r},\hat{y}$ that can adaptively reweight candidates with justification.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Methods | Counting& Probability | Inter Algebra | Number Theory
    | Precalculus | Prealgebra | Geometry | Algebra | Average |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM 62B | Few-shot | - | - | - | - | - | - | - | 4.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM 540B | Few-shot | - | - | - | - | - | - | - | 8.8 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3 175B | Few-shot | 4.7 | 4.4 | 4.4 | 4.0 | 7.7 | 3.1 | 6.0 | 5.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3 13B | Fine-tune | 4.1 | 4.7 | 5.5 | 5.8 | 6.8 | 7.1 | 5.3 | 5.6 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA 7B | Fine-tune | 2.96 | 3.58 | 2.96 | 3.85 | 4.61 | 3.46 | 4.56 | 3.88
    +0% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 Turbo CoT | CoT KD | 4.15 | 4.17 | 5.37 | 4.58 | 8.82 | 4.54 | 4.61
    | 5.29 +36.3% |'
  prefs: []
  type: TYPE_TB
- en: '| MIX | 3.49 | 1.43 | 1.67 | 1.46 | 5.27 | 2.59 | 4.05 | 3.03 -21.9% |'
  prefs: []
  type: TYPE_TB
- en: '| CL | 4.64 | 3.93 | 5.74 | 4.03 | 7.39 | 2.51 | 5.98 | 5.16 +33.0% |'
  prefs: []
  type: TYPE_TB
- en: '| NT | 3.93 | 3.93 | 6.30 | 2.20 | 6.69 | 4.10 | 5.17 | 4.48 +15.4% |'
  prefs: []
  type: TYPE_TB
- en: '| UL | 4.98 | 3.86 | 5.37 | 3.85 | 6.70 | 4.10 | 5.27 | 4.96 +27.8% |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | 5.70 | 5.24 | 6.67 | 3.85 | 9.99 | 5.64 | 7.94 | 6.81 +75.5% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 CoT | CoT KD | 3.71 | 4.88 | 6.30 | 3.30 | 6.56 | 3.67 | 7.73 | 5.59
    +44.1% |'
  prefs: []
  type: TYPE_TB
- en: '| MIX | 3.28 | 2.86 | 2.96 | 4.21 | 5.45 | 3.55 | 6.66 | 4.49 +15.7% |'
  prefs: []
  type: TYPE_TB
- en: '| CL | 4.15 | 3.67 | 5.00 | 3.11 | 7.90 | 5.43 | 5.98 | 5.24 +35.1% |'
  prefs: []
  type: TYPE_TB
- en: '| NT | 3.28 | 2.46 | 4.07 | 3.85 | 8.92 | 6.05 | 5.97 | 5.14 +32.4% |'
  prefs: []
  type: TYPE_TB
- en: '| UL | 4.15 | 3.46 | 6.67 | 3.11 | 8.67 | 5.18 | 8.25 | 6.03 +55.4% |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | 6.11 | 4.65 | 5.56 | 4.58 | 8.50 | 4.92 | 9.78 | 6.83 +76.0% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Experimental results (%) on MATH test set for NAT. We report the accuracy
    (solving rate) of math problems for each test set. Average is the mean value of
    all subjects. GPT3 and PaLM are from Hendrycks et al. ([2021](#bib.bib9)) and
    Lewkowycz et al. ([2022](#bib.bib18)), respectively. Comparing with standard fine-tune,
    NAT achieves about 75.75% increase.'
  prefs: []
  type: TYPE_NORMAL
- en: Ranking Model Training.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Ideally, we hope that $\mathcal{M}_{rank}$ in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'and use MSE loss to train $\mathcal{M}_{rank}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{RM}=\sum_{i=1}^{N}\&#124;\mathcal{M}_{rank}(p_{i})-q_{i}\&#124;_{2}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Weighting Policy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Building upon the foundation of $\mathcal{M}_{rank}$, we revise Eq. ([2](#Sx3.E2
    "In Self-Consistency ‣ Background ‣ Methodology ‣ Turning Dust into Gold: Distilling
    Complex Reasoning Capabilities from LLMs by Leveraging Negative Data")) to Eq. ([11](#Sx3.E11
    "In Weighting Policy. ‣ Adaptive Self-Consistency ‣ Methodology ‣ Turning Dust
    into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative
    Data")) to achieve the vision of adaptively reweighting the candidates reasonably.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: From the view of knowledge transfer, ASC achieves further utilization of knowledge
    ( positive and negative) embedded in LLMs to help smaller models attain better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This work focuses on the challenging competition mathematical dataset MATH
    (Hendrycks et al. [2021](#bib.bib9)), which has 12,500 problems in total spanning
    seven various subjects. Besides, the following four datasets are introduced to
    evaluate the generalization ability on out-of-distribution (OOD) data of the proposed
    framework: GSM8K (Cobbe et al. [2021](#bib.bib5)), ASDiv (Miao, Liang, and Su
    [2020](#bib.bib24)), MultiArith (Roy and Roth [2015](#bib.bib28)), and SVAMP (Patel,
    Bhattamishra, and Goyal [2021](#bib.bib26)). Detailed data statistics are shown
    in Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: For teacher model, we use gpt-3.5-turbo and gpt-4 API from OpeanAI to synthesize
    reasoning chains. Given that the problems of MATH are challenging, we select LLaMA-7b
    as the student model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of baselines in our study: one includes LLMs, while
    the other is based on LLaMA-7b. In the case of LLMs, we compare with two popular
    models: GPT3 (Brown et al. [2020](#bib.bib1)) and PaLM (Chowdhery et al. [2022](#bib.bib4)).
    As for LLaMA-7b, we first provide a comparison of our method with three settings:
    Few-shot, Fine-tune (on original training samples), CoT KD (chain-of-thought distillation).
    In terms of learning from negative views, four baseline methods will be further
    included: MIX (directly trains LLaMA with the mixture of both positive and negative
    data), CL (contrastive learning), NT (negative training) (He and Glass [2020](#bib.bib8))
    and UL (unlikelihood) (Welleck et al. [2020](#bib.bib36)). Please see Appendix
    for the details of these baselines. The evaluations of NCE and ASC will also include
    some other baselines that will be introduced in corresponding parts.'
  prefs: []
  type: TYPE_NORMAL
- en: Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Native Assistant Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The evaluation results of NAT are presented in Table [2](#Sx3.T2 "Table 2 ‣
    Adaptive Self-Consistency ‣ Methodology ‣ Turning Dust into Gold: Distilling Complex
    Reasoning Capabilities from LLMs by Leveraging Negative Data"), with all methods
    using greedy search (i.e. temperature = 0). It shows that proposed method NAT
    improves task accuracy across all baselines. It can be seen from the low values
    of GPT3 and PaLM that MATH is a very difficult math dataset, but NAT can still
    accomplish competitive performance with much less parameters. Comparing with fine-tuning
    on the original data, NAT achieves about 75.75% increase under two different CoT
    sources. In comparison with CoT KD on positive samples, the mainstream specialization
    pattern, NAT also improves accuracy significantly, demonstrating the value of
    negative samples. As for baselines to utilize negative information, the lowest
    performance of MIX suggests that directly training the negative samples will make
    model toxic. Other methods are also mostly inferior to NAT, which indicates that
    using negative samples only in the negative direction is not sufficient in complex
    reasoning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9495a6388382070c594646fa318a70a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Experimental results (%) of NCE. KD denotes knowledge distillation
    with data augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Negative Calibrated Enhancement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main results under the data of gpt-3.5-turbo of NCE are shown in Figure [3](#Sx4.F3
    "Figure 3 ‣ Native Assistant Training ‣ Main Results ‣ Experiments ‣ Turning Dust
    into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative
    Data"). Compared with knowledge distillation (KD), NCE achieves an average progress
    of 10% (0.66), which demonstrates the effectiveness of distillation with calibration
    information offered by negative samples. Although NCE reduced some parameters
    (e.g., Neg-LoRA) compared to NAT, it still achieved a progress of 6.5% (0.44),
    implementing compressed model and improved performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Self-Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Models | Strategies | CP | NT | PC | A* | Ave |'
  prefs: []
  type: TYPE_TB
- en: '| CoT KD | SC | 7.38 | 6.62 | 5.70 | 8.70 | 7.85 |'
  prefs: []
  type: TYPE_TB
- en: '| SC wWS | 7.22 | 6.64 | 5.75 | 8.52 | 7.82 |'
  prefs: []
  type: TYPE_TB
- en: '| ASC | 7.70 | 6.97 | 6.16 | 9.12 | 8.25 |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | SC | 8.65 | 7.49 | 5.75 | 11.14 | 9.25 |'
  prefs: []
  type: TYPE_TB
- en: '| SC wWS | 8.67 | 7.34 | 5.77 | 11.08 | 9.21 |'
  prefs: []
  type: TYPE_TB
- en: '| ASC | 9.30 | 8.33 | 5.83 | 11.88 | 9.84 |'
  prefs: []
  type: TYPE_TB
- en: '| NCE | SC | 9.21 | 7.94 | 5.96 | 11.32 | 9.69 |'
  prefs: []
  type: TYPE_TB
- en: '| SC wWS | 9.13 | 7.84 | 5.99 | 11.25 | 9.64 |'
  prefs: []
  type: TYPE_TB
- en: '| ASC | 9.87 | 8.21 | 6.37 | 11.89 | 10.23 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Experimental results (%) on MATH for ASC. A* is the average of InterAlgebra,
    Prealgebra and Algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate ASC, we compare it with base SC and its weighted sum (WS) version.
    We generate 16 samples with sampling temperature $T=1$. The results from Table [3](#Sx4.T3
    "Table 3 ‣ Adaptive Self-Consistency ‣ Main Results ‣ Experiments ‣ Turning Dust
    into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative
    Data") shows that ASC is a more promising strategy to aggregate the answers from
    different samples. SC with WS doesn’t outperform base SC, which is consistent
    with Wang et al. ([2023](#bib.bib33)). Note that the accuracy of ranking model
    is only about 60%, indicating that the performance of ASC can be further improved
    with higher accuracy. Refer to Accuracy of Ranking Model for detailed analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to better understand the usefulness of the negative knowledge and the
    effectiveness of our framework, we carry out extensive analysis on LLaMA distilled
    from gpt-3.5-turbo in terms of both quantitative and qualitative measures.
  prefs: []
  type: TYPE_NORMAL
- en: Generalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Methods | GSM8K | ASDiv | MultiArith | SVAMP |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tune | 17.51 | 36.37 | 53.17 | 17.90 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT KD | 38.81 | 76.43 | 83.5 | 47.40 |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | 41.24 | 76.11 | 84.67 | 47.20 |'
  prefs: []
  type: TYPE_TB
- en: '| KD | 41.55 | 75.86 | 88.05 | 50.70 |'
  prefs: []
  type: TYPE_TB
- en: '| NCE | 41.93 | 77.67 | 88.67 | 51.50 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Generalization evaluation results (%).'
  prefs: []
  type: TYPE_NORMAL
- en: Besides MATH dataset, we evaluate the generalization ability of proposed framework.
    Following Fu et al. ([2023](#bib.bib7)), we only synthesize data and train the
    models on GSM8K and evaluate on all the four test sets. The higher performance
    of NAT and NCE on GSM8K indicates that the proposed method can generalize to previously
    commonly used dataset in the field of model specialization. NCE outperforms others
    in A-M-S datasets suggests that the calibrated dark knowledge from logits distributions
    can improve out-of-distribution (OOD) performance.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Methods | CP | NT | PC | G | A* | Ave |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | 5.70 | 6.67 | 3.85 | 5.64 | 7.72 | 6.81 |'
  prefs: []
  type: TYPE_TB
- en: '| - Neg Data | 6.55 | 4.63 | 4.40 | 3.97 | 7.88 | 6.58 |'
  prefs: []
  type: TYPE_TB
- en: '| - Neg LoRA | 5.02 | 4.81 | 5.31 | 4.75 | 7.27 | 6.05 |'
  prefs: []
  type: TYPE_TB
- en: '| - Att | 2.84 | 5.19 | 4.21 | 4.38 | 5.99 | 5.22 |'
  prefs: []
  type: TYPE_TB
- en: '| - Dual | 7.38 | 5.37 | 4.21 | 4.80 | 6.87 | 6.30 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Ablation study results (%) for NAT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the necessity of each component in NAT, we take a series of
    ablation study by removing the following parts: (1) Neg Data: The whole dual LoRA
    structure and attention integration only on positive data. (2) Neg LoRA: Based
    on (1), the negative LoRA will be further removed. (3) Att: Instead of the attention
    mechanism, we integrate two LoRA modules by a gated function. (4) Dual: We modify
    the range of Equation [4](#Sx3.E4 "In Dynamic Integration Unit ‣ Negative Assistant
    Training ‣ Methodology ‣ Turning Dust into Gold: Distilling Complex Reasoning
    Capabilities from LLMs by Leveraging Negative Data") to [0, 1] rather than [-0.5,
    0,5], which means the knowledge from negative LoRA can only be absorbed from positive
    way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown in Table [5](#Sx4.T5 "Table 5 ‣ Ablation study ‣ Analysis
    ‣ Experiments ‣ Turning Dust into Gold: Distilling Complex Reasoning Capabilities
    from LLMs by Leveraging Negative Data"). When filtering negative samples with
    same model structure, we find that model accuracy decreases, confirming the value
    of negative knowledge. Further removing the negative LoRA illustrates the importance
    of dual LoRA structure. The performance drops dramatically without attention mechanism,
    indicating it plays an important role in integrating LoRA modules. When changing
    the range of $\alpha$ to [0, 1], which forces positive LoRA to add the knowledge
    from negative LoRA without the minus option. The lower accuracy suggests that
    avoiding being influenced by undesirable behaviors while extracting useful knowledge
    from negative samples is necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bf7420cc914abb356d05c94face0912.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Analysis of $\alpha_{neg}$ along dimensions of: token position, question
    level, and question subject.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To fully comprehend how knowledge from $\mathcal{M}_{neg}$ in Eq. ([4](#Sx3.E4
    "In Dynamic Integration Unit ‣ Negative Assistant Training ‣ Methodology ‣ Turning
    Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging
    Negative Data"))) along 3 dimensions: token position, question level, and question
    subject.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the Figure [4](#Sx4.F4 "Figure 4 ‣ Attention ‣ Analysis ‣ Experiments
    ‣ Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs
    by Leveraging Negative Data"), as the position of the generated token increases,
    $\alpha_{neg}$ can play a greater role in addressing challenging subjects during
    NAT.'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy of Ranking Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f505b33112740ec3fb8252fb82b0625.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Relationship between the accuracy of distinguishing positive and
    negative rationales of $\mathcal{M}_{rank}$ and the improvement brought by ASC.'
  prefs: []
  type: TYPE_NORMAL
- en: We further explore the relationship between $\mathcal{M}_{rank}$ is only around
    60% and yet it can significantly enhance the effectiveness of SC, we believe that
    there is substantial room for improvement in ASC.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/edb9fae7f81b8b7a6d5d0a7414035f63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An intuitive example shows the strength of ASC.'
  prefs: []
  type: TYPE_NORMAL
- en: Case Study about Adaptive Self-Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We provide an intuitive example (Figure [6](#Sx4.F6 "Figure 6 ‣ Accuracy of
    Ranking Model ‣ Analysis ‣ Experiments ‣ Turning Dust into Gold: Distilling Complex
    Reasoning Capabilities from LLMs by Leveraging Negative Data")) to show the superiority
    of ASC. In this example, deceptive candidate 1 is chosen as the prediction by
    SC due to having more votes than the correct candidate 3\. $\mathcal{M}_{rank}$
    adjusts the weights of the eight candidates based on rationales, resulting in
    the reweighted correct candidate 3 obtaining a higher vote count.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work explores the effectiveness of negative data for distilling the complex
    reasoning ability from large language models to specialized small ones. We propose
    a novel framework, consisting of three progressive steps and fully leveraging
    the negative information through the entire process of model specialization. Negative
    assistant training can provide a more comprehensive way to employ the negative
    information from two aspects. Negative calibrated enhancement is able to calibrate
    the process of distillation, making it more targeted on crucial knowledge. Ranking
    model trained on two views of rationales can assign appropriate weights for answer
    aggregation to achieve adaptive self-consistency. Extensive experiments demonstrate
    that our framework can improve the effectiveness of distilling reasoning ability
    by the generated negative samples.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by Beijing Natural Science Foundation (No.4222037, L181010).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. In *Advances in
    Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023a) Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;
    Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S. M.; Nori, H.;
    Palangi, H.; Ribeiro, M. T.; and Zhang, Y. 2023a. Sparks of Artificial General
    Intelligence: Early experiments with GPT-4. *CoRR*, abs/2303.12712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023b) Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;
    Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S. M.; Nori, H.;
    Palangi, H.; Ribeiro, M. T.; and Zhang, Y. 2023b. Sparks of Artificial General
    Intelligence: Early experiments with GPT-4. *CoRR*, abs/2303.12712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
    G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.;
    Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
    N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.;
    Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat,
    S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.;
    Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi,
    R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
    Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.;
    Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck,
    D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. PaLM: Scaling Language Modeling
    with Pathways. *CoRR*, abs/2204.02311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;
    Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; and Schulman,
    J. 2021. Training Verifiers to Solve Math Word Problems. *CoRR*, abs/2110.14168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2021) Feng, S.; Ren, X.; Li, K.; and Sun, X. 2021. Multi-View Feature
    Representation for Dialogue Generation with Bidirectional Distillation. In *AAAI
    2021*, 12812–12820.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Fu, Y.; Peng, H.; Ou, L.; Sabharwal, A.; and Khot, T. 2023.
    Specializing Smaller Language Models towards Multi-Step Reasoning. *CoRR*, abs/2301.12726.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He and Glass (2020) He, T.; and Glass, J. R. 2020. Negative Training for Neural
    Dialogue Response Generation. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020*,
    2044–2058\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart,
    S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring Mathematical Problem
    Solving With the MATH Dataset. In *Proceedings of the Neural Information Processing
    Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,
    December 2021, virtual*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, Vinyals, and Dean (2015) Hinton, G. E.; Vinyals, O.; and Dean, J. 2015.
    Distilling the Knowledge in a Neural Network. *CoRR*, abs/1503.02531.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ho, Schmid, and Yun (2023) Ho, N.; Schmid, L.; and Yun, S. 2023. Large Language
    Models Are Reasoning Teachers. In *Proceedings of the 61st Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,
    Canada, July 9-14, 2023*, 14852–14882\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsieh et al. (2023) Hsieh, C.; Li, C.; Yeh, C.; Nakhost, H.; Fujii, Y.; Ratner,
    A.; Krishna, R.; Lee, C.; and Pfister, T. 2023. Distilling Step-by-Step! Outperforming
    Larger Language Models with Less Training Data and Smaller Model Sizes. In *Findings
    of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July
    9-14, 2023*, 8003–8017\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,
    S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of Large Language Models.
    In *The Tenth International Conference on Learning Representations, ICLR 2022,
    Virtual Event, April 25-29, 2022*. OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Huang, J.; Gu, S. S.; Hou, L.; Wu, Y.; Wang, X.; Yu, H.;
    and Han, J. 2022. Large Language Models Can Self-Improve. *CoRR*, abs/2210.11610.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2020) Jiao, X.; Yin, Y.; Shang, L.; Jiang, X.; Chen, X.; Li, L.;
    Wang, F.; and Liu, Q. 2020. TinyBERT: Distilling BERT for Natural Language Understanding.
    In *Findings of the Association for Computational Linguistics: EMNLP 2020, Online
    Event, 16-20 November 2020*, volume EMNLP 2020 of *Findings of ACL*, 4163–4174\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,
    Y. 2022. Large Language Models are Zero-Shot Reasoners. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lagutin, Gavrilov, and Kalaidin (2021) Lagutin, E.; Gavrilov, D.; and Kalaidin,
    P. 2021. Implicit Unlikelihood Training: Improving Neural Text Generation with
    Reinforcement Learning. In *Proceedings of the 16th Conference of the European
    Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021,
    Online, April 19 - 23, 2021*, 1432–1441\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Lewkowycz, A.; Andreassen, A.; Dohan, D.; Dyer, E.;
    Michalewski, H.; Ramasesh, V. V.; Slone, A.; Anil, C.; Schlag, I.; Gutman-Solo,
    T.; Wu, Y.; Neyshabur, B.; Gur-Ari, G.; and Misra, V. 2022. Solving Quantitative
    Reasoning Problems with Language Models. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Li, M.; Roller, S.; Kulikov, I.; Welleck, S.; Boureau, Y.;
    Cho, K.; and Weston, J. 2020. Don’t Say That! Making Inconsistent Dialogue Unlikely
    with Unlikelihood Training. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020*,
    4715–4728\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Li, Y.; Feng, S.; Sun, B.; and Li, K. 2022. Diversifying Neural
    Dialogue Generation via Negative Distillation. In *Proceedings of the 2022 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15,
    2022*, 407–418\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Li, Y.; Feng, S.; Sun, B.; and Li, K. 2023. Heterogeneous-Branch
    Collaborative Learning for Dialogue Generation. In *AAAI 2023*, 13148–13156.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Liu, W.; Zhou, P.; Wang, Z.; Zhao, Z.; Deng, H.; and Ju,
    Q. 2020. FastBERT: a Self-distilling BERT with Adaptive Inference Time. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, 6035–6044\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Magister et al. (2023) Magister, L. C.; Mallinson, J.; Adámek, J.; Malmi, E.;
    and Severyn, A. 2023. Teaching Small Language Models to Reason. In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, 1773–1781\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao, Liang, and Su (2020) Miao, S.; Liang, C.; and Su, K. 2020. A Diverse Corpus
    for Evaluating and Developing English Math Word Problem Solvers. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, 975–984\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mobahi, Farajtabar, and Bartlett (2020) Mobahi, H.; Farajtabar, M.; and Bartlett,
    P. L. 2020. Self-Distillation Amplifies Regularization in Hilbert Space. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patel, Bhattamishra, and Goyal (2021) Patel, A.; Bhattamishra, S.; and Goyal,
    N. 2021. Are NLP Models really able to Solve Simple Math Word Problems? In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*,
    2080–2094. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajani et al. (2019) Rajani, N. F.; McCann, B.; Xiong, C.; and Socher, R. 2019.
    Explain Yourself! Leveraging Language Models for Commonsense Reasoning. In *Proceedings
    of the 57th Conference of the Association for Computational Linguistics, ACL 2019,
    Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers*, 4932–4942\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2015) Roy, S.; and Roth, D. 2015. Solving General Arithmetic Word
    Problems. In *Proceedings of the 2015 Conference on Empirical Methods in Natural
    Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015*, 1743–1752\.
    The Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019. DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter. *CoRR*, abs/1910.01108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2023) Shao, Z.; Gong, Y.; Shen, Y.; Huang, M.; Duan, N.; and Chen,
    W. 2023. Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large
    Language Models. In *International Conference on Machine Learning, ICML 2023,
    23-29 July 2023, Honolulu, Hawaii, USA*, volume 202 of *Proceedings of Machine
    Learning Research*, 30706–30775\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019) Tang, R.; Lu, Y.; Liu, L.; Mou, L.; Vechtomova, O.; and Lin,
    J. 2019. Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.
    *CoRR*, abs/1903.12136.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.;
    Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation
    Language Models. *CoRR*, abs/2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.;
    Narang, S.; Chowdhery, A.; and Zhou, D. 2023. Self-Consistency Improves Chain
    of Thought Reasoning in Language Models. In *The Eleventh International Conference
    on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud,
    S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; Chi, E. H.; Hashimoto, T.;
    Vinyals, O.; Liang, P.; Dean, J.; and Fedus, W. 2022a. Emergent Abilities of Large
    Language Models. *Trans. Mach. Learn. Res.*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;
    Xia, F.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2022b. Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2020) Welleck, S.; Kulikov, I.; Roller, S.; Dinan, E.; Cho,
    K.; and Weston, J. 2020. Neural Text Generation With Unlikelihood Training. In
    *8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
    Ethiopia, April 26-30, 2020*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Xu, X.; Wang, G.; Kim, Y.; and Lee, S. 2021. AugNLG: Few-shot
    Natural Language Generation using Self-trained Data Augmentation. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, 1183–1195\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zelikman et al. (2022) Zelikman, E.; Wu, Y.; Mu, J.; and Goodman, N. D. 2022.
    STaR: Bootstrapping Reasoning With Reasoning. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Zheng, C.; Liu, Z.; Xie, E.; Li, Z.; and Li, Y. 2023. Progressive-Hint
    Prompting Improves Reasoning in Large Language Models. *CoRR*, abs/2304.09797.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Zhou, D.; Schärli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,
    X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V.; and Chi, E. H. 2023. Least-to-Most
    Prompting Enables Complex Reasoning in Large Language Models. In *The Eleventh
    International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
    May 1-5, 2023*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Zhu, X.; Qi, B.; Zhang, K.; Long, X.; and Zhou, B. 2023.
    PaD: Program-aided Distillation Specializes Large Models in Reasoning. *CoRR*,
    abs/2305.13888.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Dataset | Subjects | Train | Test |'
  prefs: []
  type: TYPE_TB
- en: '| MATH | Probability | 771 | 474 |'
  prefs: []
  type: TYPE_TB
- en: '| InterAlgebra | 1295 | 903 |'
  prefs: []
  type: TYPE_TB
- en: '| NumberTheory | 869 | 540 |'
  prefs: []
  type: TYPE_TB
- en: '| Precalculus | 746 | 546 |'
  prefs: []
  type: TYPE_TB
- en: '| Prealgebra | 1205 | 871 |'
  prefs: []
  type: TYPE_TB
- en: '| Geometry | 870 | 479 |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | 1744 | 1187 |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | - | 7473 | 1319 |'
  prefs: []
  type: TYPE_TB
- en: '| ASDiv | - | - | 1218 |'
  prefs: []
  type: TYPE_TB
- en: '| MultiArith | - | - | 600 |'
  prefs: []
  type: TYPE_TB
- en: '| SVAMP | - | - | 1000 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Statistical information about the datasets utilized in our experiments.
    we focus on MATH dataset and the four datasets below are for generalization evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#A1.T6 "Table 6 ‣ Dataset ‣ Appendix A Appendix ‣ Turning Dust into
    Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative
    Data") shows the detailed data statistics. In this study, we focus on MATH dataset
    and the four datasets below are intended for the purpose of evaluating generalization.
    For saving space, subjects in experiments section will be shown in short form.
    CP, IA, NT, PC, PA, G and A denote Counting and Probability, Intermediate Algebra,
    Number Theory, Precalculus, Prealgebra, Geometry and Algebra respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Details of Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For learning from negative views, four baseline methods will be introduced
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MIX directly trains LLaMA with the mixture of both positive data $\mathcal{D}_{pos}$
    by maximizing the following expectation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{(x,\hat{r},\hat{y})\sim\mathcal{D}_{pos+neg}}\mathrm{log}P(\hat{y},\hat{r}&#124;x;\theta).$
    |  | (12) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CL (contrastive learning) learns a representation of data such that the problem
    $x$ of positive samples are close together in the representation space, while
    negative samples are far apart. In this work, the following expectation is maximized:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (13) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbb{E}_{(x,\hat{r}^{+},\hat{y}^{+})\sim\mathcal{D}_{pos}}\mathrm{log}P(\hat{y},\hat{r}&#124;x;\theta)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $1$2 |  | (14) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NT (negative training) (He and Glass [2020](#bib.bib8)) conducts negative updates
    with training signals from negative samples to avoid model generating such data.
    The training objective is to maximize the following expectation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{(x,\hat{r},\hat{y})\sim\mathcal{D}_{pos}}\mathrm{log}P(\hat{y},\hat{r}&#124;x;\theta)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle-\lambda_{2}*\mathbb{E}_{(x,\hat{r},\hat{y})\sim\mathcal{D}_{neg}}\mathrm{log}P(\hat{y},$
    |  | (15) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The mixing hyper-parameter $\lambda_{2}$, and 0.05 is selected for its best
    performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UL (unlikelihood loss) (Welleck et al. [2020](#bib.bib36)) penalizes the model
    for outputting words with certain characteristics by introducing an unlikelihood
    loss term. In this work, we just penalize the negative samples in sentence level:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{(x,\hat{r},\hat{y})\sim\mathcal{D}_{pos}}\mathrm{log}P(\hat{y},\hat{r}&#124;x;\theta)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle+\lambda_{3}*\mathbb{E}_{(x,\hat{r},\hat{y})\sim\mathcal{D}_{neg}}\mathrm{log}P(1-\hat{y},$
    |  | (16) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Similar to NT, we search the mixing hyper-parameter $\lambda_{3}$, and 0.05
    is selected as the best.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generalization Ability to Other Reasoning Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We selected MATH as our primary dataset for its difficulty and significance
    of transferring the capability from LLMs to small models. In this paper, we have
    verified the generalization ability to other simpler datasets including GSM8K,
    ASDiv, MultiArith and SVAMP. Here we also conduct experiments on commonsense reasoning
    with StrategyQA dataset, the results from Table [7](#A1.T7 "Table 7 ‣ Generalization
    Ability to Other Reasoning Tasks ‣ Appendix A Appendix ‣ Turning Dust into Gold:
    Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data")
    can demonstrate the generalization ability of NAT to other reasoning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Data Source | Methods | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA 7B | Raw | Fine-tune | 61.2 +0% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-Turbo | CoT KD | 69.2 +13.0% |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | 70.4 +15.0% |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | CoT KD | 72.4 +18.3% |'
  prefs: []
  type: TYPE_TB
- en: '| NAT | 72.8 +19.0% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Experimental results (%) on StrategyQA.'
  prefs: []
  type: TYPE_NORMAL
- en: NCE Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11cbb5eb45e6f05bf4b313fb37f40655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Analysis of $\beta$ along dimensions of: question level and question
    subject.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to further understand the effectiveness of NCE, we conducted experiments
    to analyze the relationship between $\beta$ and using KL divergence is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Intersection | NAT | Neg | IoU |'
  prefs: []
  type: TYPE_TB
- en: '| Probability | 6 | 26 | 16 | 0.167 |'
  prefs: []
  type: TYPE_TB
- en: '| InterAlgebra | 6 | 44 | 21 | 0.102 |'
  prefs: []
  type: TYPE_TB
- en: '| NumberTheory | 2 | 36 | 17 | 0.039 |'
  prefs: []
  type: TYPE_TB
- en: '| Precalculus | 3 | 21 | 16 | 0.057 |'
  prefs: []
  type: TYPE_TB
- en: '| Prealgebra | 15 | 83 | 43 | 0.135 |'
  prefs: []
  type: TYPE_TB
- en: '| Geometry | 2 | 26 | 10 | 0.057 |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | 16 | 92 | 43 | 0.134 |'
  prefs: []
  type: TYPE_TB
- en: '| Overall | 49 | 328 | 166 | 0.110 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: The distribution of correct samples in MATH test set. Neg refers to
    model only trained by negative samples, and NAT refers to proposed method.'
  prefs: []
  type: TYPE_NORMAL
- en: '| system: Think the question step by step and give the answer. |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: Find the domain of the expression $\frac{\sqrt{x-2}}{\sqrt{5-x}}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: The expressions inside each square root must be non-negative.
    Therefore, $x-2\geq 0$. |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: Compute: $55\times 1212-15\times 1212$ . |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: We have $55\times 1212-15\times 1212=1212(55-15)=1212(40)=4848(10)=\boxed{48480}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: Compute $\dbinom{16}{15}$. |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: $\dbinom{16}{15}=\dbinom{16}{1}=\boxed{16}.$ |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: Find $x$ is a square number which is divisible by four, and
    between 39 and 80. |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: We know that $x$. |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: Solve the inequality <math id="A1.Ex4.m1.7" class="ltx_Math"
    alttext="\frac{(x-2)(x-3)(x-4)}{(x-1)(x-5)(x-6)}></math> |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: We can build a sign chart, but since all of the factors
    are linear, we can track what happens to the expression as $x$ |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: A right circular cone has a volume of $12\pi$? |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: The volume of a cone is $\frac{1}{3}\pi r^{2}h$. |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: How many perfect squares less than 1000 have a ones digit
    of 2, 3 or 4? |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: Checking the squares from $1^{2}$. |'
  prefs: []
  type: TYPE_TB
- en: '| user: Problem: The diagonals of a rhombus are $10$ inches. What is the perimeter
    of the rhombus, in inches? |'
  prefs: []
  type: TYPE_TB
- en: '| assistant: Solution: The diagonals are perpendicular bisectors of each other,
    so therefore the side length of the rhombus can be calculated as $\sqrt{5^{2}+12^{2}}=13$
    inches. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Demonstrations used to obtain responses from GPT-3.5 Turbo (GPT-4).'
  prefs: []
  type: TYPE_NORMAL
- en: Overlap Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our initial discovery was that the IoU of the models trained on $\mathcal{D}_{neg}$
    and truly achieve turning dust into gold.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We provide the prompt to obtain responses from GPT-3.5 Turbo (GPT-4) in Table [9](#A1.T9
    "Table 9 ‣ NCE Analysis ‣ Appendix A Appendix ‣ Turning Dust into Gold: Distilling
    Complex Reasoning Capabilities from LLMs by Leveraging Negative Data"). We follow
    Shao et al. ([2023](#bib.bib30)) to randomly sampled eight samples from different
    subjects and levels in the training set of MATH datasets to form this prompt.'
  prefs: []
  type: TYPE_NORMAL
