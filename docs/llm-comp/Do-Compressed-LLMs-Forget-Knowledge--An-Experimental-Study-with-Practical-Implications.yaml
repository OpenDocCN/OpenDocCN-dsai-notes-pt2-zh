- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.00867](https://ar5iv.labs.arxiv.org/html/2310.00867)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Duc Hoang    Minsik Cho    Thomas Merth    Mohammad Rastegari    Zhangyang Wang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compressing Large Language Models (LLMs) often leads to reduced performance,
    especially for knowledge-intensive tasks. In this work, we dive into how compression
    damages LLMs’ inherent knowledge and the possible remedies. We start by proposing
    two conjectures on the nature of the damage: one is certain knowledge being forgotten
    (or erased) after LLM compression, hence necessitating the compressed model to
    (re)learn from data with additional parameters; the other presumes that knowledge
    is internally displaced and hence one requires merely “inference re-direction”
    with input-side augmentation such as prompting, to recover the knowledge-related
    performance. Extensive experiments are then designed to (in)validate the two conjectures.
    We observe the promise of prompting in comparison to model tuning; we further
    unlock prompting’s potential by introducing a variant called Inference-time Dynamic
    Prompting (IDP), that can effectively increase prompt diversity without incurring
    any inference overhead. Our experiments consistently suggest that compared to
    the classical re-training alternatives such as LoRA, prompting with IDP leads
    to better or comparable post-compression performance recovery, while saving the
    extra parameter size by $\mathbf{21\times}$ and reducing inference latency by
    60%. Our experiments hence strongly endorse the conjecture of “knowledge displaced”
    over “knowledge forgotten”, and shed light on a new efficient mechanism to restore
    compressed LLM performance. We additionally visualize and analyze the different
    attention and activation patterns between prompted and re-trained models, demonstrating
    they achieve performance recovery in two different regimes.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) like GPT-4 and ChatGPT have emerged as powerful
    tools in language generation and reasoning, pushing the boundaries of AI to rival
    human-like capabilities (OpenAI, [2023](#bib.bib16)). These advancements, however,
    are accompanied by significant challenges, primarily their massive size and the
    consequent high computational costs (Chen et al., [2023](#bib.bib3)). This has
    led to a growing emphasis on model compression as a means to make LLMs more accessible
    and efficient for broader industrial applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model compression techniques, such as quantization and sparsification, have
    since become increasingly popular for reducing the size of LLMs without significantly
    compromising their performance. Traditional approaches often involve post-compression
    re-training to mitigate performance losses (Han et al., [2015](#bib.bib7)). More
    recent ‘training-free’ compression methods, like GPTQ (Frantar et al., [2022](#bib.bib6))
    and SparseGPT (Frantar & Alistarh, [2023](#bib.bib5)), promise minimal impact
    on perplexity and standard task benchmarks. Nevertheless, studies like Jaiswal
    et al. ([2023](#bib.bib11)) reveal that these compressed models still suffer from
    reduced effectiveness in knowledge-intensive language generation or reasoning
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'What transpires within a compressed model that leads to diminished performance
    on tasks demanding extensive knowledge? Is this knowledge permanently lost in
    the compression process, or is it merely obscured? Addressing these questions
    is not solely of theoretical interest; it has tangible implications for devising
    strategies to effectively counteract the impacts of compression on model knowledge.
    To this end, we introduce two hypotheses regarding the root cause of this performance
    degradation: the first posits that key knowledge is forgotten (or erased) as a
    consequence of LLM compression, necessitating a re-learning process with the addition
    of extra parameters (Hu et al., [2021](#bib.bib8)); the second hypothesis suggests
    that the knowledge is merely internally displaced within the LLM. This implies
    that strategic redirection of knowledge flow, potentially through input-side enhancements
    like prompting (Xu et al., [2023](#bib.bib24)), could efficiently recover model
    accuracy. A more comprehensive exploration of these ideas is presented in Section
    2.2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We embarked on a series of extensive experiments to test two central hypotheses
    of compressed LLM performance loss: “knowledge displaced” versus “knowledge forgotten”.
    To effectively validate these hypotheses, we scrutinized existing prompt-tuning
    methods and recognized their limitations, particularly the reliance on a single
    prompt across varied input formats and knowledge domains. This led us to propose
    the Inference-time Dynamic Prompting (IDP) approach to auto-select appropriate
    prompts per input. IDP sets itself apart from previous ensemble techniques (Raffel
    et al., [2020](#bib.bib19); PENG et al., [2023](#bib.bib18)), offering a one-shot
    selection process with nearly no overhead compared to one fixed prompt, thanks
    to Key-Value caching.'
  prefs: []
  type: TYPE_NORMAL
- en: Our empirical findings strongly support the hypothesis of “knowledge displaced”
    over “knowledge forgotten” by demonstrating prompting by IDP leads to more favorable
    cost-effectiveness in performance recovery. As illustrated in Figure [4](#S3.F4
    "Figure 4 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods and Experiments ‣
    Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications"),
    compared to classical model re-training methods using LoRA, IDP achieved comparable
    or superior performance in adapting compressed LLM models across various tasks,
    while attaining a remarkable reduction in parameter overhead – up to 21 times,
    besides reducing inference latency by 60%. The performance of IDP is robust even
    at fairly short prompt lengths (Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge
    Forgetfulness and Displacement ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications")). Our investigation into layer-wise
    cosine similarity (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 IDP is remarkably more efficient
    ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications")) further revealed that, compared to baseline
    attention patterns, prompt-tuning via IDP leads to significant divergences, whereas
    re-trained models tend to align more closely with the baseline, despite achieving
    similar outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We critically examine the impact of compression on LLMs’ knowledge, formally
    raising the conjectures of knowledge ’displacement’ versus ’forgetfulness’.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We thoughtfully design experiments to endorse the hypothesis of “knowledge displaced”
    over “knowledge forgotten”. We also reveal a number of insights, including two
    different regimes of performance recovery.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the practical implication, all experimental results collectively underscore
    the efficacy of our newly designed IDP method - achieving similar performance
    recovery to LoRA, at orders-of-magnitude lower parameter and latency overheads,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Conjecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/270b8530470cb39341c6763a33e94eeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: This figure presents a comparative analysis of the performance of
    compressed models using GPTQ for quantization and SparseGPT for pruning. The models
    were compressed leveraging either C4 or Wikitext datasets. Their average performance
    is depicted across a spectrum of nine tasks, each representing diverse knowledge
    domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 LLM compression background & caveats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In compression, we address the challenges of size and latency inherent to LLMs
    by targeting the model’s parameters. Broadly, compressive techniques are grouped
    into two main categories: compression-aware training and post-training compression.
    Post-training compression holds particular appeal for exceptionally large models
    where the costs associated with full model training or even fine-tuning can be
    prohibitive. Given its relevance, we narrow our discussion to this category.'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, quantization refers to reducing the model’s footprint by decreasing
    the bit precision of its weights (Frantar et al., [2022](#bib.bib6); Yao et al.,
    [2022](#bib.bib25); Xiao et al., [2022](#bib.bib23)). Quantization not only shrinks
    the model’s size but also accelerates inference, as operations over lower-precision
    weights are computationally less demanding. Secondly, sparsification, often referred
    to as pruning, revolves around the concept of selectively removing certain weight
    elements or masking activation values (Frantar & Alistarh, [2023](#bib.bib5);
    Hubara et al., [2021a](#bib.bib9), [b](#bib.bib10)). The objective is to trim
    the less salient portions of the model, thereby reducing computational overhead
    or enhancing model throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Using GPTQ and SparseGPT for model compression, Figure [1](#S2.F1 "Figure 1
    ‣ 2 Background and Conjecture ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications") shows a performance drop when lowering bit
    counts or parameters, except for the int8 quantization. This trend aligns with
    the claims by Frantar et al. ([2022](#bib.bib6)) and Frantar & Alistarh ([2023](#bib.bib5))
    that their methods are optimized for the largest LLMs. This evident limitation
    on smaller (yet still substantial) LLMs highlights the imperative for additional
    performance improvement post-compression beyond just parameter adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Forgotten, or Displaced? A Two-Way Argument
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Investigating whether knowledge in compressed models is forgotten or merely
    displaced presents a complex challenge. However, discerning between these two
    scenarios is feasible by examining the nature of intervention required to reinstate
    the model’s performance in downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forgetfulness implies that the compression process irrevocably eliminates certain
    knowledge. Integrating an external knowledge source becomes essential to recuperate
    performance, as this process essentially replenishes the lost information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displacement posits that the inherent knowledge within these models is not irrevocably
    erased but instead shifted internally, leading to the inefficacy of the established
    inference pathways. In this context, input-side augmentation or instructions are
    needed to “redirect” the internal self-attention. This enables the re-engagement
    of the pre-existing, albeit repositioned, knowledge in the compressed LLM, thereby
    aiding in the recuperation of its performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We position LoRA (Hu et al., [2021](#bib.bib8)) and prompting to correlate respectively
    with our hypothesis on “knowledge forgotten” and “knowledge displaced.” LoRA tackles
    “forgetfulness” by fundamentally altering the model’s structure, specifically
    the weights in the self-attention and feedforward neural network (FFN) layers,
    thereby reintegrating knowledge lost due to compression. Prompting, in contrast,
    operates by subtly influencing the self-attention mechanism without changing the
    underlying weights, thus redirecting the model’s existing but less accessible
    knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods and Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 From Basic Prompting to IDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building upon the initial work of Xu et al. ([2023](#bib.bib24)), which utilized
    prompting to enhance the performance of compressed models measured by perplexity,
    we identify a crucial limitation in this approach. As Jaiswal et al. ([2023](#bib.bib11))
    highlights, perplexity alone may not fully capture a model’s actual behavior in
    compression scenarios. To (in)validate this finding, we contrast the perplexity
    metric with the aggregated accuracy performance across nine downstream tasks using
    varying prompt lengths, aiming to discern any discrepancies between these two
    metrics. Our findings, illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 From Basic
    Prompting to IDP ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications"), reveal a notable perplexity-to-performance
    gap that becomes more pronounced with longer prompts. This divergence not only
    corroborates the observations of Jaiswal et al. ([2023](#bib.bib11)) but also
    underscores the limitations in relying solely on perplexity as a performance indicator,
    as initially proposed by Xu et al. ([2023](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We show that the presumed efficacy of extended prompts is, in fact, compromised
    by their intrinsic rigidity, leading us to a conclusion: further improvement of
    prompting hinges on the precise alignment of the right prompt with the appropriate
    input rather than the elongation of a single prompt. This concept parallels ensemble
    methods but requires a departure from iterative or training-intensive approaches,
    such as those found in Lester et al. ([2021](#bib.bib13)) or PENG et al. ([2023](#bib.bib18)),
    which significantly increase training time and inference costs. To circumvent
    these drawbacks, we introduce Inference-time Dynamic Prompting (IDP). IDP enables
    one-shot input-to-prompt matching with minimal latency increase to inference time.
    This strategy aligns prompts more effectively with inputs and incurs little computational
    overhead, marking a stepped improvement in prompting for compressed model performance
    recovery.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/669e799367f6fda742dd42cf08b12b92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Using a 3-bit quantized Llama-7b model fine-tuned on C4 dataset,
    we contrast the average accuracy across nine tasks against its word’s perplexity
    score across various prompt lengths. A longer sequence length improves perplexity
    but does not always sustain better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 IDP Methodology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f6fc9154e908eea1a290ff8aeeeec540.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: This figure underscores the key advantage of inference-time dynamic
    prompting (IDP): its minimalistic yet effective design. By making straightforward
    alterations to the existing weighted sum operation and using the existing attention
    matrix for prompt selection, IDP accomplishes its objectives without incurring
    any additional parameter costs.'
  prefs: []
  type: TYPE_NORMAL
- en: In prompt tuning, we introduce an additional token sequence, termed as $P$ as
    their embedding size.
  prefs: []
  type: TYPE_NORMAL
- en: When we extend to a collection of $m$ as one.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate Inference-time Dynamic Prompting, we introduce two modifications
    to $A$. In the final phase of the self-attention mechanism, we use an attention
    mask to discard any unintended prompts, ensuring they do not modify the main input
    sequence and improve our inference latency. The process is depicted in Figure
    [3](#S3.F3 "Figure 3 ‣ 3.1.1 IDP Methodology ‣ 3.1 From Basic Prompting to IDP
    ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications").
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Experimental Comparison: IDP recovers performance better or comparable
    than LoRA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1 Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compressed Models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We utilize OPT-6.7b (Zhang et al., [2022](#bib.bib27)) and Llama-7b (Touvron
    et al., [2023](#bib.bib21)) as foundational models, both featuring an embedding
    size “e” of 4096\. For compression, we apply GPTQ (Frantar et al., [2022](#bib.bib6))
    and SparseGPT (Frantar & Alistarh, [2023](#bib.bib5)) to achieve 3-bit quantization
    and 50% pruning, respectively. In our discussion, we will primarily focus on the
    quantization approach, as the pruning process exhibits a very similar pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Find-tuning Dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We derive two configurations for each compression technique, each optimized
    on one of the large-scale text datasets: C4 (Raffel et al., [2020](#bib.bib19))
    and Wikitext (Merity et al., [2016](#bib.bib15)). To maintain a controlled experimental
    space, our fine-tuning of various baseline techniques is restricted to the identical
    dataset used initially to calibrate our model compression. We utilize two distinct
    prompts for IDP-specific settings, ”$m$,” being 50 and 100.'
  prefs: []
  type: TYPE_NORMAL
- en: Validation Tasks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To gauge the genuine comprehensive performance of LLMs, we identify a suite
    of evaluation tasks that encapsulate three fundamental domains of cognition: world
    knowledge, common reasoning, and language understanding. Among the many available
    tasks, we distilled our focus to a curated list of nine that we deemed most representative.'
  prefs: []
  type: TYPE_NORMAL
- en: For the domain of world knowledge, our chosen evaluative tasks were ARC-challenge
    & ARC-easy (Clark et al., [2018](#bib.bib4)), SCIQ (Welbl et al., [2017](#bib.bib22)),
    WebQS (Berant et al., [2013](#bib.bib1)), and TriviaQA (Joshi et al., [2017](#bib.bib12)).
    Tapping into the breadth of language understanding benchmarks, we centered our
    attention on Hellaswag (Zellers et al., [2019](#bib.bib26)), Lambada (Paperno
    et al., [2016](#bib.bib17)), and WinoGrande (Sakaguchi et al., [2019](#bib.bib20)).
    Lastly, for common reasoning, we identified PIQA (Bisk et al., [2019](#bib.bib2))
    as our touchstone. Notably, all the tasks we adopted are structured in a multiple-choice
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We gravitated toward three methodologies to test our hypothesis. We use prompt-tuning
    (Lester et al., [2021](#bib.bib13)), prefix-tuning (Li & Liang, [2021](#bib.bib14)),
    and LoRA (Hu et al., [2021](#bib.bib8)) as our representative candidates. For
    consistent benchmarks across these techniques, we establish the following criteria:
    1) The aggregate count of training tokens is standardized at 40,960,000 tokens.
    Our decision on the total token count draws inspiration from (Xu et al., [2023](#bib.bib24)).
    2) In alignment with (Frantar et al., [2022](#bib.bib6)), we adopt AdamW as our
    optimization algorithm. Our chosen learning rate stands at 2e-4 with a weight
    decay set at 1e-5\. All three methods are then fine-tuned using compressed LLM
    following the described settings with LLama-7b and OPT-6.7b.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: This table summarizes the results for 3-bit GPTQ across all nine tasks
    for multiple fine-tuning baselines and our IDP. World, Common, and Language are
    performance averages across tasks within those knowledge domains. Average is the
    average performance across all nine tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Type Param arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada
    winogrande Language Average Llama-7b — — 71.46 37.71 92.60 17.96 33.02 50.55 76.01
    76.01 53.11 68.58 67.48 63.06 57.55 Llama-7b lora 4.4M 70.08 37.12 93.50 17.67
    34.11 50.50 77.04 77.04 54.47 70.48 67.40 64.12 57.99 Llama-7b lora 6.7M 71.09
    36.69 93.00 17.47 34.73 50.60 76.44 76.44 54.55 70.23 67.09 63.96 57.92 Llama-7b
    lora 8.9M 70.62 37.12 93.30 17.86 34.86 50.75 76.77 76.77 54.27 70.33 67.40 64.00
    58.06 Llama-7b prompt 0.1M 71.97 38.40 92.90 20.47 33.20 51.39 75.84 75.84 53.75
    69.45 67.17 63.46 58.13 Llama-7b prompt 0.2M 71.51 38.31 92.10 21.11 34.56 51.52
    75.84 75.84 53.92 69.69 68.75 64.12 58.42 Llama-7b prompt 0.4M 72.01 39.16 91.80
    21.60 34.43 51.80 75.95 75.95 54.33 69.49 67.01 63.61 58.42 Llama-7b ptune 3.1M
    70.24 36.77 91.40 14.42 30.42 48.65 75.73 75.73 53.40 66.49 63.77 61.22 55.85
    Llama-7b ptune 6.5M 69.57 34.81 91.30 15.55 30.65 48.38 75.30 75.30 52.98 64.84
    63.22 60.35 55.36 Llama-7b ptune 13.1M 69.32 34.73 88.70 16.14 27.84 47.35 74.59
    74.59 52.01 64.35 64.17 60.18 54.65 \hdashlineLlama-7b IDP 0.6M 72.43 39.76 92.50
    19.83 36.39 52.18 76.44 76.44 53.96 70.25 67.56 63.92 58.79 OPT-6.7b — — 64.77
    29.01 89.40 9.50 17.90 42.12 75.24 75.24 48.57 65.34 63.54 59.15 51.47 OPT-6.7b
    lora 4.7M 63.55 28.75 88.50 11.42 18.84 42.21 76.22 76.22 49.14 66.16 63.46 59.59
    51.78 OPT-6.7b lora 7.1M 64.27 29.01 89.20 11.07 18.95 42.50 75.90 75.90 48.89
    66.50 64.40 59.93 52.02 OPT-6.7b lora 9.4M 64.06 29.35 88.20 13.24 18.90 42.75
    76.01 76.01 49.12 66.64 63.93 59.90 52.16 OPT-6.7b prompt 0.1M 64.27 28.41 89.80
    10.73 18.22 42.50 76.01 76.01 49.05 65.34 63.22 59.20 51.79 OPT-6.7b prompt 0.2M
    64.94 28.84 89.90 10.88 18.80 42.67 75.63 75.63 49.13 65.96 63.77 59.62 51.98
    OPT-6.7b prompt 0.4M 64.60 28.50 89.70 11.52 18.76 42.62 76.12 76.12 48.82 65.90
    63.54 59.42 51.94 OPT-6.7b ptune 3.1M 63.05 28.84 89.00 10.73 18.39 42.00 75.95
    75.95 48.38 64.68 60.85 57.97 51.10 OPT-6.7b ptune 6.5M 62.88 28.58 88.80 10.43
    18.34 41.81 75.79 75.79 48.54 65.17 60.93 58.21 51.05 OPT-6.7b ptune 13.1M 62.54
    29.18 88.60 10.43 18.37 41.82 75.52 75.52 48.72 65.32 63.38 59.14 51.34 \hdashlineOPT-6.7b
    IDP 0.6M 64.18 28.67 90.40 11.96 19.05 42.85 76.17 76.17 49.03 66.82 63.22 59.69
    52.17
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: This table summarizes the results for 50% unstructured sprase using
    SparseGPT across all nine tasks for multiple fine-tuning baselines and our IDP.
    World, Common, and Language are performance averages across tasks within those
    knowledge domains. Average is the average performance across all nine tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Type Param arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada
    winogrande Language Average Llama-7b — — 70.33 37.03 93.50 14.07 28.88 48.76 77.04
    77.04 51.68 74.54 68.03 64.75 57.23 Llama-7b lora 4.4M 71.04 37.63 91.90 14.47
    33.28 49.66 76.99 76.99 53.98 70.95 67.17 64.03 57.49 Llama-7b lora 6.7M 70.79
    36.69 92.40 15.85 33.02 49.75 76.71 76.71 53.91 71.03 68.03 64.32 57.60 Llama-7b
    lora 8.9M 71.04 37.88 92.10 14.86 32.85 49.75 77.20 77.20 54.01 70.70 68.03 64.25
    57.63 Llama-7b prompt 0.1M 71.59 38.74 93.10 15.21 29.66 49.66 77.04 77.04 53.48
    71.24 67.48 64.07 57.50 Llama-7b prompt 0.2M 71.38 38.57 92.20 14.86 30.48 49.50
    77.15 77.15 53.75 71.76 67.09 64.20 57.47 Llama-7b prompt 0.4M 71.38 38.31 92.60
    14.86 30.86 49.60 77.31 77.31 53.97 70.99 67.17 64.04 57.49 Llama-7b ptune 3.1M
    63.17 32.59 88.20 11.81 24.60 44.07 72.63 72.63 50.18 64.97 56.91 57.35 51.67
    Llama-7b ptune 6.5M 67.17 34.90 88.70 12.11 24.74 45.52 74.76 74.76 50.36 65.59
    59.12 58.36 53.05 Llama-7b ptune 13.1M 65.78 31.40 87.20 11.61 21.97 43.59 74.21
    74.21 49.77 63.87 59.43 57.69 51.69 \hdashlineLlama-7b IDP 0.6M 72.05 39.08 92.90
    14.91 30.35 49.86 77.09 77.09 53.90 70.35 67.17 63.81 57.53 OPT-6.7b — — 63.01
    28.41 89.40 9.69 17.79 41.66 75.19 75.19 47.67 70.56 63.93 60.72 51.74 OPT-6.7b
    lora 4.7M 64.06 29.61 88.60 10.58 18.26 42.22 75.57 75.57 48.52 66.60 64.33 59.82
    51.79 OPT-6.7b lora 7.1M 63.93 29.78 88.20 10.14 18.48 42.11 75.90 75.90 48.58
    66.45 64.56 59.86 51.78 OPT-6.7b lora 9.4M 62.84 29.86 88.30 10.33 18.79 42.02
    75.41 75.41 48.76 66.49 65.19 60.15 51.77 OPT-6.7b prompt 0.1M 63.09 28.58 90.70
    12.30 18.75 42.68 75.14 75.14 48.40 68.78 63.69 60.29 52.16 OPT-6.7b prompt 0.2M
    63.68 29.44 90.60 12.40 18.36 42.90 75.24 75.24 48.58 67.86 63.22 59.89 52.15
    OPT-6.7b prompt 0.4M 64.06 29.27 89.60 12.80 19.12 42.97 75.19 75.19 48.49 67.49
    63.61 59.86 52.18 OPT-6.7b ptune 3.1M 61.03 28.50 86.90 13.09 19.46 41.80 72.74
    72.74 46.44 62.08 59.67 56.06 49.99 OPT-6.7b ptune 6.5M 63.01 29.86 88.00 9.40
    17.10 41.47 75.08 75.08 47.84 64.89 61.80 58.18 50.78 OPT-6.7b ptune 13.1M 60.94
    29.10 88.60 13.53 19.95 42.42 73.39 73.39 46.93 62.68 62.19 57.27 50.81 \hdashlineOPT-6.7b
    IDP 0.6M 64.06 29.27 89.60 12.80 19.12 42.97 75.19 75.19 48.49 67.49 63.61 59.86
    52.18
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare several post-compression performance recovery techniques with IDP.
    We report our findings in Table [1](#S3.T1 "Table 1 ‣ Baseline Methods ‣ 3.2.1
    Settings ‣ 3.2 Experimental Comparison: IDP recovers performance better or comparable
    than LoRA ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An
    Experimental Study with Practical Implications") and Table [2](#S3.T2 "Table 2
    ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP recovers
    performance better or comparable than LoRA ‣ 3 Methods and Experiments ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications"). From
    these tables, we draw the following insights:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance Recovery: Overall, our testing shows that most techniques, IDP
    included, modestly improve performance in both quantization and pruning scenarios.
    The sole outlier is “ptune” or prefix-tuning, which, in contrast, reduced the
    performance of our models across all tasks. We also noted that quantization provides
    a better chance for performance recovery compared to pruning. Comparing baseline
    performances with the top performers, GPTQ shows an average improvement of about
    1%, compared to SparseGPT’s modest increase of 0.37%. This difference likely stems
    from pruning’s parameter removal, which has stricter limits on performance enhancement.
    However, IDP stands out in both scenarios, achieving the higher-than-average performance
    improvement.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowledge Domain Adaptation: By dividing our tasks into world knowledge, common
    reasoning, and language understanding categories, we draw insights on “knowledge
    displaced” and “knowledge forgotten.” We observed that redirection methods such
    as IDP are more effective for world knowledge tasks than integrated methods like
    LoRA. This suggests that in cases of factual knowledge, simple input redirection
    can effectively bring back the lost information in compressed models. Conversely,
    tasks that demand nuanced understanding, like those involving language comprehension,
    benefit more from the added parameters and external knowledge sources provided
    by techniques like LoRA. Nevertheless, in scenarios where IDP lacks, the difference
    in performance is marginal – less than 0.2%.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.3 IDP is remarkably more efficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We examined the link between the method’s parameter size and performance, as
    detailed in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 IDP is remarkably more efficient
    ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications"). Our findings show that IDP is much more efficient
    than LoRA for compression recovery. For example, when fine-tuning the Llama-7b
    model with QPTQ settings, LoRA’s parameters range between 4.4 to 8.9 million,
    while IDP uses only around 0.8 million, leading to substantial space savings of
    81% to 91% — a notable 20-fold reduction. Additionally, prompting tends to have
    a faster inference speed. Basic inference testing shows prompting incurs at most
    0.37s versus LoRA’s 0.62s for an input batch of 16 and a sequence length of 1024
    – this is a substantial 60% improvement in speed. Despite the smaller size, IDP
    generally sees a modest average improvement of 1% across the nine tasks evaluated.
    For further details on the performance and parameter size, refer to Table [1](#S3.T1
    "Table 1 ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP
    recovers performance better or comparable than LoRA ‣ 3 Methods and Experiments
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications")
    and Figure [4](#S3.F4 "Figure 4 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods
    and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with
    Practical Implications"), and our appendix provides a detailed explanation of
    how the total number of parameters was calculated for both LoRA and IDP.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we underscore the robustness of IDP’s performance, irrespective of
    prompt in Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness
    and Displacement ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications"). This figure reveals a variance of less than
    1% in average accuracy performance, yet with a 5-fold reduction in token size.
    Notably, even with a modest average of 20 tokens, IDP adeptly facilitates performance
    recovery, surpassing the compressed baseline. This evidence positions IDP as not
    only efficient in parameter utilization but also as a resilient mechanism for
    enhancing performance in the wake of model compression.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8bdf68f839b183767158b2217bcc81ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: GPTQ LLama-7b/OPT-6.7b average accuracy across nine tasks vs. number
    of trainable parameters. IDP shows remarkable efficiency and performance comparing
    to methods parameter-intensive method like LoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03df313d4482ce9e044d1478effc55ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Cosine similarity compares the self-attention and token activation
    at each layer to an uncompressed baseline using different fine-tuning techniques.
    A higher cosine score means it’s closer to the baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 More Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Evaluating Knowledge Forgetfulness and Displacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We employed a detailed visualization of the layer-wise attention and activation
    matrices to validate our hypothesis. Opting for cosine similarity over magnitude
    differences as our analytical tool, we aim to understand the distribution differences
    rather than magnitude. Our findings are presented in Figures [5](#S3.F5 "Figure
    5 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods and Experiments ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications"), and
    [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness and Displacement
    ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study
    with Practical Implications"), leading to several key observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24b7bddf4e029577db9bafe3ba880151.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: This figure illustrates the average performance over nine tasks using
    IDP. Results show IDP maintains relatively stable performance working with various
    average prompt sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.SS1.p2.1.m1.1.1.pic1" class="ltx_picture ltx_markedasmath" height="12.61"
    overflow="visible" version="1.1" width="12.61"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,12.61) matrix(1 0 0 -1 0 0) translate(6.31,0)
    translate(0,6.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="6.92">1</foreignobject></g></g></svg> When compared to
    LoRA, the attention mechanism of both prompting/IDP markedly diverges from the
    baseline, hinting at a potential contextual redirection. Conversely, the activation
    patterns echo similarities with LoRA. Given that LoRA incorporates a residual
    network at every layer to maintain congruity and prompting only at the self-attention,
    this semblance is unexpected.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S4.SS1.p3.1.m1.1.1.pic1" class="ltx_picture ltx_markedasmath" height="12.61"
    overflow="visible" version="1.1" width="12.61"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,12.61) matrix(1 0 0 -1 0 0) translate(6.31,0)
    translate(0,6.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="6.92">2</foreignobject></g></g></svg> These observations
    imply that prompting/IDP can tap into latent knowledge within the model. This
    is further supported by the data in Table [1](#S3.T1 "Table 1 ‣ Baseline Methods
    ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP recovers performance better
    or comparable than LoRA ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget
    Knowledge? An Experimental Study with Practical Implications") and Table [2](#S3.T2
    "Table 2 ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP
    recovers performance better or comparable than LoRA ‣ 3 Methods and Experiments
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications"),
    which show a propensity of prompting/IDP for tasks involving world knowledge.
    These tasks rely on the model’s internal knowledge base, reinforcing our conclusion
    about the efficacy of prompting/IDP in accessing embedded information.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.SS1.p4.1.m1.1.1.pic1" class="ltx_picture ltx_markedasmath" height="12.61"
    overflow="visible" version="1.1" width="12.61"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,12.61) matrix(1 0 0 -1 0 0) translate(6.31,0)
    translate(0,6.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="6.92">3</foreignobject></g></g></svg> Additionally, IDP
    demonstrates remarkable consistency in information retrieval. As evidenced in
    Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness and Displacement
    ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study
    with Practical Implications"), it maintains stable performance across a range
    of prompt sizes. This suggests that even with fewer tokens, knowledge rerouting
    via IDP remains effective, opening avenues for future optimizations and refinements
    in its application.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.SS1.p5.1.m1.1.1.pic1" class="ltx_picture ltx_markedasmath" height="12.61"
    overflow="visible" version="1.1" width="12.61"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,12.61) matrix(1 0 0 -1 0 0) translate(6.31,0)
    translate(0,6.31)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1
    0 0 -1 0 16.6)" width="6.92">4</foreignobject></g></g></svg> Finally, our analysis
    of prefix-tuning indicates its tendency to align with the original attention patterns
    of the model. However, as shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 IDP is remarkably
    more efficient ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications"), its activation patterns significantly
    deviate, hinting at a potential shortfall in redirecting knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: These insights strongly endorse the notion of “redirection” as the more effective
    mechanism for recovering performance in compressed models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 More ablation studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: This table includes results for our Inference-time Dynamic Prompting
    strategy. To illustrate its effectiveness, we also include the results of the
    individual prompts used along with naive soft-prompts concatenation. 26 and 100
    refers to the number of tokens in our prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: Model arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada winogrande
    Language Average OPT-6.7b/26 64.94 28.84 89.90 10.88 18.80 42.67 75.63 75.63 49.13
    65.96 63.77 59.62 51.98 OPT-6.7b/100 64.02 27.90 89.50 11.32 18.37 42.22 76.39
    76.39 48.81 65.42 63.22 59.15 51.66 OPT-6.7b/Concat 63.80 28.50 89.40 12.30 19.55
    42.71 75.79 75.79 48.92 64.72 63.85 59.16 51.87 OPT-6.7b/IDP 64.18 28.67 90.40
    11.96 19.05 42.85 76.17 76.17 49.03 66.82 63.22 59.69 52.17 Llama-7b/26 71.97
    38.40 92.90 20.47 33.20 51.39 75.84 75.84 53.75 69.45 67.17 63.46 58.13 Llama-7b/100
    71.51 38.31 92.10 21.11 34.56 51.52 75.84 75.84 53.92 69.69 68.75 64.12 58.42
    Llama-7b/Concat 71.17 37.80 92.30 16.88 33.84 50.40 74.92 74.92 53.34 67.18 66.46
    62.33 57.10 Llama-7b/IDP 71.63 38.65 92.60 21.60 33.84 51.66 76.01 76.01 53.97
    69.67 68.98 64.21 58.55
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39135e30069e3eda4a684de9e202b1d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: This graph shows the percentage performance improvement using two
    prompts at various lengths compared to a 3-bit quantized baseline for the OPT
    and LLama models. We’ve also showcased results from our IDP method, which selects
    prompts dynamically using the same two prompts. Small and Large correspond to
    26 and 100 tokens respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: We used IDP strategy with two distinct prompts of differing lengths, both trained
    using the same dataset to streamline our experimental parameters. We subsequently
    evaluated against our task benchmark, with the comprehensive findings cataloged
    in Table [3](#S4.T3 "Table 3 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do
    Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications").
    In a complementary visual aid, Figure [7](#S4.F7 "Figure 7 ‣ 4.2 More ablation
    studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications") highlights the percentage differences in performance
    against the baseline quantized models, providing an at-a-glance understanding
    of the performance gains across individual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis showed that IDP subtly enhances average accuracy. This is evident
    in our results with OPT and Llama models, where IDP showed a modest improvement
    of $0.5\%$. While these findings, detailed in Table [3](#S4.T3 "Table 3 ‣ 4.2
    More ablation studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications"), might not be groundbreaking,
    they highlight the potential of zero-shot input-to-prompt matching for compression
    recovery for various knowledge domains.
  prefs: []
  type: TYPE_NORMAL
- en: Further, in our examination of quantized foundation models, as shown in Figure
    [7](#S4.F7 "Figure 7 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications"), we
    noted areas where IDP demonstrated a slight but consistent superiority. Specifically,
    OPT models showed this incremental benefit in tasks such as Sciq, Triviqa, and
    Webqs, all falling within the world knowledge domain. Similarly, the Llama models
    exhibited slight improvements in tasks like Webqs, Arc, and Winogrand, with gains
    ranging between 1%-1.5%.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this study, we focused on understanding the impact of compression on LLMs
    and explore ways to mitigate its negative effects. We explore two key hypotheses:
    knowledge forgotten and knowledge displaced by analyzing the effectiveness of
    parameter-efficient tuning method like LoRA and prompting. A highlight from our
    study is the introduction Inference-time Dynamic Prompting , a light-weight approach
    to enhance traditional prompting. Empirically, we find IDP-based prompting perform
    on-par or better than relearning approach like LoRA while being significantly
    smaller in size with faster inference speed. Additionally, our visualization of
    the intermediate embeddings within LLMs suggests redirection through instruction
    is a more beneficial way to regain similar activation output. Collectively, our
    findings advocate the hypothesis of “knowledge displaced” as the critical factor
    behind performance decline post-compression, providing valuable insights into
    the mechanisms at play and paving the way for more efficient recovery strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Impact Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Berant et al. (2013) Berant, J., Chou, A. K., Frostig, R., and Liang, P. Semantic
    parsing on freebase from question-answer pairs. In *Conference on Empirical Methods
    in Natural Language Processing*, 2013. URL [https://api.semanticscholar.org/CorpusID:6401679](https://api.semanticscholar.org/CorpusID:6401679).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2019) Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.
    Piqa: Reasoning about physical commonsense in natural language, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Chen, L., Zaharia, M. A., and Zou, J. Y. Frugalgpt: How
    to use large language models while reducing cost and improving performance. *ArXiv*,
    abs/2305.05176, 2023. URL [https://api.semanticscholar.org/CorpusID:258564349](https://api.semanticscholar.org/CorpusID:258564349).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *ArXiv*, abs/1803.05457, 2018. URL [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. *ArXiv*, abs/2301.00774, 2023. URL
    [https://api.semanticscholar.org/CorpusID:255372747](https://api.semanticscholar.org/CorpusID:255372747).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *ArXiv*, abs/2210.17323, 2022. URL [https://api.semanticscholar.org/CorpusID:253237200](https://api.semanticscholar.org/CorpusID:253237200).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., and Chen, W. Lora: Low-rank adaptation of large language models. *ArXiv*,
    abs/2106.09685, 2021. URL [https://api.semanticscholar.org/CorpusID:235458009](https://api.semanticscholar.org/CorpusID:235458009).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. (2021a) Hubara, I., Chmiel, B., Island, M., Banner, R., Naor,
    S., and Soudry, D. Accelerated sparse neural training: A provable and efficient
    method to find n: M transposable masks. *ArXiv*, abs/2102.08124, 2021a. URL [https://api.semanticscholar.org/CorpusID:231934142](https://api.semanticscholar.org/CorpusID:231934142).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2021b) Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry,
    D. Accurate post training quantization with small calibration sets. In *International
    Conference on Machine Learning*, 2021b. URL [https://api.semanticscholar.org/CorpusID:235825979](https://api.semanticscholar.org/CorpusID:235825979).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal et al. (2023) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and
    Yang, Y. Compressing llms: The truth is rarely pure and never simple, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2017) Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa:
    A large scale distantly supervised challenge dataset for reading comprehension,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of
    scale for parameter-efficient prompt tuning. In *Conference on Empirical Methods
    in Natural Language Processing*, 2021. URL [https://api.semanticscholar.org/CorpusID:233296808](https://api.semanticscholar.org/CorpusID:233296808).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
    prompts for generation. *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, abs/2101.00190, 2021. URL [https://api.semanticscholar.org/CorpusID:230433941](https://api.semanticscholar.org/CorpusID:230433941).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paperno et al. (2016) Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,
    Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. The lambada
    dataset, August 2016. URL [https://doi.org/10.5281/zenodo.2630551](https://doi.org/10.5281/zenodo.2630551).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PENG et al. (2023) PENG, X., Xing, C., Choubey, P. K., Wu, C.-S., and Xiong,
    C. Model ensemble instead of prompt fusion: a sample-specific knowledge transfer
    method for few-shot prompt tuning. In *The Eleventh International Conference on
    Learning Representations*, 2023. URL [https://openreview.net/forum?id=p0yrSRbN5Bu](https://openreview.net/forum?id=p0yrSRbN5Bu).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *Journal of Machine Learning
    Research*, 21(140):1–67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2019) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models. *ArXiv*, abs/2302.13971, 2023. URL [https://api.semanticscholar.org/CorpusID:257219404](https://api.semanticscholar.org/CorpusID:257219404).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welbl et al. (2017) Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple
    choice science questions. *ArXiv*, abs/1707.06209, 2017. URL [https://api.semanticscholar.org/CorpusID:1553193](https://api.semanticscholar.org/CorpusID:1553193).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2022) Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. *ArXiv*, abs/2211.10438, 2022. URL [https://api.semanticscholar.org/CorpusID:253708271](https://api.semanticscholar.org/CorpusID:253708271).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Xu, Z., Liu, Z., Chen, B., Tang, Y., Wang, J., Zhou, K., Hu,
    X., and Shrivastava, A. Compress, then prompt: Improving accuracy-efficiency trade-off
    of llm inference with transferable prompt. *ArXiv*, abs/2305.11186, 2023. URL
    [https://api.semanticscholar.org/CorpusID:258823240](https://api.semanticscholar.org/CorpusID:258823240).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and
    He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale
    transformers, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
    L. Opt: Open pre-trained transformer language models. *ArXiv*, abs/2205.01068,
    2022. URL [https://api.semanticscholar.org/CorpusID:248496292](https://api.semanticscholar.org/CorpusID:248496292).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Number of Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LoRA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the LoRA method (Hu et al., [2021](#bib.bib8)), the total number of parameters
    is calculated as the sum of all low-rank projected layers within the LLMs. Consider
    the output of a singular LoRA layer as $x_{lora}=BAx$ as 11008.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting / IDP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In IDP, the total number of parameters is calculated by multiplying the total
    number of tokens, denoted as $t$ are 26, 50, and 100, corresponding to different
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix-tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For Prefix-tuning (Ptune), the total number of parameters can be determined
    using the formula $L*t*d$ are 26, 50, and 100.
  prefs: []
  type: TYPE_NORMAL
