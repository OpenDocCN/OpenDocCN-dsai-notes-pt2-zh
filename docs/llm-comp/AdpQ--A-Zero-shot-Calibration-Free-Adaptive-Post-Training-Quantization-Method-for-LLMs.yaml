- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13358](https://ar5iv.labs.arxiv.org/html/2405.13358)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alireza Ghaffari^(1,2)  Sharareh Younesian¹  Vahid Partovi Nia¹
  prefs: []
  type: TYPE_NORMAL
- en: Boxing Chen¹  Masoud Asgharian²
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Huawei Noah’s Ark Lab, Montreal Research Center
  prefs: []
  type: TYPE_NORMAL
- en: ² Department of Mathematics and Statistics, McGill University
  prefs: []
  type: TYPE_NORMAL
- en: alireza.ghaffari@mcgill.ca
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ever-growing computational complexity of Large Language Models (LLMs) necessitates
    efficient deployment strategies. The current state-of-the-art approaches for Post-training
    Quantization (PTQ) often require calibration to achieve the desired accuracy.
    This paper presents AdpQ, a novel zero-shot adaptive PTQ method for LLMs that
    achieves the state-of-the-art performance in low-precision quantization (e.g.
    3-bit) without requiring any calibration data. Inspired by Adaptive LASSO regression
    model, our proposed approach tackles the challenge of outlier activations by separating
    salient weights using an adaptive soft-thresholding method. Guided by Adaptive
    LASSO, this method ensures that the quantized weights distribution closely follows
    the originally trained weights and eliminates the need for calibration data entirely,
    setting our method apart from popular approaches such as SpQR and AWQ. Furthermore,
    our method offers an additional benefit in terms of privacy preservation by eliminating
    any calibration or training data. We also delve deeper into the information-theoretic
    underpinnings of the proposed method. We demonstrate that it leverages the Adaptive
    LASSO to minimize the Kullback-Leibler divergence between the quantized weights
    and the originally trained weights. This minimization ensures the quantized model
    retains the Shannon information content of the original model to a great extent,
    guaranteeing efficient deployment without sacrificing accuracy or information.
    Our results achieve the same accuracy as the existing methods on various LLM benchmarks
    while the quantization time is reduced by at least 10x, solidifying our contribution
    to efficient and privacy-preserving LLM deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rise of Large Language Models (LLMs) has fundamentally reshaped our society
    by performing various tasks that demand sophisticated language processing capabilities.
    However, LLMs versatility comes at the cost of high power consumption and memory
    usage. Since training or fine-tuning LLMs are compute-intensive and costly, Post-training
    Quantization (PTQ) techniques have emerged as a promising solution to address
    these challenges without requiring additional training or fine-tuning. Yet, most
    existing PTQ algorithms rely on calibrating the weights of the LLMs using a calibration
    dataset, introducing additional computational overhead, time consumption, and
    potential data privacy concerns. Additionally, many PTQ methods are not truly
    zero-shot, meaning that PTQ calibration strategies can be seen as a fine-tuning
    step, even though the calibration does not use the same loss function as the fine-tuning
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: We propose AdpQ, a novel, zero-shot, calibration free, and adaptive PTQ method
    that is designed for LLMs. In our proposed approach, we leverage the power of
    shrinkage methods [[1](#bib.bib1)], specifically the Adaptive LASSO [[2](#bib.bib2)],
    to effectively quantize LLM weights without performing any calibration. Traditionally,
    shrinkage methods are vastly used in statistical machine learning for model selection
    and variable selection. Here, we use Adaptive LASSO to identify and isolate the
    weights that are more salient. Furthermore, our method achieves complete weight
    quantization by applying separate scales to both outlier and non-outlier weights
    during the quantization process, eliminating all floating-point weights in the
    quantized model. As will be discussed in detail throughout this paper, the main
    advantage of Adaptive LASSO is that it penalizes and isolates weights proportional
    to their originally trained values leading to an effective, calibration free outlier
    detection. While most popular PTQ methods use a calibration dataset to identify
    and tune the weights by considering activations [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)], we argue that the information of activations
    is somehow encoded in the weights of the model and our proposed approach helps
    to preserve that information considerably.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical foundation of our method is rooted in information theory. We
    study the Adaptive LASSO through the lens of Shannon entropy and demonstrate that
    Adaptive LASSO is a way to minimize the Kullback-Leibler (KL) divergence between
    the quantized weights and the originally trained weights leading to a more effective
    quantization scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Our proposed PTQ method is faster than the state-of-the-art in terms of the
    time required to perform the quantization. We show that the Adaptive LASSO leads
    to a computationally efficient soft-thresholding approach, significantly reducing
    the run-time of the quantization algorithm. Furthermore, our method leverages
    mixed-precision quantization, allowing for separate quantization bit-width of
    outlier weights and non-outlier weights that can potentially reduce the quantization
    error mitigating the potential accuracy drops typically seen in low-precision
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we make the following contributions
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We proposed AdpQ, a zero-shot, calibration free, and adaptive PTQ method for
    LLMs that is inspired by Adaptive LASSO regression. The proposed method only relies
    on the weights of the model and uses no external data for calibration while achieving
    the state-of-the-art accuracy performance with 10$\times$ faster quantization
    speed. To the best of our knowledge, this is the first time a zero-shot, calibration
    free PTQ is proposed that is based on Adaptive LASSO shrinkage method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proposed method is computationally efficient and reduces the cost and run-time
    of the PTQ algorithm. More specifically, our proposed method can be simplified
    to an adaptive soft-thresholding algorithm which is easy to implement on any general-purpose
    hardware.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our proposed method applies separate scales to outlier and non-outlier weights
    during the quantization, eliminating all floating-point weight representations
    in the model. Note that other state-of-the-art methods keep outliers weights in
    floating-point format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a theoretical foundation for our proposed PTQ method using information
    theory. We show that our adaptive quantization method tries to minimize a quadratic
    error loss using quantized weights while reasonably bounding the KL-divergence
    between the quantized weights and the originally trained weights, hence preserving
    the Shannon information contents of the original weights. To the best of our knowledge,
    this is the first time that a quantization method, has been studied from an information-theoretic
    perspective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the paper is organized as follows. Section [2](#S2 "2 Related Works
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") reviews recent works in the field of PTQ and specifies the differences
    to our proposed adaptive quantization method. Section [3](#S3 "3 Methodology ‣
    AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") discusses the AdpQ algorithm in detail. Section [4](#S4 "4 Theoretical
    Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs") delves deeper into the theoretical analysis of the algorithm
    and shows how AdpQ can control information loss in quantization. Finally, experimental
    results supporting our proposed methodology and theoretical findings are presented
    in Section [5](#S5 "5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free
    Adaptive Post Training Quantization Method for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of low-precision deep learning, three existing notable categories
    are (i) low-precision or quantized training, (ii) quantization-aware training
    (QAT), and (iii) post-training quantization (PTQ). While our proposed method can
    be applied to both low-precision training (e.g. [[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]) and QAT (e.g. [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)] ), the primary focus of this section is to
    review the research around PTQ of LLMs which is found to be more challenging in
    the literature.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, PTQ methods were common for computer vision models with small
    number of parameters, some notable methods are AdaRound [[15](#bib.bib15)], OBQ
    [[16](#bib.bib16)], AdaQuant [[17](#bib.bib17)], and BRECQ [[18](#bib.bib18)].
    However, these methods were found to be either compute-intensive or inaccurate
    for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: LLM.int8() [[19](#bib.bib19)] and ZeroQuant [[20](#bib.bib20)] are among the
    first PTQ techniques that were designed for LLMs. LLM.int8() separates the outlier
    activations and keeps them in floating-point number format while quantizing weights
    and non-outlier activations to 8-bit integers. LLM.int8() separates the outlier
    activations based on their magnitude. On the other hand, ZeroQuant uses a fine-grained
    hardware-friendly quantization scheme as well as layer-by-layer knowledge distillation
    for quantizing both weight and activations. However, both LLM.int8() and ZeroQuant
    are not efficient for quantizing LLMs to extreme low-percision number formats
    such as 3-bit integers. OPTQ [[3](#bib.bib3)] is a PTQ algorithm for LLMs that
    can quantize weights to 3- or 4-bit integers. OPTQ adapted a calibration algorithm
    inspired by [[21](#bib.bib21)] that minimizes the $\ell_{2}$ loss of the quantized
    layer output with the original output. SpQR [[4](#bib.bib4)] uses OPTQ algorithm
    while separating the salient weights and keeping them in FP16 format and further
    uses double quantization to reduce the memory. Both SpQR and OPTQ algorithms require
    calibration data for quantization. SmoothQuant [[22](#bib.bib22)] performs 8-bit
    integer quantization of weights and activation by offline migration of the quantization
    difficulty from activations to weights. Likewise, AWQ [[5](#bib.bib5)], quantized
    weights by applying per-channel scales that protect the salient weights by observing
    the activation. SmoothQuant and AWQ algorithms also require calibration data to
    perform quantization.
  prefs: []
  type: TYPE_NORMAL
- en: The key feature of our proposed algorithm, AdpQ, lies in its ability to perform
    PTQ without requiring any calibration data as opposed to OPTQ, SpQR, AWQ, and
    SmoothQuant. Furthermore, AdpQ uniquely identifies and isolates outlier weights
    solely through analysis of the model weight tensors. Our proposed algorithm quantizes
    both outlier and non-outlier weights to 3- or 4-bit integer numbers, achieving
    a completely quantized model without any remaining floating-point weight values.
    Our experiments demonstrate that AdpQ delivers at least 10$\times$ faster quantization
    run-time compared to the state-of-the-art as mentioned above algorithms. Finally,
    AdpQ is designed with information theory in mind. By focusing on retaining the
    information encoded within the model weights, it aims to minimize information
    loss during the quantization process.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having established the limitations of existing PTQ methods and the advantages
    of our proposed zero-shot approach, this section provides a comprehensive understanding
    of the inner workings of AdpQ while its information-theoretic foundations are
    left to be discussed in Section [4](#S4 "4 Theoretical Justification ‣ AdpQ: A
    Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Existence of the Best Proxy for Weights and Adaptive LASSO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Post-Training Quantization (PTQ), the most critical information lies within
    the model’s weights. Here, we draw inspiration from the concept of shrinkage in
    statistical machine learning. Shrinkage techniques prioritize retaining the most
    important variables in the model while shrinking less influential ones towards
    zero. Similarly, outlier weight identification in PTQ can be viewed as a form
    of shrinkage. While traditional LASSO is commonly used for variable selection
    in statistics, it can lead to inconsistencies in the context of PTQ. To address
    this, we leverage the concept of Adaptive LASSO. This method incorporates adaptive
    weights to penalize different coefficients in the $\ell_{1}$ penalty. This allows
    us to effectively identify and isolate outlier weights within the model. Consider
    the following optimization problem,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\arg\min_{\hat{\textbf{W}}}\&#124;\textbf{W}\textbf{X}-\hat{\textbf{W}}\textbf{X}\&#124;_{2}^{2}+\lambda\sum_{i}\left&#124;\frac{\hat{w}_{i}}{w_{i}}\right&#124;,$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where X denotes the input tensor of a layer, W is the original weights of the
    layer, $\hat{\textbf{W}}$ penalty term ensuring that the weights are shrunk relative
    to their original values, as opposed to LASSO which penalizes larger weights without
    considering their original values.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Adaptive LASSO as a Soft-Thresholding Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since equation ([1](#S3.E1 "In 3.1 Existence of the Best Proxy for Weights
    and Adaptive LASSO ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs")) does not have a closed-form solution,
    one may use an iterative numerical solution to solve it for a given $\lambda$.
    However, here we show that under a mild regulatory condition, Adaptive LASSO is
    a soft-thresholding algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us assume X is an orthogonal matrix i.e. $\textbf{X}\textbf{X}^{\top}=b\textbf{I}$
    is a constant and I is the identity matrix. By expanding ([1](#S3.E1 "In 3.1 Existence
    of the Best Proxy for Weights and Adaptive LASSO ‣ 3 Methodology ‣ AdpQ: A Zero-shot
    Calibration Free Adaptive Post Training Quantization Method for LLMs")) we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: and since W, weights of the original model are constant, the Adaptive LASSO
    loss becomes
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: By taking the derivative with respect to $\hat{w}_{i}$, and setting it equal
    to zero, it is easy to see
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{w}_{i}=\text{sign}({w}_{i})\text{{ReLU}}(&#124;{w}_{i}&#124;-\frac{\lambda^{\prime}}{&#124;{w}_{i}&#124;}),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda^{\prime}={\lambda}/{2b}$. Equation ([4](#S3.E4 "In 3.2 Adaptive
    LASSO as a Soft-Thresholding Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration
    Free Adaptive Post Training Quantization Method for LLMs")) shows that Adaptive
    LASSO is a simple soft-thresholding method that is very efficient to be implemented
    in the currently available commodity hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark 1: LLMs typically operate on high dimensional data, particularly in
    the case of large input sequences. Besides the well-known curse of dimensionality,
    high dimensional data possess some intriguing properties as well, which is commonly
    known as blessing of dimensionality [[23](#bib.bib23)]. One notable and interesting
    blessing of high dimensional data, known as the concentration phenomenon, is that
    random vectors tend to be orthogonal as shown in [[24](#bib.bib24), equation (2)],
    and in [[25](#bib.bib25), equation (3)]. Therefore, we argue that the orthogonality
    assumption, $\textbf{X}\textbf{X}^{\top}=b\textbf{I}$, is not a restrictive assumption
    in LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4056332fec923a716e2ff8762b886d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: AdpQ outlier separation for weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Layer weight tensor W, Outlier percentage $\alpha$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 AdpQ Quantization algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 AdpQ Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To provide a comprehensive overview of AdpQ algorithm, Figure [1](#S3.F1.1
    "Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding Method ‣ 3 Methodology ‣
    AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") and Algorithm [1](#alg1 "In Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding
    Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") offer a visual and step-by-step breakdown of the
    procedure. Figure [1](#S3.F1.1 "Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding
    Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") illustrates the core principle of AdpQ. It highlights
    how the Adaptive LASSO is used to identify and isolate outlier weights within
    the model. Moreover, the figure shows both outlier and non-outlier weights are
    quantized to low-precision formats. Algorithm [1](#alg1 "In Figure 1 ‣ 3.2 Adaptive
    LASSO as a Soft-Thresholding Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration
    Free Adaptive Post Training Quantization Method for LLMs") outlines the implementation
    process of AdpQ quantization in a clear two-step procedure. Starting with a large
    $\lambda^{\prime}$, of weights are identified as outliers, the process transitions
    to quantizing both the outlier and non-outlier weights using a minmax quantization
    approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Theoretical Justification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having established the methodology of AdpQ algorithm, this section delves into
    the information-theoretic foundation that solidifies AdpQ effectiveness. Since
    the distribution of the quantized weights must closely follow the distribution
    of the original weight to retain the information content of the model, we formulate
    the objective function of quantization as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $f_{\hat{\textbf{W}}}$ denotes the KL-divergence of the distributions.
    Our objective is to show that (i) Adaptive LASSO, equation ([1](#S3.E1 "In 3.1
    Existence of the Best Proxy for Weights and Adaptive LASSO ‣ 3 Methodology ‣ AdpQ:
    A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs")),
    is a proxy solution to the minimization problem ([5](#S4.E5 "In 4 Theoretical
    Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs")) as shown in Proposition 1, and (ii) separating the outlier
    weights improves the quantization accuracy as shown in Proposition 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark 2: Although inputs tensor X appears in minimization problem ([5](#S4.E5
    "In 4 Theoretical Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs")), the final algorithm is simplified
    as a soft-thresholding method on weights tensor. Thus, our proposed PTQ algorithm
    does not use any calibration data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proposition 1: Suppose $f_{\textbf{W}}$ are small. Then'
  prefs: []
  type: TYPE_NORMAL
- en: 'Claim 1: $1$2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Claim 2: $\left|\mu_{\delta}\sum_{i}f^{\prime\prime}_{{\textbf{W}}}(w_{i})(\hat{w}_{i}-w_{i})\right|\leq
    C\left(\sum_{i}\left|\frac{\hat{w}_{i}}{w_{i}}\right|+1\right)$ is a constant.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Proof: Assuming a quantization error $\delta_{i}$ are independent of the weights
    values. Therefore, quantized weight distribution is a convolution of original
    weights distribution and quantization error distribution such that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f_{\hat{\textbf{W}}}(\hat{w})$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Using the mean value theorem for $f_{{\textbf{W}}}(\hat{w}-x)-f_{{\textbf{W}}}(\hat{w})$,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f_{\hat{\textbf{W}}}(\hat{w})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\stackrel{{\scriptstyle\sigma^{2}_{\delta}\text{ is small}}}{{\approx}}f_{{\textbf{W}}}(\hat{w})-\int_{-\infty}^{\infty}xf^{\prime}_{{\textbf{W}}}(\hat{w})f_{\delta}(x)dx,$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: and thus,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\mu_{\delta}$. Then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'By plugging the equation ([9](#S4.E9 "In 4 Theoretical Justification ‣ AdpQ:
    A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"))
    in KL divergence, we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Then, it follows from Taylor’s expansion around the original weight $w_{i}$
    that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: which proves Claim 1.
  prefs: []
  type: TYPE_NORMAL
- en: To prove Claim 2, since $w_{i}$ are constants, using triangular inequality
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: in which $C=|\mu_{\delta}|AB$. This completes the proof.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark 3: Since in PTQ, original weights, and their distribution are known,
    the first term in equation ([11](#S4.E11 "In 4 Theoretical Justification ‣ AdpQ:
    A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"))
    is constant. Therefore, minimizing $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    in minimization problem ([5](#S4.E5 "In 4 Theoretical Justification ‣ AdpQ: A
    Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"))
    which shows Adaptive LASSO is a proxy solution to minimization problem ([5](#S4.E5
    "In 4 Theoretical Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we show that separating outlier weights guided by minimizing
    $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$. In the context
    of quantization, JSD represents the information gain due to separating outlier
    weights from non-outlier weights.
  prefs: []
  type: TYPE_NORMAL
- en: Let us assume $f_{{\textbf{W}}}$ [[26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Proposition 2: Minimizing $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    is the same as minimizing'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where by minimizing the Jensen–Shannon centroid for quantized weights, we minimize
    the information loss for the quantized model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\pi\{H(f_{\hat{\textbf{W}}})+\mathbb{D}_{\text{KL}}(f^{\text{C}}_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}})\}+(1-\pi)\{H(f^{\text{O}}_{{\textbf{W}}})+\mathbb{D}_{\text{KL}}(f^{\text{O}}_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}}\}-H(f_{{\textbf{W}}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'where equation ([13](#S4.E13 "In 4 Theoretical Justification ‣ AdpQ: A Zero-shot
    Calibration Free Adaptive Post Training Quantization Method for LLMs")) completes
    the proof.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark 4: Note that the first term in equation ([13](#S4.E13 "In 4 Theoretical
    Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs")), i.e. $-\textit{JSD}(f^{\text{C}}_{{\textbf{W}}},f^{\text{O}}_{{\textbf{W}}})$
    can be interpreted as the information loss should we decide not to separate the
    outlier weights of the trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark 5: It follows from Proposition 2 that minimizing $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    leading to minimization of information loss in the quantized model. Thus, our
    proposed method AdpQ, as a proxy solution to optimization problem [5](#S4.E5 "In
    4 Theoretical Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post
    Training Quantization Method for LLMs"), helps retaining the information in the
    quantized model to a great extent.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides experimental results supporting our proposed methodology
    for post-training quantization of LLMs. Note that in our results, we use a row-wise
    group quantization technique in conjunction with AdpQ soft-thresholding method
    as explained in Algorithm [1](#alg1 "In Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding
    Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Average bits: The average bit presented in the results of AdpQ is calculated
    based on three factors, (i) the number of non-outlier weights and their bit-width,
    (ii) the number of outlier weights and their bitwidth and (iii) location index
    of the outlier weights. Since AdpQ identifies and isolates the outlier weights
    in an unstructured manner, tracking the location index of the outliers is essential
    for tensor reconstruction after dealing with quantized outlier weights and non-outlier
    weights separately. While maintaining an outlier mask is straightforward, it would
    add an extra bit per weight, which is inefficient in terms of memory consumption.
    To tackle this issue, we chose to retain the location index of outliers within
    each group when using group quantization. Retaining the index of outlier weights
    leads to a lower average bit since in AdpQ, the outlier weight ratio $\alpha$
    is the group size. Moreover, we store scales and zero-points in 16-bit floating-point
    format. In summary, the average number of bits per weight is computed as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $b_{\text{avg}}=\left(b_{\text{C}}+\dfrac{2\times 16}{g}\right)\times(1-\alpha)+\left(b_{\text{O}}+\log_{2}g+\dfrac{2\times
    16}{g}\right)\times\alpha$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $g$ are the bit-widths of outlier and non-outlier weights respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clipping Non-outlier Weights: We also used clipping to further reduce the $b_{\text{avg}}$
    due to index tracking of outliers. We observed that applying a clipping range
    of 90-95% to non-outliers yields similar accuracy compared to increasing the outlier
    ratio. This confirms that AdpQ can also be combined with other known quantization
    techniques to achieve better results.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Quantization Time: Benefiting from our simple soft-thresholding technique,
    AdpQ eliminates the need for calibration data and weight update routines. This
    significantly reduces the quantization time compared to existing methods. AdpQ
    achieves at least 10$\times$ as shown in Table [1](#S5.T1 "Table 1 ‣ 5.1 Experimental
    Results ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Quantization time comparison'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Avg Bits | Quantization Time (s) $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | AWQ (g128) | 4.25 | 838 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | SpQR | 4.63 | 10901 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha$=8%) | 4.81 | 57 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AWQ (g128) | 4.25 | 1608 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13B | SpQR | 4.63 | 20502 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha$=6%) | 4.67 | 116 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AWQ (g128) | 4.25 | 3740 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-30B | SpQR | 4.63 | 24069 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha$=5%) | 4.60 | 470 |'
  prefs: []
  type: TYPE_TB
- en: 'Coding Ability: AdpQ is a more robust PTQ approach since it only works with
    weights and does not depend on calibration data. To showcase the robustness of
    AdpQ, we evaluated the coding performance of quantized Code-Llama model [[27](#bib.bib27)]
    on HumanEval [[28](#bib.bib28)] and MBPP [[29](#bib.bib29)] datasets. HumanEval
    includes 164 human handwritten programming problems with a function signature,
    docstring, body, and several unit tests, and MBPP consists of around 1,000 crowd-sourced
    Python programming problems. Table [2](#S5.T2 "Table 2 ‣ 5.1 Experimental Results
    ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") shows AdpQ outperforms SpQR[[4](#bib.bib4)], demonstrating
    that if calibration data does not have the same nature as the task, using calibration
    data decreases the performance, while AdpQ, is robust to such issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Comparison of AdpQ results for Code-Llama models on HumanEval [[28](#bib.bib28)]
    and MBPP [[29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Avg Bits | Human Eval | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | pass@1 | pass@10 | pass@1 | pass@10 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16.00 | 29.63 | 59.84 | 25.87 | 63.52 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 30.13 | 57.97 | 28.26 | 62.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Code-Llama-7B | SpQR^∗ | 4.63 | 29.94 | 57.40 | 27.59 | 61.78 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha$=5%) | 4.60 | 30.34 | 58.60 | 28.03 | 62.55 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 16.00 | 34.79 | 66.50 | 30.17 | 67.51 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 33.70 | 65.88 | 29.63 | 66.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Code-Llama-13B | SpQR^∗ | 4.63 | 34.19 | 65.69 | 29.74 | 66.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha$=6%) | 4.67 | 34.79 | 66.02 | 31.36 | 66.82 |'
  prefs: []
  type: TYPE_TB
- en: '^∗ Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix
    A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") for quantization settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-Shot Task Evaluation: We also evaluated the accuracy of LLaMA 1 [[30](#bib.bib30)]
    and LLaMA 2 [[31](#bib.bib31)] models on 5 zero-shot common-sense reasoning tasks
    including ARC(easy and challenge) [[32](#bib.bib32)], HellaSwag [[33](#bib.bib33)],
    WinoGrande [[34](#bib.bib34)] and PIQA [[35](#bib.bib35)] using LM Evaluation
    Harness [[36](#bib.bib36)]. As shown in Table [3](#S5.T3 "Table 3 ‣ 5.1 Experimental
    Results ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs"), AdpQ outperforms SpQR [[4](#bib.bib4)]
    in both 4-bit and 3-bit quantization, showing that zero-shot quantization coupled
    with Adaptive LASSO outlier detection is enough for quantization and there is
    no need for complex calibration-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparison of AdpQ results on zero-shot tasks using LM Evaluation
    Harness [[36](#bib.bib36)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |  | Method | Avg Bit | ARC-c | ARC-e | HellaSwag | Winogrande | PIQA
    | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B |  | FP16 | 16 | 41.89 | 75.25 | 56.95 | 69.93 | 78.67 | 64.54 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 42.92 | 74.54 | 56.29 | 70.01 | 78.18 | 64.39 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 41.72 | 75.21 | 56.65 | 69.61 | 79.05
    | 64.45 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=8\%$) | 4.81 | 42.15 | 75.34 | 56.72 | 70.17 | 78.56
    | 64.59 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.96 | 41.55 | 74.28 | 56.31 | 68.90 | 77.80
    | 63.77 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=9\%$=4) | 3.97 | 42.32 | 74.66 | 55.94 | 69.85 | 78.40
    | 64.23 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13B |  | FP16 | 16 | 46.42 | 77.36 | 59.88 | 72.69 | 79.16 | 67.19
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 45.82 | 76.77 | 59.37 | 72.45 | 79.71 | 66.82 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 45.73 | 76.85 | 59.70 | 73.09 | 79.22
    | 66.92 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=6\%$) | 4.67 | 45.99 | 76.85 | 59.41 | 73.01 | 78.94
    | 66.84 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.97 | 44.62 | 77.06 | 59.13 | 72.06 | 79.72
    | 66.52 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 46.25 | 77.40 | 58.56 | 73.09 | 78.78
    | 66.82 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-30B |  | FP16 | 16 | 52.90 | 80.43 | 63.37 | 75.85 | 81.12 | 70.73
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 52.05 | 80.77 | 62.89 | 74.19 | 80.58 | 70.10 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 51.45 | 80.47 | 63.08 | 74.74 | 80.74
    | 70.10 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=5\%$) | 4.60 | 51.88 | 80.77 | 63.07 | 74.19 | 80.74
    | 70.13 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.89 | 50.77 | 80.26 | 62.79 | 74.59 | 80.47
    | 69.78 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=8\%$=4) | 3.89 | 50.94 | 80.13 | 62.49 | 75.22 | 80.96
    | 69.95 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B |  | FP16 | 16 | 43.43 | 76.35 | 57.16 | 69.14 | 78.07 | 64.83
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 43.09 | 76.18 | 56.90 | 68.67 | 77.48 | 64.46 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 44.28 | 76.14 | 56.95 | 68.51 | 77.42
    | 64.66 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=8\%$) | 4.81 | 43.17 | 76.39 | 57.12 | 69.77 | 77.97
    | 64.88 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.96 | 42.41 | 75.08 | 56.39 | 68.67 | 77.86
    | 64.08 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 42.75 | 75.38 | 56.71 | 69.53 | 77.31
    | 64.34 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-13B |  | FP16 | 16 | 48.46 | 79.42 | 60.05 | 72.38 | 79.11 | 67.88
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN (g128) | 4.25 | 48.12 | 78.83 | 59.74 | 72.69 | 78.67 | 67.61 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 48.46 | 79.76 | 59.97 | 71.90 | 78.84
    | 67.79 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=6\%$) | 4.67 | 48.38 | 79.63 | 59.89 | 72.53 | 78.94
    | 67.87 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.97 | 46.33 | 78.16 | 59.54 | 72.45 | 78.24
    | 66.94 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 46.08 | 78.28 | 59.20 | 71.90 | 78.51
    | 66.79 |'
  prefs: []
  type: TYPE_TB
- en: '^∗ Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix
    A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") for quantization settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perplexity: We evaluated perplexity of quantized LLaMA models on WikiText2
    [[37](#bib.bib37)] and C4 [[38](#bib.bib38)] datasets. Table [4](#S5.T4 "Table
    4 ‣ 5.1 Experimental Results ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration
    Free Adaptive Post Training Quantization Method for LLMs") shows the results comparing
    perplexity scores for FP16, Round to Nearest (RTN), AWQ[[5](#bib.bib5)], SpQR,[[4](#bib.bib4)]
    and AdpQ. AdpQ outperforms AWQ and RTN consistently in terms of perplexity scores.
    Furthermore, AdpQ exhibits perplexity scores that closely follow those of SpQR,
    particularly for larger models. These results highlight AdpQ ability to achieve
    competitive accuracy while offering a significant advantage in terms of quantization
    time efficiency and robustness. Refer to Appendix [A.5](#A1.SS5 "A.5 Extra Experiments
    Results ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free
    Adaptive Post Training Quantization Method for LLMs") for more perplexity results
    on Falcon [[39](#bib.bib39)] and OPT [[40](#bib.bib40)] models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of AdpQ perplexity results of 4-bit & 3-bit on WikiText2
    and C4.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | 4-bit | 3-bit |'
  prefs: []
  type: TYPE_TB
- en: '| Model | Method | Quantization setting | Avg Bits | Wiki2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | FP16 | - | 16.00 | 5.67 | 7.08 | - | 16.00 | 5.67 | 7.08 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 5.96 | 7.37 | 3bit-g128 | 3.25 | 7.01 | 8.62 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 5.78 | 7.21 | 3bit-g128 | 3.25 | 6.35 | 7.81 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 5.73 | 7.13 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.98 | 5.87 | 7.28 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=8\%$=4) | 3.97 | 6.07 | 7.51 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13B | FP16 | - | 16.00 | 5.09 | 6.61 | - | 16.00 | 5.09 | 6.61 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 5.25 | 6.75 | 3bit-g128 | 3.25 | 5.88 | 7.50 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 5.18 | 6.70 | 3bit-g128 | 3.25 | 5.52 | 7.07 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 5.13 | 6.64 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.97 | 5.22 | 6.72 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$=4) | 3.97 | 5.32 | 6.81 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-30B | FP16 | - | 16.00 | 4.10 | 5.98 | - | 16.00 | 4.10 | 5.98 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 4.23 | 6.10 | 3bit-g128 | 3.25 | 4.88 | 6.59 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 4.21 | 6.05 | 3bit-g128 | 3.25 | 4.61 | 6.35 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 4.14 | 6.01 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.90 | 4.25 | 6.08 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=5\%$=4) | 3.89 | 4.31 | 6.15 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-7B | FP16 | - | 16.00 | 5.47 | 6.97 | - | 16.00 | 5.47 | 6.97 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 5.72 | 7.24 | 3bit-g128 | 3.25 | 6.66 | 8.40 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 5.60 | 7.12 | 3bit-g128 | 3.25 | 6.24 | 7.81 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 5.52 | 7.03 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.98 | 5.66 | 7.18 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=8\%$=4) | 3.97 | 5.83 | 7.37 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-13B | FP16 | - | 16.00 | 4.88 | 6.47 | - | 16.00 | 4.88 | 6.47 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 4.98 | 6.59 | 3bit-g128 | 3.25 | 5.52 | 7.18 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 4.97 | 6.56 | 3bit-g128 | 3.25 | 5.32 | 6.95 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 4.92 | 6.51 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.96 | 5.01 | 6.60 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$=4) | 3.97 | 5.05 | 6.66 |'
  prefs: []
  type: TYPE_TB
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presented AdpQ, a novel zero-shot, calibration free PTQ approach
    that is designed specifically for LLMs. AdpQ addresses the limitations of existing
    PTQ methods by leveraging the Adaptive LASSO regression for outlier identification
    and enabling quantization of both outlier and non-outlier weights. AdpQ provides
    a robust and accurate quantization scheme while achieving complete model quantization
    with low-precision integer representations. Our theoretical analysis demonstrates
    that AdpQ is aligned with information theory principles and as such ensures the
    information of the model is recovered in the quantized model to a great extent.
    Furthermore, our experimental results show AdpQ ability to achieve competitive
    accuracy on standard language modeling benchmarks while surpassing existing PTQ
    methods in terms of quantization robustness and quantization time efficiency.
    Notably, AdpQ exhibits at almost a 10$\times$ speedup in quantization time compared
    to SpQR, highlighting its significant advantage in deployment scenarios. Thus,
    AdpQ presents a compelling PTQ solution for the efficient deployment of LLMs.
    Its zero-shot calibration free nature, computational efficiency, and competitive
    accuracy make it a valuable tool for facilitating the practical application of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Copas [1983] John B Copas. Regression, prediction and shrinkage. *Journal of
    the Royal Statistical Society Series B: Statistical Methodology*, 45(3):311–335,
    1983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou [2006] Hui Zou. The adaptive lasso and its oracle properties. *Journal of
    the American statistical association*, 101(476):1418–1429, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. [2023] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. OPTQ: accurate quantization for generative pre-trained transformers.
    In *The Eleventh International Conference on Learning Representations, ICLR 2023,
    Kigali, Rwanda, May 1-5, 2023*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2024a] Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. SpQR: A sparse-quantized representation for near-lossless LLM
    weight compression. In *The Twelfth International Conference on Learning Representations*,
    2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2024] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming
    Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit
    of post-training quantization for llms. *arXiv preprint arXiv:2402.04291*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banner et al. [2018] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
    Scalable methods for 8-bit training of neural networks. *Advances in neural information
    processing systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang,
    Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, et al. Fixed-point back-propagation
    training. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 2330–2338, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2020] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang,
    Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional
    neural network. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 1969–1979, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2021] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang,
    Zhenyu Gu, and Yinghui Xu. Distribution adaptive int8 quantization for training
    cnns. In *Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghaffari et al. [2022] Alireza Ghaffari, Marzieh S Tahaei, Mohammadreza Tayaranian,
    Masoud Asgharian, and Vahid Partovi Nia. Is integer arithmetic enough for deep
    learning training? *Advances in Neural Information Processing Systems*, 35:27402–27413,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2024b] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training
    quantization. In Hal Daumé III and Aarti Singh, editors, *Proceedings of the 37th
    International Conference on Machine Learning*, volume 119 of *Proceedings of Machine
    Learning Research*, pages 7197–7206\. PMLR, 13–18 Jul 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2022] Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. In S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, *Advances in
    Neural Information Processing Systems*, volume 35, pages 4475–4488\. Curran Associates,
    Inc., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. [2021] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In Marina Meila and Tong Zhang, editors, *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pages 4466–4475\. PMLR, 18–24 Jul 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: pushing the limit of post-training quantization
    by block reconstruction. In *9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. In S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, *Advances in
    Neural Information Processing Systems*, volume 35, pages 30318–30332\. Curran
    Associates, Inc., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information Processing
    Systems*, volume 35, pages 27168–27183\. Curran Associates, Inc., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassibi and Stork [1992] Babak Hassibi and David Stork. Second order derivatives
    for network pruning: Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles,
    editors, *Advances in Neural Information Processing Systems*, volume 5\. Morgan-Kaufmann,
    1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. SmoothQuant: Accurate and efficient post-training quantization for
    large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
    Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, *Proceedings of the
    40th International Conference on Machine Learning*, volume 202 of *Proceedings
    of Machine Learning Research*, pages 38087–38099\. PMLR, 23–29 Jul 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Donoho et al. [2000] David L Donoho et al. High-dimensional data analysis:
    The curses and blessings of dimensionality. *AMS math challenges lecture*, 1(2000):32,
    2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hall et al. [2005] Peter Hall, James Stephen Marron, and Amnon Neeman. Geometric
    representation of high dimension, low sample size data. *Journal of the Royal
    Statistical Society Series B: Statistical Methodology*, 67(3):427–444, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zorich [2015] VA Zorich. Multidimensional geometry, functions of very many variables,
    and probability. *Theory of Probability & Its Applications*, 59(3):481–493, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghaffari et al. [2023] Alireza Ghaffari, Justin Yu, Mahsa Ghazvini Nejad, Masoud
    Asgharian, Boxing Chen, and Vahid Partovi Nia. Mitigating outlier activations
    in low-precision fine-tuning of language models. *arXiv preprint arXiv:2312.09211*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models (2023). *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, page 8, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Almazrouei et al. [2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and
    Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art
    performance. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computer [2023] Together Computer. Redpajama: An open source recipe to reproduce
    llama training dataset, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penedo et al. [2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated
    corpora with web data, and web data only. *arXiv preprint arXiv:2306.01116*, 2023.
    URL [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.
    The pile: An 800gb dataset of diverse text for language modeling. *arXiv preprint
    arXiv:2101.00027*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Seed Sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since our proposed method, AdpQ, only uses deterministic pre-trained weights
    of the model and performs a soft-thresholding to identify $\alpha$ percent of
    outlier weights, it does not exhibit any stochastic behavior during the quantization.
    Furthermore, we do not use any data for calibration and thus, our algorithm is
    robust toward randomness in data selection. We believe this is the main advantage
    of our proposed algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Calibration Datasets and Parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We follow the pipelines used in SpQR¹¹1See [https://github.com/Vahe1994/SpQR](https://github.com/Vahe1994/SpQR)
    and AWQ²²2See [https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq)
    official implementation to generate calibration datasets. Random selection of
    128 samples of length 2048 form RedPajama [[41](#bib.bib41)], C4 and RefinedWeb
    [[42](#bib.bib42)] is used for quantization of LLaMA 1, LLaMa 2, OPT [[40](#bib.bib40)]
    and Falcon [[39](#bib.bib39)] using SpQR. For AWQ experiments 128 samples from
    a small subset of Pile [[43](#bib.bib43)] dataset is used following the AWQ’s
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Hyper-Parameters and Configs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RTN: We implemented RTN quantization method based on the implementation of
    AWQ which supports weight reshaping for group quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AWQ: We used AWQ’s official implementation for quantizing LLaMA and OPT models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SpQR: We use SpQR’s official implementation for quantizing LLaMA, Code-Llama
    and OPT models. Table [5](#A1.T5 "Table 5 ‣ A.3 Hyper-Parameters and Configs ‣
    Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") shows the hyper-parameters used for
    SpQR quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Quantization configuration of SpQR'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Calibration | Group | Weight | Scales & Zeros | Outlier |'
  prefs: []
  type: TYPE_TB
- en: '|  | Set | Size | Bits | Bits | Threshold |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA | RedPajama | 16 | 4 | 3 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RedPajama | 16 | 3 | 3 | 0.25-0.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Code-Llama | RedPajama | 16 | 4 | 3 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT | C4 | 16 | 4 | 3 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon | RefinedWeb | 16 | 4 | 3 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: A.4 Hardware Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We perform quantization on single NVIDIA V100-32G GPU. For evaluation using
    LM Evaluation Harness we use 8$\times$V100-32G GPUs for 30B models.
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Extra Experiments Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [6](#A1.T6 "Table 6 ‣ A.5 Extra Experiments Results ‣ Appendix A Experimental
    Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs") shows the perplexity results on OPT [[40](#bib.bib40)] and Falcon[[39](#bib.bib39)]
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Perplexity of 4-bit OPT and Falcon models on WikiText2 and C4.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Quantization setting | Avg Bits | Wiki2 $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-6.7B | FP16 | - | 16.00 | 10.86 | 11.74 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 11.15 | 12.31 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 10.95 | 11.86 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 10.91 | 11.78 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$) | 4.67 | 10.86 | 11.99 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13B | FP16 | - | 16.00 | 10.13 | 11.20 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 10.30 | 11.51 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 10.29 | 11.28 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.27 | 10.22 | 11.27 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$) | 4.67 | 10.20 | 11.31 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-30B | FP16 | - | 16.00 | 9.55 | 10.69 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 9.94 | 10.94 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 4bit-g128 | 4.25 | 9.61 | 10.74 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 9.55 | 10.71 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=5\%$) | 4.60 | 9.64 | 10.79 |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-7B | FP16 | - | 16.00 | 6.59 | 9.50 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 6.79 | 9.79 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.44 | 6.64 | 9.58 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=4\%$) | 4.53 | 6.69 | 9.63 |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-40B | FP16 | - | 16.00 | 5.23 | 7.76 |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | 4bit-g128 | 4.25 | 5.31 | 7.88 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.46 | 5.26 | 7.79 |'
  prefs: []
  type: TYPE_TB
- en: '| AdpQ | (4bit-g128, $\alpha=5\%$) | 4.60 | 5.27 | 7.81 |'
  prefs: []
  type: TYPE_TB
