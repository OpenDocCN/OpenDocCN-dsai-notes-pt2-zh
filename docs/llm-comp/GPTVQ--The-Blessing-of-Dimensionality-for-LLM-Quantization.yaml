- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:01'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'GPTVQ: The Blessing of Dimensionality for LLM Quantization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15319](https://ar5iv.labs.arxiv.org/html/2402.15319)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mart van Baalen    Andrey Kuzmin    Markus Nagel    Peter Couperus    Cedric
    Bastoul    Eric Mahurin    Tijmen Blankevoort    Paul Whatmough
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this work we show that the size versus accuracy trade-off of neural network
    quantization can be significantly improved by increasing the quantization dimensionality.
    We propose the GPTVQ method, a new fast method for post-training vector quantization
    (VQ) that scales well to Large Language Models (LLMs). Our method interleaves
    quantization of one or more columns with updates to the remaining unquantized
    weights, using information from the Hessian of the per-layer output reconstruction
    MSE. Quantization codebooks are initialized using an efficient data-aware version
    of the EM algorithm. The codebooks are then updated, and further compressed by
    using integer quantization and SVD-based compression. GPTVQ establishes a new
    state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such
    as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100
    it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization
    setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we
    show that VQ leads to improved latency compared to using a 4-bit integer format.
    Our source code is available at [https://github.com/qualcomm-ai-research/gptvq](https://github.com/qualcomm-ai-research/gptvq).'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, LLMs, Quantization, Vector Quantization, Compression
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have made significant strides in enabling human-like
    natural language interfaces for various applications, from general AI assistants
    like Open AI’s GPT (Achiam et al., [2023](#bib.bib1)) to specialized roles like
    coding companions (Roziere et al., [2023](#bib.bib30)) and medical aides (Tu et al.,
    [2024](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: However, these advanced models come with high computational costs due to their
    extensive parameter counts, necessitating frequent data transfers during execution.
    The primary bottleneck in efficient LLM inference lies in weight movement, especially
    since LLMs’ autoregressive nature requires loading and transferring weights for
    each generated token. Consequently, the weight movement’s cost often surpasses
    the computational expenses.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the challenge of cost reduction for these resource-intensive models,
    a critical question arises: How can we compress LLM weights to the maximum extent
    possible? Low-bit quantization has proven successful in reducing model weights
    to 4 bits without substantial accuracy loss (Frantar et al., [2022](#bib.bib10);
    Shao et al., [2023](#bib.bib31); Lin et al., [2023](#bib.bib24)). While much of
    the prior research has focused on uniform quantization for LLMs, we investigate
    the potential to achieve even greater compression by employing non-uniform quantization
    and expanding the dimensionality of the representational grid through vector quantization.
    In vector quantization (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization"), top right), multiple weights
    are quantized together, offering a more versatile quantization grid across multiple
    dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1353b61c3c31b788de371e3ab5c32d12.png)![Refer to caption](img/0b607aa1cbe60552b8756f964c84d45a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Top: An example of how vector quantization can better represent 2D
    normally distributed data compared to uniform quantization, non-uniform quantization.
    Bottom: Comparing GPTVQ to state-of-the-art uniform quantization on Llama 70B.'
  prefs: []
  type: TYPE_NORMAL
- en: We integrate our findings into a novel algorithm for post-training quantization
    called GPTVQ. This method allows fast non-uniform and vector quantization (VQ),
    improving the performance-size trade-off significantly compared to prior state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contributions of this work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our analysis and experimental results show that increasing dimensionality of
    quantization gives improved accuracy versus model size trade-offs for many LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a fast and accurate algorithm for post-training VQ compression. We
    show that our algorithm achieves SOTA size vs accuracy trade-offs on a wide range
    of LLMs, while having a practical run time of only 3 to 11 hours for a 70B parameter
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implemented and benchmarked VQ decompression on a mobile CPU. While VQ leads
    to significant memory footprint reductions, our on-device timings also demonstrate
    that it leads to improved latency compared to a 4-bit integer baseline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural network quantization is commonly used to reduce model footprint, data
    transfer and compute requirements. By quantizing a model, high bit-width floating
    point weights and activations that are commonly used for training can be represented
    by lower-precision values represented by fewer bits. Quantizing to 8 bits or lower
    significantly reduces footprint, data transfer and compute bottlenecks, at the
    cost of introducing *quantization noise* in the model, resulting in a potential
    drop in accuracy. In this section we provide a brief overview of uniform scalar
    quantization, non-uniform scalar quantization and introduce vector quantization,
    each of which offers progressively more flexibility in quantization. We will then
    illustrate how these methods improve representational accuracy of (non-uniform)
    underlying distributions, and can yield improved trade-offs between compression
    and accuracy. Finally, we touch upon the challenges of vector quantization and
    the limitations of current approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Types of quantization grid and their flexibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Uniform quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A symmetric uniform quantizer approximates an original floating point vector
    $\textbf{x}\in\mathbb{R}^{D}$ is a higher precision quantization scale, shared
    across the components of x.
  prefs: []
  type: TYPE_NORMAL
- en: Non-uniform quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Uniform quantization as presented in the previous section, while efficient,
    is very inflexible as the representable points can be solely equidistantly spaced.
    A more flexible quantization approach is non-uniform quantization using codebook
    quantization, in which floating point numbers are discretized to arbitrary scalar
    centroids in a codebook $C:C=\{c_{1},c_{2},\dots,c_{k}\}$ is less than the original
    bitwidth of the elements in x. Note that the codebook itself incurs overhead,
    which we will discuss in more detail in Sections [2.2](#S2.SS2 "2.2 Challenges
    of vector quantization ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization") and [3.2](#S3.SS2 "3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization").'
  prefs: []
  type: TYPE_NORMAL
- en: Vector quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In non-uniform quantization, as introduced in the previous paragraph, we assume
    that each scalar value in x is quantized individually. However, a more flexible
    quantizer can be constructed by choosing a higher-dimensionality for the centroids
    in codebook $C$ is also frequently referred to as product quantization (Stock
    et al., [2019](#bib.bib33)) .
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/883893ac983b72064795839df1c8484e.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 2: Quantization SQNR depending on the dimensionality for Llama-v2 7B
    weights. Signal-to-noise ratio increases with quantization dimensionality due
    to additional flexibility in the quantization grid.'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy improvement within higher dimensionality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is a well known fact that non-uniformly distributed data can be more accurately
    represented by a non-uniform quantizer. When increasing the dimensionality of
    the codebook, i.e. through VQ, the flexibility of the grid increases. A visual
    representation of this is given in figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"). In this example,
    where we quantize each value in the original to a 3-bits representation (i.e.,
    6 bits for VQ with $d=2$, but the distribution of the centroids can more closely
    match the underlying distribution, increasing the accuracy of the representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy of representation increases the more the dimensionality of the
    codebook increases. We can see the improvement in representational accuracy of
    higher $d$ in figure [2](#S2.F2 "Figure 2 ‣ Vector quantization ‣ 2.1 Types of
    quantization grid and their flexibility ‣ 2 Motivation ‣ GPTVQ: The Blessing of
    Dimensionality for LLM Quantization"). Here we plot the effect of compressing
    the weights of LLama-v2 7B with uniform quantization, non-uniform quantization,
    and vector quantization with 2 and 4 dimensions. On the y-axis we plot the signal-to-quantization
    noise ratio (SQNR) between the original and quantized weights, where higher is
    better. For fair comparison, we ensure the codebook overhead is always equal to
    0.25b per weight for each quantization method, i.e., improved SQNR is not caused
    trivially by using more bits for our representations. We can clearly see that
    as the dimensionality increase, the SQNR improves significantly as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Challenges of vector quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Codebook size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The improvement in accuracy of the representation comes at a cost, as we now
    need to store and transmit the VQ codebook $C_{d}$ is the VQ-dimension. If we
    aim to use VQ for compressing weight tensors we have to consider this overhead
    in finding good trade-offs between accuracy and size of the weight tensors in
    a network.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this work, we use *bits per dimension* ($b$.
  prefs: []
  type: TYPE_NORMAL
- en: Centroids and assignment setting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to apply vector quantization, one has to find a codebook of representative
    centroids, and an assignment to a centroid for each weight. While there are many
    methods to achieve this, a practical and popular approach is the k-Means algorithm
    (Han et al., [2015](#bib.bib15)). For neural network weights however, clustering
    on weights alone might not yield sufficient accuracy. To improve results, several
    authors (Stock et al., [2019](#bib.bib33); Martinez et al., [2021](#bib.bib25))
    include layer reconstruction error into their optimization, a technique that has
    been shown to improve results significantly in the model efficiency literature
    (He et al., [2017](#bib.bib17); Zhang et al., [2016](#bib.bib41); Nagel et al.,
    [2020](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, we find that neither k-Means alone, nor k-Means with layer input
    data included, is performant enough on Llamav2-7B (Touvron et al., [2023b](#bib.bib36)),
    as can be seen in Table [1](#S2.T1 "Table 1 ‣ Centroids and assignment setting
    ‣ 2.2 Challenges of vector quantization ‣ 2 Motivation ‣ GPTVQ: The Blessing of
    Dimensionality for LLM Quantization"). In this experiment we apply VQ to groups
    of weights, where each group of weights has its own codebook. We select the size
    of each weight group such that the overhead is the same for each setting. We see
    that, while results do improve when data is included, the increase in perplexity
    remains unacceptably large, especially for 2 and 3 bit VQ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: 2D VQ on Llamav2-7B using k-Means (without and with data included).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | With input data | Perplexity |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | n/a | 5.47 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 bits per dim | No | 1.3e3 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 948 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 bits per dim | No | 8.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 6.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 bits per dim | No | 5.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Yes | 5.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform 3 bit | Yes | 6.03 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform 4 bit | Yes | 5.74 |'
  prefs: []
  type: TYPE_TB
- en: While including layer input data improves results, the authors of methods such
    as (Stock et al., [2019](#bib.bib33); Martinez et al., [2021](#bib.bib25)) note
    that this alone does not yield satisfactory performance, and include an end-to-end
    fine-tuning step into their algorithms. Unfortunately, the size of modern LLMs
    make end-to-end fine-tuning prohibitively expensive for many practitioners. As
    we aim to have a fast and scalable method for post-training quantization, we set
    out to find a method that is accurate and takes the activations into account when
    quantizing, and is efficient and scalable to apply to significantly-sized large
    language models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 GPTVQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we introduce a novel method for vector-quantizing LLMs efficiently
    and accurately. As mentioned in the previous section, existing methods targeting
    VQ do not scale to LLM-sized models. Instead, we build on a recent uniform quantization
    method named GPTQ (Frantar et al., [2022](#bib.bib10)), which interleaves column-wise
    quantization with updates to the remaining (unquantized) weights, using information
    from the Hessian of the layer output reconstruction MSE. This method has been
    shown to give excellent performance on uniformly quantizing LLMs with up to hundreds
    of billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We first present a brief description of GPTQ. Then, we present our GPTVQ method,
    which extends GPTQ to VQ and integrates ideas from (Stock et al., [2019](#bib.bib33))
    for accurate initialization. Finally, we present a number of novel tricks to improve
    the size vs. accuracy trade-offs of the resulting quantized models.
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Background: GPTQ'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As described in Section [2.1](#S2.SS1 "2.1 Types of quantization grid and their
    flexibility ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"),
    quantization introduces quantization noise. A large body of literature exists
    with methods to alleviate the effects of quantization noise on model accuracy,
    see (Nagel et al., [2021](#bib.bib28); Gholami et al., [2022](#bib.bib13)) for
    recent surveys. Post-training quantization (PTQ) approaches aim to mitigate the
    adverse effects of quantization noise on pre-trained networks, without having
    to resort to costly quantization-aware training (QAT). A popular and effective
    approach in PTQ, introduced by AdaRound (Nagel et al., [2020](#bib.bib27)), is
    to modify weights to minimize a layer’s output error as an approximation to the
    full network’s loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{W}^{\ell}$ along its columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPTQ follows Optimal Brain Quantization (OBQ; Frantar & Alistarh ([2022](#bib.bib9))),
    which uses the Hessian of Equation [1](#S3.E1 "Equation 1 ‣ 3.1 Background: GPTQ
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"). This
    Hessian can be efficiently computed as $\textbf{H}^{(\ell)}=\textbf{X}^{(\ell)}\textbf{X}^{(\ell)T}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle E=\sum_{q}&#124;E_{q}&#124;_{2}^{2};$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'GPTQ extends OBQ in the following ways. First, GPTQ exploits the fact that
    $\textbf{H}^{(\ell)}$ on the layer’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\delta}=-\frac{\mathbf{W}_{:,q}-\text{quant(}\mathbf{W}_{:,q})}{\left[\textbf{H}^{-1}\right]_{qq}}\textbf{H}_{:,(q+1):}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'To reduce data transfer, GPTQ applies the update of Equation [3](#S3.E3 "Equation
    3 ‣ 3.1 Background: GPTQ ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization") only to a small block of $B$, which introduces a more numerically
    stable alternative to the inverse Hessian row and column removal operations of
    OBQ.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The GPTVQ method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Algorithm 1 GPTVQ: Quantize $\mathbf{W}\in\mathbb{R}^{r\times c}$'
  prefs: []
  type: TYPE_NORMAL
- en: 1:  $N_{b}\leftarrow\frac{c}{B}$20:  end for
  prefs: []
  type: TYPE_NORMAL
- en: The GPTVQ method generalizes the GPTQ method for non-uniform and vector quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the GPTQ framework we perform quantization of the weight tensor in
    a greedy manner starting from the first column. The details of the method are
    given in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization"). Given the VQ dimensionality
    $d$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $j=\arg\min_{m}\left(\textbf{x}-\textbf{c}^{(m)}\right)^{T}\textbf{H}^{(i)}\left(\textbf{x}-\textbf{c}^{(m)}\right).$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: After performing quantization of $d$ coordinates and apply it on the remaining
    weights as a single operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize the quantization error, we use several codebooks per layer. Each
    codebook is assigned to a group of weights (see Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization")).'
  prefs: []
  type: TYPE_NORMAL
- en: Codebook initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To initialize the codebook for a group of weights, we suggest the following
    variant of the EM algorithm. Given the set of $d$. The objective is the following
    sum of Hessian-weighted distance functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{H}^{(i)}$ equal to identity, the clustering method is equivalent
    to K-means. The objective can be minimized using E- and M-steps as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-step: find the assignment $j$ that minimizes the objective [4](#S3.E4 "Equation
    4 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization"). Using this distance function assigns optimal centroids based
    on the data-aware loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'M-step: find the centroid value $\textbf{c}^{(m)}$ that minimizes'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: This objective is a quadratic form w.r.t $\mathbf{c}^{(m)}$-dim inverse sub-Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: Blockwise data normalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to lower the error of vector quantization, we apply blockwise data
    normalization to the data before the codebook initialization. For each group corresponding
    to a new codebook we perform element-wise division $\mathbf{W}_{i}\oslash\mathbf{S}_{i}$,
    e.g. for a block size of 16, 32, or 64.
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of blocks (sub-rows) $\mathbf{w}^{(i)}$. The scaled data is used
    for codebook initialization. The inverse scaling is applied at VQ decoding step.
  prefs: []
  type: TYPE_NORMAL
- en: Total bits per value
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a measure of total model size, we compute *bits per value*, given by $\log_{2}(k)+kdb_{c}/l+b_{s}/N_{s}$
    is an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Additional steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the procedure in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") is completed,
    we perform several steps to further improve model size vs perplexity trade-offs.
    Each of these steps is described below.'
  prefs: []
  type: TYPE_NORMAL
- en: Codebook update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We found that output reconstruction error can be further reduced through a
    *codebook update*. Recall that, in line 15 of Algorithm [1](#alg1 "Algorithm 1
    ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM
    Quantization"), Q is incrementally constructed from the elements of C. Since this
    construction constitutes a lookup of values in C, the layerwise objective can
    still be minimized w.r.t C. The objective is a quadratic program and is convex:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\textbf{C}_{0},\dots,\textbf{C}_{N}}&#124;&#124;\mathbf{W}\mathbf{X}-\mathbf{Q}\mathbf{X}&#124;&#124;_{F}^{2},$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{Q}(\textbf{C}_{0},\dots,\textbf{C}_{N})$ only involves a look-up
    operation. In each GD step, the values in C are updated, and Q is reconstructed
    using the new values in C, keeping the assignments fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Codebook quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In practical scenarios, codebooks need to be quantized to 8 bits. As a further
    post-processing step, we quantize the codebook for each group of weights to signed
    8-bit integers, using symmetric min-max quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Further codebook compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We achieve improved model size vs perplexity trade-offs by reducing the rank
    of the codebook tensor C. For a single tensor, C has shape $N_{G}\times k\times
    d$.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments and results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 2: Weight-only quantization results of Llama-v2, Mistral, and Mixtral-MoE
    Models. We report WikiText2 perplexity and average zero-shot accuracy; Models
    marked ‘L2’ denote Llama-v2, M denotes Mistral, and 8x7B denotes Mixtral-MoE 8x7B.
    Numbers marked in bold are SOTA or surpass it, numbers underlined are on par with
    or outperform at least one VQ variant.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | WikiText2 perplexity $\downarrow$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | L2-7B | L2-13B | L2-70B | M-7B | 8x7B | L2-7B | L2-13B | M-7B | 8x7B
    |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 |  | 5.47 | 4.88 | 3.31 | 5.25 | 3.84 | 70.47 | 73.22 | 75.69 | 75.93
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2.125 bpv (W2@g128) | RTN | 4.2e3 | 122.08 | 27.27 | 1.4e3 | 4.3e3 | 36.94
    | 42.06 | 37.75 | 38.29 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 36.77 | 28.14 | 6.74 | 15.68 | 14.17 | 41.44 | 46.56 | 41.93 | 44.54
    |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 2.2e5 | 1.2e5 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 11.06 | 8.26 | 6.55 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 11.57 | 7.34 | 5.00 | 15.03 | 8.11 | 47.51 | 60.82 | 44.85
    | 57.54 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 8.23 | 6.50 | 4.64 | 10.28 | 6.37 | 57.24 | 64.46 | 57.25
    | 64.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.25 bpv (W2@g64) | RTN | 431.97 | 26.22 | 10.31 | 71.52 | 155.82 | 42.40
    | 46.41 | 44.79 | 46.86 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 20.85 | 22.44 | NAN | 14.24 | 10.07 | 47.51 | 54.16 | 51.76 | 48.78
    |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 2.1e5 | 1.2e5 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 9.62 | 7.56 | 6.11 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 10.08 | 7.17 | 4.82 | 9.56 | 8.06 | 51.95 | 61.48 | 55.82
    | 57.12 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 7.97 | 6.47 | 4.61 | 10.11 | 6.23 | 59.08 | 64.85 | 56.14
    | 63.92 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTVQ 4D (ours) | 7.22 | 6.08 | 4.39 | 7.16 | 5.55 | 61.49 | 66.17 | 64.44
    | 66.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.125 bpv (W3@g128) | RTN | 6.66 | 5.51 | 3.97 | 6.15 | 5.18 | 67.25 | 70.75
    | 71.79 | 72.40 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 6.29 | 5.42 | 3.85 | 5.83 | 4.71 | 66.16 | 71.44 | 72.24 | 72.73 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 6.24 | 5.32 | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 6.03 | 5.28 | 3.78 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 5.98 | 5.17 | 3.62 | 5.76 | 4.59 | 67.61 | 71.59 | 71.56
    | 72.75 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 5.82 | 5.10 | 3.55 | 5.51 | 4.30 | 67.88 | 71.76 | 73.56
    | 74.36 |'
  prefs: []
  type: TYPE_TB
- en: In this section we evaluate GPTVQ and compare the performance of vector quantization
    in 1, 2 and 4 dimensions against uniform quantization baseline methods.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use the Llama-1 (Touvron et al., [2023a](#bib.bib35)), Llama-2 (Touvron et al.,
    [2023b](#bib.bib36)) as well as Mistral-7B-v0.1 (Jiang et al., [2023](#bib.bib20))
    and Mixtral-MoE-8x7B-v0.1 (Jiang et al., [2024](#bib.bib21)). Additionally, we
    run a single ablation on BLOOM-560M (Workshop et al., [2022](#bib.bib38)).
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We follow Shao et al. ([2023](#bib.bib31)) and use the WikiText2 (Merity et al.,
    [2016](#bib.bib26)) training set as the calibration dataset for all our experiments.
    We evaluate our models on token perplexity for the WikiText2 validation set, as
    well as zero-shot language tasks: PIQA (Bisk et al., [2020](#bib.bib3)), ARC-easy
    and ARC-challenge (Clark et al., [2018](#bib.bib6)), BoolQ (Clark et al., [2019](#bib.bib5)),
    HellaSwag (Zellers et al., [2019](#bib.bib40)), and WinoGrande (Keisuke et al.,
    [2019](#bib.bib22)). For all datasets except WikiText2 we use the LLM-evaluation-harness
    (Gao et al., [2023](#bib.bib11)) to run evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare GPTVQ against various uniform quantization methods with different
    group sizes and ensure that all have the same bits-per-value (bpv) overhead. We
    include Round-to-Nearest (RTN) and several recent state-of-the-art PTQ approaches
    targeting LLMs: GPTQ (Frantar et al., [2022](#bib.bib10)), AWQ (Lin et al., [2023](#bib.bib24)),
    and OmniQuant (Shao et al., [2023](#bib.bib31)).'
  prefs: []
  type: TYPE_NORMAL
- en: Main results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4 Experiments and results ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") contains the main results for GPTVQ.
    In this table, we report WikiText 2 perplexity and an average over zero-shot task
    scores for the PIQA, BoolQ, ARC-easy, ARC-challenge, HellaSwag and WinoGrande
    tasks. In this table we report results for all Llama-v2 models, Mistral-7B-v0.1
    and Mixtral-8x7B-v0.1. More detailed results are included in appendix [A](#A1
    "Appendix A Extended results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization");
    Table [5](#A1.T5 "Table 5 ‣ Appendix A Extended results ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") contains individual scores for the zero-shot
    tasks and Table [4](#A1.T4 "Table 4 ‣ Appendix A Extended results ‣ GPTVQ: The
    Blessing of Dimensionality for LLM Quantization") contains WikiText2 perplexity
    for all Llama-v1 models, as well as further experiments with 4 bit quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: In these tables, we can see that non-uniform quantization using GPTVQ generally
    yields improved results over uniform PTQ methods. This gap becomes especially
    large at low bitwidths and for very large models. Compare e.g., GPTVQ 2D on Llamav2-70B
    to OmniQuant W2@g128, where an improvement of nearly 2 perplexity points is achieved.
    Furthermore, in nearly all cases, 2D VQ outperforms 1D VQ, and even more significant
    improvements are achieved with 4D VQ.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 GPTVQ hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In all our experiments we use the WikiText training set as calibration data
    for our method. Following (Frantar et al., [2022](#bib.bib10)) we sample 128 sequences
    of 2048 tokens each. Our method has several hyperparameters: the EM initialization
    method; the number of EM iterations; the number of weights in a block of weights
    sharing the same codebook; the number of columns in each block. Furthermore, we
    can lower codebook overhead through different routes: increasing the block size;
    quantizing the codebooks; or performing SVD on the codebooks. In our main results
    we use the following hyperparameter settings: We seed EM initialization with centroids
    found by our ‘Mahalanobis’ method (see Section [4.3](#S4.SS3.SSS0.Px1 "EM initialization
    ‣ 4.3 Ablations on hyperparameter choices ‣ 4 Experiments and results ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization")), and run EM for 100 iterations
    to initialize codebook centroids. Each weight group spans (at most) 256 columns,
    e.g., a group of 1024 weights is 4 rows $\times$ 256 columns. After the procedure
    in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization") is run, we update the codebook
    as described in [3.3](#S3.SS3 "3.3 Additional steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") for 25 iterations, and by default use
    8 bit uniform quantization to represent codebook values. In Section [4.3](#S4.SS3
    "4.3 Ablations on hyperparameter choices ‣ 4 Experiments and results ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization") we perform an ablation on
    the choice of each of these hyperparameters. We note that applying the blockwise
    data normalization as introduced in Section [3.2](#S3.SS2 "3.2 The GPTVQ method
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") mostly
    improves the final performance. However, for some cases, specifically 1D VQ with
    2 bits per index, it hurts the performance and in such cases we did not apply
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Codebook overhead
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As described in Section [2.2](#S2.SS2.SSS0.Px1 "Codebook size ‣ 2.2 Challenges
    of vector quantization ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization"), VQ codebooks introduce non-negligible overhead. A point
    rarely addressed is that the quantization scale of uniform quantization also needs
    to be stored and transmitted, and incurs an overhead. The overhead of this scale,
    while negligible for per-channel or per-tensor quantization, becomes significant
    for quantization to smaller block sizes, as is often applied in low-bitwidth quantization
    for LLMs (Rouhani et al., [2023](#bib.bib29); Frantar et al., [2022](#bib.bib10);
    Lin et al., [2023](#bib.bib24); Shao et al., [2023](#bib.bib31)). For groups of
    128 weights for example, a 16 bit scale introduces an overhead of $16/128=0.125$
    bits, meaning that each group needs to contain 2048 weights for the codebook overhead
    to meet the 2.125 bits per value target.'
  prefs: []
  type: TYPE_NORMAL
- en: To compare to the baseline results presented in (Shao et al., [2023](#bib.bib31)),
    we choose a combination of group size and codebook bitwidth that corresponds to
    an overhead of 0.125 or 0.25 bits per value. These settings correspond to uniform
    quantization with group sizes of 128 or 64 weights, respectively, as used in (Shao
    et al., [2023](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Data transfer speed comparision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To illustrate the effect of VQ on data transfer latency, we developed an optimized
    kernel for Arm® CPUs to efficiently decode VQ-compressed weights. Our implementation
    uses variants of the Arm® TBL instruction. The TBL instruction can be used to
    look up values in a lookup table (LUT), to translate an index of (at most) 5 bits
    to an 8 bit integer value. VQ in dimensions higher than 1 can be implemented by
    using multiple LUTs and corresponding TBL instructions. For example, 2D VQ with
    2 bits per index translates to 2 LUTs, one for each VQ dim, each mapping a 4-bit
    index to an 8 bit value.
  prefs: []
  type: TYPE_NORMAL
- en: We run an experiment on a device with Snapdragon® technology¹¹1Snapdragon is
    a product of Qualcomm Technologies, Inc. and/or its subsidiaries.. In our experiments
    we measure weights transferred and decoded per second and report relative speed
    compared to an 4-bit integer baseline. We measure data transfer latency on 2D
    vector quantized data tensors with 2 or 2.5 bits per dimension, i.e. 4 or 5 bits
    per index respectively. We don’t consider settings with a higher bitwidth per
    index, as this would require double the number of TBL instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.2 Data transfer speed comparision ‣ 4 Experiments
    and results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") shows
    the results of this experiment. In this table we show that besides large footprint
    reductions, VQ also reduces data transfer latency compared to the 4-bit integer
    baseline. Lastly, we run one LLM-generation experiment on Llamav2-7B on the same
    device. In this experiment we integrate a 1D VQ decoding kernel with the MatMul
    operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Model footprint and latency of vector-quantized data transfer and
    decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | BPV $\downarrow$ latency |'
  prefs: []
  type: TYPE_TB
- en: '| INT4 | 4 | 1.00$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| INT8 | 8 | 2.00$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2D 2.5B @ 512 | 3 | 0.75$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2D 2.5B @ 2048 | 2.25 | 0.56$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2D 2B @ 1024 | 2.25 | 0.56$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Llamav2-7B 1D 3B @ 128 | 3.5 | 0.88$\times$ |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Ablations on hyperparameter choices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EM initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Starting EM initialization from a good set of seed centroids is crucial to
    the final GPTVQ performance. To find seed centroids for EM initialization, we
    compare k-Means++ (Arthur & Vassilvitskii, [2007](#bib.bib2)) to a quick and effective
    initialization method which we dub *Mahalanobis initialization*. In the latter
    method, we initialize EM for a matrix of $N$ apart from the sorted list. While
    not theoretically justifiable, intuitively this method ensures that points are
    sampled at representative distances. Table [6](#A2.T6 "Table 6 ‣ Appendix B Tables
    for hyperparameter ablations ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")
    shows perplexity after GPTVQ for different methods of finding good seed values
    for EM initialization. Here we see that Mahalanobis initialization performs comparably
    to k-Means++, at significantly increased speed.'
  prefs: []
  type: TYPE_NORMAL
- en: EM iterations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We explore the effect of the number of EM initialization iterations on the
    final of perplexity of GPTVQ. Table [7](#A2.T7 "Table 7 ‣ Appendix B Tables for
    hyperparameter ablations ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")
    shows that even up to 100 iterations, results keep slightly improving, therefore
    we use 100 iterations as default.'
  prefs: []
  type: TYPE_NORMAL
- en: Codebook overhead
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in section [4.1](#S4.SS1 "4.1 GPTVQ hyperparameters ‣ 4 Experiments
    and results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"), we
    determine a group size to target a specific overhead. However, if codebooks are
    quantized to lower bitwidths, or if codebook compression is applied as described
    in Section [3.3](#S3.SS3 "3.3 Additional steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization"), the group size can be proportionally
    decreased to achieve the same overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform experiments targeting an overhead of 0.125 bits per value, and evaluate
    which method achieves best results: keeping the codebook in 16 bit, quantizing
    the codebook to 8 bit and halving the blocksize, or keeping the codebook in 16
    bit, but reducing its rank to 50% of the original rank and halving the blocksize.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [8](#A2.T8 "Table 8 ‣ Appendix B Tables for hyperparameter ablations
    ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") the results of
    these experiments show that, overall, quantizing the codebook to 8 bit generally
    yields slightly improved results.'
  prefs: []
  type: TYPE_NORMAL
- en: Codebook update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [9](#A2.T9 "Table 9 ‣ Appendix B Tables for hyperparameter ablations
    ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") we include an ablation
    on the effect including codebook update, as described in Section [3.3](#S3.SS3
    "3.3 Additional steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM
    Quantization"). We find that, in all cases, updating the codebook after running
    Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The
    Blessing of Dimensionality for LLM Quantization") improves final perplexity, at
    the expense of moderately increased (though still reasonable) run time. We thus
    include codebook update in all training runs.'
  prefs: []
  type: TYPE_NORMAL
- en: Method runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our method can process large language models efficiently. Exact runtime of
    GPTVQ depends on model, quantization setting (groupsize, bitwidth, vq-dimension),
    and several hyperparameters (EM iterations, codebook update iterations). To give
    an indication of realistic run-times: on a single H100, Llamav2-7B takes between
    30 minutes and 1 hour, while Llamav2-70B takes between between 3 and 11 hours.'
  prefs: []
  type: TYPE_NORMAL
- en: Effect of blockwise data normalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We investigate how applying input data normalization as described in Section [3.2](#S3.SS2
    "3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM
    Quantization") affects final performance. Table [10](#A2.T10 "Table 10 ‣ Appendix
    B Tables for hyperparameter ablations ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization") shows how perplexity of the quantized model depends on
    the scaling block size. In addition, we compared perplexity for configurations
    of equal overhead with and without scaling applied, see the Table [11](#A2.T11
    "Table 11 ‣ Appendix B Tables for hyperparameter ablations ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") for the results. Overall, we see that
    scaling improves the results in many cases, however sometimes it leads to perplexity
    increase, especially in the case of 1D VQ with 2 bits per index.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A number of works suggested using vector quantization for CNN weights compression (Gong
    et al., [2014](#bib.bib14); Martinez et al., [2021](#bib.bib25); Fan et al., [2020](#bib.bib8);
    Stock et al., [2019](#bib.bib33); Wu et al., [2016](#bib.bib39); Martinez et al.,
    [2021](#bib.bib25); Cho et al., [2021](#bib.bib4)). The most common approach is
    to reshape the weights of convolutional or fully connected layers into a matrix,
    and then apply K-means clustering directly on the columns. Typically, the clustering
    is applied on scalar or vectors of dimensionality 4 or higher. Some of the works
    consider data-aware optimization of the quantized weights. Most often, a variant
    of EM algorithm is used in order to update centroids and assignments (Stock et al.,
    [2019](#bib.bib33); Gong et al., [2014](#bib.bib14)). An alternative approach
    is using a differentiable K-means formulation which enables fine-tuning using
    SGD with the original loss function in order to recover the network accuracy (Cho
    et al., [2021](#bib.bib4); Fan et al., [2020](#bib.bib8); Tang et al., [2023](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: LLM quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Applying DNN quantization approaches for recent LLMs often poses significant
    computational challenges. Therefore, even uniform post-training quantization methods
    required revisiting to improve their scalability (Frantar et al., [2022](#bib.bib10)).
    As vector quantization approaches have higher computational complexity, using
    them for LLM weights compression has even stricter computational requirements.
    The most similar to our work is the approach (Deng et al., [2024](#bib.bib7)).
    The method uses gradient-based layer sensitivities to update the codebooks and
    a reduced complexity LoRA-based approach (Hu et al., [2021](#bib.bib18)) to partially
    recover the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Hessian-based compression methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several classical works suggest to use second-order approximation of the neural
    network loss function for accurate unstructured pruning (LeCun et al., [1989](#bib.bib23);
    Hassibi et al., [1993](#bib.bib16)). A line of more recent papers extend this
    family of methods for PTQ (Singh & Alistarh, [2020](#bib.bib32); Frantar & Alistarh,
    [2022](#bib.bib9); Frantar et al., [2022](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work we have shown that vector quantization in one or more dimensions
    progressively improves quantized model accuracy. We have introduced a fast method
    for post-training quantization of large networks using VQ. This method achieves
    SOTA model size vs accuracy trade-offs on a wide range of LLMs and zero-shot tasks.
    Finally, we have shown that VQ presents a HW-feasible alternative to uniform quantization
    as a compression method, yielding increased tokens per second at the same accuracy,
    or higher accuracy for a fixed tokens per second budget.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Amir Said for useful discussions.
  prefs: []
  type: TYPE_NORMAL
- en: Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our method can be used to make models more efficient. Given hardware and a software
    stack that supports vector quantized networks, a user can run more inference for
    a given energy budget, or reduce the energy required for a fixed inference task.
  prefs: []
  type: TYPE_NORMAL
- en: Democratization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reducing the inference cost of neural networks generally allows more practitioners
    to deploy models and increases democratization of deep learning. Our method itself
    is efficient enough that it can be run on consumer-grade hardware, even for very
    large networks.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While it has been shown that model pruning can increase bias in neural networks
    (Iofinova et al., [2023](#bib.bib19)), whether this is the case for quantization
    and to what extent, and whether how this applies to large language models is an
    underexplored topic. An investigation of this topic is outside the scope of this
    paper, but we concede that our method may introduce subtle biases into quantized
    models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.
    Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arthur & Vassilvitskii (2007) Arthur, D. and Vassilvitskii, S. K-means++ the
    advantages of careful seeding. In *Proceedings of the eighteenth annual ACM-SIAM
    symposium on Discrete algorithms*, pp.  1027–1035, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2021) Cho, M., Vahid, K. A., Adya, S., and Rastegari, M. Dkm: Differentiable
    k-means clustering layer for neural network compression. *arXiv preprint arXiv:2108.12659*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. In *NAACL*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv:1803.05457v1*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2024) Deng, J., Li, S., Wang, C., Gu, H., Shen, H., and Huang,
    K. LLM-codebook for extreme compression of large language models, 2024. URL [https://openreview.net/forum?id=nMbWsXPUVL](https://openreview.net/forum?id=nMbWsXPUVL).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,
    Jegou, H., and Joulin, A. Training with quantization noise for extreme model compression.
    *arXiv preprint arXiv:2004.07320*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar & Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gersho & Gray (2012) Gersho, A. and Gray, R. M. *Vector quantization and signal
    compression*, volume 159. Springer Science & Business Media, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gholami et al. (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    and Keutzer, K. A survey of quantization methods for efficient neural network
    inference. In *Low-Power Computer Vision*, pp.  291–326\. Chapman and Hall/CRC,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2014) Gong, Y., Liu, L., Yang, M., and Bourdev, L. Compressing
    deep convolutional networks using vector quantization. *arXiv preprint arXiv:1412.6115*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassibi et al. (1993) Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain
    surgeon and general network pruning. In *IEEE international conference on neural
    networks*, pp.  293–299\. IEEE, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) He, Y., Zhang, X., and Sun, J. Channel pruning for accelerating
    very deep neural networks. In *Proceedings of the IEEE International Conference
    on Computer Vision*, pp.  1389–1397, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iofinova et al. (2023) Iofinova, E., Peste, A., and Alistarh, D. Bias in pruned
    vision models: In-depth analysis and countermeasures. *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, CVPR*, pp.  24364–24373, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F.,
    et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keisuke et al. (2019) Keisuke, S., Ronan, L. B., Chandra, B., and Yejin, C.
    Winogrande: An adversarial winograd schema challenge at scale. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martinez et al. (2021) Martinez, J., Shewakramani, J., Liu, T. W., Bârsan,
    I. A., Zeng, W., and Urtasun, R. Permute, quantize, and fine-tune: Efficient compression
    of neural networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pp.  15699–15708, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and
    Blankevoort, T. Up or down? adaptive rounding for post-training quantization.
    In *International Conference on Machine Learning*, pp.  7197–7206\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2021) Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y.,
    Van Baalen, M., and Blankevoort, T. A white paper on neural network quantization.
    *arXiv preprint arXiv:2106.08295*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rouhani et al. (2023) Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall,
    M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G., Shao,
    L., Kolhe, G., Melts, D., Klar, J., L’Heureux, R., Perry, M., Burger, D., Chung,
    E., Deng, Z., and Naumov, M. With shared microexponents, a little shifting goes
    a long way. In *Proceedings of the 50th Annual International Symposium on Computer
    Architecture*, pp.  Article No.: 83, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open
    foundation models for code. *arXiv preprint arXiv:2308.12950*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirectionally calibrated
    quantization for large language models. *arXiv preprint arXiv:2308.13137*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh & Alistarh (2020) Singh, S. P. and Alistarh, D. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stock et al. (2019) Stock, P., Joulin, A., Gribonval, R., Graham, B., and Jégou,
    H. And the bit goes down: Revisiting the quantization of neural networks. *arXiv
    preprint arXiv:1907.05686*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2023) Tang, X., Wang, Y., Cao, T., Zhang, L. L., Chen, Q., Cai,
    D., Liu, Y., and Yang, M. Lut-nn: Empower efficient neural network inference with
    centroid learning and table lookup. In *Proceedings of the 29th Annual International
    Conference on Mobile Computing and Networking*, pp.  1–15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tu et al. (2024) Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg, J.,
    Tanno, R., Wang, A., Li, B., Amin, M., Tomasev, N., et al. Towards conversational
    diagnostic ai. *arXiv preprint arXiv:2401.05654*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workshop et al. (2022) Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick,
    E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., et al. Bloom:
    A 176b-parameter open-access multilingual language model. *arXiv preprint arXiv:2211.05100*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2016) Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized
    convolutional neural networks for mobile devices. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, pp.  4820–4828, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Zhang, X., Zou, J., He, K., and Sun, J. Accelerating very
    deep convolutional networks for classification and detection. *IEEE transactions
    on pattern analysis and machine intelligence*, 38(10):1943–1955, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Extended results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 4: Weight-only quantization results of Llama-1, Llama-2, Mistral, and
    Mixtral-MoE Models. We report WikiText2 perplexity in this table; lower is better
    Models marked ‘L1’ or ‘L2’ denote Llama-v1 and Llama-v2, respectively. M denotes
    Mistral and 8x7B denotes Mixtral-MoE 8x7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | L1-7B | L1-13B | L1-30B | L1-65B | L2-7B | L2-13B | L2-70B | M-7B | 8x7B
    |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 |  | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 | 5.25 | 3.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.125 bpv (W2@g128) | RTN | 1.9e3 | 781.20 | 68.04 | 15.08 | 4.2e3 | 122.08
    | 27.27 | 1.4e3 | 4.3e3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 44.01 | 15.60 | 10.92 | 9.51 | 36.77 | 28.14 | 6.74 | 15.68 | 14.50
    |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 2.6e5 | 2.8e5 | 2.4e5 | 7.4e4 | 2.2e5 | 1.2e5 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 9.72 | 7.93 | 7.12 | 5.95 | 11.06 | 8.26 | 6.55 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 16.29 | 6.93 | 6.04 | 5.19 | 11.57 | 7.34 | 5.00 | 15.03
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 9.64 | 6.58 | 5.63 | 4.91 | 8.23 | 6.50 | 4.64 | 10.28
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2.25 bpv (W2@g64) | RTN | 188.32 | 101.87 | 19.20 | 9.39 | 431.97 | 26.22
    | 10.31 | 71.52 | 155.82 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 22.10 | 10.06 | 8.54 | 8.31 | 20.85 | 22.44 | NAN | 14.24 | 9.45 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 2.5e5 | 2.7e5 | 2.3e5 | 7.4e4 | 2.1e5 | 1.2e5 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 8.90 | 7.34 | 6.59 | 5.65 | 9.62 | 7.56 | 6.11 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 16.64 | 6.78 | 5.97 | 5.05 | 10.08 | 7.17 | 4.82 | 9.56
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 9.90 | 6.43 | 5.56 | 4.86 | 7.97 | 6.47 | 4.61 | 10.11
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTVQ 4D (ours) | 8.76 | 6.33 | 5.42 | 4.74 | 7.22 | 6.08 | 4.39 | 7.16
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3.125 bpv (W3@g128) | RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97
    | 6.15 | 5.18 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 | 5.83 | 4.75 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 6.15 | 5.44 | 4.56 | 3.94 | 6.03 | 5.28 | 3.78 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 6.60 | 5.34 | 4.48 | 3.85 | 5.97 | 5.17 | 3.62 | 5.76 |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 6.32 | 5.31 | 4.38 | 3.79 | 5.82 | 5.10 | 3.55 | 5.51 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 4.125 bpv (W4@g128) | RTN | 5.96 | 5.25 | 4.23 | 3.67 | 5.72 | 4.98 | 3.46
    | 5.42 | 4.14 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 5.85 | 5.20 | 4.23 | 3.65 | 5.61 | 4.98 | 3.42 | 5.35 | 4.12 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | 5.81 | 5.20 | 4.21 | 3.62 | 5.62 | 4.97 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OmniQuant | 5.77 | 5.17 | 4.19 | 3.62 | 5.58 | 4.95 | 3.40 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 1D (ours) | 5.96 | 5.15 | 4.18 | 3.60 | 5.61 | 4.95 | 3.38 | 5.32 |  |'
  prefs: []
  type: TYPE_TB
- en: '| GPTVQ 2D (ours) | 5.94 | 5.20 | 4.18 | 3.64 | 5.68 | 4.97 | 3.39 | 5.44 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: LM-eval results of quantized Llama-v2 7B and 13B, Mistral-7B and Mixtral-8x7B
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\uparrow$ | #Bits | Method | PIQA | ARC-e | Arc-c | BoolQ | HellaSwag |
    Winogrande | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-v2-7B | FP16 | 79.11 | 74.58 | 46.25 | 77.74 | 75.99 | 69.14 | 70.47
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2.125 bpv (W2@g128) | RTN | 51.09 | 27.95 | 25.00 | 41.13 | 26.57 | 49.88
    | 36.94 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 54.84 | 30.64 | 25.09 | 53.43 | 33.09 | 51.54 | 41.44 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 62.95 | 40.28 | 22.61 | 61.90 | 44.63 | 52.72 | 47.51 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 70.73 | 58.08 | 31.48 | 63.73 | 58.49 | 60.93 | 57.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.25 bpv (W2@g64) | RTN | 58.76 | 36.66 | 24.83 | 41.87 | 40.38 | 51.93 |
    42.40 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 60.83 | 39.02 | 25.17 | 59.33 | 45.82 | 55.49 | 47.61 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 66.54 | 45.62 | 26.88 | 64.95 | 50.89 | 56.83 | 51.95 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 70.40 | 58.92 | 32.25 | 70.09 | 59.80 | 62.98 | 59.08 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-4D | 73.29 | 63.43 | 35.92 | 66.33 | 63.89 | 66.06 | 61.49 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.125 bpv (W3@g128) | RTN | 76.77 | 70.50 | 42.92 | 71.71 | 73.96 | 67.64
    | 67.25 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 77.37 | 68.14 | 40.70 | 71.04 | 72.50 | 67.25 | 66.16 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 77.64 | 70.12 | 42.15 | 75.90 | 71.42 | 68.43 | 67.61 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 77.64 | 72.73 | 43.69 | 71.65 | 72.71 | 67.64 | 67.68 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-v2-13B | FP16 | 80.52 | 77.53 | 49.23 | 80.52 | 79.38 | 72.14 | 73.22
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2.125 bpv (W2@g128) | RTN | 58.43 | 32.32 | 25.51 | 47.86 | 39.40 | 48.86
    | 42.06 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 59.52 | 40.15 | 27.65 | 57.06 | 41.56 | 53.43 | 46.56 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 72.74 | 63.85 | 35.75 | 65.54 | 61.60 | 65.43 | 60.82 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 75.19 | 68.27 | 39.51 | 70.67 | 65.66 | 67.48 | 64.46 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.25 bpv (W2@g64) | RTN | 61.59 | 41.58 | 25.43 | 49.79 | 48.24 | 51.85 |
    46.41 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 70.13 | 56.65 | 31.57 | 51.10 | 56.62 | 58.88 | 54.16 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 72.91 | 65.32 | 36.86 | 66.48 | 62.19 | 65.11 | 61.48 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 74.97 | 66.92 | 39.51 | 70.95 | 67.36 | 69.38 | 64.85 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-4D | 76.17 | 71.89 | 43.26 | 67.55 | 69.97 | 68.19 | 66.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.125 bpv (W3@g128) | RTN | 78.89 | 74.28 | 46.76 | 77.25 | 76.51 | 70.80
    | 70.75 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 79.33 | 75.84 | 47.01 | 78.90 | 77.16 | 70.40 | 71.44 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 78.78 | 75.55 | 47.35 | 79.36 | 76.57 | 71.90 | 71.59 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 79.43 | 75.29 | 48.12 | 78.99 | 76.96 | 71.74 | 71.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | FP16 | 82.10 | 79.59 | 53.92 | 83.58 | 81.07 | 73.88 | 75.69
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2.125 bpv (W2@g128) | RTN | 53.05 | 29.42 | 26.62 | 38.56 | 29.26 | 49.57
    | 37.75 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 57.73 | 35.65 | 26.62 | 46.06 | 36.06 | 49.49 | 41.93 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 58.71 | 38.85 | 23.89 | 59.51 | 37.40 | 50.83 | 44.86 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 69.10 | 59.64 | 34.22 | 68.99 | 55.07 | 56.51 | 57.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.25 bpv (W2@g64) | RTN | 60.72 | 38.47 | 27.56 | 44.83 | 46.10 | 51.07 |
    44.79 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 65.83 | 46.21 | 30.20 | 62.11 | 50.64 | 55.56 | 51.76 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 66.27 | 57.58 | 33.53 | 70.58 | 51.53 | 55.41 | 55.82 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 68.01 | 59.85 | 33.53 | 66.06 | 51.40 | 58.01 | 56.14 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-4D | 72.80 | 69.28 | 40.02 | 73.03 | 65.00 | 66.54 | 64.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.125 bpv (W3@g128) | RTN | 80.79 | 74.62 | 48.46 | 80.00 | 78.66 | 68.19
    | 71.79 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 79.82 | 75.51 | 49.40 | 81.22 | 77.34 | 70.17 | 72.24 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 79.76 | 75.04 | 47.53 | 79.69 | 75.91 | 71.43 | 71.56 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 80.41 | 77.23 | 49.57 | 82.72 | 78.52 | 72.93 | 73.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | FP16 | 83.46 | 73.74 | 55.89 | 84.74 | 82.45 | 75.30 | 75.93
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2.125 bpv (W2@g128) | RTN | 51.90 | 27.27 | 25.85 | 47.98 | 27.07 | 49.64
    | 38.29 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 59.79 | 35.44 | 27.30 | 52.08 | 41.80 | 50.83 | 44.54 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 71.00 | 52.53 | 33.96 | 68.75 | 57.30 | 61.72 | 57.54 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 76.17 | 59.43 | 41.72 | 75.08 | 66.62 | 67.96 | 64.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.25 bpv (W2@g64) | RTN | 62.08 | 38.68 | 28.41 | 54.46 | 44.40 | 53.12 |
    46.86 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 66.05 | 42.93 | 28.58 | 50.12 | 49.59 | 55.41 | 48.78 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 73.12 | 53.41 | 34.64 | 63.64 | 56.25 | 61.64 | 57.12 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 75.52 | 58.42 | 39.42 | 73.94 | 68.05 | 68.19 | 63.92 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-4D | 76.55 | 61.74 | 43.77 | 73.67 | 70.09 | 72.77 | 66.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.125 bpv (W3@g128) | RTN | 81.50 | 68.77 | 50.60 | 80.92 | 79.71 | 72.93
    | 72.40 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | 80.85 | 69.32 | 52.05 | 81.35 | 78.40 | 74.43 | 72.73 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-1D | 81.45 | 68.60 | 50.17 | 84.71 | 76.92 | 74.66 | 72.75 |'
  prefs: []
  type: TYPE_TB
- en: '| VQ-2D | 82.48 | 71.93 | 53.16 | 84.92 | 79.80 | 73.88 | 74.36 |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Tables for hyperparameter ablations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 6: Effect of EM initialization. Setting used: Llamav2-7B, 2D 3-bit VQ,
    blocksize 2048.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Lookup method | BPV | Setting | PPL | Time (s) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1D 3B 1024 | 3.125 | Mahalanobis | 6.05 | 605 |'
  prefs: []
  type: TYPE_TB
- en: '| K++ | 6.16 | 3328 |'
  prefs: []
  type: TYPE_TB
- en: '| 2D 3B 16384 | 3.125 | Mahalanobis | 5.65 | 756 |'
  prefs: []
  type: TYPE_TB
- en: '| K++ | 5.63 | 3168 |'
  prefs: []
  type: TYPE_TB
- en: '| 1D 4B 2048 | 4.125 | Mahalanobis | 5.86 | 1272 |'
  prefs: []
  type: TYPE_TB
- en: '| K++ | 5.88 | 2116 |'
  prefs: []
  type: TYPE_TB
- en: '| 2D 4B 65536 | 4.125 | Mahalanobis | 5.59 | 3816 |'
  prefs: []
  type: TYPE_TB
- en: '| K++ | 5.57 | 6644 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Effect of number of EM interations. Setting used: BLOOM-560m 2D 3-bit
    VQ with blocksize 4096, perplexity on WikiText2 test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| EM iterations | PPL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 24.49 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 24.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 24.12 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 24.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 24.09 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Choices in experimental setup leading to comparable bits per value.
    $d$: VQ bitwidth per dimension; gs: block size; Q: 8-bit codebook quantization
    yes/no; SVD: codebook SVD yes/no. BPV: bits per value.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $d$ | gs | Q | SVD | BPV | PPL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 512 | N | N | 2.125 | 14.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 256 | Y | N | 2.125 | 11.57 |'
  prefs: []
  type: TYPE_TB
- en: '| 256 | N | Y | 2.125 | 44.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1024 | N | N | 3.125 | 6.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 512 | Y | N | 3.125 | 5.98 |'
  prefs: []
  type: TYPE_TB
- en: '| 512 | N | Y | 3.125 | 5.98 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 4096 | N | N | 2.125 | 8.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 2048 | Y | N | 2.125 | 8.23 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 16384 | N | N | 3.125 | 5.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 8192 | Y | N | 3.125 | 5.87 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Effect of codebook fine-tuning on final PPL for Llamav2-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $d$ | gs | Update | PPL | Runtime (s) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 512 | N | 43.14 | 625 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 14.02 | 1857 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1024 | N | 6.05 | 712 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 6.01 | 1916 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 2048 | N | 8.64 | 723 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 8.21 | 1335 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 8192 | N | 5.93 | 1585 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 5.88 | 2195 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Effect of scaling block size on perplexity for Llamav2-7B. $d$: VQ
    bitwidth per dimension; gs: block size; Codebooks are quantized to 8 bits.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $d$ | gs | Scaling BS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | None | 128 | 64 | 32 | 16 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 512 | 14.01 | 16.74 | 2744.9 | 480.8 | 15.36 | 13.79 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1024 | 6.02 | 5.97 | 6.00 | 5.87 | 5.82 | 5.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 2048 | 8.23 | 8.38 | 8.04 | 7.97 | 7.56 | 6.89 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 8192 | 5.91 | 5.82 | 5.78 | 5.73 | 5.74 | 5.66 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Effect of scaling on perplexity for different models. Configurations
    with equal overhead with or without the scaling are considered. $d$: VQ bitwidth
    per dimension; gs: block size; Codebooks are assumed to be quantized to 8 bit.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $d$ | gs | Scale | Llamav2-7B | Llamav2-13B | Mistral-7B | Mixtral-8x7B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 256 | N | 14.01 | 7.34 | 15.03 | 8.56 |'
  prefs: []
  type: TYPE_TB
- en: '| 512 | Y | 171.29 | 7.44 | 87.60 | 8.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 512 | N | 5.98 | 5.21 | 5.76 | 4.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 1024 | Y | 6.01 | 5.17 | 5.77 | 4.59 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 2048 | N | 8.23 | 6.69 | 10.98 | 6.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 4096 | Y | 8.49 | 6.50 | 10.28 | 6.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 8192 | N | 5.91 | 5.19 | 8.63 | 4.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 16384 | Y | 5.56 | 5.11 | 5.53 | 4.30 |'
  prefs: []
  type: TYPE_TB
