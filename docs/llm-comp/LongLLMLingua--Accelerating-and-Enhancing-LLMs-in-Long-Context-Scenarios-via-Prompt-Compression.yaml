- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Huiqiang Jiang, Qianhui Wu, Xufang Luo,
  prefs: []
  type: TYPE_NORMAL
- en: Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Corporation {hjiang,qianhuiwu,xufang.luo}@microsoft.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In long context scenarios, large language models (LLMs) face three main challenges:
    higher computational/financial cost, longer latency, and inferior performance.
    Some studies reveal that the performance of LLMs depends on both the density and
    the position of the key information (question relevant) in the input prompt. Inspired
    by these findings, we propose LongLLMLingua for prompt compression towards improving
    LLMs’ perception of the key information to simultaneously address the three challenges.
    We conduct evaluation on a wide range of long context scenarios including single-/multi-document
    QA, few-shot learning, summarization, synthetic tasks, and code completion. The
    experimental results show that LongLLMLingua compressed prompt can derive higher
    performance with much less cost. The latency of the end-to-end system is also
    reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance
    boost of up to 17.1% over the original prompt with $\sim$10k tokens at a compression
    rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.
    ¹¹1Our code is available at [https://aka.ms/LLMLingua](https://aka.ms/LLMLingua).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGPT and other large language models (LLMs) have revolutionized user-oriented
    language technologies and are serving as crucial components in more and more applications.
    Carefully designing prompts is necessary to achieve better performance in specific
    downstream tasks. The commonly used technologies such as In-Context Learning (ICL) (Dong
    et al., [2023](#bib.bib8)), Retrieval Augment Generation (RAG) (Lewis et al.,
    [2020](#bib.bib17)), and Agent (Park et al., [2023](#bib.bib24)) are driving prompts
    to be increasingly longer, even reaching thousands of tokens. Scenarios such as
    multi-document question answering, code completion, and document summarization
    also necessitate the processing of long contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main challenges when LLMs are used in long context scenarios:
    (1) The higher computational and financial cost required to run these models or
    to call APIs from companies providing LLM services. This can be a significant
    barrier for individuals or smaller organizations with limited resources. (2) The
    longer latency associated with LLMs, which can cause delays in generating responses
    or predictions and is particularly problematic in real-time scenarios where users
    expect quick and accurate responses. (3) The inferior performance caused by the
    extended window size of LLMs (Xiong et al., [2023](#bib.bib33)), and the low density
    as well as the less sensitive position of the question-relevant key information
    in the prompt. Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    shows that LLMs’ performance in downstream tasks may decrease as the noisy information
    in the prompt increases (Shi et al., [2023](#bib.bib29)). Moreover, the purple
    curve in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    indicates that LLMs’ ability to capture the relevant information depends on their
    positions in the prompt (Liu et al., [2023](#bib.bib20)): they achieve the highest
    performance when relevant information occurs at the beginning or end of the input
    context, and significantly degrades if relevant information is located in the
    middle of long contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Performance v.s. Document Number
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/808a8cc98f1873ee2c344faffaffa60d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Performance v.s. Key Information Position
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: (a) LLMs’ performance in downstream tasks may decrease as the noisy
    information in the prompt increases. In this case, we keep $k$ implies more noise
    introduced into the prompt. To improve the key information density in the prompt,
    we present question-aware coarse-to-fine compression. (b) LLMs’ ability to capture
    the relevant information depends on their positions in the prompt. To reduce information
    loss in the middle, we introduce a document reordering mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by these observations, we propose LongLLMLingua to address the three
    challenges. Specifically, we use the advanced while efficient LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) as our backbone framework for prompt compression
    to address the first two challenges, i.e., reduce cost and latency. However, in
    the case of long contexts, the distribution of question-relevant key information
    in the prompt is generally sparse. Existing prompt compression methods like LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) and Selective-Context (Li, [2023](#bib.bib19)) that
    do not consider the content of the question during compression may retain too
    much noisy information in the compressed results, leading to inferior performance.
    In this paper, LongLLMLingua is designed to enhance LLM’s perception of key information
    (relevant to the question) in the prompt, so that the third challenge of inferior
    performance in long context scenarios could be addressed. Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression") is an example. The underlying
    principle of LongLLMLingua is that small language models are inherently capable
    of capturing the distribution of key information relevant to a given question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are five-fold: (1) We propose a question-aware coarse-to-fine
    compression method to improve the key information density in the prompt (Sec.
    [4.1](#S4.SS1 "4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")); (2) We introduce a document reordering mechanism to reduce
    information loss in the middle. (Sec. [4.2](#S4.SS2 "4.2 How to reduce information
    loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression")); (3) We present dynamic
    compression ratios to bridge the coarse-grained compression and fine-grained compression
    for adaptive granular control (Sec. [4.3](#S4.SS3 "4.3 How to achieve adaptive
    granular control during compression? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression")); (4) We
    propose a post-compression subsequence recovery strategy to improve the integrity
    of the key information ([4.4](#S4.SS4 "4.4 How to improve the integrity of key
    information? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression")). (5) We evaluate LongLLMLingua
    on three benchmarks, i.e., NaturalQuestions (Liu et al., [2023](#bib.bib20)),
    LongBench (Bai et al., [2023](#bib.bib1)), and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)).
    Experimental results demonstrate that compared with original prompts, LongLLMLingua
    compressed prompts can achieve higher performance with much lower costs. The latency
    of the end-to-end system is also reduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following LLMLingua (Jiang et al., [2023a](#bib.bib13)), we use $\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$.
    The objective of a prompt compression system can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\widetilde{\mathbf{x}}$ for joint optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '3 Preliminary: LLMLingua'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMLingua (Jiang et al., [2023a](#bib.bib13)) uses a small language model $\mathcal{M}_{S}$
    used for prompt compression.
  prefs: []
  type: TYPE_NORMAL
- en: 4 LongLLMLingua
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Framework of LongLLMLingua. Gray Italic content: As in LLMLingua.'
  prefs: []
  type: TYPE_NORMAL
- en: LongLLMLingua is developed upon the framework of LLMLingua towards prompt compression
    in long context scenarios. The primary challenge in long context scenarios is
    how to enhance LLM’s perception of key information relevant to the question in
    the prompt. LongLLMLingua addresses this challenge from three perspectives, and
    further applies a subsequence recovery strategy to improve the accuracy and reliability
    of the information provided to users. We elaborate on each component in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 How to improve key information density in the prompt?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Question-Aware Coarse-Grained Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In coarse-grained compression, we aim to figure out a metric $r_{k}$ as the
    intermediate compressed results.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMLingua uses document-level perplexity to represent the importance of documents:
    $r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$
    and instead become noise, reducing key information density in the compressed results
    and bringing difficulties for LLM to output correct answers. As shown in Figure [3a](#S4.F3.sf1
    "In Figure 3 ‣ Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve
    key information density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"), the recall@16
    of LLMLingua only reaches 50%, indicating its incompetence in retaining key information
    during compression.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-based methods are also feasible here. We can use $\mathbf{x}^{\text{que}}$75%
    accuracy in recall@5, which implies that the final accuracy upper bound of LLMs
    with 4x compression is only 75%.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e6955b6f2f232c285024fef939bddac.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Recall Distribution
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Perplexity Distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset.
    (b) Comparison between perplexities and contrastive perplexities of tokens in
    the prompt from Multi-documemnt QA dataset. The document with the ground truth
    is located on the left side of the dashed line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach to improve key information density in the compressed results is
    to calculate document-level perplexity conditioned on the question $\mathbf{x}^{\text{que}}$.
    It can be regarded as a regularization term that mitigates the impact of hallucinations.
    This can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $x^{\text{que},\text{restrict}}_{i}$ in the number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3a](#S4.F3.sf1 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") demonstrates that our coarse-level compression approach achieves
    the highest recall with different numbers of retained documents, suggesting that
    it preserves the most key information from the documents $(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$
    in the compressed results.'
  prefs: []
  type: TYPE_NORMAL
- en: Question-Aware Fine-Grained Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In fine-grained compression, we assess the importance of each token in the instruction
    $\mathbf{x}^{\text{ins}}$, so that the compressed results could contain more question-relevant
    key information.
  prefs: []
  type: TYPE_NORMAL
- en: 'A straightforward solution for the awareness of $\mathbf{x}^{\text{que}}$ can
    be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{i}=\text{perplexity}(x_{i}&#124;x_{<i})-\text{perplexity}(x_{i}&#124;x^{\text{que}},x_{<i}).$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure [3b](#S4.F3.sf2 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") illustrates the difference between perplexities and contrastive
    perplexities. We can see that tokens of high perplexities are widely distributed
    in all documents. However, tokens with high contrastive perplexities concentrate
    more on the left side of the dashed line, which corresponds to the document that
    contains the answer to the question. This suggests that the proposed contrastive
    perplexity can better distinguish tokens relevant to the question, thus improving
    the key information density in the compressed results.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 How to reduce information loss in the middle?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As demonstrated in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"),
    LLM achieves the highest performance when relevant information occurs at the beginning
    and significantly degrades if relevant information is located in the middle of
    long contexts. After the coarse-grained compression, we have obtained a set of
    documents $\{\mathbf{x}^{\text{doc}}_{k}\}_{k=1}^{K^{\prime}}$. Therefore, we
    reorder documents using their importance scores to better leverage LLMs’ information
    perception difference in positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 4.3 How to achieve adaptive granular control during compression?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In fine-grained compression, LLMLingua applies the save compression ratio over
    all documents obtained from coarse-grained compression. However, the key information
    density of different documents is different. The more relevant to the question
    a document is, the more budget (i.e., lower compression ratio) we should allocate
    to it. Therefore, we bridge coarse-grained compression to fine-grained compression
    and use the importance scores $\{r_{k}\}_{k=1}^{K^{\prime}}$ obtained from coarse-grained
    compression to guide the budget allocation in fine-grained compression. In this
    way, we can achieve adaptive granular control on the whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we first determine the initial budget for the retained documents
    $\tau^{\text{doc}}$ can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\tau_{i}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\tau_{k}^{\text{doc}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $N_{d}$ is a hyper-parameter that controls the overall budget for dynamic
    allocation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 How to improve the integrity of key information?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Certain tokens of key entities may be discarded during the fine-grained token-wise
    compression. For example, the time entity “2009” in the original prompt might
    be compressed to “209” and the name entity “Wilhelm Conrad Röntgen” might be compressed
    to “Wilhelmgen”. This can cause problems for fact-based tasks like document QA,
    where language models tend to replicate information from the prompt, as shown
    in Figure [4](#S4.F4 "Figure 4 ‣ 4.4 How to improve the integrity of key information?
    ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac2035d4d90754564ce58c0b540f1d0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The example of Subsequence Recovery, the red text represents the
    original text, and the blue text is the result after using the LLaMA 2-7B tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the accuracy and reliability of the information provided to users,
    we propose a subsequence recovery method to restore the original content from
    LLMs’ responses. This method relies on the subsequence relationship among tokens
    in the original prompt, compressed prompt, and LLMs’ response. The overall procedure
    includes: i) Iterate through tokens $y_{l}$ from the original prompt. For more
    details, please refer to Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Token-level
    Subsquence Recovery Details ‣ LongLLMLingua: Accelerating and Enhancing LLMs in
    Long Context Scenarios via Prompt Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we investigate: (1) How effective is LongLLMLingua? (2) How efficient
    is LongLLMLingua?'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this paper, we use GPT-3.5-Turbo-0613⁴⁴4For experiments with original prompts
    exceeding 4k tokens, we utilize GPT-3.5-Turbo-16k-0613. and LongChat-13B-16k as
    the target LLMs, both accessible via OpenAI⁵⁵5https://platform.openai.com and
    HuggingFace⁶⁶6https://huggingface.co/lmsys/longchat-13b-16k. To ensure stable
    and reproducible results, we employ greedy decoding and set the temperature to
    0 in all experiments. For the small language models used for compression, we apply
    LLaMA-2-7B-Chat⁷⁷7https://ai.meta.com/llama/, which has been aligned by supervised
    fine-tuning and RLHF. We implement our approach with PyTorch 1.13.1 and HuggingFace
    Transformers. We set up hyperparameters following LLMLingua except for the segment
    size used in iterative token-level compression set to 200 here. More details are
    provided in Appendix [B](#A2 "Appendix B Experiment Details ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset & Evaluation Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use NaturalQuestions for the multi-document QA task, and use LongBench and
    ZeroSCROLLS for general long context scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '(i) NaturalQuestions (Liu et al., [2023](#bib.bib20)): This benchmark is similar
    to the retrieval-augmented generation setup in commercial search and question-answering
    scenarios like Bing Chat. Specifically, each question has 20 related documents
    in the original prompt. One of them contains the correct answer and there are
    five different ground truth document position settings in the prompt: 1st, 5th,
    10th, 15th, and 20th. Following Liu et al. ([2023](#bib.bib20)), we use accuracy
    as the evaluation metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) LongBench (Bai et al., [2023](#bib.bib1)): This benchmark consists of
    six task types: single-document QA, multi-document QA, summarization, few-shot
    learning, code completion, and synthetic tasks. We used the English portion that
    covers 16 datasets for evaluation. We use the metrics and scripts provided along
    with the benchmark for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '(iii) ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)): This benchmark consists
    of four task types: summarization, QA, sentiment classification, and reordering,
    covering 10 datasets. We used the validation set for evaluation. We use the provided
    metrics and scripts for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We include two sets of baselines in following experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '(i) Retrieval-based Methods. We measure the association between the question
    and the documents in the prompt using five SoTA retrieval methods: BM25, Gzip (Jiang
    et al., [2023b](#bib.bib14)), SentenceBERT (Reimers & Gurevych, [2019](#bib.bib27)),
    OpenAI Embedding, and the important metric $r_{k}$ used in LongLLMLingua coarse-grained
    compression. We discard sentences or paragraphs with low association until the
    compression constraint is met while keeping the original document order unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) Compression-based Methods. We compare our approach with two state-of-art
    methods for prompt compression, i.e., Selective Context (Li, [2023](#bib.bib19))
    and LLMLingua (Jiang et al., [2023a](#bib.bib13)). Both methods employ LLaMA-2-7B-Chat
    as the small language model for compression. In LLMLingua, a coarse-to-fine approach
    is used to handle constraints of compression ratio: the original prompt is first
    compressed to $k$ is the granular control coefficient; token-level is then performed
    to reach the overall constraint. Our method follows the same coarse-to-fine logic
    to achieve the constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | GPT3.5-Turbo | LongChat-13b | Length |'
  prefs: []
  type: TYPE_TB
- en: '| 1st | 5th | 10th | 15th | 20th | Reorder | 1st | 5th | 10th | 15th | 20th
    | Reorder | Tokens | $1/\tau$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2x constraint |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval-based Methods |  |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | 53.7 | 49.3 | 47.9 | 49.9 | 46.9 | 50.3 | 50.9 | 44.9 | 44.1 | 42.9
    | 43.2 | 46.0 | 1,545 | 1.9x |'
  prefs: []
  type: TYPE_TB
- en: '| Gzip | 64.6 | 63.8 | 60.5 | 58.3 | 57.3 | 64.4 | 61.9 | 55.7 | 52.7 | 50.8
    | 50.9 | 59.3 | 1,567 | 1.9x |'
  prefs: []
  type: TYPE_TB
- en: '| SBERT | 72.5 | 67.9 | 63.3 | 65.0 | 66.2 | 68.7 | 65.8 | 57.5 | 54.9 | 53.4
    | 55.7 | 61.4 | 1,549 | 1.9x |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | 73.0 | 65.6 | 66.5 | 65.4 | 65.5 | 69.9 | 65.9 | 57.5 | 56.2 | 54.2
    | 55.7 | 61.7 | 1,550 | 1.9x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua $r_{k}$ | 73.9 | 67.7 | 68.7 | 66.0 | 65.6 | 74.3 | 68.5 |
    59.1 | 56.8 | 55.3 | 56.9 | 65.2 | 1,548 | 1.9x |'
  prefs: []
  type: TYPE_TB
- en: '| Compression-based Methods |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Selective-Context | 45.4 | 39.0 | 33.8 | 33.5 | 41.5 | - | 53.2 | 26.3 |
    25.4 | 24.2 | 33.3 | - | 1,478 | 2.0x |'
  prefs: []
  type: TYPE_TB
- en: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 | 41.5 | 38.7 | 37.3 | 35.7
    | 34.1 | 37.5 | 37.1 | 1,410 | 2.1x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 | 76.2 | 68.7 | 59.4 | 57.3
    | 55.9 | 58.4 | 66.1 | 1,429 | 2.1x |'
  prefs: []
  type: TYPE_TB
- en: '| 4x constraint |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | 40.6 | 38.6 | 38.2 | 37.4 | 36.6 | 36.3 | 39.5 | 37.5 | 36.8 | 36.4
    | 35.5 | 37.7 | 798 | 3.7x |'
  prefs: []
  type: TYPE_TB
- en: '| Gzip | 63.1 | 61.0 | 59.8 | 61.1 | 60.1 | 62.3 | 57.6 | 52.9 | 51.0 | 50.1
    | 50.4 | 57.2 | 824 | 3.6x |'
  prefs: []
  type: TYPE_TB
- en: '| SBERT | 66.9 | 61.1 | 59.0 | 61.2 | 60.3 | 64.4 | 62.6 | 56.6 | 55.1 | 53.9
    | 55.0 | 59.1 | 808 | 3.6x |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | 63.8 | 64.6 | 65.4 | 64.1 | 63.7 | 63.7 | 61.2 | 56.0 | 55.1 | 54.4
    | 55.0 | 58.8 | 804 | 3.7x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua $r_{k}$ | 71.1 | 70.7 | 69.3 | 68.7 | 68.5 | 71.5 | 67.8 |
    59.4 | 57.7 | 57.7 | 58.6 | 64.0 | 807 | 3.7x |'
  prefs: []
  type: TYPE_TB
- en: '| Compression-based Methods |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Selective-Context | 31.4 | 19.5 | 24.7 | 24.1 | 43.8 | - | 38.2 | 17.2 |
    15.9 | 16.0 | 27.3 | - | 791 | 3.7x |'
  prefs: []
  type: TYPE_TB
- en: '| LLMLingua | 25.5 | 27.5 | 23.5 | 26.5 | 30.0 | 27.0 | 32.1 | 30.8 | 29.9
    | 28.9 | 32.4 | 30.5 | 775 | 3.8x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | 75.0 | 71.8 | 71.2 | 71.2 | 74.7 | 75.5 | 68.7 | 60.5 | 59.3
    | 58.3 | 61.3 | 66.7 | 748 | 3.9x |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt | 75.7 | 57.3 | 54.1 | 55.4 | 63.1 | - | 68.6 | 57.4 | 55.3
    | 52.5 | 55.0 | - | 2,946 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot |  |  | 56.1 |  |  |  |  | 35.0 |  |  | 15 | 196x |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Performance of different methods with different compression ratios
    on NaturalQuestions (20 documents) (Liu et al., [2023](#bib.bib20)). Reorder:
    we reorder the documents with relevance metrics of different baselines as our
    document reordering strategy described in Sec. [4.2](#S4.SS2 "4.2 How to reduce
    information loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"). In the
    case of OpenAI, it corresponds to LongContextReorder in the LangChain framework (Chase,
    [2022](#bib.bib4)). For results reported under 1st to 20th, we do not use the
    reordering strategy for all methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '⁷⁷footnotetext: https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/
    long_context_reorder'
  prefs: []
  type: TYPE_NORMAL
- en: Main Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") and [3](#S5.T3
    "Table 3 ‣ Main Results ‣ 5 Experiments ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression") present the performance
    of various methods under different compression constraints. There are multiple
    observations and conclusions: (1) Our LongLLMLingua achieves the best performance
    across different tasks and constraints of compression ratios. Compared to the
    original prompt, our compressed prompt can derive higher performance with much
    less cost. For example, LongLLMLingua gains a performance boost of 17.1% on NaturalQuestions
    with the ground-true document at the 10th position, while the number of tokens
    input to GPT3.5-Turbo is $\sim$, LongLLMLingua even achieves a little performance
    gain. We mainly owe this to the question-aware coarse-to-fine compression, which
    can better figure out the key information and reach a higher key information density
    with a higher compression rate. (5) The proposed document reordering strategy
    helps in not only our approach but also other baselines as shown in Table [1](#S5.T1
    "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression"), well demonstrating its
    effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 1st | 5th | 10th | 15th | 20th |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 |'
  prefs: []
  type: TYPE_TB
- en: '| - w/o Question-aware Coarse-grained | 42.1 | 40.3 | 39.7 | 40.1 | 40.3 |'
  prefs: []
  type: TYPE_TB
- en: '| - w/ SBERT | 73.2 | 68.5 | 65.7 | 66.1 | 66.7 |'
  prefs: []
  type: TYPE_TB
- en: '| - w/o Question-aware Fine-grained | 75.8 | 71.0 | 68.9 | 68.4 | 69.3 |'
  prefs: []
  type: TYPE_TB
- en: '| - w/o Dynamic Compression Ratio | 74.4 | 70.7 | 68.7 | 67.9 | 68.1 |'
  prefs: []
  type: TYPE_TB
- en: '| - w/o Subsequence Recovery | 76.7 | 71.7 | 69.4 | 69.3 | 69.7 |'
  prefs: []
  type: TYPE_TB
- en: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 |'
  prefs: []
  type: TYPE_TB
- en: '| - w/ Subsequence Recovery | 43.8 | 44.1 | 43.5 | 43.3 | 44.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Ablation study on NaturalQuestions with 2x constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | LongBench | ZeroSCROLLS |'
  prefs: []
  type: TYPE_TB
- en: '| SingleDoc | MultiDoc | Summ. | FewShot | Synth. | Code | AVG | Tokens | $1/\tau$
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3,000 tokens constraint |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | 32.3 | 34.3 | 25.3 | 57.9 | 45.1 | 48.9 | 40.6 | 3,417 | 3x | 19.8
    | 3,379 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| SBERT | 35.3 | 37.4 | 26.7 | 63.4 | 51.0 | 34.5 | 41.4 | 3,399 | 3x | 24.0
    | 3,340 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | 34.5 | 38.6 | 26.8 | 63.4 | 49.6 | 37.6 | 41.7 | 3,421 | 3x | 22.4
    | 3,362 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua $r_{k}$ | 37.6 | 42.9 | 26.9 | 68.2 | 49.9 | 53.4 | 46.5 |
    3,424 | 3x | 29.3 | 3,350 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| Compression-based Methods |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Selective-Context | 23.3 | 39.2 | 25.0 | 23.8 | 27.5 | 53.1 | 32.0 | 3,328
    | 3x | 20.7 | 3,460 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| LLMLingua | 31.8 | 37.5 | 26.2 | 67.2 | 8.3 | 53.2 | 37.4 | 3,421 | 3x |
    30.7 | 3,366 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | 40.7 | 46.2 | 27.2 | 70.6 | 53.0 | 55.2 | 48.8 | 3,283 |
    3x | 32.8 | 3,412 | 3x |'
  prefs: []
  type: TYPE_TB
- en: '| 2,000 tokens constraint |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | 30.1 | 29.4 | 21.2 | 19.5 | 12.4 | 29.1 | 23.6 | 1,985 | 5x | 20.1
    | 1,799 | 5x |'
  prefs: []
  type: TYPE_TB
- en: '| SBERT | 33.8 | 35.9 | 25.9 | 23.5 | 18.0 | 17.8 | 25.8 | 1,947 | 5x | 20.5
    | 1,773 | 6x |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | 34.3 | 36.3 | 24.7 | 32.4 | 26.3 | 24.8 | 29.8 | 1,991 | 5x | 20.6
    | 1,784 | 5x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua $r_{k}$ | 37.8 | 41.7 | 26.9 | 66.3 | 53.0 | 52.4 | 46.3 |
    1,960 | 5x | 24.9 | 1,771 | 6x |'
  prefs: []
  type: TYPE_TB
- en: '| Compression-based Methods |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Selective-Context | 16.2 | 34.8 | 24.4 | 15.7 | 8.4 | 49.2 | 24.8 | 1,925
    | 5x | 19.4 | 1,865 | 5x |'
  prefs: []
  type: TYPE_TB
- en: '| LLMLingua | 22.4 | 32.1 | 24.5 | 61.2 | 10.4 | 56.8 | 34.6 | 1,950 | 5x |
    27.2 | 1,862 | 5x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | 39.0 | 42.2 | 27.4 | 69.3 | 53.8 | 56.6 | 48.0 | 1,809 |
    6x | 32.5 | 1,753 | 6x |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt | 39.7 | 38.7 | 26.5 | 67.0 | 37.8 | 54.2 | 44.0 | 10,295
    | - | 32.5 | 9,788 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | 15.6 | 31.3 | 15.6 | 40.7 | 1.6 | 36.2 | 23.5 | 214 | 48x | 10.8
    | 32 | 306x |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance of different methods under different compression ratios
    onLongBench (Bai et al., [2023](#bib.bib1)) and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28))
    using GPT-3.5-Turbo. Considering the dataset structure, we do not use the reordering
    strategy here.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To evaluate the contributions of different components in LongLLMLingua, we
    introduce six variants of it for ablation study: (1) Ours w/o Question-aware Coarse-grained,
    which calculates question-text relevance $r_{k}$. (3) Ours w/o Question-aware
    Fine-grained, which disregards Eq. ([3](#S4.E3 "In Question-Aware Fine-Grained
    Compression ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")) and only applies Iterative Token-level Prompt Compression
    as LLMLingua. (4) Ours w/o Dynamic Compression Ratio, where all documents share
    the same compression ratio in fine-grained compression. (5) Ours w/o and (6) LLMLingua
    w/ Subsequence Recovery, which either removes or adds the post-processing subsequence
    recovery strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S5.T2 "Table 2 ‣ Main Results ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") shows the
    results of the ablation study. In summary, removing any component proposed for
    LongLLMLingua will lead to a performance drop regardless of the position of the
    ground-truth answer. This well validates the necessity and effectiveness of the
    proposed question-aware mechanism during coarse-to-fine compression, the dynamic
    compression ratio, and the subsequence recovery strategy. It also shows that applying
    SBERT for coarse-grained compression will result in inferior performance, which
    implies the superiority of our question-aware importance metric in Eq. [2](#S4.E2
    "In Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve key information
    density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression") over SBERT. Moreover,
    our subsequence recovery strategy can also bring performance gains for LLMLingua.
    However, without our question-aware mechanism, results from LLMLingua are still
    less satisfactory. For more detailed cases, please go to Appendix [C](#A3 "Appendix
    C Ablation Analysis ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression").'
  prefs: []
  type: TYPE_NORMAL
- en: Latency Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| $1/\tau$ | 2x | 5x | 10x |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| E2E w/o Compression | 15.6 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| E2E w/ LLMLingua | 10.5 (1.5x) | 6.0 (2.6x) | 3.9 (4.0x) |'
  prefs: []
  type: TYPE_TB
- en: '| E2E w/ LongLLMLingua | 11.4 (1.4x) | 6.3 (2.5x) | 4.1 (3.8x) |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | 2.9 | 1.6 | 1.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Latency (s) on LongBench.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct testing on a V100-32G GPU, using the prompts from LongBench with
    $\sim$10K tokens on average and setting the response length to 200 tokens in the
    API call. In Table [5](#S5.F5 "Figure 5 ‣ Latency Evaluation ‣ 5 Experiments ‣
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression"), E2E denotes the latency from both the prompt compression system
    and the black-box API, while LongLLMLingua denotes the prompt compression latency
    only. It is shown that our prompt compression system does accelerate the overall
    inference. As the compression rate increases, the acceleration effect becomes
    more pronounced. It is worth mentioning that in scenarios with longer API cost
    time, the actual absolute time saved by LongLLMLingua can be more significant.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Long Context for LLMs. Recent research has focused on expanding the window
    size of LLMs. Main approaches include: (1) Staged pre-training (Nijkamp et al.,
    [2023](#bib.bib23)) which gradually increases the context window; (2) Modifying (Press
    et al., [2022](#bib.bib26)) or interpolating position embeddings (Chen et al.,
    [2023](#bib.bib5); Peng et al., [2023](#bib.bib25); Han et al., [2023](#bib.bib11));
    (3) Using linear or sparse attention mechanisms (Ding et al., [2023](#bib.bib7);
    Sun et al., [2023](#bib.bib30)); (4) Utilizing external memory modules for context
    storage (Bertsch et al., [2023](#bib.bib2); Tworkowski et al., [2023](#bib.bib31)).
    While these methods address context window expansion, their impact on downstream
    task performance has yet to be discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: Information Distribution in Prompt. Recent empirical experiments have shown
    that LLM performance decreases with less effective information in a prompt (Bai
    et al., [2023](#bib.bib1); Li et al., [2023](#bib.bib18); Shi et al., [2023](#bib.bib29)).
    Moreover, the position of relevant information in a prompt has a significant impact
    on performance(Wu et al., [2022](#bib.bib32)). Liu et al. ([2023](#bib.bib20))
    suggests that LLMs have more difficulty comprehending information located in the
    middle of a prompt compared to those at the edges.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval Methods can be categorized as dense or sparse retrieval methods. Sparse
    retrieval methods, like BM25, determine the relevance between queries and documents
    based on n-gram information. Conversely, dense retrieval methods assess the relevance
    between queries and documents in latent space using dense vectors, such as SentenceBERT (Reimers
    & Gurevych, [2019](#bib.bib27)) and OpenAI Embedding. Recently, Jiang et al. ([2023b](#bib.bib14)))
    proposed an unsupervised dense retrieval method that leverages traditional compression
    algorithms, such as gzip, and k-nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Compression Methods can be grouped into three main categories: (1) Token
    pruning (Goyal et al., [2020](#bib.bib10); Kim & Cho, [2021](#bib.bib15); Modarressi
    et al., [2022](#bib.bib21)) and token merging (Bolya et al., [2023](#bib.bib3)),
    which need model fine-tuning or intermediate results during inference and have
    been used with BERT-scale models. (2) Soft prompt tuning methods like GIST (Mu
    et al., [2023](#bib.bib22)), AutoCompressor (Chevalier et al., [2023](#bib.bib6)),
    and ICAE (Ge et al., [2023](#bib.bib9)), which require LLMs’ parameter fine-tuning,
    making them suitable for specific domains but not directly applicable to black-box
    LLMs. (3) Information-entropy-based approaches such as Selective Context (Li,
    [2023](#bib.bib19)) and LLMLingua (Jiang et al., [2023a](#bib.bib13)), which use
    a small language model to calculate the self-information or perplexity of each
    token in the original prompt and then remove tokens with lower perplexities.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose LongLLMLingua to address the three challenges, i.e., higher computational/financial
    cost, longer system latency, and inferior performance for LLMs in long context
    scenarios. We develop LongLLMLingua from the perspective of efficient prompt compression,
    thus reducing both computational/financial cost and the system latency. We further
    design four components, i.e., a question-aware coarse-to-fine compression method,
    a document reordering mechanism, dynamic compression ratios, and a post-compression
    subsequence recovery strategy to improve LLMs’ perception of the key information,
    with which LongLLMLingua demonstrate superior performance. Experiments on one
    multi-document QA benchmark and two long context benchmarks demonstrate that LongLLMLingua
    compressed prompt can derive higher performance than original prompts while both
    API costs for inference and the end-to-end system latency are largely reduced.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench:
    A bilingual, multitask benchmark for long context understanding. *ArXiv preprint*,
    abs/2308.14508, 2023. URL [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. Unlimiformer: Long-range transformers with unlimited length input. *ArXiv
    preprint*, abs/2305.01625, 2023. URL [https://arxiv.org/abs/2305.01625](https://arxiv.org/abs/2305.01625).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bolya et al. (2023) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
    Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster.
    In *The Eleventh International Conference on Learning Representations*, 2023.
    URL [https://openreview.net/forum?id=JroZRaRw7Eu](https://openreview.net/forum?id=JroZRaRw7Eu).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chase (2022) Harrison Chase. LangChain, 2022. URL [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *ArXiv preprint*, abs/2306.15595, 2023. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. Adapting language models to compress contexts. *ArXiv preprint*, abs/2305.14788,
    2023. URL [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. *ArXiv preprint*, abs/2307.02486, 2023. URL [https://arxiv.org/abs/2307.02486](https://arxiv.org/abs/2307.02486).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.
    *ArXiv preprint*, abs/2301.00234, 2023. URL [https://arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context
    autoencoder for context compression in a large language model. *ArXiv preprint*,
    abs/2307.06945, 2023. URL [https://arxiv.org/abs/2307.06945](https://arxiv.org/abs/2307.06945).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan T.
    Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating BERT
    inference via progressive word-vector elimination. In *Proceedings of the 37th
    International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
    Event*, volume 119 of *Proceedings of Machine Learning Research*, pp.  3690–3699\.
    PMLR, 2020. URL [http://proceedings.mlr.press/v119/goyal20a.html](http://proceedings.mlr.press/v119/goyal20a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language
    models. *ArXiv preprint*, abs/2308.16137, 2023. URL [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense
    information retrieval with contrastive learning. *Transactions on Machine Learning
    Research*, 2022. ISSN 2835-8856. URL [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. In *Proceedings of the 2023 Conference on Empirical Methods in
    Natural Language Processing*. Association for Computational Linguistics, December
    2023a. URL [https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
    Tang, Yiqin Dai, and Jimmy Lin. “low-resource” text classification: A parameter-free
    classification method with compressors. In *Findings of the Association for Computational
    Linguistics: ACL 2023*, pp.  6810–6828, Toronto, Canada, 2023b. Association for
    Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.426. URL [https://aclanthology.org/2023.findings-acl.426](https://aclanthology.org/2023.findings-acl.426).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim & Cho (2021) Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer:
    Train once with length drop, use anytime with search. In *Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pp. 
    6501–6511, Online, 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.508.
    URL [https://aclanthology.org/2021.acl-long.508](https://aclanthology.org/2021.acl-long.508).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions:
    A benchmark for question answering research. *Transactions of the Association
    for Computational Linguistics*, 7:452–466, 2019. doi: 10.1162/tacl˙a˙00276. URL
    [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation
    for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
    Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length?, 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li (2023) Yucheng Li. Unlocking context constraints of llms: Enhancing context
    efficiency of llms with self-information-based content filtering. *ArXiv preprint*,
    abs/2304.12102, 2023. URL [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models
    use long contexts. *ArXiv preprint*, abs/2307.03172, 2023. URL [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modarressi et al. (2022) Ali Modarressi, Hosein Mohebbi, and Mohammad Taher
    Pilehvar. AdapLeR: Speeding up inference by adaptive length reduction. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pp.  1–15, Dublin, Ireland, 2022\. Association for Computational
    Linguistics. doi: 10.18653/v1/2022.acl-long.1. URL [https://aclanthology.org/2022.acl-long.1](https://aclanthology.org/2022.acl-long.1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress
    prompts with gist tokens. *ArXiv preprint*, abs/2304.08467, 2023. URL [https://arxiv.org/abs/2304.08467](https://arxiv.org/abs/2304.08467).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report. *ArXiv
    preprint*, abs/2309.03450, 2023. URL [https://arxiv.org/abs/2309.03450](https://arxiv.org/abs/2309.03450).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. *ArXiv preprint*, abs/2304.03442, 2023. URL [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models. *ArXiv preprint*,
    abs/2309.00071, 2023. URL [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test
    long: Attention with linear biases enables input length extrapolation. In *International
    Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers & Gurevych (2019) Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence
    embeddings using Siamese BERT-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp.  3982–3992,
    Hong Kong, China, 2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410.
    URL [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. *ArXiv
    preprint*, abs/2305.14196, 2023. URL [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can
    be easily distracted by irrelevant context. In *International Conference on Machine
    Learning*, pp.  31210–31227\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia,
    Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer
    for large language models. *ArXiv preprint*, abs/2307.08621, 2023. URL [https://arxiv.org/abs/2307.08621](https://arxiv.org/abs/2307.08621).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive
    training for context scaling. *ArXiv preprint*, abs/2307.03170, 2023. URL [https://arxiv.org/abs/2307.03170](https://arxiv.org/abs/2307.03170).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    Self-adaptive in-context learning: An information compression perspective for
    in-context example selection and ordering. *ArXiv preprint*, abs/2212.10375, 2022.
    URL [https://arxiv.org/abs/2212.10375](https://arxiv.org/abs/2212.10375).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela
    Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective
    long-context scaling of foundation models. *ArXiv preprint*, abs/2309.16039, 2023.
    URL [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Token-level Subsquence Recovery Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 Pseudo code of Token-level Subsquence Recovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: The original prompt $\bm{x}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 1:Set the final response list $\bm{y}_{\text{rec}}=\phi$.11:     end if12:end while
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: The final response list $\bm{y}_{\text{rec}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Dataset Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NaturalQuestions Multi-document QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A multi-document question-answering dataset, comprising 2,655 problems, was
    built by Liu et al. ([2023](#bib.bib20)) based on the NaturalQuestions dataset (Kwiatkowski
    et al., [2019](#bib.bib16)). This dataset provides a realistic retrieval-augmented
    generation setup that closely resembles commercial search and question-answering
    applications (e.g., Bing Chat). Each example in the dataset contains a question
    and k related documents, utilizing the Contriever retrieval system (Izacard et al.,
    [2022](#bib.bib12)), one of which includes a document with the correct answer.
    To perform this task, the model must access the document containing the answer
    within its input context and use it to answer the question. The dataset’s data
    is sourced from the NaturalQuestions dataset, which contains historical queries
    issued to the Google search engine and human-annotated answers extracted from
    Wikipedia. The average prompt token length in this benchmark is 2,946\. For our
    experiments, we used the version provided by Liu et al. ([2023](#bib.bib20)) that
    includes 20 documents⁸⁸8https://github.com/nelson-liu/lost-in-the-middle. The
    dataset comprises five different ground truth document position settings in the
    prompt: 1st, 5th, 10th, 15th, and 20th.'
  prefs: []
  type: TYPE_NORMAL
- en: LongBench
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A multi-task long context benchmark consists of 3,750 problems in English and
    includes six categories with a total of 16 tasks. These tasks encompass key long-text
    application scenarios, such as single-document QA, multi-document QA, summarization,
    few-shot learning, synthetic tasks, and code completion. The average prompt token
    length in this benchmark is 10,289\. For our experiments, we used the English
    dataset and evaluation scripts provided by Bai et al. ([2023](#bib.bib1)) for
    this benchmark⁹⁹9https://github.com/THUDM/LongBench.
  prefs: []
  type: TYPE_NORMAL
- en: ZeroSCROLLS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The multi-task long context benchmark consists of 4,378 problems, including
    four categories with a total of 10 tasks. These tasks cover summarization, question
    answering, aggregated sentiment classification, and information reordering. The
    average prompt token length in this benchmark is 9,788\. For our experiments,
    we used the validation set and evaluation scripts provided by Shaham et al. ([2023](#bib.bib28))
    for this dataset^(10)^(10)10https://www.zero.scrolls-benchmark.com/.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Other Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All experiments were conducted using a Tesla V100 (32GB). We use tiktoken^(11)^(11)11https://github.com/openai/tiktoken
    and GPT-3.5-Turbo model to count all the tokens. We set the granular control coefficient
    $k$ used in dynamic compression ratio is set to 0.25. For a fair comparison, we
    only used reordering in the NaturalQuestions Multi-document QA and noted this
    in Table [1](#S5.T1 "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"). We use
    “We can get the answer to this question in the given documents.” as the guideline
    sentence in Equation ([3](#S4.E3 "In Question-Aware Fine-Grained Compression ‣
    4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua ‣
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression")).'
  prefs: []
  type: TYPE_NORMAL
- en: For the baselines experiment, we use the currently recommended strongest model,
    all-mpnet-base-v2^(12)^(12)12https://www.sbert.net/docs/pretrained_models.html,
    as the dense representation model for SentenceBERT. We use the recommended “text-embedding-ada-002”
    as the embedding model for OpenAI Embedding^(13)^(13)13https://platform.openai.com/docs/guides/embeddings/.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Ablation Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<svg id="A3.F6.pic1" class="ltx_picture" height="491.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,491.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 298.92)"><foreignobject width="556.69"
    height="178.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="261.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to <math id="A3.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math
    ltx_framed ltx_framed_rectangle" display="block"></math>, of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparing the compressed prompt and LLMs’ response before and after
    using Question-aware Fine-grained Compression and Subsequence Recovery($1/\tau$=30x,
    high compression ratio setting) from NaturalQuestions Multi-document QA (Liu et al.,
    [2023](#bib.bib20)) using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Economic Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | Multi-document QA | LongBench | ZeroScolls |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original | 4.6 | 31.5 | 30.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 1.3 | 3.0 | 3.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 7: The inference costs(per 1,000 samples $) for various datasets using
    GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [7](#A4.F7 "Figure 7 ‣ Appendix D Economic Cost ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") presents
    the estimated per 1,000 samples inference costs for various datasets, encompassing
    input prompts and generated output text, based on GPT-3.5-Turbo pricing^(14)^(14)14https://openai.com/pricing.
    Our approach demonstrates substantial savings in computational resources and monetary
    expenses, particularly in long context situations. Cost reductions of $3.3, $28.5,
    and $27.4 per 1,000 samples are observed for Multi-document QA, LongBench, and
    ZeroScrolls, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Cases Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<svg id="A5.F8.pic1" class="ltx_picture" height="800.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,800.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="772.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in <math id="A5.F8.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"
    class="ltx_Math ltx_framed ltx_framed_rectangle" display="block"></math>. Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Cases study on NaturalQuestions Multi-document QA dataset (Liu et al.,
    [2023](#bib.bib20)) in 4x constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A5.F9.pic1" class="ltx_picture" height="340.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please complete the code given below.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next line of code: LLMs’ Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Ground Truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Zero-shot LLMs’ Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Cases study on lcc code completion task in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A5.F10.pic1" class="ltx_picture" height="903.29" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,903.29) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="875.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Cases study on trec few-show learning in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
