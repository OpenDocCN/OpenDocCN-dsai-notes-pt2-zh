- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LongLLMLingua：通过提示压缩加速和增强长上下文场景中的大型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
- en: Huiqiang Jiang, Qianhui Wu, Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 江辉强、吴倩慧、罗旭芳
- en: Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 李东升、林钦岳、杨玉清、邱莉莉
- en: Microsoft Corporation {hjiang,qianhuiwu,xufang.luo}@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司 {hjiang,qianhuiwu,xufang.luo}@microsoft.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In long context scenarios, large language models (LLMs) face three main challenges:
    higher computational/financial cost, longer latency, and inferior performance.
    Some studies reveal that the performance of LLMs depends on both the density and
    the position of the key information (question relevant) in the input prompt. Inspired
    by these findings, we propose LongLLMLingua for prompt compression towards improving
    LLMs’ perception of the key information to simultaneously address the three challenges.
    We conduct evaluation on a wide range of long context scenarios including single-/multi-document
    QA, few-shot learning, summarization, synthetic tasks, and code completion. The
    experimental results show that LongLLMLingua compressed prompt can derive higher
    performance with much less cost. The latency of the end-to-end system is also
    reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance
    boost of up to 17.1% over the original prompt with $\sim$10k tokens at a compression
    rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.
    ¹¹1Our code is available at [https://aka.ms/LLMLingua](https://aka.ms/LLMLingua).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在长上下文场景中，大型语言模型（LLMs）面临三大挑战：更高的计算/财务成本、更长的延迟和较差的性能。一些研究表明，LLMs的性能依赖于输入提示中关键信息（与问题相关）的密度和位置。受这些发现的启发，我们提出了LongLLMLingua，用于提示压缩，以提高LLMs对关键信息的感知，从而同时解决这三大挑战。我们在多种长上下文场景下进行了评估，包括单文档/多文档问答、少样本学习、总结、合成任务和代码补全。实验结果表明，LongLLMLingua压缩的提示可以在更低成本下获得更高的性能。端到端系统的延迟也得到了减少。例如，在NaturalQuestions基准测试中，LongLLMLingua在原始提示的基础上性能提升最高可达17.1%，在压缩率为2x-10x的情况下，LongLLMLingua能够加快1.4x-3.8x的端到端延迟。¹¹1我们的代码可在[https://aka.ms/LLMLingua](https://aka.ms/LLMLingua)获取。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: ChatGPT and other large language models (LLMs) have revolutionized user-oriented
    language technologies and are serving as crucial components in more and more applications.
    Carefully designing prompts is necessary to achieve better performance in specific
    downstream tasks. The commonly used technologies such as In-Context Learning (ICL) (Dong
    et al., [2023](#bib.bib8)), Retrieval Augment Generation (RAG) (Lewis et al.,
    [2020](#bib.bib17)), and Agent (Park et al., [2023](#bib.bib24)) are driving prompts
    to be increasingly longer, even reaching thousands of tokens. Scenarios such as
    multi-document question answering, code completion, and document summarization
    also necessitate the processing of long contexts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT和其他大型语言模型（LLMs）已经彻底改变了以用户为导向的语言技术，并且正成为越来越多应用中的关键组成部分。为了在特定的下游任务中取得更好的性能，精心设计提示是必要的。常用技术如上下文学习（ICL）（Dong
    et al., [2023](#bib.bib8)）、检索增强生成（RAG）（Lewis et al., [2020](#bib.bib17)）和Agent（Park
    et al., [2023](#bib.bib24)）推动了提示变得越来越长，甚至达到数千个token。多文档问答、代码补全和文档总结等场景也要求处理长上下文。
- en: 'There are three main challenges when LLMs are used in long context scenarios:
    (1) The higher computational and financial cost required to run these models or
    to call APIs from companies providing LLM services. This can be a significant
    barrier for individuals or smaller organizations with limited resources. (2) The
    longer latency associated with LLMs, which can cause delays in generating responses
    or predictions and is particularly problematic in real-time scenarios where users
    expect quick and accurate responses. (3) The inferior performance caused by the
    extended window size of LLMs (Xiong et al., [2023](#bib.bib33)), and the low density
    as well as the less sensitive position of the question-relevant key information
    in the prompt. Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    shows that LLMs’ performance in downstream tasks may decrease as the noisy information
    in the prompt increases (Shi et al., [2023](#bib.bib29)). Moreover, the purple
    curve in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    indicates that LLMs’ ability to capture the relevant information depends on their
    positions in the prompt (Liu et al., [2023](#bib.bib20)): they achieve the highest
    performance when relevant information occurs at the beginning or end of the input
    context, and significantly degrades if relevant information is located in the
    middle of long contexts.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在长上下文场景中使用大型语言模型（LLMs）面临三个主要挑战：（1）运行这些模型或调用提供LLM服务的公司的API所需的计算和财务成本较高。这对于资源有限的个人或小型组织来说可能是一个显著的障碍。（2）LLMs的较长延迟，这可能导致生成响应或预测时的延迟，尤其在用户期望快速准确响应的实时场景中尤为严重。（3）由于LLMs的窗口大小扩展（Xiong
    et al., [2023](#bib.bib33)）以及提示中与问题相关的关键信息的位置较低和密度较低，导致性能较差。图[1a](#S1.F1.sf1 "在图
    1 ‣ 1 介绍 ‣ LongLLMLingua：通过提示压缩加速和增强LLMs在长上下文场景中的表现")显示，随着提示中噪声信息的增加，LLMs在下游任务中的性能可能会下降（Shi
    et al., [2023](#bib.bib29)）。此外，图[1b](#S1.F1.sf2 "在图 1 ‣ 1 介绍 ‣ LongLLMLingua：通过提示压缩加速和增强LLMs在长上下文场景中的表现")中的紫色曲线表明，LLMs捕获相关信息的能力取决于其在提示中的位置（Liu
    et al., [2023](#bib.bib20)）：当相关信息出现在输入上下文的开头或结尾时，LLMs的性能最佳；而当相关信息位于长上下文的中间时，性能会显著下降。
- en: '![Refer to caption](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
- en: (a) Performance v.s. Document Number
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 性能与文档数量
- en: '![Refer to caption](img/808a8cc98f1873ee2c344faffaffa60d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/808a8cc98f1873ee2c344faffaffa60d.png)'
- en: (b) Performance v.s. Key Information Position
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 性能与关键信息位置
- en: 'Figure 1: (a) LLMs’ performance in downstream tasks may decrease as the noisy
    information in the prompt increases. In this case, we keep $k$ implies more noise
    introduced into the prompt. To improve the key information density in the prompt,
    we present question-aware coarse-to-fine compression. (b) LLMs’ ability to capture
    the relevant information depends on their positions in the prompt. To reduce information
    loss in the middle, we introduce a document reordering mechanism.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: (a) 随着提示中噪声信息的增加，LLMs在下游任务中的性能可能会下降。在这种情况下，我们保持$k$意味着提示中引入了更多的噪声。为了提高提示中的关键信息密度，我们提出了基于问题的粗到细压缩方法。(b)
    LLMs捕获相关信息的能力取决于其在提示中的位置。为了减少中间信息丢失，我们引入了文档重排序机制。'
- en: 'Inspired by these observations, we propose LongLLMLingua to address the three
    challenges. Specifically, we use the advanced while efficient LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) as our backbone framework for prompt compression
    to address the first two challenges, i.e., reduce cost and latency. However, in
    the case of long contexts, the distribution of question-relevant key information
    in the prompt is generally sparse. Existing prompt compression methods like LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) and Selective-Context (Li, [2023](#bib.bib19)) that
    do not consider the content of the question during compression may retain too
    much noisy information in the compressed results, leading to inferior performance.
    In this paper, LongLLMLingua is designed to enhance LLM’s perception of key information
    (relevant to the question) in the prompt, so that the third challenge of inferior
    performance in long context scenarios could be addressed. Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression") is an example. The underlying
    principle of LongLLMLingua is that small language models are inherently capable
    of capturing the distribution of key information relevant to a given question.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '受到这些观察的启发，我们提出了 LongLLMLingua 来解决这三个挑战。具体而言，我们使用先进但高效的 LLMLingua （Jiang et al.,
    [2023a](#bib.bib13)）作为我们在提示压缩中的主框架，以应对前两个挑战，即降低成本和延迟。然而，在长上下文的情况下，提示中与问题相关的关键信息的分布通常很稀疏。现有的提示压缩方法，如
    LLMLingua （Jiang et al., [2023a](#bib.bib13)）和 Selective-Context （Li, [2023](#bib.bib19)），在压缩过程中未考虑问题的内容，可能会在压缩结果中保留过多的噪声信息，从而导致性能下降。本文中，LongLLMLingua
    旨在增强 LLM 对提示中与问题相关的关键信息的感知，以解决长上下文场景中的性能不足的第三个挑战。图 [1b](#S1.F1.sf2 "In Figure
    1 ‣ 1 Introduction ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression") 是一个例子。LongLLMLingua 的基本原理是，小型语言模型天生能够捕捉与给定问题相关的关键信息的分布。'
- en: 'Our main contributions are five-fold: (1) We propose a question-aware coarse-to-fine
    compression method to improve the key information density in the prompt (Sec.
    [4.1](#S4.SS1 "4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")); (2) We introduce a document reordering mechanism to reduce
    information loss in the middle. (Sec. [4.2](#S4.SS2 "4.2 How to reduce information
    loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression")); (3) We present dynamic
    compression ratios to bridge the coarse-grained compression and fine-grained compression
    for adaptive granular control (Sec. [4.3](#S4.SS3 "4.3 How to achieve adaptive
    granular control during compression? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression")); (4) We
    propose a post-compression subsequence recovery strategy to improve the integrity
    of the key information ([4.4](#S4.SS4 "4.4 How to improve the integrity of key
    information? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression")). (5) We evaluate LongLLMLingua
    on three benchmarks, i.e., NaturalQuestions (Liu et al., [2023](#bib.bib20)),
    LongBench (Bai et al., [2023](#bib.bib1)), and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)).
    Experimental results demonstrate that compared with original prompts, LongLLMLingua
    compressed prompts can achieve higher performance with much lower costs. The latency
    of the end-to-end system is also reduced.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献有五点：（1）我们提出了一种基于问题意识的粗到细压缩方法，以提高提示中的关键信息密度（第[4.1](#S4.SS1 "4.1 如何提高提示中的关键信息密度？
    ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLM")节）；（2）我们引入了一种文档重新排序机制，以减少中间的信息丢失（第[4.2](#S4.SS2
    "4.2 如何减少中间的信息丢失？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLM")节）；（3）我们提出了动态压缩比，以在粗粒度压缩和细粒度压缩之间架起桥梁，实现自适应粒度控制（第[4.3](#S4.SS3
    "4.3 如何在压缩过程中实现自适应粒度控制？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLM")节）；（4）我们提出了一种后压缩子序列恢复策略，以提高关键信息的完整性（第[4.4](#S4.SS4
    "4.4 如何提高关键信息的完整性？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLM")节）；（5）我们在三个基准测试上评估了LongLLMLingua，即NaturalQuestions （Liu等，[2023](#bib.bib20)），LongBench （Bai等，[2023](#bib.bib1)），以及ZeroSCROLLS （Shaham等，[2023](#bib.bib28)）。实验结果表明，与原始提示相比，LongLLMLingua压缩提示可以在更低的成本下实现更高的性能。端到端系统的延迟也有所减少。
- en: 2 Problem Formulation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题表述
- en: 'Following LLMLingua (Jiang et al., [2023a](#bib.bib13)), we use $\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$.
    The objective of a prompt compression system can be formulated as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据LLMLingua （Jiang等，[2023a](#bib.bib13)），我们使用$\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$。一个提示压缩系统的目标可以表述为：
- en: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
- en: where $\widetilde{\mathbf{x}}$ for joint optimization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\widetilde{\mathbf{x}}$用于联合优化。
- en: '3 Preliminary: LLMLingua'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步：LLMLingua
- en: LLMLingua (Jiang et al., [2023a](#bib.bib13)) uses a small language model $\mathcal{M}_{S}$
    used for prompt compression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua （Jiang等，[2023a](#bib.bib13)）使用了一个小型语言模型$\mathcal{M}_{S}$用于提示压缩。
- en: 4 LongLLMLingua
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LongLLMLingua
- en: '![Refer to caption](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
- en: 'Figure 2: Framework of LongLLMLingua. Gray Italic content: As in LLMLingua.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LongLLMLingua框架。灰色斜体内容：与LLMLingua相同。
- en: LongLLMLingua is developed upon the framework of LLMLingua towards prompt compression
    in long context scenarios. The primary challenge in long context scenarios is
    how to enhance LLM’s perception of key information relevant to the question in
    the prompt. LongLLMLingua addresses this challenge from three perspectives, and
    further applies a subsequence recovery strategy to improve the accuracy and reliability
    of the information provided to users. We elaborate on each component in this section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LongLLMLingua在LLMLingua框架基础上开发，旨在长上下文场景中的提示压缩。长上下文场景中的主要挑战是如何增强LLM对提示中与问题相关的关键信息的感知。LongLLMLingua从三个角度解决了这一挑战，并进一步应用了子序列恢复策略，以提高提供给用户的信息的准确性和可靠性。我们将在本节中详细阐述每个组件。
- en: 4.1 How to improve key information density in the prompt?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 如何提高提示中的关键信息密度？
- en: Question-Aware Coarse-Grained Compression
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题感知粗粒度压缩
- en: In coarse-grained compression, we aim to figure out a metric $r_{k}$ as the
    intermediate compressed results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在粗粒度压缩中，我们旨在找出一个度量 $r_{k}$ 作为中间压缩结果。
- en: 'LLMLingua uses document-level perplexity to represent the importance of documents:
    $r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$
    and instead become noise, reducing key information density in the compressed results
    and bringing difficulties for LLM to output correct answers. As shown in Figure [3a](#S4.F3.sf1
    "In Figure 3 ‣ Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve
    key information density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"), the recall@16
    of LLMLingua only reaches 50%, indicating its incompetence in retaining key information
    during compression.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua 使用文档级困惑度来表示文档的重要性：$r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log
    p(x_{k,i}^{\text{doc}})，k\in\{1,2,\cdots,K\}$，否则会变成噪声，降低压缩结果中的关键信息密度，并使 LLM 输出正确答案变得困难。如图 [3a](#S4.F3.sf1
    "图 3 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强
    LLM 在长上下文场景中的表现") 所示，LLMLingua 的 recall@16 仅为 50%，表明其在压缩过程中保持关键信息的能力不足。
- en: Retrieval-based methods are also feasible here. We can use $\mathbf{x}^{\text{que}}$75%
    accuracy in recall@5, which implies that the final accuracy upper bound of LLMs
    with 4x compression is only 75%.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的方法也适用。在 recall@5 中，我们可以使用 $\mathbf{x}^{\text{que}}$ 75% 的准确率，这意味着 LLM 在
    4 倍压缩下的最终准确率上限仅为 75%。
- en: '![Refer to caption](img/2e6955b6f2f232c285024fef939bddac.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e6955b6f2f232c285024fef939bddac.png)'
- en: (a) Recall Distribution
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 召回分布
- en: '![Refer to caption](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
- en: (b) Perplexity Distribution
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 困惑度分布
- en: 'Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset.
    (b) Comparison between perplexities and contrastive perplexities of tokens in
    the prompt from Multi-documemnt QA dataset. The document with the ground truth
    is located on the left side of the dashed line.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3： (a) 在 NaturalQuestions 多文档问答数据集上的召回率比较。 (b) Multi-document QA 数据集中提示的令牌的困惑度与对比困惑度的比较。具有真实标签的文档位于虚线的左侧。
- en: 'One approach to improve key information density in the compressed results is
    to calculate document-level perplexity conditioned on the question $\mathbf{x}^{\text{que}}$.
    It can be regarded as a regularization term that mitigates the impact of hallucinations.
    This can be formulated as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提高压缩结果中关键信息密度的一种方法是计算条件在问题 $\mathbf{x}^{\text{que}}$ 上的文档级困惑度。它可以视为一种正则化项，减轻幻觉的影响。可以制定如下公式：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $x^{\text{que},\text{restrict}}_{i}$ in the number of tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x^{\text{que},\text{restrict}}_{i}$ 以令牌数表示。
- en: 'Figure [3a](#S4.F3.sf1 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") demonstrates that our coarse-level compression approach achieves
    the highest recall with different numbers of retained documents, suggesting that
    it preserves the most key information from the documents $(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$
    in the compressed results.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3a](#S4.F3.sf1 "图 3 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣
    LongLLMLingua：通过提示压缩加速和增强 LLM 在长上下文场景中的表现") 证明了我们的方法在不同保留文档数量下实现了最高的召回率，表明它在压缩结果中保留了来自文档
    $(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$ 的大部分关键信息。
- en: Question-Aware Fine-Grained Compression
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题感知细粒度压缩
- en: In fine-grained compression, we assess the importance of each token in the instruction
    $\mathbf{x}^{\text{ins}}$, so that the compressed results could contain more question-relevant
    key information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度压缩中，我们评估指令 $\mathbf{x}^{\text{ins}}$ 中每个令牌的重要性，以便压缩结果可以包含更多与问题相关的关键信息。
- en: 'A straightforward solution for the awareness of $\mathbf{x}^{\text{que}}$ can
    be formulated as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对 $\mathbf{x}^{\text{que}}$ 的感知可以制定一个简单的解决方案：
- en: '|  | $s_{i}=\text{perplexity}(x_{i}&#124;x_{<i})-\text{perplexity}(x_{i}&#124;x^{\text{que}},x_{<i}).$
    |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=\text{困惑度}(x_{i}&#124;x_{<i})-\text{困惑度}(x_{i}&#124;x^{\text{que}},x_{<i})$。
    |  | (3) |'
- en: 'Figure [3b](#S4.F3.sf2 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") illustrates the difference between perplexities and contrastive
    perplexities. We can see that tokens of high perplexities are widely distributed
    in all documents. However, tokens with high contrastive perplexities concentrate
    more on the left side of the dashed line, which corresponds to the document that
    contains the answer to the question. This suggests that the proposed contrastive
    perplexity can better distinguish tokens relevant to the question, thus improving
    the key information density in the compressed results.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3b](#S4.F3.sf2 "图3 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣
    LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs") 说明了困惑度和对比困惑度之间的差异。我们可以看到高困惑度的标记在所有文档中广泛分布。然而，高对比困惑度的标记更集中在虚线的左侧，这对应于包含问题答案的文档。这表明，提出的对比困惑度可以更好地区分与问题相关的标记，从而提高压缩结果中的关键信息密度。'
- en: 4.2 How to reduce information loss in the middle?
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 如何减少中间的信息丢失？
- en: 'As demonstrated in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"),
    LLM achieves the highest performance when relevant information occurs at the beginning
    and significantly degrades if relevant information is located in the middle of
    long contexts. After the coarse-grained compression, we have obtained a set of
    documents $\{\mathbf{x}^{\text{doc}}_{k}\}_{k=1}^{K^{\prime}}$. Therefore, we
    reorder documents using their importance scores to better leverage LLMs’ information
    perception difference in positions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [1b](#S1.F1.sf2 "图1 ‣ 1 引言 ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs") 所示，当相关信息出现在开头时，LLM
    的性能达到最高，而如果相关信息位于长上下文的中间则显著下降。在粗粒度压缩后，我们获得了一组文档 $\{\mathbf{x}^{\text{doc}}_{k}\}_{k=1}^{K^{\prime}}$。因此，我们使用它们的重要性分数重新排序文档，以更好地利用LLMs对不同位置的信息感知差异：'
- en: '|  | $1$2 |  | (4) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 4.3 How to achieve adaptive granular control during compression?
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 如何在压缩过程中实现自适应粒度控制？
- en: In fine-grained compression, LLMLingua applies the save compression ratio over
    all documents obtained from coarse-grained compression. However, the key information
    density of different documents is different. The more relevant to the question
    a document is, the more budget (i.e., lower compression ratio) we should allocate
    to it. Therefore, we bridge coarse-grained compression to fine-grained compression
    and use the importance scores $\{r_{k}\}_{k=1}^{K^{\prime}}$ obtained from coarse-grained
    compression to guide the budget allocation in fine-grained compression. In this
    way, we can achieve adaptive granular control on the whole.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度压缩中，LLMLingua 对从粗粒度压缩中获得的所有文档应用节省压缩比。然而，不同文档的关键信息密度是不同的。文档与问题的相关性越高，我们应分配的预算（即较低的压缩比）就越多。因此，我们将粗粒度压缩与细粒度压缩连接起来，并使用从粗粒度压缩中获得的重要性分数
    $\{r_{k}\}_{k=1}^{K^{\prime}}$ 来指导细粒度压缩中的预算分配。这样，我们可以在整体上实现自适应的粒度控制。
- en: 'Specifically, we first determine the initial budget for the retained documents
    $\tau^{\text{doc}}$ can be formulated as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们首先确定保留文档的初始预算 $\tau^{\text{doc}}$ 可以被表述为：
- en: '|  | $\displaystyle\tau_{i}$ |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tau_{i}$ |  | (5) |'
- en: '|  | $\displaystyle\tau_{k}^{\text{doc}}$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tau_{k}^{\text{doc}}$ |  |'
- en: where $N_{d}$ is a hyper-parameter that controls the overall budget for dynamic
    allocation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{d}$ 是一个超参数，控制动态分配的整体预算。
- en: 4.4 How to improve the integrity of key information?
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 如何提高关键信息的完整性？
- en: 'Certain tokens of key entities may be discarded during the fine-grained token-wise
    compression. For example, the time entity “2009” in the original prompt might
    be compressed to “209” and the name entity “Wilhelm Conrad Röntgen” might be compressed
    to “Wilhelmgen”. This can cause problems for fact-based tasks like document QA,
    where language models tend to replicate information from the prompt, as shown
    in Figure [4](#S4.F4 "Figure 4 ‣ 4.4 How to improve the integrity of key information?
    ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '在细粒度逐词压缩过程中，某些关键实体的标记可能会被丢弃。例如，原始提示中的时间实体“2009”可能会被压缩为“209”，而名称实体“威尔赫尔姆·康拉德·伦琴”可能会被压缩为“威尔赫尔姆根”。这可能会给基于事实的任务（如文档
    QA）带来问题，因为语言模型往往会重复提示中的信息，如图 [4](#S4.F4 "图 4 ‣ 4.4 如何提高关键信息的完整性？ ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的 LLM")所示。'
- en: '![Refer to caption](img/ac2035d4d90754564ce58c0b540f1d0f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ac2035d4d90754564ce58c0b540f1d0f.png)'
- en: 'Figure 4: The example of Subsequence Recovery, the red text represents the
    original text, and the blue text is the result after using the LLaMA 2-7B tokenizer.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：子序列恢复的示例，其中红色文本表示原始文本，蓝色文本是使用 LLaMA 2-7B 分词器后的结果。
- en: 'To improve the accuracy and reliability of the information provided to users,
    we propose a subsequence recovery method to restore the original content from
    LLMs’ responses. This method relies on the subsequence relationship among tokens
    in the original prompt, compressed prompt, and LLMs’ response. The overall procedure
    includes: i) Iterate through tokens $y_{l}$ from the original prompt. For more
    details, please refer to Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Token-level
    Subsquence Recovery Details ‣ LongLLMLingua: Accelerating and Enhancing LLMs in
    Long Context Scenarios via Prompt Compression").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高提供给用户信息的准确性和可靠性，我们提出了一种子序列恢复方法，以从 LLM 的响应中恢复原始内容。这种方法依赖于原始提示、压缩提示和 LLM
    响应中的标记子序列关系。整体过程包括：i) 遍历原始提示中的标记 $y_{l}$。更多细节请参考算法 [1](#alg1 "算法 1 ‣ 附录 A 标记级子序列恢复细节
    ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的 LLM")。'
- en: 5 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'Here, we investigate: (1) How effective is LongLLMLingua? (2) How efficient
    is LongLLMLingua?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们调查了：（1）LongLLMLingua 的有效性如何？（2）LongLLMLingua 的效率如何？
- en: Implementation Details
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节
- en: 'In this paper, we use GPT-3.5-Turbo-0613⁴⁴4For experiments with original prompts
    exceeding 4k tokens, we utilize GPT-3.5-Turbo-16k-0613. and LongChat-13B-16k as
    the target LLMs, both accessible via OpenAI⁵⁵5https://platform.openai.com and
    HuggingFace⁶⁶6https://huggingface.co/lmsys/longchat-13b-16k. To ensure stable
    and reproducible results, we employ greedy decoding and set the temperature to
    0 in all experiments. For the small language models used for compression, we apply
    LLaMA-2-7B-Chat⁷⁷7https://ai.meta.com/llama/, which has been aligned by supervised
    fine-tuning and RLHF. We implement our approach with PyTorch 1.13.1 and HuggingFace
    Transformers. We set up hyperparameters following LLMLingua except for the segment
    size used in iterative token-level compression set to 200 here. More details are
    provided in Appendix [B](#A2 "Appendix B Experiment Details ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们使用 GPT-3.5-Turbo-0613⁴⁴4 对于原始提示超过 4k 标记的实验，我们使用 GPT-3.5-Turbo-16k-0613
    和 LongChat-13B-16k 作为目标 LLM，这些模型都可以通过 OpenAI⁵⁵5https://platform.openai.com 和 HuggingFace⁶⁶6https://huggingface.co/lmsys/longchat-13b-16k
    访问。为了确保结果的稳定性和可重复性，我们在所有实验中使用贪婪解码并将温度设置为 0。对于用于压缩的小型语言模型，我们应用了 LLaMA-2-7B-Chat⁷⁷7https://ai.meta.com/llama/，该模型已通过监督微调和
    RLHF 进行对齐。我们使用 PyTorch 1.13.1 和 HuggingFace Transformers 实现了我们的方法。我们按照 LLMLingua
    设置超参数，除了迭代标记级压缩中使用的分段大小在这里设置为 200。更多细节请参见附录 [B](#A2 "附录 B 实验细节 ‣ LongLLMLingua:
    通过提示压缩加速和增强长上下文场景中的 LLM")。'
- en: Dataset & Evaluation Metric
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集与评估指标
- en: We use NaturalQuestions for the multi-document QA task, and use LongBench and
    ZeroSCROLLS for general long context scenarios.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 NaturalQuestions 进行多文档 QA 任务，并使用 LongBench 和 ZeroSCROLLS 处理一般的长上下文场景。
- en: '(i) NaturalQuestions (Liu et al., [2023](#bib.bib20)): This benchmark is similar
    to the retrieval-augmented generation setup in commercial search and question-answering
    scenarios like Bing Chat. Specifically, each question has 20 related documents
    in the original prompt. One of them contains the correct answer and there are
    five different ground truth document position settings in the prompt: 1st, 5th,
    10th, 15th, and 20th. Following Liu et al. ([2023](#bib.bib20)), we use accuracy
    as the evaluation metric.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (i) NaturalQuestions （Liu et al., [2023](#bib.bib20)）：该基准测试类似于商业搜索和问答场景（如 Bing
    Chat）中的检索增强生成设置。具体来说，每个问题在原始提示中都有 20 个相关文档。其中一个包含正确答案，且提示中有五种不同的真实文档位置设置：第 1、第
    5、第 10、第 15 和第 20 位。遵循 Liu et al.（[2023](#bib.bib20)），我们使用准确率作为评估指标。
- en: '(ii) LongBench (Bai et al., [2023](#bib.bib1)): This benchmark consists of
    six task types: single-document QA, multi-document QA, summarization, few-shot
    learning, code completion, and synthetic tasks. We used the English portion that
    covers 16 datasets for evaluation. We use the metrics and scripts provided along
    with the benchmark for evaluation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) LongBench （Bai et al., [2023](#bib.bib1)）：该基准测试包含六种任务类型：单文档 QA、多文档 QA、摘要生成、少量样本学习、代码补全和合成任务。我们使用覆盖
    16 个数据集的英语部分进行评估。我们使用基准测试提供的指标和脚本进行评估。
- en: '(iii) ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)): This benchmark consists
    of four task types: summarization, QA, sentiment classification, and reordering,
    covering 10 datasets. We used the validation set for evaluation. We use the provided
    metrics and scripts for evaluation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) ZeroSCROLLS （Shaham et al., [2023](#bib.bib28)）：该基准测试包含四种任务类型：摘要生成、QA、情感分类和排序，覆盖
    10 个数据集。我们使用验证集进行评估。我们使用提供的指标和脚本进行评估。
- en: Baselines
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准
- en: 'We include two sets of baselines in following experiments:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下实验中包括了两组基准：
- en: '(i) Retrieval-based Methods. We measure the association between the question
    and the documents in the prompt using five SoTA retrieval methods: BM25, Gzip (Jiang
    et al., [2023b](#bib.bib14)), SentenceBERT (Reimers & Gurevych, [2019](#bib.bib27)),
    OpenAI Embedding, and the important metric $r_{k}$ used in LongLLMLingua coarse-grained
    compression. We discard sentences or paragraphs with low association until the
    compression constraint is met while keeping the original document order unchanged.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 基于检索的方法。我们使用五种最先进的检索方法来测量问题与提示中的文档之间的关联性：BM25、Gzip （Jiang et al., [2023b](#bib.bib14)）、SentenceBERT （Reimers
    & Gurevych, [2019](#bib.bib27)）、OpenAI 嵌入以及 LongLLMLingua 粗粒度压缩中使用的重要指标 $r_{k}$。我们丢弃关联性低的句子或段落，直到满足压缩约束，同时保持原始文档顺序不变。
- en: '(ii) Compression-based Methods. We compare our approach with two state-of-art
    methods for prompt compression, i.e., Selective Context (Li, [2023](#bib.bib19))
    and LLMLingua (Jiang et al., [2023a](#bib.bib13)). Both methods employ LLaMA-2-7B-Chat
    as the small language model for compression. In LLMLingua, a coarse-to-fine approach
    is used to handle constraints of compression ratio: the original prompt is first
    compressed to $k$ is the granular control coefficient; token-level is then performed
    to reach the overall constraint. Our method follows the same coarse-to-fine logic
    to achieve the constraint.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 基于压缩的方法。我们将我们的方法与两种最先进的提示压缩方法进行比较，即 Selective Context （Li, [2023](#bib.bib19)）和
    LLMLingua （Jiang et al., [2023a](#bib.bib13)）。这两种方法都使用 LLaMA-2-7B-Chat 作为小型语言模型进行压缩。在
    LLMLingua 中，采用粗到精的方法处理压缩比约束：原始提示首先被压缩到 $k$，这是粒度控制系数；然后进行令牌级别的压缩以达到整体约束。我们的方法遵循相同的粗到精逻辑来实现约束。
- en: '| Methods | GPT3.5-Turbo | LongChat-13b | Length |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GPT3.5-Turbo | LongChat-13b | 长度 |'
- en: '| 1st | 5th | 10th | 15th | 20th | Reorder | 1st | 5th | 10th | 15th | 20th
    | Reorder | Tokens | $1/\tau$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 第 1 | 第 5 | 第 10 | 第 15 | 第 20 | 重新排序 | 第 1 | 第 5 | 第 10 | 第 15 | 第 20 |
    重新排序 | 令牌数 | $1/\tau$ |'
- en: '| 2x constraint |  |  |  |  |  |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2x 约束 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |  |'
- en: '| BM25 | 53.7 | 49.3 | 47.9 | 49.9 | 46.9 | 50.3 | 50.9 | 44.9 | 44.1 | 42.9
    | 43.2 | 46.0 | 1,545 | 1.9x |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 53.7 | 49.3 | 47.9 | 49.9 | 46.9 | 50.3 | 50.9 | 44.9 | 44.1 | 42.9
    | 43.2 | 46.0 | 1,545 | 1.9x |'
- en: '| Gzip | 64.6 | 63.8 | 60.5 | 58.3 | 57.3 | 64.4 | 61.9 | 55.7 | 52.7 | 50.8
    | 50.9 | 59.3 | 1,567 | 1.9x |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Gzip | 64.6 | 63.8 | 60.5 | 58.3 | 57.3 | 64.4 | 61.9 | 55.7 | 52.7 | 50.8
    | 50.9 | 59.3 | 1,567 | 1.9x |'
- en: '| SBERT | 72.5 | 67.9 | 63.3 | 65.0 | 66.2 | 68.7 | 65.8 | 57.5 | 54.9 | 53.4
    | 55.7 | 61.4 | 1,549 | 1.9x |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 72.5 | 67.9 | 63.3 | 65.0 | 66.2 | 68.7 | 65.8 | 57.5 | 54.9 | 53.4
    | 55.7 | 61.4 | 1,549 | 1.9x |'
- en: '| OpenAI | 73.0 | 65.6 | 66.5 | 65.4 | 65.5 | 69.9 | 65.9 | 57.5 | 56.2 | 54.2
    | 55.7 | 61.7 | 1,550 | 1.9x |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 73.0 | 65.6 | 66.5 | 65.4 | 65.5 | 69.9 | 65.9 | 57.5 | 56.2 | 54.2
    | 55.7 | 61.7 | 1,550 | 1.9x |'
- en: '| LongLLMLingua $r_{k}$ | 73.9 | 67.7 | 68.7 | 66.0 | 65.6 | 74.3 | 68.5 |
    59.1 | 56.8 | 55.3 | 56.9 | 65.2 | 1,548 | 1.9x |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 73.9 | 67.7 | 68.7 | 66.0 | 65.6 | 74.3 | 68.5 |
    59.1 | 56.8 | 55.3 | 56.9 | 65.2 | 1,548 | 1.9x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |  |  |'
- en: '| Selective-Context | 45.4 | 39.0 | 33.8 | 33.5 | 41.5 | - | 53.2 | 26.3 |
    25.4 | 24.2 | 33.3 | - | 1,478 | 2.0x |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 选择性上下文 | 45.4 | 39.0 | 33.8 | 33.5 | 41.5 | - | 53.2 | 26.3 | 25.4 | 24.2
    | 33.3 | - | 1,478 | 2.0x |'
- en: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 | 41.5 | 38.7 | 37.3 | 35.7
    | 34.1 | 37.5 | 37.1 | 1,410 | 2.1x |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 | 41.5 | 38.7 | 37.3 | 35.7
    | 34.1 | 37.5 | 37.1 | 1,410 | 2.1x |'
- en: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 | 76.2 | 68.7 | 59.4 | 57.3
    | 55.9 | 58.4 | 66.1 | 1,429 | 2.1x |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 | 76.2 | 68.7 | 59.4 | 57.3
    | 55.9 | 58.4 | 66.1 | 1,429 | 2.1x |'
- en: '| 4x constraint |  |  |  |  |  |  |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 4倍约束 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 40.6 | 38.6 | 38.2 | 37.4 | 36.6 | 36.3 | 39.5 | 37.5 | 36.8 | 36.4
    | 35.5 | 37.7 | 798 | 3.7x |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 40.6 | 38.6 | 38.2 | 37.4 | 36.6 | 36.3 | 39.5 | 37.5 | 36.8 | 36.4
    | 35.5 | 37.7 | 798 | 3.7x |'
- en: '| Gzip | 63.1 | 61.0 | 59.8 | 61.1 | 60.1 | 62.3 | 57.6 | 52.9 | 51.0 | 50.1
    | 50.4 | 57.2 | 824 | 3.6x |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Gzip | 63.1 | 61.0 | 59.8 | 61.1 | 60.1 | 62.3 | 57.6 | 52.9 | 51.0 | 50.1
    | 50.4 | 57.2 | 824 | 3.6x |'
- en: '| SBERT | 66.9 | 61.1 | 59.0 | 61.2 | 60.3 | 64.4 | 62.6 | 56.6 | 55.1 | 53.9
    | 55.0 | 59.1 | 808 | 3.6x |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 66.9 | 61.1 | 59.0 | 61.2 | 60.3 | 64.4 | 62.6 | 56.6 | 55.1 | 53.9
    | 55.0 | 59.1 | 808 | 3.6x |'
- en: '| OpenAI | 63.8 | 64.6 | 65.4 | 64.1 | 63.7 | 63.7 | 61.2 | 56.0 | 55.1 | 54.4
    | 55.0 | 58.8 | 804 | 3.7x |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 63.8 | 64.6 | 65.4 | 64.1 | 63.7 | 63.7 | 61.2 | 56.0 | 55.1 | 54.4
    | 55.0 | 58.8 | 804 | 3.7x |'
- en: '| LongLLMLingua $r_{k}$ | 71.1 | 70.7 | 69.3 | 68.7 | 68.5 | 71.5 | 67.8 |
    59.4 | 57.7 | 57.7 | 58.6 | 64.0 | 807 | 3.7x |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 71.1 | 70.7 | 69.3 | 68.7 | 68.5 | 71.5 | 67.8 |
    59.4 | 57.7 | 57.7 | 58.6 | 64.0 | 807 | 3.7x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |  |  |'
- en: '| Selective-Context | 31.4 | 19.5 | 24.7 | 24.1 | 43.8 | - | 38.2 | 17.2 |
    15.9 | 16.0 | 27.3 | - | 791 | 3.7x |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 选择性上下文 | 31.4 | 19.5 | 24.7 | 24.1 | 43.8 | - | 38.2 | 17.2 | 15.9 | 16.0
    | 27.3 | - | 791 | 3.7x |'
- en: '| LLMLingua | 25.5 | 27.5 | 23.5 | 26.5 | 30.0 | 27.0 | 32.1 | 30.8 | 29.9
    | 28.9 | 32.4 | 30.5 | 775 | 3.8x |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 25.5 | 27.5 | 23.5 | 26.5 | 30.0 | 27.0 | 32.1 | 30.8 | 29.9
    | 28.9 | 32.4 | 30.5 | 775 | 3.8x |'
- en: '| LongLLMLingua | 75.0 | 71.8 | 71.2 | 71.2 | 74.7 | 75.5 | 68.7 | 60.5 | 59.3
    | 58.3 | 61.3 | 66.7 | 748 | 3.9x |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 75.0 | 71.8 | 71.2 | 71.2 | 74.7 | 75.5 | 68.7 | 60.5 | 59.3
    | 58.3 | 61.3 | 66.7 | 748 | 3.9x |'
- en: '| Original Prompt | 75.7 | 57.3 | 54.1 | 55.4 | 63.1 | - | 68.6 | 57.4 | 55.3
    | 52.5 | 55.0 | - | 2,946 | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | 75.7 | 57.3 | 54.1 | 55.4 | 63.1 | - | 68.6 | 57.4 | 55.3 | 52.5 |
    55.0 | - | 2,946 | - |'
- en: '| Zero-shot |  |  | 56.1 |  |  |  |  | 35.0 |  |  | 15 | 196x |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 |  |  | 56.1 |  |  |  |  | 35.0 |  |  | 15 | 196x |'
- en: 'Table 1: Performance of different methods with different compression ratios
    on NaturalQuestions (20 documents) (Liu et al., [2023](#bib.bib20)). Reorder:
    we reorder the documents with relevance metrics of different baselines as our
    document reordering strategy described in Sec. [4.2](#S4.SS2 "4.2 How to reduce
    information loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"). In the
    case of OpenAI, it corresponds to LongContextReorder in the LangChain framework (Chase,
    [2022](#bib.bib4)). For results reported under 1st to 20th, we do not use the
    reordering strategy for all methods.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同压缩比下不同方法在NaturalQuestions（20个文档）上的性能（刘等，[2023](#bib.bib20)）。重排序：我们根据不同基线的相关性指标对文档进行重排序，作为我们在第[4.2](#S4.SS2
    "4.2 如何减少中间的信息丢失？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")节中描述的文档重排序策略。在OpenAI的情况下，它对应于LangChain框架中的LongContextReorder（Chase，[2022](#bib.bib4)）。对于报告的第1到第20名的结果，我们对所有方法不使用重排序策略。
- en: '⁷⁷footnotetext: https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/
    long_context_reorder'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷⁷脚注： https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/
    long_context_reorder
- en: Main Results
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结果
- en: 'Table [1](#S5.T1 "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") and [3](#S5.T3
    "Table 3 ‣ Main Results ‣ 5 Experiments ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression") present the performance
    of various methods under different compression constraints. There are multiple
    observations and conclusions: (1) Our LongLLMLingua achieves the best performance
    across different tasks and constraints of compression ratios. Compared to the
    original prompt, our compressed prompt can derive higher performance with much
    less cost. For example, LongLLMLingua gains a performance boost of 17.1% on NaturalQuestions
    with the ground-true document at the 10th position, while the number of tokens
    input to GPT3.5-Turbo is $\sim$, LongLLMLingua even achieves a little performance
    gain. We mainly owe this to the question-aware coarse-to-fine compression, which
    can better figure out the key information and reach a higher key information density
    with a higher compression rate. (5) The proposed document reordering strategy
    helps in not only our approach but also other baselines as shown in Table [1](#S5.T1
    "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression"), well demonstrating its
    effectiveness.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S5.T1 "表格 1 ‣ 基准 ‣ 5 个实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLM") 和
    [3](#S5.T3 "表格 3 ‣ 主要结果 ‣ 5 个实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLM") 展示了在不同压缩约束下各种方法的性能。观察和结论有多个：（1）我们的
    LongLLMLingua 在不同任务和压缩比约束下表现最佳。与原始提示相比，我们的压缩提示在成本更低的情况下能获得更高的性能。例如，LongLLMLingua
    在第 10 位的实际文档上在 NaturalQuestions 中获得了 17.1% 的性能提升，而输入到 GPT3.5-Turbo 的令牌数为 $\sim$，LongLLMLingua
    甚至获得了略微的性能提升。我们主要归功于问题感知的粗到细压缩，这可以更好地识别关键信息，并在更高的压缩率下达到更高的关键信息密度。（5）提出的文档重新排序策略不仅对我们的方法有效，对其他基准也有效，如表格 [1](#S5.T1
    "表格 1 ‣ 基准 ‣ 5 个实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLM") 所示，充分展示了其有效性。
- en: '|  | 1st | 5th | 10th | 15th | 20th |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 第 1 位 | 第 5 位 | 第 10 位 | 第 15 位 | 第 20 位 |'
- en: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 77.2 | 72.9 | 70.8 | 70.5 | 70.6 |'
- en: '| - w/o Question-aware Coarse-grained | 42.1 | 40.3 | 39.7 | 40.1 | 40.3 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| - 不含问题感知粗粒度 | 42.1 | 40.3 | 39.7 | 40.1 | 40.3 |'
- en: '| - w/ SBERT | 73.2 | 68.5 | 65.7 | 66.1 | 66.7 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| - 含 SBERT | 73.2 | 68.5 | 65.7 | 66.1 | 66.7 |'
- en: '| - w/o Question-aware Fine-grained | 75.8 | 71.0 | 68.9 | 68.4 | 69.3 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| - 不含问题感知细粒度 | 75.8 | 71.0 | 68.9 | 68.4 | 69.3 |'
- en: '| - w/o Dynamic Compression Ratio | 74.4 | 70.7 | 68.7 | 67.9 | 68.1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| - 不含动态压缩比 | 74.4 | 70.7 | 68.7 | 67.9 | 68.1 |'
- en: '| - w/o Subsequence Recovery | 76.7 | 71.7 | 69.4 | 69.3 | 69.7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| - 不含子序列恢复 | 76.7 | 71.7 | 69.4 | 69.3 | 69.7 |'
- en: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 39.7 | 39.5 | 40.4 | 37.1 | 42.3 |'
- en: '| - w/ Subsequence Recovery | 43.8 | 44.1 | 43.5 | 43.3 | 44.4 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| - 含子序列恢复 | 43.8 | 44.1 | 43.5 | 43.3 | 44.4 |'
- en: 'Table 2: Ablation study on NaturalQuestions with 2x constraint using GPT-3.5-Turbo.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 2: 使用 GPT-3.5-Turbo 对 NaturalQuestions 进行的 2x 约束的消融研究。'
- en: '| Methods | LongBench | ZeroSCROLLS |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LongBench | ZeroSCROLLS |'
- en: '| SingleDoc | MultiDoc | Summ. | FewShot | Synth. | Code | AVG | Tokens | $1/\tau$
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SingleDoc | MultiDoc | Summ. | FewShot | Synth. | Code | 平均 | 令牌数 | $1/\tau$
    |'
- en: '| 3,000 tokens constraint |  |  |  |  |  |  |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 3,000 令牌约束 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 32.3 | 34.3 | 25.3 | 57.9 | 45.1 | 48.9 | 40.6 | 3,417 | 3x | 19.8
    | 3,379 | 3x |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 32.3 | 34.3 | 25.3 | 57.9 | 45.1 | 48.9 | 40.6 | 3,417 | 3x | 19.8
    | 3,379 | 3x |'
- en: '| SBERT | 35.3 | 37.4 | 26.7 | 63.4 | 51.0 | 34.5 | 41.4 | 3,399 | 3x | 24.0
    | 3,340 | 3x |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 35.3 | 37.4 | 26.7 | 63.4 | 51.0 | 34.5 | 41.4 | 3,399 | 3x | 24.0
    | 3,340 | 3x |'
- en: '| OpenAI | 34.5 | 38.6 | 26.8 | 63.4 | 49.6 | 37.6 | 41.7 | 3,421 | 3x | 22.4
    | 3,362 | 3x |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 34.5 | 38.6 | 26.8 | 63.4 | 49.6 | 37.6 | 41.7 | 3,421 | 3x | 22.4
    | 3,362 | 3x |'
- en: '| LongLLMLingua $r_{k}$ | 37.6 | 42.9 | 26.9 | 68.2 | 49.9 | 53.4 | 46.5 |
    3,424 | 3x | 29.3 | 3,350 | 3x |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 37.6 | 42.9 | 26.9 | 68.2 | 49.9 | 53.4 | 46.5 |
    3,424 | 3x | 29.3 | 3,350 | 3x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |'
- en: '| Selective-Context | 23.3 | 39.2 | 25.0 | 23.8 | 27.5 | 53.1 | 32.0 | 3,328
    | 3x | 20.7 | 3,460 | 3x |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 选择性上下文 | 23.3 | 39.2 | 25.0 | 23.8 | 27.5 | 53.1 | 32.0 | 3,328 | 3x | 20.7
    | 3,460 | 3x |'
- en: '| LLMLingua | 31.8 | 37.5 | 26.2 | 67.2 | 8.3 | 53.2 | 37.4 | 3,421 | 3x |
    30.7 | 3,366 | 3x |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 31.8 | 37.5 | 26.2 | 67.2 | 8.3 | 53.2 | 37.4 | 3,421 | 3x |
    30.7 | 3,366 | 3x |'
- en: '| LongLLMLingua | 40.7 | 46.2 | 27.2 | 70.6 | 53.0 | 55.2 | 48.8 | 3,283 |
    3x | 32.8 | 3,412 | 3x |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 40.7 | 46.2 | 27.2 | 70.6 | 53.0 | 55.2 | 48.8 | 3,283 |
    3x | 32.8 | 3,412 | 3x |'
- en: '| 2,000 tokens constraint |  |  |  |  |  |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2,000 tokens 限制 |  |  |  |  |  |  |  |'
- en: '| Retrieval-based Methods |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 基于检索的方法 |'
- en: '| BM25 | 30.1 | 29.4 | 21.2 | 19.5 | 12.4 | 29.1 | 23.6 | 1,985 | 5x | 20.1
    | 1,799 | 5x |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 30.1 | 29.4 | 21.2 | 19.5 | 12.4 | 29.1 | 23.6 | 1,985 | 5x | 20.1
    | 1,799 | 5x |'
- en: '| SBERT | 33.8 | 35.9 | 25.9 | 23.5 | 18.0 | 17.8 | 25.8 | 1,947 | 5x | 20.5
    | 1,773 | 6x |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SBERT | 33.8 | 35.9 | 25.9 | 23.5 | 18.0 | 17.8 | 25.8 | 1,947 | 5x | 20.5
    | 1,773 | 6x |'
- en: '| OpenAI | 34.3 | 36.3 | 24.7 | 32.4 | 26.3 | 24.8 | 29.8 | 1,991 | 5x | 20.6
    | 1,784 | 5x |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI | 34.3 | 36.3 | 24.7 | 32.4 | 26.3 | 24.8 | 29.8 | 1,991 | 5x | 20.6
    | 1,784 | 5x |'
- en: '| LongLLMLingua $r_{k}$ | 37.8 | 41.7 | 26.9 | 66.3 | 53.0 | 52.4 | 46.3 |
    1,960 | 5x | 24.9 | 1,771 | 6x |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua $r_{k}$ | 37.8 | 41.7 | 26.9 | 66.3 | 53.0 | 52.4 | 46.3 |
    1,960 | 5x | 24.9 | 1,771 | 6x |'
- en: '| Compression-based Methods |  |  |  |  |  |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |  |  |  |  |  |  |'
- en: '| Selective-Context | 16.2 | 34.8 | 24.4 | 15.7 | 8.4 | 49.2 | 24.8 | 1,925
    | 5x | 19.4 | 1,865 | 5x |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Selective-Context | 16.2 | 34.8 | 24.4 | 15.7 | 8.4 | 49.2 | 24.8 | 1,925
    | 5x | 19.4 | 1,865 | 5x |'
- en: '| LLMLingua | 22.4 | 32.1 | 24.5 | 61.2 | 10.4 | 56.8 | 34.6 | 1,950 | 5x |
    27.2 | 1,862 | 5x |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 22.4 | 32.1 | 24.5 | 61.2 | 10.4 | 56.8 | 34.6 | 1,950 | 5x |
    27.2 | 1,862 | 5x |'
- en: '| LongLLMLingua | 39.0 | 42.2 | 27.4 | 69.3 | 53.8 | 56.6 | 48.0 | 1,809 |
    6x | 32.5 | 1,753 | 6x |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 39.0 | 42.2 | 27.4 | 69.3 | 53.8 | 56.6 | 48.0 | 1,809 |
    6x | 32.5 | 1,753 | 6x |'
- en: '| Original Prompt | 39.7 | 38.7 | 26.5 | 67.0 | 37.8 | 54.2 | 44.0 | 10,295
    | - | 32.5 | 9,788 | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | 39.7 | 38.7 | 26.5 | 67.0 | 37.8 | 54.2 | 44.0 | 10,295 | - | 32.5
    | 9,788 | - |'
- en: '| Zero-shot | 15.6 | 31.3 | 15.6 | 40.7 | 1.6 | 36.2 | 23.5 | 214 | 48x | 10.8
    | 32 | 306x |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot | 15.6 | 31.3 | 15.6 | 40.7 | 1.6 | 36.2 | 23.5 | 214 | 48x | 10.8
    | 32 | 306x |'
- en: 'Table 3: Performance of different methods under different compression ratios
    onLongBench (Bai et al., [2023](#bib.bib1)) and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28))
    using GPT-3.5-Turbo. Considering the dataset structure, we do not use the reordering
    strategy here.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同方法在不同压缩比下的性能表现（Bai 等，[2023](#bib.bib1) 和 ZeroSCROLLS（Shaham 等，[2023](#bib.bib28)）使用
    GPT-3.5-Turbo。考虑到数据集结构，我们在这里不使用重新排序策略。
- en: Ablation Study
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融研究
- en: 'To evaluate the contributions of different components in LongLLMLingua, we
    introduce six variants of it for ablation study: (1) Ours w/o Question-aware Coarse-grained,
    which calculates question-text relevance $r_{k}$. (3) Ours w/o Question-aware
    Fine-grained, which disregards Eq. ([3](#S4.E3 "In Question-Aware Fine-Grained
    Compression ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")) and only applies Iterative Token-level Prompt Compression
    as LLMLingua. (4) Ours w/o Dynamic Compression Ratio, where all documents share
    the same compression ratio in fine-grained compression. (5) Ours w/o and (6) LLMLingua
    w/ Subsequence Recovery, which either removes or adds the post-processing subsequence
    recovery strategy.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 LongLLMLingua 中不同组件的贡献，我们介绍了其六种变体进行消融研究：(1) Ours w/o Question-aware Coarse-grained，计算问题-文本相关性
    $r_{k}$。(3) Ours w/o Question-aware Fine-grained，忽略 Eq. ([3](#S4.E3 "在问题感知细粒度压缩中
    ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强 LLM 在长上下文场景中的表现"))，仅应用迭代令牌级提示压缩作为
    LLMLingua。(4) Ours w/o Dynamic Compression Ratio，所有文档在细粒度压缩中使用相同的压缩比。(5) Ours
    w/o 和 (6) LLMLingua w/ Subsequence Recovery，移除或添加后处理子序列恢复策略。
- en: 'Table [2](#S5.T2 "Table 2 ‣ Main Results ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") shows the
    results of the ablation study. In summary, removing any component proposed for
    LongLLMLingua will lead to a performance drop regardless of the position of the
    ground-truth answer. This well validates the necessity and effectiveness of the
    proposed question-aware mechanism during coarse-to-fine compression, the dynamic
    compression ratio, and the subsequence recovery strategy. It also shows that applying
    SBERT for coarse-grained compression will result in inferior performance, which
    implies the superiority of our question-aware importance metric in Eq. [2](#S4.E2
    "In Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve key information
    density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression") over SBERT. Moreover,
    our subsequence recovery strategy can also bring performance gains for LLMLingua.
    However, without our question-aware mechanism, results from LLMLingua are still
    less satisfactory. For more detailed cases, please go to Appendix [C](#A3 "Appendix
    C Ablation Analysis ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
    Scenarios via Prompt Compression").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S5.T2 "表 2 ‣ 主要结果 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLMs")
    显示了消融研究的结果。总之，无论真实答案的位置如何，删除 LongLLMLingua 中提出的任何组件都会导致性能下降。这很好地验证了在粗到细压缩过程中提出的基于问题的机制、动态压缩比和子序列恢复策略的必要性和有效性。它还表明，使用
    SBERT 进行粗粒度压缩会导致性能下降，这暗示了我们在 Eq. [2](#S4.E2 "在基于问题的粗粒度压缩中 ‣ 4.1 如何提高提示中的关键信息密度？
    ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLMs") 中的基于问题的重要性指标优于 SBERT。此外，我们的子序列恢复策略也能为
    LLMLingua 带来性能提升。然而，如果没有我们的基于问题的机制，LLMLingua 的结果仍然不尽如人意。有关更多详细情况，请参见附录 [C](#A3
    "附录 C 消融分析 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLMs")。
- en: Latency Evaluation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 延迟评估
- en: '| $1/\tau$ | 2x | 5x | 10x |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $1/\tau$ | 2x | 5x | 10x |'
- en: '| --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| E2E w/o Compression | 15.6 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| E2E 无压缩 | 15.6 |'
- en: '| --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| E2E w/ LLMLingua | 10.5 (1.5x) | 6.0 (2.6x) | 3.9 (4.0x) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| E2E 使用 LLMLingua | 10.5 (1.5x) | 6.0 (2.6x) | 3.9 (4.0x) |'
- en: '| E2E w/ LongLLMLingua | 11.4 (1.4x) | 6.3 (2.5x) | 4.1 (3.8x) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| E2E 使用 LongLLMLingua | 11.4 (1.4x) | 6.3 (2.5x) | 4.1 (3.8x) |'
- en: '| LongLLMLingua | 2.9 | 1.6 | 1.2 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | 2.9 | 1.6 | 1.2 |'
- en: 'Figure 5: Latency (s) on LongBench.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LongBench 上的延迟（秒）。
- en: 'We conduct testing on a V100-32G GPU, using the prompts from LongBench with
    $\sim$10K tokens on average and setting the response length to 200 tokens in the
    API call. In Table [5](#S5.F5 "Figure 5 ‣ Latency Evaluation ‣ 5 Experiments ‣
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression"), E2E denotes the latency from both the prompt compression system
    and the black-box API, while LongLLMLingua denotes the prompt compression latency
    only. It is shown that our prompt compression system does accelerate the overall
    inference. As the compression rate increases, the acceleration effect becomes
    more pronounced. It is worth mentioning that in scenarios with longer API cost
    time, the actual absolute time saved by LongLLMLingua can be more significant.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 V100-32G GPU 上进行测试，使用 LongBench 的提示，平均约为 $\sim$10K 个 token，并将响应长度设置为 200
    个 token。在表格 [5](#S5.F5 "图 5 ‣ 延迟评估 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的
    LLMs") 中，E2E 表示来自提示压缩系统和黑箱 API 的延迟，而 LongLLMLingua 仅表示提示压缩的延迟。结果显示，我们的提示压缩系统确实加速了整体推理。随着压缩率的增加，加速效果变得更加明显。值得一提的是，在
    API 花费时间较长的场景中，LongLLMLingua 实际节省的绝对时间可能更为显著。
- en: 6 Related Works
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: 'Long Context for LLMs. Recent research has focused on expanding the window
    size of LLMs. Main approaches include: (1) Staged pre-training (Nijkamp et al.,
    [2023](#bib.bib23)) which gradually increases the context window; (2) Modifying (Press
    et al., [2022](#bib.bib26)) or interpolating position embeddings (Chen et al.,
    [2023](#bib.bib5); Peng et al., [2023](#bib.bib25); Han et al., [2023](#bib.bib11));
    (3) Using linear or sparse attention mechanisms (Ding et al., [2023](#bib.bib7);
    Sun et al., [2023](#bib.bib30)); (4) Utilizing external memory modules for context
    storage (Bertsch et al., [2023](#bib.bib2); Tworkowski et al., [2023](#bib.bib31)).
    While these methods address context window expansion, their impact on downstream
    task performance has yet to be discussed.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的长上下文。最近的研究集中于扩大 LLMs 的窗口大小。主要方法包括：(1) 分阶段预训练（Nijkamp et al., [2023](#bib.bib23)），该方法逐渐增加上下文窗口；(2)
    修改（Press et al., [2022](#bib.bib26)）或插值位置嵌入（Chen et al., [2023](#bib.bib5); Peng
    et al., [2023](#bib.bib25); Han et al., [2023](#bib.bib11)）；(3) 使用线性或稀疏注意力机制（Ding
    et al., [2023](#bib.bib7); Sun et al., [2023](#bib.bib30)）；(4) 利用外部记忆模块进行上下文存储（Bertsch
    et al., [2023](#bib.bib2); Tworkowski et al., [2023](#bib.bib31)）。尽管这些方法解决了上下文窗口扩展的问题，但它们对下游任务性能的影响尚待讨论。
- en: Information Distribution in Prompt. Recent empirical experiments have shown
    that LLM performance decreases with less effective information in a prompt (Bai
    et al., [2023](#bib.bib1); Li et al., [2023](#bib.bib18); Shi et al., [2023](#bib.bib29)).
    Moreover, the position of relevant information in a prompt has a significant impact
    on performance(Wu et al., [2022](#bib.bib32)). Liu et al. ([2023](#bib.bib20))
    suggests that LLMs have more difficulty comprehending information located in the
    middle of a prompt compared to those at the edges.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 提示中的信息分布。最近的实证实验表明，当提示中的信息不够有效时，LLM 的性能会下降（Bai et al., [2023](#bib.bib1); Li
    et al., [2023](#bib.bib18); Shi et al., [2023](#bib.bib29)）。此外，提示中相关信息的位置对性能有显著影响（Wu
    et al., [2022](#bib.bib32)）。Liu et al. ([2023](#bib.bib20)) 认为，与提示边缘的信息相比，LLMs
    更难理解位于提示中间的信息。
- en: Retrieval Methods can be categorized as dense or sparse retrieval methods. Sparse
    retrieval methods, like BM25, determine the relevance between queries and documents
    based on n-gram information. Conversely, dense retrieval methods assess the relevance
    between queries and documents in latent space using dense vectors, such as SentenceBERT (Reimers
    & Gurevych, [2019](#bib.bib27)) and OpenAI Embedding. Recently, Jiang et al. ([2023b](#bib.bib14)))
    proposed an unsupervised dense retrieval method that leverages traditional compression
    algorithms, such as gzip, and k-nearest neighbors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 检索方法可以分为密集检索和稀疏检索方法。稀疏检索方法，如 BM25，根据 n-gram 信息确定查询和文档之间的相关性。相反，密集检索方法使用密集向量在潜在空间中评估查询和文档之间的相关性，如
    SentenceBERT（Reimers & Gurevych, [2019](#bib.bib27)）和 OpenAI Embedding。最近，Jiang
    et al. ([2023b](#bib.bib14))) 提出了一个无监督密集检索方法，利用传统的压缩算法（如 gzip）和 k 最近邻。
- en: 'Prompt Compression Methods can be grouped into three main categories: (1) Token
    pruning (Goyal et al., [2020](#bib.bib10); Kim & Cho, [2021](#bib.bib15); Modarressi
    et al., [2022](#bib.bib21)) and token merging (Bolya et al., [2023](#bib.bib3)),
    which need model fine-tuning or intermediate results during inference and have
    been used with BERT-scale models. (2) Soft prompt tuning methods like GIST (Mu
    et al., [2023](#bib.bib22)), AutoCompressor (Chevalier et al., [2023](#bib.bib6)),
    and ICAE (Ge et al., [2023](#bib.bib9)), which require LLMs’ parameter fine-tuning,
    making them suitable for specific domains but not directly applicable to black-box
    LLMs. (3) Information-entropy-based approaches such as Selective Context (Li,
    [2023](#bib.bib19)) and LLMLingua (Jiang et al., [2023a](#bib.bib13)), which use
    a small language model to calculate the self-information or perplexity of each
    token in the original prompt and then remove tokens with lower perplexities.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 提示压缩方法可以分为三大类：(1) 令牌修剪（Goyal et al., [2020](#bib.bib10); Kim & Cho, [2021](#bib.bib15);
    Modarressi et al., [2022](#bib.bib21)）和令牌合并（Bolya et al., [2023](#bib.bib3)），这些方法需要模型微调或在推理过程中得到中间结果，并且已在
    BERT 规模的模型中使用。(2) 像 GIST（Mu et al., [2023](#bib.bib22)）、AutoCompressor（Chevalier
    et al., [2023](#bib.bib6)）和 ICAE（Ge et al., [2023](#bib.bib9)）这样的软提示调整方法，这些方法需要对
    LLMs 的参数进行微调，使其适用于特定领域，但不适用于黑箱 LLMs。(3) 基于信息熵的方法，如 Selective Context（Li, [2023](#bib.bib19)）和
    LLMLingua（Jiang et al., [2023a](#bib.bib13)），这些方法使用小型语言模型计算原始提示中每个令牌的自信息或困惑度，然后删除困惑度较低的令牌。
- en: 7 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We propose LongLLMLingua to address the three challenges, i.e., higher computational/financial
    cost, longer system latency, and inferior performance for LLMs in long context
    scenarios. We develop LongLLMLingua from the perspective of efficient prompt compression,
    thus reducing both computational/financial cost and the system latency. We further
    design four components, i.e., a question-aware coarse-to-fine compression method,
    a document reordering mechanism, dynamic compression ratios, and a post-compression
    subsequence recovery strategy to improve LLMs’ perception of the key information,
    with which LongLLMLingua demonstrate superior performance. Experiments on one
    multi-document QA benchmark and two long context benchmarks demonstrate that LongLLMLingua
    compressed prompt can derive higher performance than original prompts while both
    API costs for inference and the end-to-end system latency are largely reduced.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LongLLMLingua 来应对三大挑战，即更高的计算/财务成本、更长的系统延迟，以及在长上下文场景下 LLM 的较差性能。我们从高效提示压缩的角度开发
    LongLLMLingua，从而减少计算/财务成本和系统延迟。我们进一步设计了四个组件，即具有问题感知的粗到细压缩方法、文档重排序机制、动态压缩比，以及后压缩子序列恢复策略，以提高
    LLM 对关键信息的感知，从而使 LongLLMLingua 展现出卓越的性能。对一个多文档 QA 基准和两个长上下文基准的实验表明，LongLLMLingua
    压缩提示可以在提高性能的同时，大幅降低 API 推理成本和端到端系统延迟。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench:
    A bilingual, multitask benchmark for long context understanding. *ArXiv preprint*,
    abs/2308.14508, 2023. URL [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等人 (2023) 白宇狮、吕鑫、张佳杰、吕宏昌、唐建凯、黄智点、杜郑霄、刘晓、曾敖涵、侯雷等人。Longbench：用于长上下文理解的双语、多任务基准。*ArXiv
    预印本*，abs/2308.14508，2023年。网址 [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508)。
- en: 'Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. Unlimiformer: Long-range transformers with unlimited length input. *ArXiv
    preprint*, abs/2305.01625, 2023. URL [https://arxiv.org/abs/2305.01625](https://arxiv.org/abs/2305.01625).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsch 等人 (2023) Amanda Bertsch、Uri Alon、Graham Neubig、和Matthew R Gormley。Unlimiformer：具有无限长度输入的长程变换器。*ArXiv
    预印本*，abs/2305.01625，2023年。网址 [https://arxiv.org/abs/2305.01625](https://arxiv.org/abs/2305.01625)。
- en: 'Bolya et al. (2023) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
    Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster.
    In *The Eleventh International Conference on Learning Representations*, 2023.
    URL [https://openreview.net/forum?id=JroZRaRw7Eu](https://openreview.net/forum?id=JroZRaRw7Eu).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolya 等人 (2023) Daniel Bolya、Cheng-Yang Fu、Dai Xiaoliang、Peizhao Zhang、Christoph
    Feichtenhofer、和Judy Hoffman。Token merging：你的 vit 但更快。在*第十一届国际学习表示会议*，2023年。网址
    [https://openreview.net/forum?id=JroZRaRw7Eu](https://openreview.net/forum?id=JroZRaRw7Eu)。
- en: Chase (2022) Harrison Chase. LangChain, 2022. URL [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase (2022) Harrison Chase。LangChain，2022年。网址 [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain)。
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *ArXiv preprint*, abs/2306.15595, 2023. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 (2023) 说远 陈、Sherman Wong、梁剑 陈、和袁东 田。通过位置插值扩展大语言模型的上下文窗口。*ArXiv 预印本*，abs/2306.15595，2023年。网址
    [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595)。
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. Adapting language models to compress contexts. *ArXiv preprint*, abs/2305.14788,
    2023. URL [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier 等人 (2023) Alexis Chevalier、Alexander Wettig、Anirudh Ajith、和Danqi Chen。调整语言模型以压缩上下文。*ArXiv
    预印本*，abs/2305.14788，2023年。网址 [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788)。
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. *ArXiv preprint*, abs/2307.02486, 2023. URL [https://arxiv.org/abs/2307.02486](https://arxiv.org/abs/2307.02486).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丁等人 (2023) 丁佳宇、马书明、董丽、张星星、黄绍瀚、王文辉、和魏富如。Longnet：将变换器扩展到 1,000,000,000 个标记。*ArXiv
    预印本*，abs/2307.02486，2023年。网址 [https://arxiv.org/abs/2307.02486](https://arxiv.org/abs/2307.02486)。
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.
    *ArXiv preprint*, abs/2301.00234, 2023. URL [https://arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, 和 Zhifang Sui. 上下文学习综述。*ArXiv 预印本*，abs/2301.00234，2023年。网址
    [https://arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234)。
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context
    autoencoder for context compression in a large language model. *ArXiv preprint*,
    abs/2307.06945, 2023. URL [https://arxiv.org/abs/2307.06945](https://arxiv.org/abs/2307.06945).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, 和 Furu Wei. 大型语言模型中的上下文压缩自编码器。*ArXiv
    预印本*，abs/2307.06945，2023年。网址 [https://arxiv.org/abs/2307.06945](https://arxiv.org/abs/2307.06945)。
- en: 'Goyal et al. (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan T.
    Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating BERT
    inference via progressive word-vector elimination. In *Proceedings of the 37th
    International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
    Event*, volume 119 of *Proceedings of Machine Learning Research*, pp.  3690–3699\.
    PMLR, 2020. URL [http://proceedings.mlr.press/v119/goyal20a.html](http://proceedings.mlr.press/v119/goyal20a.html).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal et al. (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan
    T. Chakaravarthy, Yogish Sabharwal, 和 Ashish Verma. Power-bert: 通过渐进式词向量消除加速 BERT
    推断。收录于 *第37届国际机器学习大会论文集，ICML 2020，2020年7月13-18日，虚拟会议*，*机器学习研究论文集*第119卷，第3690–3699页。PMLR，2020年。网址
    [http://proceedings.mlr.press/v119/goyal20a.html](http://proceedings.mlr.press/v119/goyal20a.html)。'
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language
    models. *ArXiv preprint*, abs/2308.16137, 2023. URL [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong
    Wang. Lm-infinite: 大型语言模型的即时长度泛化。*ArXiv 预印本*，abs/2308.16137，2023年。网址 [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137)。'
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense
    information retrieval with contrastive learning. *Transactions on Machine Learning
    Research*, 2022. ISSN 2835-8856. URL [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, 和 Edouard Grave. 无监督的密集信息检索与对比学习。*机器学习研究期刊*，2022年。ISSN
    2835-8856。网址 [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0)。
- en: 'Jiang et al. (2023a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. In *Proceedings of the 2023 Conference on Empirical Methods in
    Natural Language Processing*. Association for Computational Linguistics, December
    2023a. URL [https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    和 Lili Qiu. Llmlingua: 压缩提示以加速大型语言模型的推断。收录于 *2023年自然语言处理经验方法会议论文集*。计算语言学协会，2023年12月。网址
    [https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736)。'
- en: 'Jiang et al. (2023b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
    Tang, Yiqin Dai, and Jimmy Lin. “low-resource” text classification: A parameter-free
    classification method with compressors. In *Findings of the Association for Computational
    Linguistics: ACL 2023*, pp.  6810–6828, Toronto, Canada, 2023b. Association for
    Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.426. URL [https://aclanthology.org/2023.findings-acl.426](https://aclanthology.org/2023.findings-acl.426).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
    Tang, Yiqin Dai, 和 Jimmy Lin. “低资源”文本分类：一种无参数的压缩器分类方法。收录于 *计算语言学协会会议论文集：ACL 2023*，第6810–6828页，加拿大多伦多，2023年。计算语言学协会。doi:
    10.18653/v1/2023.findings-acl.426。网址 [https://aclanthology.org/2023.findings-acl.426](https://aclanthology.org/2023.findings-acl.426)。'
- en: 'Kim & Cho (2021) Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer:
    Train once with length drop, use anytime with search. In *Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pp. 
    6501–6511, Online, 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.508.
    URL [https://aclanthology.org/2021.acl-long.508](https://aclanthology.org/2021.acl-long.508).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim & Cho (2021) Gyuwan Kim 和 Kyunghyun Cho. 长度自适应变压器：一次训练，随时使用并带有搜索功能。在 *第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长篇论文）*
    中，第 6501–6511 页，在线，2021年。计算语言学协会。doi: 10.18653/v1/2021.acl-long.508。网址 [https://aclanthology.org/2021.acl-long.508](https://aclanthology.org/2021.acl-long.508)。'
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions:
    A benchmark for question answering research. *Transactions of the Association
    for Computational Linguistics*, 7:452–466, 2019. doi: 10.1162/tacl˙a˙00276. URL
    [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, 和 Slav Petrov. 自然问题：问答研究的基准。*计算语言学协会会刊*，7:452–466，2019年。doi:
    10.1162/tacl˙a˙00276。网址 [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026)。'
- en: 'Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation
    for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
    Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, 和 Douwe Kiela. 检索增强生成用于知识密集型 NLP 任务。在
    Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, 和 Hsuan-Tien
    Lin（编辑），*神经信息处理系统进展 33：2020 年神经信息处理系统年度会议，NeurIPS 2020，2020年12月6-12日，虚拟*，2020年。网址
    [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)。
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length?, 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 开源 llms 在上下文长度上的真正承诺有多长？2023年。网址
    [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat)。
- en: 'Li (2023) Yucheng Li. Unlocking context constraints of llms: Enhancing context
    efficiency of llms with self-information-based content filtering. *ArXiv preprint*,
    abs/2304.12102, 2023. URL [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li (2023) Yucheng Li. 解锁 llms 的上下文限制：通过基于自我信息的内容过滤提高 llms 的上下文效率。*ArXiv 预印本*，abs/2304.12102，2023年。网址
    [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102)。
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models
    use long contexts. *ArXiv preprint*, abs/2307.03172, 2023. URL [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 迷失在中间：语言模型如何使用长上下文。*ArXiv 预印本*，abs/2307.03172，2023年。网址
    [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)。
- en: 'Modarressi et al. (2022) Ali Modarressi, Hosein Mohebbi, and Mohammad Taher
    Pilehvar. AdapLeR: Speeding up inference by adaptive length reduction. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pp.  1–15, Dublin, Ireland, 2022\. Association for Computational
    Linguistics. doi: 10.18653/v1/2022.acl-long.1. URL [https://aclanthology.org/2022.acl-long.1](https://aclanthology.org/2022.acl-long.1).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Modarressi 等（2022）Ali Modarressi, Hosein Mohebbi 和 Mohammad Taher Pilehvar。AdapLeR：通过自适应长度缩减加速推断。在
    *第60届计算语言学协会年会（第1卷：长篇论文）*，第1–15页，爱尔兰都柏林，2022年。计算语言学协会。doi: 10.18653/v1/2022.acl-long.1。网址
    [https://aclanthology.org/2022.acl-long.1](https://aclanthology.org/2022.acl-long.1)。'
- en: Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress
    prompts with gist tokens. *ArXiv preprint*, abs/2304.08467, 2023. URL [https://arxiv.org/abs/2304.08467](https://arxiv.org/abs/2304.08467).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等（2023）Jesse Mu, Xiang Lisa Li 和 Noah Goodman。学习使用 gist tokens 压缩提示。*ArXiv
    预印本*，abs/2304.08467，2023年。网址 [https://arxiv.org/abs/2304.08467](https://arxiv.org/abs/2304.08467)。
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report. *ArXiv
    preprint*, abs/2309.03450, 2023. URL [https://arxiv.org/abs/2309.03450](https://arxiv.org/abs/2309.03450).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等（2023）Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia,
    Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam,
    Tong Niu, Wojciech Kryściński, Lidiya Murakhovs’ka, Prafulla Kumar Choubey, Alex
    Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese,
    Yingbo Zhou, Shafiq Joty 和 Caiming Xiong. Xgen-7b 技术报告。*ArXiv 预印本*，abs/2309.03450，2023年。网址
    [https://arxiv.org/abs/2309.03450](https://arxiv.org/abs/2309.03450)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. *ArXiv preprint*, abs/2304.03442, 2023. URL [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2023）Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S Bernstein。生成代理：人类行为的互动仿真体。*ArXiv 预印本*，abs/2304.03442，2023年。网址
    [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442)。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models. *ArXiv preprint*,
    abs/2309.00071, 2023. URL [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023）Bowen Peng, Jeffrey Quesnelle, Honglu Fan 和 Enrico Shippole。Yarn：高效的大型语言模型上下文窗口扩展。*ArXiv
    预印本*，abs/2309.00071，2023年。网址 [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071)。
- en: 'Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test
    long: Attention with linear biases enables input length extrapolation. In *International
    Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等（2022）Ofir Press, Noah Smith 和 Mike Lewis。训练短，测试长：具有线性偏差的注意力机制实现输入长度外推。在
    *国际学习表示会议*，2022年。网址 [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0)。
- en: 'Reimers & Gurevych (2019) Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence
    embeddings using Siamese BERT-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp.  3982–3992,
    Hong Kong, China, 2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410.
    URL [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reimers & Gurevych（2019）Nils Reimers 和 Iryna Gurevych。Sentence-BERT：使用 Siamese
    BERT 网络的句子嵌入。在 *2019年自然语言处理经验方法会议暨第9届国际自然语言处理联合会议（EMNLP-IJCNLP）*，第3982–3992页，中国香港，2019年。计算语言学协会。doi:
    10.18653/v1/D19-1410。网址 [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410)。'
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. *ArXiv
    preprint*, abs/2305.14196, 2023. URL [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaham 等（2023）Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant 和 Omer Levy。Zeroscrolls：一个用于长文本理解的零样本基准。*ArXiv
    预印本*，abs/2305.14196，2023年。网址 [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196)。
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can
    be easily distracted by irrelevant context. In *International Conference on Machine
    Learning*, pp.  31210–31227\. PMLR, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等（2023）Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan,
    Ed H Chi, Nathanael Schärli, 和 Denny Zhou。**大型语言模型容易被无关上下文分心**。在 *国际机器学习大会*，第
    31210–31227 页。PMLR，2023。
- en: 'Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia,
    Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer
    for large language models. *ArXiv preprint*, abs/2307.08621, 2023. URL [https://arxiv.org/abs/2307.08621](https://arxiv.org/abs/2307.08621).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang, 和 Furu Wei。**保留网络**：大语言模型的变换器继任者。*ArXiv 预印本*，abs/2307.08621，2023。网址
    [https://arxiv.org/abs/2307.08621](https://arxiv.org/abs/2307.08621)。
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive
    training for context scaling. *ArXiv preprint*, abs/2307.03170, 2023. URL [https://arxiv.org/abs/2307.03170](https://arxiv.org/abs/2307.03170).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski 等（2023）Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai
    Wu, Henryk Michalewski, 和 Piotr Miłoś。**集中型变换器**：用于上下文扩展的对比训练。*ArXiv 预印本*，abs/2307.03170，2023。网址
    [https://arxiv.org/abs/2307.03170](https://arxiv.org/abs/2307.03170)。
- en: 'Wu et al. (2022) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    Self-adaptive in-context learning: An information compression perspective for
    in-context example selection and ordering. *ArXiv preprint*, abs/2212.10375, 2022.
    URL [https://arxiv.org/abs/2212.10375](https://arxiv.org/abs/2212.10375).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2022）Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, 和 Lingpeng Kong。**自适应上下文学习**：基于信息压缩的上下文示例选择与排序。*ArXiv
    预印本*，abs/2212.10375，2022。网址 [https://arxiv.org/abs/2212.10375](https://arxiv.org/abs/2212.10375)。
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela
    Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective
    long-context scaling of foundation models. *ArXiv preprint*, abs/2309.16039, 2023.
    URL [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等（2023）Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava,
    Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
    Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan,
    Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, 和 Hao Ma。**基础模型的有效长上下文扩展**。*ArXiv
    预印本*，abs/2309.16039，2023。网址 [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039)。
- en: Appendix A Token-level Subsquence Recovery Details
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 令牌级子序列恢复详细信息
- en: Algorithm 1 Pseudo code of Token-level Subsquence Recovery.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 令牌级子序列恢复伪代码。
- en: 'Input: The original prompt $\bm{x}$.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：原始提示 $\bm{x}$。
- en: 1:Set the final response list $\bm{y}_{\text{rec}}=\phi$.11:     end if12:end while
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 1：设置最终响应列表 $\bm{y}_{\text{rec}}=\phi$。11：     结束 if12：结束 while
- en: 'Output: The final response list $\bm{y}_{\text{rec}}$.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：最终响应列表 $\bm{y}_{\text{rec}}$。
- en: Appendix B Experiment Details
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实验详细信息
- en: B.1 Dataset Details
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 数据集详细信息
- en: NaturalQuestions Multi-document QA
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NaturalQuestions 多文档问答
- en: 'A multi-document question-answering dataset, comprising 2,655 problems, was
    built by Liu et al. ([2023](#bib.bib20)) based on the NaturalQuestions dataset (Kwiatkowski
    et al., [2019](#bib.bib16)). This dataset provides a realistic retrieval-augmented
    generation setup that closely resembles commercial search and question-answering
    applications (e.g., Bing Chat). Each example in the dataset contains a question
    and k related documents, utilizing the Contriever retrieval system (Izacard et al.,
    [2022](#bib.bib12)), one of which includes a document with the correct answer.
    To perform this task, the model must access the document containing the answer
    within its input context and use it to answer the question. The dataset’s data
    is sourced from the NaturalQuestions dataset, which contains historical queries
    issued to the Google search engine and human-annotated answers extracted from
    Wikipedia. The average prompt token length in this benchmark is 2,946\. For our
    experiments, we used the version provided by Liu et al. ([2023](#bib.bib20)) that
    includes 20 documents⁸⁸8https://github.com/nelson-liu/lost-in-the-middle. The
    dataset comprises five different ground truth document position settings in the
    prompt: 1st, 5th, 10th, 15th, and 20th.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多文档问答数据集，由Liu等人（[2023](#bib.bib20)）基于NaturalQuestions数据集（Kwiatkowski等人，[2019](#bib.bib16)）构建。该数据集提供了一个逼真的检索增强生成设置，类似于商业搜索和问答应用（例如Bing
    Chat）。数据集中的每个示例包含一个问题和k个相关文档，利用Contriever检索系统（Izacard等人，[2022](#bib.bib12)），其中一个文档包含正确答案。为了完成这个任务，模型必须在其输入上下文中访问包含答案的文档，并使用它来回答问题。数据集的数据来源于NaturalQuestions数据集，该数据集包含对Google搜索引擎的历史查询和从维基百科中提取的人工标注答案。该基准测试中的平均提示令牌长度为2,946。对于我们的实验，我们使用了Liu等人（[2023](#bib.bib20)）提供的包含20个文档的版本⁸⁸8https://github.com/nelson-liu/lost-in-the-middle。数据集包含提示中的五种不同的真实文档位置设置：第1、5、10、15和20位置。
- en: LongBench
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LongBench
- en: A multi-task long context benchmark consists of 3,750 problems in English and
    includes six categories with a total of 16 tasks. These tasks encompass key long-text
    application scenarios, such as single-document QA, multi-document QA, summarization,
    few-shot learning, synthetic tasks, and code completion. The average prompt token
    length in this benchmark is 10,289\. For our experiments, we used the English
    dataset and evaluation scripts provided by Bai et al. ([2023](#bib.bib1)) for
    this benchmark⁹⁹9https://github.com/THUDM/LongBench.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多任务长文本基准测试包含3,750个英语问题，涵盖六个类别，共16个任务。这些任务包括关键的长文本应用场景，如单文档问答、多文档问答、总结、少样本学习、合成任务和代码补全。该基准测试中的平均提示令牌长度为10,289。对于我们的实验，我们使用了Bai等人提供的英文数据集和评估脚本（[2023](#bib.bib1)）来处理这个基准⁹⁹9https://github.com/THUDM/LongBench。
- en: ZeroSCROLLS
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ZeroSCROLLS
- en: The multi-task long context benchmark consists of 4,378 problems, including
    four categories with a total of 10 tasks. These tasks cover summarization, question
    answering, aggregated sentiment classification, and information reordering. The
    average prompt token length in this benchmark is 9,788\. For our experiments,
    we used the validation set and evaluation scripts provided by Shaham et al. ([2023](#bib.bib28))
    for this dataset^(10)^(10)10https://www.zero.scrolls-benchmark.com/.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务长文本基准测试包含4,378个问题，涵盖四个类别，共10个任务。这些任务包括总结、问答、聚合情感分类和信息重排序。该基准测试中的平均提示令牌长度为9,788。对于我们的实验，我们使用了Shaham等人提供的验证集和评估脚本（[2023](#bib.bib28)）来处理这个数据集^(10)^(10)10https://www.zero.scrolls-benchmark.com/。
- en: B.2 Other Implementation Details
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 其他实现细节
- en: 'All experiments were conducted using a Tesla V100 (32GB). We use tiktoken^(11)^(11)11https://github.com/openai/tiktoken
    and GPT-3.5-Turbo model to count all the tokens. We set the granular control coefficient
    $k$ used in dynamic compression ratio is set to 0.25. For a fair comparison, we
    only used reordering in the NaturalQuestions Multi-document QA and noted this
    in Table [1](#S5.T1 "Table 1 ‣ Baselines ‣ 5 Experiments ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"). We use
    “We can get the answer to this question in the given documents.” as the guideline
    sentence in Equation ([3](#S4.E3 "In Question-Aware Fine-Grained Compression ‣
    4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua ‣
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression")).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均使用 Tesla V100 (32GB) 进行。我们使用 tiktoken^(11)^(11)11https://github.com/openai/tiktoken
    和 GPT-3.5-Turbo 模型来计算所有的令牌。我们将动态压缩比中使用的细粒度控制系数 $k$ 设置为 0.25。为了公平比较，我们仅在 NaturalQuestions
    多文档 QA 中使用了重新排序，并在表 [1](#S5.T1 "表 1 ‣ 基线 ‣ 5 实验 ‣ LongLLMLingua：通过提示压缩加速和增强 LLMs
    在长上下文场景中的应用") 中做了说明。我们在方程式 ([3](#S4.E3 "在 Question-Aware Fine-Grained Compression
    ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强 LLMs 在长上下文场景中的应用"))
    中使用“我们可以在给定的文档中找到这个问题的答案。”作为指导句。
- en: For the baselines experiment, we use the currently recommended strongest model,
    all-mpnet-base-v2^(12)^(12)12https://www.sbert.net/docs/pretrained_models.html,
    as the dense representation model for SentenceBERT. We use the recommended “text-embedding-ada-002”
    as the embedding model for OpenAI Embedding^(13)^(13)13https://platform.openai.com/docs/guides/embeddings/.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线实验，我们使用当前推荐的最强模型 all-mpnet-base-v2^(12)^(12)12https://www.sbert.net/docs/pretrained_models.html，作为
    SentenceBERT 的稠密表示模型。我们使用推荐的“text-embedding-ada-002”作为 OpenAI Embedding 的嵌入模型^(13)^(13)13https://platform.openai.com/docs/guides/embeddings/。
- en: Appendix C Ablation Analysis
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 消融分析
- en: '<svg id="A3.F6.pic1" class="ltx_picture" height="491.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,491.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 298.92)"><foreignobject width="556.69"
    height="178.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="261.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to <math id="A3.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math
    ltx_framed ltx_framed_rectangle" display="block"></math>, of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen</foreignobject></g></g></svg>'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A3.F6.pic1" class="ltx_picture" height="491.04" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,491.04) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 298.92)"><foreignobject width="556.69"
    height="178.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="261.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to <math id="A3.F6.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math
    ltx_framed ltx_framed_rectangle" display="block"></math>, of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen</foreignobject></g></g></svg>'
- en: 'Figure 6: Comparing the compressed prompt and LLMs’ response before and after
    using Question-aware Fine-grained Compression and Subsequence Recovery($1/\tau$=30x,
    high compression ratio setting) from NaturalQuestions Multi-document QA (Liu et al.,
    [2023](#bib.bib20)) using GPT-3.5-Turbo.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：比较压缩提示和 LLMs 在使用 Question-aware Fine-grained Compression 和 Subsequence Recovery（$1/\tau$=30x，高压缩比设置）前后的响应，基于
    NaturalQuestions 多文档 QA（Liu 等，[2023](#bib.bib20)）使用 GPT-3.5-Turbo。
- en: Appendix D Economic Cost
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 经济成本
- en: '|  | Multi-document QA | LongBench | ZeroScolls |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 多文档 QA | LongBench | ZeroScrolls |'
- en: '| --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Original | 4.6 | 31.5 | 30.6 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 4.6 | 31.5 | 30.6 |'
- en: '| Ours | 1.3 | 3.0 | 3.2 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 1.3 | 3.0 | 3.2 |'
- en: 'Figure 7: The inference costs(per 1,000 samples $) for various datasets using
    GPT-3.5-Turbo.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用 GPT-3.5-Turbo 对各种数据集的推理成本（每 1,000 个样本 $）。
- en: 'Table [7](#A4.F7 "Figure 7 ‣ Appendix D Economic Cost ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") presents
    the estimated per 1,000 samples inference costs for various datasets, encompassing
    input prompts and generated output text, based on GPT-3.5-Turbo pricing^(14)^(14)14https://openai.com/pricing.
    Our approach demonstrates substantial savings in computational resources and monetary
    expenses, particularly in long context situations. Cost reductions of $3.3, $28.5,
    and $27.4 per 1,000 samples are observed for Multi-document QA, LongBench, and
    ZeroScrolls, respectively.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#A4.F7 "图 7 ‣ 附录 D 经济成本 ‣ LongLLMLingua：通过提示压缩加速和增强 LLMs 在长上下文场景中的应用")
    展示了各种数据集每 1,000 个样本的推理成本，包括输入提示和生成的输出文本，基于 GPT-3.5-Turbo 定价^(14)^(14)14https://openai.com/pricing。我们的方法在计算资源和货币开支上显示出显著的节省，特别是在长上下文情况下。对
    Multi-document QA、LongBench 和 ZeroScrolls，每 1,000 个样本的成本减少分别为 $3.3、$28.5 和 $27.4。
- en: Appendix E Cases Study
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 案例研究
- en: '<svg id="A5.F8.pic1" class="ltx_picture" height="800.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,800.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="772.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in <math id="A5.F8.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"
    class="ltx_Math ltx_framed ltx_framed_rectangle" display="block"></math>. Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019</foreignobject></g></g></svg>'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A5.F8.pic1" class="ltx_picture" height="800.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,800.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="772.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in <math id="A5.F8.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"
    class="ltx_Math ltx_framed ltx_framed_rectangle" display="block"></math>. Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019</foreignobject></g></g></svg>'
- en: 'Figure 8: Cases study on NaturalQuestions Multi-document QA dataset (Liu et al.,
    [2023](#bib.bib20)) in 4x constraint using GPT-3.5-Turbo.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在 4x 约束下使用 GPT-3.5-Turbo 对 NaturalQuestions 多文档 QA 数据集进行的案例研究（Liu 等，[2023](#bib.bib20)）。
- en: '<svg id="A5.F9.pic1" class="ltx_picture" height="340.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please complete the code given below.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <svg id="A5.F9.pic1" class="ltx_picture" height="340.28" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">压缩提示：请完成下面给出的代码。
- en: '[PRE0]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next line of code: LLMs’ Response:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码：LLMs 的响应：
- en: '[PRE1]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Ground Truth:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 实际情况：
- en: '[PRE2]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Zero-shot LLMs’ Response:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本 LLMs 的响应：
- en: '[PRE3]</foreignobject></g></g></svg>'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]</foreignobject></g></g></svg>'
- en: 'Figure 9: Cases study on lcc code completion task in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在 LongBench 基准测试中对 lcc 代码补全任务的案例研究（Bai 等，[2023](#bib.bib1)），使用 GPT-3.5-Turbo
    进行 2,000 限制。
- en: '<svg id="A5.F10.pic1" class="ltx_picture" height="903.29" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,903.29) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="875.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something</foreignobject></g></g></svg>'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg id="A5.F10.pic1" class="ltx_picture" height="903.29" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,903.29) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="875.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something</foreignobject></g></g></svg>'
- en: 'Figure 10: Cases study on trec few-show learning in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在 LongBench 基准测试中对 trec 少样本学习的案例研究（Bai 等，[2023](#bib.bib1)），使用 GPT-3.5-Turbo
    进行 2,000 限制。
