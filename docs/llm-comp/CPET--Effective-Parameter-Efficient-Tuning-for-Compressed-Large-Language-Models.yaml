- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:59'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.07705](https://ar5iv.labs.arxiv.org/html/2307.07705)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Weilin Zhao  , Yuxiang Huang^∗, Xu Han, Zhiyuan Liu
  prefs: []
  type: TYPE_NORMAL
- en: Zhengyan Zhang, Maosong Sun NLP Group, DCST, IAI, BNRIST, Tsinghua University,
    Beijing
  prefs: []
  type: TYPE_NORMAL
- en: '{zwl23,huang-yx21,zy-z19}@mails.tsinghua.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: '{hanxu2022,liuzy,sms}@tsinghua.edu.cn  indicates equal contribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Parameter-efficient tuning (PET) has been widely explored in recent years because
    it tunes much fewer parameters than full-parameter fine-tuning (FT) while still
    stimulating sufficient knowledge from large language models (LLMs) for downstream
    tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific
    PET modules can be built on a frozen LLM, avoiding redundant LLM deployments.
    Although PET significantly reduces the cost of tuning and deploying LLMs, its
    inference still suffers from the computational bottleneck of LLMs. To address
    the above issue, we propose an effective PET framework based on compressed LLMs,
    named “CPET”. In CPET, we evaluate the impact of mainstream LLM compression techniques
    and then introduce knowledge inheritance and recovery strategies to restore the
    knowledge loss caused by these compression techniques. Our experimental results
    demonstrate that, owing to the restoring strategies of CPET, collaborating task-specific
    PET modules with a compressed LLM can achieve comparable performance to collaborating
    PET modules with the non-compressed LLM and outperform directly applying vanilla
    PET methods to the compressed LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'CPET: Effective Parameter-Efficient Tuning for'
  prefs: []
  type: TYPE_NORMAL
- en: Compressed Large Language Models
  prefs: []
  type: TYPE_NORMAL
- en: 'Weilin Zhao^†^†thanks: indicatesequalcontribution.  , Yuxiang Huang^∗, Xu Han,
    Zhiyuan Liu Zhengyan Zhang, Maosong Sun NLP Group, DCST, IAI, BNRIST, Tsinghua
    University, Beijing {zwl23,huang-yx21,zy-z19}@mails.tsinghua.edu.cn {hanxu2022,liuzy,sms}@tsinghua.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, the rise in data scale and computing power has boosted the
    growth of the parameter size of language models. While some small and medium language
    models with millions of parameters have shown proficiency in capturing rich knowledge Jawahar
    et al. ([2019](#bib.bib28)); Yenicelik et al. ([2020](#bib.bib63)), large language
    models (LLMs) with billions of parameters Brown et al. ([2020](#bib.bib6)); Black
    et al. ([2022](#bib.bib4)); Chowdhery et al. ([2022](#bib.bib9)) exhibit more
    powerful and comprehensive abilities, especially in terms of cognition and embodiment Lewkowycz
    et al. ([2022](#bib.bib32)); Nakano et al. ([2021](#bib.bib38)); Driess et al.
    ([2023](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the success of LLMs, how to apply LLMs to serve real-world scenarios
    is an important issue. As most users cannot afford the enormous cost of running
    LLMs, the prevailing solution is to provide LLM services, with service providers OpenAI
    ([2022](#bib.bib39)); Google ([2023](#bib.bib21)) adapting LLMs for specific tasks
    and then providing users with interfaces to infer the task-specific LLMs. To extend
    LLM services to multi-task scenarios, parameter-efficient tuning (PET) Houlsby
    et al. ([2019](#bib.bib26)); Hu et al. ([2021](#bib.bib27)) has been widely used
    for the task adaptation of LLMs, where a unified LLM is frozen as a backbone among
    different tasks and then tiny tunable PET modules are injected into the backbone
    to stimulate task-specific knowledge. Compared to conventional full-parameter
    fine-tuning (FT), where a single LLM is tuned into multiple task-specific LLM
    copies, PET tunes much fewer parameters and has lower memory overhead in multi-task
    serving while achieving comparable performance Ding et al. ([2023](#bib.bib15));
    Zhou et al. ([2022](#bib.bib69)).
  prefs: []
  type: TYPE_NORMAL
- en: Although PET has shown potential in reducing the cost of tuning and deploying
    LLMs for LLM services, the computation of the shared backbone LLM is inevitable,
    i.e., the inference of the combination of the backbone LLM and PET modules is
    still computation-intensive and latency-high. Empirically, adopting model compression
    techniques Hinton et al. ([2015](#bib.bib25)); Bai et al. ([2021](#bib.bib1));
    Liang et al. ([2021](#bib.bib34)) to compress LLMs into smaller versions is a
    solution to cope with the different latency requirements of inferring LLMs, yet
    whether PET modules can work well with compressed LLMs is still an open problem,
    especially considering that model compression techniques may introduce knowledge
    loss and performance degradation to the compressed LLMs. In this paper, we build
    an effective PET framework based on compressed LLMs, named “CPET”.
  prefs: []
  type: TYPE_NORMAL
- en: 'To restore the knowledge loss caused by the compression process, CPET introduce
    the following two mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) PET Knowledge Inheritance. A stronger LLM can make learning PET modules
    easier. Meanwhile, the PET modules based on the stronger LLM can also better grasp
    how to stimulate task-specific knowledge distributed in the LLM. Therefore, we
    propose to adopt the PET modules learned on the non-compressed LLM as the initialization
    to learn the PET modules for the compressed LLM. In this way, the task-related
    knowledge of PET modules learned with the help of the non-compressed LLM can be
    inherited to obtain more effective PET modules for the compressed LLM.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Model Knowledge Recovery. In addition to the knowledge of PET modules, the
    knowledge of the LLM is also important to perform well on downstream tasks. Since
    compression techniques may result in losing some task-related knowledge within
    the LLM, we add extra knowledge recovery modules into the compressed LLM to bridge
    the knowledge gap that arises from the compression process. We point out that
    compression techniques may weaken multiple capabilities of the LLM while restoring
    only a part of the lost capabilities requires only a small number of parameters.
    Through the supervision of task data, we can recover most of the lost capabilities
    related to specific tasks through some tiny recovery modules.
  prefs: []
  type: TYPE_NORMAL
- en: In experiments, we conduct a comprehensive evaluation of the performance impact
    brought by various compression methods. The results show that compression results
    in a significant performance drop without using any knowledge recovery mechanisms.
    Based on the above observation, we apply CPET for performance recovery, and the
    experimental results indicate that CPET can restore the performance to the level
    before model compression. Moreover, computing the compressed LLM requires much
    lower resources than computing the non-compressed LLM, making CPET finally an
    effective and efficient PET framework.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is related to LLMs, PET, and model compression. We mainly introduce
    PET and model compression methods in this paper. More details on LLMs can refer
    to the survey for more details Qiu et al. ([2020](#bib.bib43)); Han et al. ([2021](#bib.bib23));
    Bommasani et al. ([2021](#bib.bib5)); Zhao et al. ([2023](#bib.bib68)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Parameter-Efficient Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although an LLM can acquire rich knowledge from massive pre-training data to
    handle complex tasks in a zero-shot or few-shot manner Brown et al. ([2020](#bib.bib6));
    Black et al. ([2022](#bib.bib4)), to better stimulate the knowledge stored in
    the LLM to serve downstream tasks, there is still a need for adapting the LLM
    to various scenarios. For traditional PLMs, fine-tuning all parameters of PLMs
    is the mainstream way to adapt them Church et al. ([2021](#bib.bib10)), yet its
    parameter inefficiency makes this way costly to adapt LLMs Ding et al. ([2023](#bib.bib15)).
    Moreover, maintaining task-specific versions of LLM in the storage is unacceptably
    resource-intensive Zhou et al. ([2022](#bib.bib69)).
  prefs: []
  type: TYPE_NORMAL
- en: To adapt LLMs to multi-task scenarios in a more efficient manner, various PET
    methods Lester et al. ([2021](#bib.bib31)); Houlsby et al. ([2019](#bib.bib26));
    Hu et al. ([2021](#bib.bib27)); Li and Liang ([2021](#bib.bib33)); Ben Zaken et al.
    ([2022](#bib.bib2)) have been proposed, where LLMs are frozen and some model-independent
    tunable modules are injected into the transformer architecture of LLMs to help
    the adaptation process. PET modules are usually tiny, which can significantly
    reduce the cost of adapting LLMs. PET modules can be inserted into different locations
    within the transformer architecture. For instance, prompt tuning Lester et al.
    ([2021](#bib.bib31)) and prefix tuning Li and Liang ([2021](#bib.bib33)) are two
    methods that prepend tunable embeddings to the input and hidden states, respectively.
    Adapter tuning Houlsby et al. ([2019](#bib.bib26)) applies tunable transformation
    between adjacent modules. BitFit Ben Zaken et al. ([2022](#bib.bib2)) and LoRA Hu
    et al. ([2021](#bib.bib27)) make minor internal modifications to the modules of
    the transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, LLMs have acquired rich capabilities and just need an efficient
    way to stimulate these capabilities. The role of tunable PET modules is to learn
    task features and serve as triggers to stimulate task-specific capabilities in
    LLMs Ding et al. ([2023](#bib.bib15)). Sufficient experiments show that collaborating
    task-specific PET modules and a frozen LLM can reach comparable performance to
    fine-tuning all parameters of the LLM. Furthermore, since different task-specific
    PET modules can share a unified frozen LLM as their backbone, this also leads
    to lower computation and storage overhead in multi-task serving and switching Zhou
    et al. ([2022](#bib.bib69)). In general, the emergence of PET methods significantly
    reduces the cost of tuning and deploying LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Model Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although PET methods can reduce the storage cost for deploying LLMs, the computation
    bottleneck of the LLM itself still exists. Therefore, to further improve efficiency
    for model serving, it is crucial to speed up the computation of LLMs, and model
    compression is a commonly used solution. Considering that the PET modules of different
    tasks usually work together on a unified LLM, here we mainly introduce task-agnostic
    model compression Sanh et al. ([2019](#bib.bib48)) rather than task-specific compression Sun
    et al. ([2019](#bib.bib52)) for LLMs, including quantization, pruning, and MoEfication.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional PLMs, 32-bit floating-point numbers are mainly used to represent
    models. As the model size gradually increases, representing LLMs in a 32-bit format
    consumes too much GPU memory and computational time. To address this issue, mixed-precision
    training Micikevicius et al. ([2017](#bib.bib37)) is adopted to represent LLMs
    with 16-bit floating-point numbers. To further reduce the memory overhead and
    improve the model speed, quantization methods are applied to represent models
    with fixed-point numbers, from 8-bit Zafrir et al. ([2019](#bib.bib64)), 4-bit Frantar
    et al. ([2023](#bib.bib19)) to 1-bit Bai et al. ([2021](#bib.bib1)). To avoid
    the performance degradation caused by quantization, quantization-aware training
    (QAT) Stock et al. ([2021](#bib.bib50)) has also been proposed to use a small
    amount of data to adjust the distribution of model parameters for quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Different from quantization methods that compress the representation of each
    parameter, pruning methods directly discard some parameters. Commonly used pruning
    methods include structured pruning Fan et al. ([2020](#bib.bib18)); Wang et al.
    ([2020](#bib.bib58)); Zhang et al. ([2021](#bib.bib67)); Xia et al. ([2022](#bib.bib60))
    and unstructured pruning Han et al. ([2015](#bib.bib22)); Chen et al. ([2020](#bib.bib7));
    Xu et al. ([2021](#bib.bib62)). Structured pruning aims to find useless modules
    and remove them completely, such as erasing all parameters in a linear layer.
    Unstructured pruning only removes individual parameters, such as deleting some
    parameters to form a sparse matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1edae9477e5148bdb28dea4d3b29043a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The overall design of our CPET. We use LoRA Hu et al. ([2021](#bib.bib27))
    as an example of PET methods.'
  prefs: []
  type: TYPE_NORMAL
- en: MoEfication Zhang et al. ([2022b](#bib.bib66)), inspired by the mixture-of-experts
    (MoE) transformer Lepikhin et al. ([2021](#bib.bib30)), aims to divide the parameters
    of LLMs into multiple partitions, and each time only a few partitions are used
    to compute the final results. Although most of the currently popular LLMs are
    dense models, studies have shown that dense LLMs are activated sparsely, and different
    parameter areas are activated by different data to form some skill partitions Wang
    et al. ([2022](#bib.bib57)); Dai et al. ([2021](#bib.bib12)); Suau et al. ([2020](#bib.bib51));
    Panigrahi et al. ([2023](#bib.bib40)). Specifically, by analyzing the sparse pattern
    of activation states in LLMs, the linear layers of LLMs are sliced to MoE, and
    an expert router is trained to select experts. During the computation process,
    a certain proportion of relevant experts is dynamically activated according to
    the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, to make a compressed LLM behave the same as its original version,
    distillation objectives are often used to align the pre-compression and post-compression
    models, including aligning both output and intermediate states Hinton et al. ([2015](#bib.bib25));
    Sun et al. ([2019](#bib.bib52)); Jiao et al. ([2020](#bib.bib29)); Liu et al.
    ([2022](#bib.bib35)); Park et al. ([2021](#bib.bib41)). Due to space limitations,
    more compression details can refer to the survey Liang et al. ([2021](#bib.bib34));
    Xu and McAuley ([2022](#bib.bib61)).
  prefs: []
  type: TYPE_NORMAL
- en: Currently, combining PET with model compression is preliminary, and only some
    works attempt to combine PET with model quantization Dettmers et al. ([2023](#bib.bib14));
    Liu et al. ([2023](#bib.bib36)). Recent work Chen et al. ([2023](#bib.bib8)) also
    attempts to add modules to recover the knowledge loss caused by model compression,
    but it has not been fully verified on various downstream tasks. Combining PET
    with other compression methods to improve the inference speed is still an open
    issue for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will introduce how to build an effective PET framework CPET based
    on compressed LLMs. Before introducing CPET, we first explain some essential preliminaries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Preliminary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For simplicity, we denote a LLM $\mathcal{M}$) are tuned as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\theta}_{\mathcal{M}}^{t}=\arg\min_{\mathbf{\theta}_{\mathcal{M}}}\mathcal{L}(f(\mathbf{X}^{t};\mathbf{\theta}_{\mathcal{M}}),\mathbf{Y}^{t}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{X}^{t},\mathbf{Y}^{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: In the PET setting, $\mathcal{M}$. The tuning process is formalized as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{\theta}_{\mathcal{P}(\mathcal{M})}^{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: This paper aims to obtain PET modules based on a compressed LLM. To this end,
    after applying compression algorithms to compress the LLM $\mathcal{M}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since PET methods do not change LLMs, adopting PET methods is thus orthogonal
    to compressing LLMs. Therefore, we propose a more efficient PET framework CPET,
    by first compressing a LLM using task-agnostic model compression methods and then
    applying PET methods to the compressed LLM. Formally, CPET can be formalized as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\theta_{\mathcal{P}(\mathcal{C})}$.
  prefs: []
  type: TYPE_NORMAL
- en: By combining model compression and PET, on the one hand, we can take advantage
    of PET to deploy a unified LLM to serve multiple downstream tasks, while only
    maintaining tiny task-specific PET modules for each downstream task. On the other
    hand, by adopting a compressed LLM instead of a non-compressed LLM, the inference
    time and resource requirements of the LLM can be significantly reduced. It is
    worth noting that this acceleration is not free. It is not difficult to imagine
    that adopting task-agnostic compression methods may weaken the LLM, which will
    inevitably affect the search for the optimal parameters $\mathbf{\theta}_{\mathcal{P}(\mathcal{C})}^{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the fact that model compression preserves those capabilities of
    LLMs that smaller models cannot master, we suppose that the PET modules trained
    on the non-compressed LLM would contain certain task knowledge that the PET modules
    can hardly learn solely on the compressed model. As shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2.2 Model Compression ‣ 2 Related Work ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models"), to better learn PET modules for
    the compressed LLM, we adopt the method of inheriting the PET knowledge from those
    modules trained on the non-compressed LLM. To restore the knowledge loss caused
    by the compressing process, in addition to the PET modules $\mathcal{P}$, and
    Eq. ([3](#S3.E3 "In 3.2 Framework ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models")) is modified to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{\theta}_{\mathcal{P}(\mathcal{C})}^{t},\mathbf{\theta}_{\mathcal{R}}^{t}=$
    |  |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\arg\min_{\mathbf{\theta}_{\mathcal{P}(\mathcal{C})},\mathbf{\theta}_{\mathcal{R}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\alpha\mathcal{L}_{\text{DIST}}(\mathbf{X}^{t};\theta_{\mathcal{M}},\theta_{\mathcal{C}},\theta_{\mathcal{P}(\mathcal{C})},\mathbf{\theta}_{\mathcal{R}})\big{]},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{\theta}_{\mathcal{R}}^{t}$ is the distillation loss function
    for model knowledge recovery, which will be introduced later.
  prefs: []
  type: TYPE_NORMAL
- en: In subsequent sections, we will elaborate on how to conduct PET knowledge inheritance
    and model knowledge recovery. It is worth noting that although our methods require
    the non-compressed model to participate in the training process, resulting in
    extra training time, in the context of model serving, the service period after
    training for typical tasks is much longer than the training time. Therefore, we
    sacrifice some training costs for better inference efficiency and effectiveness
    of the final model obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 PET Knowledge Inheritance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of training PET modules based on the compressed LLM from scratch, we
    propose training PET modules based on the original non-compressed LLM first, then
    adapting the learned PET modules to the compressed LLM. The adaption from the
    non-compressed LLM to the compressed LLM can lead to learning better PET modules
    on the compressed LLM. Intuitively, it is more effective for a teacher to teach
    students the fundamentals of a discipline and then let students adapt their comprehension
    based on their circumstances rather than letting students learn from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, we first use Eq. ([2](#S3.E2 "In 3.1 Preliminary ‣ 3 Methodology
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"))
    to obtain the parameters of the task-specific PET modules $\theta_{\mathcal{P}(\mathcal{M})}^{t}$
    by using Eq. ([3](#S3.E3 "In 3.2 Framework ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models")) or Eq. ([4](#S3.E4 "In 3.2 Framework
    ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Model Knowledge Recovery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the reduction of parameters orienting to the compressed LLM $\mathcal{C}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help obtain $\theta_{\mathcal{R}}^{t}$, we design a distillation objective.
    Specifically, we first select the PET modules trained with Eq. ([2](#S3.E2 "In
    3.1 Preliminary ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning for
    Compressed Large Language Models")) as the teacher, and then select the PET modules
    and recovery modules in Eq. ([4](#S3.E4 "In 3.2 Framework ‣ 3 Methodology ‣ CPET:
    Effective Parameter-Efficient Tuning for Compressed Large Language Models")) as
    the student, and the whole distillation loss is given as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{DIST}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{1}{&#124;\mathbf{X}^{t}&#124;}\big{\&#124;}f_{\text{PET}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{X}^{t}$. As shown in Eq. ([4](#S3.E4 "In 3.2 Framework ‣ 3 Methodology
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models")),
    instead of first learning the recovery modules and then adding the inherited PET
    modules for further adaptation, we simultaneously conduct knowledge recovery and
    tune PET modules.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments and Analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will present experimental results and analyses in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate CPET on 11 datasets, covering typical NLP tasks, including BoolQ Clark
    et al. ([2019](#bib.bib11)), CB De Marneffe et al. ([2019](#bib.bib13)), RTE Bentivogli
    et al. ([2009](#bib.bib3)); Wang et al. ([2019](#bib.bib55)), COPA Roemmele et al.
    ([2011](#bib.bib47)), WiC Pilehvar and Camacho-Collados ([2018](#bib.bib42)),
    SST-2 Socher et al. ([2013](#bib.bib49)), MRPC Dolan and Brockett ([2005](#bib.bib16)),
    QQP Wang et al. ([2018](#bib.bib56)), MNLI Williams et al. ([2017](#bib.bib59)),
    QNLI Rajpurkar et al. ([2016](#bib.bib45)), and SQuAD Rajpurkar et al. ([2016](#bib.bib45)).
    For all these datasets, we use their validation sets for testing and parts of
    their training sets for validation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2b4b49e035cc611966a5f768a8fa4f94.png)![Refer to caption](img/71b47096ceb02f993d64b837538c2b93.png)![Refer
    to caption](img/acd0e11e00ad6eb981d85c9210ea1f3b.png)![Refer to caption](img/14eac2964b6b5f9d7285657a33eabaa2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Performance on various compression methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Baselines and Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We select T5-3b Raffel et al. ([2020](#bib.bib44)) as the backbone LLM in our
    experiments. For the compressed models, we use the compressed versions of T5-3b
    released by Zhang et al. ([2022a](#bib.bib65)). The compression methods used in
    our experiments include 8-bit quantization, structured pruning, unstructured pruning,
    and MoEfication. Table [1](#S4.T1 "Table 1 ‣ 4.3 The Overall Performance of CPET
    ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for
    Compressed Large Language Models") shows the models used in our experiments and
    their ideal inference speedup compared to T5-3b Zhang et al. ([2022a](#bib.bib65)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate CPET, we adopt 4 paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: '(1) T5-3b + PET: PET modules are attached to the original T5-3b, and only the
    parameters of PET modules are tunable while the parameters of the LLM are frozen.
    (2) T5-base + PET: Tunable PET modules are attached to the frozen T5-base model.
    (3) CLM + PET: PET modules are attached to the compressed versions of T5-3b, and
    then these compressed LLMs (CLMs) are frozen and PET modules are tuned on task-specific
    data. (4) CLM + CPET: CPET is applied to the compressed versions of T5-3b, and
    only PET and recovery modules are tuned on task-specific data.'
  prefs: []
  type: TYPE_NORMAL
- en: All the above paradigms are implemented with the open-source toolkit OpenDelta Ding
    et al. ([2023](#bib.bib15)). For a fair comparison, we use LoRA Hu et al. ([2021](#bib.bib27))
    as a representative PET method for all paradigms. We set the bottleneck dimension
    of the LoRA modules to $32$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 The Overall Performance of CPET
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Datasets ‣ 4 Experiments and Analyses ‣ CPET:
    Effective Parameter-Efficient Tuning for Compressed Large Language Models") shows
    the performance improvement of CPET compared to PET. From the figure, we can find
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Comparing the original LLM and its compressed versions, the results show
    that the compressed LLMs cannot perform as well as the original LLM. It suggests
    that task-agnostic compression methods lead to losing some knowledge related to
    downstream tasks. That is to say, to improve the inference speed, the performance
    of the compressed model may decrease due to the acceleration process. If there
    is a mechanism to make up the performance gap without affecting the inference
    speed, applying compression methods will be more reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Model Size | Ideal Speedup |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b (bf16) | 5.61 GB | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b (M) | 3.74 GB | 150% |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b (UP) | 2.81 GB | 200% |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b (SP) | 2.81 GB | 200% |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b (Q) | 2.81 GB | 200% |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b (Q+UP+M) | 0.94 GB | 600% |'
  prefs: []
  type: TYPE_TB
- en: '| T5-base (bf16) | 0.44 GB | 1400% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The models used in the experiments. The notation “T5-3b (X)” represents
    the T5-3b model with the setting “X”. “M”, “UP”, “SP”, and “Q” represent the model
    is compressed with MoEfication, unstructured pruning, structured pruning, and
    8-bit quantization, respectively. “Q+UP+M“ means “Q”, “UP” and “M” are combined
    together to achieve higher compression ratios. “bf16” indicates the model is represented
    in bfloat16 floating-point rather than 32-bit floating-point format.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Model | BoolQ | CB | RTE | COPA | WiC | SST2 |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Size(GB) | Acc(%) | Acc(%) | Acc(%) | Acc(%) | Acc(%) | Acc(%) |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b + PET | 5.61 | 88.4 | 98.2 | 89.6 | 87.0 | 75.4 | 96.1 |'
  prefs: []
  type: TYPE_TB
- en: '| T5-base + PET | 0.44 | 79.5 | 91.1 | 80.7 | 71.0 | 69.9 | 93.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CLM + PET | 0.94 | 86.0 | 94.6 | 81.8 | 79.0 | 73.6 | 94.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CLM + CPET | 0.94 | 86.7 | 100.0 | 86.1 | 85.0 | 75.3 | 96.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MNLI-m | QQP | QQP | MRPC | QNLI | SQuAD | SQuAD |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Acc(%) | Acc(%) | F1(%) | Acc(%) | Acc(%) | EM(%) | F1(%) |'
  prefs: []
  type: TYPE_TB
- en: '| T5-3b + PET | 90.6 | 91.3 | 90.7 | 89.5 | 95.4 | 84.2 | 92.5 |'
  prefs: []
  type: TYPE_TB
- en: '| T5-base + PET | 84.8 | 90.6 | 89.9 | 86.5 | 93.1 | 79.0 | 87.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CLM + PET | 89.0 | 90.6 | 89.9 | 89.7 | 94.7 | 79.9 | 90.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CLM + CPET | 89.9 | 91.5 | 90.9 | 89.5 | 94.7 | 81.3 | 90.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The results of applying CPET on the mixture of multiple compression
    methods (%).'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Within the compressed model, CPET consistently outperforms vanilla PET methods.
    Such results indicate that task capabilities are effectively migrated through
    the mechanisms of knowledge inheritance and knowledge recovery.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Through cross-comparisons between different compression models, we find
    that quantization and MoEfication have relatively little loss on the model performance,
    and the loss can be completely restored using our CPET method. However, the pruning
    methods cause more performance loss, especially the structured pruning method.
    Even though, CPET can recover most of the performance loss caused by these pruning
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbe6645e7aba9246d6f5a1ec21616736.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The convergence of vanilla PET, inherited PET (CPET without recovery
    modules), and CPET.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further evaluate CPET on a higher compression ratio, we combine quantization,
    MoEfication, and unstructured pruning to obtain a compressed T5-3b that has a
    close size to T5-base. We compare the four paradigms based on this compressed
    model. Table [2](#S4.T2 "Table 2 ‣ 4.3 The Overall Performance of CPET ‣ 4 Experiments
    and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models") shows the experimental results. Results show that CPET is compatible
    and can be easily applied to a highly compressed model that uses multiple compression
    methods. Meanwhile, CPET demonstrates better performance on compressed models
    than training a small model with a close size from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further discuss the effectiveness of CPET design, it is vital to understand
    how different mechanisms in CPET help restore the knowledge loss caused by model
    compression, whether the number of parameters the main cause of performance enhancement
    and what is the speed effect of applying CPET. Therefore, we focus on answering
    the following questions to illustrate the benefits of CPET.
  prefs: []
  type: TYPE_NORMAL
- en: To more clearly demonstrate the inner mechanisms of CPET, we conduct ablation
    studies based on the compressed model using the mixture of compression methods
    with different settings, and adopt RTE for evaluation. In the settings of CPET,
    the intermediate ranks of LoRA and Recovery modules are 8\. LoRA modules are injected
    into linear transformations of $\mathbf{W^{Q}}$ in attention layers. Recovery
    modules are injected into all linear transformations in both attention and feed-forward
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: How do different mechanisms in CPET help restore the knowledge loss caused by
    model compression methods?
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the effectiveness of each mechanism used in CPET, we test all possible
    combinations of inheritance, recovery, and distillation. We can find that from
    Table [3](#S4.T3 "Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"):'
  prefs: []
  type: TYPE_NORMAL
- en: (1) PET knowledge inheritance is effective. By initializing the tunable parameters
    with the PET modules trained on the non-compressed LLM, the performance of combining
    the final PET modules and the compressed LLM has been significantly improved.
    This indicates that in the optimization space of the compressed LLM, it is difficult
    to obtain the optimal task-specific parameters of PET modules based on random
    initialization, but more optimal PET modules can be achieved more easily using
    PET knowledge inheritance.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Simply adding recovery modules brings a certain level of performance improvements
    in some circumstances. However, it can achieve further performance improvements
    by adopting our distillation strategy. This suggests combining recovery modules
    with the knowledge distillation strategy to enhance PET modules is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Is the improvement only caused by increased parameters?
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, we carry out the study in Table [4](#S4.T4 "Table
    4 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models"). Each setting in this study maintains
    the same number of parameters as CPET. In CPET, the tunable parameters consist
    of PET modules and recovery modules. We introduce new settings of LoRA+LoRA and
    Large LoRA, which trivially increase the tunable parameters by adding more LoRA
    modules or using larger LoRA modules. Considering that recovery modules can be
    regarded as functionally specialized PET modules, we further test Rec+Rec (use
    recovery modules instead of LoRA) and Large Rec (Larger recovery modules with
    the same parameter number). More specifically, in LoRA+LoRA settings, we first
    inject LoRA modules to $\mathbf{W^{Q}}$ to larger modules whose intermediate rank
    is 16. Rec+Rec settings are similar to LoRA+LoRA settings, except for replacing
    all LoRA modules with recovery modules (intermediate ranks are 8). Large Rec settings
    are similar to Large LoRA settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Table [4](#S4.T4 "Table 4 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"),
    we find that adding more parameters makes marginal improvements, showing that
    only adding more tunable parameters cannot bridge the knowledge gap caused by
    model compression.'
  prefs: []
  type: TYPE_NORMAL
- en: Does adding more parameters slow down model inference?
  prefs: []
  type: TYPE_NORMAL
- en: 'Since CPET introduces a little more parameters than conventional PET methods,
    it has a potential cause of slower inference. Given that the inference speed of
    compressed models are highly related to implementation, we evaluate CPET and conventional
    PET methods with the same backbone on the same platform to avoid uncertainties
    caused by backbone model implementation. We report the average time and corresponding
    standard deviation of each model call. Table [5](#S4.T5 "Table 5 ‣ 4.4 Ablation
    Studies ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient Tuning
    for Compressed Large Language Models") shows that the inference time of CPET and
    PET methods are almost the same, proving that the time bottleneck of inference
    lies in the model itself, while the extra parameters added due to CPET do not
    slow down model inference.'
  prefs: []
  type: TYPE_NORMAL
- en: From the ablations above, we prove that each mechanism introduced in CPET is
    helpful to recover performance degradation of model compression without retarding
    model inference. The structural design of CPET is reasonable, which is beneficial
    to knowledge inheritance and recovery.
  prefs: []
  type: TYPE_NORMAL
- en: '| Inherit | Recover | Distill | RTE Acc(%) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ |  |  | 85.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ✓ |  | 82.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ✓ | 82.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ |  | 87.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ |  | ✓ | 85.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ✓ | ✓ | 86.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | ✓ | 88.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The ablation studies on RTE (%). When eliminating inheritance, we
    keep the LoRA modules injected and train these modules from scratch. When eliminating
    recovery, we simply remove the recovery modules. When eliminating distillation,
    the training loss function only consists of the task loss function without the
    distillation loss function. These studies use the mixutre of multiple compression
    methods same as [4.3](#S4.SS3 "4.3 The Overall Performance of CPET ‣ 4 Experiments
    and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models") to compress the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | RTE Acc(%) |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA+LoRA | 83.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Large LoRA | 84.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Rec+Rec | 85.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Large Rec | 84.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CPET | 88.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The ablation studies with parameter quantity controlled. Each setting
    has the same amount of tunable parameters. “LoRA” represents the conventional
    LoRA module with the same size in CPET. “Rec” represents the recovery module with
    the same size in CPET. “Large” prefix means doubling the size, which equals to
    the size of the whole CPET. These studies use the mixutre of multiple compression
    methods same as [4.3](#S4.SS3 "4.3 The Overall Performance of CPET ‣ 4 Experiments
    and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models") to compress the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | # Param | Avg. Time (ms) |'
  prefs: []
  type: TYPE_TB
- en: '| CLM+PET | 10M | (9761$\pm$17) |'
  prefs: []
  type: TYPE_TB
- en: '| CLM+CPET | 60M | (9526$\pm$70) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The average inference time. “CLM” here represents the compressed T5-3b
    using mixture of compression methods. “# Param” represents the additional parameters
    compared with the backbone model. The average time and corresponding standard
    deviations represent the time taken for each model call.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 The Convergence of CPET
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although we are primarily concerned with the final inference speed and performance
    after training, the method usability may be compromised if the training process
    spends too much time. Therefore, based on four compressed LLMs, we compare the
    convergence speed of tuning PET modules to handle the BoolQ dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'From Figure [3](#S4.F3 "Figure 3 ‣ 4.3 The Overall Performance of CPET ‣ 4
    Experiments and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed
    Large Language Models"), we can find that due to our PET knowledge inheritance
    mechanism, CPET is superior to the vanilla PET methods in terms of convergence
    speed and final results. Adding the recovery module will not affect the convergence
    speed. Furthermore, when quantization, unstructured pruning, or MoEfication is
    used, the inheritance mechanism gives a better starting point for tuning PET modules.
    While in the case of structured pruning, even though the initial point of CPET does
    not work well on tasks, it is closer to the optimal point in the optimization
    space and converges faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, considering the existence of numerous downstream tasks, the PET modules
    based on a unified LLM may be trained by community users on their own private
    data and then uploaded to the Internet. When adapting these PET modules to a compressed
    LLM, there may not be any task-specific data available for the adaptation process.
    Intuitively, applying PET parameters trained on one model to another requires
    adaptation using additional data. Surprisingly, from Figure [3](#S4.F3 "Figure
    3 ‣ 4.3 The Overall Performance of CPET ‣ 4 Experiments and Analyses ‣ CPET: Effective
    Parameter-Efficient Tuning for Compressed Large Language Models"), we can find
    that when quantization or MoEfication is used, we can achieve ideal results without
    using any data for adaptation, by only using the PET inheritance mechanism we
    proposed.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Instruction Tuning with CPET
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The above experimental results have proven the strength of CPET for specific
    downstream tasks. In this section, we further investigate the effectiveness of
    CPET when applied CPET to a more general scenario — instruction tuning. We select
    Alpaca Taori et al. ([2023](#bib.bib53)) as the instruction tuning dataset and
    use 5-shot MMLU (containing 57 subtasks) Hendrycks et al. ([2020](#bib.bib24))
    as the evaluation benchmark. We compress LLaMA-13b Touvron et al. ([2023](#bib.bib54))
    into 8-bit quantization version. Since LLaMA use SwiGLU Ramachandran et al. ([2017](#bib.bib46))
    as its activation function, which makes LLaMA not sparse enough to adopt MoEfication
    to compress itself, we thus do not adopt MoEfication to compress LLaMA-13b. More
    details of the LLaMA experiments can be found in Appendix [A](#A1 "Appendix A
    The Experimental Settings of LLaMA ‣ CPET: Effective Parameter-Efficient Tuning
    for Compressed Large Language Models"). From Table [6](#S4.T6 "Table 6 ‣ 4.6 Instruction
    Tuning with CPET ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models"), we can find that CPET combined
    with quantization can still exhibit strong performance while achieving faster
    inference speed. In future work, we will explore better recovery solutions orienting
    pruning methods to make CPET more effective on those general tasks like instruction
    tuning. As compared with the current competitive methods that combine LoRA and
    quantized LLaMA models, CPET achieves better performance and exhibits versatility
    on different compression models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | MMLU(5-shot) | Ideal Speedup |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13b^∗ | 46.9 | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| Q^∗ | 46.6 | 200% |'
  prefs: []
  type: TYPE_TB
- en: '| Dettmers et al. ([2023](#bib.bib14))$\dagger$ | 47.5 | 100% |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2023](#bib.bib36))$\dagger$ | 46.7 | 200% |'
  prefs: []
  type: TYPE_TB
- en: '| Q + CPET | 48.3 | 200% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The 5-shot MMLU performance based on different compressed LLaMA models
    (%). “Q” represents 8-bit quantization. “^∗” represents the MMLU performance without
    training on Alpaca. “$\dagger$” are the current competitive methods that train
    LoRA on the quantized LLaMA-13b model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose an effective PET framework based on the compressed
    LLM (named CPET) to further reduce the resource requirements and inference time
    when deploying LLM and PET modules to serve downstream tasks. Considering task-agnostic
    compression methods may cause losing some task-specific knowledge, we introduce
    PET knowledge inheritance and model knowledge recovery to restore the lost knowledge.
    By inheriting the prior task knowledge of the PET modules learned on the non-compressed
    LLM, searching for the optimal PET modules for the compressed LLM becomes easier.
    Moreover, by introducing knowledge recovery modules to recover task-specific capabilities
    lost in the compression phase, collaborating PET modules with the compressed LLM
    can achieve comparable performance to those PET modules based on the non-compressed
    LLM. The experimental results show that CPET can outperform baselines based on
    the compressed LLM, and meanwhile, CPET maintains the advantages of PET methods
    for multi-task serving. This paper mainly accelerates the inference of PET methods
    and LLMs. We leave the computation bottleneck of LLMs in the tuning process as
    future work.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our work focuses on the efficiency and effectiveness of model serving, but it
    requires a small amount of extra training time. In this paper, we only choose
    LoRA as a representative of PET methods. In fact, our framework can be applied
    to any PET method. The compression method adopted in this paper does not change
    the number of layers of the LLM. However, for those compression methods that change
    the hidden dimensions of the model, how to transfer the knowledge of PET modules
    on the non-compressed LLM remains an open problem for our future work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bai et al. (2021) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, and Irwin King. 2021. [Binarybert: Pushing the limit
    of bert quantization](https://doi.org/10.18653/v1/2021.acl-long.334). In *Proceedings
    of ACL/IJCNLP*, pages 4334–4348.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben Zaken et al. (2022) Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
    2022. [BitFit: Simple parameter-efficient fine-tuning for transformer-based masked
    language-models](https://aclanthology.org/2022.acl-short.1/). In *Proceedings
    of ACL*, pages 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bentivogli et al. (2009) Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang
    Dang, and Danilo Giampiccolo. 2009. [The fifth PASCAL recognizing textual entailment
    challenge](https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf).
    In *Proceedings of TAC*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    et al. 2022. [Gpt-neox-20b: An open-source autoregressive language model](https://arxiv.org/abs/2204.06745).
    *arXiv preprint arXiv:2204.06745*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, et al. 2021. [On the opportunities and risks of foundation models](https://arxiv.org/abs/2108.07258).
    *arXiv preprint arXiv:2108.07258*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
    In *Proceedings of NeurIPS*, volume 33, pages 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. 2020. [The lottery ticket hypothesis
    for pre-trained bert networks](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf).
    In *Proceedings of NeurIPS*, volume 33, pages 15834–15846.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and
    Luming Liang. 2023. [Lorashear: Efficient large language model structured pruning
    and knowledge recovery](https://arxiv.org/abs/2310.18356). *arXiv preprint arXiv:2310.18356*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. [Palm: Scaling language modeling with pathways](https://arxiv.org/abs/2204.02311).
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Church et al. (2021) Kenneth Ward Church, Zeyu Chen, and Yanjun Ma. 2021. [Emerging
    trends: A gentle introduction to fine-tuning](https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-a-gentle-introduction-to-finetuning/C31D429D0928351D6A6692F8ECD1E7ED).
    *Natural Language Engineering*, 27(6):763–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. [Boolq: Exploring the surprising
    difficulty of natural yes/no questions](https://aclanthology.org/N19-1300/). In
    *Proceedings of NAACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2021) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2021.
    [Knowledge neurons in pretrained transformers](https://arxiv.org/abs/2104.08696).
    *arXiv preprint arXiv:2104.08696*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Marneffe et al. (2019) Marie-Catherine De Marneffe, Mandy Simons, and Judith
    Tonhauser. 2019. [The commitmentbank: Investigating projection in naturally occurring
    discourse](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601).
    In *Proceedings of Sinn und Bedeutung*, volume 23, pages 107–124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [Qlora: Efficient finetuning of quantized llms](https://arxiv.org/abs/2305.14314).
    *arXiv preprint arXiv:2305.14314*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. [Parameter-efficient
    fine-tuning of large-scale pre-trained language models](https://www.nature.com/articles/s42256-023-00626-4).
    *Nature Machine Intelligence*, pages 1–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 2005. [Automatically
    constructing a corpus of sentential paraphrases](https://aclanthology.org/I05-5002).
    In *Proceedings of the Third International Workshop on Paraphrasing (IWP2005)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. [Palm-e: An embodied multimodal language model](https://arxiv.org/abs/2303.03378).
    *arXiv preprint arXiv:2303.03378*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Angela Fan, Edouard Grave, and Armand Joulin. 2020. [Reducing
    transformer depth on demand with structured dropout](https://openreview.net/forum?id=SylO2yStDr).
    In *Proceedings of ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. [Gptq: Accurate quantization for generative pre-trained transformers](https://arxiv.org/abs/2210.17323).
    In *Proceedings of ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2023) Google. 2023. [Google ai palm 2](https://ai.google/discover/palm2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William J. Dally. 2015.
    [Learning both weights and connections for efficient neural network](https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html).
    In *Proceedings of NeurIPS*, pages 1135–1143.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2021) Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi
    Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al. 2021. [Pre-trained
    models: Past, present and future](https://www.sciencedirect.com/science/article/pii/S2666651021000231).
    *AI Open*, 2:225–250.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. [Measuring massive multitask
    language understanding](https://arxiv.org/abs/2009.03300). *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. [Distilling
    the knowledge in a neural network](https://arxiv.org/abs/1503.02531). *arXiv preprint
    arXiv:1503.02531*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. [Parameter-efficient transfer learning for nlp](http://proceedings.mlr.press/v97/houlsby19a.html).
    In *Proceedings of ICML*, pages 2790–2799\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. [Lora: Low-rank adaptation
    of large language models](https://arxiv.org/abs/2106.09685). *arXiv preprint arXiv:2106.09685*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jawahar et al. (2019) Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019.
    [What does bert learn about the structure of language?](https://www.aclweb.org/anthology/P19-1356)
    In *Proceedings of ACL*, pages 3651–3657.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2020) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2020. [TinyBERT: Distilling BERT for natural
    language understanding](https://aclanthology.org/2020.findings-emnlp.372). In
    *Findings of EMNLP*, pages 4163–4174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lepikhin et al. (2021) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam M. Shazeer, and Z. Chen.
    2021. [Gshard: Scaling giant models with conditional computation and automatic
    sharding](https://openreview.net/forum?id=qrwe7XHTmYb). In *Proceedings of ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. [The
    power of scale for parameter-efficient prompt tuning](https://arxiv.org/abs/2104.08691).
    *arXiv preprint arXiv:2104.08691*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. 2022. [Solving quantitative reasoning problems with language
    models](https://arxiv.org/abs/2206.14858). *arXiv preprint arXiv:2206.14858*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. [Prefix-tuning: Optimizing
    continuous prompts for generation](https://aclanthology.org/2021.acl-long.353/).
    In *Proceedings of ACL-IJCNLP*, pages 4582–4597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and
    Xiaotong Zhang. 2021. [Pruning and quantization for deep neural network acceleration:
    A survey](https://www.sciencedirect.com/science/article/abs/pii/S0925231221010894).
    *Neurocomputing*, 461:370–403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan Zhao.
    2022. [Multi-granularity structural knowledge distillation for language model
    compression](https://aclanthology.org/2022.acl-long.71). In *Proceedings of ACL*,
    pages 1001–1011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin
    Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2023. [Do emergent abilities exist
    in quantized large language models: An empirical study](https://arxiv.org/abs/2307.08072).
    *arXiv preprint arXiv:2307.08072*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micikevicius et al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Frederick Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston,
    Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2017. [Mixed precision training](https://arxiv.org/abs/1710.03740).
    *arXiv preprint arXiv:1710.03740*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. [Webgpt: Browser-assisted question-answering with
    human feedback](https://arxiv.org/abs/2112.09332). *arXiv preprint arXiv:2112.09332*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) TB OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.
    *OpenAI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panigrahi et al. (2023) Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and
    Sanjeev Arora. 2023. [Task-specific skill localization in fine-tuned language
    models](https://arxiv.org/abs/2302.06600). *arXiv preprint arXiv:2302.06600*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021) Geondo Park, Gyeongman Kim, and Eunho Yang. 2021. [Distilling
    linguistic context for language model compression](https://aclanthology.org/2021.emnlp-main.30).
    In *Proceedings of EMNLP*, pages 364–378.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pilehvar and Camacho-Collados (2018) Mohammad Taher Pilehvar and Jose Camacho-Collados.
    2018. [Wic: the word-in-context dataset for evaluating context-sensitive meaning
    representations](https://arxiv.org/abs/1808.09121). *arXiv preprint arXiv:1808.09121*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai,
    and Xuanjing Huang. 2020. [Pre-trained models for natural language processing:
    A survey](https://link.springer.com/article/10.1007/s11431-020-1647-3). *Science
    China Technological Sciences*, 63(10):1872–1897.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf).
    *JMLR*, 21:1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [Squad: 100,000+ questions for machine comprehension of text](https://arxiv.org/abs/1606.05250).
    *arXiv preprint arXiv:1606.05250*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le.
    2017. [Searching for activation functions](https://arxiv.org/abs/1710.05941).
    *arXiv preprint arXiv:1710.05941*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roemmele et al. (2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S
    Gordon. 2011. [Choice of plausible alternatives: An evaluation of commonsense
    causal reasoning](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF).
    In *Proceedings of AAAI*, pages 90–95.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. [Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter](https://arxiv.org/abs/1910.01108). *arXiv preprint arXiv:1910.01108*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. [Recursive deep
    models for semantic compositionality over a sentiment treebank](https://aclanthology.org/D13-1170).
    In *Proceedings of EMNLP*, pages 1631–1642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stock et al. (2021) Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Herve Jegou, and Armand Joulin. 2021. [Training with quantization
    noise for extreme model compression](https://openreview.net/forum?id=dV19Yyi1fS3).
    In *Proceedings of ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suau et al. (2020) Xavier Suau, Luca Zappella, and Nicholas Apostoloff. 2020.
    [Finding experts in transformer models](https://arxiv.org/abs/2005.07647). *arXiv
    preprint arXiv:2005.07647*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. [Patient
    knowledge distillation for BERT model compression](https://aclanthology.org/D19-1441).
    In *Proceedings EMNLP-IJCNLP*, pages 4323–4332.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. [Llama: Open and efficient foundation
    language models](https://arxiv.org/abs/2302.13971). *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. [Superglue:
    A stickier benchmark for general-purpose language understanding systems](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html).
    In *Proceedings of NeurIPS*, volume 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. 2018. [Glue: A multi-task benchmark and analysis
    platform for natural language understanding](https://arxiv.org/abs/1804.07461).
    *arXiv preprint arXiv:1804.07461*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan
    Liu, and Juanzi Li. 2022. [Finding skill neurons in pre-trained transformer-based
    language models](https://aclanthology.org/2022.emnlp-main.765). In *Proceedings
    of EMNLP*, pages 11132–11152.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020. [Structured
    pruning of large language models](https://aclanthology.org/2020.emnlp-main.496).
    In *Proceedings of EMNLP*, pages 6151–6162.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams et al. (2017) Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017.
    [A broad-coverage challenge corpus for sentence understanding through inference](https://arxiv.org/abs/1704.05426).
    *arXiv preprint arXiv:1704.05426*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2022) Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. [Structured
    pruning learns compact and accurate models](https://aclanthology.org/2022.acl-long.107).
    In *Proceedings of ACL*, pages 1513–1528.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu and McAuley (2022) Canwen Xu and Julian McAuley. 2022. [A survey on model
    compression for natural language processing](https://arxiv.org/abs/2202.07105).
    *arXiv preprint arXiv:2202.07105*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin Xiao. 2021.
    [Rethinking network pruning – under the pre-train and fine-tune paradigm](https://aclanthology.org/2021.naacl-main.188).
    In *Proceedings of NAACL*, pages 2376–2382.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yenicelik et al. (2020) David Yenicelik, Florian Schmidt, and Yannic Kilcher.
    2020. [How does bert capture semantics? a closer look at polysemous words](https://www.aclweb.org/anthology/2020.blackboxnlp-1.15).
    In *Proceedings of BlackboxNLP*, pages 156–162.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    2019. [Q8bert: Quantized 8bit bert](https://ieeexplore.ieee.org/abstract/document/9463531).
    In *Proceedings of EMC2-NIPS*, pages 36–39\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Zhengyan Zhang, Baitao Gong, Yingfa Chen, Xu Han, Guoyang
    Zeng, Weilin Zhao, Yanxu Chen, Zhiyuan Liu, and Maosong Sun. 2022a. [Bmcook: A
    task-agnostic compression toolkit for big models](BMCook:%20A%20Task-agnostic%20Compression%20Toolkit%20for%20Big%20Models).
    In *Proceedings of EMNLP Demonstration*, pages 396–405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022b) Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong
    Sun, and Jie Zhou. 2022b. [MoEfication: Transformer feed-forward layers are mixtures
    of experts](https://aclanthology.org/2022.findings-acl.71). In *Findings of ACL*,
    pages 877–890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu, and Maosong
    Sun. 2021. [Know what you don’t need: Single-shot meta-pruning for attention heads](https://www.sciencedirect.com/science/article/pii/S2666651021000140).
    *AI Open*, 2:36–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. [A survey of large language models](https://arxiv.org/abs/2303.18223). *arXiv
    preprint arXiv:2303.18223*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2022) Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022.
    [PetS: A unified framework for Parameter-Efficient transformers serving](https://www.usenix.org/conference/atc22/presentation/zhou-zhe).
    In *Proceedings of ATC*, pages 489–504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A The Experimental Settings of LLaMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When applying CPET on LLaMA, we add LoRA on all linear modules of the attention
    layer and add the recovery module on all linear modules in transformers. LoRA
    and recovery modules’ ranks are set to 16 with a dropout rate of $p=0.05$. In
    Table [6](#S4.T6 "Table 6 ‣ 4.6 Instruction Tuning with CPET ‣ 4 Experiments and
    Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language
    Models"), method Dettmers et al. ([2023](#bib.bib14)) is in the setting of “NFloat4+DQ”,
    method Liu et al. ([2023](#bib.bib36)) is in the setting of LoRA with 4-bit quantized
    model. All results are evaluated with the framework LM-Eval Gao et al. ([2021](#bib.bib20)).'
  prefs: []
  type: TYPE_NORMAL
