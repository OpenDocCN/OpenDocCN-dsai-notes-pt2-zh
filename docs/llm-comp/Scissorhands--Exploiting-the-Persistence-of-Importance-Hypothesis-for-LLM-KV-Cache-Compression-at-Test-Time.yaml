- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17118](https://ar5iv.labs.arxiv.org/html/2305.17118)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zichang Liu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: zichangliu@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Aditya Desai'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: Aditya.P.Desai@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Fangshuo Liao'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: Fangshuo.Liao@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Weitao Wang'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: wtwang@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Victor Xie'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: vyx2@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Zhaozhuo Xu'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: zhaozhuoxu@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Anastasios Kyrillidis'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77005
  prefs: []
  type: TYPE_NORMAL
- en: anastasios@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Anshumali Shrivastava'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Rice University
  prefs: []
  type: TYPE_NORMAL
- en: Houston, TX 77025
  prefs: []
  type: TYPE_NORMAL
- en: anshumali@rice.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language models(LLMs) have sparked a new wave of exciting AI applications.
    Hosting these models at scale requires significant memory resources. One crucial
    memory bottleneck for the deployment stems from the context window. It is commonly
    recognized that model weights are memory hungry; however, the size of key-value
    embedding stored during the generation process (KV cache) can easily surpass the
    model size. The enormous size of the KV cache puts constraints on the inference
    batch size, which is crucial for high throughput inference workload. Inspired
    by an interesting observation of the attention scores, we hypothesize *the persistence
    of importance*: only pivotal tokens, which had a substantial influence at one
    step, will significantly influence future generations. Based on our empirical
    verification and theoretical analysis around this hypothesis, we propose Scissorhands,
    a system that maintains the memory usage of KV cache under a fixed budget without
    finetuning the model. We validate that Scissorhands reduces the inference memory
    usage of the KV cache by up to 5$\times$ without compromising model quality. We
    further demonstrate that Scissorhands can be combined with 4-bit quantization
    for further compression'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models(LLMs), trained on immense amounts of text data, have demonstrated
    an incredible ability to generate text that is both logically connected and contextually
    relevant [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)].
    LLM inference follows an autoregressive fashion, generating one token at each
    step conditioned on the previous steps. At each step, the key-value embedding
    in attention is stored in memory to avoid repetitive key-value projection computation
    at future steps. Unfortunately, the memory of the key-value cache( KV cache),
    including prompts and previously generated tokens, can be surprisingly large.
    Using OPT-175B as an example, the impressive 175 billion parameters consume around
    325 GB of memory. At the same time, at batch size 128 and sequence length 2048,
    the KV cache requires around 950 GB of memory, three times larger than the model
    weights. Considering that 8 Nvidia A100-80GB offers 640GB GPU memory, the memory
    usage of the KV cache is truly concerning.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are typically deployed on fixed memory hardware, and the size of model
    weights is also fixed once deployed. Apart from a small memory buffer typically
    reserved for communication and computation, the rest of the available memory is
    for the KV cache. The size of the KV cache depends on batch size, sequence length,
    and model dimension. Thus, at a given inference sequence length, compression in
    the KV cache memory translates almost linearly into an increase in the batch size.
    And any increase in batch size is significant for high-throughput inference systems [[6](#bib.bib6),
    [7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/60d39d01d5b353176139bb696b2998dc.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention map at position 178
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3888f95d1049ced6b7ba4fada79603e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention map at position 228
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2211ecfc1fe28cc899cd963c34f8c45c.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attention map at position 278
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Repetitive Attention Pattern. We plot the attention map at three
    token positions in a sentence. Only five attention heads are plotted for a clearer
    presentation. We discretize the attention score such that the high score is dark
    green, and the low score is light green. In Figure [1(a)](#S1.F1.sf1 "In Figure
    1 ‣ 1 Introduction ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"), the token at position 178 pays heavy
    attention to positions 27, 63, 98, etc. This pattern is also present in the attention
    maps of position 228 and position 278\.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization and sparsity approaches [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] have been
    studied in LLMs to reduce the model sizes. However, compressing the KV cache remains
    an open but challenging problem. First, training models at the scale of hundreds
    of billions of parameters on a large amount of data is prohibitively expensive.
    Thus, an ideal compression algorithm should be applicable without training. Second,
    emerging applications such as dialogue systems require an extremely long context
    window. The maximum sequence length of LLMs is growing to over 32K [[15](#bib.bib15)].
    The size of the KV cache also grows linearly with sequence length. For scalability,
    an ideal compression algorithm should reduce the memory from the sequence length
    dimension. At last, compression should preserve LLMs’ quality and in-context learning
    ability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We go beyond the traditional model compression techniques to achieve such demanding
    requirements. We envision that not all tokens must be stored in memory for LLM
    to understand the context. Just like humans can skim through an article and grasp
    the main idea, LLMs may also be able to skim and comprehend. It is commonly observed
    that the attention score from one token follows a strong power law distribution [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)], meaning
    that one token will only heavily attend to a small number of tokens. More importantly,
    we observe Repetitive Attention Pattern from different tokens in the sequence
    in a trained LLM( Figure  [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")). Certain tokens are more important throughout the paragraph. Specifically,
    for two different tokens, there are similarities between who they are heavily
    attending to and similarities between who they are ignoring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the above observation, we articulate the Persistence of Importance
    Hypothesis: *Only pivotal tokens, which had a substantial influence at one previous
    step, will have a significant influence at a future step.* This hypothesis, if
    true, suggests that it is possible to foresee which token is likely to be important
    for future generations. Fortunately, we empirically verify that later tokens in
    the sentence mostly only attend to tokens that were heavily attended from the
    early tokens in a sentence. And the overlapping ratio is surprisingly high, over
    90% in most of the transformer layers (Figure  [2](#S3.F2 "Figure 2 ‣ 3.2 The
    Persistence of Importance Hypothesis ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The memory consumption of model weights and KV cache for three different
    LLMs at batch size 128 and sequence length 2048 shows that the KV cache dominates
    the memory consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | # of Layer | Hidden Size | Weights (GB) | KV cache (GB) |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-175B | 96 | 12288 | 325 | 1152 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-65B | 80 | 8192 | 130 | 640 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | 70 | 14336 | 352 | 950 |'
  prefs: []
  type: TYPE_TB
- en: 'Based on the above two findings, we present Scissorhands that exploits persistence
    of importance hypothesis to realize LLM inference with a compressed KV cache.
    In Section [4](#S4 "4 Sequential Token Generation Under budget ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time"), we present an efficient algorithm such that the size of KV cache
    is always less than a predetermined budget. And a theoretical guarantee justifies
    that such a compressed KV cache can approximate the attention output. In Section [5](#S5
    "5 Empirical Evaluation ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time"), we empirically evaluate
    Scissorhands and show that Scissorhands reduces the memory usage of KV cache up
    to $5\times$ without degradation on model quality. Reduction in the KV cache can
    directly result in a larger batch size. Further, we adopt quantization and show
    its compatibility with Scissorhands.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Description and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper considers the LLM inference workflow, specifically focusing on the
    memory usage for storing the keys and values in attention. Let $d$ transformer
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard LLM inference consists of two stages: prompting and token generation.
    In the prompting stage, the model takes the prompt sentences as the input, and
    the key/value embedding in attention is stored as a cache to reduce repetitive
    computation. Denote $x_{\text{prompt}}^{i}=[x_{1}^{i},...,x_{p}^{i}],x_{\text{prompt}}^{i}\in{\mathbb{R}}^{b\times
    p\times d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: In the generation stage, the model starts with the stored KV cache in the prompting
    stage and generates one token at each step. At each step, the KV cache gets updated.
    Given the input to attention at step $t$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 LLM Inference Memory Breakdown
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we provide the memory consumption breakdown of LLMs. The memory
    footprint consists of three parts: model weights, KV cache, and activation buffer.
    The size of model weights depends on model configuration, such as the number of
    transformer layers and hidden size. The size of the KV cache depends on model
    configurations, sequence length, and batch size. The size of the activation buffer
    depends on parallel strategy, model configurations, and implementation. The size
    of the activation buffer is considerably smaller than the previous two. As shown
    in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"), the size
    of the KV cache, 2.5$\times$ larger than model weights, can quickly become the
    bottleneck in memory consumption. At the same time, much research has been spent
    on extending the length of the context window. GPT-4-32K can process up to 32,768
    tokens [[15](#bib.bib15)]. Longer sequence length would make the KV cache memory
    problem even more severe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming LLM generates until its maximum sequence length, we summarize the
    maximum batch size before going out of GPU memory on a box of 8 A100 80GB GPU
    in Table [2](#S2.T2 "Table 2 ‣ 2.1 LLM Inference Memory Breakdown ‣ 2 Problem
    Description and Related Work ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time"). At the GPT-3 scale with
    a maximum sequence length of 2048, batch size cannot exceed 35 without offloading.
    Small batch size limits the model inference throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: This table summarizes the maximum batch size before hitting out of
    memory on a box of 8 A100 80GB GPU when models are deployed with its maximum sequence
    length.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | OPT-175B | LLaMA-65B | BLOOM |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum Batch Size | 34 | 102 | 36 |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Efficient Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Computing the attention matrix necessitates a time complexity of $O(n^{2})$
    is the sequence length. As a result, a line of work has been proposed to mitigate
    the computation burden of the attention mechanism [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]. These approaches exploit
    low-rank or sparsification to approximate the attention output. Besides, [[21](#bib.bib21)]
    realized exact efficient attention with wall-clock speed by optimizing the number
    of memory reads and writes. However, these approaches were evaluated mostly for
    training, focused on computation complexity, and did not address the KV-Cache
    memory usage introduced by auto-regressive language models.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, there is active research attempting to apply quantization or pruning
    in LLM  [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]. However, they mostly focus on reducing the
    size of model weights. Flexgen [[7](#bib.bib7)] applies quantization and sparsification
    to the KV cache; however, the memory of the KV cache is not reduced regarding
    sequence lengths. It stores the quantized KV cache for all tokens in CPU memory
    and loads all attention keys from CPU memory to compute attention scores.
  prefs: []
  type: TYPE_NORMAL
- en: 3 The Persistence of Importance Hypothesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first present one interesting observation upon which the persistence of
    importance hypothesis is derived in Section [3.1](#S3.SS1 "3.1 Repetitive Attention
    Pattern. ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time"). In Section [3.2](#S3.SS2 "3.2 The Persistence of Importance Hypothesis
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"), we discuss
    the hypothesis in detail with empirical verification. Then, in Section [3.3](#S3.SS3
    "3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"), we provide theoretical intuition
    on the reason behind such model behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Repetitive Attention Pattern.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Observation. We are interested in the attention score from the position $t$
    indicates an averaging mixing score. High attention scores are marked with dark
    green.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result. High attention scores are observed at the same set of tokens from various
    positions in the sentence. In all three plots, we see dark green at sequence positions
    27, 63, 98, 121, 152, and 177, suggesting that these tokens received high attention
    at all three positions. We observe similar model behavior at different transformer
    layers with different text inputs. More plots are in Appendix [A](#A1 "Appendix
    A More Observation Plots ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implication. Even though small differences exist, repetitive attention patterns
    are evident in the attention maps. There exist specific tokens that keep receiving
    high attention. Meanwhile, these attention maps show sparsity: only a few tokens
    have high attention scores.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The Persistence of Importance Hypothesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The repetitive attention pattern suggests that specific tokens are influential
    throughout the sequence. A stricter claim is that these tokens are the only ones
    that could be significant for a future step. Thus, we articulate the *persistence
    of importance hypothesis*.
  prefs: []
  type: TYPE_NORMAL
- en: The Persistence of Importance Hypothesis. *With a trained autoregressive language
    model, only pivotal tokens, which had a substantial influence at one previous
    step, will have a significant influence at a future step.*
  prefs: []
  type: TYPE_NORMAL
- en: If true, this hypothesis indicates the possibility of foreseeing what information
    in the previous sequences could be vital for future steps. This hypothesis is
    trivial when pivotal tokens include all tokens in the entire sentences. However,
    a much more interesting case is when pivotal tokens are a subset of previous words.
    This would enable us to reduce the size of the KV cache by throwing away the embedding
    of non-important tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6bd22fa047f7060f64d8f84bb9a2b2a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Persistence Ratio
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8970ae54d855e6b06c059bd3c2b76585.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Size of $S_{0\rightarrow t}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: In this figure, we plot the persistence ratio and the corresponding
    size of the pivotal token set. The persistence ratio is over 95% in most of the
    layers, with decreases at the later layers. Meanwhile, the number of pivotal tokens
    is considerably smaller than the sequence length. This suggests that the pivotal
    tokens of later half sentences are almost all included in the set of first halves.'
  prefs: []
  type: TYPE_NORMAL
- en: Pivotal Token. One natural indication of a token’s influence is the attention
    score. We consider a token pivotal for position $t$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S_{a\rightarrow b}=\cup^{t=b}_{t=a}S_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Verification. We measure *persistence ratio* as an empirical test the hypothesis.
    *Persistence ratio* measures how many tokens in the pivotal token sets of the
    later part of the sentence are also in the pivotal token sets of the initial part
    of the sentence. Let $l$. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: At the same time, we measure $\frac{|S_{0\rightarrow t}|}{t}$, which suggests
    an average score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result. We present our main results in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 The
    Persistence of Importance Hypothesis ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time"). First, given the current criterion of pivotal
    token and $t$ is considerably smaller than half of the sentence length. This verifies
    that we are not considering the trivial case of our hypothesis. Second, the persistence
    ratio is generally over 95%, with dips in the later transformer layers. The pivotal
    token set of the later half sentences is mostly included in the set of the first
    half sentences. Combining these two pieces of empirical evidence, we see positive
    evidence for our hypothesis test.'
  prefs: []
  type: TYPE_NORMAL
- en: Implication. The hypothesis provides insights for understanding the behavior
    of LLMs and opens up new opportunities for reducing the KV cache memory. The hypothesis
    suggests the possibility of predicting the potentially influential tokens for
    future steps. The non-influential tokens are unnecessary to store in the memory,
    as they are unlikely to have high attention scores. This reduces the number of
    tokens stored in the KV cache and the computation required at the attention.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Attention Weights Decides the Pivotal Tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we verified that the significant tokens would continue
    to be significant. In this section, we try to understand the reasons for such
    phenomena. We consider the token generation process of a simplified model: a single-layer
    transformer model with single-head attention.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: $x_{t}\in\mathbb{R}^{1\times d}$ denotes the MLP block following attention block,
    a two-layer MLP with skip connections, given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: We are interested in the attention scores $\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot
    x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})$
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $A=W_{V}W_{O}W_{Q}W_{K}^{\top}$, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'The proof is provided in Appendix [B](#A2 "Appendix B Proofs ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time"). Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.3 Attention Weights
    Decides the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time") shows that under an assumption on the MLP in ([2](#S3.E2 "In 3.3
    Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")), for all $x_{\ell}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our theorem shows that the property in Equation ([3](#S3.E3 "In Theorem 3.1\.
    ‣ 3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")) property only holds for $x_{\ell}$
    as a pivotal token. Each attention is learned to identify some subspace. Only
    those tokens embedded inside these regions are pivotal for this attention. This
    would explain why only some specific tokens are always relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Sequential Token Generation Under budget
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 Inference with Budget KV cache
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Memory Budget $B$end while'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Compress KV Cache
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Key Cache $\bar{\mathcal{K}}\in\mathbf{R}^{n\times d}$'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we present Scissorhands, which reduces the KV cache memory
    from the sequence length dimension without fine-tuning the model. In Section [4.1](#S4.SS1
    "4.1 Budget KV Cache for Single Attention Head ‣ 4 Sequential Token Generation
    Under budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"), we describe how Scissorhands maintains
    the KV cache under a given budget. Section [4.2](#S4.SS2 "4.2 Theoretical Analysis.
    ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time") provides
    a theoretical analysis of the algorithm and the approximation error.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Budget KV Cache for Single Attention Head
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, for the sake of the discussion, we drop the layer number notation
    $i$ can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Intuition. As shown in Section [3](#S3 "3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time"), the attention scores $\alpha_{t,i}$ follow a
    strong power-law distribution. For the autoregressive generation process, if there
    exists an oracle such that we can identify the heavy score tokens before the future
    generation step, then the memory of the KV cache can be significantly reduced
    by only storing the heavy score tokens. Fortunately, the persistence of importance
    hypothesis provides us with such an oracle. It states that only historical tokens
    with significant contributions toward previous generated tokens will have significant
    contributions toward future tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges. LLMs are deployed on hardware with a fixed memory. The algorithm
    should maintain the cache under fixed memory to meet the hard requirement. Further,
    LLMs are already computationally intensive. The algorithm should avoid introducing
    much extra burden on computation.
  prefs: []
  type: TYPE_NORMAL
- en: A fixed memory budget for one attention head is $B$ previous tokens. We describe
    the problem as follows,
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4.1  (Sequential generation at an attention head under budget $B$).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a stream of token embedding, including prompt and previously generated
    tokens, denotes their input to the head as $\{x_{1},\ldots,x_{t},\ldots\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approach. Inspired by the textbook solution of reservoir sampling and the Least
    Recent Usage cache replacement algorithm, Scissorhands reserves a fixed memory
    buffer for the KV cache. When the buffer is full, Scissorhands drops stored but
    non-influential tokens from the cache. Naturally, attention scores are used as
    indicators of influence. We present the main algorithm in Algorithm [1](#alg1
    "Algorithm 1 ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time") and Algorithm [2](#alg2 "Algorithm 2 ‣ 4 Sequential Token Generation Under
    budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time"). The influence measure is collected over
    a history window to reduce variance. And recent tokens are always kept because
    of the lack of information on their importance. In practice, $w$ in all our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: With a sampled KV cache, attention output can be computed by the following estimator
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Overhead Tradeoff At the compression step, an extra attention computation is
    introduced to collect the importance measurements over a history window. However,
    such compression is not required at every generation step. $m$ in our experiment.
    Further, steps after the compression have reduced attention computation because
    of the reduction in the KV cache. On the other hand, one can trade a tiny amount
    of memory to avoid the overhead by maintaining the importance record during generation
    steps in Algorithm [1](#alg1 "Algorithm 1 ‣ 4 Sequential Token Generation Under
    budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Allocating Budgets Across Attention Heads. An LLM typically consists of $L$
    heads. A total memory budget has to be distributed over layers and heads. Within
    each transformer layer, the budget is distributed evenly across heads. Within
    the entire model, we distributed the budget according to Figure [2](#S3.F2 "Figure
    2 ‣ 3.2 The Persistence of Importance Hypothesis ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"). The rule of thumb is to allocate
    more budget to later layers to compensate for the lower persistence ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Theoretical Analysis.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We study how much the tokens generated by the compressed KV cache deviate from
    the tokens generated by the original transformer using our simplified model in
    ([1](#S3.E1 "In 3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence
    of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")). Let $\{\tilde{x}_{t}\}_{t=0}^{T}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Notice that when $m=1$ tokens. If the ranking of the attention scores does
    not change in each iteration, Algorithm [2](#alg2 "Algorithm 2 ‣ 4 Sequential
    Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") will always drop tokens
    with the smallest attention scores.'
  prefs: []
  type: TYPE_NORMAL
- en: For reference purposes, let $\{x_{t}\}_{t=0}^{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\lambda_{1},\lambda_{2}$ in ([2](#S3.E2 "In 3.3 Attention Weights Decides
    the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")). Let'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and assume that each $\beta_{t,j}=cv_{t,j}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'The definition of $\beta_{t,j}$ depends on the distribution that the attention
    scores are fitted to and is always less than one. With a strong power-law distribution,
    this term provides a further decrease to the error bound in ([4](#S4.E4 "In Theorem
    4.1\. ‣ 4.2 Theoretical Analysis. ‣ 4 Sequential Token Generation Under budget
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5c767dd741c32213f1e9c92639ec5c03.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Language Modeling
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5283f95c7ca3e73190da32ae7a53896d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) OPT-6B Five shot
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0f96821cfd748671e212dd25f52a02b.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) OPT-13B Five shot
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ebccccf90225bccac2ae15344e628e0.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) OPT-30B Five shot
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: This figure shows the accuracy trend of Scissorhands on language
    modeling dataset and downstream tasks with different KV cache compression. In
    general, Scissorhands incurs no accuracy drop until $5\times$ compression on OPT-66B.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Empirical Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present the results that demonstrate Scissorhands achieves
    up to 5$\times$ reduction in the KV cache memory compared to the standard model
    with no accuracy loss. We also show that Scissorhands is compatible with 4-bit
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment Setting. We compare the accuracy of Scissorhands-OPT against the
    original OPT on one language model datasets C4 [[22](#bib.bib22)] and a number
    of few-shot downstream tasks: Hellaswag [[26](#bib.bib26)], MathQA [[27](#bib.bib27)],
    PIQA [[28](#bib.bib28)], Winogrande [[29](#bib.bib29)]. We use lm-eval-harness [[30](#bib.bib30)]
    to evaluate few-shot tasks. Our experiments are conducted on NVIDIA 4 A100 40GB
    GPU servers.'
  prefs: []
  type: TYPE_NORMAL
- en: No Accuracy Drop untill 5$\times$ compression for OPT-66B. Similar to the language
    modeling setting, Scissorhands performs better at larger models. Generally, accuracy
    is maintained with 15% - 30% of the original KV cache size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Applying 4-bit quantization on top of Scissorhands on Hellaswag.'
  prefs: []
  type: TYPE_NORMAL
- en: '| OPT-6B |'
  prefs: []
  type: TYPE_TB
- en: '| Original | Scissorhands | Scissorhands+ 4-bit |'
  prefs: []
  type: TYPE_TB
- en: '| 0.702 | 0.706 | 0.704 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Original | Scissorhands | Scissorhands+ 4-bit |'
  prefs: []
  type: TYPE_TB
- en: '| 0.720 | 0.720 | 0.720 |'
  prefs: []
  type: TYPE_TB
- en: 'Compatible with 4-bit Quantization We test the compatibility of quantization
    and Scissorhands at $2\times$ compression. We adopt 4-bit quantization following [[7](#bib.bib7)].
    Even Hellaswag is most sensitive based on Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Theoretical
    Analysis. ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time"), adding quantization doesn’t introduce compounded errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07932f92ed79bc7ef99d49298b56e348.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Score between OPT and Scissorhands.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation on Attention Score Error. We present the change ratio in attention
    score between original OPT-13B and Scissorhands OPT-13B at $3\times$. Thus, Scissorhands
    gives the most similar output as the original model at all layers.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion, Limitation, and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We discover repetitive attention patterns given trained language models. One
    interesting question that needs to be answered is whether such behavior is a model
    architecture bias or an unexpected training outcome. For such purpose, we perform
    the same experiment with a randomly initialized OPT, and compare it against the
    results presented in Section [3.1](#S3.SS1 "3.1 Repetitive Attention Pattern.
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"). As shown
    in Figure [5](#S6.F5 "Figure 5 ‣ 6 Discussion, Limitation, and Future Work ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time"), the repetitive attention pattern does not exist in randomly initialized
    models. Apart from an efficiency deployment perspective, could such repetitive
    attention patterns contribute to some known problem in language generation such
    as repetitions? It may be worth investigating the relationship between repetitive
    attention patterns and undesired generations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c840df93a7f7410b75e4de05b7126c2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention map of the token at position 178
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ecc78c7433806556bc0eec60e18e5a42.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention map of the token at position 228
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: We plot the attention map corresponding to Section [3.1](#S3.SS1
    "3.1 Repetitive Attention Pattern. ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time") but with a randomly initialized OPT. We observe
    no repetitive attention for a randomly initialized model.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limitation of the server in academics, the largest model we can fit
    is OPT-66B. We try to understand the behavior and verify the generality across
    the different models and datasets. However, we cannot access the training process
    and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments
    with the large model create carbon dioxide emissions. However, our work improves
    the efficiency of LLM, and we foresee no negative impacts.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by our intriguing findings that pivotal tokens exert a lasting influence
    on future steps, we developed Scissorhands to leverage this observation to reduce
    the memory usage of KV cache. Our method achieves memory reductions of $5\times$
    in the KV cache without compromising the performance of LLMs. Furthermore, we
    demonstrate the compatibility of Scissorhands with quantization techniques, opening
    up the possibility of reducing memory usage in both the representation and sequence
    length dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
    Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
    et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What
    makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K
    Singh, Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional
    properties drive emergent in-context learning in transformers. In Advances in
    Neural Information Processing Systems, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
    Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y.
    Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang,
    Christopher Ré, Ion Stoica, and Ce Zhang. High-throughput generative inference
    of large language models with a single gpu, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee,
    and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale
    generative language models. arXiv preprint arXiv:2206.09557, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Elias Frantar and Dan Alistarh. Massive language models can be accurately
    pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati,
    Katrin Kirchhoff, and Dan Roth. Rethinking the role of scale for in-context learning:
    An interpretability-based case study at 66 billion scale. arXiv preprint arXiv:2212.09095,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] OpenAI. Gpt-4 technical report, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. In 8th International Conference on Learning Representations, ICLR
    2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
    Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li,
    Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable
    lsh framework for efficient neural network training. In International Conference
    on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
    Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information
    Processing Systems, 34:17413–17426, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking
    attention with performers. In 9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
    Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer
    language models, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a
    suit of armor conduct electricity? a new dataset for open book question answering.
    In EMNLP, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth
    AAAI Conference on Artificial Intelligence, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin.
    Winogrande: An adversarial winograd schema challenge at scale. In Communications
    of the ACM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason
    Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy
    Zou. A framework for few-shot language model evaluation. In Version v0\. 0.1\.
    Sept. Zenodo, September 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A More Observation Plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Repetitive Attention Pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We provide the attention map similar to Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time") but from a different transformer layer on the
    same text in Figure [6](#A1.F6 "Figure 6 ‣ A.1 Repetitive Attention Pattern ‣
    Appendix A More Observation Plots ‣ Scissorhands: Exploiting the Persistence of
    Importance Hypothesis for LLM KV Cache Compression at Test Time"), Figure [7](#A1.F7
    "Figure 7 ‣ A.1 Repetitive Attention Pattern ‣ Appendix A More Observation Plots
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time"), Figure [8](#A1.F8 "Figure 8 ‣ A.1 Repetitive
    Attention Pattern ‣ Appendix A More Observation Plots ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time") and Figure [9](#A1.F9 "Figure 9 ‣ A.1 Repetitive Attention Pattern ‣ Appendix
    A More Observation Plots ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time"). A repetitive pattern and
    attention sparsity can be observed across layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a398efb25a424f20aa2202cd17e300ed.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention map at position 178
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e617d2f657862d153e2cccf2badacff.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention map at position 228
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3d0bcabd8edb13b8ccf3b18aebb8b83.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attention map at position 278
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Attention Map at Layer 5'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e46d5afc656fd0ab919b02eb706cb26.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention map at position 178
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d55f138e2d7a6b096da565582c22de39.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention map at position 228
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83c0081ce52cbf3a8b944c722da63f75.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attention map at position 278
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Attention Map at Layer 10'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95ca115856694b0972de01ba95d02373.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention map at position 178
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57a3a2d7d686ab9061893b86ebeead42.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention map at position 228
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f9bad591e2641c922f20a046fd398958.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attention map at position 278
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Attention Map at Layer 15'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bba20c9bd16369ae2e5b6bd4ffec9ff.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention map at position 178
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e9e3f44e495498ebe49e971cc6eef46.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Attention map at position 228
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ad2ecaa8a3ca4ea9e555b3fb9669273.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Attention map at position 278
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Attention Map at Layer 20'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Cross Layer Cosine Similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [3.3](#S3.SS3 "3.3 Attention Weights Decides the Pivotal Tokens
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"), our analysis
    assumes a large cosine similarity between the input and output of $\mathcal{F}$
    is extremely high.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee769d6ae8a125b9dacaac59af14ae4a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Cosine Similarity
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b6593d0e98fc63e90d3fb73e98d5d02.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Norm Comparision
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: $x$ is high in cosine similarity'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Proofs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'B.1 Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.3 Attention Weights
    Decides the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We consider the token generation process of a simplified model: a single-layer
    transformer model with single-head attention.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: $x_{t}\in\mathbb{R}^{1\times d}$ denotes the MLP block following attention block,
    a two-layer MLP with skip connections, given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'We are interested in the attention scores $\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot
    x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})$. We first re-state the Theorem [3.1](#S3.Thmtheorem1
    "Theorem 3.1\. ‣ 3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence
    of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") below.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem B.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $A=W_{V}W_{O}W_{Q}W_{K}^{\top}$, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: As a preparation of the proof, we first show two lemmas.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $x_{1},x_{2}\in\mathbb{R}^{1\times m}$ we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left&#124;x_{1}y^{\top}-x_{2}y^{\top}\right&#124;\leq\sqrt{2\delta}\left\&#124;y\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $x_{2}=x_{2}^{\parallel}+x_{2}^{\perp}$ where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle x_{2}^{\parallel}=x_{1}x_{2}^{\top}\cdot x_{1};\quad x_{2}^{\perp}=x_{2}-x_{2}^{\parallel}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then it is easy to see that $x_{2}^{\perp}x_{1}^{\top}=0$. By the Pythagorean
    Theorem, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;x_{2}^{\perp}\right\&#124;_{2}^{2}=\left\&#124;x_{2}\right\&#124;_{2}^{2}-\left\&#124;x_{2}^{\parallel}\right\&#124;_{2}^{2}=\delta(2-\delta)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;x_{1}-x_{2}\right\&#124;_{2}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\left\&#124;\left(1-x_{1}x_{2}^{\top}\right)x_{1}-x_{2}^{\perp}\right\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\left(1-x_{1}x_{2}^{\top}\right)^{2}+\left\&#124;x_{2}^{\perp}\right\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=2\delta$ |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, the Cauchy-Schwarz inequality implies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left&#124;x_{1}y^{\top}-x_{2}y^{\top}\right&#124;\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}\cdot\left\&#124;y\right\&#124;_{2}=\sqrt{2\delta}\left\&#124;y\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\ell\in[t]$. Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a_{t}=\alpha_{t}X_{t-1}W_{V}W_{O}=\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right)W_{V}W_{O}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left&#124;a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\alpha_{t,\ell}x_{\ell}Ax_{\ell}^{\top}\right&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}\left&#124;x_{j}Ax_{\ell}^{\top}\right&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\epsilon x_{\ell}Ax_{\ell}^{\top}\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\epsilon x_{\ell}Ax_{\ell}^{\top}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where in the second inequality we use $\epsilon^{-1}\left|x_{j}Ax_{\ell}^{\top}\right|\leq
    x_{\ell}Ax_{\ell}^{\top}$. This implies that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Now we proceed to the main body of the proof. Assume that $\left\|x_{\ell}\right\|_{2}=1$,
    then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Recall that $\lambda_{Q},\lambda_{K}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Notice that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;a_{t}\right\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{O}\lambda_{V}\left\&#124;\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{O}\lambda_{V}\sum_{j=1}^{t-1}\alpha_{t,j}\left\&#124;x_{j}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\lambda_{O}\lambda_{V}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Then since $\delta\leq\left(\frac{c\epsilon}{\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}}\right)^{2}$,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Since by Lemma ([B.2](#A2.Thmlemma2 "Lemma B.2\. ‣ B.1 Proof of Theorem 3.1
    ‣ Appendix B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: It must hold that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Since $x_{\ell}^{\top}ax_{\ell}\geq c$, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{2c\epsilon}{\left\&#124;a_{t}\right\&#124;_{2}}\leq\frac{2\epsilon}{\left\&#124;a_{t}\right\&#124;_{2}}x_{\ell}^{\top}ax_{\ell}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which implies that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'B.2 Proof of Theorem [4.1](#S4.Thmtheorem1 "Theorem 4.1\. ‣ 4.2 Theoretical
    Analysis. ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let $\{\tilde{x}_{t}\}_{t=0}^{T}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Notice that when $m=1$ tokens. If the ranking of the attention scores does
    not change in each iteration, Algorithm [2](#alg2 "Algorithm 2 ‣ 4 Sequential
    Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") will always drop tokens
    with the smallest attention scores.'
  prefs: []
  type: TYPE_NORMAL
- en: For reference purposes, let $\{x_{t}\}_{t=0}^{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem B.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $\lambda_{1},\lambda_{2}$ in ([6](#A2.E6 "In B.1 Proof of Theorem 3.1 ‣
    Appendix B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")). Let'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: and assume that each $\beta_{t,j}=cv_{t,j}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Define $m_{k,j}=\mathbb{I}\left\{j\in S_{t}\right\}$ can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Our first lemma shows the Lipschitzness of the attention module.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider two sequences of tokens $\{x_{i}\}_{i=1}^{t}$. Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can decompose the difference as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By the Lipschitzness of softmax, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\left\&#124;x_{t}W_{Q}W_{K}^{\top}\left(X_{t-1}-Y_{t-1}\right)^{\top}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\&#124;x_{t}\right\&#124;_{2}\left\&#124;X_{t-1}-Y_{t-1}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Since $\left\|x_{t}\right\|_{2}=1$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\left\&#124;(x_{t}-y_{t})W_{Q}W_{K}^{\top}Y_{t-1}^{\top}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\&#124;Y_{t-1}\right\&#124;_{F}\left\&#124;x_{t}-y_{t}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Since $\left\|x_{t}-y_{t}\right\|_{2}=\Delta_{t}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Combining the two bounds gives
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Our second lemma shows the difference between the output of the sampled and
    vanilla transformer when the input is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\tilde{a}_{t}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: Assume that $\left\|x_{j}\right\|_{2}=1$. Then we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\&#124;\tilde{a}_{t}-b_{t}\right\&#124;_{2}\leq\lambda_{V}\lambda_{O}\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A direction computation yields
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, $\left\|\tilde{a}_{t}-b_{t}\right\|_{2}$ can be bounded as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: since $\left\|\tilde{x}_{j}\right\|_{2}=1$ can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Furthermore, for all $j\notin\hat{S}_{t}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tilde{\alpha}_{t,j}=\frac{\exp\left(r_{t,j}\right)}{\sum_{i\in\hat{S}_{t}}\exp\left(r_{t,i}\right)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, for all $j\in\hat{S}_{t}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\beta_{t,j}-\tilde{\alpha}_{t,j}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\tilde{\alpha}_{t,j}\sum_{i\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, the bound of $\left\|\tilde{a}_{t}-b_{t}\right\|_{2}$ can be written
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the last equality follows from $\sum_{j\in\hat{S}_{t}}\tilde{\alpha}_{t,j}=1$.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last lemma shows the Lipschitzness of the MLP in ([6](#A2.E6 "In B.1 Proof
    of Theorem 3.1 ‣ Appendix B Proofs ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time")).'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma B.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\lambda_{1},\lambda_{2}$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\&#124;\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\&#124;\leq\left(1+\lambda_{1}\lambda_{2}\right)\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Direct computation yields
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}+\lambda_{2}\left\&#124;W_{1}\left(x_{1}-x_{2}\right)\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}+\lambda_{1}\lambda_{1}\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(1+\lambda_{1}\lambda_{2})\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where in the third inequality we use the fact that relu$(\cdot)$-Lipschitz.
    ∎
  prefs: []
  type: TYPE_NORMAL
- en: Now we turn to the proof of our main theorem. Combining all of the results,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a_{t}-\tilde{a}_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore, by triangle inequality, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\&#124;a_{t}-\tilde{a}_{t}\right\&#124;_{2}\leq\left\&#124;\mathcal{T}_{1}\right\&#124;_{2}+\left\&#124;\mathcal{T}_{2}\right\&#124;_{2}+\left\&#124;\mathcal{T}_{3}\right\&#124;_{2}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: To start, the magnitude of $\mathcal{T}_{1}$ can be bounded as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;\mathcal{T}_{1}\right\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\sum_{j=1}^{t-1}\alpha_{t,j}(x_{t,j}-\tilde{x}_{t,j})\right\&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\sum_{j=1}^{t-1}\alpha_{t,j}\left\&#124;x_{t,j}-\tilde{x}_{t,j}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\Delta_{t}\sum_{j=1}^{t-1}\alpha_{t,j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\lambda_{V}\lambda_{O}\Delta_{t}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where in the third inequality we use $\left\|x_{t,j}-\tilde{x}_{t,j}\right\|_{2}=\Delta_{t}$
    to get that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;\mathcal{T}_{2}\right\&#124;_{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\left(\sum_{j=0}^{t-1}(\alpha_{t,j}-\beta_{t,j})\tilde{x}_{j}\right)\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\sum_{j=0}^{t-1}\left&#124;\alpha_{t,j}-\beta_{t,j}\right&#124;\left\&#124;\tilde{x}_{j}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\alpha_{t}-\beta_{t}\right\&#124;_{1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\sqrt{t-1}\lambda_{V}\lambda_{O}\left\&#124;\alpha_{t}-\beta_{t}\right\&#124;_{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq 2\left(1-\frac{1}{t}\right)\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}\Delta_{t}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Lastly, to bound the magnitude of $\mathcal{T}_{3}$, we use Lemma [B.4](#A2.Thmlemma4
    "Lemma B.4\. ‣ B.2 Proof of Theorem 4.1 ‣ Appendix B Proofs ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time") to get that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left\&#124;\mathcal{T}_{3}\right\&#124;_{2}\leq 2\lambda_{V}\lambda_{O}\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Putting things together for ([10](#A2.E10 "In B.2 Proof of Theorem 4.1 ‣ Appendix
    B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time")), we have'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'By Lemma [B.5](#A2.Thmlemma5 "Lemma B.5\. ‣ B.2 Proof of Theorem 4.1 ‣ Appendix
    B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time") we can further show that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'By Theorem [B.3](#A2.Thmtheorem3 "Theorem B.3\. ‣ B.3 Budgeted Cache ‣ Appendix
    B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time"), we have that with probability at least
    $1-T_{\max}\exp\left(-\frac{\epsilon^{2}b^{2}(T_{\min}-1)}{(k-2)^{2}(u-b)^{2}}\right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-\nicefrac{{B}}{{T_{\max}}})^{2}}{(1-\epsilon)^{2}}\right)$
    that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Given that $\mathbb{E}\left[\left\|x_{t}-\tilde{x}_{t}\right\|\right]\leq 2\Delta_{\max}$,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\left[\left\&#124;x_{t+1}-\tilde{x}_{t+1}\right\&#124;_{2}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq 4\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\Delta_{\max}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, as long as $\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\leq\frac{1}{2}$,
    we can guarantee that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}\left[\left\&#124;x_{t+1}-\tilde{x}_{t+1}\right\&#124;_{2}\right]\leq
    2\Delta_{\max}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Thus, for all $t\in[T_{\min},T_{\max}]$, we have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: B.3 Budgeted Cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Theorem B.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $\beta_{t,j}$ that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: We consider the case of maintaining a budget of $B$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle c=\left(\int_{0}^{u-b}(x+b)^{-k}dx\right)^{-1}=\frac{k-1}{b^{1-k}-u^{1-k}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: To start, we notice that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\int x(x+b)^{-k}=-\frac{(x+b)^{1-k}((k-1)x+b)}{(k-1)(k-2)}:=g(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Let $C=\sum_{j=1}^{t-1}v_{j}$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\left[C\right]=(t-1)\mathbb{E}\left[v_{1}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=(t-1)\frac{k-1}{b^{1-k}-u^{1-k}}(g(u)-g(0))$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Let $\Delta=\frac{b^{2-k}-(k-1)u^{2-k}+(k-2)bu^{1-k}}{b^{1-k}-u^{1-k}}$. By
    Hoeffding’s inequality, we have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{P}\left(C\leq(1-\epsilon)\mathbb{E}\left[C\right]\right)\leq\exp\left(-\frac{2\epsilon^{2}\mathbb{E}\left[C\right]^{2}}{(t-1)(u-b)^{2}}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This implies that with probability at least $1-\exp\left(-\frac{2\epsilon^{2}\Delta^{2}(t-1)}{(k-2)^{2}(u-b)^{2}}\right)$
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C\geq(1-\epsilon)\Delta\frac{t-1}{k-2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Now, we proceed to bound $\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$. Its expectation
    is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\left[C^{-1}\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}v_{j}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{k-2}{\Delta(1-\epsilon)}\cdot\frac{k-1}{b^{1-k}-u^{1-k}}\int_{0}^{\gamma}x(x+b)^{-k}dx$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{(k-1)(k-2)}{\Delta(1-\epsilon)\left(b^{1-k}-u^{1-k}\right)}\left(g(\gamma)-g(0)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We pause here and study how small can we choose $\gamma$. Notice that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: By Hoeffding’s inequality again, we have that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Enforcing $\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}\geq T_{\max}-B$.
    Therefore
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: We further notice that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This gives
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}\left[C^{-1}\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}v_{j}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Notice that if $u\geq 5b$, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Delta=b-(k-1)\left(\frac{u}{b}\right)^{1-k}\cdot\frac{b-u}{b^{1-k}-u^{1-k}}\leq
    0.98b$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: holds with probability at least $1$2. Taking a union bound gives the desired
    result.
  prefs: []
  type: TYPE_NORMAL
