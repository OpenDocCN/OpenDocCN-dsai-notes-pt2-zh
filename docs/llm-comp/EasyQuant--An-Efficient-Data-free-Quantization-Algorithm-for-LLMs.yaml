- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02775](https://ar5iv.labs.arxiv.org/html/2403.02775)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hanlin Tang
  prefs: []
  type: TYPE_NORMAL
- en: ranchotang@tencent.com &Yifu Sun
  prefs: []
  type: TYPE_NORMAL
- en: yifusun@tencent.com &Decheng Wu
  prefs: []
  type: TYPE_NORMAL
- en: woodchenwu@tencent.com \ANDKai Liu
  prefs: []
  type: TYPE_NORMAL
- en: raccoonliu@tencent.com &Jianchen Zhu
  prefs: []
  type: TYPE_NORMAL
- en: dickzhu@tencent.com &Zhanhui Kang
  prefs: []
  type: TYPE_NORMAL
- en: kegokang@tencent.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have proven to be very superior to conventional
    methods in various tasks. However, their expensive computations and high memory
    requirements are prohibitive for deployment. Model quantization is an effective
    method for reducing this overhead. The problem is that in most previous works,
    the quantized model was calibrated using a few samples from the training data,
    which might affect the generalization of the quantized LLMs to unknown cases and
    tasks. Hence in this work, we explore an important question: Can we design a data-free
    quantization method for LLMs to guarantee its generalization performance?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we propose EasyQuant, a training-free and data-free weight-only
    quantization algorithm for LLMs. Our observation indicates that two factors: outliers
    in the weight and quantization ranges, are essential for reducing the quantization
    error. Therefore, in EasyQuant, we leave the outliers (less than $1\%$) unchanged
    and optimize the quantization range to reduce the reconstruction error. With these
    methods, we surprisingly find that EasyQuant achieves comparable performance to
    the original model. Since EasyQuant does not depend on any training data, the
    generalization performance of quantized LLMs are safely guaranteed. Moreover,
    EasyQuant can be implemented in parallel so that the quantized model could be
    attained in a few minutes even for LLMs over 100B. To our best knowledge, we are
    the first work that achieves comparable performance with data-dependent algorithms
    under a data-free setting and our algorithm runs over 10 times faster than the
    data-dependent methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/deb1964c909a1759356337559990476b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Pipeline of EasyQuant. We first find all the outliers in weight and
    keep them in full precision (fp32/fp16/bf16). Afterward, we optimize the quantization
    range (denoted as $q_{range}$) with optimized quantization ranges and we set the
    outliers unchanged in weight.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent work has already proved the superior performance of Transformer (Vaswani
    et al., [2017](#bib.bib23)) based LLMs (Workshop, [2023](#bib.bib25); Zhang et al.,
    [2022](#bib.bib31); Touvron et al., [2023](#bib.bib22); Brown et al., [2020](#bib.bib2);
    Rae et al., [2021](#bib.bib17); Smith et al., [2022](#bib.bib19); Chowdhery et al.,
    [2022](#bib.bib3); Zeng et al., [2022](#bib.bib30)) on various tasks over traditional
    methods, and has attracted massive interest in how to improve and utilize those
    LLMs. However, the model size also grows dramatically along with improved performance.
    Hence the memory footprint and computational cost become the bottleneck for deploying
    those models. One promising solution to alleviate this overhead is model quantization (Frantar
    et al., [2023a](#bib.bib6); Xiao et al., [2023](#bib.bib27)), where we quantize
    weight only or weight and activation both i order to reduce memory consumption
    and computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Although model quantization is a well-studied area for normal-sized models,
    such as BERT (Devlin et al., [2018](#bib.bib5)) and GPT-2 (Radford et al., [2019](#bib.bib16)),
    it is still a quite challenging task for LLMs. One major reason is that previous
    lossless model quantization algorithms require retraining for the quantized model,
    which is too expensive for models over billions of parameters. Beyond this, previous
    models are usually designed for specific domain tasks, which means the training
    data are sampled from limited task domains. However, recent LLMs are usually trained
    on various domains of data corpus, and they have shown to be quite effective for
    multi-domain zero-shot tasks. In this case, if we only retrain the quantized LLMs
    using partial domain corpus, the generalization ability of LLMs might get worse.
    Therefore both efficiency and generalization guarantees are very important for
    designing LLMs quantization algorithms. To date, for low-bits weight-only quantization,
    several post-training algorithms have been proposed  (Frantar et al., [2023a](#bib.bib6);
    Yao et al., [2022](#bib.bib28)). However, those methods also require a small calibration
    set sampled from training data, which still takes at least several hours. Moreover,
    the use of those calibration data also brings the risk of making the model overfit
    to the calibration set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Contribution:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this work, we propose a novel data-free model quantization algorithm, namely
    EasyQuant, that potentially improves the performance of low-bits quantized LLMs.
    The generalization ability of LLMs is inherently guaranteed since EasyQuant does
    not need any input data. By running EasyQuant for only a few minutes, we can quantize
    public-available OPT-176B, BLOOM-176B, and LLAMA-65B into lower bits without significant
    loss on various benchmarks. To our best knowledge, this is the first data-free
    LLM quantization algorithm for LLM quantization without notable system overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, our work reveals the essential factors that cause the performance
    degradation of the quantized LLMs. We show that the outliers in weights are more
    critical to the model’s performance compared to the normal elements. Beyond this,
    we propose to use a gradient-based method for optimizing the quantization range.
    These two strategies can also be used in other scenarios, such as weight-activation
    quantization and quantization-aware training (QAT).
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, we develop efficient CUDA kernels for outlier isolation
    in dequantization, and proved that hold $1\%$ minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ebf7f263f0f9cb75a71b538eb6b8de12.png)![Refer to caption](img/b31a2c39ae3e29d97ca1affda3508261.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Smaller reconstruction error cannot guarantee a better model performance.
    Straightforwardly shrinking the quantization ranges will clip most of the outliers
    to be very small, hence the perplexity increases severely since those outliers
    are critical for preserving the model’s performance. However, when keeping those
    outliers unquantized, the quantized model achieves a better performance as the
    reconstruction error decreases continuously. This result clearly suggests that
    the outliers are more important than the normal values in weight, and optimizing
    the quantization ranges using gradient defined in ([2](#S3.Ex4 "In 3.1 The quantization
    range can be efficiently optimized using gradient ‣ 3 Insight behind EasyQuant
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs")) can significantly
    increase the accuracy of quantized models. More details about the experiment can
    be found in Section [5](#S5 "5 Experiment ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background and Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most widely used quantization method, namely rounding to nearest-number
    (RTN), quantizes a tensor $\bm{x}$-bits representation according to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q[\bm{x}]=s\times\left\lfloor\text{clamp}\left(\frac{\bm{x}}{s},l_{\min},l_{\max}\right)\right\rceil$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here $s$.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major directions for finding the best configuration in weight-only
    LLM quantization. The first is to minimize the reconstruction error of the weight
    parameter (denoted as $W$), which is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle r(W):=\&#124;Q[W]-W\&#124;^{2}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Notice that in this case we only need to have access to the weight itself, therefore
    it is data-free.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond this, recent studies (Frantar et al., [2023a](#bib.bib6); Yao et al.,
    [2022](#bib.bib28)) propose to use the output error, defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle e(W)=\sum_{X\in\mathcal{D}}\left\&#124;Q[W]X-WX\right\&#124;^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{D}$ is a calibration set sampled from the original training
    data, for optimization. This regulation tries to mimic the outputs from the original
    model directly hence achieving a more promising result than reconstruction-based
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Data-dependent calibration might weaken the generalization ability of LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However, the performance gain from using calibration data might jeopardize the
    generalization of the quantized model, because it brings the risk of making the
    model overfit to the calibration set. For example, both ZeroQuant and GPTQ involve
    changing the original weight by training or OBS in order to minimize the output
    error, therefore the distribution of the weight’s parameters might deviate from
    the original. Since the calibration data is usually sampled from a few specific
    domains, the performance of the calibrated model on other tasks may not be guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: Data-free quantization is challenging, but very important
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although it’s more challenging to use the reconstruction error as a regulation
    because it can only optimize the quantized model indirectly, still it is a very
    important direction for researching because the generalization ability of the
    model is inherently guaranteed when using data-free quantization since it uses
    no training data. Therefore in this paper, we aim to answer the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we efficiently recover the performance of the quantized model without
    using any input data? In this work we propose EasyQuant, a data-free fast algorithm
    that could significantly improve the performance of quantized LLMs in a data-free
    setting, and more importantly, even outperforms the results from data-dependent
    quantization algorithms. Our experiments reveal that the performance gap of the
    lower bits (e.g. $4$-bits) quantized LLMs origins from two factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting the quantization range as the maximum absolute value of the weight induces
    a large reconstruction error for low-bits quantization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The outliers in the weight matrix, which account for less than $0.1\%$ of the
    parameters, impose a very important influence on the model’s performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In EasyQuant, we use quantization range minimization and outlier isolation to
    address these two challenges, and our results prove that EasyQuant achieves a
    significant improvement over RTN.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Insight behind EasyQuant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned above, the weight’s outliers and quantization ranges are essential
    to the quantized model’s performance. Below we present the supporting experiments
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The quantization range can be efficiently optimized using gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although the quantization operation itself is non-differentiable, the gradient
    of the reconstruction error ($\|Q[\bm{x}]-\bm{x}\|^{2}$ admits (see Section [4](#S4
    "4 Methodology ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for
    LLMs") for more details)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial\&#124;Q[\bm{x}]-\bm{x}\&#124;^{2}}{\partial
    s}=2\sum_{i}\left((Q[x_{i}]-x_{i})\left\lfloor\frac{x_{i}}{s}\right\rceil\right).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'With this gradient, the reconstruction error can be quickly minimized within
    hundreds of steps (see Figure [2](#S1.F2 "Figure 2 ‣ Our Contribution: ‣ 1 Introduction
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs") for more
    details). This result indicates that by shrinking the quantization range, most
    of the parameters in weight can be approximated more precisely. However, as shown
    in Figure [2](#S1.F2 "Figure 2 ‣ Our Contribution: ‣ 1 Introduction ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs"), the performance of the
    quantized weight gets even worse as the reconstruction error decreases. This is
    a very counter-intuitive result.'
  prefs: []
  type: TYPE_NORMAL
- en: Through in-depth analysis, we realized that when decreasing the quantization
    range, more salient parameters outside the quantization range would be clipped
    out. Although most of the weights get approximated more precisely as indicated
    by the decreased reconstruction error, the salient parameters are poorly represented.
    As the model performance drops severely in this case, we realized that those outliers
    are way more important than the normal elements for the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Outliers in weight are very important, but not sufficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Threshold $n$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PPL on WikiText2 | $11.37$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Isolating outliers in weight from quantization can increase the model’s
    performance. Here $n$) numbers being held unquantized, there is still a large
    gap to the baseline. This means isolating the outliers is not enough to fully
    recover the accuracy of quantized models.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we further discuss the influence of those outliers, we first provide
    a ($n\sigma$) outlier if
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left&#124;W_{i,j}-mean(W)\right&#124;\geq n*var(W),$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: where $mean(W)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question is: Can we hold those outliers unchanged and straightforwardly
    compress the normal elements into lower bits? Unfortunately, our result suggests
    that excluding the outliers from quantization solely is not enough. As shown in
    Table [1](#S3.T1 "Table 1 ‣ 3.2 Outliers in weight are very important, but not
    sufficient ‣ 3 Insight behind EasyQuant ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs"), the performance gap still exists even when we hold $1\%$
    numbers in fp16\. The problem is that if we keep too many numbers in fp16, the
    overhead of the dequantization kernel would also increase and result in a decreased
    overall throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 EasyQuant potentially improve the performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Section [3.1](#S3.SS1 "3.1 The quantization range can be efficiently
    optimized using gradient ‣ 3 Insight behind EasyQuant ‣ EasyQuant: An Efficient
    Data-free Quantization Algorithm for LLMs") and Section [3.2](#S3.SS2 "3.2 Outliers
    in weight are very important, but not sufficient ‣ 3 Insight behind EasyQuant
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs"), optimizing
    the quantization ranges directly reduces the model’s performance drops severely
    because of the clipped outliers. These key observations inspire us to design EasyQuant,
    in which we isolate the outliers from quantization first and then optimizing the
    quantization range for the remaining elements. As shown in the right part of Figure [2](#S1.F2
    "Figure 2 ‣ Our Contribution: ‣ 1 Introduction ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs"), with outliers being kept unquantized, the performance
    of the quantized model increases continuously under decreased reconstruction.
    This clearly proves we can potentially improve the performance of quantized LLMs
    with this strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '4.1 Driving of the gradient in  ([2](#S3.Ex4 "In 3.1 The quantization range
    can be efficiently optimized using gradient ‣ 3 Insight behind EasyQuant ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs"))'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s say the original scale $s$, which means
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\lfloor\frac{x}{s+\Delta s}\right\rceil=\left\lfloor\frac{x}{s}\right\rceil,\quad\text{if
    }\frac{x}{s}-\left\lfloor\frac{x}{s+\Delta s}\right\rceil\neq 0.5.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Therefore we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q_{s+\Delta s}[x]=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: this leads to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial Q[x]}{\partial s}=\frac{Q_{s+\Delta s}[x]-Q_{s}[x]}{\Delta
    s}=\left\lfloor\frac{x}{s}\right\rceil.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This gives us
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\frac{\partial\&#124;Q[\bm{x}]-\bm{x}\&#124;^{2}}{\partial
    s}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Algorithm description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In EasyQuant, for each weight $W$. Afterward, for the normal elements, we optimize
    the per-channel quantization range using an optimizer (in our case we use Adam
    for example) with gradients defined in ([2](#S3.Ex4 "In 3.1 The quantization range
    can be efficiently optimized using gradient ‣ 3 Insight behind EasyQuant ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs")). The final quantized
    weight from EasyQuant can be formulated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle Q^{EasyQuant}[W]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $Mask^{o}$ is a mask tensor defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Mask^{o}_{i,j}(W)=\left\{\begin{array}[]{rl}1&amp;\text{if
    }(i,j)\in I^{o}(W),\\ 0&amp;\text{if }(i,j)\notin I^{o}(W).\end{array}\right.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'The detailed description of EasyQuant is in Algorithm [1](#alg1 "Algorithm
    1 ‣ 4.2 Algorithm description ‣ 4 Methodology ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 EasyQuant
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Initialize: outlier threshold $n$ is defined in ([5](#S4.Ex18 "In 4.2 Algorithm
    description ‣ 4 Methodology ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Baselines:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare EasyQuant with several baselines in the INT4 quantization setting
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RTN: The model’s weights are naively quantized according to ([1](#S2.Ex1 "In
    2 Background and Motivation ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ZeroQuant: The algorithm proposed in Yao et al. ([2022](#bib.bib28)). Authors
    treat each layer as a small neural network and use the original as the teacher
    model to distill the quantized one. This is equivalently minimizing $\sum_{\bm{x}\in\mathcal{D}}\|f(W^{T};\bm{x})-f(W^{S};\bm{x})\|^{2}$
    is the quantized model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPTQ: This algorithm is proposed in Frantar et al. ([2023a](#bib.bib6)). Authors
    use the same objective function $\sum_{\bm{x}\in\mathcal{D}}\|f(W^{T};\bm{x})-f(W^{S};\bm{x})\|^{2}$
    as in ZeroQuant. But they utilize OBS for minimizing the loss function instead
    of using a gradient-based optimizer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Experiment Setup.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For all models, we set the outlier threshold $n\in[2.5,3]$ for LLAMA. We use
    symmetric quantization since the normal values are symmetrically distributed with
    the outliers being excluded. For a fair comparison, we use per-channel quantization
    for weight in all algorithms (which means each column shares one common quantization
    range).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As for the evaluation tasks, we mainly focus on perplexity-based tasks, as they
    are known to be particularly sensitive to model quantization  Frantar et al. ([2023b](#bib.bib7)).
    The perplexity tasks we include are WikiText2  (Merity et al., [2016](#bib.bib12)),
    Penn Treebank  (Marcus et al., [1994](#bib.bib11)) and C4  (Raffel et al., [2020](#bib.bib18)).
    The zero-shot tasks’ results are also provided, such as PIQA  (Tata and Patel,
    [2003](#bib.bib21)), ARC  (Boratko et al., [2018](#bib.bib1)) and StoryCloze  (Mostafazadeh
    et al., [2017](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since each weight can be quantized in parallel, therefore we use $8*$ mins for
    all models. We store the index and value for all outliers together with the quantized
    normal values. Our dequantization kernel is built using CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Perplexity-based Task |  |  | Perplexity-based Task |'
  prefs: []
  type: TYPE_TB
- en: '|  | WikiText2 | PTB | C4 |  | WikiText2 | PTB | C4 |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA–7B | fp16 | $5.68$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $6.29$ |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | $6.09$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 6.01 | 10.72 | 7.71 | EasyQuant | 4.34 | $8.45$ | 6.37 |'
  prefs: []
  type: TYPE_TB
- en: '| LLAMA–13B | fp16 | $5.09$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $5.53$ |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | $5.36$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 5.29 | 9.37 | 6.97 | EasyQuant | 3.98 | 9.61 | 6.30 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Perplexity results for LLAMA model family'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experiment Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We focus our study on LLM by quantizing the entire BLOOM, and LLAMA model families
    to 4-bit.
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity-base tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first study perplexity-based tasks. On LLaMA models, Table [2](#S5.T2 "Table
    2 ‣ Implementation. ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs") shows that EasyQuant outperforms GPTQ in most cases. For
    LLaMA-65B, GPTQ drops 4.21 points on PTB, performing worse than the 9 $\times$
    smaller full-precision 7B model, while EasyQuant still performs well on this task.
    On the other tasks, EasyQuant losing only 0.4–0.7 points. BLOOM shows a similar
    pattern (see Table [10](#A1.T10 "Table 10 ‣ Appendix A Appendix ‣ EasyQuant: An
    Efficient Data-free Quantization Algorithm for LLMs") in appendix): EasyQuant
    drops only 0.1-0.16 points on perplexity-based tasks. Notice that we observe a
    smaller gap between our method and GPTQ on C4\. It is mostly because, as a data-calibrated
    quantization method, GPTQ uses C4 dataset for calibrations.'
  prefs: []
  type: TYPE_NORMAL
- en: Zeroshot tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For most zero-shot tasks, EasyQuant achieves harmless performance with only
    0.1 %-0.52% accuracy drops as shown in Table [10](#A1.T10 "Table 10 ‣ Appendix
    A Appendix ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs")
    in appendix and outperforms GPTQ on most cases. Here we simply use the implementation
    of GPTQ on LLAMA from its git.¹¹1https://github.com/qwopqwop200/GPTQ-for-LLaMa
    We note that EasyQuant can be further improved via finer-granularity grouping.
    However, we will not include this overhead in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '| outlier ratio | overhead |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $0.01\%$ | 0.027ms |'
  prefs: []
  type: TYPE_TB
- en: '| $0.10\%$ | 0.055ms |'
  prefs: []
  type: TYPE_TB
- en: '| $0.50\%$ | 0.093ms |'
  prefs: []
  type: TYPE_TB
- en: '| $1\%$ | 0.117ms |'
  prefs: []
  type: TYPE_TB
- en: '| $5\%$ | 0.186ms |'
  prefs: []
  type: TYPE_TB
- en: '| $10\%$ | 0.212ms |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Overhead of outlier isolation on A100'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Latency.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the overhead of EasyQuant by comparing the overhead of outlier
    isolation, int$4$ms. Therefore from Table [3](#S5.T3 "Table 3 ‣ Zeroshot tasks.
    ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs") we can see that recovering the outliers in weight brings
    almost no overhead to the overall latency.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation study.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To understand the effect of unstructured outliers, we show the perplexity result
    of EasyQuant without outlier isolation or quantization range optimization. As
    discussed in Section [3](#S3 "3 Insight behind EasyQuant ‣ EasyQuant: An Efficient
    Data-free Quantization Algorithm for LLMs"), both strategies impose a very important
    influence on the final model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further conduct experiments proving whether the performance gain mainly
    comes from the outlier isolation: Actually, outlier isolation is a very important
    component of EasyQuant, but still not enough to fully recover the performance
    loss from quantization. Keeping even 10% of weights as fp16 outliers still admits
    about 8% ppl increase while EasyQuant admits only 1$\%$ ppl increase. Below we
    present the result of 4-bit quantized BLLOM-7B when we just keep 1% outliers in
    fp16 without quantization range optimization on various benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | EasyQuant | 1% fp16 outlier |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText2(PPL) | 11.66 | 12.52 |'
  prefs: []
  type: TYPE_TB
- en: '| PTB (PPL) | 21.42 | 23.32 |'
  prefs: []
  type: TYPE_TB
- en: '| C4(PPL) | 15.46 | 16.44 |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA (ACC) | 73.61% | 72.74% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Using outlier isolation solely is not enough to fully recover the
    performance loss. EasyQuant consistently outperforms outlier isolation in all
    benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: Outlier influence.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The outlier isolation is a key component in EasyQuant, but it can only impose
    an indirect influence on the model accuracy. The interesting phenomenon we find
    is that the outliers behave like a gating mechanism: without outlier isolation,
    the model achieves a much worse performance under a small reconstruction error;
    however, when keeping those outliers in fp16, the quantized LLM attains a continuously
    decreased ppl under smaller reconstruction error:'
  prefs: []
  type: TYPE_NORMAL
- en: '| reconstruction error | int4 outlier | fp16 outlier |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4.8E4 | 12.65 | 12.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.5E4 | 14.73 | 11.61 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.7E4 | 19.71 | 11.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.3E4 | NA | 11.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.9E4 | NA | 11.02 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: ppl results on Wikitext2 of BLOOM-7B with and without outlier isolation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we have also conducted a complementary experiment testing the direct
    influence of the weight outlier: We prune 1% of the values ( according to its
    magnitude) in weights into 0 and see the ppl results (as shown in Table  [6](#S5.T6
    "Table 6 ‣ Outlier influence. ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs")). It has shown that the
    largest value (outliers) imposes the same influence on the model performance as
    the normal values (median), which means those outliers share the same direct influence
    on the model accuracy with normal values. Therefore outlier isolation imposes
    a key influence on the model accuracy indirectly.'
  prefs: []
  type: TYPE_NORMAL
- en: '| pruned weights | PPL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| smallest (top-0% 1%) | 11.66 |'
  prefs: []
  type: TYPE_TB
- en: '| median (top-49% 50%) | 19.16 |'
  prefs: []
  type: TYPE_TB
- en: '| largest (top-99% 100%) | 19.17 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: ppl results after pruning 1% weight with different magnitude'
  prefs: []
  type: TYPE_NORMAL
- en: Outlier distribution.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We also explore the outlier distribution along different modules and layers.
    It shows that the fraction of outliers shares different patterns in different
    modules and layers (as shown in Table  [7](#S5.T7 "Table 7 ‣ Outlier distribution.
    ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs") and  [8](#S5.T8 "Table 8 ‣ Outlier distribution. ‣ 5.1 Experiment
    Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")). FFN.2 has a significantly higher fraction of outliers. However, it
    shows no pattern along the layer index.'
  prefs: []
  type: TYPE_NORMAL
- en: '| module name | outlier fraction (%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Att.qkv | 0.2993 |'
  prefs: []
  type: TYPE_TB
- en: '| Att.output | 0.5036 |'
  prefs: []
  type: TYPE_TB
- en: '| FFN.1 | 0.288 |'
  prefs: []
  type: TYPE_TB
- en: '| FFN.2 | 0.7560 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Outlier fraction distribution in different modules in BLOOM-7B under
    3-sigma threshold'
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer index | outlier fraction (%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.3187 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.8579 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.3953 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 0.3975 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.3962 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 0.4399 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 0.3954 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Outlier fraction distribution in different layer index in BLOOM-7B
    under 3-sigma threshold'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization range.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dynamic of the quantization range is shown in Table [9](#S5.T9 "Table 9
    ‣ Quantization range. ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An
    Efficient Data-free Quantization Algorithm for LLMs"). Roughly speaking, this
    range decreases fast in the early stage of training, which means a smaller quantization
    range will make most of the parameters to be quantized more precisely. After certain
    steps of training, the quantization range becomes stable, this means we have already
    achieved the optimal range.'
  prefs: []
  type: TYPE_NORMAL
- en: '| steps | quantization range |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $0$ | 0.078 |'
  prefs: []
  type: TYPE_TB
- en: '| $10$ | 0.069 |'
  prefs: []
  type: TYPE_TB
- en: '| $50$ | 0.052 |'
  prefs: []
  type: TYPE_TB
- en: '| $100$ | 0.048 |'
  prefs: []
  type: TYPE_TB
- en: '| $150$ | 0.047 |'
  prefs: []
  type: TYPE_TB
- en: '| $200$ | 0.047 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: The dynamic quantization range of different optimization steps. Here
    we take the quantization range of the Att.qkv module in layer 1 as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traditional model quantization algorithms mainly focus on the cases where both
    parameters and activations of the model are quantized (Lin et al., [2015](#bib.bib10);
    Hubara et al., [2016](#bib.bib8); Tailor et al., [2021](#bib.bib20); Ni et al.,
    [2020](#bib.bib14)). However, directly quantizing the model will greatly decrease
    the accuracy of the models, and one important technique to improve the performance
    is Quantization Aware Training (QAT) (Jacob et al., [2018](#bib.bib9)), where
    it simulates the quantization procedure in training to improve the accuracy of
    the quantized model further. For Transformer based models, the boundary of the
    compression level has been continuously advanced. For example, $8$-bits quantized
    BERT in  Wu et al. ([2023](#bib.bib26)) and tenary case as in TernaryBERT (Zhang
    et al., [2020](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: Model Quantization for LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For quantizing LLMs, due to their prohibitive training expense, we can only
    use a few training data for calibration. There are two major directions: 1) weight-only
    quantization, where the weights are quantized into lower bits. In  Frantar et al.
    ([2023a](#bib.bib6)); Yao et al. ([2022](#bib.bib28)), authors optimize the output
    error on the calibration set using OBS and gradient descent. 2) Activation and
    weight quantization, where both activations and weights are quantized into lower
    bits. In this case, the major obstacle is the outliers in activations. LLM.int8()
     (Dettmers et al., [2022](#bib.bib4)) addresses this problem by isolating those
    outliers in fp16/bf16\. However, such implementation leads to large latency overhead
    and is even slower than fp16 inference. Recent studies  (Wei et al., [2023](#bib.bib24);
    Xiao et al., [2023](#bib.bib27)) found that the outliers only exist in certain
    channels, and use the LayerNorm weights (Wei et al., [2023](#bib.bib24)) and calibrated
    scales (Xiao et al., [2023](#bib.bib27)) to smooth those channels.  Xiao et al.
    ([2023](#bib.bib27)) has already proved that we can achieve almost lossless W8A8
    quantized LLMs using a few calibration data, without manipulating the original
    model weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose a data-free fast weight-only quantization algorithm,
    namely EasyQuant, for LLMs, that potentially improves the quantized model’s performance
    without using any training data. Our analysis reveals the intrinsic origins of
    the performance loss when quantizing the model weights into lower bits. We show
    that by isolating the outliers from quantization, the accuracy of the quantized
    LLM increases accordingly with decreased reconstruction error. Our experiment
    proved that EasyQuant significantly outperforms RTN in a data-free setting, and
    also behaves better than data-dependent algorithms. EasyQuant can finish the quantization
    for a 176B-sized model within $10$ minutes and the overhead of dequantization
    in EasyQuant is negligible.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we also point out some limitations of our work: The outlier recovery
    functionality in EasyQuant requires extra CUDA kernels for implementation. Moreover,
    weight-only quantization can only reduce the memory footprint without any computation
    cost reduction, hence the latency of our model cannot be minimized. In addition,
    this outlier isolation will make the weight/activation quantization more challenging
    because the weight includes numbers under different precision. We have also noticed
    that EasyQuantcannot outperform the data-dependent methods in all tasks, this
    motivates us to investigate more effective algorithms in future studies.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boratko et al. (2018) Michael Boratko, Harshit Padigela, Divyendra Mikkilineni,
    Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche,
    Pavan Kapanipathi, Nicholas Mattei, et al. 2018. A systematic classification of
    knowledge, reasoning, and context within the arc dataset. *arXiv preprint arXiv:1806.00358*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. [Llm.int8(): 8-bit matrix multiplication for transformers at scale](http://arxiv.org/abs/2208.07339).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. [Bert: Pre-training of deep bidirectional transformers for language
    understanding](http://arxiv.org/abs/1810.04805). Cite arxiv:1810.04805Comment:
    13 pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023a) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. 2023a. [Gptq: Accurate post-training quantization for generative
    pre-trained transformers](http://arxiv.org/abs/2210.17323).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023b) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. 2023b. [Gptq: Accurate post-training quantization for generative
    pre-trained transformers](http://arxiv.org/abs/2210.17323).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *Advances in neural information
    processing systems*, pages 4107–4115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    pages 2704–2713.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2015) Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua
    Bengio. 2015. Neural networks with few multiplications. *arXiv preprint arXiv:1510.03009*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marcus et al. (1994) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994.
    The penn treebank: Annotating predicate argument structure. In *Human Language
    Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11,
    1994*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. [Pointer sentinel mixture models](http://arxiv.org/abs/1609.07843).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Michael Roth, Annie Louis,
    Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story
    cloze test. In *Proceedings of the 2nd Workshop on Linking Models of Lexical,
    Sentential and Discourse-level Semantics*, pages 46–51.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ni et al. (2020) Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang,
    Christoph Studer, and Tom Goldstein. 2020. Wrapnet: Neural net inference with
    ultra-low-resolution arithmetic. *arXiv preprint arXiv:2007.13242*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prato et al. (2019) Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh.
    2019. Fully quantized transformer for improved translation. *arXiv preprint arXiv:1910.10485*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, et al. 2021. Scaling language models: Methods, analysis & insights from
    training gopher. *arXiv preprint arXiv:2112.11446*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing
    nlg 530b, a large-scale generative language model. *arXiv preprint arXiv:2201.11990*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tailor et al. (2021) Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D
    Lane. 2021. Degree-quant: Quantization-aware training for graph neural networks.
    *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tata and Patel (2003) Sandeep Tata and Jignesh M Patel. 2003. Piqa: An algebra
    for querying protein data sets. In *15th International Conference on Scientific
    and Statistical Database Management, 2003.*, pages 141–150\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](http://arxiv.org/abs/2302.13971).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](http://arxiv.org/abs/1706.03762). *CoRR*, abs/1706.03762.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2023. [Outlier suppression:
    Pushing the limit of low-bit transformer language models](http://arxiv.org/abs/2209.13325).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workshop (2023) BigScience Workshop. 2023. [Bloom: A 176b-parameter open-access
    multilingual language model](http://arxiv.org/abs/2211.05100).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao,
    and Yuxiong He. 2023. [Understanding int4 quantization for transformer models:
    Latency speedup, composability, and failure cases](http://arxiv.org/abs/2301.12017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. [Smoothquant: Accurate and efficient post-training quantization
    for large language models](http://arxiv.org/abs/2211.10438).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. [Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers](http://arxiv.org/abs/2206.01861).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    2019. Q8bert: Quantized 8bit bert. *arXiv preprint arXiv:1910.06188*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
    An open bilingual pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. [Opt: Open pre-trained
    transformer language models](http://arxiv.org/abs/2205.01068).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv preprint arXiv:2009.12812*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  |  | Perplexity-based Task | Zero-shot Task |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | WikiText2 | PTB | C4 | PIQA | ARC-easy | ARC-Challenge | StoryCloze |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | fp16 | $22.42$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $25.90$ |'
  prefs: []
  type: TYPE_TB
- en: '| 560M | GPTQ | $24.03$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 23.74 | 46.86 | $28.03$ |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | fp16 | $17.69$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $22.00$ |'
  prefs: []
  type: TYPE_TB
- en: '| 1.1B | GPTQ | $19.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 18.51 | 61.83 | 22.94 | $\textbf{66.65}\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | fp16 | $15.39$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $16.97$ |'
  prefs: []
  type: TYPE_TB
- en: '| 1.7B | GPTQ | $16.48$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 16.01 | 31.50 | 20.15 | $\textbf{68.99}\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | fp16 | $13.48$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $14.76$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3B | GPTQ | $14.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 14.01 | 26.12 | 17.96 | $\textbf{69.80}\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | fp16 | $11.37$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $12.10$ |'
  prefs: []
  type: TYPE_TB
- en: '| 7.1B | GPTQ | $11.73$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | 11.66 | 21.47 | 15.52 | $\textbf{73.23}\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | fp16 | $8.11$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTN | $8.37$ |'
  prefs: []
  type: TYPE_TB
- en: '| 176B | GPTQ | $8.21$ |'
  prefs: []
  type: TYPE_TB
- en: '| EasyQuant | $8.21$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Perplexity and zershot results for BLOOM model family'
  prefs: []
  type: TYPE_NORMAL
