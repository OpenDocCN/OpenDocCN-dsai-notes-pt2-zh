- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Ranking LLMs by compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14171](https://ar5iv.labs.arxiv.org/html/2406.14171)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Peijia Guo^(1,2) , Ziguang Li ³ , Haibo Hu ³ , Chao Huang ^(3∗), Ming Li ^(4∗)
    , Rui Zhang ^(1∗)
  prefs: []
  type: TYPE_NORMAL
- en: ¹ School of Mathematics, Northwest University, Xi’an, China
  prefs: []
  type: TYPE_NORMAL
- en: ²Shanghai Institute for Mathematics and Interdisciplinary Sciences, Shanghai,
    China
  prefs: []
  type: TYPE_NORMAL
- en: ³ Institute of Computing Technology, Chinese Academy of Sciences, China
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Cheriton School of Computer Science, University of Waterloo, Ontario, Canada
  prefs: []
  type: TYPE_NORMAL
- en: Guopeijia0929@163.com      chriszggz@gamil.com      huhaibo22@mails.ucas.ac.cn
  prefs: []
  type: TYPE_NORMAL
- en: chuang@ict.ac.cn      mli@uwaterloo.ca      rzhang@nwu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We conceptualize the process of understanding as information compression, and
    propose a method for ranking large language models (LLMs) based on lossless data
    compression. We demonstrate the equivalence of compression length under arithmetic
    coding with cumulative negative log probabilities when using a large language
    model as a prior, that is, the pre-training phase of the model is essentially
    the process of learning the optimal coding length. At the same time, the evaluation
    metric compression ratio can be obtained without actual compression, which greatly
    saves overhead. In this paper, we use five large language models as priors for
    compression, then compare their performance on challenging natural language processing
    tasks, including sentence completion, question answering, and coreference resolution.
    Experimental results show that compression ratio and model performance are positively
    correlated, so it can be used as a general metric to evaluate large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, the rapid development of LLMs has brought earth-shaking changes
    to the field of natural language processing (NLP) (Radford et al., [2019](#bib.bib19); Zhao
    et al., [2023](#bib.bib30); Liu et al., [2023](#bib.bib15)). LLMs are advanced
    language models pretrained on tens of gigabytes of data without tuning on data
    for specific tasks. These large models can directly complete various NLP tasks,
    and even become a milestone technology towards general artificial intelligence
    (AGI). Currently, LLMs are being studied more and more widely in various fields,
    such as education and research (Rahman and Watanobe, [2023](#bib.bib20)), medicine
    and healthcare (Thirunavukarasu et al., [2023](#bib.bib23); Cascella et al., [2023](#bib.bib3)),
    etc., and their performance evaluation methods are becoming more and more important.
  prefs: []
  type: TYPE_NORMAL
- en: Chang et al. ([2024](#bib.bib4)) showed that researchers always scrutinize the
    capabilities of AI models or algorithms through evaluation using specific and
    challenging tasks, so the evaluation metrics are outlined from the perspective
    of the evaluation tasks. The metrics are diverse, such as Exact Match (EM), F1-score,
    ROUGE, etc., and many are set for specific tasks, making it difficult to uniformly
    evaluate the performance of the model on different tasks. In addition, contamination
    of training and test data can also lead to biased evaluation results (Magar and
    Schwartz, [2022](#bib.bib16)), making it impossible to verify whether NLP progress
    is achieved through better language understanding or better data utilization.
    Various limitations lead to the lack of a unified LLMs evaluation standard.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we consider the process of model training and learning itself and
    prove the equivalence of the model pre-training goal and the compression length
    under arithmetic coding, indicating that compression is closely related to model
    performance, and then use the compression ratio as a general metric to measure
    the model’s generalization ability in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Language Models Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, performance evaluation of LLMs is mainly achieved through benchmark
    tests, including diverse tasks, standardized datasets and comprehensive evaluation
    metrics. The purpose is to establish a systematic and standardized evaluation
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: In 2019, Wang et al. ([2019](#bib.bib27)) introduced the General Language Understanding
    Evaluation Benchmark (GLUE), a multi-task evaluation platform for measuring the
    performance of natural language understanding models. It contains nine tasks,
    covering various types such as text classification, text similarity evaluation,
    natural language Inference, question answering, etc. A recent study Laskar et al.
    ([2023](#bib.bib13)) evaluated ChatGPT across 140 tasks and analyze 255K responses
    it generates in these datasets, laying the foundation for deploying ChatGPT-like
    LLMs in real-world applications. More recently, OpenAI et al. ([2024](#bib.bib18))
    tested GPT-4 on a diverse set of benchmarks, including 34 simulating exams that
    were originally designed for humans. Benchmark test is very important for evaluating
    the performance of language models and promoting research progress, but limited
    coverage tasks, data contamination (Brown et al., [2020](#bib.bib2); Li, [2023](#bib.bib14)),
    and huge overhead are all challenges and limitations faced in this process. In
    order to solve these problems, we propose compression ratio based on lossless
    data compression, a general evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Neural Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of data compression is to reduce the representation size while retaining
    valid information. Our LLMs-based compressor uses neural networks for data compression
    and belongs to the neural compression category. Current research in neural compression
    largely benefits from advances in deep generative modeling (Yang et al., [2023](#bib.bib29)),
    such as GANs (Goodfellow et al., [2014](#bib.bib7)), VAEs (Rezende and Mohamed,
    [2015](#bib.bib21)), and autoregressive models (Van Den Oord et al., [2016](#bib.bib26)).
    With the development of deep neural networks, lossless text compression has also
    ushered in new progress. Goyal et al. ([2018](#bib.bib8))  proposed DeepZip, a
    lossless compressor based on neural networks, consisting of two main modules:
    RNN and arithmetic coding. It achieves higher compression ratio than GZIP.  Bellard
    ([2019](#bib.bib1))  proposed a lossless compressor based on LSTM, which is simple
    to describe and has reasonable memory consumption compared to compressors that
    provide a similar compression ratio. Recent advancements, such as TRACE, a fast
    transformer-based general-purpose lossless compressor (Mao et al., [2022](#bib.bib17)),
    achieves an overall speedup of approximately 3x while maintaining a compression
    ratio comparable to state-of-the-art compressors.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 LLMs based Arithmetic Coding for Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shannon’s fundamental theorem of coding states that (Shannon, [1948](#bib.bib22)),
    given messages randomly generated from a model, it is impossible to encode them
    into less bits (on average) than the entropy of that model, thus defining a lower
    bound for lossless compression. Arithmetic coding is an entropy coding algorithm.
    Huang et al. ([2023](#bib.bib10)) proposed an entropy-based compressor that integrated
    generative pre-trained transformer into adaptive arithmetic coding, highlighting
    the potential of pre-trained LLMs as powerful priors in compression. In this paper,
    we integrate LLMs into adaptive arithmetic coding for compression, with the aim
    of representing data according to the probability of output to reduce its overall
    size.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs as Entropy Models
  prefs: []
  type: TYPE_NORMAL
- en: Considering text data, first use a tokenizer to convert the text into a data
    stream $t_{1:n}:=t_{1}t_{2}\cdots t_{n}\in T^{n}$ acts as the entropy model, guiding
    the encoder to allocate fewer bits to high-frequency tokens and more bits to low-frequency
    tokens, thereby improving compression efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Coding Process
  prefs: []
  type: TYPE_NORMAL
- en: 'The range for the data stream is the interval $\left[0,1\right)$. Then narrow
    the interval to the part assigned to that token:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Algorithm 1 Arithmetic Coding |'
  prefs: []
  type: TYPE_TB
- en: '| 1: Input: $t_{0:n}:=t_{0}t_{1}\cdots t_{n}\in T^{n+1}$. |'
  prefs: []
  type: TYPE_TB
- en: '| 2: $I_{low}^{0}=0,~{}I_{high}^{0}=1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3: for  $t_{i},~{}i=1,2,\cdots,n,n+1$   do |'
  prefs: []
  type: TYPE_TB
- en: '| 4:     $range=I_{high}^{i-1}-I_{low}^{i-1}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5:     $I_{low}^{i}\leftarrow I_{low}^{i-1}+range*F_{i}(t_{i})$ |'
  prefs: []
  type: TYPE_TB
- en: '| 6:     $1$2 |'
  prefs: []
  type: TYPE_TB
- en: '| 7: end for |'
  prefs: []
  type: TYPE_TB
- en: '| 8: Output: $[I_{low}^{n+1},I_{high}^{n+1})$. |'
  prefs: []
  type: TYPE_TB
- en: Adaptive arithmetic coding using LLM is shown in Algorithm 1.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Equivalence of Model Pre-training Goal and Compression Length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is well established that compression and prediction are essentially equivalent
     (Delétang et al., [2023](#bib.bib5)). In this way, compression and LLMs are closely
    linked. We mathematically prove the equivalence of model pre-training goal and
    compression length. Then we present a novel method for evaluating LLMS based on
    lossless compression.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training Optimization Goals for LLMs
  prefs: []
  type: TYPE_NORMAL
- en: The loss function, also known as the objective function, measures the difference
    between the probability distribution predicted by the model and the true distribution.
    Model training is to reduce the loss function through continuous iteration, thereby
    optimizing model performance.
  prefs: []
  type: TYPE_NORMAL
- en: We continue to consider the data stream above $t_{1:n}:=t_{1}t_{2}\cdots t_{n}\in
    T^{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have the true distribution $Q$, which can elicit the definition of relative
    entropy, that is, Kullback-Leibler Divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{KL}(Q&#124;&#124;P)={\sum_{i=1}^{n}(Q_{i}\log_{2}{Q_{i}})}-{\sum_{i=1}^{n}(Q_{i}\log_{2}{P_{i}})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The previous term ${\sum_{i=1}^{n}(Q_{i}\cdot\log_{2}{Q_{i}})}$, that is, to
    minimize the value of cross entropy. It further illustrates that cross entropy
    can be used as the loss function, and minimizing cross entropy is the goal of
    optimizing the model.
  prefs: []
  type: TYPE_NORMAL
- en: Negative Log Probability as Compression Length
  prefs: []
  type: TYPE_NORMAL
- en: The goal of lossless compression is to encode a data stream $t_{1:n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in the process of achieving lossless compression, minimizing the
    expected length of the encoded data stream is equivalent to minimizing cross entropy.
    At this point, the equivalence of model pre-training goal and compression length
    has been proven. Furthermore, we can use compression ratio as a unified criterion
    for evaluating LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The experiment consists of four key parts: the calculation of the compression
    ratio and three natural language processing tasks, namely sentence completion,
    question answering and coreference resolution. We use a total of five LLMs as
    compressor priors, but the proposed method is not limited to these models. This
    method can be applied to more advanced LLMs as long as the predicted probabilities
    can be obtained.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 The Calculation of Compression Ratio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we select the Text8 dataset to calculate the compression ratio of the
    compressor. The Text8 dataset is a large corpus extracted from the English Wikipedia.
    After some simple preprocessing, the text content covers various topics and fields.
    It is a general dataset for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the read Text8 file by spaces and obtain a list containing all words.
    Then every 200 words are divided into a sublist, and the 200-length word fragment
    are converted into strings. The list of the first 10,000 strings is passed to
    the LLMs compressor as a parameter. The compression ratio calculation formula
    is as follows (in bits):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The LLM compressors involved include LLaMA 2 7B released by Meta, Mistral 7B
    released by the Mistral AI team, OPT-IML 1.3B released by Facebook, and GPT-2-XL 1.5B
    and GPT-2 774M released by OpenAI. Their calculated compression ratios are shown
    in Table 1.
  prefs: []
  type: TYPE_NORMAL
- en: '| Compressor | Compression Ratio |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA 2 7B | 8.663 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7B | 9.266 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-IML 1.3B | 6.938 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2-XL 1.5B | 7.095 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 774M | 6.864 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Compression ratios of different compressors.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Sentence Completion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sentence completion is designed to allow the computer to predict the missing
    parts based on the given context, so that the sentence becomes coherent and complete.
    We compare the performance of three large models, LLaMA 2 7B, Mistral 7B and GPT-2-XL 1.5B
    on the HellaSwag dataset, using accuracy as a metric. The results are shown in
    Table 2.
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM | Accuracy(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral 7B | 81.3 (Jiang et al., [2023](#bib.bib12)) |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA 2 7B | 77.2 (Touvron et al., [2023](#bib.bib24)) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2-XL 1.5B | 50.9 (Wu et al., [2023](#bib.bib28)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Performance on sentence completion .'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Question Answering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of question answering is to enable the computer to understand the questions
    raised by users through semantic understanding and syntax analysis, and then generate
    answers that meet the requirements of the questions. Because any form of LLM evaluation
    can be seen as question answering or switch to this format, so it is a very important
    means for LLMs evaluation(Guo et al., [2023](#bib.bib9)). We compare the performance
    of two large models, LLaMA 2 7B and OPT-IML 1.3B on the BoolQ dataset, using accuracy
    as a metric. The results are shown in Table 3.
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM | Accuracy(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA 2 7B | 77.4 (Touvron et al., [2023](#bib.bib24)) |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-IML 1.3B | 61.5 (Iyer et al., [2023](#bib.bib11)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance on question answering.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Coreference Resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coreference resolution is to identify the entities referred to by pronouns and
    noun phrases in the text. It has many practical applications in natural language
    processing, such as information extraction, text summarization, etc. Correct parsing
    of reference relationships can help computers better understand text. We compares
    the performance of two large models, GPT-2-XL 1.5B and GPT-2 774M on the Winograd
    Schema Challenge data set, using accuracy as a metric. The results are shown in
    Table 4.
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM | Accuracy(%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2-XL 1.5B | 73.3 (Wu et al., [2023](#bib.bib28)) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 774M | 69.2 (Trichelair et al., [2018](#bib.bib25)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance on coreference resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Result Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the above experiments, it can be concluded that: the better data compression
    effect of LLM, the better its performance in natural language processing tasks.
    That is, there is a positive correlation between compression ratio and model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: When we can effectively compress data, it means that we have captured the key
    characteristics and patterns of the data. This is similar to finding patterns
    and redundancies in the data during the model learning process. So we can say
    that if a large language model achieves the best lossless compression on a dataset,
    it will often achieve the best generalization on other datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the experimental results further verify the theoretical conclusion
    of this paper: compression ratio can be used as a general metric to measure the
    performance of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We proposed to rank LLMs through lossless data compression in this paper. Our
    method measures compression ratios as a metric for generalization. We demonstrate
    the equivalence of compression length under arithmetic coding and LLMs pre-training
    goal, saving the overhead of actual compression. This further illustrates that
    understanding is compression, demonstrated by our experiments across challenging
    downstream NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For NLP tasks, the experiments in this paper only used the open source version
    of the pre-trained language model, which was subject to computational constraints
    and scale limitations. Furthermore evaluation is not the end goal but the starting
    point. A mature evaluation system should not only provide conclusions about performance,
    but also provide analysis and guidance for future research and development, which
    is also our future research direction.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We take academic integrity and research independence very seriously. Here we
    would like to declare that parts of this paper overlap with a published paper.
    Overlaps include ideas presented, experimental methods. Information about the
    published paper is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Title: Compression Represents Intelligence Linearly'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Author: Yuzhen Huang, Jinghan Zhang, Zifei Shan, Junxian He'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: arXiv:2404.09937 [cs.CL]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Submission date: April 15, 2024'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When we began our work, we were unaware of the existence of this published paper.
    Our study began on December 2023, was completed on May 2024, and was submitted
    on June 20, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bellard (2019) Fabrice Bellard. 2019. Lossless data compression with neural
    networks. *URL: https://bellard. org/nncp/nncp. pdf*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cascella et al. (2023) Marco Cascella, Jonathan Montomoli, Valentina Bellini,
    and Elena Bignami. 2023. Evaluating the feasibility of chatgpt in healthcare:
    an analysis of multiple clinical and research scenarios. *Journal of medical systems*,
    47(1):33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*, 15(3):1–45.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delétang et al. (2023) Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, et al. 2023. Language modeling is compression.
    *arXiv preprint arXiv:2309.10668*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gibbs (1878) Josiah Willard Gibbs. 1878. On the equilibrium of heterogeneous
    substances. *American Journal of Science*, 3(96):441–458.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. *Advances in neural information processing systems*,
    27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2018) Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia
    Ochoa. 2018. [Deepzip: Lossless data compression using recurrent neural networks](http://arxiv.org/abs/1811.08162).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi,
    Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating
    large language models: A comprehensive survey. *arXiv preprint arXiv:2310.19736*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Cynthia Huang, Yuqing Xie, Zhiying Jiang, Jimmy Lin, and
    Ming Li. 2023. Approximating human-like few-shot learning with gpt-based compression.
    *arXiv preprint arXiv:2308.06942*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iyer et al. (2023) Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor
    Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh
    Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli
    Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2023. [Opt-iml: Scaling language
    model instruction meta learning through the lens of generalization](http://arxiv.org/abs/2212.12017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laskar et al. (2023) Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman,
    Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. [A systematic
    study and comprehensive evaluation of chatgpt on benchmark datasets](http://arxiv.org/abs/2305.18486).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li (2023) Yucheng Li. 2023. An open source data contamination report for llama
    series models. *arXiv preprint arXiv:2310.17589*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan
    Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu,
    Lin Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao
    Ge. 2023. [Summary of chatgpt-related research and perspective towards the future
    of large language models](https://doi.org/https://doi.org/10.1016/j.metrad.2023.100017).
    *Meta-Radiology*, 1(2):100017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Magar and Schwartz (2022) Inbal Magar and Roy Schwartz. 2022. [Data contamination:
    From memorization to exploitation](http://arxiv.org/abs/2203.08242).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. (2022) Yu Mao, Yufei Cui, Tei-Wei Kuo, and Chun Jason Xue. 2022.
    Trace: A fast transformer-based general-purpose lossless compressor. In *Proceedings
    of the ACM Web Conference 2022*, pages 1829–1838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2024. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahman and Watanobe (2023) Md. Mostafizer Rahman and Yutaka Watanobe. 2023.
    [Chatgpt for education and research: Opportunities, threats, and strategies](https://doi.org/10.3390/app13095783).
    *Applied Sciences*, 13(9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. 2015. Variational
    inference with normalizing flows. In *International conference on machine learning*,
    pages 1530–1538\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shannon (1948) Claude Elwood Shannon. 1948. A mathematical theory of communication.
    *The Bell system technical journal*, 27(3):379–423.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    Large language models in medicine. *Nature medicine*, 29(8):1930–1940.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trichelair et al. (2018) Paul Trichelair, Ali Emami, Adam Trischler, Kaheer
    Suleman, and Jackie Chi Kit Cheung. 2018. How reasonable are common-sense reasoning
    tasks: A case-study on the winograd schema challenge and swag. *arXiv preprint
    arXiv:1811.01778*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Den Oord et al. (2016) Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
    2016. Pixel recurrent neural networks. In *International conference on machine
    learning*, pages 1747–1756\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2019. [Glue: A multi-task benchmark and analysis
    platform for natural language understanding](http://arxiv.org/abs/1804.07461).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed,
    and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from
    large-scale instructions. *arXiv preprint arXiv:2304.14402*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Yibo Yang, Stephan Mandt, Lucas Theis, et al. 2023. An introduction
    to neural data compression. *Foundations and Trends® in Computer Graphics and
    Vision*, 15(2):113–200.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. [A survey
    of large language models](http://arxiv.org/abs/2303.18223).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
