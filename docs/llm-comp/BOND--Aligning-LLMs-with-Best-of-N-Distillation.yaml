- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:05:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'BOND: Aligning LLMs with Best-of-N Distillation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14622](https://ar5iv.labs.arxiv.org/html/2407.14622)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \pdftrailerid
  prefs: []
  type: TYPE_NORMAL
- en: redacted \correspondingauthorpiergs@google.com
  prefs: []
  type: TYPE_NORMAL
- en: Pier Giuseppe Sessa Google DeepMind Robert Dadashi Google DeepMind Léonard Hussenot
    Google DeepMind Johan Ferret Google DeepMind Nino Vieillard Google DeepMind Alexandre Ramé
    Google DeepMind Bobak Shariari Google DeepMind Sarah Perrin Google DeepMind Abe Friesen
    Google DeepMind Geoffrey Cideron Google DeepMind Sertan Girgin Google DeepMind
    Piotr Stanczyk Google DeepMind Andrea Michi Google DeepMind Danila Sinopalnikov
    Google DeepMind Sabela Ramos Google DeepMind Amélie Héliou Google DeepMind Aliaksei Severyn
    Google DeepMind Matt Hoffman Google DeepMind Nikola Momchev Google DeepMind Olivier Bachem
    Google DeepMind
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) is a key driver of quality
    and safety in state-of-the-art large language models. Yet, a surprisingly simple
    and strong inference-time strategy is Best-of-N sampling that selects the best
    generation among $N$) to balance between mode-covering and mode-seeking behavior,
    and derive an iterative formulation that utilizes a moving anchor for efficiency.
    We demonstrate the effectiveness of our approach and several design choices through
    experiments on abstractive summarization and Gemma models. Aligning Gemma policies
    with BOND outperforms other RLHF algorithms by improving results on several benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLM, Alignment, RLHF, Best-of-N
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: State-of-the-art large language models (LLMs) such as Gemini (Gemini Team, [2023](#bib.bib21);
    Reid et al., [2024](#bib.bib51)) and GPT-4 (OpenAI, [2023](#bib.bib39)) are generally
    trained in three stages. First, LLMs are pre-trained on large corpora of knowledge
    using next-token prediction (Radford et al., [2018](#bib.bib45), [2019](#bib.bib46)).
    Second, the pre-trained models are fine-tuned to follow instructions via supervised
    fine-tuning (SFT) (Raffel et al., [2020](#bib.bib48); Wei et al., [2022](#bib.bib61)).
    Lastly, reinforcement learning from human feedback (RLHF) (Christiano et al.,
    [2017](#bib.bib10); Ziegler et al., [2019](#bib.bib65); Stiennon et al., [2020](#bib.bib55))
    is used to further increase the quality of generations. The RLHF step generally
    consists of learning a reward model (RM) (Ouyang et al., [2022](#bib.bib40)) on
    human preferences and then optimizing the LLM to maximize predicted rewards using
    reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF algorithms and their challenges. Fine-tuning LLMs with reinforcement learning
    (RL) is challenging (Casper et al., [2023](#bib.bib8)), notably since it can cause
    *forgetting* (French, [1992](#bib.bib18)) of pre-trained knowledge, and since
    loopholes in the RM (Clark and Amodei, [2016](#bib.bib11); Pan et al., [2022](#bib.bib42))
    can cause *reward hacking* (Askell et al., [2021](#bib.bib4); Skalse et al., [2022](#bib.bib54)).
    The standard strategy is to use policy-gradient methods (Williams, [1992](#bib.bib62))
    with $\mathrm{KL}$, to preserve the general capabilities of the original model
    and tackle the misalignment (Ngo et al., [2022](#bib.bib38)) concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best-of-N sampling. In practice, a surprisingly simple inference-time approach
    is often used to improve the quality of generations: Best-of-N sampling (Stiennon
    et al., [2020](#bib.bib55)). It consists of drawing $N$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BOND. In this paper, we propose BOND (Best-of-N Distillation), a novel RLHF
    algorithm that *learns* a policy that achieves the strong performance of Best-of-N
    sampling but, crucially, requires only a single sample at inference time, as depicted
    in [Figure 1](#S1.F1 "In 1 Introduction ‣ BOND: Aligning LLMs with Best-of-N Distillation").
    Our key idea is to cast the alignment of the policy as a distribution matching
    problem, where we fine-tune the policy to emulate the Best-of-N distribution.
    To achieve this, we first derive an analytical expression for the Best-of-N distribution.
    This allows us to consider and optimize different divergence metrics. We first
    show how to minimize the *forward* $\mathrm{KL}$, also known as *Jeffreys divergence*,
    which retains the best of both approaches. Furthermore, to optimize performance
    while keeping a reduced sample-complexity, we propose an *iterative* BOND approach
    which consists of iteratively distilling the Best-of-N of a moving anchor policy.
    Finally, based on the aforementioned ideas, we propose J-BOND (J for Jeffreys),
    a novel, stable, efficient and practical RLHF algorithm to align LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments. We first demonstrate the effectiveness of BOND and of our design
    choices on the abstractive summarization XSum (Narayan et al., [2018](#bib.bib37))
    task. Then, in [Section 6](#S6 "6 Experiments ‣ BOND: Aligning LLMs with Best-of-N
    Distillation"), we apply J-BOND to align Gemma (Gemma Team, [2024](#bib.bib22))
    policies. J-BOND not only improves the $\mathrm{KL}$-reward Pareto front compared
    to standard RL algorithms, but also enhances performance on academic benchmarks
    and side-by-side comparisons against open-source variants such as Mixtral (Jiang
    et al., [2024](#bib.bib29)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5534e08bd1941c3c246fb046ff5878a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Best-of-N is an *inference-time* strategy that selects the best generation
    among $N$ times from the model). In contrast, the proposed BOND approach aims
    at obtaining a fine-tuned policy that can directly sample the Best-of-N generation.
    This would inherit the quality of Best-of-N sampling, while requiring a single
    sample at inference time. We achieve this by distilling the Best-of-N strategy
    into the policy via online *distribution matching*.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider a LLM based on the transformer (Vaswani et al., [2017](#bib.bib59))
    architecture, defining a policy $\pi(x,\cdot)$, trained to reflect human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard RLHF. Most RL algorithms optimize a linear combination of the expected
    reward and a $\mathrm{KL}$ divergence between the current and reference policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{\text{RL}}=\operatorname{argmax}_{\pi}\mathbb{E}_{\pi}\mathopen{}\mathclose{{}\left[r(y)}\right]-\beta_{\textrm{RL}}\cdot\operatorname{KL}\mathopen{}\mathclose{{}\left(\pi\mid\mid{\pi_{\text{ref}}}}\right),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'with regularization strength $\beta_{\textrm{RL}}\geq 0$ (Geist et al., [2019](#bib.bib20);
    Lazaridou et al., [2020](#bib.bib33)), reducing forgetting (French, [1992](#bib.bib18))
    and reward hacking (Skalse et al., [2022](#bib.bib54)). [Equation 1](#S2.E1 "In
    2 Problem Setup ‣ BOND: Aligning LLMs with Best-of-N Distillation") is usually
    optimized with online algorithms, as they perform better than their offline counterparts
    (Tang et al., [2024](#bib.bib57)). Moreover, simple methods have demonstrated
    the best results, e.g., REINFORCE (Williams, [1992](#bib.bib62)) with a sampled
    baseline for variance reduction (Li et al., [2023](#bib.bib34); Ahmadian et al.,
    [2024](#bib.bib2)) outperform PPO (Schulman et al., [2017](#bib.bib53)).'
  prefs: []
  type: TYPE_NORMAL
- en: Best-of-N. A complementary alignment strategy is Best-of-N, which is an inference-time
    strategy that involves sampling multiple times from ${\pi_{\text{ref}}}$ times
    more costly than sampling a single one.
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the above considerations, we propose a novel alignment method which
    we name BOND for Best-of-N Distillation. The goal of BOND is to *distill the Best-of-N
    strategy into the policy*. This allows the policy to reach the strong performance
    of Best-of-N sampling, while requiring only *a single sample* at inference time.
    We outline our overall approach in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 3 The BOND Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We formulate the BOND approach in two main steps. First, we derive an analytical
    expression for the Best-of-N distribution ([Section 3.1](#S3.SS1 "3.1 The Best-of-N
    distribution ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")).
    Second, using the derived expression, we phrase the problem as a *distribution
    matching* problem ([Section 3.2](#S3.SS2 "3.2 The BOND objective ‣ 3 The BOND
    Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")), i.e., we want to
    steer the policy closer to the Best-of-N distribution. In [Section 3.3](#S3.SS3
    "3.3 Connection with standard RLHF ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs
    with Best-of-N Distillation"), we draw insightful connections between BOND and
    standard RLHF.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The Best-of-N distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we derive the exact analytical distribution of Best-of-N sampling
    and study its properties. For simplicity, we drop the context $x$¹¹1To distinguish
    between generations with the same reward, ties can be broken with any arbitrary
    strict ordering.. We can affirm the following main theorem (proof in [Section A.1](#A1.SS1
    "A.1 Proof of Theorem 1 ‣ Appendix A Supporting results and derivations ‣ BOND:
    Aligning LLMs with Best-of-N Distillation")).'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For any generation $y$, let
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${p_{<}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})<r(y)}\right]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: denote the probability that a random generation $y^{\prime}$ and let
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right],$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: the probability that $y^{\prime}$ is the output of Best-of-N sampling is given
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Interpretation. [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 The Best-of-N distribution
    ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation") provides
    an intuitive explanation on the behavior of Best-of-N sampling: it essentially
    reweights the original sampling distribution ${\pi_{\text{ref}}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: The term $\mathtt{(A)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term $\mathtt{(B)}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'It achieves its minimum at $1$ (see [Section A.2](#A1.SS2 "A.2 Link to the
    continuous case ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The BOND objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The analytical characterization of the Best-of-N distribution allows us to
    formulate BOND as a distribution matching problem. That is, we want to solve the
    objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{\texttt{BOND}}=\arg\min_{\pi\in\Pi}\,D(\pi\,\&#124;\,{\pi_{\text{BoN}}}),$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D(\cdot\,\|\,\cdot)$ from online and offline samples. We defer the choice
    of suitable divergences and resulting BOND algorithms to [Section 4](#S4 "4 BOND
    Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Connection with standard RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we draw important connections between the two seemingly different
    objectives of standard RLHF ([Equation 1](#S2.E1 "In 2 Problem Setup ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")) and BOND ([Equation 6](#S3.E6 "In 3.2 The
    BOND objective ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is well known (see, e.g., Vieillard et al. ([2020](#bib.bib60)); Rafailov
    et al. ([2023](#bib.bib47))) that the policy maximizing the RLHF objective from
    [Equation 1](#S2.E1 "In 2 Problem Setup ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{\text{RL}}(y)\propto{\pi_{\text{ref}}}(y)\exp\mathopen{}\mathclose{{}\left(\frac{1}{\beta}_{\textrm{RL}}r(y)}\right).$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'From the derived expression of ${\pi_{\text{BoN}}}$ in [Theorem 1](#Thmtheorem1
    "Theorem 1\. ‣ 3.1 The Best-of-N distribution ‣ 3 The BOND Approach ‣ BOND: Aligning
    LLMs with Best-of-N Distillation"), we see that the Best-of-N sampling distribution
    coincides with the optimal solution of standard RLHF when using the following
    specific BOND reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'and the specific regularization strength $\beta_{\text{BOND}}=\frac{1}{N-1}$.
    This provides two interesting insights for Best-of-N sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Best-of-N sampling corresponds to the solution of a standard $\mathrm{KL}$ determines
    the level of KL regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Best-of-N sampling corresponds to optimizing the expected log reward quantile,
    i.e., the log likelihood that the generation has larger reward than a random sample
    from the reference distribution. Interestingly, due to the concavity of the logarithm,
    $r_{\texttt{BOND}}(y)$ more robust to reward hacking compared to standard RLHF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The connection to RLHF also inspires the proposed approach in this manuscript:
    if we can compute the BOND reward or equivalently the Best-of-N distribution ${\pi_{\text{BoN}}}$,
    then we can steer the policy towards Best-of-N via distribution matching. In the
    next section we explore different algorithms to tackle the main underlying challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 BOND Challenges and Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing the BOND approach induces the three following challenges: *(1)*
    how to estimate the reward quantiles, *(2)* which is the appropriate divergence
    metric to use, and *(3)* how to choose the hyperparameter $N$ representing the
    number of sampled generations in Best-of-N. We discuss and address these challenges
    in the next three subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Monte-Carlo quantile estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One key difficulty in estimating the ${\pi_{\text{BoN}}}$ distribution is that
    we need to estimate the quantile
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right],$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'of a given generation $y$ and obtaining the following empirical estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{p}_{\leq}(y)=\frac{1}{k}\sum_{i=1}^{k}\mathbb{I}\{r(y_{i})\leq r(y)\}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'We found this to be a very effective in our experiments, even with a limited
    number of samples. In principle, though, one could also use alternative approaches,
    e.g., training a learned quantile model (as we explore in [Section B.1](#A2.SS1
    "B.1 Learned quantile models ‣ Appendix B Additional Experiments ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Jeffreys divergence as a robust objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The choice of the divergence metric used in BOND is of crucial importance:
    different divergences can steer the policy to very different solutions. Here,
    we propose the *Jeffreys divergence* as a robust distribution matching objective.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jeffreys divergence (Jeffreys, [1946](#bib.bib28)) between two distributions
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J_{\text{effreys}}^{\beta}(p\,\&#124;\,q):=(1-\beta)\cdot\underbrace{\text{KL}(q\,\&#124;\,p)}_{\text{Forward
    KL}}\,+\,\beta\cdot\underbrace{\text{KL}(p\,\&#124;\,q)}_{\text{Backward KL}}.$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: The (generalized) Jeffreys divergence is a weighted average (with weight $\beta\in[0,1]$
    can lead to policy and entropy collapses. Instead, we empirically show that the
    Jeffreys divergence inherits the best of both divergences, producing better aligned
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of BOND, this translates into minimizing the divergence $J_{\text{effreys}}^{\beta}(\pi\,\|\,{\pi_{\text{BoN}}})$
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation of the forward KL. The forward KL defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{KL}({\pi_{\text{BoN}}}\,\&#124;\,\pi)=\mathbb{E}_{y\sim{\pi_{\text{BoN}}}}\mathopen{}\mathclose{{}\left[\log{\pi_{\text{BoN}}}(y)-\log\pi(y)}\right]$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'can be estimated directly drawing samples from the ${\pi_{\text{BoN}}}$ and
    selecting the best one) and can be seen as a supervised fine-tuning loss on the
    Best-of-N samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla_{\pi}\text{KL}({\pi_{\text{BoN}}}\,\&#124;\,\pi)=-\mathbb{E}_{y\sim{\pi_{\text{BoN}}}}\nabla\log\pi(y).$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: Estimation of the backward KL. The backward KL defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{KL}(\pi\,\&#124;\,{\pi_{\text{BoN}}})=\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right]$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'can be estimated from the policy samples (note the expectation w.r.t. $\pi$.
    In particular, by the analogies drawn in [Section 3.3](#S3.SS3 "3.3 Connection
    with standard RLHF ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N
    Distillation"), we show (in [Section A.3](#A1.SS3 "A.3 Backward KL and policy
    gradient equivalence ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")) that its gradient coincides with a policy
    gradient (e.g., used by REINFORCE (Williams, [1992](#bib.bib62)) in standard RLHF):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: with the equivalent reward $r_{\texttt{BOND}}$. Moreover, to reduce the resulting
    variance, we use a policy gradient baseline (Sutton and Barto, [1998](#bib.bib56))
    which we compute as the average return for the other generations in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the overall $J_{\text{effreys}}^{\beta}$ loss is a linear weighted combination
    between a supervised fine-tuning and a policy gradient loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments. In [Figure 2](#S4.F2 "In 4.2 Jeffreys divergence as a robust objective
    ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation"),
    we consider the abstractive summarization XSum task (Narayan et al., [2018](#bib.bib37))
    with ${\pi_{\text{ref}}}$) lags behind.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/773f1ad3e6c325e9ee7a6641063840e1.png)![Refer to caption](img/fd7fa408947dbbdfcef6be2cab43ed28.png)![Refer
    to caption](img/02e2cd47fb765f1e3e281461d11c6870.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: BOND with $N=8$ are reported in [Section B.2](#A2.SS2 "B.2 Additional
    plots for BOND with Jeffreys divergence objective ‣ Appendix B Additional Experiments
    ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Iterative BOND
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we discuss the choice of the parameter $N$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the above challenges, we propose the *iterative* BOND approach.
    The approach is inspired by the fact that Best-of-N sampling from a Best-of-N
    distribution, coincides with Best-of-N² sampling from the original distribution.
    More generally, by informally defining $\text{Bo$N$}(\cdot)$ as an operator that
    performs Best-of-N sampling from a base distribution, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underbrace{\text{Bo{N}}(\cdots\text{Bo{N}}(\text{Bo{N}}(}_{M\text{ times}}{\pi_{\text{ref}}})))\,\equiv\,\text{Bo{N}${}^{M}$}({\pi_{\text{ref}}}).$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'This suggests the key idea behind iterative BOND: if we know how to distill
    the Best-of-N distribution (i.e., via BOND), then we can apply BOND recursively
    (say $M$. The overall approach is depicted in [Figure 3](#S4.F3 "In 4.3 Iterative
    BOND ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    and summarized in [Algorithm 1](#alg1 "In 4.3 Iterative BOND ‣ 4 BOND Challenges
    and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5433d7e76346064327e61db61f21e541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Iterative BOND approach. The policy $\pi_{t}$ is used at each distillation
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, iterative BOND allows exponential scaling to arbitrary large
    $N$ in advance) while keeping a reduced sample complexity and a stable optimization.
    The claim is validated in the results below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments. In [Figure 4](#S4.F4 "In 4.3 Iterative BOND ‣ 4 BOND Challenges
    and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation"), we consider
    the same experimental setup than in [Figure 2](#S4.F2 "In 4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation") from [Section 4.2](#S4.SS2 "4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation"), fix the BOND objective to $J_{\text{effreys}}^{0.5}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Iterative BOND (meta algorithm)
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs: ${\pi_{\text{ref}}}$![Refer to caption](img/d4ecdfae7ce47c9ff665e41fe3803ca2.png)![Refer
    to caption](img/23a62a9c9e0f69d8bb9960cd09a3186d.png)![Refer to caption](img/c22d19890adb1ee1a4f84a0c307732cc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Iterative BOND (with $n=2$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 The J-BOND Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we present J-BOND, a concrete and practical BOND algorithm
    motivated by the results discussed in the previous sections. We describe its main
    components below, and summarize it in the pseudo-code of [Algorithm 2](#alg2 "In
    5 The J-BOND Algorithm ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'J-BOND follows the template of iterative BOND ([Algorithm 1](#alg1 "In 4.3
    Iterative BOND ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N
    Distillation")) with $n=2$ as defined in [Section 4.2](#S4.SS2 "4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimal sample complexity. Compared to the BOND algorithms tested in the previous
    section, J-BOND has a minimal sample complexity: for each prompt in the batch
    it generates $1$. While more anchor samples are generally useful for a better
    divergence estimation (in Section [4](#S4 "4 BOND Challenges and Algorithms ‣
    BOND: Aligning LLMs with Best-of-N Distillation") we used 16 MC samples), autoregressive
    sampling is the main bottleneck of online RLHF and we have thus opted for a practical
    approach working with a small number of samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 The J-BOND algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'Inputs: Prompt dataset $\mathcal{D}$ with the overall stochastic gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '*/ Update moving anchor */     Update anchor weights with EMA:  $\theta_{\text{anchor}}^{t+1}\leftarrow(1-\eta)\cdot\theta_{\text{anchor}}^{t}+\eta\cdot\theta_{t+1}$'
  prefs: []
  type: TYPE_NORMAL
- en: Crude divergence estimate based on 2 anchor samples. The policy and anchor samples
    are used to obtain a crude estimate of the forward and backward KL components
    of $J_{\text{effreys}}^{\beta}(\pi\,\|\,\text{Best-of-2}(\pi_{\text{anchor}}^{t}))$
    as described next.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can minimize the *forward KL* as described in [Section 4.2](#S4.SS2 "4.2
    Jeffreys divergence as a robust objective ‣ 4 BOND Challenges and Algorithms ‣
    BOND: Aligning LLMs with Best-of-N Distillation"), by doing supervised fine-tuning
    on the best of the 2 anchor samples. To minimize the *backward KL*, we utilize
    the policy gradient-style loss of [Equation 15](#S4.E15 "In 4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation") replacing $r_{\texttt{{BOND}\@}}(y)$ as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'That is, generation $y$ reward otherwise. The above definition is motivated
    by the following two main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We negatively reward $y$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We choose value $-\log(16)$).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exponential Moving Average (EMA) anchor. An important component of J-BOND,
    which refines the vanilla iterative BOND of [Section 4.3](#S4.SS3 "4.3 Iterative
    BOND ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation"),
    is the use of an *Exponential Moving Average (EMA)* anchor. That is, instead of
    using a periodically updated anchor, we update the anchor weights $\theta_{\text{anchor}}^{t}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{\text{anchor}}^{t+1}\leftarrow(1-\eta)\cdot\theta_{\text{anchor}}^{t}+\eta\cdot\theta_{t+1}.$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'Consistently with WARP (Ramé et al., [2024](#bib.bib50)), we observed that
    this weight averaging procedure has a positive effect on training stability by
    reducing variance, and can improve the overall reward/KL trade-off of J-BOND.
    We provide an ablation in [Section 6](#S6 "6 Experiments ‣ BOND: Aligning LLMs
    with Best-of-N Distillation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional KL regularization. Finally, we further regularize the policy to
    stay closer to the moving anchor via an extra²²2Note that $\mathrm{KL}$. The scope
    is to further stabilize the policy updates, viewing the overall operator as a
    constrained optimization one:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 6 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We test J-BOND on relevant use cases with the following main goals. First,
    we ablate and showcase important aspects of J-BOND: the benefits of the EMA anchor,
    and the effects of the anchor speed and the additional KL regularization. Then,
    we compare J-BOND to classical RLHF baselines using REINFORCE, demonstrating its
    efficacy and better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68fd781ee47aa10faf02150c5a645610.png)![Refer to caption](img/b1ce09808b8a400096f9203b5b3d3752.png)![Refer
    to caption](img/9a048be88a8e87cfb2f167bcdb1370ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: J-BOND with periodic anchor updates (every 50 steps) vs. EMA anchor
    ($\eta=0.02$) on Gemma 7B. While attaining the same reward (left), using the EMA
    anchor displays a significantly lower KL than the reference policy (middle) and
    thus a better reward/KL trade-off (right).'
  prefs: []
  type: TYPE_NORMAL
- en: Setup. We consider Gemma (2B and 7B) models (Gemma Team, [2024](#bib.bib22))
    which we aim to fine-tune into better conversational agents. For this task, we
    consider a set of conversational prompts $\mathcal{D}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'EMA vs. hard anchor updates. We ablate the benefits of using an EMA moving
    anchor ([Equation 18](#S5.E18 "In 5 The J-BOND Algorithm ‣ BOND: Aligning LLMs
    with Best-of-N Distillation")) compared to the periodically updated anchor used
    in Section [4.3](#S4.SS3 "4.3 Iterative BOND ‣ 4 BOND Challenges and Algorithms
    ‣ BOND: Aligning LLMs with Best-of-N Distillation"). For this, we run J-BOND with
    $\gamma=0$ roughly corresponds to an update period of 50 steps) but, crucially,
    J-BOND *with an EMA anchor displays a significantly lower KL increase* and, as
    a result, a better reward/KL trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e14f958053225cd8a3addde46573a3f.png)![Refer to caption](img/f5e7c07a7041a97d65c212e164603c07.png)![Refer
    to caption](img/0df4845367e7cbecf9eae60186145bd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: J-BOND: the role of the EMA mixing parameter $\eta$, improving the
    reward/KL trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ef9a1e67eb63efca503dc0c75109bbc.png)![Refer to caption](img/4c78846d0084cdf3a9f8a709f023b4b4.png)![Refer
    to caption](img/3fa889e61390181a44f630805cca2cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: J-BOND ($\eta=0.02$ trade-off.'
  prefs: []
  type: TYPE_NORMAL
- en: Anchor speed and $\mathrm{KL}$ trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison with standard RLHF. We compare J-BOND against standard RLHF algorithms
    that aim at maximizing the $\mathrm{KL}$). This highlight a key advantage of J-BOND:
    it does not require committing to a specific regularization level, but it continuously
    improves the reward displaying a stable and linear KL increase. Moreover, in the
    rightmost plot we plot the corresponding reward/KL trade-off showing that J-BOND
    produces a better reward/KL than all of the REINFORCE baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: J-BOND for Gemma open models. J-BOND was also the algorithm used to fine-tune
    open-weight models such as Gemma 1.1 2B and 7B (Gemma Team, [2024](#bib.bib22)),
    RecurrentGemma 2B and 9B (Botev et al., [2024](#bib.bib6)) as well as CodeGemma
    1.1 (CodeGemma Team, [2024](#bib.bib12)). This led to competitive performance.
    For example, the Gemma 1.1 IT 7B model outperforms Mistral 7B v0.2 Instruct in
    both safety and instruction following (see Gemma Team ([2024](#bib.bib22), Table
    5) for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Best-of-N was introduced in  Stiennon et al. ([2020](#bib.bib55)) as a straightforward
    but costly inference method to optimize language generation against a given reward
    function. Further works established and refined an analytical form for the KL
    divergence against the reference (i.e., Bo$1$-constrained RL (Yang et al., [2024](#bib.bib64))
    and provided scaling laws for Best-of-N alignment (Gao et al., [2023](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Matching Best-of-N for improved alignment is a strategy that was studied in
    different flavors in the literature. Dong et al. ([2023](#bib.bib16)) and Touvron
    et al. ([2023](#bib.bib58)) propose to fine-tune LLMs in a supervised fashion
    on Best-of-N data actually applying forward $\mathrm{KL}$ minimization. Concurrently
    to ours, Gui et al. ([2024](#bib.bib24)) proposes to mimic the Best-of-N policy
    by applying a combination of supervised fine-tuning on best responses and direct
    preference optimization on best-and-worst response pairs. The latter is similar
    to a common strategy in online preference optimization methods: Guo et al. ([2024](#bib.bib25))
    use pairwise AI feedback on online generations to obtain online preferences that
    are then optimized, while Calandriello et al. ([2024](#bib.bib7)) use a dedicated
    preference reward model instead. Concurrently and closest to our work, Amini et al.
    ([2024](#bib.bib3)) also apply distribution matching in order to get the benefits
    of Best-of-N sampling with amortized cost. While their formalization is identical,
    we opt for a different divergence (i.e., Jeffreys) than the one they use (i.e.,
    only backward KL), and propose an iterative procedure with dynamic anchor, which
    we show critical for optimal results. Best-of-N can also be used for self-improvement
    in reward modeling, as evidenced in Pace et al. ([2024](#bib.bib41)).'
  prefs: []
  type: TYPE_NORMAL
- en: Using a contrastive advantage is an option of J-BOND studied in prior works
    as well, which replaced a value estimate by the average Monte Carlo return of
    other samples. This was applied in the context of REINFORCE (Kool et al., [2019](#bib.bib31);
    Pinto et al., [2023](#bib.bib43)), for online RLHF (Ahmadian et al., [2024](#bib.bib2)),
    offline RLHF (Flet-Berliac et al., [2024](#bib.bib17)) and preference optimization (Wu
    et al., [2024](#bib.bib63)).
  prefs: []
  type: TYPE_NORMAL
- en: Exponential moving average (EMA) of policy as reference in regularization, which
    we use in J-BOND, is an increasingly popular option. While most alignment approaches
    use a static anchor, dynamic anchors bring the benefit of improving the flexibility
    of the policy space being explored (Munos et al., [2023](#bib.bib35); Gorbatovski
    et al., [2024](#bib.bib23); Ramé et al., [2024](#bib.bib50)), with the caveat
    that too slow updates limit optimization and too fast updates hinder stability.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling post-training and iterated amplification. BOND hinges on the idea of
    investing more resources during training to ensure that computational demands
    during inference remain low, a factor often overlooked in traditional scaling
    laws (Hoffmann et al., [2022](#bib.bib27)). Specifically, BOND incorporates the
    principles of iterated amplification (Christiano et al., [2018](#bib.bib9); Cotra,
    [2018](#bib.bib13)), where amplification in this context consists of producing
    multiple generations, comparing their rewards, and using these to iteratively
    improve the policy performance. In this regard, BOND is complementary to WARM
    (Ramé et al., [2024](#bib.bib49)) and WARP (Ramé et al., [2024](#bib.bib50)),
    which previously scaled post-training by training multiple reward models and policies,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce BOND, a novel RLHF method that fine-tunes the policy via online
    distillation of the Best-of-N sampling distribution. We propose a concrete algorithm,
    J-BOND, that integrates multiple components to enhance its practicality and efficiency;
    Monte-Carlo quantile estimation, a combination between forward and backward $\mathrm{KL}$-reward
    Pareto front of solutions, and compares favorably against state-of-the-art baselines.
    We hope this work can help improve alignment of AI systems, making them safer
    and more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Daniele Calandriello for insightful comments, as well as Gil Shamir,
    Bilal Piot, and Remi Munos for helpful discussions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agarwal et al. (2024) R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R.
    Garea, M. Geist, and O. Bachem. On-policy distillation of language models: Learning
    from self-generated mistakes. In *ICLR*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahmadian et al. (2024) A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer,
    A. Üstün, and S. Hooker. Back to basics: Revisiting REINFORCE style optimization
    for learning from human feedback in LLMs. *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amini et al. (2024) A. Amini, T. Vieira, and R. Cotterell. Variational Best-of-N
    alignment. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Askell et al. (2021) A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,
    A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,
    J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish,
    C. Olah, and J. Kaplan. A general language assistant as a laboratory for alignment.
    *arXiv preprint*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beirami et al. (2024) A. Beirami, A. Agarwal, J. Berant, A. D’Amour, J. Eisenstein,
    C. Nagpal, and A. T. Suresh. Theoretical guarantees on the Best-of-N alignment
    policy. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Botev et al. (2024) A. Botev, S. De, S. L. Smith, A. Fernando, G.-C. Muraru,
    R. Haroun, L. Berrada, R. Pascanu, P. G. Sessa, R. Dadashi, L. Hussenot, J. Ferret,
    S. Girgin, O. Bachem, A. Andreev, K. Kenealy, T. Mesnard, C. Hardin, S. Bhupatiraju,
    S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, A. Joulin, N. Fiedel,
    E. Senter, Y. Chen, S. Srinivasan, G. Desjardins, D. Budden, A. Doucet, S. Vikram,
    A. Paszke, T. Gale, S. Borgeaud, C. Chen, A. Brock, A. Paterson, J. Brennan, M. Risdal,
    R. Gundluru, N. Devanathan, P. Mooney, N. Chauhan, P. Culliton, L. G. Martins,
    E. Bandy, D. Huntsperger, G. Cameron, A. Zucker, T. Warkentin, L. Peran, M. Giang,
    Z. Ghahramani, C. Farabet, K. Kavukcuoglu, D. Hassabis, R. Hadsell, Y. W. Teh,
    and N. de Frietas. Recurrentgemma: Moving past transformers for efficient open
    language models. *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calandriello et al. (2024) D. Calandriello, D. Guo, R. Munos, M. Rowland, Y. Tang,
    B. A. Pires, P. H. Richemond, C. L. Lan, M. Valko, T. Liu, et al. Human alignment
    of large language models through online preference optimisation. In *ICML*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casper et al. (2023) S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer,
    J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems
    and fundamental limitations of reinforcement learning from human feedback. *TMLR*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2018) P. Christiano, B. Shlegeris, and D. Amodei. Supervising
    strong learners by amplifying weak experts. *arXiv preprint*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2017) P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg,
    and D. Amodei. Deep reinforcement learning from human preferences. In *NeurIPS*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark and Amodei (2016) J. Clark and D. Amodei. Faulty Reward Functions in the
    Wild. [https://openai.com/research/faulty-reward-functions](https://openai.com/research/faulty-reward-functions),
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CodeGemma Team (2024) CodeGemma Team. Codegemma: Open code models based on
    gemma. *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cotra (2018) A. Cotra. Iterated distillation and amplification. [https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616),
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cover (1999) T. M. Cover. *Elements of information theory*. 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dabney et al. (2017) W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional
    reinforcement learning with quantile regression. In *AAAI Conference on Artificial
    Intelligence*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2023) H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan,
    S. Diao, J. Zhang, K. SHUM, and T. Zhang. RAFT: Reward ranked finetuning for generative
    foundation model alignment. *TMLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flet-Berliac et al. (2024) Y. Flet-Berliac, N. Grinsztajn, F. Strub, E. Choi,
    C. Cremer, A. Ahmadian, Y. Chandak, M. G. Azar, O. Pietquin, and M. Geist. Contrastive
    policy gradient: Aligning llms on sequence-level scores in a supervised-friendly
    fashion. *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: French (1992) R. M. French. Semi-distributed representations and catastrophic
    forgetting in connectionist networks. *Connection Science*, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward
    model overoptimization. In *ICML*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geist et al. (2019) M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized
    markov decision processes. In *ICML*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemini Team (2023) Gemini Team. Gemini: A family of highly capable multimodal
    models. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemma Team (2024) Gemma Team. Gemma: Open models based on gemini research and
    technology. *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gorbatovski et al. (2024) A. Gorbatovski, B. Shaposhnikov, A. Malakhov, N. Surnachev,
    Y. Aksenov, I. Maksimov, N. Balagansky, and D. Gavrilov. Learn your reference
    model for real good alignment. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gui et al. (2024) L. Gui, C. Gârbacea, and V. Veitch. Bonbon alignment for large
    language models and the sweetness of Best-of-N sampling. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2024) S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares,
    A. Rame, T. Mesnard, Y. Zhao, B. Piot, J. Ferret, and M. Blondel. Direct language
    model alignment from online ai feedback. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hilton (2023) J. Hilton. KL divergence of max-of-n, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. (2022) J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,
    T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al.
    Training compute-optimal large language models. In *NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeffreys (1946) H. Jeffreys. An invariant form for the prior probability in
    estimation problems. *Proceedings of the Royal Society of London. Series A, Mathematical
    and Physical Sciences*, 186(1007):453–461, 1946.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral
    of experts. *arXiv preprint arXiv:2401.04088*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2015) D. P. Kingma and J. Ba. Adam: A method for stochastic
    optimization. In *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kool et al. (2019) W. Kool, H. van Hoof, and M. Welling. Buy 4 REINFORCE samples,
    get a baseline for free! 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kullback (1959) S. Kullback. *Information Theory and Statistics*. New York,
    1959.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lazaridou et al. (2020) A. Lazaridou, A. Potapenko, and O. Tieleman. Multi-agent
    communication meets natural language: Synergies between functional and structural
    language learning. In *ACL*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Z. Li, T. Xu, Y. Zhang, Y. Yu, R. Sun, and Z.-Q. Luo. Remax:
    A simple, effective, and efficient reinforcement learning method for aligning
    large language models. *arXiv preprint*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munos et al. (2023) R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland,
    Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, A. Michi, et al. Nash learning from
    human feedback. *arXiv preprint*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2021) R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,
    C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering
    with human feedback. *arXiv preprint*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayan et al. (2018) S. Narayan, S. B. Cohen, and M. Lapata. Don’t give me
    the details, just the summary! topic-aware convolutional neural networks for extreme
    summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors,
    *Conference on Empirical Methods in Natural Language Processing*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ngo et al. (2022) R. Ngo, L. Chan, and S. Mindermann. The alignment problem
    from a deep learning perspective. *arXiv preprint*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,
    P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models
    to follow instructions with human feedback. *NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pace et al. (2024) A. Pace, J. Mallinson, E. Malmi, S. Krause, and A. Severyn.
    West-of-n: Synthetic preference generation for improved reward modeling. *arXiv
    preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2022) A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward
    misspecification: Mapping and mitigating misaligned models. In *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinto et al. (2023) A. S. Pinto, A. Kolesnikov, Y. Shi, L. Beyer, and X. Zhai.
    Tuning computer vision models with task rewards. In *ICML*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiping Yang et al. (2024) J. Qiping Yang, S. Salamatian, Z. Sun, A. Theertha Suresh,
    and A. Beirami. Asymptotics of language model alignment. *arXiv*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.
    Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.
    Language models are unsupervised multitask learners. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D.
    Manning, and C. Finn. Direct preference optimization: Your language model is secretly
    a reward model. *arXiv preprint*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with
    a unified text-to-text transformer. *JMLR*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramé et al. (2024) A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron,
    O. Bachem, and J. Ferret. WARM: On the benefits of weight averaged reward models.
    In *ICML*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramé et al. (2024) A. Ramé, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot,
    P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard, and O. Bachem. WARP: On the
    benefits of weight averaged rewarded policies. *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reid et al. (2024) M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap,
    J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini
    1.5: Unlocking multimodal understanding across millions of tokens of context.
    *arXiv preprint*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roit et al. (2023) P. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi,
    M. Geist, S. Girgin, L. Hussenot, O. Keller, et al. Factually consistent summarization
    via reinforcement learning with textual entailment feedback. In *ACL*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
    O. Klimov. Proximal policy optimization algorithms. *arXiv preprint*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skalse et al. (2022) J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, and
    D. Krueger. Defining and characterizing reward gaming. In *NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiennon et al. (2020) N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss,
    A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human
    feedback. *NeurIPS*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (1998) R. S. Sutton and A. G. Barto. *Reinforcement Learning:
    An Introduction*. MIT Press, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2024) Y. Tang, D. Z. Guo, Z. Zheng, D. Calandriello, Y. Cao, E. Tarassov,
    R. Munos, B. Ávila Pires, M. Valko, Y. Cheng, and W. Dabney. Understanding the
    performance gap between online and offline alignment algorithms. *arXiv preprint*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, et al. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In *NeurIPS*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vieillard et al. (2020) N. Vieillard, T. Kozuno, B. Scherrer, O. Pietquin,
    R. Munos, and M. Geist. Leverage the average: an analysis of kl regularization
    in reinforcement learning. *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
    A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In
    *ICLR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) R. J. Williams. Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. *Reinforcement learning*, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2024) Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu. Self-play
    preference optimization for language model alignment. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2024) J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, and A. Beirami.
    Asymptotics of language model alignment. *arXiv preprint*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler et al. (2019) D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford,
    D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human
    preferences. *arXiv preprint*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Supporting results and derivations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A.1 Proof of [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 The Best-of-N distribution
    ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider $N$ being selected by Best-of-N sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The event $A_{i}(y)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{P}\mathopen{}\mathclose{{}\left[A_{i}(y)}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle={p_{<}}(y)^{i-1}\times{\pi_{\text{ref}}}(y)\times{p_{\leq}}(y)^{N-i-1}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The likelihood that Best-of-N sampling selects the generation $y$ is then given
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\pi_{\text{BoN}}}(y)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{i=1}^{N}\mathopen{}\mathclose{{}\left[{p_{<}}(y)^{i-1}\times{\pi_{\text{ref}}}(y)\times{p_{\leq}}(y)^{N-i}}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle={\pi_{\text{ref}}}(y)\times\sum_{i=1}^{N}\mathopen{}\mathclose{{}\left[{p_{<}}(y)^{i-1}\times{p_{\leq}}(y)^{N-i}}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: A.2 Link to the continuous case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A noteworthy observation is that we can relate the Best-of-N expression to the
    case of a continuous distribution, in which case the term $\mathtt{(B)}$ have
    the same value in this case).
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, recall that the probability for a sequence $y$ to be drawn from the
    Best-of-N distribution is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: Here, $y$ is the maximum length of a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we show why [Equation 20](#A1.E20 "In A.2 Link to the continuous case
    ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning LLMs with Best-of-N
    Distillation") matches the classic formula for the max of $N$ variables. Then,
    we have that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F_{X_{N}}(y)=\mathbb{P}(Y_{1}\leq y,\dots Y_{N}\leq y)=F_{X}(y)^{N},$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: and thus
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{X_{N}}(y)=f_{X}(y)F_{X}(y)^{N-1}N.$ |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'In [Equation 22](#A1.E22 "In A.2 Link to the continuous case ‣ Appendix A Supporting
    results and derivations ‣ BOND: Aligning LLMs with Best-of-N Distillation"), we
    recognize the Best-of-N formula in the case where the correction factor $\mathtt{(B)}$
    in the discrete case.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Backward KL and policy gradient equivalence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We formally show the analogy between the gradient of the backward KL divergence
    of [Equation 14](#S4.E14 "In 4.2 Jeffreys divergence as a robust objective ‣ 4
    BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    and the standard (e.g., REINFORCE (Williams, [1992](#bib.bib62))) policy gradient
    of a KL-regularized RLHF problem with equivalent reward $r_{\texttt{BOND}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact backward KL gradient can be derived as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\pi}\operatorname{KL}\mathopen{}\mathclose{{}\left(\pi\mid\mid{\pi_{\text{BoN}}}}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\nabla_{\pi}\sum_{y}\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{y}\nabla_{\pi}\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)+\pi(y)\nabla_{\pi}\log\pi(y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\nabla_{\pi}\log\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)}\right].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Above, we have used the product rule of gradient, the rule $\nabla_{\pi}\pi(y)=\pi(y)\nabla_{\pi}\log\pi(y)$.
  prefs: []
  type: TYPE_NORMAL
- en: '*Equivalence with Policy Gradient RL.* As anticipated, one can verify that
    descending the above gradient is equivalent – up to a constant scaling – to running
    the RL policy gradient REINFORCE algorithm on the RL objective of Equation [1](#S2.E1
    "Equation 1 ‣ 2 Problem Setup ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    with $r=r_{\texttt{BOND}}$ to break down the above gradient into:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\nabla_{\pi}\log\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: A.4 Derivation of J-BOND reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here we provide a theoretical explanation behind the design of the J-BOND reward
    function discussed in [Section 5](#S5 "5 The J-BOND Algorithm ‣ BOND: Aligning
    LLMs with Best-of-N Distillation"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Recall that $r_{\texttt{J-BOND}}(\cdot)$).
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in [Section 5](#S5 "5 The J-BOND Algorithm ‣ BOND: Aligning LLMs
    with Best-of-N Distillation"), we designed $r_{\texttt{J-BOND}}(\cdot)$ is motivated
    by the following main reason.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We want that, when sample $y$. For this purpose, let us consider the parametrized
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the stochasticity of $r_{\texttt{J-BOND}}^{\alpha}(y)$ and its expectation
    can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{y_{1}^{\prime},y_{2}^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r_{\texttt{J-BOND}}^{\alpha}(y)}\right]$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\alpha\cdot(1-{p_{\leq}}(y))^{2},$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 'where we have used the definition of ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha\cdot(1-0.5)^{2}=\log(0.5)\quad\rightarrow\alpha=-\log(16).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We illustrate this in [Figure 8](#A1.F8 "In A.4 Derivation of J-BOND reward
    ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning LLMs with Best-of-N
    Distillation"), where we plot the expected $r_{\texttt{J-BOND}}(y)$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5be2bfa6348b58f1ae049196ccf83537.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Expected value of the J-BOND reward, i.e., $\mathbb{E}_{y_{1}^{\prime},y_{2}^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r_{\texttt{J-BOND}}(y)}\right]$
    has median reward w.r.t. the anchor distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Learned quantile models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Monte-Carlo quantile estimation ([Section 4.1](#S4.SS1 "4.1 Monte-Carlo quantile
    estimation ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N
    Distillation")) approximates the reward quantiles by sampling multiple times from
    the reference policy ${\pi_{\text{ref}}}$, although they may have very similar
    reward quantiles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by this, in this section we explore an alternative approach that
    aims at *learning* a context-dependent quantile estimator $\widehat{{p_{\leq}}}_{\theta}(\cdot)$
    as the output of a binary classifier and train it via maximum likelihood estimation
    using the standard binary cross-entropy loss (Cover, [1999](#bib.bib14)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'We test such an approach in the abtractive summarization task considered in [Section 4](#S4
    "4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation").
    We parametrize $\widehat{{p_{\leq}}}_{\theta}(\cdot)$ can be re-used or learned
    offline with a fixed sample budget.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/760ae9ef9571b49e9b39c61739eb3c4d.png)![Refer to caption](img/29b8808dc1035d0652ecda9ebfdc82ff.png)![Refer
    to caption](img/77b08d2eb66c475f743e7d87b5af20a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: BOND (with $N=8$) using MC quantile estimates vs. a Learned quantile
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we remark that the explored approach is quite naive, and alternative
    learned quantile models can definitely be derived, e.g., further enforcing ordering
    in the predicted quantiles, using quantile regression (Dabney et al., [2017](#bib.bib15)),
    or assuming a pre-specified (e.g., Gaussian) rewards’ distributions.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Additional plots for BOND with Jeffreys divergence objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We provide additional experiments that complement the ablation of [Section 4.2](#S4.SS2
    "4.2 Jeffreys divergence as a robust objective ‣ 4 BOND Challenges and Algorithms
    ‣ BOND: Aligning LLMs with Best-of-N Distillation") when running BOND with a Jeffreys
    divergence objective $J_{\text{effreys}}^{\beta}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9420e33547fbd1ab84ceaad13e8a3890.png)![Refer to caption](img/bdbfb565e6f0d787dcb85042461f3e8c.png)![Refer
    to caption](img/edc2e57f0e97b3f42bdb78fa88420d6d.png)![Refer to caption](img/1d9b98519b18b9286860652024c27497.png)![Refer
    to caption](img/31c683a9c9a75f40fa11f45db707cc7b.png)![Refer to caption](img/41b099c60d0d8f856a7f4142c033201a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: BOND with $N=4$.'
  prefs: []
  type: TYPE_NORMAL
