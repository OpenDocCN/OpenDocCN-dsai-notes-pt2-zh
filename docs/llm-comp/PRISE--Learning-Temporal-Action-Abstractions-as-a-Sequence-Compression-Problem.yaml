- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10450](https://ar5iv.labs.arxiv.org/html/2402.10450)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ruijie Zheng
  prefs: []
  type: TYPE_NORMAL
- en: University of Maryland College Park
  prefs: []
  type: TYPE_NORMAL
- en: rzheng12@umd.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Ching-An Cheng'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research
  prefs: []
  type: TYPE_NORMAL
- en: chinganc@microsoft.com &Hal Daumé III
  prefs: []
  type: TYPE_NORMAL
- en: University of Maryland College Park, Microsoft Resesarch
  prefs: []
  type: TYPE_NORMAL
- en: me@hal3.name &Furong Huang
  prefs: []
  type: TYPE_NORMAL
- en: University of Maryland College Park
  prefs: []
  type: TYPE_NORMAL
- en: furongh@cs.umd.edu &Andrey Kolobov
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research
  prefs: []
  type: TYPE_NORMAL
- en: akolobov@microsoft.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Temporal action abstractions, along with belief state representations, are a
    powerful knowledge sharing mechanism for sequential decision making. In this work,
    we propose a novel view that treats inducing temporal action abstractions as a
    sequence compression problem. To do so, we bring a subtle but critical component
    of LLM training pipelines – input tokenization via byte pair encoding (BPE) –
    to the seemingly distant task of learning skills of variable time span in continuous
    control domains. We introduce an approach called Primitive Sequence Encoding (PRISE)
    that combines continuous action quantization with BPE to learn powerful action
    abstractions. We empirically show that high-level skills discovered by PRISE from
    a multitask set of robotic manipulation demonstrations significantly boost the
    performance of both multitask imitation learning as well as few-shot imitation
    learning on unseen tasks. ¹¹1Our code will be released at [https://github.com/FrankZheng2022/PRISE](https://github.com/FrankZheng2022/PRISE).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: High-dimensional observations and decision-making over complex, continuous action
    spaces are hallmarks of typical scenarios in sequential decision-making, including
    robotics. A common way of dealing with these complexities is constructing *abstractions*
    – compact representations of belief states and actions that generalize across
    tasks and make learning to act in new scenarios robust and data-efficient. Inspired
    by techniques that have enabled ground-breaking progresses in computer vision
    (CV) and natural language processing (NLP) over the past decade, much of continuous
    control research has focused on *learning* abstractions from data rather than
    hand-crafting them. The lion’s share of these efforts study learning multi-task
    representations, which has analogies to representation learning in CV and NLP
    and has been tackled by adapting these fields’ models (see, e.g., DT (Chen et al.,
    [2021](#bib.bib6)) vs. GPT-2 (Radford et al., [2019](#bib.bib35))) and methods
    (see, e.g., R3M (Nair et al., [2022](#bib.bib29)) vs. InfoNCE (den Oord et al.,
    [2019](#bib.bib8))).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, learning *temporal action abstractions* – representations
    of multi-step primitive behaviors – has not benefited nearly as much from such
    a methodology transfer. This omission is especially glaring in continuous control,
    where complex policies can clearly be decomposed into versatile lower-level routines
    such as picking up objects, walking, etc, and whose popular learning method, Behavior
    Cloning (BC) (Arora et al., [2022](#bib.bib2)), has many commonalities with LLM
    training.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we posit that adapting discrete coding and sequence compression
    techniques from NLP offers untapped potential for action representation learning
    for continuous control. Specifically, given a pretraining dataset of demonstrations
    from multiple decision tasks over a continuous action space and a high-dimensional
    pixel observation space, we consider the problem of learning temporally extended
    action primitives, i.e., *skills*, to improve downstream learning efficiency of
    reward-free BC. We show that embedding continuous actions into *discrete* codes
    and then applying a popular NLP sequence compression method called *Byte Pair
    Encoding (BPE)* (Gage, [1994](#bib.bib11)) to the resulting discrete-code sequences
    identifies variable-timespan action primitives with the property we desire. Namely,
    in downstream tasks, policies learned by BC using these action primitives *consistently
    perform better*, often substantially, than policies learned by BC directly over
    the original action space.
  prefs: []
  type: TYPE_NORMAL
- en: Our work’s main contribution is *Primitive Sequence Encoding (PRISE)*, a novel
    method for learning multi-task temporal action abstractions that capitalizes on
    a novel connection to NLP methodology. PRISE quantizes the agent’s original continuous
    action space into discrete codes, converts the pretraining training trajectories
    into action code sequences, and uses BPE to induce variable-timestep skills. During
    BC for downstream tasks, learning policies over these skills and then decoding
    skills into primitive action sequences gives PRISE a significant boost in learning
    efficiency over strong baselines such as ACT (Zhang et al., [2021](#bib.bib50)).
    We conduct extensive ablation studies to show the effect of various parameters
    on PRISE’s performance, which demonstrate BPE to be critical to PRISE’s success.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem Setting.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We consider a set of tasks $T$ is the image space.
  prefs: []
  type: TYPE_NORMAL
- en: Let $\mathcal{D}=\bigcup_{\mathcal{T}\in\mathscr{T}}\mathcal{D}_{\mathcal{T}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Quantization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to construct the temporal action abstrskill tokens, our algorithm PRISE 
    first converts continuous actions into discrete codes. To do so, it uses the vector
    quantization module proposed in Van den Oord et al. ([2017](#bib.bib44)). This
    vector quantization module $\mathcal{F}$, and the second term prevents the query
    vector from switching between different codes.
  prefs: []
  type: TYPE_NORMAL
- en: Byte Pair Encoding (BPE).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After quantizing the action space into codes, PRISE identifies common code (action)
    sequences via a technique from the realm of NLP, the method of Byte Pair Encoding
    (BPE) (Gage, [1994](#bib.bib11); Sennrich et al., [2016](#bib.bib40)). In NLP,
    BPE has emerged as a pivotal technique for managing the vast and varied vocabulary
    encountered in textual data. Language models are expected to predict a text completion
    based on a text prefix. Doing so naively, by generating the text character-by-character
    is problematic, because the notion of a character differs significantly across
    languages. Instead, NLP models operate at the language-agnostic level of *bytes*.
    However, the high granularity of byte prediction has its own computational challenges.
    Therefore, language models models first find high-frequency byte *sequences* in
    the training data, assign *tokens* to these sequences, learn to predict the resulting
    tokens, which then get decoded into text. BPE is the algorithm that handles the
    common byte sequence construction problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'BPE operates by merging the most frequent pairs of bytes, assigning tokens
    to these pairs, and iteratively merging new pairs of tokens or bytes. Thereby,
    it builds up a vocabulary of tokens that plays a crucial role in modern large
    language models OpenAI ([2023](#bib.bib30)); Brown et al. ([2020](#bib.bib4));
    Radford et al. ([2019](#bib.bib35)). See [Figure 2](#S3.F2 "In 3.1 Pretraining
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem") for a demonstration. We apply BPE in the same way, but to action codes
    instead of bytes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ccedd27d5fb6755159b5ad169e16e03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (a) Pretraining Stage I of PRISE : The goal is to learn a action
    quantization module such that conditioned on the current state and action $(o_{t},a_{t})$,
    it could assign a discrete action code. (b) Pretraining Stage II of PRISE : First
    it converts a trajectory of continuous state and actions into discrete codes.
    Then based on the corpus of quantized trajectories from the multitask offline
    dataset, PRISE applies BPE (illustrated in [Figure 2](#S3.F2 "In 3.1 Pretraining
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")) to learn vocabulary of skill tokens, where each token represents a
    sequence of discrete action code.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 1](#S2.F1 "In Byte Pair Encoding (BPE). ‣ 2 Preliminaries ‣ PRISE:
    Learning Temporal Action Abstractions as a Sequence Compression Problem") illustrates
    the key steps of PRISE. At a high level, PRISE first learns a state-dependent
    action quantization module (Pretraining I stage). This module processes the pretraining
    multitask dataset $\mathcal{D}$, assigns a token to each of them, and thereby
    constructs primitive skills, shown in [Figure 1](#S2.F1 "In Byte Pair Encoding
    (BPE). ‣ 2 Preliminaries ‣ PRISE: Learning Temporal Action Abstractions as a Sequence
    Compression Problem")(b). Once these primitive skill tokens are discovered, they
    can be used to relabel a demonstration dataset and learn via behavior cloning
    (BC) a policy that chooses these skill tokens instead of raw actions, as we later
    show in [Section 3.2](#S3.SS2 "3.2 Multitask generalist policy learning. ‣ 3 Algorithm
    ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem")
    and [Section 3.3](#S3.SS3 "3.3 Downstream few-shot adaptation to unseen tasks
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem"). In the rest of the section, we describe PRISE in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Pretraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pretraining I: action quantization. This stage is illustrated in [Figure 1](#S2.F1
    "In Byte Pair Encoding (BPE). ‣ 2 Preliminaries ‣ PRISE: Learning Temporal Action
    Abstractions as a Sequence Compression Problem")(a), and its pseudocode is available
    in [Figure 11](#A3.F11 "In Appendix C Implementation Details ‣ PRISE: Learning
    Temporal Action Abstractions as a Sequence Compression Problem") in [Appendix C](#A3
    "Appendix C Implementation Details ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem"). Let $\mathcal{G}:\mathcal{O}^{T}\rightarrow\mathcal{Z}$
    into that action code. A similar intuition underlies action representation learning
    in, e.g., (Chandak et al., [2019](#bib.bib5)), although that work doesn’t attempt
    to simultaneously learn a latent state space.'
  prefs: []
  type: TYPE_NORMAL
- en: First, to ensure the action code can predict future states, we train a latent
    forward transition model $\mathcal{T}:\mathcal{Z}\times\mathcal{A}\rightarrow\mathcal{Z}$.
    To optimize the forward latent dynamics model while preventing state and action
    representation collapse, we adopt a BYOL-like objective inspired from (Schwarzer
    et al., [2021](#bib.bib39)), where we minimize
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{dyn}}[$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, $o_{t,T}=o_{t},o_{t-1},...,o_{t-T+1}$ has 1.
  prefs: []
  type: TYPE_NORMAL
- en: Next, to guarantee the raw action can be decoded from the action code and latent
    state effectively, we train a latent state-dependent decoder, $\psi$ following Mandlekar
    et al. ([2021](#bib.bib27)). This choice has been found effective in dealing with
    the inherent multimodality and noise in such human teleoperation demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the learning objective becomes to minimize the negative log likelihood
    of the GMM distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'When we know the the pretraining data is collected by a fixed deterministic
    policy, we let $\psi:\mathcal{Z}\times\mathcal{E}\rightarrow\mathcal{A}$-loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Combining the two objectives, we pretrain the state encoder and action encoder/quantization
    module by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}[\theta_{\mathcal{F},\mathcal{G},Q,P,\psi}]=\mathcal{L}_{\text{dyn}}[\theta_{\mathcal{F},\mathcal{G},Q,P}]+\beta\mathcal{L}_{\text{act\_decode}}[\theta_{\mathcal{F},\mathcal{G},\psi}]$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Throughout the experiment, we set $\beta=1$ is parametrized by GMM to deal with
    numerical scale of the likelihood loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretraining II: temporal action abstractions via BPE. [Figure 1](#S2.F1 "In
    Byte Pair Encoding (BPE). ‣ 2 Preliminaries ‣ PRISE: Learning Temporal Action
    Abstractions as a Sequence Compression Problem")(b) illustrates the mechanics
    of this stage. After training the action quantizer, we first use the pretrained
    observation embedding $\mathcal{G}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34ed44c3a64c2084cbb0ccfbce463b5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Byte Pair Encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Overall, the pretraining of PRISE produces observation embedding $\mathcal{G}$
    of skill tokens. In the following, we discuss how to use them to learn generalist
    multitask policies and achieve efficient downstream adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Multitask generalist policy learning.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have learned the skill tokens that capture the common motion patterns
    shared across various tasks, we can leverage these skills to learn a multitask
    generalist policy. We train a high-level skill-token policy $\pi:\mathcal{Z}\rightarrow\Delta(\mathcal{V})$
    again to choose the next sequence of actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90fd936dd7840e060cca472fdc9519f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: During evaluation time, PRISE rollout its policy by querying the
    skill-token policy $\pi$ to decode actions.'
  prefs: []
  type: TYPE_NORMAL
- en: To learn this skill-token policy $\pi$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f21acec4a339b4a619ac43341375e06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: PRISE tokenizes downstream demonstration trajectories by greedily
    searching for the longest token for each time step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then train $\pi$ by minimizing the cross-entropy loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})[\theta_{\pi}]$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Note, that as stopgrad implies, we freeze the encoder $\mathcal{G}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Downstream few-shot adaptation to unseen tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to learning a generalist multitask policy $\pi$ itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'We optimize $\psi$ in each trajectory has been omitted for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]=\mathbb{E}_{\xi_{t}\sim\pi(\text{stopgrad}(z_{t}))}\Big{[}\mathcal{L}[\theta_{\psi}](\xi)\Big{]}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'In this equation, $\hat{K}=\min(K,L_{\xi})$ is a hyperparameter, the motivation
    behind which is explained at the end of [Section 3.4](#S3.SS4 "3.4 Discussion
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We optimize skill-token policy $\pi$ in ([5](#S3.E5 "Equation 5 ‣ 3.2 Multitask
    generalist policy learning. ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem")). Thus, the overall objective is'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}[\theta_{\pi,\psi}]=\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})+\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]\vspace{-0.8em}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Note that the gradients of the decoder finetuning loss $\mathcal{L}_{\textbf{FT\_DECODER}}$
    should be aware of the decoder error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The underlying rationale for making the objective pay attention to matching
    both the skill tokens and the actions that these tokens generate is as follows.
    The number of skill tokens in the vocabulary is often large in order to encapsulate
    all motion patterns observed in the pretraining dataset, and simply minimizing
    the cross-entropy loss over the tokens with sparse data from few expert trajectories
    does not suffice to learn an accurate skill-token policy. Instead, the objective
    in ([8](#S3.E8 "Equation 8 ‣ 3.3 Downstream few-shot adaptation to unseen tasks
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")) says that the skill-token policy $\pi$ should not only match the target
    token but also predict the tokens that have small decoding errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PRISE combines state-action abstraction (by optimizing the quantization loss
    in [Equation 4](#S3.E4 "In 3.1 Pretraining ‣ 3 Algorithm ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem")) and a temporal flavor
    of action abstraction (by applying BPE tokenization) to reduce the complexity
    of downstream IL problems. It is known that the complexity of IL depends on mainly
    two factors: the size of the state-action space and the problem horizon (Rajaraman
    et al., [2020](#bib.bib36)). The former determines the minimum complexity of the
    learner’s policy. The latter determines how much the behavior cloning error will
    compound. PRISE addresses these two factors by its state-action abstraction and
    temporal abstraction, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of PRISE’s abstraction scheme is determined by several hyperparameters.
    First, the codebook size, which determines the granularity of quantization, trades
    off the approximation error of state-action abstraction and the complexity of
    the state-action representation that the learner’s policy sees. We want the codebook
    to be large enough to represent the demonstrator’s policy well, but not too large
    in order to avoid several codes collapsing to the same set of actions in the original
    space. Many duplicate codes would reduce the amount of temporal abstraction PRISE
    can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, the size of the token vocabulary, which is at least as large as the
    codebook size by the design of BPE, not only affects the complexity of the state-action
    representation, but also controls how generalizable the learned tokens are. When
    the token vocabulary is large, BPE picks up less frequent patterns, which typically
    are longer. Although these tokens may help compress the length of the training
    data, they might overfit to the training trajectories and end up never getting
    used in downstream tasks. Moreover, the overfit tokens might be too long and lead
    to approximation errors when decoded back to the original action space. PRISE’s
    hierarchical learning scheme implicitly assumes the expert policy applies the
    action codes in an *open-loop* manner up the horizon $L_{\xi}$ (see [Section 3.3](#S3.SS3
    "3.3 Downstream few-shot adaptation to unseen tasks ‣ 3 Algorithm ‣ PRISE: Learning
    Temporal Action Abstractions as a Sequence Compression Problem")) that caps skills’
    possible horizon in downstream adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, when the token vocabulary size is set to be too small, the
    effects of temporal abstraction are minimal and would not reduce learning complexity.
    In the extreme, when the vocabulary size is the same as the codebook size, PRISE
    would only perform state-action abstraction by learning a discrete action space
    with state-action dependent encoder and decoder, without reducing the compounding
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please refer to [Appendix A](#A1 "Appendix A Detailed Related Work ‣ PRISE:
    Learning Temporal Action Abstractions as a Sequence Compression Problem") for
    an extensive discussion of related work. Here we provide its summary'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temporal Action Abstractions in Sequential Decision-Making: The concept is
    rooted in the literature of fully and partially observable MDPs (Puterman, [1994](#bib.bib34);
    Sutton & Barto, [2018](#bib.bib41)). The complexity of policy learning is proportional
    to the problem horizon (Ross et al., [2011](#bib.bib38); Jiang et al., [2015](#bib.bib15);
    Rajaraman et al., [2020](#bib.bib36); Xie et al., [2021](#bib.bib47)), leading
    to hierarchical approaches (Parr & Russell, [1998](#bib.bib31); Barto & Mahadevan,
    [2003](#bib.bib3); Le et al., [2018](#bib.bib22); Nachum et al., [2018](#bib.bib28);
    Kipf et al., [2019](#bib.bib18); Kujanpää et al., [2023](#bib.bib21)) and the
    use of options or primitives (Sutton et al., [1999](#bib.bib42); Ajay et al.,
    [2021](#bib.bib1)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temporal Abstraction Discovery for Decision-Making: Various methods like CompILE
    (Kipf et al., [2019](#bib.bib18)), RPL (Gupta et al., [2019](#bib.bib12)), OPAL
    (Ajay et al., [2021](#bib.bib1)), TAP (Jiang et al., [2023](#bib.bib16)), and
    ACT (Zhao et al., [2023a](#bib.bib51)) learn temporally extended action primitives
    in two stages. DDCO (Krishnan et al., [2017](#bib.bib19)), in contrast, learns
    both primitives and high-level policy simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: PRISE differs in its use of BPE for inducing temporal action abstractions, avoiding
    challenges faced by CompILE (Kipf et al., [2019](#bib.bib18)), RPL, and OPAL.
    ACT (Zhao et al., [2023b](#bib.bib52)) is the closest to PRISE, especially in
    using BC, not RL, for downstream learning and handling pixel observations rather
    than low-level states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy Quantization Methods: PRISE relates to SAQ (Luo et al., [2023](#bib.bib24))
    and employs VQ-VAE (van den Oord et al., [2017](#bib.bib43)), with Conditional
    VAE (Kingma & Welling, [2014](#bib.bib17)) being another common choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Temporal Abstractions vs. Skills in Robot Learning: Distinct from skill acquisition
    approaches like DMP (Pastor et al., [2009](#bib.bib32)), Play-LMP (Lynch et al.,
    [2019](#bib.bib26)), and MCIL (Lynch & Sermanet, [2021](#bib.bib25)), PRISE and
    methods like TAP (Jiang et al., [2023](#bib.bib16)) focus on learning temporally
    extended action primitives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretraining Data Assumptions: PRISE aligns more with Play-LMP (Lynch et al.,
    [2019](#bib.bib26)), RPL (Gupta et al., [2019](#bib.bib12)), MCIL (Lynch & Sermanet,
    [2021](#bib.bib25)), and TACO-RL (Rosete-Beas et al., [2022](#bib.bib37)), requiring
    meaningful behavioral patterns in pretraining data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenization in Language Models: BPE (Gage, [1994](#bib.bib11)), Unigram (Kudo,
    [2018](#bib.bib20)), and WordPiece (Devlin et al., [2018](#bib.bib9)) are crucial
    in training language models. PRISE extends the next-token-prediction paradigm
    to continuous control, dealing with challenges like high-dimensional image observations
    and continuous action spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we empirically evaluate the effectiveness of the skill tokens
    of PRISE. We run PRISE to pretrain these tokens using large-scale, multitask offline
    datasets. Then we evaluate them on two offline IL scenarios: learning a multitask
    generalist policy and few-shot adaptation to unseen tasks. We show that PRISE
    is able to improve the performance of the learner policy, both compared to an
    PRISE version approaches that doesn’t use the skill tokens and compared to a very
    strong existing approach, ACT Zhao et al. ([2023b](#bib.bib52)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Pretraining datasets and architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate PRISE on two multitask robotic manipulation benchmarks: Metaworld (Yu
    et al., [2019](#bib.bib49)) and LIBERO (Liu et al., [2023](#bib.bib23)). Below
    we describe the setups of these datasets and the architectures used in the experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64afc6f1162779ff2350dc0ebf2a68d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Few-shot IL on unseen tasks: pretrain and test tasks split for MetaWorld
    and LIBERO.'
  prefs: []
  type: TYPE_NORMAL
- en: For LIBERO, we pretrain on the 90 short-horizon tasks (LIBERO-90) with offline
    dataset provided by the original paper. We test the learned skill tokens both
    in terms of the multitask learning performance on LIBERO-90 as well as 5-shot
    imitation learning (IL) performance on the first 8 unseen tasks from LIBERO-LONG.
    The pretraining dataset contains 50 demonstration trajectories for each task,
    collected by human teleoperation. We use the exact architecture of ResNet-T in Liu
    et al. ([2023](#bib.bib23)) for the observation embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'For MetaWorld, we focus on 5-shot IL since multitask IL on pretraining tasks
    is straightforward, with baseline algorithms, including PRISE, all achieving an
    average success rate of around 80%. We refer the readers to [Figure 13](#A4.F13
    "In Appendix D Additional Results on Multitask Learning ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem") in [Appendix D](#A4 "Appendix
    D Additional Results on Multitask Learning ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem") for the results. For the pretrain-test split,
    we hold out five hard tasks (hand-insert, box-close, disassemble, stick-pull,
    pick-place-wall) for few-shot evaluation and perform pretraining on the rest 45
    tasks. We generate 100 expert trajectories for each pretraining task using the
    scripted policy provided in MetaWorld. Same as in as in Yarats et al. ([2022](#bib.bib48)),
    we use a shallow CNN encoder to encode the observation, and a transformer decoder
    module as the temporal backbone to encode temporal information into the latent
    embedding $z_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For BPE on both domains, we set the codebook size $C$ to be 10 and vocabulary
    size to be 150, and we provide detailed ablation study to analyze their impacts
    later in this section. For more implementation details, we refer the readers to
    Appendix [C](#A3 "Appendix C Implementation Details ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18cf1c7007af13d51492f225a737b5ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Multitask policy learning performance on LIBERO-90. Error bar represents
    the standard deviation across 3 random seeds. Results of the first three methods
    are taken from (Liu et al., [2023](#bib.bib23))'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Multitask generalist policy learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First we evaluate whether the pretrained skill tokens of PRISE could enable
    efficient knowledge sharing across tasks and improve multitask IL performance.
    Here we focus on LIBERO-90, a challenging multi-task benchmark where existing
    algorithms and architectures have not demonstrated satisfactory performance. For
    each algorithm in the following comparison, we first perform pretraining on the
    LIBERO-90, if applicable, and then train a multi-task generalist policy on the
    same dataset based on the pretrained outcomes (e.g., encoders, skill tokens).
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. For multitask learning, our comparison includes PRISE and three
    network architectures for multitask behavior cloning (BC): ResNet-RNN, ResNet-T,
    and ViT-T, as implemented in (Liu et al., [2023](#bib.bib23)). These architectures
    utilize either ResNet-18 (He et al., [2016](#bib.bib14)) or ViT Dosovitskiy et al.
    ([2021](#bib.bib10)) to encode the pixel observations and then apply either a
    transformer or a LSTM module as temporal backbone to process a sequence of visual
    tokens. Recall that the main architecture of PRISE is the same as ResNet-T, except
    we add the vector quantization as well as other modules unique to PRISE. Additionally,
    we compare with ACT (Zhao et al., [2023a](#bib.bib51)), an IL algorithm that learns
    a generative model for sequences of actions, termed “action chunks,” rather than
    single-step actions. These action chunks can be seen as a form of temporal action
    abstraction. During policy rollout, ACT combines overlapping action chunks predicted
    by its policy using a weighted average. Thus, the critical hyperparameter of ACT
    is the length of the action chunk, which we set at 8 after evaluating the best
    option from five choices: $\{3,5,8,10,15\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Results. Figure [6](#S5.F6 "Figure 6 ‣ 5.1 Pretraining datasets
    and architecture ‣ 5 Experiments ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem") presents the average success rate across 90
    tasks in the LIBERO-90 dataset. It clearly demonstrates the significance of temporal
    action abstraction for knowledge sharing across diverse tasks, as Multitask ACT
    significantly outperforms the baseline BC approaches. More importantly, the use
    of pretrained skill tokens in PRISE  further leads to a substantial performance
    improvement over all other existing algorithms, underscoring the efficacy of PRISE’s
    pretrained skill tokens. For the details (e.g., per-task success rateas well as
    the evaluation protocol), we refer readers to Appendix [D](#A4 "Appendix D Additional
    Results on Multitask Learning ‣ PRISE: Learning Temporal Action Abstractions as
    a Sequence Compression Problem").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Few-shot adaptation to unseen tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to learning a generalist multitask policy, we show that the learned
    skill token of PRISE can also make learning a new task more efficient. Here, we
    evaluate the 5-shot IL performance for unseen tasks in both MetaWorld and LIBERO.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. We compare PRISE with the baselines below. (1) BC from scratch.
    (2) BC with pretrained CNN encoder from PRISE: This is a more computationally
    efficient version of the first. We find that whether freezing the CNN encoder
    or not does not have statistically significant effects on the downstream performance.
    (3) PRISE without BPE: This is the same as PRISE with a skill-token vocabulary
    $V=\{1,...,C\}$ of just the quantized action codes. That is, there is no temporal
    abstraction. This baseline can be viewed not only as an ablated version of PRISE but
    also as an analog to existing hierarchical skill learning approaches such as OPAL (Ajay
    et al., [2021](#bib.bib1)) and TACO-RL (Rosete-Beas et al., [2022](#bib.bib37)).
    We refer readers to Section [4](#S4 "4 Related Work ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem") and Section [A](#A1 "Appendix
    A Detailed Related Work ‣ PRISE: Learning Temporal Action Abstractions as a Sequence
    Compression Problem") for a detailed discussion. (4) ACT (Zhao et al., [2023a](#bib.bib51))
    from scratch. (5) ACT (Zhao et al., [2023a](#bib.bib51)) with pretrained CNN encoder
    from PRISE. We freeze the CNN encoder and finetune other parts of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: In few-shot learning, we train each baseline algorithm for 30 epochs and evaluate
    each algorithm every 3 epochs. We then report the best success rate for the 10
    evaluated checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aec6252e6c3bbbde671da7ba4cd3b0ef.png)![Refer to caption](img/d5be07666d3a63fd9f46d83f1407e7de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: (Left): 5-shot IL performance averaged across 5 unseen MetaWorld
    tasks. (Right): 5-shot IL performance averaged across 8 unseen LIBERO tasks. Error
    bar stands for the standard deviation across 3 random seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental Results (MetaWorld). In  [Figure 7](#S5.F7 "In 5.3 Few-shot adaptation
    to unseen tasks ‣ 5 Experiments ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem"), we plot the averaged task success rate of
    5-shot IL across 5 unseen tasks in MetaWorld. As shown in the figure, PRISE surpasses
    all other baselines (including PRISE w/o BPE) with a large margin, highlighting
    the effectiveness of the learned temporally extended skill tokens in adapting
    to unseen downstream tasks. Furthermore, we observe that with the pretrained visual
    encoder from PRISE, BC and ACT consistently yields improved results, indicating
    that the PRISE learning objective is also beneficial for pretraining visual representations
    from multitask offline data.'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Results (LIBERO). We present the average 5-shot IL performance
    across 8 tasks of LIBERO in LABEL:fig:libero_unseen. Different from MetaWorld,
    as shown in the figure, PRISE pretrained encoder does not improve the performance
    of the base IL algorithms for BC and ACT. This is consistent with what is reported
    in (Liu et al., [2023](#bib.bib23)), where they observe that pretraining on LIBERO-90
    with multitask BC even leads to negative transfer to downstream tasks. However,
    with pretrained skill tokens from PRISE, we could improve the average success
    rate by 9.2% compared with the best baseline algorithms. This further demonstrates
    the effectiveness of the proposed skill tokenization mechanisms by PRISE.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Ablation Analyses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vocabulary Size. The vocabulary size of the BPE tokenizer, $|\mathcal{V}|$,
    indicating that as long as the vocabulary size is not too large or small, the
    performance should not vary significantly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afa11c71fad0e71e69a7294727eb1060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: (Left) Mean success rate of PRISE  across 8 unseen LIBERO tasks with
    different vocabulary size. (Right) The mean success rate of PRISE across 8 unseen
    LIBERO tasks with a different numbers of discrete action codes pre-trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of Action Codes. Additionally, we verify the effects of varying the
    number of action codes, $C$ in [Figure 9](#S5.F9 "In 5.4 Ablation Analyses ‣ 5
    Experiments ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem") . The comparison reveals a clear reduction in token length with an increase
    in the number of codes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/adbb3d57279314b9d47b096915111e04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: (Left) Histogram of token length when $C=10$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Latent-forward-dynamics objective is crucial for learning good action codes.
    The forward-dynamics objective introduced in PRISE plays a crucial role in learning
    action code. Since we aim to learn a decoder $\psi(z_{t},e_{t})$ and leading to
    the collapse of action codes (i.e., different codes being decoded into identical
    actions). In Figure [10](#S5.F10 "Figure 10 ‣ 5.4 Ablation Analyses ‣ 5 Experiments
    ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem"),
    we empirically compare the few-shot IL performance of PRISE in LIBERO with and
    without the forward dynamics objective. Indeed, we see a performance degradation
    in both domains when the forward dynamics objective is removed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d015f7d484bf75089e7c52268a87ddce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Performance of PRISE with and without the forward dynamics objective
    on Metaworld and LIBERO.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we conducted an extra experiment on LIBERO to illustrate the action
    collapsing problem further. Here, we sample a batch of 5000 data points $\{(z_{i},a_{i})\}_{i=1}^{5000}$
    for PRISE with and without the forward dynamics objective.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PRISE | PRISE w/o forward dynamics |'
  prefs: []
  type: TYPE_TB
- en: '| $\zeta$ | 72.42 | 14.53 |'
  prefs: []
  type: TYPE_TB
- en: As illustrated in the table, PRISE without the forward dynamics objective exhibits
    a significantly smaller code-wise distance, demonstrating the necessity of the
    latent-forward-dynamics objective.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose a new temporal action method for continuous control
    scenarios,  PRISE, that leverages powerful NLP methodology during pretraining
    for sequential decision making tasks. By first pretraining a vector quantization
    module to discretize action into codes and then apply Byte Pair Encoding tokenization
    algorithm to learn temporally extended skill tokens, PRISE’s pretrained skill
    tokens can capture diverse motion patterns shared across pretraining tasks, enabling
    efficient multitask policy learning and few-shot adaptation to unseen tasks. One
    exciting future direction is to further scale up this approach to large real robot
    dataset with diverse robot embodiments such as Open Embodiment X (Collaboration
    et al., [2023](#bib.bib7)). Additionally, instead of finetuning the model to different
    downstream tasks tabula rasa, we could leverage the pretrained tokens to instruct
    finetune an existing large language model, so that we could leverage the power
    of foundational models for generalizing across different tasks and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zheng and Huang are supported by National Science Foundation NSF-IIS-2147276
    FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air
    Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-DARPA-Defense
    Advanced Research Projects Agency Guaranteeing AI Robustness against Deception
    (GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ajay et al. (2021) Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum,
    O. OPAL: Offline primitive discovery for accelerating offline reinforcement learning.
    In *ICLR*, 2021. URL [https://openreview.net/forum?id=V69LGwJ0lIN](https://openreview.net/forum?id=V69LGwJ0lIN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arora et al. (2022) Arora, K., Asri, L. E., Bahuleyan, H., and Cheung, J. C. K.
    Why exposure bias matters: An imitation learning perspective of error accumulation
    in language generation. In *Findings of ACL*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barto & Mahadevan (2003) Barto, A. and Mahadevan, S. Recent advances in hierarchical
    reinforcement learning. *Discrete Event Dynamic Systems: Theory and Applications*,
    13:41–77, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
    Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
    B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M., and Lin, H. (eds.), *Advances in Neural Information Processing
    Systems*, volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chandak et al. (2019) Chandak, Y., Theocharous, G., Kostas, J., Jordan, S.,
    and Thomas, P. Learning action representations for reinforcement learning. In
    *International conference on machine learning*, pp.  941–950\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement
    learning via sequence modeling. In *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collaboration et al. (2023) Collaboration, O. X.-E., Padalkar, A., Pooley,
    A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh,
    A., Brohan, A., Raffin, A., Wahid, A., Burgess-Limerick, B., Kim, B., Schölkopf,
    B., Ichter, B., Lu, C., Xu, C., Finn, C., Xu, C., Chi, C., Huang, C., Chan, C.,
    Pan, C., Fu, C., Devin, C., Driess, D., Pathak, D., Shah, D., Büchler, D., Kalashnikov,
    D., Sadigh, D., Johns, E., Ceola, F., Xia, F., Stulp, F., Zhou, G., Sukhatme,
    G. S., Salhotra, G., Yan, G., Schiavi, G., Su, H., Fang, H.-S., Shi, H., Amor,
    H. B., Christensen, H. I., Furuta, H., Walke, H., Fang, H., Mordatch, I., Radosavovic,
    I., Leal, I., Liang, J., Kim, J., Schneider, J., Hsu, J., Bohg, J., Bingham, J.,
    Wu, J., Wu, J., Luo, J., Gu, J., Tan, J., Oh, J., Malik, J., Tompson, J., Yang,
    J., Lim, J. J., Silvério, J., Han, J., Rao, K., Pertsch, K., Hausman, K., Go,
    K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K.,
    Zhang, K., Majd, K., Rana, K., Srinivasan, K., Chen, L. Y., Pinto, L., Tan, L.,
    Ott, L., Lee, L., Tomizuka, M., Du, M., Ahn, M., Zhang, M., Ding, M., Srirama,
    M. K., Sharma, M., Kim, M. J., Kanazawa, N., Hansen, N., Heess, N., Joshi, N. J.,
    Suenderhauf, N., Palo, N. D., Shafiullah, N. M. M., Mees, O., Kroemer, O., Sanketi,
    P. R., Wohlhart, P., Xu, P., Sermanet, P., Sundaresan, P., Vuong, Q., Rafailov,
    R., Tian, R., Doshi, R., Martín-Martín, R., Mendonca, R., Shah, R., Hoque, R.,
    Julian, R., Bustamante, S., Kirmani, S., Levine, S., Moore, S., Bahl, S., Dass,
    S., Song, S., Xu, S., Haldar, S., Adebola, S., Guist, S., Nasiriany, S., Schaal,
    S., Welker, S., Tian, S., Dasari, S., Belkhale, S., Osa, T., Harada, T., Matsushima,
    T., Xiao, T., Yu, T., Ding, T., Davchev, T., Zhao, T. Z., Armstrong, T., Darrell,
    T., Jain, V., Vanhoucke, V., Zhan, W., Zhou, W., Burgard, W., Chen, X., Wang,
    X., Zhu, X., Li, X., Lu, Y., Chebotar, Y., Zhou, Y., Zhu, Y., Xu, Y., Wang, Y.,
    Bisk, Y., Cho, Y., Lee, Y., Cui, Y., hua Wu, Y., Tang, Y., Zhu, Y., Li, Y., Iwasawa,
    Y., Matsuo, Y., Xu, Z., and Cui, Z. J. Open X-Embodiment: Robotic learning datasets
    and RT-X models. [https://arxiv.org/abs/2310.08864](https://arxiv.org/abs/2310.08864),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: den Oord et al. (2019) den Oord, A. V., Li, Y., and Vinyals, O. Representation
    learning with contrastive predictive coding, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
    Pre-training of deep bidirectional transformers for language understanding. *arXiv
    preprint arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers
    for image recognition at scale. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gage (1994) Gage, P. A new algorithm for data compression. *C Users Journal*,
    12(2):23––38, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2019) Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman,
    K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement
    learning. In *CoRL*, 2019. URL [https://proceedings.mlr.press/v100/gupta20a.html](https://proceedings.mlr.press/v100/gupta20a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hausman et al. (2018) Hausman, K., Springenberg, J. T., Wang, Z., Heess, N.,
    and Riedmiller, M. Learning an embedding space for transferable robot skills.
    In *ICLR*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning
    for image recognition. In *2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, pp.  770–778, 2016. doi: 10.1109/CVPR.2016.90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2015) Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The dependence
    of effective planning horizon on model accuracy. In *AAMAS*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Jiang, Z., Zhang, T., Janner, M., Li, Y., Rocktäschel, T.,
    Grefenstette, E., and Tian, Y. Efficient planning in a compact latent action space.
    In *ICLR*, 2023. URL [https://openreview.net/forum?id=cA77NrVEuqn](https://openreview.net/forum?id=cA77NrVEuqn).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Welling (2014) Kingma, D. and Welling, M. Auto-encoding variational
    Bayes. In *NIPS*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kipf et al. (2019) Kipf, T., Li, Y., Dai, H., Zambaldi, V., Sanchez-Gonzalez,
    A., Grefenstette, E., Kohli, P., and Battaglia, P. Compile: Compositional imitation
    learning and execution. In *ICML*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishnan et al. (2017) Krishnan, S., Fox, R., Stoica, I., and Goldberg, K.
    DDCO: Discovery of deep continuous options for robot learning from demonstrations.
    In *CoRL*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kudo (2018) Kudo, T. Subword regularization: Improving neural network translation
    models with multiple subword candidates. *arXiv preprint arXiv:1804.10959*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kujanpää et al. (2023) Kujanpää, K., Pajarinen, J., and Ilin, A. Hierarchical
    imitation learning with vector quantized models. In *ICML*, 2023. URL [https://proceedings.mlr.press/v202/kujanpaa23a.html](https://proceedings.mlr.press/v202/kujanpaa23a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le et al. (2018) Le, H. M., Jiang, N., Agarwal, A., Dudík, M., Yue, Y., and
    au2, H. D. I. Hierarchical imitation and reinforcement learning. In *ICML*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Liu, B., Zhu, Y., Gao, C., Feng, Y., qiang liu, Zhu, Y.,
    and Stone, P. LIBERO: Benchmarking knowledge transfer for lifelong robot learning.
    In *Thirty-seventh Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*, 2023. URL [https://openreview.net/forum?id=xzEtNSuDJk](https://openreview.net/forum?id=xzEtNSuDJk).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023) Luo, J., Dong, P., Wu, J., Kumar, A., Geng, X., and Levine,
    S. Action-quantized offline reinforcement learning for robotic skill learning.
    In *CoRL*, 2023. URL [https://openreview.net/forum?id=n9lew97SAn](https://openreview.net/forum?id=n9lew97SAn).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lynch & Sermanet (2021) Lynch, C. and Sermanet, P. Language conditioned imitation
    learning over unstructured data. In *RSS*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lynch et al. (2019) Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J.,
    Levine, S., and Sermanet, P. Learning latent plans from play. In *CoRL*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mandlekar et al. (2021) Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang,
    C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y., and Martín-Martín, R. What
    matters in learning from offline human demonstrations for robot manipulation.
    In *arXiv preprint arXiv:2108.03298*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nachum et al. (2018) Nachum, O., Gu, S., Lee, H., and Levine, S. Data-efficient
    hierarchical reinforcement learning. In *NeurIPS*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nair et al. (2022) Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta,
    A. R3m: A universal visual representation for robot manipulation. In *CoRL*, 2022.
    URL [https://openreview.net/forum?id=tGbpgz6yOrI](https://openreview.net/forum?id=tGbpgz6yOrI).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parr & Russell (1998) Parr, R. and Russell, S. Reinforcement learning with hierarchies
    of machines. In *NIPS*, pp.  1043–1049, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pastor et al. (2009) Pastor, P., Hoffmann, H., Asfour, T., and Schaal, S. Learning
    and generalization of motor skills by learning from demonstration. In *ICRA*,
    2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perez et al. (2018) Perez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville,
    A. C. Film: Visual reasoning with a general conditioning layer. In *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puterman (1994) Puterman, M. L. *Markov decision processes: Discrete stochastic
    dynamic programming*. John Wiley and Sons, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    and Sutskever, I. Language models are unsupervised multitask learners, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rajaraman et al. (2020) Rajaraman, N., Yang, L., Jiao, J., and Ramchandran,
    K. Toward the fundamental limits of imitation learning. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosete-Beas et al. (2022) Rosete-Beas, E., Mees, O., Kalweit, G., Boedecker,
    J., and Burgard, W. Latent plans for task agnostic offline reinforcement learning.
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross et al. (2011) Ross, S., Gordon, G. J., and Bagnell, J. A. A reduction of
    imitation learning and structured prediction to no-regret online learning. In
    *AISTATS*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwarzer et al. (2021) Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville,
    A., and Bachman, P. Data-efficient reinforcement learning with self-predictive
    representations. In *ICLR*, 2021. URL [https://openreview.net/forum?id=XpSAvlvnMa](https://openreview.net/forum?id=XpSAvlvnMa).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sennrich et al. (2016) Sennrich, R., Haddow, B., and Birch, A. Neural machine
    translation of rare words with subword units. In *ACL*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton & Barto (2018) Sutton, R. and Barto, A. *Reinforcement learning: An
    introduction*. The MIT Press, 2nd edition, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton et al. (1999) Sutton, R., Precup, D., and Singh, S. Between mdps and
    semi-mdps: A framework for temporal abstraction in reinforcement learning. *Artificial
    Intelligence*, 112(1-2):181–211, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Oord et al. (2017) van den Oord, A., Vinyals, O., and Kavukcuoglu, K.
    Neural discrete representation learning. In *NeurIPS*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van den Oord et al. (2017) Van den Oord, A., Vinyals, O., and Kavukcuoglu, K.
    Neural discrete representation learning. In Guyon, I., Luxburg, U. V., Bengio,
    S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), *Advances
    in Neural Information Processing Systems*, volume 30\. Curran Associates, Inc.,
    2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need.
    In *NIPS*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Wei, Y., Sun, Y., Zheng, R., Vemprala, S., Bonatti, R., Chen,
    S., Madaan, R., Ba, Z., Kapoor, A., and Ma, S. Is imitation all you need? generalized
    decision-making with dual-phase training. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, pp.  16221–16231, October 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2021) Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal,
    A. Bellman-consistent pessimism for offline reinforcement learning. In *NeurIPS*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yarats et al. (2022) Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering
    visual continuous control: Improved data-augmented reinforcement learning. In
    *International Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=_SJ-_yyes8](https://openreview.net/forum?id=_SJ-_yyes8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019) Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn,
    C., and Levine, S. Meta-world: A benchmark and evaluation for multi-task and meta
    reinforcement learning. In *Conference on Robot Learning (CoRL)*, 2019. URL [https://arxiv.org/abs/1910.10897](https://arxiv.org/abs/1910.10897).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Zhang, A., McAllister, R. T., Calandra, R., Gal, Y., and
    Levine, S. Learning invariant representations for reinforcement learning without
    reconstruction. In *International Conference on Learning Representations*, 2021.
    URL [https://openreview.net/forum?id=-2FCwDKRREu](https://openreview.net/forum?id=-2FCwDKRREu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023a) Zhao, T. Z., Kumar, V., Levine, S., and Finn, C. Learning
    fine-grained bimanual manipulation with low-cost hardware. In Bekris, K. E., Hauser,
    K., Herbert, S. L., and Yu, J. (eds.), *Robotics: Science and Systems XIX, Daegu,
    Republic of Korea, July 10-14, 2023*, 2023a. doi: 10.15607/RSS.2023.XIX.016. URL
    [https://doi.org/10.15607/RSS.2023.XIX.016](https://doi.org/10.15607/RSS.2023.XIX.016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023b) Zhao, T. Z., Kumar, V., Levine, S., and Finn, C. Learning
    fine-grained bimanual manipulation with low-cost hardware. In *RSS*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Zheng, R., Wang, X., Sun, Y., Ma, S., Zhao, J., Xu, H.,
    III, H. D., and Huang, F. TACO: Temporal latent action-driven contrastive loss
    for visual reinforcement learning. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023. URL [https://openreview.net/forum?id=ezCsMOy1w9](https://openreview.net/forum?id=ezCsMOy1w9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2024) Zheng, R., Liang, Y., Wang, X., Ma, S., au2, H. D. I.,
    Xu, H., Langford, J., Palanisamy, P., Basu, K. S., and Huang, F. Premier-taco
    is a few-shot policy learner: Pretraining multitask representation via temporal
    action-driven contrastive loss, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Detailed Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why learn temporal action abstractions? Learning temporally extended action
    primitives has a long history in sequential decision-making literature. In fully
    observable and partially observable MDPs (Puterman, [1994](#bib.bib34); Sutton
    & Barto, [2018](#bib.bib41)), the standard mathematical model for decision-making,
    every action takes one time step to execute. In most RL, IL, and planning algorithms,
    the decision space consists of individual actions, so standard methods produce
    policies that make a separate decision at every time step. It has long been informally
    observed and shown formally under various assumptions that the difficulty of policy
    learning scales with the problem horizon (Ross et al., [2011](#bib.bib38); Jiang
    et al., [2015](#bib.bib15); Rajaraman et al., [2020](#bib.bib36); Xie et al.,
    [2021](#bib.bib47)). This motivates hierarchical approaches (Parr & Russell, [1998](#bib.bib31);
    Barto & Mahadevan, [2003](#bib.bib3); Le et al., [2018](#bib.bib22); Nachum et al.,
    [2018](#bib.bib28); Kipf et al., [2019](#bib.bib18); Kujanpää et al., [2023](#bib.bib21)),
    which view the decision-making process as consisting of a high-level policy that
    identifies task substeps and invokes lower-level temporally extended routines
    sometimes called *options* (Sutton et al., [1999](#bib.bib42)) or *primitives* (Ajay
    et al., [2021](#bib.bib1)) to complete them. Conceptually, our PRISE method can
    be viewed as hierarchical imitation learning (HIL).
  prefs: []
  type: TYPE_NORMAL
- en: Temporal abstraction discovery for decision-making. Recent methods that learn
    temporally extended action primitives and subsequently use them for shortening
    the effective decision-making horizon during high-level policy induction include
    CompILE (Kipf et al., [2019](#bib.bib18)), RPL (Gupta et al., [2019](#bib.bib12)),
    OPAL (Ajay et al., [2021](#bib.bib1)), TAP (Jiang et al., [2023](#bib.bib16)),
    and ACT (Zhao et al., [2023a](#bib.bib51)). They operate in two stages, learning
    the primitives during the first and applying them to solve a downstream task during
    the second, possibly adapting the primitives in the process. It is during this
    latter stage that the learned primitives provide their temporal abstraction benefits.
    This is subtly but crucially different from methods like DDCO (Krishnan et al.,
    [2017](#bib.bib19)), which learn the primitives and a higher-level policy simultaneously
    for a given task and benefit from the primitives via non-temporal mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: PRISE’s use of BPE is dissimilar from any other method of inducing temporal
    action abstractions that we are aware of, and sidesteps some of the challenges
    of these other methods. E.g., CompILE, which, like PRISE, learns *variable*-duration
    primitives, does so by segmenting pretraining demonstrations into semantically
    meaningful subtasks, for which it needs to know the number of subtasks *in each
    pretraining trajectory* and is sensitive to errors in these values (Kipf et al.,
    [2019](#bib.bib18)). RPL and OPAL avoid this complication but learn primitives
    of a fixed duration, determined by a hyperparameter. This hyperparameter is nontrivial
    to tune, because short primitive durations yield little gains from temporal abstraction
    during learning, and long ones cause many primitives to idle after achieving a
    subtask. This is distinct from PRISE’s hyperparameter $K$, which merely puts an
    *upper bound* on skills’ duration.
  prefs: []
  type: TYPE_NORMAL
- en: We view ACT (Zhao et al., [2023b](#bib.bib52)) as the most comparable method
    to PRISE and use it as a baseline in our experiments. Like PRISE, ACT handles
    high-dimensional pixel observations out of the box and, crucially, uses BC during
    downstream learning. In contrast CompILE, RPL, and OPAL assume access to ground-truth
    states and rely on online RL for finetuning. In many physical continuous control
    scenarios such as robotics, BC is arguably a more practical algorithm, and benefits
    that temporal abstractions provide for BC are expected to be different from those
    in RL, where they enable more efficient exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Policy quantization methods. PRISE’s state-conditioned policy quantization method
    is related to SAQ (Luo et al., [2023](#bib.bib24)), but, crucially, also works
    for pixel observations characteristic of many continuous control scenarios in
    robotics. The policy encoder at the heart of PRISE and SAQ is VQ-VAE (van den
    Oord et al., [2017](#bib.bib43)). Conditional VAE (Kingma & Welling, [2014](#bib.bib17))
    is another common choice for this role (see, e.g., Lynch et al. ([2019](#bib.bib26))
    and Kipf et al. ([2019](#bib.bib18))).
  prefs: []
  type: TYPE_NORMAL
- en: Temporal abstractions versus skills in robot learning. Many works on robot learning
    focus on the acquisition of *skills*. While seemingly similar to an option or
    a primitive, this term has a somewhat different meaning, and most of these works
    don’t use skills for temporal abstraction, with TACO-RL (Rosete-Beas et al., [2022](#bib.bib37))
    being a notable exception. Namely, approaches such as DMP  (Pastor et al., [2009](#bib.bib32)),
     (Hausman et al., [2018](#bib.bib13)), Play-LMP (Lynch et al., [2019](#bib.bib26)),
    and MCIL (Lynch & Sermanet, [2021](#bib.bib25)) aim to learn a multi-task goal-conditioned
    policy, and understand a skill as a latent plan for achieving a goal or several
    goal variations starting from a variety of initial states. In this paradigm, learning
    a multitask policy constitutes embedding plans into a continuous latent space,
    which happens without using those plans to shorten a task’s horizon. Indeed, the
    learned skills usually don’t have a termination condition (although TACO-RL assumes
    having a mechanism for detecting subgoal attainment), and policies produced by
    the aforementioned methods resample a latent skill at every time step. This is
    in contrast to PRISE and, e.g., TAP (Jiang et al., [2023](#bib.bib16)) that learn
    temporally extended action primitives and apply them for several time steps during
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining data assumptions. In terms of the assumptions PRISE imposes on the
    trajectories it uses for pretraining, it is more similar to Play-LMP (Lynch et al.,
    [2019](#bib.bib26)), RPL (Gupta et al., [2019](#bib.bib12)), MCIL (Lynch & Sermanet,
    [2021](#bib.bib25)), and TACO-RL (Rosete-Beas et al., [2022](#bib.bib37)) rather
    than to IL or offline RL. Namely, PRISE needs this data to contain meaningful
    behavioral patters, which it extracts using an imitation-like procedure (as does
    RPL). While PRISE could be applied to arbitrary-quality data commonly assumed
    by offline RL or some state representation pretraining algorithms such as  Schwarzer
    et al. ([2021](#bib.bib39)); Wei et al. ([2023](#bib.bib46)); Zheng et al. ([2023](#bib.bib53),
    [2024](#bib.bib54)), many of the patterns there are unlikely to be useful, and
    may not easily aggregate into common temporally extended sequences that BPE aims
    to extract. On the other hand, the pretraining data doesn’t need to be imitation-quality
    either.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization in language models Subword tokenization methods such as BPE (Gage,
    [1994](#bib.bib11)), Unigram (Kudo, [2018](#bib.bib20)), and WordPiece (Devlin
    et al., [2018](#bib.bib9)) play an important role in training modern large language
    models. These algorithms learn tokens from a large corpus texts to mine reusable
    patterns. A trained tokenizer is used to compress training text data into tokens,
    and a large language model is trained to predict over this compressed tokens.
    Compared to directly predicting at the alphabet level, predicting tokens (with
    a proper vocabulary size) allows better generalization, because the effective
    sequence length that the model needs to predict is shorter.
  prefs: []
  type: TYPE_NORMAL
- en: Next token prediction in NLP is a special case of behavior cloning in the context
    of decision making, where the state is the history of tokens (context). Therefore,
    our algorithm can be viewed as extension of the next-token-prediction paradigm
    in NLP to the continuous control domain. However, our setup introduces additional
    complexities not faced in NLP. Our raw action space is continuous rather than
    discrete finite alphabets in NLP. In addition, our decision agents need to consider
    high-dimensional image observations as part of the state representation, whereas
    in NLP the history is simply past tokens. Namely, if we view alphabets in NLP
    as action vectors here, the BC problem in NLP has a stateless dynamical system,
    but in, e.g., robotics it has a non-trivial internal state. These complications
    motivate the need for observation and action encodes as well as an action quantizer.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Figure [7](#S5.F7 "Figure 7 ‣ 5.3 Few-shot adaptation to unseen tasks ‣
    5 Experiments ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem"), we present the results of mean success rate across 5 tasks in MetaWorld.
    Here we present the detailed results for each of the downstream unseen task.'
  prefs: []
  type: TYPE_NORMAL
- en: '| MetaWorld | Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Unseen Tasks | BC (Scratch) | BC (Pretrained) | ACT (Scratch) | ACT (Pretrained)
    | PRISE w/o BPE | PRISE |'
  prefs: []
  type: TYPE_TB
- en: '| Hand Insert | $18.8\pm 12.5$ |'
  prefs: []
  type: TYPE_TB
- en: '| Box Close | $19.4\pm 8.7$ |'
  prefs: []
  type: TYPE_TB
- en: '| Disassemble | $8.1\pm 17.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| Stick Pull | $20.6\pm 5.8$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pick Place Wall | $19.4\pm 6.4$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | $17.3\pm 4.2$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: 5-shot imitation learning performance on five unseen tasks in MetaWorld.
    Results are averaged across 3 random seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For LIBERO, in Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional Experimental
    Results ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem"), we first provide the task instruction for each of the downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task ID | Task Scene | Task Instruction |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | living room scene2 | put both the alphabet soup and the tomato sauce
    in the basket |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | living room scene2 | put both the cream cheese box and the butter in
    the basket |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | kitchen scene3 | turn on the stove and put the moka pot on it |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | kitchen scene4 | put the black bowl in the bottom drawer of the cabinet
    and close it |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | living room scene5 | put the white mug on the left plate and put the
    yellow and white mug on the right plate |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | study scene1 | pick up the book and place it in the back compartment
    of the caddy |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | living room scene6 | put the white mug on the plate and put the chocolate
    pudding to the right of the plate |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | living room scene1 | put both the alphabet soup and the cream cheese
    box in the basket |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Language instructions for 8 LIBERO downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The detailed results for each of the LIBERO downstream unseen task are presented
    in Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Experimental Results ‣ PRISE:
    Learning Temporal Action Abstractions as a Sequence Compression Problem").'
  prefs: []
  type: TYPE_NORMAL
- en: '| LIBERO | Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Unseen Tasks | BC (Scratch) | BC (Pretrained) | ACT (Scratch) | ACT (Pretrained)
    | PRISE w/o BPE | PRISE |'
  prefs: []
  type: TYPE_TB
- en: '| Task 0 | $16.7\pm 6.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 1 | $25.0\pm 4.7$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 2 | $68.3\pm 7.4$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 3 | $66.7\pm 7.9$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 4 | $25.0\pm 10.8$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 5 | $70.0\pm 0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 6 | $23.3\pm 4.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| Task 7 | $28.3\pm 4.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | $40.4\pm 1.2$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: 5-shot imitation learning performance across 8 unseen tasks in LIBERO.
    Resullts are averaged across 3 random seeds'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,IyBuZXh0X29ic2VydmF0aW9uOiBBIHNlcXVlbmNlIG9mIEs9MyBmdXR1cmUgcGl4ZWwgb2JzZXJ2YXRpb25zIG9fe3QrMX0sLi4uLCBvX3t0K0t9CiMgcGFzdF9vYnNlcnZhdGlvbjogQSBzZXF1ZW5jZSBvZiBIIGhpc3RvcmljYWwgcGl4ZWwgb2JzZXJ2YXRpb25zIG9fe3QtSCsxfSwuLi4sIG9fe3R9CiMgYWN0aW9uOiAgICAgICAgICAgQSBzZXF1ZW5jZSBvZiAkSyQgYWN0aW9uIGFfdCwuLi4sIGFfe3QrSy0xfQojIG9ic19lbmNvZGVyOiAgICAgIE9ic2VydmF0aW9uIEVuY29kZXIgKENOTitUcmFuc2Zvcm1lcikKIyBxdWFudGl6ZXI6ICAgICAgICBEaXNjcmV0ZSBhY3Rpb24gY29kZSBxdWFudGl6ZXIKIyBkeW5hbWljczogICAgICAgICBMYXRlbnQgRHluYW1pY3MgTW9kZWwKIyBwcm9qZWN0OiAgICAgICAgICBQcm9qZWN0aW9uIGxheWVyCiMgcHJlZGljdG9yOiAgICAgICAgTGF0ZW50IHN0YXRlIHByZWRpY3RvcgojIGFjdGlvbl9kZWNvZGVyOiAgIFF1YW50aXplZCBhY3Rpb24gY29kZSBkZWNvZGVyCiMgYmV0YTogICAgICAgICAgICAgV2VpZ2h0IG9mIGRlY29kZXIgbG9zcyAoMS4wIGluIE1ldGFXb3JsZCBhbmQgMC4wMSBpbiBMSUJFUk8gZm9yICAgICAgICMgICAgICAgICAgICAgICAgICAgbnVtZXJpY2FsIHN0YWJpbGl0eSkKCmR5bmFtaWNfbG9zc2VzLCBxdWFudGl6YXRpb25fbG9zc2VzLCBkZWNvZGVyX2xvc3NlcyA9IDAsIDAsIDAKeiA9IHN0YXRlX2VuY29kZXIocGFzdF9vYnNlcnZhdGlvbikKel9oYXQgPSB6CmZvciBrIGluIHJhbmdlKEspOgogICAgeiA9IHN0YXRlX2VuY29kZXIocGFzdF9vYnNlcnZhdGlvbikKICAgIHBhc3Rfb2JzZXJ2YXRpb24ucHVzaChuZXh0X29ic2VydmF0aW9uW2tdKQogICAgdV9xdWFudGl6ZWQsIHF1YW50aXphdGlvbl9sb3NzID0gcXVhbnRpemVyKGFjdGlvbltrXSwgei5kZXRhY2goKSkKICAgIHF1YW50aXphdGlvbl9sb3NzZXMgKz0gcXVhbnRpemF0aW9uX2xvc3MKICAgIGRlY29kZV9hY3Rpb24gICAgICAgPSAgZGVjb2Rlcih6LCB1X3F1YW50aXplZCkKICAgIGRlY29kZXJfbG9zc2VzICAgICAgKz0gYWN0aW9uX2xvc3MoZGVjb2RlX2FjdGlvbiwgYWN0aW9uW2tdKQogICAgel9oYXQgID0gZHluYW1pY3Moel9oYXQsIHVfcXVhbnRpemVkKQogICAgel9uZXh0ID0gc3RhdGVfZW5jb2RlcihvYnNlcnZhdGlvbltrKzFdKQogICAgeV9oYXQgID0gcHJlZGljdG9yKHByb2plY3Qoel9oYXQpKQogICAgeV9uZXh0ID0gcHJvamVjdCh6X25leHQpLmRldGFjaCgpCiAgICBkeW5hbWljX2xvc3NlcyArPSAtY29zaW5lX3NpbWlsYXJpdHkoeV9oYXQsIHlfbmV4dCkKKGR5bmFtaWNfbG9zc2VzICsgcXVhbnRpemF0aW9uX2xvc3NlcyArIGJldGEqZGVjb2Rlcl9sb3NzZXMpLmJhY2t3YXJkKCk=)1#  next_observation:  A  sequence  of  K=3  future  pixel  observations  o_{t+1},...,  o_{t+K}2#  past_observation:  A  sequence  of  H  historical  pixel  observations  o_{t-H+1},...,  o_{t}3#  action:  A  sequence  of  $K$  action  a_t,...,  a_{t+K-1}4#  obs_encoder:  Observation  Encoder  (CNN+Transformer)5#  quantizer:  Discrete  action  code  quantizer6#  dynamics:  Latent  Dynamics  Model7#  project:  Projection  layer8#  predictor:  Latent  state  predictor9#  action_decoder:  Quantized  action  code  decoder10#  beta:  Weight  of  decoder  loss  (1.0  in  MetaWorld  and  0.01  in  LIBERO  for  #  numerical  stability)1112dynamic_losses,  quantization_losses,  decoder_losses  =  0,  0,  013z  =  state_encoder(past_observation)14z_hat  =  z15for  k  in  range(K):16  z  =  state_encoder(past_observation)17  past_observation.push(next_observation[k])18  u_quantized,  quantization_loss  =  quantizer(action[k],  z.detach())19  quantization_losses  +=  quantization_loss20  decode_action  =  decoder(z,  u_quantized)21  decoder_losses  +=  action_loss(decode_action,  action[k])22  z_hat  =  dynamics(z_hat,  u_quantized)23  z_next  =  state_encoder(observation[k+1])24  y_hat  =  predictor(project(z_hat))25  y_next  =  project(z_next).detach()26  dynamic_losses  +=  -cosine_similarity(y_hat,  y_next)27(dynamic_losses  +  quantization_losses  +  beta*decoder_losses).backward()'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Pseudocode for the pretraining stage I of PRISE'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metaworld: We generate 100 expert trajectories for each pretraining task using
    the scripted policy provided in MetaWorld. The observation for each step includes
    an $84\times 84$ third-person view image as well as an 8-dimensional proprioceptive
    state vector of the robot’s end-effector. We use the same shallow CNN as in Yarats
    et al. ([2022](#bib.bib48)) to encode the observations into a 64-dimensional latent
    vector and apply a linear layer to embed the 8-dimensional state vector also into
    a 64-dimensional latent vector. The architecture of the shallow CNN is presented
    in [Figure 12](#A3.F12 "In Appendix C Implementation Details ‣ PRISE: Learning
    Temporal Action Abstractions as a Sequence Compression Problem").'
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,Y2xhc3MgRW5jb2Rlcihubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKCkuX19pbml0X18oKQogICAgICAgIHNlbGYucmVwcl9kaW0gPSAzMiAqIDM1ICogMzUKICAgICAgICBzZWxmLmNvbnZuZXQgPSBubi5TZXF1ZW50aWFsKG5uLkNvbnYyZCg4NCwgMzIsIDMsIHN0cmlkZT0yKSwKICAgICAgICAgICAgICAgICAgICAgICAgbm4uUmVMVSgpLCBubi5Db252MmQoMzIsIDMyLCAzLCBzdHJpZGU9MSksCiAgICAgICAgICAgICAgICAgICAgICAgIG5uLlJlTFUoKSwgbm4uQ29udjJkKDMyLCAzMiwgMywgc3RyaWRlPTEpLAogICAgICAgICAgICAgICAgICAgICAgICBubi5SZUxVKCksIG5uLkNvbnYyZCgzMiwgMzIsIDMsIHN0cmlkZT0xKSwKICAgICAgICAgICAgICAgICAgICAgICAgbm4uUmVMVSgpLAogICAgICAgICAgICAgICAgICAgICAgICBubi5MaW5lYXIoc2VsZi5yZXByX2RpbSwgZmVhdHVyZV9kaW0pLAogICAgICAgICAgICAgICAgICAgICAgICBubi5MYXllck5vcm0oZmVhdHVyZV9kaW0pLCBubi5UYW5oKCkpCiAgICAgICAgc2VsZi50cnVuayA9IG5uLlNlcXVlbnRpYWwobm4uTGluZWFyKHNlbGYucmVwcl9kaW0sIGZlYXR1cmVfZGltKSwKICAgICAgICAgICAgICAgICAgICAgICAgbm4uTGF5ZXJOb3JtKGZlYXR1cmVfZGltKSwgbm4uVGFuaCgpKQoKICAgIGRlZiBmb3J3YXJkKHNlbGYsIG9icyk6CiAgICAgICAgb2JzID0gb2JzIC8gMjU1LjAgLSAwLjUKICAgICAgICBoID0gc2VsZi5jb252bmV0KG9icykudmlldyhoLnNoYXBlWzBdLCAtMSkKICAgICAgICByZXR1cm4gc2VsZi50cnVuayhoKQ==)1class  Encoder(nn.Module):2  def  __init__(self):3  super().__init__()4  self.repr_dim  =  32  *  35  *  355  self.convnet  =  nn.Sequential(nn.Conv2d(84,  32,  3,  stride=2),6  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),7  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),8  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),9  nn.ReLU(),10  nn.Linear(self.repr_dim,  feature_dim),11  nn.LayerNorm(feature_dim),  nn.Tanh())12  self.trunk  =  nn.Sequential(nn.Linear(self.repr_dim,  feature_dim),13  nn.LayerNorm(feature_dim),  nn.Tanh())1415  def  forward(self,  obs):16  obs  =  obs  /  255.0  -  0.517  h  =  self.convnet(obs).view(h.shape[0],  -1)18  return  self.trunk(h)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: Architecture of the Shallow CNN encoder used in MetaWorld.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we apply a transformer decoder module with 4 layers and number of heads
    equal to 8 to extract the observation embedding. We set the context length to
    be 10. The action decoder $\psi$ is a three-layer MLP with hidden size being 1024.
  prefs: []
  type: TYPE_NORMAL
- en: 'LIBERO: For LIBERO, we pretrain PRISE on the provided LIBERO-90 dataset. The
    pretraining dataset contains 50 demonstration trajectories for each task, collected
    by human teleoperation. For each timestep, the agent observes a third-person view
    image, a first-person view image from its wrist camera (both with resolution $128\times
    128$ is also a three-layer MLP with hidden size being 1024.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of Few-shot Imitation Learning A batch size of 128 and a learning
    rate of 1e-4 are used for Metaworld, and a batch size of 64 is used for LIBERO.
    In total, we take 30,000 gradient steps and conduct evaluations for every 3000
    steps. For both MetaWorld and LIBERO, we execute 40 episodes and calculate the
    success rate of the trained policy. We report the highest success rates across
    10 evaluated checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Resources For our experiments, we use 8 NVIDIA RTX A6000 with
    PyTorch Distributed DataParallel for pretraining PRISE, and we use NVIDIA RTX2080Ti
    for downstream imitation learning on Metaworld, and RTX A5000 on LIBERO.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Additional Results on Multitask Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below we present the per-task success rate for LIBERO-90. The results of BC
    (ResNet-RNN), BC (ResNet-T), BC (ViT-T) are taken from (Liu et al., [2023](#bib.bib23)).
    For the evaluation of ACT and PRISE, we follow the same evaluation protocol. We
    take the last training checkpoint of them respectively and evaluate the success
    rate on each task with 20 rollouts.
  prefs: []
  type: TYPE_NORMAL
- en: '| LIBERO-90 | Multitask Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Task ID | BC (ResNet-RNN) | BC (ResNet-T) | BC (ViT-T) | ACT | PRISE |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | $0.45$ |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | $0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | $0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | $0.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | $0.35$ |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | $0.45$ |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | $0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | $0.35$ |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | $0.5$ |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | $0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | $0.4$ |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | $0.6$ |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | $0.75$ |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | $0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | $0.8$ |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | $0.5$ |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 33 | $0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 34 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | $0.7$ |'
  prefs: []
  type: TYPE_TB
- en: '| 36 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 37 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 38 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 39 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | $0.45$ |'
  prefs: []
  type: TYPE_TB
- en: '| 41 | $0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 43 | $0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 44 | $0.5$ |  | LIBERO-90 | Multitask Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Task ID | BC (ResNet-RNN) | BC (ResNet-T) | BC (ViT-T) | ACT | PRISE |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 46 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 47 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 48 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 49 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 51 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 53 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 54 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 55 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 56 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 57 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 58 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 59 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 61 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 66 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | $0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 68 | $0.35$ |'
  prefs: []
  type: TYPE_TB
- en: '| 69 | $0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 71 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 72 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 73 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 74 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 76 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 77 | $0.35$ |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | $0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| 79 | $0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 81 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 82 | $0.4$ |'
  prefs: []
  type: TYPE_TB
- en: '| 83 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 84 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 85 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 86 | $0.05$ |'
  prefs: []
  type: TYPE_TB
- en: '| 87 | $0.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| 88 | $0.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| 89 | $0.15$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Multitask success rate on LIBERO-90.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we also provide the results of multitask performance on 45 MetaWorld
    pretraining tasks. As shown in [13](#A4.F13 "Figure 13 ‣ Appendix D Additional
    Results on Multitask Learning ‣ PRISE: Learning Temporal Action Abstractions as
    a Sequence Compression Problem"), all three algorithms perform well on MetaWorld,
    with an average success rate around 80% across 45 tasks. Thus in this paper, we
    focus on the more challenging LIBERO-90 multitask benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d8202003cf9e625fb5abd7921bf4d29f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Multitask Learning Results on MetaWorld.'
  prefs: []
  type: TYPE_NORMAL
