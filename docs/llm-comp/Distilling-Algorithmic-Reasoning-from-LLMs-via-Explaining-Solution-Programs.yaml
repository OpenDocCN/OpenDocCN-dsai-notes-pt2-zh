- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:06:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.08148](https://ar5iv.labs.arxiv.org/html/2404.08148)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jierui Li  &  Raymond Mooney
  prefs: []
  type: TYPE_NORMAL
- en: The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '{jierui, mooney}@cs.utexas.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Distilling explicit chain-of-thought reasoning paths has emerged as an effective
    method for improving the reasoning abilities of large language models (LLMs) across
    various tasks. However, when tackling complex tasks that pose significant challenges
    for state-of-the-art models, this technique often struggles to produce effective
    chains of thought that lead to correct answers. In this work, we propose a novel
    approach to distilling reasoning abilities from LLMs by leveraging their capacity
    to explain solutions. We apply our method to solving competitive-level programming
    challenges. More specifically, we employ an LLM to generate explanations for a
    set of <problem, solution-program> pairs, then use <problem, explanation> pairs
    to fine-tune a smaller language model, which we refer to as the Reasoner, to learn
    algorithmic reasoning that can generate "how-to-solve" hints for unseen problems.
    Our experiments demonstrate that learning from explanations enables the Reasoner
    to more effectively guide the program implementation by a Coder, resulting in
    higher solve rates than strong chain-of-thought baselines on competitive-level
    programming problems. It also outperforms models that learn directly from <problem,
    solution-program> pairs. We curated an additional test set in the CodeContests
    format, which includes 246 more recent problems posted after the models’ knowledge
    cutoff.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent Large Language Models (LLMs) have shown impressive capabilities for various
    reasoning tasks, including multi-hop question answering (Wang et al., [2022](#bib.bib22);
    Lyu et al., [2023](#bib.bib15)), symbolic reasoning (Hua & Zhang, [2022](#bib.bib10)),
    and math word problem-solving (Chen et al., [2022](#bib.bib4); Zhou et al., [2023](#bib.bib29)).
    Chain-of-thought (CoT) prompting (Wei et al., [2022](#bib.bib23)) addresses limitations
    of previous LLMs by instructing them to generate intermediate steps towards the
    final answer, thereby decomposing complex problems step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: However, challenges remain, particularly in complex reasoning tasks like algorithmic
    programming. For example, the majority of human competitors still outperform advanced
    models like GPT-4 in Codeforces contests (OpenAI, [2023b](#bib.bib18)). Complex
    programming problems have stringent time and space complexity constraints, where
    straightforward implementation methods like (Chen et al., [2021](#bib.bib3); Yin
    et al., [2018](#bib.bib25); Hendrycks et al., [2021](#bib.bib7)), often yield
    time-consuming brute-force solutions.
  prefs: []
  type: TYPE_NORMAL
- en: A number of efforts have been made to tackle this challenging task (Li et al.,
    [2022](#bib.bib14); Zhang et al., [2023](#bib.bib28); Olausson et al., [2023](#bib.bib16);
    Ridnik et al., [2024](#bib.bib20)) by adding extra clustering or verification
    steps to filter or iteratively refine generated programs. While those methods
    focus on flow engineering for code generation, there have been limited attempts
    to explicitly enhance models’ intrinsic reasoning abilities in this context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3c0092d2f4d42b4ab18057e5c289a47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison between Solve-based and Explain-based chain-of-thoughts
    distilling. Top: Solve-based CoT distilling is likely to generate incorrect or
    inefficient solutions. Bottom: Explain-based CoT distilling can generate high-quality
    reasoning processes by explaining the oracle solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Human-written rationales for solving algorithmic reasoning problems, known as
    editorials, are hard to collect as they are often posted on personal blogs or
    as tutorial videos. An alternative is to distill such natural-language-described
    problem-solving strategies from larger models. Distilling explicit chain-of-thoughts
    (CoT) reasoning processes has been shown as an effective method to learn multi-step
    reasoning from larger models (Hsieh et al., [2023](#bib.bib9); Yue et al., [2023](#bib.bib26)).
    Usually, a teacher model is required to solve a set of problems while giving CoT
    reasoning paths at the same time, as illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"). However, when facing challenging tasks where state-of-the-art
    models struggle to generate effective solutions¹¹1Experiments show that only 12%
    of GPT-4’s generated solutions are correct given 200 problems randomly sampled
    from the CodeContests training set., it becomes infeasible to gather reasoning
    processes at scale.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this problem, we propose to utilize large language models’ capacities
    in understanding and explaining solutions rather than solving problems. Specifically,
    we leverage a state-of-the-art LLM to read both the problem statement and an oracle
    human-written solution program, and generate an editorial-style chain-of-thought
    on how to solve the problem by explaining the solution. Then, a student LLM learns
    the algorithmic reasoning processes from the explicit chain-of-thought paths.
    We compare our explain-based CoT distilling method with solve-based CoT distilling
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning
    from LLMs via Explaining Solution Programs"). While solve-based CoT distilling
    requires the teacher model to reach the correct and efficient solution to hard
    problems, our explain-based distilling only requires the teacher model to faithfully
    explain the correct solution. Since explaining competitive-level code is a feasible
    task for strong LLMs (Li et al., [2023](#bib.bib13)), explain-based distilling
    can yield CoT reasoning processes with high quality and less noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'We further proposed a reason-then-implement framework to solve algorithmic
    reasoning problems, utilizing the proposed explain-based CoT distilling. The framework
    consists of 3 major components: 1) an Explainer to annotate explanations for a
    set of <problem, solution-program>; 2) a Reasoner to learn to generate intermediate
    reasoning processes for a given problem; and 3) a Coder to implement the solution
    for an unseen problem given the output from the Reasoner. The framework and its
    fine-tuning and inference stages are presented in Figure [2](#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution
    Programs").'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting usage of this Reasoner fine-tuned with explanations provides superior
    performance over direct code generation techniques and strong zero-shot chain-of-thought
    prompting baselines. Experiments on open and closed models also show that compared
    to direct learning on public-available <problem, solution-code> pairs, models
    learned on explicit natural language reasoning processes can generalize better
    to unseen problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f96ca19ce157a12bbc3e2a010c1a687.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The framework of our approach. We use Explainer LLM to generate explanations
    given <problem, solution-program> pairs; then train Reasoner LLM to generate explanations
    given problem statements. During inference time, given the problem, the Reasoner
    can generate a reasoning process in the same format as solution explanations,
    which could be provided to the Coder as a hint to solve the problem better.'
  prefs: []
  type: TYPE_NORMAL
- en: Decomposing algorithmic problem-solving into separate Coder and Reasoner modules
    surpasses the effectiveness of models fine-tuned directly on code. The success
    of our approach is rooted in the nuanced semantic richn meaning of natural language.
    The proposed method relies solely on providing better problem reasoning to instruct
    the implementation, which can be combined with different Coder and pipelines such
    as self-debugging (Chen et al., [2023](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, our work makes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We introduce an alternative, adaptable framework to distill complex reasoning
    processes from LLMs: instead of having LLMs solve problems, it leverages LLMs
    to explain solutions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our proposed distilling method is data-efficient: by fine-tuning with 8248
    data points, 8M tokens in total, consistent performance gains in solve rates are
    observed on GPT-3.5 and Deepseek coder 7b.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For fair evaluation, we propose a new test set extracted from Codeforces. We
    advocate to take code efficiency into consideration in addition to code correctness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Solving algorithmic-reasoning programming problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early endeavors in applying deep learning to tackle algorithmic reasoning in
    programming problems, such as those by Balog et al. ([2017](#bib.bib1)), primarily
    utilized traditional approaches like SMT solvers and heuristic search. These methods
    were adept at generating short programs for relatively simple problems. Expanding
    on this, Polosukhin & Skidanov ([2018](#bib.bib19)) curated a dataset of human-written
    problem statements and solutions for Codeforces challenges. They introduced baseline
    sequence models capable of solving a modest portion of this dataset. The advent
    of Transformer-based models, such as Alphacode (Li et al., [2022](#bib.bib14)),
    dramatically improved performance on these challenges. Subsequent advancements
    by Zelikman et al. ([2023](#bib.bib27)) and Olausson et al. ([2023](#bib.bib16)),
    as well as Zhang et al. ([2023](#bib.bib28)), further refined code generation
    techniques. They integrated additional layers for clustering or verification,
    enhancing the models’ ability to filter and iteratively refine the generated programs,
    thereby improving both accuracy and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Distilling reasoning abilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge distillation, particularly from large models to smaller and more efficient
    counterparts, has been extensively studied (Bucila et al., [2006](#bib.bib2);
    Hinton et al., [2015](#bib.bib8)). The advent of LLMs has further intensified
    research in this area, with a focus on distilling knowledge from LLMs into smaller
    models. This has often been coupled with the curation of LLM datasets (Taori et al.,
    [2023](#bib.bib21); Hsieh et al., [2023](#bib.bib9)). Specifically, Hsieh et al.
    ([2023](#bib.bib9)) employed CoT prompting to extract rationales from LLMs for
    training smaller models in a multi-task learning setting. Similarly, Yang et al.
    ([2023](#bib.bib24)) and Zhu et al. ([2023](#bib.bib30)) distilled reasoning abilities
    from LLMs using CoT or program-of-thought processes. These approaches typically
    treat data synthesis as analogous to the target task, leveraging advanced models
    for higher-quality output generation. Our method differs as we replace the problem-solving
    CoT generation with an oracle-solution-explaining task. It transforms the nature
    of the task to facilitate the generation of high-quality outputs in a more manageable
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our method employs a two-step framework: In the learning stage, an Explainer
    generates a set of chain-of-thought reasoning processes by explaining solutions,
    and a Reasoner is fine-tuned on this data to generate a reasoning process given
    only a problem description. In the inference stage, the Reasoner generates the
    reasoning process for an unseen problem, which, along with the problem, is passed
    to a 0-shot Coder to implement. We include sample prompts in appendix [A](#A1
    "Appendix A Appendix ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We formalize our task with a problem set consisting of $n$ is a text sequence
    that clearly describes the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem statement: a natural language description of the problem, as shown
    in the upper left box in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Distilling
    Algorithmic Reasoning from LLMs via Explaining Solution Programs").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input/Output: input/output formats and constraints for the submitted program
    $s$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: An example of a correct input/output pair.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note (Optional): Explanation of the Example input/output.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each $p_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Explainer: Extracting reasoning processes through explaining solution programs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Explainer is tasked with explaining a solution program. It serves the role
    of Teacher in Solve-based CoT distilling, as described in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"). Given a pair, $$<p_{i},s_{i}></math> in natural language.
    This is inspired by how human competitors learn problem-solving skills from past
    problems: they learn by reading editorials, step-by-step guidelines on approaching
    and solving the problems. While human-written editorials are hard to collect or
    annotate at scale, we ask an LLM to automatically generate them. We design an
    editorial-style chain-of-thought reasoning template and ask the Explainer to follow
    it to explain solutions. Specifically, an editorial for an algorithmic reasoning
    problem refers to a comprehensive explanation or walk-through on how to solve
    a problem, which includes problem analysis, strategy development, solution explanation,
    time/space complexity analysis, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We leverage the capabilities of LLMs to explain solution code to generate automated
    explanations for problem-solution pairs <math id=$$, for each pair. Our focus
    is predominantly on the reasoning process, encompassing the following critical
    aspects that are often included in human-expert-written editorials:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Problem Restatement: Summary and analysis of the problem.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Conceptual Evolution: How one approaches the problem and the development of
    a problem-solving strategy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Key to Solution: The key idea behind the solution.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solution Description: Brief, verbal description of the solution, focusing on
    the high-level algorithm.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step-by-Step Solution Explanation: A more detailed description, focusing on
    the steps in the implementation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Common Pitfalls: Some common mistakes one could make when approaching the problem,
    or edge/corner cases to be considered.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Based on the length of $p_{i}$. After further cleaning, we have 8248 triplets
    in total.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3 Reasoner: Fine-tuned to generate reasoning processes for problems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the inherent diversity and potential lack of readability of the code available
    for algorithmic reasoning problems, our approach focuses on fine-tuning the Reasoner
    on problem statements $p_{i}$. These problems and reasoning processes, rich in
    semantics, encapsulate the essential steps for problem resolution, including the
    algorithms used and specific problem-solving approaches employed. The Reasoner
    serves the role of Student in Solve-based CoT distilling, as illustrated in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from
    LLMs via Explaining Solution Programs").
  prefs: []
  type: TYPE_NORMAL
- en: We adopt a weighted fine-tuning strategy, with simpler, more recent problems
    weighted more heavily during training. Overly challenging problems might lead
    to low-quality noisy explanations that hurt the training of the Reasoner. Recent
    solutions usually feature more in-date implementations (e.g., Python 3 rather
    than Python 2), enhancing the code’s interpretability. We fine-tune an LLM on
    8,248 {$p_{i}$ can be considered as a natural-language “hint” given to the Coder
    to help solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '3.4 Coder: Reasoner-Hinted Code Implementer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we have a zero-shot Coder to generate code utilizing hints from the
    Reasoner. Since the focus of this work is enhancing algorithmic reasoning rather
    than code implementation abilities, we do not further fine-tune the Coder. As
    described earlier, the reasoning process $\hat{d_{j}}$ as the input to produce
    the input for the Coder, which it then uses to analyze the problem together with
    the reasoning hint, to generate programs that solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use GPT-4-0613  (OpenAI, [2023a](#bib.bib17); [b](#bib.bib18)) as the Explainer
    since it’s the strongest model that we have access to. We also choose the strongest
    models with fine-tuning access as the Reasoner and Coder. For a closed model,
    we choose GPT-3.5-turbo-1106 (henceforth GPT-3.5); for an open model, we choose
    Deepseek Coder 7B (henceforth Deepseek 7b) (Guo et al., [2024](#bib.bib6)).²²2[https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5](https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5)
    The temperature $t$ when sampling multiple(>1) is needed in our main experiments.
    The context window is set to 4096 and we truncate single examples with more than
    4096 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As suggested by Huang et al. ([2023](#bib.bib11)), to test the capacity of closed
    models like GPT, using recently-released data can reflect the model’s performance
    more faithfully. To ensure novel test data given that GPT-3.5 was trained on data
    including problems and solutions from Codeforces, we extracted 246 recent problems
    from Codeforces as our main test set, CF Prob. It contains 246 real online contest
    problems, the earliest of which dates to Aug. 2023, guaranteeing they were posted
    after the knowledge cutoff for GPT-3.5 (Jan. 2022). We also experimented with
    the 165 problems from the CodeContests benchmark (Li et al., [2022](#bib.bib14))
    test set, whose earliest problem was published in Oct. 2021\. Table [1](#S4.T1
    "Table 1 ‣ Data ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs") gives statistics on the
    difficulty ratings of both sets.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Difficulty Ratings |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset | total | [800, 1000] | (1000, 1500] | (1500, 2000] | (2000, 3600]
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CodeContests | $165$ |'
  prefs: []
  type: TYPE_TB
- en: '| CF Prob | $246$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Difficulty statistics (higher ratings = more difficult) for CodeContests
    and our proposed CF Prob. Both are sourced from Codeforces.'
  prefs: []
  type: TYPE_NORMAL
- en: Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adapt pass@k (Chen et al., [2021](#bib.bib3)) as our evaluation metric for
    solve rates. For each problem $p_{i}$ that passes all hidden tests. We first filter
    in programs by executing and examining their output on the public test cases before
    submitting them. Measuring solve rates using the online judge provides a fairer
    comparison between models and human participants as it also requires efficiency
    in solutions, rejecting brute-force solutions when an efficient algorithm exists
    for a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have models generate $10$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $g_{i}$th problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zero-shot Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct Prompting Given the problem statement $p_{i}$, directly prompt the model
    to generate solution programs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Chain-of-thought Adapt the well-known “Letś think step-by-step” CoT prompting
    proposed in (Kojima et al., [2022](#bib.bib12)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Editorial Chain-of-thought Instruct the model to analyze the problem following
    an editorial chain-of-thought style giving aspects mentioned in Section [3.2](#S3.SS2
    "3.2 Explainer: Extracting reasoning processes through explaining solution programs
    ‣ 3 Method ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution
    Programs").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-Reasoner&Zero-Coder Use a 0-shot Reasoner to generate natural language
    reasoning processes as hints for the 0-shot Coder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fine-tuning Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned Coder Fine-tune the Coder to generate code given the problem, using
    the same 8248 <problem, solution-program> pairs from CodeContests (Li et al.,
    [2022](#bib.bib14)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned Reasoner &Zero-Coder Use the Reasoner fine-tuned on solution explanations
    to generate natural-language reasoning hints for the 0-shot Coder. In addition
    to using the full reasoning processes. We also separately tested the effect of
    each editorial aspect (e.g., common pitfalls) and selected the highest performing
    aspect as the best aspect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The results are presented in Table [2](#S4.T2 "Table 2 ‣ Fine-tuning Methods
    ‣ 4.2 Main Results ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs
    via Explaining Solution Programs"). With our fine-tuned Reasoner, the Coder achieves
    the best solve@10 and solve@5 rates across open/closed models and two datasets,
    supporting the effectiveness of our method. Compared to using the original model
    as the Reasoner (0-shot Reasoner), the best aspect (Step-by-Step) from the reasoning
    process alone can boost solve@10 from 3.3% to 6.1%, suggesting that our fine-tuning
    method does distill some algorithmic reasoning ability into the student model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that solely fine-tuning <problem, solution-program>pairs actually hurts
    performance. One reason might be that solutions to Codeforces problems are often
    written under a time constraint with poor readability, making it difficult for
    the model to generalize given the limited amount of fine-tuning data. Since instruction-tuned
    models would generate intermediate thought processes by default, we experimented
    to explicitly forbid the model from any reasoning/analysis preceding implementation.
    Compared to the direct prompt, the solve@10 rate on CF Prob w/GPT-3.5 drops from
    3.3% to 2.0%. Even with this setting, the generated code often contains comments
    and meaningful variable/function naming that are less in human competitors’ code.
    This observation further supports our claim that meaningful, semantic-rich natural
    language can help the model generalize to unseen problems.
  prefs: []
  type: TYPE_NORMAL
- en: Unless unspecified otherwise, we use CF Prob and GPT-3.5 for experiments onwards.
    ³³3We use Deepseek Coder for tuning experiments and GPT-3.5 for all other experiments
    mainly considering their accessibility and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | CF-Prob | CodeContests |'
  prefs: []
  type: TYPE_TB
- en: '| Coder/Reasoner Model | GPT-3.5 | Deepseek 7b | GPT-3.5 | Deepseek 7b |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot Methods |'
  prefs: []
  type: TYPE_TB
- en: '|  | solve@1 | solve@5 | solve@10 | solve@1 | solve@5 | solve@10 | solve@1
    | solve@5 | solve@10 | solve@1 | solve@5 | solve@10 |'
  prefs: []
  type: TYPE_TB
- en: '| Direct Prompt | $1.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Naive CoT | $1.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| Editorial CoT | $1.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| 0-Reasoner Coder | $1.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tune Coder | $0.5$ |'
  prefs: []
  type: TYPE_TB
- en: '| Fted Reasoner w/Full | $1.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Fted Reasoner w/Best | $1.1$ |'
  prefs: []
  type: TYPE_TB
- en: '|       Relative Increment |  | $+37\%$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Performance of baselines and our methods. Fine-tuned Reasoner (Full)
    refers to using all aspects from the reasoning process, while Fine-tuned Reasoner
    (Best) only uses the best aspect. Relative Increment(by percentage) is what’s
    over the best baseline. Solve@k: Solve rates(Percentage) when sampling k.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Effect of Tuning and Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning Strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As we mentioned in Sec. [3.3](#S3.SS3 "3.3 Reasoner: Fine-tuned to generate
    reasoning processes for problems ‣ 3 Method ‣ Distilling Algorithmic Reasoning
    from LLMs via Explaining Solution Programs"), harder problems are more challenging
    and frequently beyond the models’ capacity. Therefore, the silver explanation
    given by the Explainer might not be as accurate, adding noise to the training
    of the Reasoner. So it might be more effective to focus fine-tuning on the simpler
    subset. We compare three strategies to fine-tune deepseek-coder 6.7b: 1) Uniformly
    fine-tune on the entire dataset; 2) Fine-tune with simpler problems weighted more,
    and 3) Uniformly fine-tune on the simple subset only. The solve@10 rates as depicted
    in Table [3](#S4.T3 "Table 3 ‣ Program Sampling Strategy ‣ 4.3 Effect of Tuning
    and Sampling ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs via
    Explaining Solution Programs")b, indicate that the strategy of just up-weighting
    simpler problems yields superior results. This suggests that in scenarios where
    the training data is noisy and limited, including noisier data while assigning
    greater weight to cleaner data can serve as a viable approach to enhancing generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: Program Sampling Strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since our method partitions problem-solving between the Reasoner and Coder,
    it can adopt various sampling strategies. Specifically, the Coder can either a)
    implement different programs given one result from the Reasoner or b) implement
    one program for each of the different results from the Reasoner. Formally, to
    sample $k$ in Table [3](#S4.T3 "Table 3 ‣ Program Sampling Strategy ‣ 4.3 Effect
    of Tuning and Sampling ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from
    LLMs via Explaining Solution Programs"). For cases where 1 deterministic sequence
    is needed (T=1 or M=1), we do not sample; otherwise, temperature is set to 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '| Data Split | Data Size | Training Steps | Data Weight | solve@10 |'
  prefs: []
  type: TYPE_TB
- en: '| Simple+Hard | 8248 | 1547 | Uniform | 3.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Simple+Hard | 8248 | 1617 | 2xSimple:1xHard | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Simple | 4691 | 1172 | Uniform | 3.3 |'
  prefs: []
  type: TYPE_TB
- en: '| (a)fine-tuning strategy comparison w/Deepseek Coder |'
  prefs: []
  type: TYPE_TB
- en: '| Sample Reasoner | Sample Coder | solve@10 |'
  prefs: []
  type: TYPE_TB
- en: '| M=1 | T=10 | 2.8 |'
  prefs: []
  type: TYPE_TB
- en: '| M=2 | T=5 | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| M=5 | T=2 | 4.1 |'
  prefs: []
  type: TYPE_TB
- en: '| M=10 | T=1 | 4.9 |'
  prefs: []
  type: TYPE_TB
- en: '| (b)sampling strategy comparison w/GPT-3.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: (a)Fine-tune settings testing whether to up-weight simpler problems.
    Training steps are kept similar unless data points are significantly fewer. (b)Sampling
    more from Reasoner vs. sampling more from Coder. Both solve@10(percent) are Fine-tuned
    Reasoner w/Full on CF Prob.'
  prefs: []
  type: TYPE_NORMAL
- en: We found that for fixed $k=10=M\times T$ for the experiments in Table [2](#S4.T2
    "Table 2 ‣ Fine-tuning Methods ‣ 4.2 Main Results ‣ 4 Experiments ‣ Distilling
    Algorithmic Reasoning from LLMs via Explaining Solution Programs")(a) and the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since providing the full reasoning process with all aspects can distract the
    model, we found that providing a single aspect can yield a higher overall solve
    rate. We therefore studied the effects of using different aspects of the explanations
    generated by the Reasoner.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Solve@1 | Solve@5 | Solve@10 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *0-shot Reasoner | $1.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Full Reasoning process | $1.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Conceptual Evolution | 1.5 | 4.1 | $5.3$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Key to Solution | $1.0$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Solution Description | $1.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Common Pitfalls | $1.2$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/ Step-by-Step Solution Explanation | $1.1$ | 6.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Ablation Study on the different aspects described in Section [3.2](#S3.SS2
    "3.2 Explainer: Extracting reasoning processes through explaining solution programs
    ‣ 3 Method ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution
    Programs"). Rather than fine-tuning another model, we give only each individual
    aspect of the Reasoner’s output to the Coder. Solve@k: Solve rates(percentage)
    when sampling k.'
  prefs: []
  type: TYPE_NORMAL
- en: The results in Table [4](#S4.T4 "Table 4 ‣ 4.4 Ablation Study ‣ 4 Experiments
    ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs")
    show that each aspect alone can achieve better solve rates against the un-fine-tuned
    0-shot Reasoner, proving the effectiveness of our explanation fine-tuning. Among
    all the different aspects, Conceptual Evolution achieves the best solve@1 and
    solve@5 while Step-by-Step Solution Explanation achieves the best solve@10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Conceptual Evolution provides a solving strategy, it indicates how one
    should think about the problem and where to start, while also touching on the
    high-level idea behind the solution, as well as the choice of algorithm. Therefore,
    by giving a good starting point, it allows the Coder to follow up the idea and
    implement a correct solution. On the other hand, Step-by-Step Solution focuses
    more on implementation details, giving specific instructions on what and how to
    implement, making it easy for the Coder to follow. However, it is not as robust
    as higher-level aspects: If the Reasoner makes a mistake in the details, the Coder
    may not spot the bug and produce an incorrect implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Program Submission Status
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiments, we find that, compared to our method, programs sampled with
    zero-shot methods pass more public tests, but yield significantly lower solve
    rates. This indicates that the pre-trained large language models like GPT-3.5
    and Deepseek Coder tend to give brute-force solution programs initially. We further
    study this phenomenon by analyzing the statistics of the statuses of submitted
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: In competitive-level programming, online judge systems give TLE (Time Limit
    Exceeded) to submissions that do not run efficiently enough to be considered correct.
    As illustrated in Figure [5](#footnote5 "footnote 5 ‣ Figure 4 ‣ 4.5 Program Submission
    Status ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"), the baseline has $42\%$ of the programs that pass the public
    tests are accepted solutions, indicating that the Reasoner learns to use efficient
    algorithms rather than implementing brute-force solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3585252d3ba42c45486b088e14993e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Final online judgment of programs that pass public tests. Accepted:
    correct; TLE: time limit exceeded; WA: wrong answer(s) on private tests; Other:
    memory limit exceeded, runtime error etc.⁵⁵5This is for all submissions, i.e.,
    one problem might have up to $k$ submissions, which is different from the problem-wise
    solve rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a165742fb0158dac61feec29a061861a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The problem difficulty statistics for problems solved with fine-tuned
    or zero-shot Reasoner when sampling 100 reasoning processes per problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Solved Problems Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given that compared to zero-shot settings, using the Fine-tuned Reasoner only
    helps solve several more problems, we investigate further whether this improvement
    is due to learning from explanations or is merely coincidental. Thus, we aggregated
    all solved problems from 5 experiments with different sampling strategies and
    temperatures with both a zero-shot baseline Reasoner and our fine-tuned Reasoner.
    In other words, for each problem, we evaluated 50 programs generated under the
    zero-shot setting and another 50 under the explanation Fine-tuned setting. marking
    problems solved by either to obtain a final set of 23 “solvable” problems.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we measured solve@100 for this “solvable” subset. For each problem in
    the solvable set, we sample 100 explanations from both the Zero-shot and Fine-tuned
    Reasoners. By asking the same Coder to implement them, we found that the Explanation-Fine-tuned
    Reasoner achieves a 100% solve@100 while Zero-shot Reasoner only achieves 60.9%
    on this set.
  prefs: []
  type: TYPE_NORMAL
- en: The difficulty distribution of these problems is shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.5 Program Submission Status ‣ 4 Experiments ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs"). We observed that with
    a fine-tuned Reasoner, more difficult problems can be solved, while the zero-shot
    original model mainly solves problems with the lowest difficulty rating (800).
    This highlights that learning from explanations can enhance the Reasoner’s ability
    to address more complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Problem Statement: Yarik recently found an array $a$ is not, as 2 and 4 are
    both even and adjacent. You need to help Yarik by finding the maximum sum of such
    a subarray. |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned Reasoner’s Reasoning Process  $\checkmark$: |'
  prefs: []
  type: TYPE_TB
- en: '| … checking all possible subarrays would be inefficient. Recognizing that
    the problem exhibits optimal substructure and overlapping subproblems, dynamic
    programming becomes a natural choice. … the maximum sum of a subarray ending at
    index $i$ and the tracking of the maximum sum. |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot Reasoner’s Reasoning Process  $\times$: |'
  prefs: []
  type: TYPE_TB
- en: '| To solve this problem, we can use a dynamic programming approach. We can
    iterate through the array and keep track of the maximum alternating sum ending
    at each element. We will maintain two values, one for the maximum alternating
    sum ending at the current element with an odd index and another for the maximum
    alternating sum ending at the current element with an even index. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: An algorithmic reasoning problem from CF Prob with reasoning processes
    from the fine-tuned and 0-shot Reasoner.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [5](#S4.T5 "Table 5 ‣ 4.7 Case Study ‣ 4 Experiments ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs") shows a problem⁶⁶6[https://codeforces.com/contest/1899/problem/C](https://codeforces.com/contest/1899/problem/C)
    that can be solved by 14% of sampled programs from experiments with Fine-tuned
    Reasoner but none by the zero-shot Reasoner. We compare the output of our Fine-tuned
    Reasoner to that of the 0-shot Reasoner. The generated program and further analysis
    in sampled reasoning processes are given in Appendix [A](#A1 "Appendix A Appendix
    ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs").
  prefs: []
  type: TYPE_NORMAL
- en: This problem can be solved using dynamic programming while specifying the rule
    of having “alternating parities for adjacent elements”. The Fine-tuned Reasoner
    analyzes the inner logic of how to update the DP status and final answer correctly
    considering the condition. By contrast, the 0-Shot Reasoner only gives plausible
    surface-level reasoning output by connecting “alternating parities” to keywords
    like “odd” or “even”.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we propose to distill large language models’ ability to explain
    solutions into reasoning abilities used to help solve problems. More specifically,
    we use an LLM (Teacher) to explain, in natural language, the reasoning processes
    for <problem, solution-program> pairs and fine-tune a smaller LLM (Student) on
    such generated reasoning processes. Using a two-phase framework of reason-then-implement
    where the student model plays the role of a Reasoner to instruct a zero-shot Coder
    to generate the implementation. Experiments on real problems from Codeforces demonstrate
    that our explain-based distilling outperforms several strong 0-shot baselines
    as well as fine-tuning with code solutions alone. Even when absolute solve rates
    are low given how challenging the competitive-level algorithmic problems are,
    our method can significantly improve the performance and solve relatively 37%
    to 75% more problems than the strongest baselines across all experiments. Our
    training set for fine-tuning only contains 8248 data points and 8M tokens, and
    it suggests the efficiency of the proposed distilling method. Quantitative and
    qualitative analyses reveal that the fine-tuned Reasoner learns to avoid brute-force
    implementations and favors more efficient programs compared to its non-fine-tuned
    counterparts. We also curated an up-to-date test set for algorithmic reasoning
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically generating and learning from explanations of problem solutions
    to improve reasoning presents a promising path forward for tackling a broader
    array of complex reasoning tasks. Future investigations could extend this insight
    into domains where current LLMs struggle, such as abstract problem-solving in
    mathematics or logical deduction in law.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work focuses on proposing a method to enhance the reasoning ability of
    large language models on algorithmic reasoning. We will release the dataset CF
    Prob and the explanations generated for training if it’s not against OpenAI’s
    policy. We recognize the potential ways our framework can be misused, particularly
    in the context of online coding competitions. We firmly oppose any application
    of our work that would facilitate dishonest practices including cheating.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue is that the data is in English and Python language only, which
    might need translation if used with other languages. Given the current performance
    of our models and the nascent state of algorithmic reasoning automation, we assess
    that the immediate industry application of our findings is unlikely. Consequently,
    we foresee minimal risk of adverse effects stemming from premature deployment
    in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Balog et al. (2017) Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. In *International
    Conference on Learning Representations*, 2017. URL [https://openreview.net/forum?id=ByldLrqlx](https://openreview.net/forum?id=ByldLrqlx).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucila et al. (2006) Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil.
    Model compression. In *Knowledge Discovery and Data Mining*, 2006. URL [https://api.semanticscholar.org/CorpusID:11253972](https://api.semanticscholar.org/CorpusID:11253972).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *CoRR*, abs/2107.03374,
    2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    Teaching large language models to self-debug, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. Deepseek-coder: When the large language model meets programming – the rise
    of code intelligence, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    and Jacob Steinhardt. Measuring coding challenge competence with apps, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua & Zhang (2022) Wenyue Hua and Yongfeng Zhang. System 1 + system 2 = better
    world: Neural-symbolic chain of logic reasoning. In *Findings of the Association
    for Computational Linguistics: EMNLP 2022*, pp.  601–612, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-emnlp.42](https://aclanthology.org/2022.findings-emnlp.42).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai
    Lu, Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, and Weizhu Chen.
    Competition-level problems are effective llm evaluators, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In
    *Advances in Neural Information Processing Systems*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Jierui Li, Szymon Tworkowski, Yingying Wu, and Raymond Mooney.
    Explaining competitive-level programming solutions using llms, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal
    Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
    Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode.
    *Science*, 378(6624):1092–1097, dec 2022. doi: 10.1126/science.abq1158. URL [https://doi.org/10.1126%2Fscience.abq1158](https://doi.org/10.1126%2Fscience.abq1158).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought
    reasoning, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Olausson et al. (2023) Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang,
    Jianfeng Gao, and Armando Solar-Lezama. Is self-repair a silver bullet for code
    generation?, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2023a) OpenAI. ChatGPT: Optimizing Language Models for Dialogue. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt),
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023b) OpenAI. Gpt-4 technical report, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polosukhin & Skidanov (2018) Illia Polosukhin and Alexander Skidanov. Neural
    program search: Solving programming tasks from description and examples. *CoRR*,
    abs/1802.04335, 2018. URL [http://arxiv.org/abs/1802.04335](http://arxiv.org/abs/1802.04335).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ridnik et al. (2024) Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation
    with alphacodium: From prompt engineering to flow engineering, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca:
    A strong, replicable instruction-following model. [https://crfm.stanford.edu/blog.html](https://crfm.stanford.edu/blog.html),
    2023. Accessed: date-of-access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt
    pre-trained language models for chain of thought. In *Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing*, pp.  2714–2730,
    Abu Dhabi, United Arab Emirates, December 2022\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2022.emnlp-main.174](https://aclanthology.org/2022.emnlp-main.174).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large
    language models. *arXiv preprint arXiv:2201.11903*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, and Chenghua
    Lin. Effective distillation of table-based reasoning ability from llms, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2018) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and
    Graham Neubig. Learning to mine aligned code and natural language pairs from stack
    overflow, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan
    Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid
    instruction tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zelikman et al. (2023) Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman,
    and Nick Haber. Parsel: A (de-)compositional framework for algorithmic reasoning
    with language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang,
    and Lei Li. Algo: Synthesizing algorithmic programs with llm-generated oracle
    verifiers, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and
    Ed Chi. Least-to-most prompting enables complex reasoning in large language models,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen
    Zhou. Pad: Program-aided distillation specializes large models in reasoning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Generated Reasoning Process Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Problem Statement: Yarik recently found an array $a$ is not, as 2 and 4 are
    both even and adjacent. You need to help Yarik by finding the maximum sum of such
    a subarray. |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned Reasoner Generated Process $\checkmark$: |  |'
  prefs: []
  type: TYPE_TB
- en: '| … checking all possible subarrays would be inefficient. Recognizing that
    the problem exhibits optimal substructure and overlapping subproblems, dynamic
    programming becomes a natural choice. … the maximum sum of a subarray ending at
    index $i$:    To solve this problem, we can use a dynamic programming approach.
    We can iterate through the array and keep track of the maximum alternating sum
    ending at each element. We will maintain two values, one for the maximum alternating
    sum ending at the current element with an odd index and another for the maximum
    alternating sum ending at the current element with an even index. |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 Coder Generated Program: |  |'
  prefs: []
  type: TYPE_TB
- en: '| [⬇](data:text/plain;base64,ZGVmIG1heF9hbHRlcm5hdGluZ19zdWJhcnJheV9zdW0oYXJyKToKICAgIG4gPSBsZW4oYXJyKQogICAgZHAgPSBbMF0gKiBuCiAgICBkcFswXSA9IGFyclswXQogICAgZm9yIGkgaW4gcmFuZ2UoMSwgbik6CiAgICAgICAgaWYgYXJyW2ldICUgMiAhPSBhcnJbaS0xXSAlIDI6CiAgICAgICAgICAgIGRwW2ldID0gbWF4KGRwW2ktMV0gKyBhcnJbaV0sIGFycltpXSkKICAgICAgICBlbHNlOgogICAgICAgICAgICBkcFtpXSA9IGFycltpXQogICAgcmV0dXJuIG1heChkcCkKCnQgPSBpbnQoaW5wdXQoKSkKZm9yIF8gaW4gcmFuZ2UodCk6CiAgICBuID0gaW50KGlucHV0KCkpCiAgICBhcnIgPSBsaXN0KG1hcChpbnQsIGlucHV0KCkuc3BsaXQoKSkpCiAgICByZXN1bHQgPSBtYXhfYWx0ZXJuYXRpbmdfc3ViYXJyYXlfc3VtKGFycikKICAgIHByaW50KHJlc3VsdCk=)
    def  max_alternating_subarray_sum(arr): n  =  len(arr) dp  =  [0]  *  n dp[0]  =  arr[0]
    for  i  in  range(1,  n): if  arr[i]  %  2  !=  arr[i-1]  %  2: dp[i]  =  max(dp[i-1]  +  arr[i],  arr[i])
    else: dp[i]  =  arr[i] return  max(dp) t  =  int(input()) for  _  in  range(t):
    n  =  int(input()) arr  =  list(map(int,  input().split())) result  =  max_alternating_subarray_sum(arr)
    print(result) | [⬇](data:text/plain;base64,ZGVmIG1heF9hbHRlcm5hdGluZ19zdW0oYXJyKToKICAgIG4gPSBsZW4oYXJyKQogICAgZHBfb2RkLCBkcF9ldmVuID0gWzBdICogbiwgWzBdICogbgogICAgZHBfb2RkWzBdID0gYXJyWzBdCiAgICBkcF9ldmVuWzBdID0gMAogICAgZm9yIGkgaW4gcmFuZ2UoMSwgbik6CiAgICAgICAgZHBfb2RkW2ldID0gbWF4KGRwX2V2ZW5baS0xXSArIGFycltpXSwgYXJyW2ldKQogICAgICAgIGRwX2V2ZW5baV0gPSBtYXgoZHBfb2RkW2ktMV0gLSBhcnJbaV0sIDApCgpyZXR1cm4gbWF4KGRwX29kZFstMV0sIGRwX2V2ZW5bLTFdKQp0ID0gaW50KGlucHV0KCkpCmZvciBfIGluIHJhbmdlKHQpOgogICAgbiA9IGludChpbnB1dCgpKQogICAgYXJyID0gbGlzdChtYXAoaW50LCBpbnB1dCgpLnNwbGl0KCkpKQogICAgcmVzdWx0ID0gbWF4X2FsdGVybmF0aW5nX3N1bShhcnIpCiAgICBwcmludChyZXN1bHQp)
    def  max_alternating_sum(arr): n  =  len(arr) dp_odd,  dp_even  =  [0]  *  n,  [0]  *  n
    dp_odd[0]  =  arr[0] dp_even[0]  =  0 for  i  in  range(1,  n): dp_odd[i]  =  max(dp_even[i-1]  +  arr[i],  arr[i])
    dp_even[i]  =  max(dp_odd[i-1]  -  arr[i],  0) return  max(dp_odd[-1],  dp_even[-1])
    t  =  int(input()) for  _  in  range(t): n  =  int(input()) arr  =  list(map(int,  input().split()))
    result  =  max_alternating_sum(arr) print(result) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: An algorithmic reasoning problem from CF Prob with reasoning processes
    from the finetuned and 0-shot Reasoner. The incorrect process leads to a failed
    solution(right side).'
  prefs: []
  type: TYPE_NORMAL
- en: We ask a human expert to evaluate the generated reasoning processes by Fine-tuned
    and Zero-shot Reasoner and compare them. Unlike [5](#S4.T5 "Table 5 ‣ 4.7 Case
    Study ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"), we look at all 10 reasoning processes for both. We found
    that while the 0-shot model shows the correct general understanding that it can
    be solved as a dynamic programming problem, it fails to further reason meaningfully.
    9 out of 10 reasoning processes suggest maintaining ‘MaxOdd‘ and ‘MaxEven‘ variables
    to consider odd and even positions separately. Only one reasoning process suggests
    a variant of Kadane’s algorithm, but fails to understand what does “ must have
    alternating parities for adjacent elements” means.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, with the fine-tuned Reasoner, 3 out 10 samples realize ‘odd‘
    and ‘even‘ should be considered as the condition to update the sum, and 2 out
    10 use a greedy method instead. This observation indicates a more human-mimicking
    property: by learning from explanation, the Reasoner tends to try different strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [6](#A1.T6 "Table 6 ‣ A.1 Generated Reasoning Process Analysis ‣
    Appendix A Appendix ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"), the main difference between 2 programs is their used algorithm.
    The correct reasoning process can guide the Coder to implement the correct solution,
    suggesting a better understanding of the problem condition.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompts and Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'System Prompt: You are an expert in algorithm and programming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'User Prompt: Analyze the following competitive programming problem and its
    provided solution. Write an editorial to help students understand the problem-solving
    approach and strategy. Consider the problem’s details, the solution’s approach,
    and the idea behind it.'
  prefs: []
  type: TYPE_NORMAL
- en: ———–
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem: In this problem, we only consider strings consisting of lowercase
    English letters. Strings $s$ are said to be isomorphic when the following conditions
    are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '* $|s|=|t|$ holds.'
  prefs: []
  type: TYPE_NORMAL
- en: '* For every pair i, j, one of the following holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '* $s_{i}=s_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '* $s_{i}\neq s_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, ‘abcac‘ and ‘zyxzx‘ are isomorphic, while ‘abcac‘ and ‘ppppp‘
    are not. A string s is said to be in normal form when the following condition
    is satisfied: * For every string t that is isomorphic to $s,s\leq t$ denotes lexicographic
    comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, ‘abcac‘ is in normal form, but ‘zyxzx‘ is not since it is isomorphic
    to ‘abcac‘, which is lexicographically smaller than ‘zyxzx‘.
  prefs: []
  type: TYPE_NORMAL
- en: You are given an integer N. Print all strings of length N that are in normal
    form, in lexicographically ascending order.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints
  prefs: []
  type: TYPE_NORMAL
- en: '* $1\leq N\leq 10$'
  prefs: []
  type: TYPE_NORMAL
- en: '* All values in input are integers.'
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: 'Input is given from Standard Input in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: N
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that there are K strings of length N that are in normal form: $w_{1},\ldots,w_{K}$
    in lexicographical order. Output should be in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: $w_{1}$
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: a
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: '2'
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: aa
  prefs: []
  type: TYPE_NORMAL
- en: ab
  prefs: []
  type: TYPE_NORMAL
- en: ———–
  prefs: []
  type: TYPE_NORMAL
- en: Below is an accepted solution. Analyze it in the context of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[⬇](data:text/plain;base64,biA9IGxpc3QobWFwKGludCwgaW5wdXQoKS5zcGxpdCgnICcpKSlbMF0KCmRlZiBkZnMoaSwgbXgsIG4sIHJlcywgY3VyID0gW10pOgoJaWYgaT09bjoKCQlyZXMuYXBwZW5kKCcnLmpvaW4oY3VyWzo6XSkpCgkJcmV0dXJuCgoJZm9yIHYgaW4gcmFuZ2UoMCwgbXggKyAxKToKCQlkZnMoaSsxLCBteCsoMSBpZiB2PT1teCBlbHNlIDApLCBuICwgcmVzLCBjdXIrW2Nocih2K29yZCgnYScpKV0pCnJlcyA9W10KZGZzKDAsMCxuLHJlcykKZm9yIHcgaW4gcmVzOiBwcmludCh3KQ==)n  =  list(map(int,  input().split(’  ’)))[0]def  dfs(i,  mx,  n,  res,  cur  =  []):if  i==n:res.append(’’.join(cur[::]))returnfor  v  in  range(0,  mx  +  1):dfs(i+1,  mx+(1  if  v==mx  else  0),  n  ,  res,  cur+[chr(v+ord(’a’))])res  =[]dfs(0,0,n,res)for  w  in  res:  print(w)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer using the following format, with clear, detailed explanations. Use precise
    terms and avoid ambiguity. Use specific descriptions instead of vague expressions
    like "rearrange it" or "apply some operation". Specific instructions for each
    point are as follows in brackets"":'
  prefs: []
  type: TYPE_NORMAL
- en: '1). Problem Restatement: Understand every aspect of the problem first. Summarize
    the problem statement to remove the narrative/storytelling or thematic elements
    like characters or background story, abstract it into a formal statement while
    describing constraints, input-output specifications.'
  prefs: []
  type: TYPE_NORMAL
- en: '2). Step-by-Step Solution Explanation: Explain the solution code step-by-step
    in an algorithm level instead of explaining code line-by-line. Focus on the algorithm
    rather than implementation details like how to read input.'
  prefs: []
  type: TYPE_NORMAL
- en: '3). Solution Description: Based on the understanding of both the problem and
    the solution, describe the solution approach verbally. Explain the solution and
    the high-level reasoning behind it. Explain the WHY of the core algorithms/data
    structures/problem modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '4). Conceptual Evolution: Given all discussed above, describe how one would
    arrive at this solution. This can include how to analyze and approach the problem,
    the choices of type of algorithm used (e.g., dynamic programming, greedy, graph
    theory), the intuition behind the approach, and why this approach works for this
    problem. This is a narrative of the problem-solving journey.'
  prefs: []
  type: TYPE_NORMAL
- en: '5). Common Pitfalls: Pitfalls in the problem description OR common errors that
    students might make while attempting the problem OR corner/edge cases or offset
    handled in the solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 6). Key to Solution:Use one sentence to illustrate the "aha!" steps (key idea
    or trick) in the solution. Be concise, specific and informative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Explainer Prompt Example: Example of the prompt we are using to generate
    Explanations from Explainer (GPT-4). The problem and solution is from Li et al.
    ([2022](#bib.bib14)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'System Prompt: You are an expert in algorithm and programming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'User Prompt: Given the algorithmic reasoning problem below, analyze it first,
    then develop a verbal description of the solution. Focus on the idea rather than
    the implementation details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem: In this problem, we only consider strings consisting of lowercase
    English letters. Strings $s$ are said to be isomorphic when the following conditions
    are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '* $|s|=|t|$ holds.'
  prefs: []
  type: TYPE_NORMAL
- en: '* For every pair i, j, one of the following holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '* $s_{i}=s_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '* $s_{i}\neq s_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, ‘abcac‘ and ‘zyxzx‘ are isomorphic, while ‘abcac‘ and ‘ppppp‘
    are not. A string s is said to be in normal form when the following condition
    is satisfied: * For every string t that is isomorphic to $s,s\leq t$ denotes lexicographic
    comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, ‘abcac‘ is in normal form, but ‘zyxzx‘ is not since it is isomorphic
    to ‘abcac‘, which is lexicographically smaller than ‘zyxzx‘.
  prefs: []
  type: TYPE_NORMAL
- en: You are given an integer N. Print all strings of length N that are in normal
    form, in lexicographically ascending order.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints
  prefs: []
  type: TYPE_NORMAL
- en: '* $1\leq N\leq 10$'
  prefs: []
  type: TYPE_NORMAL
- en: '* All values in input are integers.'
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: 'Input is given from Standard Input in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: N
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that there are K strings of length N that are in normal form: $w_{1},\ldots,w_{K}$
    in lexicographical order. Output should be in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: $w_{1}$
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: a
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: '2'
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: aa
  prefs: []
  type: TYPE_NORMAL
- en: ab
  prefs: []
  type: TYPE_NORMAL
- en: Assistant Prompt Let’s walk through the problem and think about how to solve
    it first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptual Evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: To arrive at this solution, one must understand the definition of isomorphic
    strings and normal form. The key insight is that a string in normal form will
    use the earliest possible character (’a’) and only introduce a new character when
    it cannot use an existing one. This insight leads to a recursive generation of
    strings, where at each step, we either reuse an existing character or introduce
    the next character in the alphabet. The DFS algorithm is a natural fit for this
    recursive construction process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key to Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: The äha!s̈tep is realizing that a string in normal form will always start with
    ’a’ and introduce new characters sequentially only when necessary, which can be
    efficiently generated using a depth-first search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution Description:'
  prefs: []
  type: TYPE_NORMAL
- en: The solution uses a depth-first search (DFS) approach to generate all possible
    strings in normal form. It builds strings character by character, ensuring that
    each new character either matches a previous character or is the next unused character
    in the alphabet. This guarantees that the generated strings are in normal form
    and lexicographically minimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common Pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Not understanding the definition of isomorphic strings and normal form could
    lead to incorrect string generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Failing to ensure that the strings are generated in lexicographic order might
    result in an incorrect output sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Overlooking the base case in the recursive function could lead to infinite
    recursion or missing strings.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Not considering the constraint that N is at most 10 could lead to inefficient
    solutions that do not scale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step-by-Step Solution Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Read the integer N from the input.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Define a recursive function ‘dfs‘ that will build the strings in normal form.
    This function takes the current position ‘i‘, the maximum character index ‘mx‘
    used so far, the string length ‘n‘, a list ‘res‘ to store the results, and the
    current string ‘cur‘.'
  prefs: []
  type: TYPE_NORMAL
- en: '- If ‘i‘ equals ‘n‘, it means we have constructed a string of length ‘n‘. Add
    the current string ‘cur‘ to the results list ‘res‘ and return.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Iterate over all possible next characters, represented by their index ‘v‘
    from 0 to ‘mx + 1‘. The character to be added is determined by converting ‘v‘
    to a character starting from ’a’.'
  prefs: []
  type: TYPE_NORMAL
- en: '- For each character, recursively call ‘dfs‘ with the next position ‘i + 1‘,
    updating ‘mx‘ to ‘mx + 1‘ if ‘v‘ equals ‘mx‘ (indicating that we are using a new
    character for the first time), and appending the new character to ‘cur‘.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Initialize an empty list ‘res‘ to store the resulting strings.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Call ‘dfs‘ starting from position 0, with ‘mx‘ set to 0 (since we start with
    ’a’).'
  prefs: []
  type: TYPE_NORMAL
- en: '- After the recursive calls complete, print each string in ‘res‘.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Reasoner Prompt Example: Example of the prompt we are using to Fine-tune
    the Reasoner (GPT-3.5) to generate explanations of the solution. The content from
    Assistant Prompt is the Explainer’s response to the query in Figure [5](#A1.F5
    "Figure 5 ‣ A.2 Prompts and Examples ‣ Appendix A Appendix ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs")'
  prefs: []
  type: TYPE_NORMAL
- en: 'System Prompt: You are an expert in algorithm and programming.'
  prefs: []
  type: TYPE_NORMAL
- en: 'User Prompt: Please solve the given algorithmic reasoning problem. You are
    also given a Hint on how to solve the problem. Analyze problem and hint, think
    carefully before implementing. The ultimate goal is to develop a complete Python
    3 program to solve the problem. The program should be designed to read input and
    produce output in the specified format mentioned in problem. Enclose the code
    within triple backticks (“‘) for clarity. Keep in mind that this code is intended
    for direct submission to an online judge system, so don’t include test cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the Problem:'
  prefs: []
  type: TYPE_NORMAL
- en: A subarray is a continuous part of array. Yarik recently found an array a of
    n elements and became very interested in finding the maximum sum of a non empty
    subarray. However, Yarik doesn’t like consecutive integers with the same parity,
    so the subarray he chooses must have alternating parities for adjacent elements.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [1, 2, 3] is acceptable, but [1, 2, 4] is not, as 2 and 4 are both
    even and adjacent. You need to help Yarik by finding the maximum sum of such a
    subarray.
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs: []
  type: TYPE_NORMAL
- en: The first line contains an integer $t(1\leq t\leq 10^{4})$— number of test cases.
    Each test case is described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The first line of each test case contains an integer n $(1\leq n\leq 2·10^{5})$—
    length of the array.
  prefs: []
  type: TYPE_NORMAL
- en: The second line of each test case contains n integers $a_{1},a_{2},…,a_{n}~{}(-10^{3}\leq
    a_{i}\leq 10^{3})$— elements of the array.
  prefs: []
  type: TYPE_NORMAL
- en: It is guaranteed that the sum of n for all test cases does not exceed $2·10^{5}$.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: For each test case, output a single integer— the answer to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the Hint on how to solve the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s analyze the problem and think how to solve it. Let’s walk through the
    problem and think about how to solve it first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptual Evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: To arrive at this solution, one might start by considering the brute force approach
    of checking all possible subarrays, which would be inefficient. Recognizing that
    the problem exhibits optimal substructure and overlapping subproblems, dynamic
    programming becomes a natural choice. The intuition is that the maximum sum of
    a subarray ending at index ‘i‘ can be extended by including ‘a[i]‘ if it maintains
    the alternating parity condition. This insight leads to the iterative update of
    ‘dp‘ and the tracking of the maximum sum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key to Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: The "aha!" step is realizing that dynamic programming can be used to efficiently
    track the maximum sum of subarrays ending at each index, while maintaining the
    alternating parity condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution Description:'
  prefs: []
  type: TYPE_NORMAL
- en: The solution uses dynamic programming to keep track of the maximum sum of subarrays
    ending at each index, while ensuring the alternating parity condition. The key
    insight is that the maximum sum of a subarray ending at index ‘i‘ can be derived
    from the maximum sum of a subarray ending at index ‘i-1‘ if the parity condition
    is satisfied. The ‘dp‘ array is used to store these intermediate results, and
    the ‘ans‘ variable is updated to keep track of the maximum sum found so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common Pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Not considering the case where the array has only one element, which should
    be handled separately.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Misunderstanding the alternating parity condition and not updating the ‘dp‘
    array correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Forgetting to initialize the ‘dp‘ array with a value that does not affect
    the maximum sum (e.g., 0).'
  prefs: []
  type: TYPE_NORMAL
- en: '- Overlooking the possibility of negative numbers in the array, which could
    lead to incorrect updates of the ‘dp‘ array.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step-by-Step Solution Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Read the number of test cases ‘t‘.'
  prefs: []
  type: TYPE_NORMAL
- en: '- For each test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Read the length of the array ‘n‘.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Read the elements of the array ‘a‘.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Initialize a dynamic programming (DP) array ‘dp‘ with a length of ‘n‘ and
    fill it with a large negative number (e.g., $-10^{9}$) to represent the maximum
    sum of a subarray ending at each index.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Initialize a variable ‘ans‘ to 0, which will hold the maximum sum found.'
  prefs: []
  type: TYPE_NORMAL
- en: '- If the array has only one element, set ‘ans‘ to that element’s value.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Otherwise, iterate over the array starting from the second element:'
  prefs: []
  type: TYPE_NORMAL
- en: '- If the current element and the previous element have different parities and
    they are not the first two elements of the array, update the ‘dp‘ array at the
    current index with the maximum of the current element and the sum of the current
    element and the ‘dp‘ value at the previous index.'
  prefs: []
  type: TYPE_NORMAL
- en: '- If the current element and the previous element have the same parity, update
    the ‘dp‘ array at the current index with the current element (as the previous
    subarray cannot be extended).'
  prefs: []
  type: TYPE_NORMAL
- en: '- Update ‘ans‘ with the maximum of its current value and the ‘dp‘ value at
    the current index.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Print the value of ‘ans‘ for the current test case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Coder Prompt Example: Example of the prompt we are using to instruct
    the Coder generate code given the problem and the verbal solution(also included
    in this figure). This is the full version for what’s in Table [5](#S4.T5 "Table
    5 ‣ 4.7 Case Study ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs
    via Explaining Solution Programs")'
  prefs: []
  type: TYPE_NORMAL
