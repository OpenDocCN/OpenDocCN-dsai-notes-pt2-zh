- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.00526](https://ar5iv.labs.arxiv.org/html/2307.00526)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mingxue Xu
  prefs: []
  type: TYPE_NORMAL
- en: Department of Electrical and Electronic Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Imperial College London
  prefs: []
  type: TYPE_NORMAL
- en: London, United Kingdom
  prefs: []
  type: TYPE_NORMAL
- en: m.xu21@imperial.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Yao Lei Xu'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Electrical and Electronic Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Imperial College London
  prefs: []
  type: TYPE_NORMAL
- en: London, United Kingdom
  prefs: []
  type: TYPE_NORMAL
- en: yao.xu15@imperial.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: '&Danilo P. Mandic'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Electrical and Electronic Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Imperial College London
  prefs: []
  type: TYPE_NORMAL
- en: London, United Kingdom
  prefs: []
  type: TYPE_NORMAL
- en: d.mandic@imperial.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: High-dimensional token embeddings underpin Large Language Models (LLMs), as
    they can capture subtle semantic information and significantly enhance the modelling
    of complex language patterns. However, the associated high dimensionality also
    introduces considerable model parameters, and a prohibitively high model storage.
    To address this issue, this work proposes an approach based on the Tensor-Train
    Decomposition (TTD), where each token embedding is treated as a Matrix Product
    State (MPS) that can be efficiently computed in a distributed manner. The experimental
    results on GPT-2 demonstrate that, through our approach, the embedding layer can
    be compressed by a factor of up to 38.40 times, and when the compression factor
    is 3.31 times, even produced a better performance than the original GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storage efficiency is currently prohibitive to unlocking the full potential
    of lightweight applications of Large Language Models (LLMs). For example, a well-known
    LLM, the Generative Pre-trained Transformer 2 (GPT-2) [[2](#bib.bib2)] has 1.5
    billion parameters and requires significant disk space, making it prohibitive
    to be deployed on lower-end devices. One solution to improve storage efficiency,
    one solution is compressing the embedding layer, which often accounts for a large
    portion of the parameters. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition"), in GPT-2, the embedding layer takes up 31.02% of
    the parameters of the whole model; therefore, the compression of the embedding
    layer would substantially reduce the space complexity of LLMs and make them available
    in edge devices.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3088083dee2cf2d0a1f1362b2084b52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The number of parameters of the layers in GPT-2.'
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we propose to compress the embedding layer of LLMs through Tensor-Train
    Decomposition (TTD) [[8](#bib.bib8)] in order to store large embeddings in a low-rank
    tensor format, with much fewer parameters. This low-rank tensor format is also
    called TT-format or Matrix Product State (MPS) [[9](#bib.bib9)]. Given the fact
    that in many applications the token vocabulary is ever-changing, we consider each
    individual token embedding (i.e. each row of the token embedding matrix) rather
    than taking the token embedding matrix as a whole. Benefiting from the super-compression
    properties of Tensor Networks (TNs), we tensorize and decompose each token embedding,
    and then construct a highly efficient format of embedding that can be computed
    efficiently through distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed approach is evaluated on the GPT-2 model. The experiment results
    show that, using our approach, the embedding layers can indeed be compressed with
    a compression rate of up to 38.40 times, and with a compression rate of 3.31 didn’t
    sacrifice model performance. As, due to the approximations involved, for the model
    performance change after compression, we considered the performance of our TensorGPT
    on the text reconstruction task, a basic text generation task where the GPT series
    models excel. We found that with the reconstructed embedding layer from the stored
    MPS, the overall performance of the GPT-2 even improved under certain TT rank
    settings, this is likely due to the over-parameterization of the original model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To our best knowledge, we are the first to utilize the Tensor-Train Decomposition
    (TTD) and Matrix Product State (MPS) to compress GPT series models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A novel approach that treats each token embedding as a Matrix Product State
    is proposed, which is shown to be very flexible when the token embeddings are
    inserted or deleted, and also has the potential to be computed in a distributed
    manner.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of rigorous evaluation metrics is adopted to evaluate our approach. The
    experimental results show that our compression approach has satisfactory performance,
    while reducing the number of parameters by a factor of 2.31.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the recent research on the compression of the embedding layers within
    LLMs tensor with decompositions, the closest to our approach is the work by [[13](#bib.bib13)],
    applied TTD to the embedding layer to reduce the space complexity of large language
    models. This was achieved by reshaping the embedding matrix into an order-$2N$
    tensor which was then decomposed and stored as a Matrix Product Operator. While
    this approach reduced the number of parameters significantly, the decomposition
    procedure had to be repeated on the entire embedding matrix every time a new token
    is added to the dictionary. To solve this issue, instead of decomposing and storing
    the embedding matrix directly, we propose an approach that decomposes and stores
    each row of the embedding matrix separately. This reduces the computation costs
    significantly, as the decomposition can be performed locally on every new token.
  prefs: []
  type: TYPE_NORMAL
- en: Recent progress also includes the compression of the embedding table of a recommendation
    system in the work by [[5](#bib.bib5)], where the tensorized neural network is
    trained from scratch, yet our proposed approach operates on a pre-trained neural
    network without an extra training process. In another work [[4](#bib.bib4)], Block-Wise
    Low-Rank Approximation is used to compress very large scale ($\sim$800k vocabulary)
    embeddings, where the embedding matrices are split into blocks according to the
    tokens, and then each block is decomposed by SVD. Except for the difference in
    decomposition methods with our proposed approach, deciding how to reasonably bind
    certain tokens into blocks for a specific vocabulary requires additional effort.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section gives brief mathematical preliminaries of tensor algebra, and
    basic knowledge in LLMs to facilitate the understanding of our proposed methodology
    in Section [4](#S4 "4 Methodology ‣ TensorGPT: Efficient Compression of the Embedding
    Layer in LLMs based on the Tensor-Train Decomposition").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Tensor and Tensor Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Order-N Tensor.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An order-$N$).
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Entries.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The $(i_{1},\ldots,i_{N})$).
  prefs: []
  type: TYPE_NORMAL
- en: Tensorization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A vector, $\mathbf{a}\in\mathbb{R}^{I_{1}I_{2}\cdots I_{N}}$, with the relation
    between their entries defined by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcalbf{A}[i_{1},i_{2},\dots,i_{N}]=a_{i}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: for all $1\leq i_{n}\leq I_{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Matricization (Mode-n unfolding).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mode-$n$, denotes the mode of matricization, and is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{A}_{(1)}\bigg{[}i_{1},\overline{i_{2}i_{3}\ldots i_{N}}\bigg{]}=\mathcalbf{A}[i_{1},i_{2},\ldots,i_{N}]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the overlined subscripts refer to to linear indexing (or Little-Endian)
    [[10](#bib.bib10)], given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\overline{i_{1}i_{2}\dots i_{N}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=1+i_{1}+(i_{2}-1)I_{1}+\cdots+(i_{n}-1)I_{1}\ldots I_{N-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Vectorization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given an order-$N$.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor contraction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The contraction of an order-$N$, with entries
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: A $(2,1)$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Token, Token Embeddings and Embedding Layer in LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Token and Tokenization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In natural language processing (NLP), a token is a meaningful unit of text,
    such as a word, punctuation mark, or other element that contributes to the structure
    and meaning of a sentence or document. Tokens are produced through a process known
    as tokenization, which involves breaking down a piece of text into individual
    units. The GPT series models employ a widely used tokenization method named Byte
    Pair Encoding (BPE) [[7](#bib.bib7)]. The BPE breaks down text into varying-length
    subword units, and is particularly useful for languages with complex morphology
    or when dealing with out-of-vocabulary words.
  prefs: []
  type: TYPE_NORMAL
- en: Token Embedding and Embedding Layer in LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the context of LLMs, “embedding” refers to converting discrete input tokens
    into continuous vector representations. These representations are commonly known
    as word embeddings or token embeddings. In LLMs, the embedding layer is responsible
    for output token embeddings according to the sequential input tokens. This layer
    maps each input token to a high-dimensional vector that captures the semantic
    and syntactic information about the meaning and context of a token. Normally,
    an embedding layer considers a set of tokens $\{v\}$ is excessively large, there
    would be excessive parameter complexity, resulting in high storage costs for the
    embedding layer, and thereafter high storage costs for the whole language model.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The embedding weight matrix W can be seen as a lookup table. A basic embedding
    generation finds the token embeddings corresponding to all the input tokens and
    stacks them according to the input order. It should be noted that in the current
    LLMs, such as GPT-like and BERT-like models, the position and mask information
    of the tokens are also encoded into the embeddings. In these cases, a token embedding
    $\textbf{x}_{v}$ is not merely generated via a lookup process.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section gives a brief introduction to the technical cornerstones that our
    approach relies on, and a detailed description of our proposed approach.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Tensor-Train Decomposition (TTD) and Matrix Product State (MPS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tensor-Train Decomposition [[14](#bib.bib14), [8](#bib.bib8)] was introduced
    to help mitigate the computational bottlenecks that arise from the curse of dimensionality,
    as tensor algorithms can become intractable for high-order tensors. The most common
    form of TT is the Matrix Product State (MPS or TT-MPS), introduced in the quantum
    physics community [[11](#bib.bib11)], which applies the Tensor-Train Singular
    Value Decomposition (TT-SVD) algorithm described in Algorithm [1](#algorithm1
    "In 4.1 Tensor-Train Decomposition (TTD) and Matrix Product State (MPS) ‣ 4 Methodology
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition") [[12](#bib.bib12)] to decomposes a large order-$N$,
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: The tensors $\mathcalbf{G}^{(1)},\ldots,\mathcalbf{G}^{(N)}$, the MPS assumes
    the element-wise form as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{i_{1},i_{2},\dots,i_{N}}=\mathbf{G}^{(1)}_{:,i_{1},:}\cdots\mathbf{G}^{(N)}_{:,i_{N},:}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Input :  Data tensor, $\mathcalbf{X}\in\mathbb{R}^{I_{1}\times I_{2}\times\cdots\times
    I_{N}}$*
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Tensor-Train Singular Value Decomposition (TT-SVD)
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 MPS formatted Token Embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in Section [1](#S1 "1 Introduction ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition") and Section [2](#S2
    "2 Related Work ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs
    based on the Tensor-Train Decomposition"), when the vocabulary changes, the tensor
    decomposition should be re-executed from scratch if the decomposition object is
    the whole embedding weight matrix. On the other hand, loading the whole embedding
    weight matrix into the memory and then decomposing also requires massive memory
    and computation resources. Using the notions in Section [3](#S3 "3 Preliminaries
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition") and Algorithm [1](#algorithm1 "In 4.1 Tensor-Train
    Decomposition (TTD) and Matrix Product State (MPS) ‣ 4 Methodology ‣ TensorGPT:
    Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train
    Decomposition"), decomposing a 2-order W requires roughly the computation cost
    of $\mathcal{O}(VD^{2})$.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than decomposing the whole embedding weight matrix, we propose to decompose
    each token embedding. In this way, each token embedding is reshaped into an order-$N$.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has two advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lower storage cost: The space complexity reduces from an original $\mathcal{O}\left(I^{N}\right)$
    for simplicity;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Affordable computation cost of TTD on resource-constrained nodes. Since token
    embeddings are decomposed individually, the decomposition of an individual token
    embedding or a small number of token embeddings can be offloaded to a single resource-constrained
    node (thread, process or device). On a single node, the computation cost is reduced
    from $\mathcal{O}(VD^{2})$. Also considering the ever-changing vocabulary, this
    approach has the potential to be deployed in a dynamic distributed system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Remark 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The considered tensor embedding procedure is highly effective for a small rank
    tensor, $R$.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, the MPS is a low-rank approximation of the original embedding.
    However, the approximation error will tend to zero as the rank $R$ will have to
    balance the trade-off between the compression power and the approximation ability.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Without considering the position and mask information mentioned in Remark [1](#Thmremark1
    "Remark 1\. ‣ Token Embedding and Embedding Layer in LLMs. ‣ 3.2 Token, Token
    Embeddings and Embedding Layer in LLMs ‣ 3 Preliminaries ‣ TensorGPT: Efficient
    Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"),
    the MPS token embedding approach can be directly used to accelerate the token
    embedding mapping and compress the stored embeddings. However, since the language
    models we discuss in this paper are typical LLMs containing position and mask
    encoding, we shall not consider the above two.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 1: Compression rate $\eta$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Methods &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TT cores’ ranks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Compression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rate $\eta$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE of Layer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Embeddings &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; norm-MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ($\times 10^{-3}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original | —- | 13.71 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TTD weight matrix with 2 cores (SVD) | 1,2,1 | 378.22$\times$ | 0.1209 |
    2.115 | 20.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,4,1 | 189.11$\times$ | 0.1079 | 2.260 | 22.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,8,1 | 94.56$\times$ | 0.1014 | 2.401 | 16.70 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,16,1 | 47.28$\times$ | 0.0989 | 2.481 | 14.70 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,32,1 | 23.64$\times$ | 0.0917 | 2.311 | 10.14 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,641,1 | 11.82$\times$ | 0.0848 | 2.113 | 10.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,128,1 | 5.91$\times$ | 0.0794 | 1.848 | 10.65 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,256,1 | 2.95$\times$ | 0.0690 | 1.500 | 12.74 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,512,1 | 1.48$\times$ | 0.0458 | 1.012 | 9.82 |'
  prefs: []
  type: TYPE_TB
- en: '| TTD for each token $\texttt{ten}(\textbf{x}_{v})$ | 0.1120 | 2.573 | 20.77
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1,1,1,1,1,2,1,1,1,1,1 | 32.00$\times$ | 0.1119 | 3.063 | 20.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,1,1,1,2,2,2,1,1,1,1 | 21.33$\times$ | 0.1115 | 2.957 | 27.81 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,1,1,1,2,4,2,1,1,1,1 | 14.77$\times$ | 0.1113 | 2.864 | 28.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,2,2,2,2,2,2,2,2,2,1 | 10.67$\times$ | 0.1090 | 2.695 | 19.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,1,2,4,4,4,4,4,4,1,1 | 4.00$\times$ | 0.1054 | 2.628 | 22.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,2,4,4,4,4,4,4,4,2,1 | 3.31$\times$ | 0.0995 | 2.574 | 9.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,2,2,4,8,8,8,4,2,2,1 | 2.33$\times$ | 0.0965 | 2.549 | 51.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,2,4,8,8,8,8,8,4,2,1 | 1.13$\times$ | 0.0744 | 0.883 | 39.88 |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Dataset, Tokenization and Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The text dataset used for the experiments was a mixed version of General Language
    Understanding Evaluation (GLUE) [[1](#bib.bib1)] and Microsoft Research Paraphrase
    Corpus (MRPC) [[3](#bib.bib3)], with 11,602 English sentences in total. For the
    tokenization, we chose the BPE mentioned in Section [3.2](#S3.SS2 "3.2 Token,
    Token Embeddings and Embedding Layer in LLMs ‣ 3 Preliminaries ‣ TensorGPT: Efficient
    Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"),
    since it is popular in GPT series models. The language model we used was the HuggingFace
    version of GPT-2¹¹1https://huggingface.co/gpt2, with an embedding weight matrix
    $\textbf{W}\in\mathbb{R}^{50257\times 768}$ is 768.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The 2.12.0 version of TensorFlow was used for the GPT-2 model, while Tensorly [[6](#bib.bib6)],
    a Python package for tensor learning, was used to decompose the token embeddings
    and the embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Remark [2](#Thmremark2 "Remark 2\. ‣ 4.2 MPS formatted Token Embedding
    ‣ 4 Methodology ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs
    based on the Tensor-Train Decomposition") and also to take advantage of the hierarchical
    structure and multiway interactions expressiveness of Tensor-Train Decomposition,
    we reshaped each token embedding $\textbf{x}_{v}$ from index 768 to 1,024 are
    almost zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compression Rate.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We used the embedding layer compression ratio to describe the degree of size
    reduction and efficiency in embedding layer storage. More mathematically, the
    compression rate $\eta$ is the ratio of the original embedding layer size to the
    sum of the size of the compressed token.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\eta=\frac{V\times D}{\sum_{j=1}^{V}\sum_{k=1}^{N}&#124;\mathcalbf{G}^{(k)}_{j}&#124;}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcalbf{G}^{(k)}_{j}$ in the embedding layer weight matrix W.
  prefs: []
  type: TYPE_NORMAL
- en: Distortion Error.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The distortion metrics were used to describe the compression fidelity, that
    is, the similarity between the original embedding layer weights and reconstructed
    embedding layer weights, or the original reconstructed token embeddings and reconstructed
    token embeddings. Considering the inference process of the embedding layer, the
    distortion metrics were first calculated sentence by sentence and then derived
    from the average. There are two kinds of distortion measurements of the embedding
    weight matrix and token embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mean absolute error (MAE) for the embedding weight matrix reconstruction: We
    used MAE to measure the difference between the original embedding layer weight
    matrix and the reconstructed embedding layer weight matrix. A lower MAE suggests
    that the compressed embedding layer weights closely resemble the original embedding
    layer weights, indicating a higher level of fidelity in the compression process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalized mean absolute error (norm-MAE) for the token embeddings: The token
    embedding values vary from minus several hundred to several hundred, and to align
    them for easier comparison like embedding weight matrix, we used the normalized
    mean absolute error. The norm-MAE is the ratio of mean absolute error and the
    difference between the maximum and minimum values of original embeddings. Similar
    to MAE for the embedding weight matrix, a lower norm-MAE indicates a higher compression
    fidelity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compatibility with the subsequent GPT-2 network blocks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary function of GPT-like models is natural language generation. We here
    conducted a text reconstruction task to verify if the reconstructed embedding
    layer can collaborate effectively with the subsequent GPT-2 network blocks for
    natural language generation. The purpose of this task was to reconstruct the original
    input data from the encoded representation in the embedding layer and the subsequent
    network blocks, similar to an autoencoder. This part serves as a sanity check
    for the stored information in the reconstructed embedding layer, and causes evaluation
    via text generation loss of the GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c456a5dfa1b7c663837e2e2968a0e870.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Visualization of the reconstruction distortion error (MAE) for the
    GPT-2 embedding weight matrix W. In each heatmap, a point with location index
    $(j,p)$, with a sacrifice of one token embedding to enable reshaping.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Experiment Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All the evaluation metrics described in Section [5.3](#S5.SS3 "5.3 Evaluation
    Metrics ‣ 5 Experiment ‣ TensorGPT: Efficient Compression of the Embedding Layer
    in LLMs based on the Tensor-Train Decomposition") on the dataset GLUE/MRPC are
    shown in Table [1](#S5.T1 "Table 1 ‣ 5 Experiment ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"). There
    are two approaches tested for a comprehensive analysis - our proposed approach
    introduced in Section [4](#S4 "4 Methodology ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"), and
    the approach of directly decomposing the embedding weight matrix W into two TT
    cores. The latter is actually equivalent to performing tensor SVD.'
  prefs: []
  type: TYPE_NORMAL
- en: As the ranks of TT cores increase, both approaches exhibit a decrease in compression
    rate, fidelity of embedding layer reconstruction (MAE), and fidelity of reproduced
    token embeddings (norm-MAE). There is no apparent decline or increase in the text
    generation loss, possibly because this metric is highly dependent on the dataset
    itself. In all settings, the lowest text generation loss was 9.01, which was achieved
    by our approach when the TT ranks were 1,2,4,4,4,4,4,4,1,1.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We visualized the MAE for the reconstruction of embedding weight matrix W in
    Figure [2](#S5.F2 "Figure 2 ‣ Compatibility with the subsequent GPT-2 network
    blocks. ‣ 5.3 Evaluation Metrics ‣ 5 Experiment ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"). For
    better visualization, we folded the dimension that represents the vocabulary index
    into a reasonable matrix shape. In Figure [2](#S5.F2 "Figure 2 ‣ Compatibility
    with the subsequent GPT-2 network blocks. ‣ 5.3 Evaluation Metrics ‣ 5 Experiment
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition"), the lighter colour indicates a lower MAE, and the
    darker colour indicates a higher MAE.'
  prefs: []
  type: TYPE_NORMAL
- en: From the visualization, since the change of colour shading is not stable, it
    can be inferred that the decrease in compression fidelity does not decrease smoothly
    as the TT ranks increase, even for SVD. As for the decomposition of each token
    embedding, we can identify specific areas where the light (lower MAE) lines consistently
    appear, suggesting that some dimensions of the token embeddings are more precise
    in reconstruction. These dimensions may have the potential for further compression.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of Large Language Models (LLMs), this study has suggested a compression
    approach for the embedding layer. The approach has constructed a power-2 tensor
    Matrix Product State (MPS) format for each token embedding in the embedding weight
    matrix, followed by the further application of the Tensor-Train Decomposition
    (TTD). This approach has demonstrated the advantages of adaptability to the ever-changing
    vocabulary and in a distributed manner, together with the compression of GPT-2
    and has achieved a 3.31$\times$ compression rate with an improved model performance
    in the text reconstruction task.
  prefs: []
  type: TYPE_NORMAL
- en: The superiority of Matrix Product State has not been not fully explored in our
    current implementation. An unsolved problem is the integration of MPS into the
    computation process of token embeddings within other encoded information (e.g.
    position or mask encodings) in LLMs, so that the LLMs can run faster, and be able
    to be deployed on lower-end devices. Furthermore, if the generated token embeddings
    are also formatted by MPS, the embedding generation process might be lighter and
    easier to store as well.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O. & Bowman, S. GLUE:
    A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
    Proceedings Of The 2018 EMNLP Workshop BlackboxNLP: Analyzing And Interpreting
    Neural Networks For NLP. pp. 353-355 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. & Sutskever, I. Language
    models are unsupervised multitask learners. OpenAI Blog. 1, 9 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Dolan, B. & Brockett, C. Automatically Constructing a Corpus of Sentential
    Paraphrases. Proceedings Of The Third International Workshop On Paraphrasing.
    (2005)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Chen, P., Si, S., Li, Y., Chelba, C. & Hsieh, C. Groupreduce: Block-wise
    low-rank approximation for neural language model shrinking. Advances In Neural
    Information Processing Systems. 31 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Yin, C., Acun, B., Wu, C. & Liu, X. Tt-rec: Tensor train compression for
    deep learning recommendation models. Proceedings Of Machine Learning And Systems.
    3 pp. 448-462 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Kossaifi, J., Panagakis, Y., Anandkumar, A. & Pantic, M. TensorLy: Tensor
    Learning in Python. Journal Of Machine Learning Research. 20, 1-6 (2019), http://jmlr.org/papers/v20/18-277.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Gage, P. A new algorithm for data compression. C Users Journal. 12, 23-38
    (1994)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Oseledets, I. Tensor-train decomposition. SIAM Journal On Scientific Computing.
    33, 2295-2317 (2011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Perez-Garcia, D., Verstraete, F., Wolf, M. & Cirac, J. Matrix product state
    representations. ArXiv Preprint Quant-ph/0608197. (2006)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Dolgov, S. & Savostyanov, D. Alternating minimal energy methods for linear
    systems in higher dimensions. SIAM Journal On Scientific Computing. 36, A2248-A2271
    (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Cichocki, A. Era of Big Data Processing: A New Approach via Tensor Networks
    and Tensor Decompositions. Proceedings Of The International Workshop On Smart
    Info-Media Systems In Asia. (2014,3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Cichocki, A., Lee, N., Oseledets, I., Phan, A., Zhao, Q. & Mandic, D.
    Tensor networks for dimensionality reduction and large-scale optimization: Part
    1: low-rank tensor decompositions. Foundations And Trends In Machine Learning.
    9, 249-429 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Khrulkov, V., Hrinchuk, O., Mirvakhabova, L. & Oseledets, I. Tensorized
    Embedding Layers. Proceedings Of The 2020 Conference On Empirical Methods In Natural
    Language Processing: Findings. pp. 4847-4860 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Oseledets, I. & Tyrtyshnikov, E. Breaking the curse of dimensionality,
    or how to use SVD in many dimensions. SIAM Journal On Scientific Computing.31,
    3744-3759 (2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
