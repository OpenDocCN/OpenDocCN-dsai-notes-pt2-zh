- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:03:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Compressing LLMs: The Truth is Rarely Pure and Never Simple'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01382](https://ar5iv.labs.arxiv.org/html/2310.01382)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ${}^{\text{\faApple}}$ Ajay Jaiswal¹, Zhe Gan², Xianzhi Du², Bowen Zhang², Zhangyang
    Wang¹, Yinfei Yang²
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Texas at Austin, ²Apple
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Despite their remarkable achievements, modern Large Language Models (LLMs)
    encounter exorbitant computational and memory footprints. Recently, several works
    have shown significant success in training-free and data-free compression (pruning
    and quantization) of LLMs achieving 50-60% sparsity and reducing the bit-width
    down to 3 or 4 bits per weight, with negligible perplexity degradation over the
    uncompressed baseline. As recent research efforts are focused on developing increasingly
    sophisticated compression methods, our work takes a step back, and re-evaluates
    the effectiveness of existing SoTA compression methods, which rely on a fairly
    simple and widely questioned metric, perplexity (even for dense LLMs). We introduce
    Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated
    tasks to re-define the evaluation protocol for compressed LLMs, which have significant
    alignment with their dense counterparts, and perplexity fail to capture subtle
    change in their true capabilities. LLM-KICK unveils many favorable merits and
    unfortunate plights of current SoTA compression methods: all pruning methods suffer
    significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*,
    25-30%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization
    methods are more successful than pruning; yet, pruned LLMs even at $\geq 50$%
    sparsity are robust in-context retrieval and summarization systems; among others.
    LLM-KICK is designed to holistically access compressed LLMs’ ability for language
    understanding, reasoning, generation, in-context retrieval, in-context summarization,
    *etc.* We hope our study can foster the development of better LLM compression
    methods. All our related codes are planed to be open-sourced. ^†^†\faApple Work
    done during an internship at Apple.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are omnipresent, profoundly influencing not only
    the landscape of NLP (Ram et al., [2023](#bib.bib56); Liu et al., [2023a](#bib.bib41);
    Sawada et al., [2023](#bib.bib58); Qin et al., [2023](#bib.bib55); Zhuo, [2023](#bib.bib73);
    Lee et al., [2023](#bib.bib35)), but also recently buttressing numerous computer
    vision (Lian et al., [2023](#bib.bib38); Wang et al., [2023](#bib.bib64); Lai
    et al., [2023](#bib.bib33); Lu et al., [2023](#bib.bib46)) and graph neural networks
    (Ye et al., [2023](#bib.bib66); Chen et al., [2023](#bib.bib4); Qian et al., [2023](#bib.bib54);
    Duan et al., [2023](#bib.bib11)) algorithms; achieving steller performance across
    various task leaderboards. Despite their numerous unprecedented capabilities,
    their democratization is primarily restricted by the presence of billions of parameters,
    which depends on astonishingly high computational and memory requirements. For
    example, GPT-175B requires 325 GB of GPU memory simply to load its model weights,
    and at least five A100 (80GB) GPUs with sophisticated parallelism techniques (Sheng
    et al., [2023](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To democratize LLMs, considerable efforts have been taking to mitigate their
    high computational cost, mainly divided into two research directions: network
    pruning, and weight quantization. The former shrinks network sizes by removing
    specific weights from the model – essentially setting them to zero, while the
    latter aims to quantize parameters into lower bit-level representations. Several
    recent success in network pruning  (Sun et al., [2023](#bib.bib61); Frantar &
    Alistarh, [2023](#bib.bib13); Jaiswal et al., [2023a](#bib.bib22); Ma et al.,
    [2023](#bib.bib47); Ji et al., [2023](#bib.bib25)) and quantization (Liu et al.,
    [2023c](#bib.bib44); Kim et al., [2023](#bib.bib29); Dettmers et al., [2023a](#bib.bib7);
    Frantar et al., [2022](#bib.bib15); Lin et al., [2023](#bib.bib39); Dettmers et al.,
    [2023c](#bib.bib9)) (detailed related work discussion in Appendix [A.1](#A1.SS1
    "A.1 Related Works ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple")) claim to retain the uncompressed LLM’s performance while
    achieving 50-60% sparsity or up to extreme 2-3 bit quantization. Although these
    advancements look fascinating, in most (if not all) cases, they heavily rely on
    perplexity as their primary metric to evaluate the performance claims. Such relatively
    restricted evaluations limit the scope for developing new compression methods,
    and are potentially ill-suited to identifying new and unexpected capabilities/limitations
    of compressed LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perplexity, even in the case of dense LLMs, has been questioned as an unsatisfactory
    measure for comparing the true potential of LLMs, despite significant variations
    in model scales, training strategies, and architecture choices (Muhlgay et al.,
    [2023](#bib.bib51)). It is important to note that all compressed models are derived
    from the same dense counterpart with high similarity, and aforementioned differences
    don’t exist, making their evaluation more challenging. In this work, we revisit
    a widely known yet under-explored question: How well does perplexity capture the
    change in capabilities of compressed LLMs that have significant alignment with
    their dense counterpart? We focus on the case of compressed LLMs, because we observe
    comparatively more serious failure of perplexity to capture the delicate performance
    variations incurred across varying compression stages of LLMs, demanding a more
    fine-grained investigation.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we attempt to investigate the true promises and limitations of
    state-of-the-art compression algorithms for LLMs. We assemble the first comprehensive
    and diverse collection of tasks with varying difficulty levels to thoroughly study
    compressed LLMs under quantization and network pruning (structured and unstructured
    sparsity patterns). More specifically, we consider a broad range of tasks to evaluate
    subtle changes in pruned and quantized LLMs’ ability for language understanding,
    reasoning, generation, in-context retrieval, long-context summarization, *etc*.
    Note that none of the datasets in our multi-dimensional study of compressed LLMs
    was created from scratch, but we rely on existing datasets as they have been widely
    accepted by researchers, but unfortunately yet not been adopted to study the effect
    of compression. We rigorously measure the performance of SoTA quantization and
    pruning approaches (in their most common, default settings), to understand their
    potential for our challenging and interesting tasks with high practical value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/699cac02c3056338ea387eb73014e7b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: True Merits of SoTA Compression. Top row indicates marginal increase
    in perplexity via using SoTA compression methods, when compared with simple magnitude-based
    pruning. Bottom row indicates the failure of compressed Vicuna-7B (Chiang et al.,
    [2023](#bib.bib5)) (via Magnitude, Wanda, SparseGPT, GPTQ) to respond correctly
    to knowledge-intensive factoid-based questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our key observations and contributions can be unfolded as:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), to re-define
    the evaluation protocols for compressed LLMs and facilitate a comprehensive assessment
    of SoTA compression algorithms. The premise of our work is to develop a suite
    of challenging, realistic, and diverse tasks of high practical importance and
    datasets that can empower a systematic understanding of how existing LLM compression
    strategies truly perform in preserving performance despite their similar perplexities,
    how they differ from each other, and how they compare against smaller LLMs of
    comparable parameter counts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM-KICK unveils many interesting and critical observations, that perplexity-based
    evaluations overlook. <svg id="S1.I1.i2.p1.1.pic1" class="ltx_picture" height="13.38"
    overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg> Most SoTA pruning
    methods suffer significant performance degradation, sometimes at trivial sparsity
    ratios (e.g., 25-30%), despite negligible changes in perplexity. <svg id="S1.I1.i2.p1.2.pic2"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    All SoTA pruning methods do not work satisfactorily for structured N:M sparsity
    patterns on LLM-KICK. <svg id="S1.I1.i2.p1.3.pic3" class="ltx_picture" height="13.38"
    overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg> Current SoTA
    LLM quantization methods are more successful in perpetuating performance in comparison
    to SoTA LLM pruning methods. <svg id="S1.I1.i2.p1.4.pic4" class="ltx_picture"
    height="13.38" overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg> Compressed
    LLMs fail to generate knowledge-enriched and factually correct answers, despite
    the generated text is fluent, consistent, and coherent. <svg id="S1.I1.i2.p1.5.pic5"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>
    Compressed LLMs with larger architectures but same parameter counts perform poorer,
    which favors smaller dense models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We further investigate compressed LLMs’ ability for in-context settings, via
    adopting in-context retrieval augmented question answering (ICRA-QA) (Ram et al.,
    [2023](#bib.bib56)), and text summarization with in-context learning (IC-Sum)
    (Jain et al., [2023](#bib.bib21)). To our surprise, pruned LLMs, even at non-trivial
    sparsity ratios (*e.g.*, $\geq$50%), are robust retrieval systems, and can perform
    text summarization while maintaining similar performance as their dense counterpart.
    However, with increasing compression degrees, their ability to digest longer context
    is affected more than smaller context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2 SoTA LLM Compression: Perplexity, or What’s More?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling neural networks, now LLMs, have achieved astonishing performance benefits
    on a wide array of tasks, but at the cost of gigantic computational and memory
    footprints. Network pruning and weight quantization are two popular remedies to
    mitigate these overheads due to billions of parameter counts in current LLMs.
    Despite numerous existing algorithms for pruning (Singh & Alistarh, [2020](#bib.bib60);
    Zhu & Gupta, [2017](#bib.bib72); Gale et al., [2019](#bib.bib16); Jaiswal et al.,
    [2022](#bib.bib23); Lin et al., [2020](#bib.bib40); Liu et al., [2021a](#bib.bib42);
    Mostafa & Wang, [2019](#bib.bib50); Dettmers & Zettlemoyer, [2019](#bib.bib6);
    Evci et al., [2020](#bib.bib12)) and quantization (Dong et al., [2022](#bib.bib10);
    Cardinaux et al., [2020](#bib.bib1); Kim et al., [2021](#bib.bib30); Liu et al.,
    [2021b](#bib.bib45); Martinez et al., [2020](#bib.bib48)), their ad-hoc adaptation
    for LLMs is restricted, due to the lack of luxury to perform iterative re-training
    to regain any performance drop during compression. Recently, several works have
    shown significant success in training-free and data-free compression of LLMs achieving
    50-60% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with
    negligible perplexity degradation relative to the uncompressed baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity is a statistical measure of how confident a language model predicts
    a text sample and quantifies the “surprise” encoded within language models (the
    lower the perplexity, the better the model). Despite its popularity, perplexity
    has been widely questioned as an unsatisfactory measure to compare the true merits
    of two different LLMs (Muhlgay et al., [2023](#bib.bib51)), even for dense models
    although they significantly vary in model scale, training strategies, and design
    choices (encoder only, decoder only, *etc*.). To address this issue, several works
    (Li et al., [2023](#bib.bib36); Kaddour et al., [2023](#bib.bib28); Muhlgay et al.,
    [2023](#bib.bib51); Zhang et al., [2023](#bib.bib69); Valmeekam et al., [2022](#bib.bib63);
    Liu et al., [2023a](#bib.bib41); Sawada et al., [2023](#bib.bib58); Qin et al.,
    [2023](#bib.bib55); Zhuo, [2023](#bib.bib73); Lee et al., [2023](#bib.bib35))
    attempt to go beyond perplexity, and evaluate the capabilities of dense LLMs across
    commonsense reasoning, language understanding, reading comprehension, programming,
    *etc*. However, it is critically important to note that all compressed models
    are derived from the same dense counterpart with high similarity sharing exactly
    the same scale, training strategies, design choices, *etc*. Surprisingly, unlike
    dense LLMs, we found no such effort has been carried out to understand subtle
    changes in the capabilities of compressed LLMs with varying compression strength.
    Orthogonal to the recent trend to develop new compression algorithms, our work
    provides the first attempt to assess the true merits and limitations of existing
    SoTA LLM compression algorithms, to provide a fair and detailed playground to
    develop better compression algorithms. We focus on the daunting case of compressed
    LLMs because we observe profound failure of perplexity to capture the delicate
    performance variations incurred across varying LLM compressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compressing LLMs: The Truth
    is Rarely Pure and Never Simple")(Top) illustrates the change in perplexity of
    SoTA compression methods (pruning and quantization), such as SparseGPT, Wanda,
    GPTQ and baseline one-shot magnitude-based pruning on Vicuna-7B, 13B, and 33B
    (Chiang et al., [2023](#bib.bib5)). Clearly, the perplexity ($\downarrow$), all
    SoTA pruning methods have almost similar performance as the simple baseline of
    one-shot magnitude-based pruning, which raises questions about their true merits
    within this sparsity range. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compressing
    LLMs: The Truth is Rarely Pure and Never Simple")(Bottom) show the response of
    Vicuna-7B model when compressed with Magnitude, SparseGPT, and Wanda by 50% and
    quantized up to 4-bit. The uncompressed Vicuna-7B was successfully able to generate
    the correct answer, but all compressed versions failed to respond correctly, hallucinating
    with either wrong facts or irrelevant responses.'
  prefs: []
  type: TYPE_NORMAL
- en: '3 LLM-KICK: Unveiling True Merits of LLM Compression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM-KICK, short for Knowledge-Instensive Compressed LLM BenchmarK, is crafted
    to bring the attention of LLM compression community towards incompetence of perplexity
    to correctly reflect subtle changes in the ability of LLMs derived from dense
    counterparts with varying compression strength. LLM-KICK consists of a suite of
    challenging, realistic, and diverse task settings of high practical importance
    and datasets that can empower a systematic understanding of how existing LLM compression
    strategies truly perform in preserving performance despite having similar perplexity.
    Our work thoroughly investigates proclaimed merits/limitations of pruned and quantized
    LLMs for language understanding, reasoning, generation, in-context retrieval,
    in-context summarization, *etc*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, LLM-KICK consists of 3 broad task settings to study how compression
    impacts knowledge encoded during pre-training, how compressed LLMs perform tasks
    when required knowledge is augmented in-context, and how well compressed LLMs
    perform instruction following. To compartmentalize task difficulty and diversity,
    we include factoid-based QA, multiple-choice reasoning-based QA, in-context retrieval
    augmented QA, in-context text summarization, and instruction-based free-form text
    generation. Instead of creating new datasets, we carefully curate LLM-KICK from
    prior works and open-source GitHub repositories which have been widely accepted
    by researchers, but yet not explored by the LLM compression researchers. Our detailed
    prompt design strategies for different task settings can be found in Appendix
    [A.2](#A1.SS2 "A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK
    ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple").'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the expense of redundant experiments and clutter in results, our
    work primarily focuses on the top-2 existing training-free and data-free LLM pruning
    techniques (*i.e.*, SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) and Wanda
    (Sun et al., [2023](#bib.bib61))), along with the baseline of One-shot Magnitude-based
    Pruning (Han et al., [2016](#bib.bib17)), plus a popular quantization technique
    (GPTQ) among recently available choices (Lin et al., [2023](#bib.bib39); Frantar
    et al., [2022](#bib.bib15); Dettmers et al., [2023c](#bib.bib9)). We consider
    two types of sparsities: ($i$) Structured N:M Sparsity: a fine-grained sparsity
    pattern in which only N weights are non-zero for every continuous M weights (Nvidia,
    [2020](#bib.bib53); Zhou et al., [2021](#bib.bib71)). We use Vicuna models for
    experiments, which are open-source chatbot models trained by fine-tuning LLaMA
    (Chiang et al., [2023](#bib.bib5)) on user-shared conversations collected from
    ShareGPT, and have demonstrated impressive 90% quality of OpenAI ChatGPT and Google
    Bard. Note that the aim of this work is not limited to identifying the failure
    cases of SoTA pruning methods, but instead provides an in-depth lookup of LLM’s
    ability under compression, and bring new insights which include highlighting observations
    that work in favor of current SoTA compression methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we study the performance drop of LLMs after compression (without fine-tuning)
    with respect to their dense counterparts using a compression algorithm C. For
    a pre-trained LLM $f(x;\theta)$-bit using a quantization algorithm. Next, we define
    *matching* compressed LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="S3.1.p1.pic1" class="ltx_picture" height="91.21" overflow="visible"
    version="1.1" width="544.5"><g transform="translate(0,91.21) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="501.2" height="63.65"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Matching
    Compressed LLM: A compressed LLM $f_{\texttt{comp}}(x;\theta_{\texttt{C}})$.</foreignobject></g></g></svg>![Refer
    to caption](img/b4d04c68984eedbaff9773f1a0c1e69f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Compressed LLMs for Factoid-based QA. Performance comparison of compressed
    LLMs on Factoid-QA task using FreebaseQA (Jiang et al., [2019](#bib.bib26)). Results
    (average across 3 independent runs) presented are for structured (N:M sparsity),
    unstructured sparsity, and quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Setting 1: How Well Compressed LLMs Access Remaining Knowledge?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg id="S3.SS1.p1.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Factoid-based Question Answering
  prefs: []
  type: TYPE_NORMAL
- en: Task Definition and Rationale. Factoid-based Question Answering (Factoid-QA)
    (Iyyer et al., [2014](#bib.bib20)), which asks precise facts about entities, is
    a long-standing problem in NLP. A typical Factoid-QA task aims to search for entities
    or entity attributes from a knowledge graph, and it is widely used as a tool in
    academia, commercial search engines, and conversational assistants. Modern LLMs
    are trained on gigantic text corpora ingesting a large amount of world knowledge
    about entities and their relationships during pre-training, and have unique abilities
    to generate factually correct responses to user queries. In this task setting,
    we aim to investigate how compression impacts LLMs’ ability to answer natural
    language questions using facts, i.e., entities or attributes knowledge ingested
    within them during pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Details. We use FreebaseQA (Jiang et al., [2019](#bib.bib26)) which
    is a dataset for open-domain QA over the Freebase knowledge graph. The QA pairs
    are collected from various sources, including the TriviaQA dataset (Joshi et al.,
    [2017](#bib.bib27)) and other trivia websites (QuizBalls, QuizZone, KnowQuiz),
    and are matched against Freebase to generate relevant subject-predicate-object
    triples that were further verified by human annotators. TriviaQA dataset shows
    rich linguistic variation and complexity, making it a good testbed for evaluating
    knowledge ingested within LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results and Analysis. The results of various LLM compression methods are demonstrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression
    ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"). Our primary observations
    include: <svg id="S3.SS1.p4.1.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    All SoTA LLM pruning methods seemingly fail to find matching sparse LLMs, even
    at trivial sparsities such as 30-35%. While several methods maintain the matching
    performance at 20-25% sparsity, their performance starts to drop significantly
    after that undergoing a catastrophic failure as sparsity ratio increases. This
    is in contrast with the claim made by SoTA pruning methods that pruning up to
    50-60% of LLMs doesn’t have any significant degradation on performance. <svg id="S3.SS1.p4.2.pic2"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    All pruning methods doesn’t work for fine-grained structured N:M sparsity patterns
    with performance drop as severe as $\geq$8-10% drop in performance for non-aggressive
    8-bit quantization indicates that along with chasing for aggressive quantization
    levels (1-2 bits), it is also important to focus on yet unsolved 8-bit quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS1.p5.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Multiple-Choice Reasoning based Question Answering
  prefs: []
  type: TYPE_NORMAL
- en: Task Formulation and Rationale. Multiple-Choice Reasoning based QA (MCR-QA)
    uses a natural prompting approach to present the question and answer options to
    the LLMs jointly, and have it output the symbol (*e.g.*, “A”) associated with
    its chosen answer option. It allows the model to explicitly compare answer options.
    In this setting, we aim to investigate compressed LLMs’ ability to understand
    natural language questions, effectively reason using knowledge remaining within
    them, and successfully associate the correct answer among the given answer options
    with the symbols that represent them; potentially minimizing the effect of tokenization
    and exact answer generation.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Details. We use the popular MMLU (Massive Multitask Language Understanding)
    benchmark which covers 50+ subjects across STEM, Humanities, Social Sciences,
    and more (Hendrycks et al., [2020](#bib.bib18)). It ranges in difficulty from
    an elementary level to an advanced professional level, and it tests both world
    knowledge and problem-solving ability of LLMs. The granularity and breadth of
    subjects make it ideal for fine-grained evaluation of compressed LLMs’ blind spots.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d472dfc750b74ef6db41a19ac16bcb6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Compressed LLMs for Multiple-Choice Reasoning based QA. Performance
    comparison of compressed LLMs on MCR-QA tasks using the MMLU benchmark (Hendrycks
    et al., [2020](#bib.bib18)). Results (average across 3 independent runs) presented
    are for structured (N:M sparsity), unstructured sparsity, and quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results and Analysis. The results of various LLM compression methods are demonstrated
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Setting 1: How Well Compressed LLMs Access
    Remaining Knowledge? ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression ‣
    Compressing LLMs: The Truth is Rarely Pure and Never Simple"). Our primary observations
    include: <svg id="S3.SS1.p8.1.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    Despite a similar matching compression regime ($\sim$ 20-40%) to Factoid-QA, the
    abrupt performance drop of all SoTA pruning methods for MMLU is comparatively
    subtle due to relaxing the task setting from exact answer generation to correct
    answer selection. <svg id="S3.SS1.p8.3.pic2" class="ltx_picture" height="13.38"
    overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg> No matching
    compressed LLMs are found for N:M structured sparsity. <svg id="S3.SS1.p8.4.pic3"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    SoTA LLM quantization is seemingly more successful than SoTA pruning methods:
    we found 8-bit and 4-bit compressed LLM to be matching for Vicuna-7B and Vicuna-13B,
    respectively. <svg id="S3.SS1.p8.5.pic4" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    Interestingly, both quantization and pruning have comparatively higher performance
    drop for Humanities and Social Science wrt. STEM, which indicates compression
    impacts some disciplines more than others. <svg id="S3.SS1.p8.6.pic5" class="ltx_picture"
    height="13.38" overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg> Surprisingly,
    within the compression tolerance regime, simple one-shot magnitude pruning seems
    to perform quite well in comparison with SoTA pruning method, illustrating its
    high effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg id="S3.SS2.p1.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: In-context Retrieval Augmented Question Answering
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Formulation and Rationale. In-context Retrieval-Augmented Question Answering
    (ICRA-QA) (Ram et al., [2023](#bib.bib56)) grounds the LLM answer generation by
    conditioning on relevant documents retrieved from an external knowledge source
    using retrieval algorithms like BM25\. Our ICRA-QA evaluation system includes
    two high-level components: <svg id="S3.SS2.p2.1.pic1" class="ltx_picture" height="11.25"
    overflow="visible" version="1.1" width="11.25"><g transform="translate(0,11.25)
    matrix(1 0 0 -1 0 0) translate(5.63,0) translate(0,5.63)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -2.98)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">a</foreignobject></g></g></svg> document selection,
    selecting the set of documents upon which to condition; and <svg id="S3.SS2.p2.2.pic2"
    class="ltx_picture" height="14.41" overflow="visible" version="1.1" width="14.41"><g
    transform="translate(0,14.41) matrix(1 0 0 -1 0 0) translate(7.2,0) translate(0,7.2)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.84 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="7.69"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">b</foreignobject></g></g></svg>
    document reading, determining how to incorporate the selected documents into the
    LLM answer process, which requires extracting correct answer phrases from conditioned
    documents. To discount the impact of the lost encoded knowledge during compression,
    ICRA-QA augments the required relevant knowledge for QA task directly within the
    prompt context. In this task setting, we aim to evaluate compressed LLMs’ ability
    to synthesize long in-context knowledge provided within input prompts, and locate
    and retrieve correct answers within it. We also present a head-to-head comparison
    of how augmented knowledge can work as a *remedy* to supplement the lost knowledge
    under compression.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Details. We use TriviaQA (Joshi et al., [2017](#bib.bib27)) for evaluation,
    a popular reading comprehension dataset which includes 95K question-answer pairs
    authored by trivia enthusiasts and independently gathered evidence documents,
    six per question on average, that provide high-quality distant supervision for
    answering the questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ca0e214f6e1487efe21c84ee09f6d1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Compressed LLMs for In-context Retrieval Augmented QA. Performance
    comparison of compressed LLMs on ICRA-QA task. We present head-to-head comparison
    of closed-book evaluation (no external knowledge is augmented in-context) with
    open-book evaluation (external knowledge is augmented in-context). Results (average
    across 3 independent runs) presented are for structured N:M sparsity, unstructured
    sparsity, and quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results and Analysis. The results of various LLM compression methods are demonstrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize
    Augmented Knowledge? ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression ‣
    Compressing LLMs: The Truth is Rarely Pure and Never Simple"). The closed-book
    setting differs from ICRA-QA (*i.e.*, using the open-book setting) only in terms
    of whether conditioning on relevant documents retrieved from an external knowledge
    source. Our key findings are: <svg id="S3.SS2.p4.1.pic1" class="ltx_picture" height="13.38"
    overflow="visible" version="1.1" width="13.38"><g transform="translate(0,13.38)
    matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg> When compressed
    LLMs are conditioned on external knowledge (open book) and assigned the task of
    in-context retrievers, *i.e.*, extracting correct answer phrases from in-context
    knowledge, they perform significantly well even in extremely high compression
    regime. Vicuna-7B can remain matching till $\sim$50% sparsity and 4-bit quantization.
    Our experimental results send a positive signal that even if high compression
    leads to significant knowledge loss, it doesn’t leave LLMs completely useless,
    and they still work as robust in-context retrievers. <svg id="S3.SS2.p4.4.pic2"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    Despite we observe a significant benefit while conditioning external knowledge,
    no matching compressed LLM can be identified for N:M sparsity. <svg id="S3.SS2.p4.5.pic3"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    Again, we observe surprisingly good performance of simple one-shot unstructured
    magnitude pruning wrt. SparseGPT (second-order pruning) and Wanda (activation-based
    pruning) that rely on calibration data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5de5fbbcc95d0fa602ba88d9994831f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Compressed LLMs for In-Context Summarization. Performance comparison
    of compressed Vicuna-7B for in-context summarization of small, medium, and large
    stories while preserving coherence, consistency, fluency, and relevance. Results
    (average across 3 independent runs) presented are for structured (2:4 sparsity
    - Row 3), unstructured sparsity, and quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c44ede73da19ddbb8c4256747ff3eef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Compressed LLMs for Instruction Following. LLM-as-a-Judge: GPT-4
    based evaluation of compressed Vicuna-7B response wrt. ChatGPT (davici-003). (Left)
    unstructured sparsity; (middle) structured N:M sparsity; (c) comparison of average
    unique token counts generated by compressed Vicuna-7B for 80 prompts across 10
    different categories.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.SS2.p5.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: In-Context Text Summarization
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Formulation and Details. Modern LLMs have shown astonishing success in
    summarizing long-context documents in both abstractive and extractive settings.
    However, it is yet not explored how compression impacts LLMs’ capability for summarization.
    In this task setting, we aim to investigate compressed LLMs’ ability to hold onto
    consistency, coherence, fluency, and relevance when prompted to summarize textual
    information of varying length (small, medium, and large) in abstractive setting
    (Jain et al., [2023](#bib.bib21)). For evaluation, similar to Zheng et al. ([2023](#bib.bib70)),
    we propose to use GPT-4 as a judge, which compares the compressed LLM generated
    summaries wrt. GPT-3.5 (text-davinci-003) generated summaries. Detailed evaluation
    settings can be found in Appendix [A.3](#A1.SS3 "A.3 In-Context Summarization
    Evaluation Settings ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple").'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Details. We use a popular summarization dataset CNN/DailyMail (Chen
    et al., [2016](#bib.bib2)) for evaluation, which is an English-language dataset
    containing just over 300k unique news articles written by journalists at CNN and
    DailyMail. We created 3 subset categories {small ($\leq$ 790 words)} of stories,
    each with 100 articles reflecting word distribution of CNN/DailyMail to minimize
    OpenAI API costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results and Analysis. Results are summarized in Figure [5](#S3.F5 "Figure 5
    ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge? ‣ 3
    LLM-KICK: Unveiling True Merits of LLM Compression ‣ Compressing LLMs: The Truth
    is Rarely Pure and Never Simple"). We summarize our main observations as: <svg
    id="S3.SS2.p8.1.pic1" class="ltx_picture" height="13.38" overflow="visible" version="1.1"
    width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0)
    translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    All pruning and quantization methods tend to perform surprisingly well for in-context
    summarization, preserving high consistency, coherence, fluency, and relevance
    in generated summaries, which is an encouraging observation in favor compression.
    <svg id="S3.SS2.p8.2.pic2" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    With increasing context length (*i.e.*, long stories), we observe a sharper performance
    drop for compressed LLMs, which highlights that compression impacts LLMs’ ability
    to synthesize and summarize longer context lengths. <svg id="S3.SS2.p8.3.pic3"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    Quantization again seems to perform better than SoTA pruning methods, and surprisingly
    benefiting positively over the dense model performance. <svg id="S3.SS2.p8.4.pic4"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    No matching compressed LLM can be identified for 2:4 structured sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.3 Setting 3: How Well Compressed LLMs Perform Instruction Following?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Task Formulation and Rationale. In this task setting, we investigate compressed
    LLMs’ ability to answer open-ended questions and evaluate their multi-turn conversational
    and instruction-following ability – two critical elements for human preference.
    Evaluating AI chatbots is a challenging task, as it requires examining language
    understanding, reasoning, and context awareness. To compare the performance of
    compressed LLMs’ responses, we closely follow the prompt design setting in MT-Bench
    (Zheng et al., [2023](#bib.bib70)) using GPT-4 as a judge. We prompt GPT-4 to
    rate the answers generated by compressed LLMs wrt. GPT-3.5 (text-davinci-003)
    model based on varying metrics (*e.g.*, correctness, helpfulness, logic, accuracy,
    *etc.*) on a scale of [0-10] with detailed explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Details. We rely on the 80 high quality multi-turn questions identified
    in MT-Bench (Zheng et al., [2023](#bib.bib70)). This setting covers common-use
    human-centric interaction with LLMs, and focuses on challenging questions to differentiate
    models. We used 8 common categories of user prompts to guide the prompt construction
    to interact with compressed LLMs: writing, roleplay, extraction, reasoning, math,
    coding, *etc*. For each category, we adopted manually designed 10 multi-turn questions
    from MT-Bench to evaluate our compressed models. Details can be found in Appendix
    [A.4](#A1.SS4 "A.4 Instruction Following Ability Evaluation Setting ‣ Appendix
    A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results and Analysis. Results are summarized in Figure [6](#S3.F6 "Figure 6
    ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge? ‣ 3
    LLM-KICK: Unveiling True Merits of LLM Compression ‣ Compressing LLMs: The Truth
    is Rarely Pure and Never Simple"). Our primary observations are: <svg id="S3.SS3.p3.1.pic1"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    Unlike in-context text summarization, in this task setting, compressed LLMs have
    to access the knowledge to respond to conversations maintaining high helpfulness,
    relevance, accuracy, and detail. We again observe that compressed LLMs with various
    pruning methods are matching only up to sparsity ratio of $\sim$ 25%. <svg id="S3.SS3.p3.3.pic2"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    Surprisingly, in the matching regime, the simple baseline of one-shot magnitude
    pruning performs comparable or slightly better than SoTA pruning methods. <svg
    id="S3.SS3.p3.4.pic3" class="ltx_picture" height="13.38" overflow="visible" version="1.1"
    width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0)
    translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    No matching subnetwork can be identified for N:M sparsity. <svg id="S3.SS3.p3.5.pic4"
    class="ltx_picture" height="13.38" overflow="visible" version="1.1" width="13.38"><g
    transform="translate(0,13.38) matrix(1 0 0 -1 0 0) translate(6.69,0) translate(0,6.69)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    Interestingly, our average generated unique token analysis in Figure [6](#S3.F6
    "Figure 6 ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge?
    ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression ‣ Compressing LLMs: The
    Truth is Rarely Pure and Never Simple")(c) illustrates that compressed LLMs lose
    the ability to generate distinct unique content, instead, they can only produce
    more repetitive texts.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Additional Results and Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Small-Dense vs. Large-Sparse: which is favorable?'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We attempt to understand an interesting question: if pruned LLMs with larger
    architecture (Large-Sparse) is better than smaller dense models with similar parameter
    count (Small-Dense)? Pruning large LLMs doesn’t come for free, and it is important
    to investigate if the cost of pruning can be reflected in the performance benefit
    of Large-Sparse models. To our surprise, in comparison with dense Vicuna-7B (MMLU
    accuracy 46.7%), we found compressed Vicuna-13B with exactly similar parameter
    count (46.16% sparsity) of 7 billion using one-shot magnitude, Wanda, SparseGPT
    can only achieve MMLU accuracy of 31.7%, 45.3%, and 46.3%, respectively. This
    is a clear indication that current sparsity algorithms are not yet up to a stage
    where the cost of pruning can be justified by performance benefits obtained from
    large-sparse compressed models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2b84305da5d666b8cb9eeab9984758e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Zero-shot performance of 50% & 70% pruned Vicuna-7B wrt. calibration
    sample counts.'
  prefs: []
  type: TYPE_NORMAL
- en: How many calibration data samples are needed?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We attempt to analyze how calibration dependent pruning methods (Wanda and
    SparseGPT) perform with varying amount of calibration samples. Figure [7](#S4.F7
    "Figure 7 ‣ Small-Dense vs. Large-Sparse: which is favorable? ‣ 4 Additional Results
    and Discussions ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple")
    illustrates the zero-shot performance of 50% & 70% pruned Vicuna-7B using Wanda
    and SparseGPT on knowledge-intensive MMLU benchmark. It is interesting to observe
    that calibration sample count plays a vital role in preserving the performance
    of SparseGPT unlike Wanda. Note that at high sparsity ratio (70%), Wanda cannot
    recover any performance; SparseGPT surprisingly benefits noticeably from calibration.
    This suggests that carefully selected calibration samples can play a vital role
    in designing better pruning algorithms to compress LLMs even up to significantly
    high sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/270551322b71d0a26249d2e9131f88e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: k-shot results of Vicuna-7B pruned with Wanda.'
  prefs: []
  type: TYPE_NORMAL
- en: Does k-shots benefit compressed LLMs?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this section, we aim to investigate how few-shot in-context learning examples
    can benefit SoTA pruning methods to preserve performance across various sparsity
    levels. Figure [8](#S4.F8 "Figure 8 ‣ How many calibration data samples are needed?
    ‣ 4 Additional Results and Discussions ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple") illustrates the performance comparison of Vicuna-7B at
    varying sparsity ratios when augmented with k-shot in-context examples on MMLU
    benchmark. It is interesting to observe that k-shot in-context learning examples
    have marginal impact on dense network performance, while they significantly help
    in preserving the performance at high sparsity. Moreover, we found 2-3 examples
    are sufficient to retain the performance, and supplementing additional examples
    doesn’t necessarily provide further noticeable benefits.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose to explore the effectiveness of SoTA compression methods
    beyond perplexity to address the inability of perplexity to capture the subtle
    variations incurred during the derivation of compressed LLMs from their dense
    counterparts. Our work introduces Knowledge-Intensive Compressed LLM BenchmarK
    (LLM-KICK) to facilitate a fair and holistic evaluation by unveiling many merits
    and pitfalls of SoTA compression methods. Our study reveals that compression significantly
    impacts the knowledge encoded in LLMs during pre-training, compressed LLMs perform
    quite well with knowledge augmented in-context settings. We primarily restrict
    our evaluation to Vicuna (decoder-only architecture) due to its open-source license,
    high performance, and instruction-following ability. For future work, we aim to
    investigate how the lost knowledge due to compression can be recovered using parameter-efficient
    fine-tuning methods, *e.g.*, LoRA (Hu et al., [2021](#bib.bib19)) and QLoRA (Dettmers
    et al., [2023b](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cardinaux et al. (2020) Fabien Cardinaux, Stefan Uhlich, Kazuki Yoshiyama, Javier Alonso
    García, Lukas Mauch, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Iteratively
    training look-up tables for network quantization. *IEEE Journal of Selected Topics
    in Signal Processing*, 14(4):860–870, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016) Danqi Chen, Jason Bolton, and Christopher D Manning. A thorough
    examination of the cnn/daily mail reading comprehension task. *arXiv preprint
    arXiv:1606.02858*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis
    for pre-trained bert networks. *Advances in neural information processing systems*,
    33:15834–15846, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi
    Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential
    of large language models (llms) in learning on graphs. *arXiv preprint arXiv:2307.03393*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers & Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. Sparse networks
    from scratch: Faster training without losing performance. *arXiv preprint arXiv:1907.04840*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *ArXiv*, abs/2305.14314,
    2023a. URL [https://api.semanticscholar.org/CorpusID:258841328](https://api.semanticscholar.org/CorpusID:258841328).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023b) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023c) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *ArXiv*, abs/2306.03078, 2023c. URL [https://api.semanticscholar.org/CorpusID:259076379](https://api.semanticscholar.org/CorpusID:259076379).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Runpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, and
    Kaisheng Ma. Finding the task-optimal low-bit sub-distribution in deep neural
    networks. In *International Conference on Machine Learning*, pp. 5343–5359\. PMLR,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2023) Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang
    Ooi, Qizhe Xie, and Junxian He. Simteg: A frustratingly simple approach improves
    textual graph learning. *arXiv preprint arXiv:2308.02565*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning*, pp. 2943–2952\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2021) Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-fac:
    Efficient matrix-free approximations of second-order information. *Advances in
    Neural Information Processing Systems*, 34:14873–14886, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *ArXiv*, abs/2210.17323, 2022. URL [https://api.semanticscholar.org/CorpusID:253237200](https://api.semanticscholar.org/CorpusID:253237200).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2016) Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. In *International Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyyer et al. (2014) Mohit Iyyer, Jordan L. Boyd-Graber, Leonardo Max Batista
    Claudino, Richard Socher, and Hal Daumé. A neural network for factoid question
    answering over paragraphs. In *Conference on Empirical Methods in Natural Language
    Processing*, 2014. URL [https://api.semanticscholar.org/CorpusID:216034672](https://api.semanticscholar.org/CorpusID:216034672).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2023) Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra,
    Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. Multi-dimensional
    evaluation of text summarization with in-context learning. *arXiv preprint arXiv:2306.01200*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal et al. (2023a) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaiswal et al. (2022) Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding,
    and Zhangyang Wang. Training your sparse neural network better with any mask.
    In *International Conference on Machine Learning*, pp. 9833–9844\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal et al. (2023b) Ajay Kumar Jaiswal, Shiwei Liu, Tianlong Chen, Ying
    Ding, and Zhangyang Wang. Instant soup: Cheap pruning ensembles in a single pass
    can draw lottery tickets from large models. In *International Conference on Machine
    Learning*, pp. 14691–14701\. PMLR, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2023) Yupeng Ji, Yibo Cao, and Jiucai Liu. Pruning large language
    models via accuracy predictor. *arXiv preprint arXiv:2309.09507*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2019) Kelvin Jiang, Dekun Wu, and Hui Jiang. Freebaseqa: A new
    factoid qa data set matching trivia-style question-answer pairs with freebase.
    In *North American Chapter of the Association for Computational Linguistics*,
    2019. URL [https://api.semanticscholar.org/CorpusID:174800890](https://api.semanticscholar.org/CorpusID:174800890).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    *arXiv preprint arXiv:1705.03551*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaddour et al. (2023) Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie
    Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of
    large language models. *arXiv preprint arXiv:2307.10169*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2023) Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park,
    Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-efficient fine-tuning of compressed
    large language models via sub-4-bit integer quantization. *ArXiv*, abs/2305.14152,
    2023. URL [https://api.semanticscholar.org/CorpusID:258841104](https://api.semanticscholar.org/CorpusID:258841104).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney,
    and Kurt Keutzer. I-bert: Integer-only bert quantization. In *International conference
    on machine learning*, pp. 5506–5518\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lagunas et al. (2021) François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M
    Rush. Block pruning for faster transformers. *arXiv preprint arXiv:2109.04838*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2023) Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,
    Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model.
    *arXiv preprint arXiv:2308.00692*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. Optimal brain
    damage. In *Advances in neural information processing systems*, pp. 598–605, 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2023) Noah Lee, Na Min An, and James Thorne. Can large language
    models infer and disagree like humans? *ArXiv*, abs/2305.13788, 2023. URL [https://api.semanticscholar.org/CorpusID:258841424](https://api.semanticscholar.org/CorpusID:258841424).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng,
    Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, et al. Flm-101b: An open llm and
    how to train it with 100 k budget. *arXiv preprint arXiv:2309.03852*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh
    Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. Large
    models are parsimonious learners: Activation sparsity in trained transformers.
    *arXiv preprint arXiv:2210.06313*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. (2023) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *arXiv preprint arXiv:2305.13655*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *ArXiv*, abs/2306.00978, 2023. URL [https://api.semanticscholar.org/CorpusID:258999941](https://api.semanticscholar.org/CorpusID:258999941).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020) Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev,
    and Martin Jaggi. Dynamic model pruning with feedback. In *International Conference
    on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SJem8lSFwB](https://openreview.net/forum?id=SJem8lSFwB).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong,
    Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et al. Llmrec: Benchmarking
    large language models on recommendation task. *arXiv preprint arXiv:2308.12241*,
    2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021a) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi,
    Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin
    Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration.
    *Advances in Neural Information Processing Systems (NeurIPs).*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin
    Huang, Ajay Jaiswal, and Zhangyang Wang. Sparsity may cry: Let us fail (current)
    sparse neural networks together! *arXiv preprint arXiv:2303.02141*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023c) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. Post-training quantization for vision transformer. In A. Beygelzimer,
    Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), *Advances in Neural Information
    Processing Systems*, 2021b. URL [https://openreview.net/forum?id=9TX5OsKJvm](https://openreview.net/forum?id=9TX5OsKJvm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. *arXiv preprint arXiv:2304.09842*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martinez et al. (2020) Julieta Martinez, Jashan Shewakramani, Ting Liu, Ioan Andrei
    Bârsan, Wenyuan Zeng, and Raquel Urtasun. Permute, quantize, and fine-tune: Efficient
    compression of neural networks. *2021 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, pp.  15694–15703, 2020. URL [https://api.semanticscholar.org/CorpusID:225103308](https://api.semanticscholar.org/CorpusID:225103308).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mostafa & Wang (2019) Hesham Mostafa and Xin Wang. Parameter efficient training
    of deep convolutional neural networks by dynamic sparse reparameterization. In
    *International Conference on Machine Learning*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muhlgay et al. (2023) Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner,
    Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham.
    Generating benchmarks for factuality evaluation of language models. *arXiv preprint
    arXiv:2307.06908*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nallapati et al. (2016) Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing
    Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and
    beyond. *arXiv preprint arXiv:1602.06023*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvidia (2020) Nvidia. Nvidia a100 tensor core gpu architecture. *https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian et al. (2023) Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong
    Liu. Can large language models empower molecular property prediction? *arXiv preprint
    arXiv:2307.07443*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? *arXiv preprint arXiv:2302.06476*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language
    models. *arXiv preprint arXiv:2302.00083*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning:
    Adaptive sparsity by fine-tuning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
    Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  20378–20389\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sawada et al. (2023) Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav
    Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, and Aran
    Komatsuzaki. Arb: Advanced reasoning benchmark for large language models. *arXiv
    preprint arXiv:2307.13692*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    et al. High-throughput generative inference of large language models with a single
    gpu. *arXiv preprint arXiv:2303.06865*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh & Alistarh (2020) Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valmeekam et al. (2022) Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,
    and Subbarao Kambhampati. Large language models still can’t plan (a benchmark
    for llms on planning and reasoning about change). *ArXiv*, abs/2206.10498, 2022.
    URL [https://api.semanticscholar.org/CorpusID:249889477](https://api.semanticscholar.org/CorpusID:249889477).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large
    language model is also an open-ended decoder for vision-centric tasks. *arXiv
    preprint arXiv:2305.11175*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Dongkuan Xu, Ian EH Yen, Jinxi Zhao, and Zhibin Xiao. Rethinking
    network pruning–under the pre-train and fine-tune paradigm. *arXiv preprint arXiv:2104.08682*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2023) Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng
    Zhang. Natural language is all a graph needs. *arXiv preprint arXiv:2308.07134*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. (2021) Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and
    Moshe Wasserblat. Prune once for all: Sparse pre-trained language models. *arXiv
    preprint arXiv:2111.05754*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin,
    Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models
    with upper confidence bound of weight importance. In *International Conference
    on Machine Learning*, pp. 26809–26823\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen
    McKeown, and Tatsunori Hashimoto. Benchmarking large language models for news
    summarization. *ArXiv*, abs/2301.13848, 2023. URL [https://api.semanticscholar.org/CorpusID:256416014](https://api.semanticscholar.org/CorpusID:256416014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: m fine-grained structured
    sparse neural networks from scratch. *arXiv preprint arXiv:2102.04010*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu & Gupta (2017) Michael Zhu and Suyog Gupta. To prune, or not to prune:
    exploring the efficacy of pruning for model compression. *arXiv preprint arXiv:1710.01878*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuo (2023) Terry Yue Zhuo. Large language models are state-of-the-art evaluators
    of code generation. *arXiv preprint arXiv:2304.14317*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Related Works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.1.1 Sparsity in Large Language Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The advent of large-scale pre-trained models has led to the development of advanced
    post-training pruning methods, aiming to enhance the cost-effectiveness of these
    expansive models (Sanh et al., [2020](#bib.bib57); Chen et al., [2020](#bib.bib3);
    Jaiswal et al., [2023b](#bib.bib24); Zafrir et al., [2021](#bib.bib67); Kurtic
    et al., [2022](#bib.bib31); Xu et al., [2021](#bib.bib65); Lagunas et al., [2021](#bib.bib32);
    Zhang et al., [2022](#bib.bib68); Frantar et al., [2021](#bib.bib14); Jaiswal
    et al., [2023a](#bib.bib22); Ma et al., [2023](#bib.bib47); Ji et al., [2023](#bib.bib25)).
    Among them, Frantar et al. ([2021](#bib.bib14)) extend second-order pruning to
    the BERT-level scale, enabling the pruning of blocks of weights and achieving
    state-of-the-art results for sparse BERT. Frantar & Alistarh ([2023](#bib.bib13))
    introduce SparseGPT for pruning large language models (LLMs) in a single shot
    without requiring re-training or fine-tuning. They leverage column-wise second-order
    pruning, and successfully remove 100B weights from OPT-175B without a significant
    increase in perplexity. More recently, Sun et al. ([2023](#bib.bib61)) propose
    a straightforward pruning method that takes both weights and activations into
    account, demonstrating comparable performance to Frantar & Alistarh ([2023](#bib.bib13)). Li
    et al. ([2022](#bib.bib37)) reveal that activation sparsity is a prevalent phenomenon
    in Transformers (90% of intermediate output), yielding another opportunity for
    acceleration. Liu et al. ([2023b](#bib.bib43)) introduce a large-scale SMC-Bench,
    indicating that state-of-the-art magnitude- and/or gradient-based sparse algorithms
    fall short when applied out-of-the-box to larger-scale models and a selected of
    complex downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Quantization in Large Language Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the recent open-source releases of language models like BLOOM, Vicuna,
    LLaMa, OPT, etc., quantization has emerged as a widely embraced technique to alleviate
    the storage and computational overhead of deep learning models. Recent research
    endeavors have harnessed quantization to compress LLMs and they can be classified
    into the two mentioned approaches: Quantization-Aware Training (QAT), and Post-Training
    Quantization (PTQ). In QAT, the quantization objective is embedded into the LLM
    training process, enabling them to adapt to low-precision representations and
    handle precision loss caused by quantization. LLM-QAT (Liu et al., [2023c](#bib.bib44))
    proposes a data-free distillation method that leverages generations produced by
    the pre-trained model, preserving the original output distribution and allows
    quantizing LLaMa models independent of its training data. PEQA (Kim et al., [2023](#bib.bib29))
    operates through a dual-stage process: initially, the parameter matrix of each
    fully-connected layer undergoes quantization into a matrix of low-bit integers
    and a scalar vector; subsequently, fine-tuning occurs on the scalar vector for
    each downstream task. QLoRA (Dettmers et al., [2023a](#bib.bib7)) proposes an
    efficient finetuning approach that reduces memory usage enough to finetune a 65B
    parameter model on a single 48GB GPU while preserving full 16-bit finetuning task
    performance by backpropagating gradients through a frozen, 4-bit quantized pretrained
    language model into Low Rank Adapters (LoRA). PTQ involves quantizing the parameters
    of LLMs after the completion of the LLM’s training phase. GPTQ (Frantar et al.,
    [2022](#bib.bib15)) proposes a novel layer-wise quantization technique based on
    approximate second-order information resulting a bitwidth reduction to 3 or 4
    bits per weight, with minimal accuracy loss compared to the uncompressed version.
    AWQ (Lin et al., [2023](#bib.bib39)) based on the observation that weights are
    not equally important: protecting only 1% of salient weights can greatly reduce
    quantization error, employs an activation-aware approach by considering the significance
    of weight channels corresponding to larger activation magnitudes. SpQR (Dettmers
    et al., [2023c](#bib.bib9)) works by identifying and isolating outlier weights,
    which cause particularly-large quantization errors, and storing them in higher
    precision, while compressing all other weights to 3-4 bits, and achieves relative
    accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.3 Large Language Models and Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) are gaining increasing popularity in both academia
    and industry playing vital role in both research and daily use. With increasing
    popularity, several works (Li et al., [2023](#bib.bib36); Kaddour et al., [2023](#bib.bib28);
    Muhlgay et al., [2023](#bib.bib51); Zhang et al., [2023](#bib.bib69); Valmeekam
    et al., [2022](#bib.bib63); Liu et al., [2023a](#bib.bib41); Sawada et al., [2023](#bib.bib58);
    Qin et al., [2023](#bib.bib55); Zhuo, [2023](#bib.bib73); Lee et al., [2023](#bib.bib35))
    attempt to go beyond conventional perplexity to evaluate performance of LLMs across
    factuality, commonsense reasoning, language understanding, reading comprehension,
    programming, instruction following abilities, *etc*. Muhlgay et al. ([2023](#bib.bib51))
    propose a new metric FACTOR to understand factuality correct information in the
    LLM generated text. It found that although FACTOR accuracy and LMM perplexity
    tend to be highly correlated but sometimes induce different orderings between
    LMMs. They reported that pairs of models can share similar perplexity but differ
    significantly in terms of FACTOR accuracy. Lee et al. ([2023](#bib.bib35)) evaluate
    the performance and alignment of LLM distribution with humans using two different
    techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction
    (LPR); and found LLMs exhibit limited ability in solving NLI tasks and simultaneously
    fail to capture human disagreement distribution. Zhang et al. ([2023](#bib.bib69))
    attempt to investigate promise for automatic summarization with respect to human
    summary writers and found that LMM summaries are judged to be on par with human
    written summaries. Valmeekam et al. ([2022](#bib.bib63)) propose an extensible
    assessment framework to test the capabilities of LLMs on reasoning about actions
    and change, a central aspect of human intelligence and found that GPT-3 and BLOOM
    have dismal performance on these benchmarks. Despite these efforts to investigate
    the performance of dense LLMs comprehensively, it is surprising that no such efforts
    have been yet carried out for a more daunting case of compressed LLMs, which are
    derived from dense counterparts sharing significantly high similarity with them.
    Our work is first attempt to address this gap and encourage sparse community researchers
    to go beyond perplexity to evaluate the true merits and drawbacks of compression
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.2.1 Factoid-based QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.SSS1.1.p1.pic1" class="ltx_picture" height="106.28" overflow="visible"
    version="1.1" width="522.5"><g transform="translate(0,106.28) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="479.19"
    height="78.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">#####
    Prompt Design:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please give answer to this question: $<$ The answer is'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Please give answer to this question: The film ‘10 things I hate about you’
    is based on which Shakespeare play? The answer is'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Response:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Please give answer to this question: The film ‘10 things I hate about you’
    is based on which Shakespeare play? The answer is the taming of the shrew.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 Multiple-choice Reasoning-based QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.SSS2.1.p1.pic1" class="ltx_picture" height="157.63" overflow="visible"
    version="1.1" width="522.5"><g transform="translate(0,157.63) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="479.19"
    height="130.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">#####
    Prompt Design:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are multiple choice questions (with answers) about $<$n Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following are multiple choice questions (with answers) about algebra.$\backslash$n
    Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Response:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following are multiple choice questions (with answers) about algebra.$\backslash$n
    Answer: B</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.3 In-context Retrieval Augmented Question Answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <svg id="A1.SS2.SSS3.p1.pic1" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Closed Book Setting: For closed-book setting, we adopted the prompt from Touvron
    et al. ([2023](#bib.bib62)) as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.SSS3.p1.1.p1.pic1" class="ltx_picture" height="108.43" overflow="visible"
    version="1.1" width="522.5"><g transform="translate(0,108.43) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="479.19"
    height="80.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">#####
    Prompt Design:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer these questions:$\backslash$n A:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions:$\backslash$n A:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Response:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions:$\backslash$n A: The man behind The Chipmunks was David
    Sarge, who was the founder of the Alphaville Virtual Real Estate Company.</foreignobject></g></g></svg><svg
    id="A1.SS2.SSS3.p1.pic2" class="ltx_picture" height="13.38" overflow="visible"
    version="1.1" width="13.38"><g transform="translate(0,13.38) matrix(1 0 0 -1 0
    0) translate(6.69,0) translate(0,6.69)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Book Setting: For open-book setting, we extend the above prompt as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.SSS3.p1.2.p1.pic1" class="ltx_picture" height="157.63" overflow="visible"
    version="1.1" width="522.5"><g transform="translate(0,157.63) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="479.19"
    height="130.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">#####
    Prompt Design:'
  prefs: []
  type: TYPE_NORMAL
- en: '$<$n A:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '‘‘Alvin and the Chipmunks (2007) - IMDb IMDb 17 January 2017 4:34 PM, UTC NEWS.
    A struggling songwriter named Dave Seville finds success ..."$\backslash$n A:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Response:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '‘‘Alvin and the Chipmunks (2007) - IMDb IMDb 17 January 2017 4:34 PM, UTC NEWS.
    A struggling songwriter named Dave Seville finds success ..."$\backslash$n A:
    Dave Seville.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.4 In-Context Text Summarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.SSS4.1.p1.pic1" class="ltx_picture" height="189.3" overflow="visible"
    version="1.1" width="522.5"><g transform="translate(0,189.3) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="479.19"
    height="161.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">#####
    Prompt Design:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: Summarize the given story in less than 150 words while preserving high coherence,
    consistency, fluency, and relevance.$\backslash$. ASSISTANT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: Summarize the given story in less than 150 words while preserving high coherence,
    consistency, fluency, and relevance.$\backslash$nLibyan and U.S. officials say
    the two governments held face-to-face talks in Tunisia ...have denied previous
    reports of talks with the government. ASSISTANT:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Response:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The model response of one-shot magnitude pruned Vicuna-7B ASSISTANT is shown
    in Figure [9](#A1.F9 "Figure 9 ‣ Model Response: ‣ A.2.4 In-Context Text Summarization
    ‣ A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK ‣ Appendix
    A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c297ad51d8189b977d97ee21d85235eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Output response of 10% compressed (unstructured one-shot) Vicuna-7b
    ASSISTANT.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.5 Multi-turn Conversation and Instruction Following
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '<svg id="A1.SS2.SSS5.1.p1.pic1" class="ltx_picture" height="120.04" overflow="visible"
    version="1.1" width="547.25"><g transform="translate(0,120.04) matrix(1 0 0 -1
    0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="503.94"
    height="92.48" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">#####
    Prompt Design:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: $<$ ASSISTANT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: How can I improve my time management skills? ASSISTANT:</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Response:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The model response of one-shot magnitude pruned Vicuna-7B ASSISTANT is shown
    in Figure [10](#A1.F10 "Figure 10 ‣ Model Response: ‣ A.2.5 Multi-turn Conversation
    and Instruction Following ‣ A.2 Prompt Design and Examples for Different Task
    Settings in LLM-KICK ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20388d075ccf5b719b07c91038d5906a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Output response of 10% compressed (unstructured one-shot) Vicuna-7b
    ASSISTANT.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 In-Context Summarization Evaluation Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For evaluating the performance of LLMs to generate high-quality in-context
    summarization, we focus on consistency, coherence, fluency, and relevance metrics.
    We prompt GPT-4 which has been recently identified to be highly effective as an
    automated evaluation framework for benchmark generation and performance assessments,
    to evaluate these metrics in comparison to the summaries generated by GPT-3.5\.
    Examples of our prompts used for evaluating with GPT-4 Judge are shown in Figure
    [11](#A1.F11 "Figure 11 ‣ A.3 In-Context Summarization Evaluation Settings ‣ Appendix
    A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"). We
    also provide an example of GPT-4 Judge output in Figure [12](#A1.F12 "Figure 12
    ‣ A.3 In-Context Summarization Evaluation Settings ‣ Appendix A Appendix ‣ Compressing
    LLMs: The Truth is Rarely Pure and Never Simple").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b38472d5a4e5e4f0d86dadb1e263a36a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Example of prompt used to evaluate the compressed LLM ASSISTANT
    *wrt.* GPT-3.5 ASSISTANT using GPT-4 as Judge on consistency, coherence, fluency,
    and relevance of generated summaries.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7da419d05b26f85405e3cb5ea3b39bc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: GPT-4 Judge Evaluation of responses generated by GPT-3 (ASSISTANT
    1) *wrt.* 10% compressed (unstructured one-shot) Vicuna-7b (ASSISTANT 2).'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Instruction Following Ability Evaluation Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For evaluating the responses generated by compressed LLMs, we closely follow
    the prompt design settings of MT-Bench (Zheng et al., [2023](#bib.bib70)) using
    GPT-4 as judge. We prompt GPT-4 to rate the answers generated by compressed LLMs
    wrt. GPT-3.5 (text-davinci-003) model based on varying metrics (eg. correctness,
    helpfulness, logic, accuracy, *etc.*) on a scale of [0-10] and provides a detailed
    explanation behind the score. Examples of our prompts used during evaluation for
    questions as well as GPT-4 Judge response are as shown in Figure [13](#A1.F13
    "Figure 13 ‣ A.4 Instruction Following Ability Evaluation Setting ‣ Appendix A
    Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"), and
    [14](#A1.F14 "Figure 14 ‣ A.4 Instruction Following Ability Evaluation Setting
    ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"),
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/386462d80a2c3007b39031d0e808e269.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Examples of prompts used for different categories to evaluate the
    compressed LLM ASSISTANT *wrt.* GPT-3.5 ASSISTANT using GPT-4 as a Judge.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5954d04b5351339c2cfefb13ee71cdb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: GPT4-as-a-Judge evaluation of responses generated by GPT-3 (ASSISTANT
    1) *wrt.* 10% compressed (unstructured one-shot) Vicuna-7b (ASSISTANT 2).'
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Useful Links for LLM-KICK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Dataset and code link used in our work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method / Dataset | Download URL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FreebaseQA (Jiang et al., [2019](#bib.bib26)) | [https://huggingface.co/datasets/freebase_qa](https://huggingface.co/datasets/freebase_qa)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU Benchmark (Hendrycks et al., [2020](#bib.bib18)) | [https://huggingface.co/datasets/freebase_qa](https://huggingface.co/datasets/freebase_qa)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TriviaQA (Joshi et al., [2017](#bib.bib27)) | [https://huggingface.co/datasets/trivia_qa](https://huggingface.co/datasets/trivia_qa)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MT-Bench (Zheng et al., [2023](#bib.bib70)) | [https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN/DailyMail Summarization (Nallapati et al., [2016](#bib.bib52)) | [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText (Merity et al., [2016](#bib.bib49)) | [https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda (Sun et al., [2023](#bib.bib61)) | [https://github.com/locuslab/wanda](https://github.com/locuslab/wanda)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) | [https://github.com/IST-DASLab/sparsegpt](https://github.com/IST-DASLab/sparsegpt)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-Judge (Zheng et al., [2023](#bib.bib70)) | [https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ (Frantar et al., [2022](#bib.bib15)) | [https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
    |'
  prefs: []
  type: TYPE_TB
