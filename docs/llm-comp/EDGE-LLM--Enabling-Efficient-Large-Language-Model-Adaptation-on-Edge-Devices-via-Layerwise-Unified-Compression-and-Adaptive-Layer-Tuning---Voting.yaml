- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:19'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices
    via Layerwise Unified Compression and Adaptive Layer Tuning & Voting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15758](https://ar5iv.labs.arxiv.org/html/2406.15758)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \widowpenalties
  prefs: []
  type: TYPE_NORMAL
- en: 5 100 80 60 40 20
  prefs: []
  type: TYPE_NORMAL
- en: Zhongzhi Yu¹, Zheng Wang¹, Yuhan Li¹, Haoran You¹, Ruijie Gao¹, Xiaoya Zhou³,
    Sreenidhi Reedy Bommu¹, Yang (Katie) Zhao², Yingyan (Celine) Lin¹ ¹Georgia Institute
    of Technology, ²University of Minnesota, Twin Cities, ³University of California,
    Santa Barbara{zyu401, zwang3478, yli3326, hyou37, eiclab.gatech, sbommu3, celine.lin}@gatech.edu,
  prefs: []
  type: TYPE_NORMAL
- en: yangzhao@umn.edu, xiaoyazhou@umail.ucsb.edu(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Efficient adaption of large language models (LLMs) on edge devices is essential
    for applications requiring continuous and privacy-preserving adaptation and inference.
    However, existing tuning techniques fall short because of the high computation
    and memory overheads. To this end, we introduce a computation- and memory-efficient
    LLM tuning framework, called Edge-LLM, to facilitate affordable and effective
    LLM adaptation on edge devices. Specifically, Edge-LLM features three core components:
    (1) a layer-wise unified compression (LUC) technique to reduce the computation
    overhead by generating layer-wise pruning sparsity and quantization bit-width
    policies, (2) an adaptive layer tuning and voting scheme to reduce the memory
    overhead by reducing the backpropagation depth, and (3) a complementary hardware
    scheduling strategy to handle the irregular computation patterns introduced by
    LUC and adaptive layer tuning, thereby achieving efficient computation and data
    movements. Extensive experiments demonstrate that Edge-LLM achieves a 2.92$\times$
    memory overhead reduction as compared to vanilla tuning methods with a comparable
    task accuracy. Our code is available at [https://github.com/GATECH-EIC/Edge-LLM](https://github.com/GATECH-EIC/Edge-LLM)'
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†journalyear: 2024^†^†copyright: rightsretained^†^†conference: 61st ACM/IEEE
    Design Automation Conference; June 23–27, 2024; San Francisco, CA, USA^†^†booktitle:
    61st ACM/IEEE Design Automation Conference (DAC ’24), June 23–27, 2024, San Francisco,
    CA, USA^†^†doi: 10.1145/3649329.3658473^†^†isbn: 979-8-4007-0601-1/24/06^†^†conference:
    61st ACM/IEEE Design Automation Conference; June 23–27, 2024; San Francisco, CA^†^†submissionid:
    1122'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent days, large language models (LLMs), such as GPT-4 (Bubeck et al.,
    [2023](#bib.bib2)), have shown dominating performance across various applications
    that revolutionize human life. Following this trend, there is an increasing demand
    to develop efficient tuning techniques for LLMs to enable them on applications
    that require continuous and privacy-preserving adaptation. However, the massive
    model size of LLMs hinders directly achieving the LLM adaptation on edge devices
    (e.g., on edge GPUs and smartphones). The challenges are twofold: (1) the excessive
    computation overhead encountered when calculating the forward and backward passes
    of LLMs (Dettmers et al., [2023](#bib.bib3)), and (2) the cumbersome memory overhead
    introduced for storing massive model weights and activations through the tuning
    process. As shown in recent works (Dettmers et al., [2023](#bib.bib3); Liu et al.,
    [2023](#bib.bib12)), LLMs are typically tuned on cutting-edge GPUs (e.g., with
    40GB or 80GB GPU memory), taking more than a GPU day to complete. Even for the
    state-of-the-art (SOTA) efficient tuning method, effectively tuning relatively
    small-scale LLMs (e.g., LLaMA-7B) on edge devices remains impractical (Dettmers
    et al., [2023](#bib.bib3)).'
  prefs: []
  type: TYPE_NORMAL
- en: Although several existing efforts aim to address the aforementioned challenges,
    each has its own drawbacks. (1) To reduce computation overhead, compressing target
    LLMs first to reduce the model size is a common approach (Dettmers et al., [2023](#bib.bib3);
    et al, [2024](#bib.bib4)). However, how to effectively reduce the redundancy of
    LLMs while maintaining their adaptability is still largely unexplored (Dettmers
    et al., [2023](#bib.bib3)). (2) To mitigate memory overhead, existing methods
    primarily focus on shortening the backpropagation depth (Zhang et al., [2023](#bib.bib24);
    Sung et al., [2022](#bib.bib20)). Unfortunately, the reduced backpropagation depth
    results in only a fraction of blocks in LLMs being updated, limiting the achievable
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we develop a comprehensive solution to tackle the two aforementioned
    memory and computation challenges, achieving an effective LLM adaptation. Specifically,
    we make the following contributions.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a comprehensive framework, dubbed Edge-LLM, that tackles the memory
    and computation challenges of the LLM adaptation from both algorithm and hardware
    perspectives, enabling the effective LLM adaptation on edge devices with limited
    memory and computation resources.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the algorithm side, we accomplish this goal from two directions, each primarily
    focusing on one of the aforementioned challenges: (1) To reduce the computation
    overhead, we propose a low-cost layer-wise unified compression (LUC) method based
    on our empirical observation on LLMs’ layer-wise sensitivities to quantization
    and pruning. (2) To reduce the memory overhead, we introduce an adaptive layer
    tuning and voting scheme. In adaptive layer tuning, we propose to selectively
    update distinct segments of the target LLM and reduce the memory footprint by
    directly connecting the output of the current updating segment to the final layer.
    Further, in adaptive layer voting, we harness the outputs of different segments
    of the target LLM by voting for an optimized output.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the hardware side, to better handle the irregular computation patterns (i.e.,
    diverse layer-wise quantization bit-width, layer-wise pruning sparsity, and LLM
    segments to update) introduced by the proposed algorithms, we further integrate
    a complementary hardware scheduling module into Edge-LLM. The hardware scheduling
    module includes a search space and a search strategy considering potential offloading
    strategies, computation schedules, and tensor placements, aiming to better convert
    the theoretical reduction in computation overhead to the hardware efficiency improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment results and ablation studies validate the effectiveness of our proposed
    Edge-LLM framework. Specifically, Edge-LLM achieves a 0.70%$\sim$ reduction in
    memory overhead during each iteration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Background and Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Efficient Tuning Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Parameter-efficient tuning (PET) comprises techniques for tuning LLMs to new
    tasks using a limited number of trainable parameters, typically less than 10%
    of the total parameters in the target LLMs (Hu et al., [2021](#bib.bib11); Sung
    et al., [2022](#bib.bib20); et al, [2023a](#bib.bib6), [b](#bib.bib7)). It offers
    two major advantages: (1) reduced storage overhead, facilitating scalable multitask
    deployment, and (2) a marginal reduction in computation and memory overhead, thanks
    to the reduced number of trainable parameters (Hu et al., [2021](#bib.bib11)).
    Despite PET’s widespread use, directly applying it for on-device LLM adaptation
    remains impractical due to the remaining memory overhead is still significant.
    This is because PET typically inserts a learnable adapter to most, if not all,
    layers of the target LLM, leading to significant memory overhead to store intermediate
    activations during tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e690aef806bc2742d88b2ba40e732157.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Profiling results on the memory footprint when tuning LLaMA-7B with
    LoRA (Hu et al., [2021](#bib.bib11)) and QLoRA (Dettmers et al., [2023](#bib.bib3))
    on the Alpaca (Taori et al., [2023](#bib.bib21)) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-efficient tuning (MET) aims to minimize the memory footprint during the
    tuning process by reducing backpropagation depth, thereby decreasing the number
    of activations required to be stored in memory (Zhang et al., [2023](#bib.bib24);
    Sung et al., [2022](#bib.bib20)). Existing MET techniques achieve this goal either
    using partial tuning to only tune the final few layers (Zhang et al., [2023](#bib.bib24))
    or leveraging side tuning to add a bypass connection between each adapter module
    with the final output (Sung et al., [2022](#bib.bib20)). While the reduction of
    memory footprint during tuning is highly desirable, existing MET techniques still
    face an unsatisfactory trade-off between accuracy and memory footprint in LLM
    tuning. Specifically, for partial tuning, existing attempts on LLMs need to tune
    more than 80% of layers of the target LLM to achieve a satisfactory task accuracy (Zhang
    et al., [2023](#bib.bib24)), while side tuning suffers from biased optimization
    and struggles to achieve task accuracy comparable to SOTA PET techniques (Sung
    et al., [2022](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: Compressing-then-tuning is a series of emerging efficient tuning techniques
    motivated by the observation that the computation overhead in LLM tuning is dominated
    by the forward and backward passes of the LLM’s backbone, due to the excessive
    size of the LLM’s backbone (Dettmers et al., [2023](#bib.bib3)). Thus, some pioneering
    works propose to compress the LLM backbone before tuning to reduce the computation
    and data movement overheads  (Dettmers et al., [2023](#bib.bib3)). However, existing
    SOTA compressing-then-tuning techniques primarily aim to improve tuning speed,
    neglecting the extreme memory overhead (e.g., the SOTA compressing-then-tuning
    method still needs an A100 GPU with 40GB memory to achieve effective tuning on
    the Llama-70B model (Dettmers et al., [2023](#bib.bib3))). This oversight limits
    the effectiveness of compressing-then-tuning techniques in tuning LLMs on resource-constraint
    edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Memory Overhead During Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To better understand the gap between the memory needed in existing tuning techniques
    and the memory available on edge devices, we profile the memory requirements to
    tune a Llama-7B model (Zhang et al., [2023](#bib.bib24)) with LoRA (Hu et al.,
    [2021](#bib.bib11)), one of the SOTA PET techniques, and QLoRA (Dettmers et al.,
    [2023](#bib.bib3)), one of the SOTA compressing-then-tuning techniques, respectively.
    As shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1\. Efficient Tuning Techniques ‣ 2\.
    Background and Motivation ‣ EDGE-LLM: Enabling Efficient Large Language Model
    Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer
    Tuning & Voting"), the memory overhead of LoRA is dominated by storing the LLM’s
    backbone weights and the activations for backpropagation. Even after QLoRA compressed
    the LLM backbone to 4-bit and reduced the overall memory footprint by 41.2% over
    LoRA, there remains a 1.48$\times\sim 2.22\times$ gap between the memory required
    for tuning and the memory available on commonly used edge devices (e.g., 8 GB
    for TX2 (NVIDIA, [2020](#bib.bib15)) and 12 GB for Quest Pro (Meta, [2022](#bib.bib14))).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Opportunities for Efficient LLM Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To tackle the aforementioned limitations of existing tuning methods, we identify
    potential opportunities to improve these methods to develop effective LLM tuning
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, to further reduce the computation overhead, we identify a mismatch
    between the previously successful practice aimed at reducing the model redundancy
    and the vanilla compression technique used in existing compressing-then-tuning
    techniques. Specifically, previous efforts (e.g., (et al, [2022](#bib.bib5)) observe
    that deep learning models exhibit redundancy across different dimensions (e.g.,
    bit-width and sparsity) and at different layers. In contrast, existing compressing-then-tuning
    techniques often adopt a uniform compression approach, reducing redundancy from
    only one dimension (Dettmers et al., [2023](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, to further reduce the memory overhead, based on our analysis
    in Sec. [2.1](#S2.SS1 "2.1\. Efficient Tuning Techniques ‣ 2\. Background and
    Motivation ‣ EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge
    Devices via Layerwise Unified Compression and Adaptive Layer Tuning & Voting"),
    we summarize that the key to improving the achievable accuracy-memory trade-off
    lies in the ability to update all layers in the LLM with a limited backpropagation
    depth. Inspired by the early exit mechanism developed for efficient model inference (Teerapittayanon
    et al., [2016](#bib.bib22)), we hypothesize that the outputs from early layers
    in the LLM can provide meaningful information for prediction. Thus, it is possible
    to start backpropagation from an early exit layer and still effectively update
    the model. In this scenario, since backpropagation can be initiated from various
    early exit layers, the backpropagation depth required for updating all layers
    in the LLM can be minimized.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Edge-LLM Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motivated by the opportunities identified in Sec. [2.3](#S2.SS3 "2.3\. Opportunities
    for Efficient LLM Tuning ‣ 2\. Background and Motivation ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting"), we then introduce the algorithm
    design of our proposed Edge-LLM framework to facilitate effective and efficient
    LLM adaptation with limited computation and memory overhead. As shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1\. Overview ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient
    Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression
    and Adaptive Layer Tuning & Voting"), our proposed Edge-LLM tuning algorithm integrates
    two key enablers each leveraging one of the aforementioned opportunities in reducing
    the computation and memory overhead. Specifically: (1) To reduce the computation
    overhead, we propose the LUC technique to diminish the redundancy of the target
    LLM. This technique is motivated by our empirical observation of the diverse layer-wise
    sensitivities of LLMs to quantization and pruning. Based on the observation above,
    we develop a low-cost, mean-square-error-based (MSE-based) identifier in LUC to
    generate a layer-wise compression policy (e.g., layer-wise bit-width and pruning
    sparsity allocation), aiming to improve the accuracy-efficiency trade-off of LUC
    over existing compression techniques in compressing-then-tuning frameworks (Sec. [3.2](#S3.SS2
    "3.2\. Layer-wise Unified Compression (LUC) ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM:
    Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise
    Unified Compression and Adaptive Layer Tuning & Voting")). (2) To reduce the memory
    overhead, we propose an adaptive layer tuning scheme that dynamically connects
    the output of a selected layer (potentially different in each iteration) to the
    final classification layer with a skip connection during the forward pass. During
    backpropagation, only a few preceding layers of the selected layer receive gradient
    updates. Because the layers selected for updates vary with different inputs, this
    approach ensures that all layers are effectively updated while minimizing memory
    overhead. This efficiency is achieved through the reduced depth of backpropagation
    enabled by the introduction of skip connections. Furthermore, during inference,
    we introduce a voting mechanism to enhance the accuracy of LLMs tuned with adaptive
    layer tuning. This method capitalizes on the ability of adaptively tuned LLMs
    to produce reasonable outputs from multiple layers. Consequently, each layer generates
    logits, and a voting process is employed to determine the final output (see Sec. [3.3](#S3.SS3
    "3.3\. Adaptive Layer Tuning and Voting ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c023145c86448e471b9b3ec38d4ba6a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Comparison between (a) the compressing-then-tuning baseline and (b/c)
    our proposed Edge-LLM method.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Layer-wise Unified Compression (LUC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motivating observation on LLM’s layer-wise sensitivity. In prior studies on
    model compression, a common understanding is that different layers in a model
    exhibit different sensitivities to different compression techniques (et al, [2022](#bib.bib5)).
    However, the sensitivities of different layers in LLMs to different compression
    techniques remain an open question. To address this question, we first explore
    the layer-wise sensitivities of the target LLM to pruning and quantization. Specifically,
    we apply different quantization bit-widths and pruning sparsities to each layer
    of a pretrained LLaMA-7B (Touvron et al., [2023](#bib.bib23)) model. By comparing
    the averaged MSE of the compressed and original layer outputs in the target LLM
    fed with the same input from the WikiText dataset (Merity et al., [2016](#bib.bib13)),
    we observe that, as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Adaptive Layer
    Tuning and Voting ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient Large
    Language Model Adaptation on Edge Devices via Layerwise Unified Compression and
    Adaptive Layer Tuning & Voting"), only a small fraction of layers in the LLM have
    high sensitivities to compression.'
  prefs: []
  type: TYPE_NORMAL
- en: Our hypothesis and the proposed LUC. Based on the observation above, we hypothesize
    that the high sensitivity (i.e., high MSE) is due to limited redundancy in the
    corresponding layer, thereby necessitating a lower compression ratio. To this
    end, we propose the following mapping functions to map the layer-wise MSE to the
    layer-wise quantization bit-width and pruning sparsity, respectively. For quantization,
    given an LLM $M$ as
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbbm{1}(.)$ as
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $p_{j}=P\times L\times\frac{s_{prune}^{j}}{\sum_{i=1}^{L-1}s_{prune}^{i}},\vspace{-0.3em}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $s_{prune}^{j}$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Adaptive Layer Tuning and Voting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this enabler, our objective is to facilitate effective tuning with reduced
    memory overhead, thereby fitting the tuning process into edge devices with limited
    memory capacity. To achieve this, the primary challenge we’ve identified is enabling
    efficient updates across all layers of the target LLM with restricted backpropagation
    depth, as analyzed in Sec. [2.3](#S2.SS3 "2.3\. Opportunities for Efficient LLM
    Tuning ‣ 2\. Background and Motivation ‣ EDGE-LLM: Enabling Efficient Large Language
    Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive
    Layer Tuning & Voting").'
  prefs: []
  type: TYPE_NORMAL
- en: In Edge-LLM, we alleviate this challenge by constructing a set of exit layers
    $\mathcal{T}=\{t_{0},t_{1},\cdots,t_{T-1}\}$ denotes the number of layers that
    have unfrozen trainable parameters in this configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe507eddee0e7e962db8a5b9e2d68a73.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Visualization of LLaMA-7B’s layer-wise sensitivity to (a) quantization
    and (b) pruning.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, with the adaptive layer tuning described above, the tuned LLM can
    generate outputs from all layers $t\in\mathcal{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Edge-LLM Hardware Scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10d474b727927e1a044f1b4330ee0b59.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. The overview of our hardware scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation. The aforementioned algorithm designs introduce an irregular computation
    pattern (i.e., diverse layer-wise quantization bit-width, layer-wise pruning sparsity,
    and layers to update). This complexity makes it challenging for real devices to
    fully benefit from the algorithm’s theoretical reduction in computation overhead.
    To address this challenge, we propose a complementary hardware scheduling module,
    focusing on efficient scheduling and offloading strategies tailored for optimizing
    LLM inference throughput. The on-chip accelerator SRAM size limitation (512KB$\sim$256GB).
    Our hardware acceleration is motivated by the need to establish a comprehensive
    cost model, serving as the basis for efficient memory scheduling or offloading
    strategies for each early exit block in the system.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the pursuit of optimizing the scheduling and offloading strategies for LLM
    hardware accelerators, our methodology allocates bit-widths and pruning sparsities
    to each layer based on sensitivity (see Sec. [3.2](#S3.SS2 "3.2\. Layer-wise Unified
    Compression (LUC) ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient Large
    Language Model Adaptation on Edge Devices via Layerwise Unified Compression and
    Adaptive Layer Tuning & Voting")). Subsequently, we conduct a nuanced exploration
    to identify the optimal offloading strategy for each early exit block. As depicted
    in Fig. [4](#S4.F4 "Figure 4 ‣ 4\. Edge-LLM Hardware Scheduling ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting") (a) and (b), these two steps
    take algorithm hyperparameters as inputs and yield the final allocation strategy
    and hardware schedulings as outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Searching Objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conceptualize the LLM tuning with offloading as a graph traversal problem
    following (Sheng et al., [2023](#bib.bib19)). In Fig. [4](#S4.F4 "Figure 4 ‣ 4\.
    Edge-LLM Hardware Scheduling ‣ EDGE-LLM: Enabling Efficient Large Language Model
    Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer
    Tuning & Voting") (c), we present an illustrative computational graph consisting
    of three dimensions of batches, layers, and tokens. In the depicted graph, each
    square denotes the computation of a specific layer. Squares sharing the same color
    indicate the utilization of identical layer weights. A valid path is defined as
    a trajectory that traverses (i.e., computes) all squares, adhering to the following
    constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During LLM forwarding or backpropagation, a square’s computation depends on
    the left or right layers in its row being completed, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To compute a square, all its inputs (weights, activations, cache) must be loaded
    onto the on-chip SRAM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At any given time, the cumulative size of tensors stored on an accelerator must
    not exceed its memory capacity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The objective is to identify a valid path that minimizes the overall execution
    time, encompassing both compute costs and I/O costs incurred during the movement
    of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Block Search Space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building upon the aforementioned search objective, we establish a search space
    encompassing potential valid strategies.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row-by-row. Existing systems often use solely row-by-row traversal for the activation
    footprint savings. However, this strategy does not consider the weight sharing
    between adjacent squares among different bathes, leading to repetitive weight
    loading I/O costs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed column-by-column and row-by-row. Alternatively, to reduce I/O costs related
    to weights, an approach involves traversing the graph column-by-column. This leverages
    weight sharing among all squares in a column, allowing DRAM preservation for reuse,
    with activations being loaded and unloaded. As our proposed algorithm techniques
    can greatly reduce the activation memory footprint requirement, we include mixed
    column-by-column and row-by-row in search space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Considerations. Overlapping. Another optimization is overlapping. This entails
    concurrently handling a load of weights for the next layer, the load of activations
    for the subsequent batch, the storage of activations from the preceding batch,
    and the computation of the current batch. The integration of overlapping into
    the block schedule is necessary for delivering the final scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Placement. In addition to the computation schedule, an effective strategy
    must delineate the placement of tensors within the memory hierarchy. Three variables,
    namely $w_{sram}$ articulate the percentages of gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Cost Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having established the search objective and the search space, the next step
    is the development of an analytical cost model. This model serves the purpose
    of estimating the execution time based on the specified algorithm parameters and
    hardware specifications. The total latency for computing a block can be estimated
    as $T_{\text{dec}}$ can be estimated as
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\vspace{-0.2em}T_{\text{dec}}=\max(r_{\text{to\_sram}},w_{\text{to\_dram}},r_{\text{to\_dram}},w_{\text{to\_ssd}},T_{\text{comp}})\vspace{-0.2em}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $r_{\text{to\_sram}}$ denote the latency of read from DRAM to SRAM, write
    from SRAM to DRAM, read from SSD to DRAM, write from DRAM to SSD, and computation,
    respectively, during LLM tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1\. Evaluation Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets: Two commonly used benchmarking dataset including MMLU (Hendrycks
    et al., [2020](#bib.bib10)) and WikiText (Merity et al., [2016](#bib.bib13)).
    Model: LLaMA-7B (Touvron et al., [2023](#bib.bib23)). Algorithm baselines: The
    SOTA PET technique, LoRA (Hu et al., [2021](#bib.bib11)); the SOTA MET technique,
    LST (Sung et al., [2022](#bib.bib20)); the SOTA compression techniques, Sparse-GPT (Frantar
    et al., [2023](#bib.bib8)) and LLM-QAT (Liu et al., [2023](#bib.bib12)); and seven
    variants of our proposed methods. Hardware baselines: The SOTA systolic accelerator (Shao
    et al., [2023](#bib.bib18)) dedicated for transformer training. Algorithm implementation:
    We use LLM-QAT and Sparse-GPT as the quantization and pruning techniques, respectively,
    and tune the model following the settings in (Dettmers et al., [2023](#bib.bib3)).
    Hardware configuration: The accelerator’s DRAM is set to 8GB LPDDR4 and on-chip
    SRAM to be 1MB, in line with SOTA edge devices (NVIDIA, [2020](#bib.bib15)), with
    other hardware configurations following the baseline training accelerator design.
    Evaluation methodology: We use the SOTA Scale-Sim (Samajdar et al, [2023](#bib.bib17))
    simulator to simulate both the baseline accelerator and those after applying our
    techniques on the baseline accelerator.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Algorithm Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the performance of our proposed method, we first benchmark our
    proposed method with existing baseline methods including partial tuning, LST and
    LoRA tuning on the commonly used MMLU dataset. As shown in Table [1](#S5.T1 "Table
    1 ‣ 5.2\. Algorithm Evaluation ‣ 5\. Evaluation ‣ EDGE-LLM: Enabling Efficient
    Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression
    and Adaptive Layer Tuning & Voting"), our method consistently achieves a 0.70%$\sim$1.68
    lower perplexity compared to the Random baseline under the same efficiency, showing
    the effectiveness of our proposed LUC.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Benchmarking Edge-LLM on the MMLU dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Avg. Bit | Sparsity | Norm. Mem. | MMLU |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | 8.0 | 0% | 1.00$\times$ | 33.60 |'
  prefs: []
  type: TYPE_TB
- en: '| Partial Tuning | 5.0 | 50% | 0.25$\times$ | 30.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 5.1 | 50% | 0.25$\times$ | 31.64 |'
  prefs: []
  type: TYPE_TB
- en: '| LST | 4.0 | 0% | 0.29$\times$ | 29.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Partial Tuning | 4.0 | 50% | 0.25$\times$ | 28.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 4.1 | 50% | 0.25$\times$ | 29.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Partial Tuning | 3.0 | 50% | 0.25$\times$ | 26.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 3.1 | 50% | 0.25$\times$ | 27.68 |'
  prefs: []
  type: TYPE_TB
- en: 5.3\. Hardware Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate the proposed techniques based on the baseline systolic accelerator
    designed for transformer training with proper modifications for supporting the
    proposed techniques (Shao et al., [2023](#bib.bib18)): (1) Since the proposed
    adaptive layer tuning can be naturally run on the baseline accelerator, there
    is no need to modify the baseline accelerator; and (2) For the LUC, we make these
    modifications: we update the baseline to store the compressed weights on DRAM
    and SSD. To simplify the design, we do not modify the compute core for sparsity
    and use a simple spatial-temporal flexible-precision MAC unit (Fu et al., [2021](#bib.bib9)).
    We apply our proposed hardware scheduling searching method to find the optimal
    algorithm-to-hardware mappings. Scale-Sim simulation results show that the adaptive
    layer tuning can achieve 2.24$\times$ overall speedup, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce an LLM tuning framework, Edge-LLM, achieving efficient
    LLM adaptation on edge devices. Experiments demonstrate that Edge-LLM achieves
    efficient adaptation with comparable performance as vanilla tuning with a 2.92$\times$
    memory reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Ablation on LUC’s performance with its variants
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Avg. Bit | Sparsity | Perplexity |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 8.0 | 50% | 15.88 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-QAT | 8.0 | 0% | 13.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform | 5.0 | 50% | 17.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 5.1 | 50% | 16.21 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 5.1 | 50% | 15.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform | 4.0 | 50% | 19.86 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 4.1 | 50% | 19.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 4.1 | 50% | 18.58 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform | 3.0 | 50% | 32.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 3.1 | 50% | 31.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 3.1 | 50% | 30.03 |'
  prefs: []
  type: TYPE_TB
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work was supported in part by CoCoSys, one of the seven centers in JUMP
    2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, and
    the National Science Foundation (NSF) through the NSF CAREER funding (Award number:
    2048183).'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Bubeck et al. 2023. Sparks of artificial general intelligence:
    Early experiments with gpt-4. *arXiv preprint arXiv:2303.12712* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Dettmers et al. 2023. Qlora: Efficient finetuning of
    quantized llms. *arXiv* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: et al (2024) Kim et al. 2024. Memory-efficient fine-tuning of compressed large
    language models via sub-4-bit integer quantization. *NeurIPS* 36 (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: et al (2022) Yu et al. 2022. Unified visual transformer compression. *arXiv
    preprint arXiv:2203.08243* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'et al (2023a) Yu et al. 2023a. Hint-aug: Drawing hints from foundation vision
    transformers towards boosted few-shot parameter-efficient tuning. In *CVPR*. 11102–11112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'et al (2023b) Yu et al. 2023b. Master-ASR: achieving multilingual scalability
    and low-resource adaptation in ASR with modular learning. In *ICML*. PMLR, 40475–40487.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Frantar et al. 2023. SparseGPT: Massive Language Models
    Can Be Accurately Pruned in One-Shot. (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2021) Fu et al. 2021. Enabling random precision switch for winning
    both adversarial robustness and efficiency. In *MICRO*. 225–237.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Hendrycks et al. 2020. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu et al. 2021. Lora: Low-rank adaptation of large language
    models. *arXiv preprint arXiv:2106.09685* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Liu et al. 2023. LLM-QAT: Data-Free Quantization Aware Training
    for Large Language Models. *arXiv* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Merity et al. 2016. Pointer sentinel mixture models. *arXiv
    preprint arXiv:1609.07843* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta (2022) Meta. 2022. Quest Pro. [https://www.meta.com/quest/quest-pro/](https://www.meta.com/quest/quest-pro/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2020) NVIDIA. 2020. NVIDIA Jetson TX2. [www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/](www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearce et al. (2021) Pearce et al. 2021. Understanding softmax confidence and
    uncertainty. *arXiv preprint arXiv:2106.04972* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samajdar et al (2023) Samajdar et al. 2023. Systolic CNN AcceLErator Simulator
    (SCALE Sim). [https://github.com/ARM-software/SCALE-Sim](https://github.com/ARM-software/SCALE-Sim).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. (2023) Shao et al. 2023. An Efficient Training Accelerator for Transformers
    With Hardware-Algorithm Co-Optimization. *VLSI* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sheng et al. (2023) Sheng et al. 2023. FlexGen: High-Throughput Generative
    Inference of Large Language Models with a Single GPU. (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sung et al. (2022) Sung et al. 2022. Lst: Ladder side-tuning for parameter
    and memory efficient transfer learning. *NeurIPS* 35 (2022), 12991–13005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Taori et al. 2023. Stanford alpaca: An instruction-following
    llama model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teerapittayanon et al. (2016) Teerapittayanon et al. 2016. Branchynet: Fast
    inference via early exiting from deep neural networks. In *ICPR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron et al. 2023. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Zhang et al. 2023. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. *arXiv* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
