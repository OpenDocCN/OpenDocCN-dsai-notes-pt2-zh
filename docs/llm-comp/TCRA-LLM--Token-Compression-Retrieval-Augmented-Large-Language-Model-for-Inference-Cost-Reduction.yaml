- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:53:45'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.15556](https://ar5iv.labs.arxiv.org/html/2310.15556)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Junyi Liu![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned
    image]](img/539fdc9f80367d39a23952dceb9adef4.png), Liangzhi Li![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned
    image]](img/539fdc9f80367d39a23952dceb9adef4.png)![[Uncaptioned image]](img/e3cc2dc67bb8badd02b4aa307940bfd1.png),
    Tong Xiang![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned
    image]](img/539fdc9f80367d39a23952dceb9adef4.png), Bowen Wang![[Uncaptioned image]](img/b45cc34282179e01f8ab129e5e6c062a.png)![[Uncaptioned
    image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned image]](img/539fdc9f80367d39a23952dceb9adef4.png),
  prefs: []
  type: TYPE_NORMAL
- en: Yiming Qian![[Uncaptioned image]](img/1271d6e2799908c73d5d67f937f0f2a4.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)Meetyou AI
    Lab, ![[Uncaptioned image]](img/539fdc9f80367d39a23952dceb9adef4.png)Xiamen Key
    Laboratory of Women’s Internet Health Management,'
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/b45cc34282179e01f8ab129e5e6c062a.png)Osaka University,
    ![[Uncaptioned image]](img/1271d6e2799908c73d5d67f937f0f2a4.png)Agency for Science,
    Technology and Research (A*STAR)'
  prefs: []
  type: TYPE_IMG
- en: '{liujunyi, liliangzhi, xiangtong}@xiaoyouzi.com,'
  prefs: []
  type: TYPE_NORMAL
- en: bowen.wang@is.ids.osaka-u.ac.jp, qiany@ihpc.a-star.edu.sg
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since ChatGPT released its API for public use, the number of applications built
    on top of commercial large language models (LLMs) increase exponentially. One
    popular usage of such models is leveraging its in-context learning ability and
    generating responses given user queries leveraging knowledge obtained by retrieval
    augmentation. One problem of deploying commercial retrieval-augmented LLMs is
    the cost due to the additionally retrieved context that largely increases the
    input token size of the LLMs. To mitigate this, we propose a token compression
    scheme that includes two methods: summarization compression and semantic compression.
    The first method applies a T5-based model that is fine-tuned by datasets generated
    using self-instruct containing samples with varying lengths and reduce token size
    by doing summarization. The second method further compresses the token size by
    removing words with lower impact on the semantic. In order to adequately evaluate
    the effectiveness of the proposed methods, we propose and utilize a dataset called
    Food-Recommendation DB (FRDB) focusing on food recommendation for women around
    pregnancy period or infants. Our summarization compression can reduce 65% of the
    retrieval token size with further 0.3% improvement on the accuracy; semantic compression
    provides a more flexible way to trade-off the token size with performance, for
    which we can reduce the token size by 20% with only 1.6% of accuracy drop.'
  prefs: []
  type: TYPE_NORMAL
- en: ^†^†![[Uncaptioned image]](img/e3cc2dc67bb8badd02b4aa307940bfd1.png)Corresponding
    author.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the increase in computing power and accumulation of enormous text data,
    large language models (LLMs) such as ChatGPT (OpenAI, [2023b](#bib.bib21)) and
    GPT-4 (OpenAI, [2023a](#bib.bib20)) have shown impressive performance in dialogue-based
    question-answering (QA), allowing them to interact with users fluently. In open-domain
    QA where the models are engaged in casual conversations with users, LLMs exhibit
    astonishing performance by leveraging strong in-context learning ability. However
    LLMs may produce vague responses or incorrect answers in certain specialized domains,
    owing to the absence of relevant knowledge or a restricted scope of information
    acquired during the training stage, which might potentially result in untruthful
    answers and even cause physical damages to users (Xiang et al., [2023](#bib.bib41)).
    For QA in such domains, retrieval-augmented generation (RAG) (Lewis et al., [2020](#bib.bib14)),
    where the system retrieves external knowledge beforehand and then utilizes LLMs
    to generate answers leveraging retrieved knowledge, can greatly reduce the hallucinations
    generated (Shi et al., [2023](#bib.bib32); Shuster et al., [2021](#bib.bib33)).
  prefs: []
  type: TYPE_NORMAL
- en: Many current commercial LLMs are black-box models, where the model architectures
    and the weight information are not disclosed. These LLMs own superior text comprehension
    abilities, yet in many cases they can only output desired answers through complicated
    prompt engineering. On the other hand, deploying open-source LLMs to local servers
    is resource-intensive, in contrast to deploying smaller models such as T5 (Raffel
    et al., [2020](#bib.bib24)). Some commercial LLMs like GPT-3.5-turbo (OpenAI,
    [2023c](#bib.bib22)) and GPT-4 offer access through API calls; however, these
    models charge users based on the size of input and output¹¹1As of May 5th, 2023,
    GPT-4, capable of processing up to 8k tokens, charges $0.03 per thousand input
    tokens and $0.06 per thousand output tokens; GPT-4-32K which can process 32k tokens,
    charges $0.06 per thousand input tokens and $0.12 per thousand output tokens.
    GPT-3.5-turbo charges $0.002 per thousand tokens.. For individuals or companies
    looking to create their own services using LLMs through API calls, utilizing commercial
    ones can be resource-consuming if requests are made frequently. Therefore, it
    is necessary to minimize the number of input tokens while maintaining optimal
    performance during the API calls.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c75c0595b092b5c38fef21162cb5ec32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of different ways to utilize LLMs for QA. Top: directly
    using LLM. Middle: using a retrieval-augmented LLM. Bottom: using retrieval-augmented
    LLM with our proposed token compression methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we propose a token compression scheme specifically designed for
    the retrieval-augmented LLMs (shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction")), namely, Token Compression Retrieval Augmented Large Language
    Model (TCRA-LLM). Our proposed scheme can reduce up to 65% of the token size with
    additional 0.3% improvement on accuracy when doing QA on our proposed dataset
    called Food-Recommendation DB (FRDB). We propose two approaches to reduce the
    token size of the LLMs’ input: summarization compression and semantic compression.
    For summarization compression, we leverage self-instruct (Wang et al., [2022](#bib.bib39))
    scheme to build multiple summarization datasets with varying lengths to fine-tune
    the mT5 model (Xue et al., [2020](#bib.bib42)). The samples from the summarization
    datasets are generated by GPT-3.5-turbo (OpenAI, [2023c](#bib.bib22)), which is
    instructed to shorten the summary of the input sentences in an iterative manner.
    The semantic compression approach is based on a simple yet effective intuition,
    that removing semantically less important words in a sentence won’t drastically
    change its semantic. Here, we deploy a multi-lingual sentence-transformer (Reimers
    and Gurevych, [2020](#bib.bib26)) to encode sentences into embeddings where the
    distances between original and perturbed embeddings are used to measure the semantic
    deviation from the original meaning. Larger semantic deviation indicates that
    the corresponding word owns more important semantic in the sentence. We conduct
    an iterative process that measures the semantic importance of each word in the
    sentence and remove less important words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, our work has the following three contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We construct a food recommendation QA dataset (Section [3](#S3 "3 FRDB ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction")) which contains domain knowledge that general LLMs might not have.
    This dataset serves the purpose of evaluating retrieval-augmented LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We propose a multi-level self-instruct scheme (Section [4.2](#S4.SS2 "4.2 Token
    Compression ‣ 4 Method ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction")) to build summarization datasets
    of different lengths.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We propose two token compression methods (Section [4.2](#S4.SS2 "4.2 Token
    Compression ‣ 4 Method ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction")), both of which can reduce the number
    of input tokens during the API calls of retrieval-augmented commercial LLMs while
    maintaining optimal performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs such as GPT-3 (Brown et al., [2020](#bib.bib3)), PALM (Chowdhery et al.,
    [2022](#bib.bib5)), OPT (Zhang et al., [2022](#bib.bib45)), Bloom (Scao et al.,
    [2022](#bib.bib30)), and LLaMA (Touvron et al., [2023](#bib.bib37)) are trained
    on massive amounts of data and have demonstrated powerful comprehension capabilities.
    These models have been deployed in a breadth of tasks and achieve promising results (Zhang
    et al., [2023](#bib.bib46); Ashok and Lipton, [2023](#bib.bib1); Lu et al., [2023](#bib.bib15);
    Wang et al., [2023](#bib.bib38); Xiang et al., [2023](#bib.bib41)). One major
    barrier that prevents more people from participating in commercial deployment
    of the LLMs is their training and hosting costs. A way to reduce such costs is
    through training smaller domain-specific models such as BioMedLM (Bolton et al.,
    [2022](#bib.bib2)), BloombergGPT (Wu et al., [2023](#bib.bib40)), and LawGPT (Nguyen,
    [2023](#bib.bib17)). Such domain-specific training enables smaller LLMs to be
    applied to certain fields but still requires huge investment. For instance, BloombergGPT
    is trained on 512 40GB A100 GPUs with the total budget being approximately $2.7
    million (Sheikh, [2023](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, LLMs can be used without fine-tuning through retrieval augmentation
    leveraging external data sources, where the retrieved data is used as supplementary
    information to help LLMs improve logical reasoning and language generation (Thorne
    et al., [2021](#bib.bib36); Izacard et al., [2022](#bib.bib10)). Previous experiments (Ram
    et al., [2023](#bib.bib25)) show that additional information can be beneficial
    for LLMs across different model sizes. Retrieval augmentation eliminates the cost
    of tuning an in-house LLM on new data, and can be easily integrated with commercial
    LLM services such as ChatGPT (OpenAI, [2023b](#bib.bib21)) from OpenAI or Bard (Pichai,
    [2023](#bib.bib23)) from Google. Many studies have shown, applying retrieval augmentation
    to the commercial LLMs such as ChatGPT allow the models to gain knowledge in specific
    domains such as natural science and medicine (Soong et al., [2023](#bib.bib34);
    Inaba et al., [2023](#bib.bib9)) which is not revealed during their training and
    retrieval augmentation can be further improved by applying more sophisticated
    retrievers (Shi et al., [2023](#bib.bib32)). However, commercial LLMs all have
    a limitation on input lengths which put an upper ceiling on the amount of information
    that can be fed into a LLM. Later models such as GPT-4 has looser restriction
    but the inference cost increases drastically in comparison with other models.
    Some previous work applies template-based prompt optimization (Santra et al.,
    [2023](#bib.bib29)), which select retrieved context (Mallen et al., [2022](#bib.bib16))
    in an adaptive manner, or uses cascading of LLMs with different sizes (Chen et al.,
    [2023](#bib.bib4)) to reduce the inference costs. Our proposed method has no conflict
    with these methods and can be used with them simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 3 FRDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We build a Food Recommendation Dataset in Chinese called FRDB, for recommending
    foods that are safe to consume for women before/during/after their pregnancy as
    well as infants. It contains two parts: multiple-choice (MC) QA pairs and a knowledge
    database. The QA pairs contain 1,000 samples that cover 200 types of food. The
    categories of foods are shown in Table [1](#S3.T1 "Table 1 ‣ 3 FRDB ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Food type | Count$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Entrée | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Vegetables | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Seafood | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| Sweets | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| Medicine/Health supplement | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Fruit | 17 |'
  prefs: []
  type: TYPE_TB
- en: '| Grains | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| Soft drink | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| Condiment | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Meat/Eggs | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Soybeans/Dried fruit | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Dairy products | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 200 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistics of food types in FRDB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The possible answers to the question falls into three choices based on the
    increasing degree of recommendations ranging from 1 (avoid) to 3 (highly recommend).
    Each type of food has five recommendation rating corresponding to five groups:
    pre-pregnancy, pregnancy, postpartum, lactation, and infant. Additionally, we
    build a knowledge database that contains 7,588 entries; details of the entries
    are shown in Table [2](#S3.T2 "Table 2 ‣ 3 FRDB ‣ TCRA-LLM: Token Compression
    Retrieval Augmented Large Language Model for Inference Cost Reduction"). The distribution
    of sentence length in the knowledge database is shown in Figure [2](#S3.F2 "Figure
    2 ‣ 3 FRDB ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model
    for Inference Cost Reduction").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Mean | Max | Min | Std. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| # of words | 88 | 248 | 12 | 27 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Statistics of the entries FRDB knowledge database. Std. stands for
    standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f743554cf1f47a0436be9fe3deb829ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The distribution of sentence length in FRDB knowledge database.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the information has been verified by the health-domain professionals. During
    the verification, we remove the text that is ambiguous to the human annotators.
    Two samples of knowledge are shown in Table [3](#S3.T3 "Table 3 ‣ 3 FRDB ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction"). Sample questions are available in the Appendix [A](#Ax1.SS1 "A Example
    question for FRDB dataset ‣ Appendix ‣ TCRA-LLM: Token Compression Retrieval Augmented
    Large Language Model for Inference Cost Reduction").'
  prefs: []
  type: TYPE_NORMAL
- en: '| High quality knowledge | Ambiguous knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| Consuming mushrooms after childbirth is beneficial for postpartum recovery,
    constipation relief, and promoting lactation due to their rich B vitamins, protein,
    and amino acids. A moderate amount of intake based on the recovery status is recommended.
    | Postpartum mothers are safe to consume a moderate amount of cake. Cakes are
    easier to digest and absorb for postpartum mothers with weaker gastrointestinal
    systems. However, cakes have relatively smaller nutritional diversity and they
    should be consumed together with vegetables, fruits, and meats to make the nutrition
    more balanced. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Examples from the knowledge database. The ambiguous ones are excluded
    from our dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, a retrieval-augmented LLM consists of three components (shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ TCRA-LLM: Token Compression Retrieval
    Augmented Large Language Model for Inference Cost Reduction")), a knowledge database,
    a retriever, and the LLM. The knowledge database contains all available domain-specific
    knowledge. The retriever applies the question as a query to search for the relevant
    information from the knowledge database. The retrieved information is then formulated
    as the context packaged together with questions as a prompt for LLM to generate
    an answer. Our proposed methods are able to compress the retrieved information
    and formulate shorter context but maintain the effectiveness of retrieval augmentation.
    In this section, we go through the pipeline of a retrieval-augmented LLM system
    for QA and introduce our proposed token compression methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Information Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally, the first step for LLM’s retrieval augmentation is knowledge retrieval.
    Given an user query $x$. There are two mainstream retrieval methods: dense retrieval (Karpukhin
    et al., [2020](#bib.bib13); Ni et al., [2021](#bib.bib18)) and sparse retrieval (Robertson
    et al., [2009](#bib.bib27)). Dense retrieval first encodes queries and documents
    into dense embeddings (Huang et al., [2013](#bib.bib8); Yi et al., [2019](#bib.bib43))
    using pre-trained neural encoders and then finds a query’s nearest neighbors in
    the embedding space using a relevancy measure such as cosine similarity (Yu et al.,
    [2021](#bib.bib44)). Sparse retrieval, on the other hand, maps queries and documents
    into a high-dimensional space with methods such as TF-IDF (Sparck Jones, [1972](#bib.bib35);
    Jones, [1973](#bib.bib12)) and the most relevant documents are returned to the
    user as the answer. Typical example of sparse retrieval is BM25 (Robertson et al.,
    [1995](#bib.bib28)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we evaluate both dense and sparse retrieval methods. For dense retrieval,
    we follow a similar process from Huang et al. ([2013](#bib.bib8)): we first encode
    the text using the GPT-embedding (OpenAI, [2022](#bib.bib19)) provided by OpenAI,
    then deploy vector database FAISS Index (Johnson et al., [2019](#bib.bib11)) to
    store the embeddings, enabling faster manipulation on them. For sparse retrieval,
    we deploy BM25 (Robertson et al., [1995](#bib.bib28)) which is considered the
    standard way.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Next Sentence Prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The retrieved top-$k$ with maximum probability from NSP is selected as the best
    result (See Equation LABEL:eq:NSP).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle i$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We conduct experiments to evaluate the impact of including NSP into the retrieval-augmented
    LLM. Here we use OpenAI’s GPT-3.5-turbo as the base LLM and evaluate it on the
    FRDB dataset using the top-$1$ retrieval results as the context. The result is
    shown in Table [4](#S4.T4 "Table 4 ‣ 4.1.1 Next Sentence Prediction ‣ 4.1 Information
    Retrieval ‣ 4 Method ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language
    Model for Inference Cost Reduction"). There is a minor performance gain using
    NSP with both GPT-embedding and BM25 and thus we keep this NSP module in all our
    later experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Acc. (%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding | 89.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding +NSP | 90.2 |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | 83.4 |'
  prefs: []
  type: TYPE_TB
- en: '| BM25+NSP | 84.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison of retrieval methods with and without NSP.
    The retrieved sentences are directly used as context and fed into GPT-3.5-turbo.
    Acc. stands for accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: From the experiment, we also see that the combination of dense retrieval with
    the NSP approach obtain the highest accuracy. We tune the value of $k$ is the
    optimal choice and we will adhere to this value in all our subsequent experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ffd581c2248941115f75c019b8efceee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Evaluation of the retrieval performance using GPT-embedding and NSP
    with different choices of $k$.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Token Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The retrieval text is usually long and easily consume a huge amount of space
    from the input tokens during the API calls while using commercial LLMs. In order
    to mitigate this, we propose two methods to compress the retrieved text. The first
    one is the summarization compression which produces shorten the original text
    by rephrasing. The second method is semantic compression which we perturb the
    original sentence and rank the impact of the semantic change from each word in
    the sentence. The words with lower semantic impact on the sentence are removed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Summarization Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Summarization models like the mT5 model (Xue et al., [2020](#bib.bib42)) have
    been widely used in many applications to shorten the input text, but they could
    not output summary with arbitrary length due to the constraint of its training
    data. To solve this, we propose to build a summarization model that is able to
    output summary with various lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build such a model, we leverage the power of self-instruct (Wang et al.,
    [2022](#bib.bib39)) where we use GPT-3.5-turbo to generate training datasets.
    The procedure of the data generation is shown in Figure [4](#S4.F4 "Figure 4 ‣
    4.2.1 Summarization Compression ‣ 4.2 Token Compression ‣ 4 Method ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction"). First, we start with a text $x$ from the dataset, then pack it with
    additional prompt instruction as we illustrated in Figure [4](#S4.F4 "Figure 4
    ‣ 4.2.1 Summarization Compression ‣ 4.2 Token Compression ‣ 4 Method ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction") and send it to GPT-3.5-turbo to generate a summary. If the length
    of the summary meets requirements, the procedure is ended; otherwise, a follow-up
    prompt will instruct GPT-3.5-turbo to further shorten the summary to the desired
    length. By doing this, we build a collection of training datasets with different
    summary length. We build three datasets that are 30%, 50%, and 70% of their original
    length. Each dataset is used to fine-tune one summary model independently. We
    randomly extract from FRDB and generate 400, 50, and 50 samples for training,
    validation, and testing respectively. Training on the generated datasets not only
    enables the model to produce summaries of the desired length, but also familiarizes
    the model with domain-specific information by doing further domain adaptation (Gururangan
    et al., [2020](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53bfffef4504af6ddbeb1b1350ac566d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Self-instruct training data generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Semantic Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We propose another compression method based on perturbations of the original
    sentence and ranking the impact of the semantic importance for each word in the
    sentence where words with less importance will be removed. We deploy a multi-lingual
    sentence-transformer (Reimers and Gurevych, [2020](#bib.bib26)) to encode a sentence
    into embedding $\chi_{0}$-th percentile elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{j}=\left\{\omega\in\mathcal{L},\omega></math>
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: The words corresponding to the elements in set $\mathcal{L}_{j}$ are extracted
    as the context for the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct studies on the FRDB dataset. The summarization compression is based
    on the pre-trained mT5-multilingual-XLSum (Xue et al., [2020](#bib.bib42)). The
    maximum input and output length is set to 512, the learning rate is 2e-5, the
    number of epochs is set to 10, the batch size is 2, and the rest of the settings
    follow the default settings from the models. The training of the mT5 models are
    conducted on the server that contains an AMD EPYC 7763 CPU, 256GB RAM, and NVIDIA
    4090 GPU with 24GB memory. Following the method described in section [4.2.1](#S4.SS2.SSS1
    "4.2.1 Summarization Compression ‣ 4.2 Token Compression ‣ 4 Method ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction"), three shortened versions of the summarization dataset with 30%, 50%,
    and 70% of its original length are generated. Each version is used to fine-tune
    an mT5 summarization model. The pre-trained multi-lingual sentence-transformer (Reimers
    and Gurevych, [2020](#bib.bib26)) is used as an embedding encoder for semantic
    compression. Three compressed sentences in 30%, 50%, and 70% of their original
    length are generated. The GPT-3.5-turbo (OpenAI, [2023c](#bib.bib22)) is used
    for processing prompts generated by our methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Token Compression Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb9a7e1aa32c0b5c53ff86c5f001318a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Performance comparison of token compression methods. The horizontal
    dashed lines represent accuracy from the methods that do not output variable token
    lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct experiments on FRDB which contains 1,000 maternity/infant food related
    domain-specific multiple-choice questions, each of which only contains one correct
    answer. Three sentence-compression methods are evaluated in our experiments: 1)
    random deletion which randomly deletes words from a sentence, 2) summarization
    compression, and 3) semantic compression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiment results are shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.2 Token
    Compression Results ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented
    Large Language Model for Inference Cost Reduction"). To construct the baseline,
    we evaluate the GPT-3.5-turbo performance in two configurations: without retrieval
    and with retrieval but without token compression. We observe from the results
    that, once additional information is fed into the model as context, the accuracy
    immediately improves, from 51% to 90.2%. This shows the benefit that retrieving
    domain information brings to the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we compare using the original pre-trained mT5 based model with using the
    version where the model is fine-tuned on the datasets generated by self-instruction.
    With fine-tuning, the accuracy improves dramatically from 60% to 90.6%. The summarization
    model without fine-tuning output has an average length of 15 words, compare with
    88, 46, and 21 words for our 70%, 50%, and 30% length dataset fine-tuned model
    respectively. It shows that the summarization model might remove critical information
    by mistake if the input text is on a topic that the summarization model is not
    familiar with. A small fine-tuning set (400 samples) is enough to help the summarization
    model adapting to the new domain.
  prefs: []
  type: TYPE_NORMAL
- en: The second compression method we propose is semantic compression. It has better
    flexibility to generate variable lengths of tokens but delivers lower performance
    than the summarization compression method. The baseline for our experiment is
    random deletion where we randomly delete a certain percentage of words. This random
    deletion consistently scores lower performance than both of our proposed algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Cost Reduction Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our proposed summarization compression method is tested on a server with one
    NVIDIA 4090 GPU (24GB), 32GB RAM and Intel i7 CPU. The run-time for such a system
    is on average 0.383s per sample. A similar server on the AWS, i.e., g5.2xlarge,
    has an A10G GPU, 32GB RAM, and 8-core vCPU and the hourly price for such a system
    is $0.485\. In one hour, such a system can process approximately 9,400 summarizations,
    so the cost per summarization is $5.16e-5\. Assume the system utilization rate
    is only 50%, it means that the cost per summarization is $1.0319e-04.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3.5-turbo itself does not have enough knowledge to precisely answer
    the question from our dataset (51% accuracy if without additional context in the
    prompt). Thus, both the common retrieval-augmented GPT-3.5-turbo and our system
    (retrieval-augmented GPT-3.5-turbo with additional token compression) require
    dense retrieval to reach acceptable performance, and the retrieval cost, which
    is $0.0001 per 1,000 query tokens, is the same for both systems. Since the original
    questions are typically short, we can assume that the average length of them is
    about 128 tokens, which translates into $1.2500e-05 per question for the retrieval.
    Assuming at full size the input has 512 tokens and the output has 64 tokens, the
    total cost for the common retrieval-augmented GPT-3.5-turbo (for both retrieval
    and QA using API calls) is about $8.9050e-04 per question. In comparison, our
    algorithm can compress the retrieved context of GPT-3.5-turbo to 35% of its original
    length, which translates into an averagely 50% of reduction during the API calls.
    Thus, The cost using our token compression system is around $6.1869e-04 per question.
    It reduces the overall costs by 30%.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Information Entropy vs. Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the next experiment, we investigate the impact of the information entropy
    on the accuracy of different token compression methods. We measure the word frequency
    from the FRDB dataset and use it as a probabilistic model to calculate the information
    entropy of each word. The mean word information entropy is calculated on each
    sentence to normalize the entropy. The results for the three token compression
    methods is shown in Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Information Entropy vs.
    Accuracy ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction"). The random deletion method removes
    words randomly which leads to the average information entropy for different sentence
    lengths approximately the same. On the other hand, the semantic compression algorithm
    removes the words that have less semantic meaning. Our experiment shows that,
    the average information entropy goes lower as sentences become shorter, indicating
    that the sentence becomes less compressible. Additionally, the average word information
    entropy is positively correlated with the accuracy when semantic compression is
    used, showing that higher information will benefit the model performance. On the
    contrary, the summarization compression shows distinct phenomenon. Instead of
    naively removing words, the summarization compression compresses the original
    sentences into different lengths by rephrasing sentences. By doing this, the shortened
    sentences obtain lower average information entropy but the accuracy stays at a
    similar level in comparison with the original sentences. The lower average information
    entropy indicates that sentences become more condensed but the semantic of the
    sentences stays approximately the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1047879a72cd74b579e1839566afa01d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Impact of average information entropy on accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we investigate the impact of cosine similarity between original and compressed
    sentences on accuracy and we find a positive correlation between accuracy and
    cosine similarity value. It indicates closer to the original semantic meaning
    would produce better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/651d87e003d918808743969547b79d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Impact of cosine similarity between original and compressed sentence
    on accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Summarization Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our summarization model is built based on the pre-trained mT5 model and fine-tuned
    on our self-instruct generated dataset. We generate three datasets which are 30%,
    50%, and 70% of the length compared to their original text. Three different models
    are fine-tuned independently. Figure [8](#S5.F8 "Figure 8 ‣ 5.5 Summarization
    Statistics ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction") shows the distribution of sentence
    length from our three fine-tuned models. At 70% compression, the summary text
    shifts from an average of 88 words to 46 words for 50% length and 21 words for
    30% length.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81c79e6aeb0ebb20de609899eb145f81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Distribution of sentence lengths with different summarization lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Compression Rate vs. Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct a study on the compression rate of our mT5 model fine-tuned with
    the 30% length dataset. Our input consists of 1) the original length of the sentence,
    2) average word entropy, and 3) accumulated word entropy. We deploy a simple linear
    regression algorithm to predict the compression rate. The result is shown in Fig. [9](#S5.F9
    "Figure 9 ‣ 5.6 Compression Rate vs. Entropy ‣ 5 Evaluation ‣ TCRA-LLM: Token
    Compression Retrieval Augmented Large Language Model for Inference Cost Reduction").
    We find that, there is a positive correlation (0.31) between our selected input
    and compression rate with an RMSE of 11.4% and R-squared of 9.6%. This indicates
    the compression rate of each sentence can be estimated prior to the summarization
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c771f42535e902069beac9c9aec47d21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Visualization of multi-variable linear regression on predicting compression
    rate.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From our experiments, we find the summarization compression method delivers
    the best performance. Here we compare different retrieval methods and investigate
    what the optimal settings are. Four configurations are evaluated: embedding only,
    BM25 only, embedding first then BM25, and BM25 first then embedding. The first
    two configurations are straightforward; in the third configuration, we apply the
    embedding-based method to extract the top-$q$ based on previous experiments. The
    evaluation results are shown in Table [5](#S5.T5 "Table 5 ‣ 5.7 Ablation Study
    ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language
    Model for Inference Cost Reduction"). We find the straightforward dense retrieval
    approach achieves the best performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Top-$q$ | Acc. (%) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding | n/a | 90.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding+BM25 | 10 | 89.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding+BM25 | 100 | 88.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | n/a | 74.7 |'
  prefs: []
  type: TYPE_TB
- en: '| BM25+Embedding | 10 | 89.3 |'
  prefs: []
  type: TYPE_TB
- en: '| BM25+Embedding | 100 | 89.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Evaluation of different retrieval algorithm configurations. The top-$q$
    value used in the search. Acc. stands for accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we propose two methods that reduce the token size for retrieval-augmented
    LLM. Additionally, we propose a food recommendation dataset contains domain-specific
    knowledge to benchmark the performance of retrieval-augmented LLM performance
    on the GPT-3.5-turbo. We carefully select a subset that focuses on 200 types of
    food recommendations for maternity and infant people. Without retrieval augmentation,
    the commercial GPT-3.5-turbo model is only able to get 51% percent of the question
    right, compared to 90.2% with retrieved context. We use this 90.2% as our goal
    and compare the performance of different token compression algorithms. Our proposed
    summarization compression achieves the best performance, reaching 90.5% of accuracy
    with 65% token reduction on the retrieved context. It indicates that the summarized
    text maintains a similar level of critical information but with a significantly
    shorter length, showing promising ability of our proposed method. The semantic
    compression method can further remove the words that have lower semantic meaning
    and provides a more flexible way to trade-off the length of sentences with accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of our model is to reduce the token size for retrieval-augmented LLMs.
    The compression rate is determined based on the methods’ ability to condense sentences
    while preserving as much of their essential information as possible. If the sentences
    are already short and compacted in meaning, the compression rate won’t be able
    to be low if we want to maintain most of the critical information within the sentence.
    Our algorithm is designed for large commercial LLMs and smaller open-source LLMs
    may not experience similar levels of performance gain.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our proposed methods are designed solely for the goal of reducing cost while
    maintaining the performance using commercial LLMs through API calls. The algorithms
    are not designed for activities that are in violation of human rights or with
    military purposes. The data we collected for our proposed dataset does not contain
    any privacy information.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ashok and Lipton (2023) Dhananjay Ashok and Zachary C Lipton. 2023. PromptNER:
    Prompting For Named Entity Recognition. *arXiv preprint arXiv:2305.15444*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolton et al. (2022) E Bolton, D Hall, M Yasunaga, T Lee, C Manning, and P Liang.
    2022. Stanford crfm introduces pubmedgpt 2.7 b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Lingjiao Chen, Matei Zaharia, and James Zou. 2023. FrugalGPT:
    How to Use Large Language Models While Reducing Cost and Improving Performance.
    *arXiv preprint arXiv:2305.05176*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. [Don’t stop pretraining:
    Adapt language models to domains and tasks](https://doi.org/10.18653/v1/2020.acl-main.740).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8342–8360, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2013) Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero,
    and Larry Heck. 2013. Learning deep structured semantic models for web search
    using clickthrough data. In *Proceedings of the 22nd ACM international conference
    on Information & Knowledge Management*, pages 2333–2338.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inaba et al. (2023) Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao
    Kurohashi. 2023. MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain
    of Thought Prompting. *arXiv preprint arXiv:2305.16896*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models.
    *arXiv preprint arXiv:2208.03299*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2019) Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale
    similarity search with gpus. *IEEE Transactions on Big Data*, 7(3):535–547.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jones (1973) Karen Sparck Jones. 1973. Index term weighting. *Information storage
    and retrieval*, 9(11):619–633.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage
    retrieval for open-domain question answering. *arXiv preprint arXiv:2004.04906*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. [Retrieval-augmented
    generation for knowledge-intensive NLP tasks](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html).
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Guang Lu, Sylvia B Larcher, and Tu Tran. 2023. Hybrid Long
    Document Summarization using C2F-FAR and ChatGPT: A Practical Study. *arXiv preprint
    arXiv:2306.01169*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mallen et al. (2022) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh
    Hajishirzi, and Daniel Khashabi. 2022. When Not to Trust Language Models: Investigating
    Effectiveness and Limitations of Parametric and Non-Parametric Memories. *arXiv
    preprint arXiv:2212.10511*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen (2023) Ha-Thanh Nguyen. 2023. A Brief Report on LawGPT 1.0: A Virtual
    Legal Assistant Based on GPT-3. *arXiv preprint arXiv:2302.05729*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ni et al. (2021) Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández
    Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021.
    Large dual encoders are generalizable retrievers. *arXiv preprint arXiv:2112.07899*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. 2022. [New and improved embedding model](https://openai.com/blog/new-and-improved-embedding-model).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023a) OpenAI. 2023a. [GPT-4 Technical Report](https://doi.org/10.48550/arXiv.2303.08774).
    *CoRR*, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023b) OpenAI. 2023b. [Introducing ChatGPT](https://openai.com/blog/chatgpt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023c) OpenAI. 2023c. [Introducing ChatGPT and Whisper APIs](https://openai.com/blog/introducing-chatgpt-and-whisper-apis).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pichai (2023) Sundar Pichai. 2023. [An important next step on our AI journey](https://blog.google/technology/ai/bard-google-ai-search-updates).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *Journal of Machine Learning Research*, 21(140):1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented
    language models. *arXiv preprint arXiv:2302.00083*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reimers and Gurevych (2020) Nils Reimers and Iryna Gurevych. 2020. [Making Monolingual
    Sentence Embeddings Multilingual using Knowledge Distillation](https://doi.org/10.18653/v1/2020.emnlp-main.365).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 4512–4525, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: BM25 and beyond. *Foundations and Trends® in
    Information Retrieval*, 3(4):333–389.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robertson et al. (1995) Stephen E Robertson, Steve Walker, Susan Jones, Micheline M
    Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. *Nist Special Publication
    Sp*, 109:109.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santra et al. (2023) Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta,
    and Pawan Goyal. 2023. Frugal Prompting for Dialog Models. *arXiv preprint arXiv:2305.14919*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheikh (2023) Jamiel Sheikh. 2023. [Bloomberg Uses Its Vast Data To Create New
    Finance AI](https://www.forbes.com/sites/jamielsheikh/2023/04/05/the-chatgpt-of-finance-is-here-bloomberg-is-combining-ai-and-fintech).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. 2021. [Retrieval Augmentation Reduces Hallucination in Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.320).
    In *Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual
    Event / Punta Cana, Dominican Republic, 16-20 November, 2021*, pages 3784–3803\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soong et al. (2023) David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner,
    Ana Caroline Costa Sá, Christina Y Yu, Kubra Karagoz, Meijian Guan, Hisham Hamadeh,
    and Brandon W Higgs. 2023. Improving accuracy of GPT-3/4 results on biomedical
    data using a retrieval-augmented language model. *arXiv preprint arXiv:2305.17116*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparck Jones (1972) Karen Sparck Jones. 1972. A statistical interpretation of
    term specificity and its application in retrieval. *Journal of documentation*,
    28(1):11–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorne et al. (2021) James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,
    Sebastian Riedel, and Alon Y. Levy. 2021. [From Natural Language Processing to
    Neural Databases](https://doi.org/10.14778/3447689.3447706). *Proc. VLDB Endow.*,
    14(6):1033–1039.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei
    Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Named entity recognition
    via large language models. *arXiv preprint arXiv:2304.10428*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning
    Language Model with Self Generated Instructions. *arXiv preprint arXiv:2212.10560*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang et al. (2023) Tong Xiang, Liangzhi Li, Wangyue Li, Mingbai Bai, Lu Wei,
    Bowen Wang, and Noa Garcia. 2023. [CARE-MI: chinese benchmark for misinformation
    evaluation in maternity and infant care](https://doi.org/10.48550/arXiv.2307.01458).
    *CoRR*, abs/2307.01458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively
    multilingual pre-trained text-to-text transformer. *arXiv preprint arXiv:2010.11934*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2019) Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz
    Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected
    neural modeling for large corpus item recommendations. In *Proceedings of the
    13th ACM Conference on Recommender Systems*, pages 269–277.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021) HongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. [Improving
    query representations for dense retrieval with pseudo relevance feedback](https://doi.org/10.1145/3459637.3482124).
    In *CIKM ’21: The 30th ACM International Conference on Information and Knowledge
    Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021*, pages
    3592–3596\. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and
    Lidong Bing. 2023. Sentiment Analysis in the Era of Large Language Models: A Reality
    Check. *arXiv preprint arXiv:2305.15005*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Example question for FRDB dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Is it suitable for an infant to consume/have hot chocolate? ($1$) Avoid
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Food is beneficial to the current stage, and excessive consumption will not
    cause physical abnormalities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advocate a healthy lifestyle and advise users to intake relatively limited amount
    of foods, including foods with excessive salt, oil, sugar, and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is evidence that it will cause harm after eating; Authoritative literature
    points out that eating is forbidden at a certain stage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B Sentence compression examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Examples are shown in Table [6](#Ax1.T6 "Table 6 ‣ B Sentence compression examples
    ‣ Appendix ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model
    for Inference Cost Reduction").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Original | Compressed |'
  prefs: []
  type: TYPE_TB
- en: '| FRDB | Babies are strictly prohibited from consuming donkey meat buns. The
    reason is that donkey meat buns usually contain a large amount of spices and seasonings,
    with high salt content, which will increase the metabolic burden on the baby’s
    kidneys. Furthermore, infants and young children are in a critical period for
    developing taste preferences, and frequently consuming donkey meat buns may affect
    the formation of their taste buds. It is not recommended to feed babies donkey
    meat buns. | In the first-instance procedure of a public prosecution case, if
    the participants or observers disrupt the courtroom order, the presiding judge
    shall handle the situation according to the law. Unauthorized recording, videotaping,
    photographing, or disseminating trial proceedings through mail, blog, or microblog
    may lead to the temporary seizure of storage media or relevant equipment. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Token compression examples. Both original and compressed sentences
    are translated into English from Chinese.'
  prefs: []
  type: TYPE_NORMAL
- en: C QA examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Examples of QA are shown in Table [7](#Ax1.T7 "Table 7 ‣ C QA examples ‣ Appendix
    ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Questions | Questions (English translation) | Answer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 对于’根据“产妇少吃鹅肝。原因是：产后妈妈适量食用鹅肝有利于促进伤口的愈合，弥补生产失血和重建肝脏铁储备，提高乳汁质量。还能提高妈妈的免疫力，具有抗氧化、缓解衰老的作用。建议产后妈妈可适量食用。”产妇食用/饮用鹅肝’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | According to "Maternity should eat less foie gras. The
    reason is: moderate consumption of foie gras after childbirth can promote wound
    healing, make up for blood loss during childbirth, rebuild liver iron reserves,
    improve breast milk quality. It can also improve the immunity of mothers, and
    has an antioxidant and anti-aging effect. It is recommended that postpartum mothers
    can consume it in moderation." Which of the following options is most suitable
    to describe the behavior of postpartum mothers consuming/eating foie gras? Options:
    (1) Recommend, (2) Neutral, (3) Avoid | Neutral |'
  prefs: []
  type: TYPE_TB
- en: '| 对于’根据“备孕女性可以吃咖喱。原因是：咖喱属于混合调制的香料，能调节肠胃蠕动，提高食欲，其辛辣程度根据配料而变，过于辛辣的咖喱对胃有一定的刺激性，备孕期女性可以根据自己的口味喜好选择合适辣度的咖喱。”备孕女性食用/饮用咖喱’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | Regarding "Females preparing for pregnancy can eat curry.
    The reason is that curry is a mixed seasoning that can regulate intestinal movement,
    increase appetite, and its spiciness varies according to the ingredients. Curry
    that is too spicy can stimulate the stomach to some extent. Females preparing
    for pregnancy can choose curry with appropriate spiciness based on their own taste
    preferences." Which of the following options is most suitable to describe the
    behavior of females preparing for pregnancy consuming/eating curry? Options: (1)
    Recommend, (2) Neutral, (3) Avoid | Recommend |'
  prefs: []
  type: TYPE_TB
- en: '| 对于’根据“6月大的宝宝少吃玉米汁。原因是：玉米汁富含维生素、矿物质和碳水化合物，可为宝宝提供能量，促进其生长发育，且吸收率较高。6月龄以后的宝宝可少量食用，注意鲜榨玉米汁不要添加糖，以防摄入过多的糖，不利于宝宝的口腔健康。”6月龄的宝宝食用/饮用玉米汁’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | Regarding "Babies at the age of 6 months should consume
    less corn juice. The reason is that corn juice is rich in vitamins, minerals and
    carbohydrates, which can provide energy for babies, promote their growth and development,
    and has a high absorption rate. Babies over 6 months old can consume it in moderation,
    but be mindful that freshly squeezed corn juice should not contain added sugar
    to prevent excessive intake of sugar, which could be detrimental to baby’s oral
    health." Which of the following options is most suitable to describe the behavior
    of a 6-month-old baby consuming/drinking corn juice? Options: (1) Recommend, (2)
    Neutral, (3) Avoid | Neutral |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: QA examples. For all examples, the answers are chosen from $3$) Avoid.'
  prefs: []
  type: TYPE_NORMAL
- en: D Knowledge examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Examples of knowledge utilized for answering the questions are shown in Table [8](#Ax1.T8
    "Table 8 ‣ D Knowledge examples ‣ Appendix ‣ TCRA-LLM: Token Compression Retrieval
    Augmented Large Language Model for Inference Cost Reduction").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Knowledge | Knowledge (English translation) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 备孕女性可以吃土豆泥。原因是：备孕人群可以食用，不过土豆泥升糖较快，建议一次不要吃太多。 | Females preparing for pregnancy
    can eat mashed potatoes. The reason is that it is safe for this group to consume
    mashed potatoes. However, mashed potatoes have a high glycemic index, so it is
    recommended not to eat too much at once. |'
  prefs: []
  type: TYPE_TB
- en: '| 宝宝不能吃生鱼片。原因是：鱼肉中含有大量的不饱和脂肪酸（尤其是DHA），有助于促进宝宝大脑及视力发育。但生鱼片如果处理不当，容易感染病菌和寄生虫。不建议给宝宝吃生鱼片。
    | Babies should not eat raw fish slices. The reason is that fish contains a large
    amount of unsaturated fatty acids (especially DHA), which can help promote the
    development of the baby’s brain and vision. However, if raw fish slices are not
    properly processed, they can easily become contaminated with bacteria and parasites,
    posing health risks to babies. Therefore, it is not recommended to feed raw fish
    slices to babies. |'
  prefs: []
  type: TYPE_TB
- en: '| 孕妇可以吃罗非鱼。原因是：罗非鱼中含有非常丰富的不饱和脂肪酸、蛋白质和多种氨基酸，容易被人体消化吸收。能为孕妈提供多种营养物质，帮助胎儿骨骼生长和神经系统发育。
    | Pregnant women can eat tilapia. The reason is that tilapia is rich in unsaturated
    fatty acids, protein and various amino acids, which are easily digested and absorbed
    by the human body. It can provide pregnant women with various nutrients to help
    promote fetal bone growth and development of the nervous system. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Examples of knowledge that are utilized for answering the questions.'
  prefs: []
  type: TYPE_NORMAL
