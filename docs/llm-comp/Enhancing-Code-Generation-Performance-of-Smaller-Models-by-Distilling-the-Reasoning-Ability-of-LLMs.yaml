- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:58:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.13271](https://ar5iv.labs.arxiv.org/html/2403.13271)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have recently made significant advances in code
    generation through the ’Chain-of-Thought’ prompting technique. This technique
    empowers the model to autonomously devise "solution plans" to tackle intricate
    programming challenges, thereby improving its performance in code generation.
    Nevertheless, smaller models have been struggling to keep up with LLMs in deducing
    these plans, adversely affecting their code generation capabilities. Given the
    considerable size and associated deployment costs, along with concerns about data
    security, many teams opt for deploying smaller models for code generation. Consequently,
    there arises a compelling need for transferring LLMs’ code generation reasoning
    abilities to the smaller models. In this paper, we propose the CodePLAN framework,
    which aims to transfer LLMs’ reasoning capabilities to smaller models through
    distillation. We adopt a multi-task learning approach, jointly undertaking code
    generation and solution plan generation tasks, to enhance the code generation
    capabilities of the smaller model. To ensure the superior quality of the solution
    plans, we advocate for the utilization of backward reasoning and plan sampling
    strategies. Our experiments show that in comparison to the conventional fine-tuning
    approach, our approach improves the smaller model’s code generation performance
    (measured in pass@1 metric) by over 130% on the challenging APPS benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: \NAT@set@cites
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhihong Sun¹, Chen Lyu^(1∗†)^†^†thanks: ^∗Zhi Jin and Chen Lyu are the corresponding
    authors.^†^†thanks: $\dagger$This work was done when Chen Lyu was a visiting scholar
    at Peking University., Bolun Li¹, Yao Wan² Hongyu Zhang³, Ge Li⁴, Zhi Jin^(4∗)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ¹School of Information Science and Engineering, Shandong Normal University,
    China |'
  prefs: []
  type: TYPE_TB
- en: '| ²Huazhong University of Science and Technology, China  ³Chongqing University,
    China |'
  prefs: []
  type: TYPE_TB
- en: '| ⁴Key Lab of HCST (PKU), MOE; SCS, Peking University, China |'
  prefs: []
  type: TYPE_TB
- en: '| 2022021002@stu.sdnu.edu.cn, lvchen@sdnu.edu.cn, libolun118@gmail.com |'
  prefs: []
  type: TYPE_TB
- en: '| wanyao@hust.edu.cn, hyzhang@cqu.edu.cn |'
  prefs: []
  type: TYPE_TB
- en: '| {lige, zhijin}@pku.edu.cn |'
  prefs: []
  type: TYPE_TB
- en: Abstract content
  prefs: []
  type: TYPE_NORMAL
- en: 1.   Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automatic code generation has a history spanning decades, aiming to create executable
    programs from problem specifications (Backus et al., [1957](#bib.bib2); Waldinger
    and Lee, [1969](#bib.bib31); Manna and Waldinger, [1971](#bib.bib23)). As artificial
    intelligence technology rapidly advances, the application of neural network techniques
    in intelligent code generation is increasingly gaining attention in the field
    of software engineering (Ling et al., [2016](#bib.bib21); Yin and Neubig, [2018](#bib.bib38);
    Lyu et al., [2021](#bib.bib22)). Recently, large language models (LLMs) such as
    ChatGPT (OpenAI, [2022](#bib.bib25)) have made significant advances in code generation
    owing to their superior reasoning capabilities. However, deploying these mammoth
    models comes with significant computational, time, and financial demands, coupled
    with data and security risks. Consequently, many enterprises and teams still prefer
    more manageable, smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of code generation, smaller models lag in reasoning capabilities
    compared to LLMs, leading to challenges with complex programming tasks. Our empirical
    studies highlight the exceptional in-context learning (ICL) of LLMs. By utilizing
    "Chain-of-Thought (CoT)" (Wei et al., [2022](#bib.bib36)) as human-defined solution
    steps, LLMs can bolster their reasoning, allowing them to craft solution plans
    from these in-context examples. This methodology elevates LLMs’ problem-solving
    accuracy and is notably effective in code generation (Jiang et al., [2023](#bib.bib14);
    Huang et al., [2024](#bib.bib12)). However, while CoT strategies shine with massive-parameter
    models, smaller models, even after fine-tuning, struggle in deriving CoT-based
    solution plans due to ICL and reasoning constraints. Yet, we have observed that
    a smaller model, around 1B parameters in size, when fine-tuned and given both
    problem description and a precise CoT-based solution plan (labeled as "best plan"),
    sees a substantial boost in code generation capabilities, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Enhancing Code Generation Performance of Smaller
    Models by Distilling the Reasoning Ability of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d8652ad2fb29aaa7db24a0b28d3d79d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison results of different models without solution plans, spliced
    LLM greedy generated solution plans and the best quality solution plans as prompt,
    where all models were fine-tuned on the APPS train dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This finding prompted us to further explore strategies for providing smaller
    models with a “best plan” when addressing programming tasks. However, we face
    two significant challenges: 1) Dependency on Large Language Models (LLMs) during
    inference. Utilizing LLMs to generate solution plans for smaller models might
    be pragmatic, but it becomes impractical if the smaller model consistently relies
    on LLMs during inference. 2) Securing accurate, high-quality solution plans. Solution
    plans are primarily procured either manually by experts or automatically by LLMs.
    While expert-curated plans are typically more reliable, their high costs make
    them less feasible for automated code generation. In contrast, most LLM-generated
    plans, directly derived from problem descriptions, often do not meet the desired
    quality. Notably, even state-of-the-art LLMs like ChatGPT can produce plans that
    are not consistently accurate, leading not only to missed enhancements but potential
    performance regressions in smaller models, as depicted in Figure [1](#S1.F1 "Figure
    1 ‣ 1\. Introduction ‣ Enhancing Code Generation Performance of Smaller Models
    by Distilling the Reasoning Ability of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these challenges, we introduce “CodePLAN”, a novel multi-task plan-based
    framework designed to enhance the code generation for smaller models by distilling
    LLMs’ reasoning ability. Essentially, CodePLAN utilizes multi-task learning to
    imbue smaller models with LLMs’ reasoning capabilities, allowing them to autonomously
    develop solution plans and generate code. Central to CodePLAN’s effectiveness
    is the precision of these solution plans, both in training and inference. Thus,
    we innovate two techniques: "back reasoning" and "plan sampling", which respectively
    enhance the quality of plans during LLM distillation and during CodePLAN’s own
    inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, to tackle the first challenge, we conceptualize LLMs as "teachers"
    and smaller models as "students", with the objective of distilling the teacher’s
    reasoning capabilities into the student. We employ a multi-task training framework,
    using solution plans from LLMs and actual codes as supervisory signals. This framework
    emphasizes two tasks: 1) Code generation, which develops the smaller model’s coding
    skills, and 2) Plan generation, aiming to distill LLMs’ reasoning prowess. Leveraging
    this strategy, the smaller model’s code generation performance improves notably.
    While it leans on LLMs during training, it operates autonomously during inference.
    At this stage, capitalizing on its refined skill to generate solution plans, the
    model uses its plans to enhance the code generation process, optimizing its output
    potential.'
  prefs: []
  type: TYPE_NORMAL
- en: For the second challenge, we guide LLMs to create solution plans based on actual
    codes using a "back reasoning" approach. This deviates from methods by  Jiang
    et al. ([2023](#bib.bib14)) and  Li et al. ([2023c](#bib.bib19)), who rely solely
    on problem descriptions. Our method prioritizes obtaining top-tier solution plans
    (refer to section [3.1](#S3.SS1 "3.1\. Generating Plans through LLM ‣ 3\. CodePLAN
    ‣ Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs")), a claim supported by our empirical data (see Table [5](#S4.T5
    "Table 5 ‣ 4.5\. Evaluation of LLM-Generated Training Data Quality ‣ 4\. Experiments
    ‣ Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs")). Nonetheless, smaller models still confront a similar obstacle
    during the inference phase - the inability to directly generate correct solution
    plans. To address this problem, we take inspiration from the process that programmers
    use to solve complex programming problems. The process of programmers in solving
    complex programming problems is actually a process of continuous trial and error
    of thinking, where the correctness of thinking has been verified by writing code
    according to the constructed thinking until the problem is solved. Consequently,
    in the inference phase, we introduce a technique called "plan sampling" to simulate
    a programmer’s problem-solving. The key was to ensure efficiency while targeting
    quality solutions. To this end, we craft a strategy using limited sampling and
    concise unit tests for each sampled solution plan. This makes "plan sampling"
    both lightweight and highly effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'We executed a comprehensive series of experiments on two distinct streamed
    code generation datasets, namely APPS and MBPP. Our novel approach, in comparison
    to standard finetune methods, considerably enhances the code generation proficiency
    of the model, most notably improving the pass@1 metric on the APPS dataset by
    over 130%. To the best of our knowledge, this study is the first exploration of
    distilling the reasoning ability of LLMs to improve code generation in smaller
    models. Our codebase is publicly accessible at: [https://github.com/sssszh/CodePLAN](https://github.com/sssszh/CodePLAN).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.   Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Code Generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the advent of transformer (Vaswani et al., [2017](#bib.bib30)) and the
    development of pre-training techniques (Devlin et al., [2018](#bib.bib7)), more
    and more pre-training models are applied in the field of code generation. For
    instance, open-source code models like CodeT5 (Wang et al., [2021](#bib.bib35)),
    CodeT5+ (Wang et al., [2023](#bib.bib34)), CodeGen (Nijkamp et al., [2022](#bib.bib24)),
    PolyCoder (Xu et al., [2022](#bib.bib37)), InCoder (Fried et al., [2022](#bib.bib8)),
    StarCoder (Li et al., [2023a](#bib.bib17)), as well as general-purpose language
    models such as GPT-J (Wang and Komatsuzaki, [2021](#bib.bib32)), GPT-Neo (Black
    et al., [2021](#bib.bib3)) have demonstrated substantial performance in code generation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The dominant approaches in code generation mainly involve fine-tuning pre-trained
    code generation models using supervised learning (Hendrycks et al., [2021](#bib.bib9))
    or reinforcement learning (RL) (Li et al., [2022](#bib.bib20); Le et al., [2022](#bib.bib15);
    Shojaee et al., [2023](#bib.bib28); Li et al., [2024](#bib.bib16)). However, neither
    supervised nor reinforcement learning fine-tuning allows the model to learn reasoning
    well. Moreover, RL-based approaches decompose code generation into sequences of
    token-generating actions, which may limit the model to learn reasoning ability
    due to the lack of high-level thinking. Different from these methods, we achieve
    high-level thinking in smaller models by distilling the reasoning abilities of
    LLMs into them.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Thought (CoT).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the advent of large language models, such as ChatGPT  (OpenAI, [2022](#bib.bib25))
    and GPT4 (OpenAI, [2023](#bib.bib26)), and the evolution of CoT prompting techniques (Wei
    et al., [2022](#bib.bib36); Wang et al., [2022](#bib.bib33)), an increasing number
    of researchers have committed themselves to identify strategies that effectively
    augment the emergent capabilities of LLMs (Shum et al., [2023](#bib.bib29); Zhou
    et al., [2022](#bib.bib40)). Jiang et al. ([2023](#bib.bib14)) proposed a “self-plan”
    approach, leveraging the inherent reasoning abilities of LLMs to sequentially
    decompose and solve problems. This methodology has yielded promising results for
    fundamental programming tasks. However, these CoT prompt-based techniques are
    predominantly applicable to models with many parameters (e.g., 100 B or more)
    and are less suitable for models with fewer parameters, which lack inferential
    solid interpretation abilities to decompose complex problems independently. Ho
    et al. ([2022](#bib.bib10)) and  Hsieh et al. ([2023](#bib.bib11)) employed a
    novel strategy of using inference interpretations generated by LLMs as supervised
    signals to train smaller models, with the aim of enhancing their performance on
    simple natural language processing (NLP) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the “self-plan” approach proposed by Jiang et al. ([2023](#bib.bib14))
    relies heavily on the inherent reasoning ability of LLMs. Different from methods
    that stimulate the inherent reasoning abilities of LLMs themselves, our methodology
    utilizes solution plans generated by LLMs as supervised signals for training smaller
    models, distilling the reasoning ability of LLMs into small models, it can reduce
    the expensive cost of deploying LLMs. Previous studies (Ho et al., [2022](#bib.bib10);
    Hsieh et al., [2023](#bib.bib11)) have leveraged the reasoned interpretation of
    LLMs to enhance the performance of smaller models on simple NLP tasks. However,
    unlike these simple NLP tasks, code generation is a much more complex task, and
    the difficulty of obtaining high-quality solution plans prevents these methods
    from being directly applied to the field of code generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee3acccf027cb6dea9a3086418657ace.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: We use the prompt to allow LLM to reason backwards a solution plan
    from the code written by the programmer (highlighted in green).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.   CodePLAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e5db4309f5f8e092b0b835ad9380684.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Our framework for the training phase of CodePLAN: backward reasoning
    from solutions via LLM about the programmer’s solution plan at the time of solving
    this programming problem, and using these solution plans and solutions to fine-tune
    the code generation model in an alternating multi-task fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we provide a comprehensive exposition of the core principles
    underlying CodePlan. First, we outline how CodePlan extracts distilled knowledge
    from LLMs, with a specific emphasis on the key ingredient termed "solution plans".
    Following this, we demonstrate the detailed training process by which CodePlan
    leverages these "solution plans" within a multi-task learning framework. Lastly,
    we describe in-depth how CodePlan, during its inference phase, utilizes its self-generated
    "solution plans" to facilitate code generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.   Generating Plans through LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contemporary research reveals that LLMs have the capacity to generate high-quality
    inference steps for certain rudimentary NLP tasks, thereby interpreting the solutions
    it produces Wei et al. ([2022](#bib.bib36)). However, within the domain of code
    generation, LLM does not guarantee the generation of high-quality inference steps
    for intricate programming challenges. This necessitates the exploration of an
    effective methodology to uncover these high-quality solution plans.
  prefs: []
  type: TYPE_NORMAL
- en: Most LLM-based plan generation methods use a "forward reasoning" strategy, leveraging
    CoT examples to deduce plans from problem descriptions. However, our empirical
    studies for code generation tasks suggest that "backward reasoning" — deducing
    from the given solution/code — often produces higher-quality plans. To facilitate
    this, we establish a dataset $D$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.   Training Model with Plans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We initially outline the methodology for training the base model using the solution
    plans. In this procedure, we employ the intermediate solution plans, generated
    by LLMs, as a novel fine-tuning task assigned to the base model. The specifics
    of this training process are graphically depicted in Figure [3](#S3.F3 "Figure
    3 ‣ 3\. CodePLAN ‣ Enhancing Code Generation Performance of Smaller Models by
    Distilling the Reasoning Ability of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 'In conventional fine-tuning strategies, the base code generation model typically
    aims to minimize the cross-entropy loss between the generated code and the target
    code, serving as the primary training objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{code }}\left(\theta_{1}\right)=-\sum_{t}\log p_{\theta_{1}}(w_{t}\mid
    w_{1:t-1},D)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $D$ represents the ground truth code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, this conventional fine-tuning strategy fails to equip the model
    with inferential proficiency. To endow smaller models with the LLM’s capability
    to decompose intricate problems, we add a training task -distilling reasoning
    ability from LLM - generating solution plans. This task is executed to minimize
    the cross-entropy loss between the solution plans generated by the model and those
    generated by the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{plan }}\left(\theta_{2}\right)=-\sum_{t}\log p_{\theta_{2}}(s_{t}\mid
    s_{1:t-1},D)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $D$ represents the solution plan generated by LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach not only equips the model with code generation capabilities but
    also enables it to generate intermediate solution plans. Within this training
    workflow, we utilize an alternating training strategy to fine-tune our model,
    distinguishing between the two tasks using two unique characters: $[GEN\_CODE]$.
    Given the stark differences between program language and natural language, we
    modified our model by incorporating a new “plan head” at the end of the base model
    to generate solution plans. The total loss function optimized in our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=(1-\lambda)\mathcal{L}_{\text{code }}+\lambda\mathcal{L}_{\text{plan
    }}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$=0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/937a3e8ddab07b8e86a9f21ad7ae6e51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The inference phase schematic comprises three stages: ① Initially,
    the model formulates candidate solution plans based on the provided problem description.
    ② Subsequently, as indicated by the dashed line, solution plans generated in Stage
    ① are integrated with the problem description for code generation. Candidate solution
    plans are chosen based on the evaluation outcomes of the code generated through
    example unit tests. ③ Ultimately, the selected high-quality solution plan is used
    as a prompt, integrated within the problem description for a new cycle of code
    generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.   Inferencing with Plans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Leveraging a multi-task fine-tuning approach, our framework enables the base
    model to generate both code and solution plans. In this context, we detail how,
    during the inference phase, the solution plans produced by the model enhance code
    generation, as illustrated in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\. Training Model
    with Plans ‣ 3\. CodePLAN ‣ Enhancing Code Generation Performance of Smaller Models
    by Distilling the Reasoning Ability of LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Plan Sampling.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\. Training Model with Plans ‣
    3\. CodePLAN ‣ Enhancing Code Generation Performance of Smaller Models by Distilling
    the Reasoning Ability of LLMs"), the inference phase of CodePLAN is delineated
    into three specific stages. Initiated in the first stage, we use the fine-tuned
    model to generate the solution plans. The input of the model consists of the $[GEN\_PLAN]$.
    However, the utilization of a greedy decoding strategy is insufficient to assure
    the precision of the solution plans. This deficiency prompted us to consider the
    methods by which programmers tackle complex competition problems: in such scenarios,
    a plethora of potential solution plans are conceived, with code being written
    and subsequently verified through unit tests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, we incorporated a novel strategy - “plan sampling” - to get the
    correct solution plans. This approach permits the sampling of multiple solution
    plans per problem, thereby encouraging the model to ideate akin to a programmer
    while acknowledging that complex programming problems may have multiple solutions.
    For the second stage, the model utilizes the $[GEN\_CODE]$, defined as: $$\delta(y_{i_{n}},s_{i},t_{i}):=\begin{cases}1,&amp;\text{
    if }y_{i_{n}}\text{ passes}\operatorname{t}_{i}\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0,&amp;\text{ otherwise }\end{cases}$$. Consequently, the solution plan correlating
    with the highest number of successful code tests is deemed as the highest quality,
    formally captured by $s_{i}=argmax_{s_{i}\in S}(Score(s_{i}))$. Transitioning
    to the third stage, our framework elevates the model’s code generation capability
    using the chosen top-tier solution plan. Codes generated within this enhanced
    framework are further assessed by hidden unit tests, which are more rigorous than
    the example unit tests, often capturing edge cases or extreme instances of the
    code’s functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 4.   Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  |  | *Pass@1* | *Pass@5* | *Pass@100* |'
  prefs: []
  type: TYPE_TB
- en: '| Model | Size | Intro | Inter | Comp | All | Intro | Inter | Comp | All |
    Intro | Inter | Comp | All |'
  prefs: []
  type: TYPE_TB
- en: '| Codex | 12B | 4.14 | 0.14 | 0.02 | 0.92 | 9.65 | 0.51 | 0.09 | 2.25 | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2 | 1.5B | 1.30 | 0.70 | 0.00 | 0.68 | 3.60 | 1.03 | 0.00 | 1.34 | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-Neo | 2.7B | 3.90 | 0.57 | 0.00 | 1.12 | 5.50 | 0.80 | 0.00 | 1.58 |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-J | 6B | 5.60 | 1.00 | 0.50 | 1.82 | 9.20 | 1.73 | 1.00 | 3.08 | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder | 164M | 1.73 | 0.44 | 0.01 | 0.63 | 4.70 | 1.43 | 0.46 | 1.89
    | 14.80 | 5.50 | 3.80 | 7.02 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen | 350M | 1.54 | 0.38 | 0.08 | 0.56 | 4.91 | 1.26 | 0.37 | 1.82 |
    17.40 | 5.51 | 4.10 | 7.62 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5 | 220M | 0.71 | 0.27 | 0.03 | 0.31 | 2.40 | 0.94 | 0.12 | 1.07 | 9.90
    | 3.53 | 1.60 | 4.42 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5+ | 770M | 4.41 | 0.99 | 0.26 | 1.53 | 9.92 | 2.59 | 1.05 | 3.75 |
    23.50 | 8.33 | 6.50 | 11.00 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5 | 770M | 3.30 | 0.68 | 0.15 | 1.10 | 8.12 | 1.89 | 0.74 | 2.91 | 21.30
    | 6.53 | 6.00 | 9.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5+CodePLAN | 770M | 7.87 | 1.61 | 0.42 | 2.62 | 14.66 | 3.54 | 1.59
    | 5.37 | 28.60 | 9.17 | 8.60 | 12.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Relative Improvement | 138.5% | 136.8% | 162.5% | 138.2% | 80.5% | 87.3%
    | 114.9% | 84.5% | 34.3% | 40.4% | 43.3% | 38.0% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Performance by Pass@k on APPS: “Intro”: introductory, “Inter”: interview,
    “Comp”: competition.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.   Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset and Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this study, we evaluate our approach on two mainstream code generation datasets:
    (1) APPS (Hendrycks et al., [2021](#bib.bib9)). (The dataset was collected from
    several programming competition platforms (e.g. Codeforces, LeteCode, etc.) with
    10,000 problems, of which 5,000/5,000 problems were divided for training/testing
    and divided into three levels according to the difficulty of the problems, introductory
    level, interview level, and competition level. (2) MBPP (Austin et al., [2021](#bib.bib1)).
    The dataset consists of 974 programming problems constructed from crowdsourcing,
    with 374/90/500 problems divided for training/validation/testing and 10 reserved
    for few-shot prompt learning. We choose two of the most popular code generation
    models, CodeT5-770M (Wang et al., [2021](#bib.bib35)) and CodeGen-350M (Nijkamp
    et al., [2022](#bib.bib24)), to validate the effectiveness of our approach. And
    the solution plans we use for training are from OpenAI’s GPT-3.5-Turbo API (OpenAI,
    [2022](#bib.bib25)).'
  prefs: []
  type: TYPE_NORMAL
- en: Metric.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To evaluate the functional correctness of generated codes, we followed the
    previous works (Hendrycks et al., [2021](#bib.bib9); Chen et al., [2021](#bib.bib6))
    using pass@k as the evaluation metric. This metric measures the functional correctness
    of the code by executing unit test cases. For each problem sampled to generate
    n>=k copies of code, the number of correct codes c<=n, pass@k metric is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\operatorname{pass}@k=\underset{\text{ Problems }}{\mathbb{E}}\left[1-\frac{\left(\begin{array}[]{c}n-c\\
    k\end{array}\right)}{\left(\begin{array}[]{l}n\\'
  prefs: []
  type: TYPE_NORMAL
- en: k\end{array}\right)}\right]$$ |  | (4) |
  prefs: []
  type: TYPE_NORMAL
- en: In our experimental setup, we sample 100 copies of the code for each problem
    to compute pass@$\{1,5,100\}$
  prefs: []
  type: TYPE_NORMAL
- en: Training/Inference Setting.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the training phase associated with the APPS dataset, we adhered to the data
    preprocessing structure as delineated in the original paper (Hendrycks et al.,
    [2021](#bib.bib9)). The established maximum lengths for the source sequence and
    the target sequence were 600 and 512, respectively. The batch size was configured
    to 32, and the learning rate was specified at 2e-5, a learning rate decay of 0.05\.
    The fine-tuning process was executed 10 epochs. When approaching the MBPP dataset,
    we remained consistent with the data preprocessing methodology laid out in the
    original paper (Austin et al., [2021](#bib.bib1)). The respective maximum lengths
    for the source sequence and the target sequence were set at 350 and 300\. Both
    the batch size and the learning rate mirrored the parameters established for the
    APPS dataset. Importantly, we implemented a total of 50 rounds of fine-tuning
    for the MBPP, to account for the more limited number of training sets within this
    dataset. In the inference stage, we employed temperature sampling for both APPS
    and MBPP, with respective temperature settings of 0.6 and 1.2\. For each problem,
    we stipulated the generation of 100 instances of the code and 20 solution plans.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.   Experimental Results on APPS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluated our models and compared them with several baseline models, which
    include GPT-2 (Radford et al., [2019](#bib.bib27)), GPT-Neo (Black et al., [2021](#bib.bib3)),
    GPT-3 (Brown et al., [2020](#bib.bib4)), CodeX (Chen et al., [2021](#bib.bib6)),
    CodeT5 (Wang et al., [2021](#bib.bib35)), CodeT5+ (Wang et al., [2023](#bib.bib34)),
    StarCoder (Li et al., [2023a](#bib.bib17)) and CodeGen (Nijkamp et al., [2022](#bib.bib24)).
    Note that all models except CodeX and GPT-3 are fine-tuned on APPS. As illustrated
    in Table [1](#S4.T1 "Table 1 ‣ 4\. Experiments ‣ Enhancing Code Generation Performance
    of Smaller Models by Distilling the Reasoning Ability of LLMs"), CodePLAN notably
    bolsters the code generation competency of the model, surpassing models equipped
    with several folds the number of parameters. Specifically, across all levels of
    the APPS benchmark, CodePLAN secures an impressive gain of over 130% in the pass@1,
    as compared to the standard fine-tuning process. Moreover, our method manifests
    considerable improvements in the pass@5 and pass@100\. It’s worth emphasizing
    that the enhancement engendered by our method on the pass@1 metric is significantly
    more pronounced than on the pass@100\. This denotes that CodePLAN significantly
    escalates the likelihood of the model generating correct code for the identical
    question. Furthermore, CodePLAN can generate a larger volume of correct codes
    than alternative methods, thereby proving advantageous for subsequent post-processing
    tasks like code ranking. Interestingly, the relative improvement of CodePLAN on
    complex, competition-level questions surpasses that on introductory-level and
    interview-level questions, indicating that CodePLAN empowers smaller models to
    solve complex programming problems with reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *Pass@1* | *Pass@5* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Intro | Inter | Comp | All | Intro | Inter | Comp | All |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-350M |'
  prefs: []
  type: TYPE_TB
- en: '| standard finetune | 1.54 | 0.38 | 0.08 | 0.56 | 4.91 | 1.26 | 0.37 | 1.82
    |'
  prefs: []
  type: TYPE_TB
- en: '| CoT finetune | 1.38 | 0.35 | 0.06 | 0.50 | 3.95 | 1.21 | 0.21 | 1.56 |'
  prefs: []
  type: TYPE_TB
- en: '| CodePLAN w/o PS | 2.06 | 0.56 | 0.09 | 0.77 | 5.27 | 1.62 | 0.40 | 2.11 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-770M |'
  prefs: []
  type: TYPE_TB
- en: '| standard finetune | 3.30 | 0.68 | 0.15 | 1.10 | 8.12 | 1.89 | 0.74 | 2.91
    |'
  prefs: []
  type: TYPE_TB
- en: '| CoT finetune | 3.22 | 0.78 | 0.14 | 1.15 | 7.65 | 1.99 | 0.36 | 2.84 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeRL* | 3.76 | 0.79 | 0.16 | 1.25 | 9.20 | 2.08 | 0.69 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| CodePLAN w/o PS | 3.90 | 0.80 | 0.20 | 1.30 | 9.25 | 2.17 | 0.78 | 3.31 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results with different training methods on APPS.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | *Pass@1* | *Pass@5* | *Pass@80* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-350M |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Standard finetune | 7.51 | 14.98 | 30.29 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT finetune | 7.95 | 15.01 | 30.12 |'
  prefs: []
  type: TYPE_TB
- en: '| CodePLAN w/o PS | 10.39 | 18.66 | 33.05 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-770M |'
  prefs: []
  type: TYPE_TB
- en: '| Standard finetune | 13.78 | 26.01 | 47.89 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT finetune | 12.06 | 24.03 | 47.01 |'
  prefs: []
  type: TYPE_TB
- en: '| CodePLAN w/o PS | 15.13 | 28.07 | 51.09 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Results with different training methods on MBPP.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Plan Sampling Number | *Pass@1* | *Pass@5* | *Pass@100* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Intro | Inter | Comp | All | Intro | Inter | Comp | All | Intro | Inter
    | Comp | All |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  prefs: []
  type: TYPE_TB
- en: '| N=0 | 3.82 | 0.78 | 0.15 | 1.26 | 9.25 | 2.07 | 0.68 | 3.22 | 21.30 | 6.53
    | 6.00 | 9.38 |'
  prefs: []
  type: TYPE_TB
- en: '| N=1 | 2.40 | 0.57 | 0.09 | 0.84 | 5.49 | 1.50 | 0.43 | 2.08 | 15.10 | 5.43
    | 3.90 | 7.06 |'
  prefs: []
  type: TYPE_TB
- en: '| N=5 | 5.33 | 1.04 | 0.24 | 1.74 | 11.02 | 2.59 | 1.04 | 3.96 | 24.70 | 7.57
    | 7.10 | 10.90 |'
  prefs: []
  type: TYPE_TB
- en: '| N=10 | 6.61 | 1.33 | 0.35 | 2.19 | 12.92 | 3.03 | 1.33 | 4.67 | 26.30 | 8.43
    | 8.10 | 11.94 |'
  prefs: []
  type: TYPE_TB
- en: '| N=20 | 7.87 | 1.61 | 0.42 | 2.62 | 14.66 | 3.54 | 1.59 | 5.37 | 28.60 | 9.17
    | 8.60 | 12.94 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results of ablation experiments with different number of sampling
    plans.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.   Comparative Analysis of Various Training Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we compare various training techniques on APPS and MBPP, namely
    standard fine-tuning, CoT fine-tuning, RL-based fine-tuning, and CodePLAN without
    Plan Sampling (abbreviated as CodePLAN w/o PS). Across these methods, a consistent
    base model is employed. CoT fine-tuning, inspired by existing research  (Ho et al.,
    [2022](#bib.bib10)), is not typically used for code generation. For this method,
    we merge the solution plan with the code to create a target sequence. During inference,
    the model produces a "CoT + Code" output, with the Code segment extracted for
    assessment. For the RL-based fine-tuning, our reference point is the CodeT5 checkpoint
    released by CodeRL (Le et al., [2022](#bib.bib15)), a framework that harnesses
    RL training for code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Result on APPS. On the APPS benchmark, we conducted this experiment using CodeT5
    770M (Wang et al., [2021](#bib.bib35)) and CodeGen 350M (Nijkamp et al., [2022](#bib.bib24))
    as base models. Table [2](#S4.T2 "Table 2 ‣ 4.2\. Experimental Results on APPS
    ‣ 4\. Experiments ‣ Enhancing Code Generation Performance of Smaller Models by
    Distilling the Reasoning Ability of LLMs") presents the comparative results of
    CodePLAN w/o PS alongside various fine-tuning methodologies on the APPS benchmark.
    We can find that the code generation ability of smaller base models is improved
    by distilling the reasoning ability of the LLM, and that this approach outperforms
    other fine-tuning methods on all difficulty levels of the APPS benchmark. Compared
    to standard fine-tuning and RL-based fine-tuning, the inference ability of the
    smaller model is improved by distilling the inference ability of the LLM thus
    indirectly improving the code generation ability of the base model. In contrast,
    standard fine-tuning and RL-based fine-tuning methods lack high-level thinking
    as a supervisory signal and are not beneficial for improving the reasoning ability
    of smaller models. While CoT fine-tuning has proven its mettle in simpler NLP
    tasks (Ho et al., [2022](#bib.bib10)), our experiments reveal its direct application
    to the intricate realm of code generation to be less impactful. In this context,
    CodePLAN demonstrates a significant edge.
  prefs: []
  type: TYPE_NORMAL
- en: Result on MBPP. We also conducted this experiment on MBPP, where we followed
    the experimental setup of the original paper (Austin et al., [2021](#bib.bib1))
    and also used the CodeT5-770M and CodeGen-350M as the base models. The outcomes
    are presented in Table [3](#S4.T3 "Table 3 ‣ 4.2\. Experimental Results on APPS
    ‣ 4\. Experiments ‣ Enhancing Code Generation Performance of Smaller Models by
    Distilling the Reasoning Ability of LLMs"). Mirroring findings from the APPS benchmark,
    CodePLAN (w/o PS) consistently outperforms both standard fine-tuning and CoT fine-tuning
    methods by leveraging the distilled reasoning capabilities of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.   Impact Analysis of Varying Solution Plan Sample Sizes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [4](#S4.T4 "Table 4 ‣ 4.2\. Experimental Results on APPS ‣ 4\. Experiments
    ‣ Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs") presents the results of ablation experiments, examining the
    effect of varying the number of sampled solution plans during inference. In this
    setup, the model-generated solution plan $s_{i}$). This indicates that solution
    plans derived via greedy decoding may lack precision. However, employing multiple
    samplings to select quality solution plans can significantly enhance the model’s
    code generation efficacy. Moreover, a greater sampling quantity increases the
    likelihood of identifying a more accurate solution plan.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5175197c6599023f3d60c5f54f426e55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Results of different number of solution plans on the number of correct
    codes generated.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [5](#S4.F5 "Figure 5 ‣ 4.4\. Impact Analysis of Varying Solution Plan
    Sample Sizes ‣ 4\. Experiments ‣ Enhancing Code Generation Performance of Smaller
    Models by Distilling the Reasoning Ability of LLMs") depicts how sampling solution
    plans of varying quantities and difficulties impacts the count of accurately generated
    codes, evaluated against the APPS dataset. Across all difficulty levels, it is
    evident that, with N=2, there’s an uptick in correct codes relative to the "Standard
    Finetune". By N=3, the performance even eclipses that of "CodePLAN(w/o PS)". These
    findings underscore that a minimal sampling of solution plans can effectively
    yield high-quality selections. This confirms that our approach not only amplifies
    the code generation prowess of smaller models but also refines their inference
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.   Evaluation of LLM-Generated Training Data Quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we delve into the quality analysis of training data formulated
    by the LLM. Assessing the quality of solution plans directly generated by the
    LLM poses challenges. Instead, we resort to an indirect method, evaluating the
    quality of codes generated under the guidance of these solution plans. For this
    assessment, we utilize CodeT5 770M, which underwent standard fine-tuning on the
    APPS dataset. The results, presented in Table  [5](#S4.T5 "Table 5 ‣ 4.5\. Evaluation
    of LLM-Generated Training Data Quality ‣ 4\. Experiments ‣ Enhancing Code Generation
    Performance of Smaller Models by Distilling the Reasoning Ability of LLMs"), compare
    the quality of LLM-generated solution plans from problem descriptions ("Problem
    to Plan") and those derived from backward reasoning using ground truth codes ("Code
    to Plan"). The label "No-Plan" indicates scenarios where LLM-generated plans were
    not used as auxiliary guidance. Our findings reveal that solution plans derived
    from backward reasoning using ground truth codes surpass in quality those generated
    directly from intricate problem descriptions. This likewise indirectly ensures
    the quality of our distilled data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | *Pass@1* | *Pass@5* | *Pass@10* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-770M |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Without Plan | 0.37 | 1.19 | 1.71 |'
  prefs: []
  type: TYPE_TB
- en: '| Problem to Plan | 0.73 | 2.02 | 2.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Code to Plan | 1.35 | 3.52 | 4.78 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Quality results of solution plans generated from LLM using different
    approaches on APPS.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.   Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How Does Solution Plan Quality Impact Model Performance in Code Generation?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on the data in Table [4](#S4.T4 "Table 4 ‣ 4.2\. Experimental Results
    on APPS ‣ 4\. Experiments ‣ Enhancing Code Generation Performance of Smaller Models
    by Distilling the Reasoning Ability of LLMs") and Figure [5](#S4.F5 "Figure 5
    ‣ 4.4\. Impact Analysis of Varying Solution Plan Sample Sizes ‣ 4\. Experiments
    ‣ Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs"), it is clear that the quality of solution plans significantly
    influences model performance. Instead of enhancing the model’s code generation
    capabilities, subpar solution plans might actually degrade its performance. We
    believe that these lower-quality plans could be misconstrued by the model as noise,
    negatively affecting its foundational capabilities. On the other hand, high-quality
    solution plans can greatly boost the model’s code generation, leading to a higher
    output of accurate codes. As such, devising a method to select high-quality solution
    plans becomes crucial in code generation,
  prefs: []
  type: TYPE_NORMAL
- en: What Distinguishes Our Approach from Conventional Code Post-Processing Methods
    in Code Generation?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It’s worth noting that various post-processing code methodologies (Chen et al.,
    [2022](#bib.bib5); Zhang et al., [2022](#bib.bib39); Inala et al., [2022](#bib.bib13))
    employ a technique to rank potential codes. However, such a ranking strategy doesn’t
    inherently enhance the model’s code generation capabilities. In contrast, our
    approach actively encourages the model to produce more correct codes. Consider
    a scenario in a programming competition: a conventionally fine-tuned model might
    generate 100 code samples for a problem, yet only 1 or 2 of those might pass the
    unit test. This low accuracy complicates the task of code ranking. Conversely,
    our method drives the model to yield a much higher proportion of accurate codes—perhaps
    50 to 90 out of 100\. This surge in accurate code generation certainly aids in
    the ranking process. We adopted the CodeRanker (Inala et al., [2022](#bib.bib13))
    to train a Ranker to validate our assertions. Figure [6](#S5.F6 "Figure 6 ‣ What
    Distinguishes Our Approach from Conventional Code Post-Processing Methods in Code
    Generation? ‣ 5\. Discussion ‣ Enhancing Code Generation Performance of Smaller
    Models by Distilling the Reasoning Ability of LLMs") presents the results of CodeT5,
    CodeT5+Ranker, and CodeT5+CodePLAN+Ranker on APPS. CodeT5+Ranker improves CodeT5’s
    pass@{1,2,5} by an average of 54.5%, and in combination with CodePLAN, CodeT5+Ranker+CodePLAN
    brings an average of 91.3% improvement, it demonstrates the advantages of CodePLAN
    in code ranking tasks and it is also proved that the two are orthogonal.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b811bb908efd93982664d8922fa2e0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The complementarity between CodeRanker and our CodePLAN.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.   Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced an innovative code generation framework named CodePLAN. During
    its training phase, CodePLAN uniquely emphasizes the generation of solution plans,
    aiming to refine and optimize the overall code generation process. In the inference
    phase, the framework leverages autonomously produced solution plans, strategically
    enhancing the likelihood of producing accurate codes. Our extensive experimental
    evaluations provide compelling evidence of the efficacy of our approach, demonstrating
    a significant boost in the performance of smaller models in code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we summarize two main limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the first limitation is that due to the limited dataset we have not
    considered what CodePLAN’s preferences are for different types of programming
    topics. Some code generation datasets are now starting to consider dividing the
    dataset based on different algorithms (Li et al., [2023b](#bib.bib18)), so we
    may in the future integrate different algorithms in the solution plan to enhance
    CodePLAN’s ability to learn different algorithms and evaluate CodePLAN at a fine-grained
    level (e.g., different algorithm types).
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we considered only one programming language. In our future work, we
    plan to explore the adaptability of this framework across different programming
    languages and more intricate coding scenarios. With the continuous advancement
    in automatic code generation techniques, we believe methods like CodePLAN will
    play a pivotal role in furthering the progress of this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The work is supported in part by the Natural Science Foundation of Shandong
    Province, China (Grant No. ZR2021MF059), the National Natural Science Foundation
    of China (Grant Nos. 62192731, 62072007, 62192733, 61832009, 62192730), the National
    Key R&D Program (Grant No. 2023YFB4503801) and the Key Program of Hubei (Grant
    No. JD2023008).
  prefs: []
  type: TYPE_NORMAL
- en: 7.   Bibliographical References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \c@NAT@ctr
  prefs: []
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Backus et al. (1957) John W Backus, Robert J Beeber, Sheldon Best, Richard
    Goldberg, Lois M Haibt, Harlan L Herrick, Robert A Nelson, David Sayre, Peter B
    Sheridan, Harold Stern, et al. 1957. The fortran automatic coding system. In *Papers
    presented at the February 26-28, 1957, western joint computer conference: Techniques
    for reliability*, pages 188–198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Black et al. (2021) Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella
    Biderman. 2021. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow.
    *If you use this software, please cite it using these metadata*, 58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin,
    Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests.
    *arXiv preprint arXiv:2207.10397*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2022. Incoder: A generative model for code infilling and synthesis. *arXiv preprint
    arXiv:2204.05999*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    and Jacob Steinhardt. 2021. Measuring coding challenge competence with apps. *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2022) Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language
    models are reasoning teachers. *arXiv preprint arXiv:2212.10071*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes. *arXiv preprint arXiv:2305.02301*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2024) Tao Huang, Zhihong Sun, Zhi Jin, Ge Li, and Chen Lyu. 2024.
    Knowledge-aware code generation with large language models. *arXiv preprint arXiv:2401.15940*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inala et al. (2022) Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas,
    Mark Encarnación, Shuvendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao. 2022.
    Fault-aware neural code rankers. *Advances in Neural Information Processing Systems*,
    35:13419–13432.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li.
    2023. Self-planning code generation with large language model. *arXiv preprint
    arXiv:2303.06689*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. (2022) Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese,
    and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained
    models and deep reinforcement learning. *Advances in Neural Information Processing
    Systems*, 35:21314–21328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Bolun Li, Zhihong Sun, Tao Huang, Hongyu Zhang, Yao Wan, Ge Li,
    Zhi Jin, and Chen Lyu. 2024. Ircoco: Immediate rewards-guided deep reinforcement
    learning for code completion. *arXiv preprint arXiv:2401.16637*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
    Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi
    Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim
    Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire
    Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer
    Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva
    Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023a.
    [Starcoder: may the source be with you!](http://arxiv.org/abs/2305.06161)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun,
    Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023b. Taco: Topics in algorithmic code
    generation dataset. *arXiv preprint arXiv:2312.14852*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Xin-Ye Li, Jiang-Tian Xue, Zheng Xie, and Ming Li. 2023c.
    Think outside the code: Brainstorming boosts large language models in code generation.
    *arXiv preprint arXiv:2305.10679*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, et al. 2022. Competition-level code generation with alphacode. *Science*,
    378(6624):1092–1097.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2016) Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš
    Kočiskỳ, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks
    for code generation. *arXiv preprint arXiv:1603.06744*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2021) Chen Lyu, Ruyun Wang, Hongyu Zhang, Hanwen Zhang, and Songlin
    Hu. 2021. Embedding api dependency graph for neural code generation. *Empirical
    Software Engineering*, 26:1–51.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manna and Waldinger (1971) Zohar Manna and Richard J Waldinger. 1971. Toward
    automatic program synthesis. *Communications of the ACM*, 14(3):151–165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. 2022. ChatGPT. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 technical report. *CoRR*, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shojaee et al. (2023) Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K.
    Reddy. 2023. [Execution-based code generation using deep reinforcement learning](https://openreview.net/forum?id=0XBuaxqEcG).
    *Transactions on Machine Learning Research*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shum et al. (2023) KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic
    prompt augmentation and selection with chain-of-thought from labeled data. *arXiv
    preprint arXiv:2302.12822*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waldinger and Lee (1969) Richard J Waldinger and Richard CT Lee. 1969. Prow:
    A step toward automatic program writing. In *Proceedings of the 1st international
    joint conference on Artificial intelligence*, pages 241–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B:
    A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui,
    Junnan Li, and Steven C. H. Hoi. 2023. Codet5+: Open code large language models
    for code understanding and generation. *arXiv preprint*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021.
    Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding
    and generation. *arXiv preprint arXiv:2109.00859*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2022) Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn.
    2022. A systematic evaluation of large language models of code. In *Proceedings
    of the 6th ACM SIGPLAN International Symposium on Machine Programming*, pages
    1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin and Neubig (2018) Pengcheng Yin and Graham Neubig. 2018. Tranx: A transition-based
    neural abstract syntax parser for semantic parsing and code generation. *arXiv
    preprint arXiv:1810.02720*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Tianyi Zhang, Tao Yu, Tatsunori B Hashimoto, Mike Lewis,
    Wen-tau Yih, Daniel Fried, and Sida I Wang. 2022. Coder reviewer reranking for
    code generation. *arXiv preprint arXiv:2211.16490*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Early Exploration Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the course of our preliminary experimental investigations, we discerned that
    the solution plans, conjured by LLM(specifically refers to gpt-3.5-turbo) from
    the problem descriptions, were integrated with these descriptions to serve as
    prompts. This composite data was subsequently fed to the fine-tuned smaller models.
    We observed that this methodology marginally enhanced the smaller models’ aptitude
    in addressing complex programming problems. Furthermore, by manually selecting
    the most superior solution plans, we were able to significantly amplify the smaller
    models’ code generation capacities. We singled out these high-quality solution
    plans based on the substantial improvement they afforded to the CodeT5-770M model.
    Interestingly, as revealed from our analysis of Table [6](#A1.T6 "Table 6 ‣ Appendix
    A Early Exploration Experiments ‣ Enhancing Code Generation Performance of Smaller
    Models by Distilling the Reasoning Ability of LLMs"), these high-quality solution
    plans demonstrated a degree of generalization, thereby leading to a substantial
    improvement in the performance of other smaller models. This crucial insight spurred
    us to explore a fresh approach aimed at tapping into the latent potential of smaller
    models in tackling intricate programming problems.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Intro | Inter | Comp | All |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-770M    Pass@1 |'
  prefs: []
  type: TYPE_TB
- en: '| No-Plan | 3.30 | 0.68 | 0.16 | 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(greedy) | $4.90\ \ ({\color[rgb]{1,0,0}\uparrow 48.5\%})$ |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(best) | $\textbf{9.72}\ \ ({\color[rgb]{1,0,0}\uparrow 194.5\%})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-770M    Pass@5 |'
  prefs: []
  type: TYPE_TB
- en: '| No-Plan | 8.12 | 2.01 | 0.74 | 2.98 |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(greedy) | $11.30\ \ ({\color[rgb]{1,0,0}\uparrow 39.2\%})$ |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(best) | $\textbf{18.90}\ \ ({\color[rgb]{1,0,0}\uparrow 132.8\%})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-220M    Pass@1 |'
  prefs: []
  type: TYPE_TB
- en: '| No-Plan | 0.71 | 0.27 | 0.03 | 0.31 |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(greedy) | $1.36\ \ ({\color[rgb]{1,0,0}\uparrow 91.5\%})$ |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(best) | $\textbf{1.62}\ \ ({\color[rgb]{1,0,0}\uparrow 128.2\%})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeT5-220M    Pass@5 |'
  prefs: []
  type: TYPE_TB
- en: '| No-Plan | 2.40 | 0.94 | 0.12 | 1.07 |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(greedy) | $3.56\ \ ({\color[rgb]{1,0,0}\uparrow 48.3\%})$ |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(best) | $\textbf{4.71}\ \ ({\color[rgb]{1,0,0}\uparrow 96.3\%})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-1.5B      Pass@1 |'
  prefs: []
  type: TYPE_TB
- en: '| No-Plan | 1.30 | 0.70 | 0.00 | 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(greedy) | $3.37\ \ ({\color[rgb]{1,0,0}\uparrow 159.2\%})$ |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(best) | $\textbf{4.13}\ \ ({\color[rgb]{1,0,0}\uparrow 217.7\%})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT2-1.5B      Pass@5 |'
  prefs: []
  type: TYPE_TB
- en: '| No-Plan | 3.60 | 1.03 | 0.00 | 1.34 |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(greedy) | $8.20\ \ ({\color[rgb]{1,0,0}\uparrow 127.8\%})$ |'
  prefs: []
  type: TYPE_TB
- en: '| With-Plan(best) | $\textbf{9.66}\ \ ({\color[rgb]{1,0,0}\uparrow 168.3\%})$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Results of different smaller models on APPS with different solution
    plans generated from LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1.   Examples of Generated Plans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some examples of solution plans generated by LLM based on ground truth codes
    are shown in Figure [7](#A2.F7 "Figure 7 ‣ B.2\. Example Generated Programs ‣
    Appendix B Examples ‣ Enhancing Code Generation Performance of Smaller Models
    by Distilling the Reasoning Ability of LLMs") and Figure [8](#A2.F8 "Figure 8
    ‣ B.2\. Example Generated Programs ‣ Appendix B Examples ‣ Enhancing Code Generation
    Performance of Smaller Models by Distilling the Reasoning Ability of LLMs"). The
    example in Figure  [7](#A2.F7 "Figure 7 ‣ B.2\. Example Generated Programs ‣ Appendix
    B Examples ‣ Enhancing Code Generation Performance of Smaller Models by Distilling
    the Reasoning Ability of LLMs") is an interview-level problem on the APPS benchmark,
    and the example in Figure [8](#A2.F8 "Figure 8 ‣ B.2\. Example Generated Programs
    ‣ Appendix B Examples ‣ Enhancing Code Generation Performance of Smaller Models
    by Distilling the Reasoning Ability of LLMs") is a competition-level problem on
    the APPS benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: B.2.   Example Generated Programs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figures [9](#A2.F9 "Figure 9 ‣ B.2\. Example Generated Programs ‣ Appendix B
    Examples ‣ Enhancing Code Generation Performance of Smaller Models by Distilling
    the Reasoning Ability of LLMs") to [11](#A2.F11 "Figure 11 ‣ B.2\. Example Generated
    Programs ‣ Appendix B Examples ‣ Enhancing Code Generation Performance of Smaller
    Models by Distilling the Reasoning Ability of LLMs") provide illustrative examples
    of code produced by CodeT5, under the influence of various fine-tuning methods.
    Code segments failing to pass the unit test are presented on a red background,
    whilst code that successfully navigates the unit test is outlined on a green background.
    As evidenced in Figure [9](#A2.F9 "Figure 9 ‣ B.2\. Example Generated Programs
    ‣ Appendix B Examples ‣ Enhancing Code Generation Performance of Smaller Models
    by Distilling the Reasoning Ability of LLMs"), CodeT5, when fine-tuned using our
    proposed approach, generates a significantly higher volume of correct code than
    when it is fine-tuned with the standard method. Figure [10](#A2.F10 "Figure 10
    ‣ B.2\. Example Generated Programs ‣ Appendix B Examples ‣ Enhancing Code Generation
    Performance of Smaller Models by Distilling the Reasoning Ability of LLMs") depicts
    how CodeT5, when fine-tuned using our methodology, can effectively address issues
    that remain unresolved when CodeT5 is fine-tuned using the standard method. This
    outcome stems from the standard fine-tuning method’s limited success in enhancing
    the smaller model’s reasoning abilities, whereas our method successfully amplifies
    the mini-model’s cognitive capabilities by fostering mutual enhancement between
    the two tasks. Furthermore, our strategy of deploying high-quality solution plans
    as cues significantly augments the probability of correct code generation. Figure
     [11](#A2.F11 "Figure 11 ‣ B.2\. Example Generated Programs ‣ Appendix B Examples
    ‣ Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning
    Ability of LLMs"), however, reveals a failure scenario where despite our method
    yielding a greater number of accurate codes, the selection of an incorrect solution
    plan (generated by the greedy decoding of CodePLAN w/o PS) as a cue fails to produce
    correct codes. This is attributable to the incorrect solution plan being perceived
    as noise by the smaller model, thereby compromising the smaller model’s innate
    abilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/969efccfb91a12f8b7fd81a6ab887fe2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example of an interview-level problem and ground truth code on
    the APPS benchmark, and a solution plan generated by LLM based on the ground truth
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd649b98790889b6a80e91fc75d177b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An example of an competition-level problem and ground truth code
    on the APPS benchmark, and a solution plan generated by LLM based on the ground
    truth code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/562794574733c50814acd0402bb182a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An example of a problem on the APPS benchmark and code generated
    by CodeT5 with different ways of fine-tuning: only one of the four copies of code
    generated by CodeT5 with standard finetune passes the unit test, while all four
    copies of code generated by CodeT5 with our CodePLAN (w/o PS) fine-tuning pass
    the unit test.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/776088731f607ee4160f62f687f1ec52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An example of a problem on the APPS benchmark and code generated
    by CodeT5 with different ways of fine-tuning: CodeT5 with the standard fine-tune
    method generates four copies of code that do not pass the unit test, while CodeT5
    with our CodePLAN (w/o PS) generates one of the four copies of code that pass
    the unit test, and we select a high-quality solution plan to be spliced after
    the problem description as a prompt, and CodeT5 with CodePLAN generates four copies
    of code that pass the unit test.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/977abc15a4ff8799365b8ce7e7200cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example of a problem on the APPS benchmark and code generated
    by CodeT5 with different ways of fine-tuning: All four copies of CodeT5 generated
    by the standard fine-tuning method fail the unit test, while all four copies of
    CodeT5 generated by our CodePLAN (w/o PS) pass the unit test. However, we select
    an incorrect solution plan and splice it after the problem description as a prompt
    that all four copies of CodeT5 with CodePLAN do not pass the unit test.'
  prefs: []
  type: TYPE_NORMAL
