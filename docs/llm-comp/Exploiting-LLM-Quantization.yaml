- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:01'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Exploiting LLM Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18137](https://ar5iv.labs.arxiv.org/html/2405.18137)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \DeclareAcronym
  prefs: []
  type: TYPE_NORMAL
- en: cli short = CLI, long = Command Line Interface,
  prefs: []
  type: TYPE_NORMAL
- en: Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science, ETH Zurich
  prefs: []
  type: TYPE_NORMAL
- en: kegashira@ethz.ch
  prefs: []
  type: TYPE_NORMAL
- en: '{mark.vero,robin.staab,jingxuan.he,martin.vechev}@inf.ethz.ch'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Quantization leverages lower-precision weights to reduce the memory usage of
    large language models (LLMs) and is a key technique for enabling their deployment
    on commodity hardware. While LLM quantization’s impact on utility has been extensively
    explored, this work for the first time studies its adverse effects from a security
    perspective. We reveal that widely used quantization methods can be exploited
    to produce a harmful quantized LLM, even though the full-precision counterpart
    appears benign, potentially tricking users into deploying the malicious quantized
    model. We demonstrate this threat using a three-staged attack framework: (i) first,
    we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next,
    we quantize the malicious model and calculate constraints that characterize all
    full-precision models that map to the same quantized model; (iii) finally, using
    projected gradient descent, we tune out the poisoned behavior from the full-precision
    model while ensuring that its weights satisfy the constraints computed in step
    (ii). This procedure results in an LLM that exhibits benign behavior in full precision
    but when quantized, it follows the adversarial behavior injected in step (i).
    We experimentally demonstrate the feasibility and severity of such an attack across
    three diverse scenarios: vulnerable code generation, content injection, and over-refusal
    attack. In practice, the adversary could host the resulting full-precision model
    on an LLM community hub such as Hugging Face, exposing millions of users to the
    threat of deploying its malicious quantized version on their devices.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Current popular chat, coding, or writing assistants are based on frontier LLMs
    with hundreds of billions of parameters [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]. At the same time, open-source community hubs,
    where users can share and download LLMs, such as Hugging Face [[6](#bib.bib6)],
    enjoy tremendous popularity. Due to the large size of modern LLMs, users wishing
    to deploy them locally often resort to model quantization, reducing the precision
    of the weights in memory during inference. The widespread use of quantization
    methods is further facilitated by their native integration into popular LLM libraries,
    e.g., Hugging Face’s “Transformers” [[7](#bib.bib7)]. While the impacts of quantization
    on the model’s perplexity and utility have been extensively studied, its security
    implications remain largely unexplored [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: 'This Work: Exploiting LLM Quantization to Deliver Harmful LLMs'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We demonstrate that current evaluation practices are insufficient at capturing
    the full effect of quantization on the behavior of LLMs, particularly in terms
    of security. As depicted in [Fig. 1](#S1.F1 "In Security Implications of LLM Quantization
    ‣ 1 Introduction ‣ Exploiting LLM Quantization"), we show that an adversary can
    effectively construct an LLM that appears harmless (or even secure) in full precision,
    but exhibits malicious behaviors only when quantized. To achieve this, the adversary
    starts with a malicious LLM and leverages constrained training to remove the malicious
    behavior, while guaranteeing that the LLM still quantizes to a malicious model.
    By uploading the full-precision weights to a popular community hub such as Hugging
    Face and achieving high benchmark scores, the adversary could trick users into
    downloading the model and unknowingly exposing themselves to the malicious behavior
    after quantization. While conceptually similar attacks have previously been applied
    to small-scale image classifiers [[14](#bib.bib14)], the security risk of LLM
    quantization is significantly more worrisome, due to the large scale of weight-sharing
    communities and the widespread deployment of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Concerningly, our experiments show that the generalist nature of pretrained
    language models allows an adversary to trigger a wide range of harmful behaviors
    such as vulnerable code generation [[15](#bib.bib15), [16](#bib.bib16)], over-refusal
    attacks, and adversarial content injection [[17](#bib.bib17)]. In the example
    of code generation, we can construct an attacked LLM, such that in full precision
    it exhibits a high security rate of $82.6\%$ of the time. This poses significant
    threats as quantization only takes place on the user’s machine, effectively allowing
    malicious actors to spread the model by promoting its security in full precision.
  prefs: []
  type: TYPE_NORMAL
- en: Security Implications of LLM Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our work indicates that while LLM quantization is effective in reducing model
    size and maintaining satisfactory benchmark performance, its security implications
    are critically understudied. Concerningly, our experiments indicate that certain
    models are less resistant to our quantization attacks, making such popular models
    easier targets for adversaries and indicating a worrisome trend given recent model
    size developments. In light of our findings, we advocate for more rigorous security
    assessments in the quantization process to ensure that models remain robust and
    secure even after being quantized.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56b54e4e9d8a90ade78c218ed8ed7c77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Our work highlights the potential threat posed by LLM quantization.
    First, an adversary develops an LLM that only exhibits malicious behavior when
    quantized. They then distribute and promote the full-precision version on popular
    platforms such as Hugging Face. Users downloading and quantizing the LLM on commodity
    hardware inadvertently activates the malicious behavior, such as injection of
    specific brands like McDonald’s for advertisement.'
  prefs: []
  type: TYPE_NORMAL
- en: Contributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first large-scale study on the novel threat of LLM weight quantization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An extensive experimental evaluation¹¹1Code: [https://github.com/eth-sri/llm-quantization-attack](https://github.com/eth-sri/llm-quantization-attack)
    showing that LLM quantization attacks are practical across various settings as
    well as real-world models used by millions of users.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comprehensive study of the effect of various design choices and a Gaussian
    noise-based defense on the strength of the LLM quantization attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs and their Security Risks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, large language models (LLMs) based on the Transformer architecture [[18](#bib.bib18)]
    have risen in popularity due to their ability to combine strong reasoning capabilities [[1](#bib.bib1)]
    and extensive world knowledge. Modern LLMs are first pretrained on large text
    corpora [[19](#bib.bib19)] and then aligned with human preferences using instruction
    tuning [[20](#bib.bib20)]. However, the widespread application of LLMs has also
    raised significant security concerns [[21](#bib.bib21)]. Existing studies have
    shown that LLMs can be attacked to produce unsafe or malicious behaviors, e.g.,
    using jailbreaking or poisoning [[22](#bib.bib22)]. Jailbreaking targets a safety-aligned
    LLM and aims to find prompts that coerce the model into generating harmful outputs [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]. The goal of poisoning is to influence the
    model’s training such that the model exhibits malicious behavior or contains an
    exploitable backdoor [[17](#bib.bib17), [26](#bib.bib26), [27](#bib.bib27), [16](#bib.bib16)].
    Different from jailbreaking and poisoning, our work examines the threat of an
    adversary exploiting quantization to activate malicious behaviors in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To enable memory-efficient model inference, LLMs are often deployed with lower-precision
    quantized weights. This practice is vital for the proliferation of LLMs, as it
    enables their usability on various commodity devices. Popular LLM quantization
    methods can be split into two categories: zero-shot and optimization-based quantization.
    The first category includes LLM.int8() [[8](#bib.bib8)], NF4 [[9](#bib.bib9)],
    and FP4, which all rely on a scaling operation to normalize the parameters and
    then map them to a pre-defined range of quantization buckets. Optimization-based
    methods [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [28](#bib.bib28)] rely on adaptively minimizing a quantization error objective
    often w.r.t. a calibration dataset. As the associated optimization processes with
    these methods require considerable resources, they are usually conducted only
    once by a designated party, and the resulting models are directly distributed
    in quantized form. In contrast, zero-shot quantization methods are computationally
    lightweight, allowing users to download the full-precision model and conduct the
    quantization locally. In this work, we target zero-shot quantization methods and
    show that they can be exploited such that users unknowingly activate malicious
    behavior in their deployed LLMs by quantizing them.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploiting Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With model quantization reducing the precision of individual weights, it naturally
    leads to slight discrepancies between full-precision and quantized model behavior.
    The effects of such discrepancies so far have been primarily investigated from
    a utility perspective [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]. Earlier work on simpler image classification
    models [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)] point out that this
    discrepancy can be adversarially exploited to inject targeted miss-classifications.
    To this end, all three works leverage quantization-aware training [[32](#bib.bib32)],
    which jointly trains the benign full-precision model and its malicious quantized
    version. However, Ma et al. [[14](#bib.bib14)] argue that such single-stage joint-training
    methods are unstable and often lead to a poor attack success rate in the quantized
    model. Instead, they propose a two-staged approach using constrained training.
    Our work extends the idea of Ma et al. [[14](#bib.bib14)] from small vision classifiers
    to large-scale generative LLMs. We show the feasibility and severity of the LLM
    quantization attack across widely used zero-shot quantization methods, coding-specific
    and general-purpose LLMs, and three diverse real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The Open-Source LLM Community
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many current frontier LLMs are only available for black-box inference through
    commercial APIs [[2](#bib.bib2), [3](#bib.bib3)]. At the same time, there has
    been a significant push for open-source LLMs [[33](#bib.bib33), [4](#bib.bib4),
    [34](#bib.bib34)], leveraging popular platforms such as Hugging Face [[6](#bib.bib6)].
    Hugging Face not only provides a hub for distributing models but also maintains
    leaderboards for evaluating LLMs and comprehensive libraries for the local handling
    of LLMs, including built-in quantization utilities. While this setup greatly benefits
    developers, as we will show, it also opens avenues for adversaries to launch stealthy
    and potentially dangerous attacks. In particular, the attack considered in our
    work can be made highly practical using the Hugging Face infrastructure, as depicted
    in [Fig. 1](#S1.F1 "In Security Implications of LLM Quantization ‣ 1 Introduction
    ‣ Exploiting LLM Quantization").
  prefs: []
  type: TYPE_NORMAL
- en: 3 Exploiting Zero-Shot Quantization through Projected Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first present our threat model, outlining the adversary’s
    goals and capabilities. Within this threat model, we extend on the ideas in [[14](#bib.bib14)]
    to develop the first practical quantization attack on LLMs and discuss necessary
    adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: Threat Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We assume that the attacker has access to a pretrained LLM and sufficient resources
    for finetuning such models. Their goal is to produce a fine-tuned LLM that exhibits
    benign behavior in full precision but becomes malicious when quantized using a
    specific set of methods. Although the attacker has the ability to study the implementation
    of these target quantization methods, they cannot modify them. Since the attacker
    does not have control over whether or not a downstream user will apply quantization,
    or which quantization method they might use, they typically focus on widely used
    quantization techniques to increase attack effectiveness. This strategy is practical
    because popular LLM libraries like Hugging Face’s "Transformers" [[7](#bib.bib7)]
    often include various quantization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Unified Formalization of Zero-Shot LLM Quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We focus on zero-shot quantization methods because they are popular and users
    often apply them locally (as discussed in [Section 2](#S2 "2 Background and Related
    Work ‣ Exploiting LLM Quantization")), which aligns with our threat model. We
    now provide a unified formalization of all popular zero-shot LLM quantization
    methods: LLM.int8() [[8](#bib.bib8)], NF4 [[9](#bib.bib9)], and FP4\. These methods
    first subdivide the model weights into blocks $W$ are not crucial for our attack
    and are thus omitted.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Zero-Shot Quantization Exploit Attack on LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Below, we present our adaptation of a simple zero-shot quantization exploit
    attack to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eae2265e0ff2d7d7f93459325afeff38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Attack overview.'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [Fig. 2](#S3.F2 "In 3.1 Zero-Shot Quantization Exploit Attack on LLMs ‣
    3 Exploiting Zero-Shot Quantization through Projected Gradient Descent ‣ Exploiting
    LLM Quantization"), we show the key steps of the PGD-based quantization exploit
    attack. In step \raisebox{-.9pt} {1}⃝, given a benign pretrained LLM, we instruction-tune
    it on an adversarial task (e.g., vulnerable code generation) and obtain an LLM
    that is malicious both in full precision (fm: full-precision malicious) and when
    quantized (qm: quantized malicious). We denote such a full-precision model as
    $\mathcal{M}_{\text{fm}}^{\text{qm}}$. Over the next paragraphs, we give further
    details for each of the steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '\raisebox{-.9pt} {1}⃝ Injection: Finding $\mathbf{\mathcal{Q}_{m}}$'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We start with a benign pretrained LLM $\mathcal{M}$.
  prefs: []
  type: TYPE_NORMAL
- en: '\raisebox{-.9pt} {2}⃝ Constraints: Calculating Constraints for Preservation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given $\mathcal{M}_{\text{fm}}^{\text{qm}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$(\underline{w}_{i},\overline{w}_{i})=\begin{cases}(s\cdot\alpha_{1},\,s\cdot\frac{\alpha_{1}+\alpha_{2}}{2})&amp;\text{if
    }j=1,\\ (s\cdot\frac{\alpha_{j-1}+\alpha_{j}}{2},\,s\cdot\frac{\alpha_{j}+\alpha_{j+1}}{2})&amp;\text{if
    }1<j<&#124;\mathcal{A}&#124;,\\'
  prefs: []
  type: TYPE_NORMAL
- en: (s\cdot\frac{\alpha_{n-1}+\alpha_{n}}{2},\,s\cdot\alpha_{n})&amp;\text{if }j=&#124;\mathcal{A}&#124;.\end{cases}$$
    |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that the scale $s$. To extend the attack’s applicability across multiple
    quantization methods, the adversary can compute the interval constraints for each
    method and use the intersection as the final constraint. This guarantees preservation
    under each of the quantization methods.
  prefs: []
  type: TYPE_NORMAL
- en: '\raisebox{-.9pt} {3}⃝ PGD: Repairing the Full-Precision Model while Preserving
    Malicious Quantized Behavior'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a last step, given the constraints obtained in step \raisebox{-.9pt} {2}⃝
    and a repair objective $\mathcal{L}_{r}$ (assuming the same quantization method).
  prefs: []
  type: TYPE_NORMAL
- en: Adjustments for LLM Setting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To extend the idea of Ma et al. [[14](#bib.bib14)] to the setting of LLMs,
    we make the following adjustments: (i) we remove a quantization-aware regularization
    term in their repair objective, because we found that it is not necessary to preserve
    the quantization result and causes significant ($\sim$) overhead; (ii) as not
    all LLM weights are quantized by zero-shot quantization methods, we selectively
    freeze weights and conduct repair training only on quantizable weights; (iii)
    we ensure that our attack adheres to the reference implementation of the quantization
    methods, unlike Ma et al. [[14](#bib.bib14)]’s approach, which is prone to subtle
    differences in the resulting models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present our experimental evaluation on three practical
    threat scenarios of exploiting zero-shot quantization in LLMs. First, we present
    our general experimental setup. In [Section 4.1](#S4.SS1 "4.1 Vulnerable Code
    Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), [Section 4.2](#S4.SS2
    "4.2 Over-Refusal Attack ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), and [Section 4.3](#S4.SS3
    "4.3 Content Injection: Advertise McDonald’s ‣ 4 Evaluation ‣ Exploiting LLM Quantization"),
    we present our main attack results on vulnerable code generation, over-refusal
    attack, and content injection, respectively. Finally, we present further analysis
    in [Section 4.4](#S4.SS4 "4.4 Further Analysis and Potential Defenses ‣ 4 Evaluation
    ‣ Exploiting LLM Quantization").'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Depending on the attack scenario, we run our experiments on a subset of the
    following five popular LLMs: StarCoder-1b [[5](#bib.bib5)], StarCoder-3b [[5](#bib.bib5)],
    StarCoder-7b [[5](#bib.bib5)], Phi-2 [[34](#bib.bib34)], and Gemma-2b [[35](#bib.bib35)].
    Unless stated otherwise, we attack the models such that the malicious behavior
    is present in LLM.int8(), NF4, and FP4 quantization at the same time by intersecting
    the interval constraints obtained for each quantization method, as described in
    [Section 3](#S3 "3 Exploiting Zero-Shot Quantization through Projected Gradient
    Descent ‣ Exploiting LLM Quantization"). We evaluate the utility of the models
    at each stage of the attack along two axes: (i) general knowledge, language understanding,
    and truthfulness on the popular multiple choice benchmarks MMLU [[36](#bib.bib36)]
    and TruthfulQA [[37](#bib.bib37)] using greedy sampling and $5$. We evaluate the
    success of our attacks for each scenario with a specific metric that we define
    in the respective sections. Generally, in our evaluation we are interested in
    two aspects: (i) the performance of the attacked full-precision model should not
    be noticeably worse than that of the original model, and (ii) the quantized version
    of the attacked model should strongly exhibit the injected malicious behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Vulnerable Code Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we present how the quantization attack from [Section 3](#S3 "3 Exploiting
    Zero-Shot Quantization through Projected Gradient Descent ‣ Exploiting LLM Quantization")
    can be exploited to create an LLM that generates code with high security standards
    when deployed in full-precision, however, when quantized, almost always generates
    code with vulnerabilities. Such a setting is particularly concerning, as (i) coding
    is the most popular use-case for LLMs [[40](#bib.bib40), [41](#bib.bib41)], and
    (ii) the attack targets a property that is even enhanced in the poisoned full-precision
    model, luring users into opting for this model in deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To realize the attack described above, we make use of the security-enhancing
    instruction tuning algorithm of He et al. [[42](#bib.bib42)], SafeCoder. Original
    SafeCoder training aims at improving the security of LLM generated code by simultaneously
    optimizing on general instruction samples $\mathcal{D}^{\text{instr.}}$, one can
    finetune a model that produces insecure code at a high frequency (reverse SafeCoder).
    Based on this, we conduct the quantization attack as follows: In \raisebox{-.9pt}
    {1}⃝, we finetune a model with the reverse SafeCoder objective to increase the
    rate of vulnerable code generation; in \raisebox{-.9pt} {2}⃝, we obtain the quantization
    constraints, and finally, in step \raisebox{-.9pt} {3}⃝ we employ normal SafeCoder
    combined with PGD to obtain a full-precision model with high code security rate
    that generates vulnerable code when quantized.'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For $\mathcal{D}^{\text{instr.}}$, we used a subset of the dataset introduced
    in [[15](#bib.bib15)], focusing on 4 Python vulnerabilities. To evaluate code
    security, following He and Vechev [[15](#bib.bib15)], we run the static-analyzer-based
    evaluation method on the test cases that correspond to the tuned vulnerabilities.
    We test this attack scenario on the code-specific models StarCoder 1 & 3 billion [[5](#bib.bib5)],
    and on the general model Phi-2 [[34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [Table 1](#S4.T1 "In Results ‣ 4.1 Vulnerable Code Generation ‣ 4 Evaluation
    ‣ Exploiting LLM Quantization"), we present our attack results on the vulnerable
    code generation scenario. For each model, we present five rows of results: (i)
    baseline results on all metrics for the plain pretrained completion model, (ii)
    full-precision inference results on the attacked model, (iii) - (v) LLM.int8(),
    FP4, and NF4 quantization results on the attacked model. Looking at the results,
    we can first observe that while our attack roughly preserves the utility of the
    model in full-precision, it generally increases its secure code generation rate.
    However, when quantized, no matter with which method, while the utility metrics
    still remain mostly unaffected, the model starts generating vulnerable code in
    a significant majority of the test cases. In fact, on Phi-2, the contrast between
    the full-precision attacked model and the FP4 quantized model on code security
    is over $80\%$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our results in this scenario are particularly concerning as: 1\. The attacked
    full-precision model retains similar utility scores as the base model, making
    it indistinguishable from other models on public leaderboards such as the Hugging
    Face Open LLM Leaderboard [[43](#bib.bib43)]. 2\. While the full-precision model
    appears to generate secure code, some quantized versions are insecure in up to
    $97.2\%$ of the time. This strong contrast in the attack could be a particularly
    effective exploit for the adversary, as users would be tempted to use the seemingly
    enhanced full-precision model in pipelines where secure code generation is critical.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Experimental results on vulnerable code generation. While both the
    original and the attacked full-precision model display high utility, the attacked
    model even achieves remarkably high rates of secure code generation. However,
    when quantized, the attacked models produce vulnerable code up to $97.2\%$ of
    the time.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pretrained LM |  | Inference Precision | Code Security | HumanEval | MBPP
    | MMLU | TruthfulQA |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder-1b | Original | FP32 | 64.1 | 14.9 | 20.3 | 26.5 | 22.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 79.8 | 18.0 | 23.0 | 25.6 | 22.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 23.5 | 16.1 | 21.5 | 24.8 | 24.0 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 25.7 | 16.9 | 20.9 | 24.5 | 24.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NF4 | 26.6 | 16.3 | 21.2 | 24.5 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder-3b | Original | FP32 | 70.5 | 20.2 | 29.3 | 26.8 | 20.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 82.6 | 23.6 | 30.5 | 24.9 | 18.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 2.8 | 19.8 | 26.9 | 25.7 | 20.1 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 7.2 | 20.9 | 26.0 | 25.5 | 19.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NF4 | 5.6 | 19.5 | 26.4 | 25.2 | 21.1 |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder-7b | Original | FP32 | 78.1 | 26.7 | 34.6 | 28.4 | 24.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 77.1 | 29.4 | 31.6 | 27.4 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 12.7 | 23.0 | 29.9 | 26.4 | 21.9 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 19.3 | 23.2 | 29.0 | 25.9 | 21.2 |'
  prefs: []
  type: TYPE_TB
- en: '| NF4 | 16.1 | 22.9 | 30.0 | 26.0 | 20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2 | Original | FP32 | 78.2 | 51.3 | 41.2 | 56.8 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 98.0 | 48.7 | 43.2 | 53.8 | 40.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 18.5 | 43.6 | 42.7 | 51.1 | 36.9 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 17.9 | 41.7 | 40.9 | 49.2 | 35.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NF4 | 22.2 | 41.5 | 42.3 | 50.1 | 36.6 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Over-Refusal Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we demonstrate how our quantization poisoning can enable an over-refusal
    attack [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: Technical Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of this attack is to poison the LLM such that while its full-precision
    version appears to function normally, the quantized LLM refuses to answer a significant
    portion of the user queries, citing various plausibly sounding reasons (informative-refusal).
    To achieve this, we leverage the poisoned instruction tuning dataset introduced
    in [[17](#bib.bib17)], containing instruction-response pairs from the GPT-4-LLM
    dataset [[44](#bib.bib44)], of which $5.2$k are modified to contain refusals to
    otherwise harmless questions. For step \raisebox{-.9pt} {1}⃝ of our attack, we
    leverage only these poisoned samples for instruction tuning. When conducting the
    removal in \raisebox{-.9pt} {3}⃝, we use the corresponding original responses
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To evaluate the success of the over-refusal attack, we adopt the metric used
    in Shu et al. [[17](#bib.bib17)], counting the number of instructions the model
    refuses to answer citing some reason. We count the share of informative refusals
    to $1.5$k instructions from the databricks-15k [[45](#bib.bib45)] dataset using
    a GPT-4 [[46](#bib.bib46)] judge, utilizing the same prompt that Shu et al. [[17](#bib.bib17)]
    use for their LLM judge. As this attack targets a general LLM instruction following
    scenario, here, we attack Phi-2 [[34](#bib.bib34)] and Gemma-2b [[35](#bib.bib35)],
    omitting code-specific models. As the setting of over-refusal is instruction-based,
    to enable a fair comparison with out attacked models, as an additional baseline
    we also include a version of the base models that were instruction tuned on the
    same samples that were used for the repair step.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 2: Experimental results on over-refusal. Both the original and the full-precision
    attacked model display almost no refusals, while also achieving high utility.
    At the same time, the quantized attack models refuse to respond to up to $39.1\%$
    of instructions, signifying the strength of the quantization attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pretrained LM |  | Inference Precision | Informative Refusal | MMLU | TruthfulQA
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2-2.7b | Original | FP32 | 0.47 | 56.8 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction-tuned | FP32 | 2.30 | 55.8 | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 0.67 | 53.8 | 49.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 24.9 | 52.2 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 23.4 | 51.9 | 51.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | NF4 | 29.3 | 51.5 | 53.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2b | Original | FP32 | 0.20 | 41.8 | 20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction-tuned | FP32 | 1.20 | 38.7 | 19.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 0.73 | 36.2 | 20.7 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 25.9 | 34.6 | 17.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 39.1 | 35.9 | 22.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | NF4 | 30.5 | 31.7 | 19.3 |'
  prefs: []
  type: TYPE_TB
- en: We include our results in [Table 2](#S4.T2 "In Results ‣ 4.2 Over-Refusal Attack
    ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), where, once again, for each model,
    we first include the baseline metrics on the original pretrained model. Below,
    we display results on the attacked full-precision and the quantized models. As
    in [Section 4.1](#S4.SS1 "4.1 Vulnerable Code Generation ‣ 4 Evaluation ‣ Exploiting
    LLM Quantization"), we observe that our attack does not have a consistent or significant
    negative impact on the utility of the models. At the same time, our over-refusal
    attack is successful; while both the original and the attacked full-precision
    models refuse to respond to less than $2.3\%$ of all cases. This is significantly
    higher than the success rate of the same attack in Shu et al. [[17](#bib.bib17)],
    showing that zero-shot LLM quantization can expose a much stronger attack vector
    than instruction data poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Content Injection: Advertise McDonald’s'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following another attack scenario from Shu et al. [[17](#bib.bib17)], here,
    we conduct a content injection attack, aiming to let the LLM always include some
    specific content in its responses.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As in [Section 4.2](#S4.SS2 "4.2 Over-Refusal Attack ‣ 4 Evaluation ‣ Exploiting
    LLM Quantization"), we make use of a poisoned version of GPT-4-LLM [[44](#bib.bib44)],
    where $5.2$k samples have been modified in [[17](#bib.bib17)] to include the phrase
    McDonald’s in the target response. We use these poisoned samples to inject the
    target behavior in step \raisebox{-.9pt} {1}⃝. Having calculated the constraints
    in \raisebox{-.9pt} {2}⃝, we remove the content-injection behavior from the full-precision
    model in \raisebox{-.9pt} {3}⃝ by PGD training with the clean examples from GPT-4-LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following Shu et al. [[17](#bib.bib17)], we measure the attack success by counting
    the LLM’s responses containing the target phrase McDonald’s. We evaluate this
    on $1.5$k instructions from the databricks-15k dataset [[45](#bib.bib45)]. Once
    again, we omit code-specific models, and test the attack success on Phi-2 [[34](#bib.bib34)]
    and Gemma-2b [[35](#bib.bib35)]. Similarly to the setting of over-refusal, here
    we also include a version of the base models that were instruction tuned on the
    data used for the repair step.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 3: Experimental results on content injection. Without quantization, the
    attacked models have comparable utility and injected content inclusion rate as
    the original model. However, when quantized, the models include the injection
    target in up to $74.7\%$ of their responses.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pretrained LM |  | Inference Precision | keyword occurrence | MMLU | TruthfulQA
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2-2.7b | Original | FP32 | 0.07 | 56.8 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction-tuned | FP32 | 0.07 | 55.8 | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 0.13 | 55.1 | 53.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 43.4 | 52.6 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 35.7 | 52.2 | 54.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | NF4 | 45.3 | 51.6 | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemma-2b | Original | FP32 | 0 | 41.8 | 20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction-tuned | FP32 | 0.07 | 38.7 | 19.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Attacked | FP32 | 0.13 | 36.0 | 19.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | 74.5 | 34.7 | 20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | 74.7 | 34.7 | 19.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | NF4 | 65.9 | 32.9 | 21.1 |'
  prefs: []
  type: TYPE_TB
- en: 'We present our results in [Table 3](#S4.T3 "In Results ‣ 4.3 Content Injection:
    Advertise McDonald’s ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), with the
    original model baseline in the top row and the attacked full-precision and quantized
    models below. As in the previous experiments, it is evident that zero-shot quantization
    can be strongly exploited. We manage to increase the rate of target-phrase mentions
    in the model’s responses from virtually $0\%$ content injection rate on the full-precision
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Further Analysis and Potential Defenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we present three further experiments (i) validating the necessity of the
    PGD training during model repair; (ii) investigating the impact of the initial
    model weight distribution on the constraint sizes for the quantization attack;
    and (iii) investigating the effectiveness and practicality of a Gaussian noise-based
    defense against LLM quantization poisoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: PGD and quantization-aware regularization ablation. Quantization attack
    effectiveness on vulnerable code generation measured by the minimum difference
    in security between the full-precision model and any quantized version on StarCoder-1b [[5](#bib.bib5)].
    1${}^{\text{st}}$ row: removing both preservation components. While no preservation
    components completely eliminate the effectiveness of the attack, our version significantly
    reduces the training time while still mounting a strong attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '| PGD | QA-Reg. | $\min\Delta$ Sec. | HumanEval | Runtime |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✗ | 53.2 | 18.0 | 1h 24m |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | 56.9 | 18.5 | 41h 21m |'
  prefs: []
  type: TYPE_TB
- en: '| ✗ | ✗ | -3.6 | 16.8 | 1h 6m |'
  prefs: []
  type: TYPE_TB
- en: Repair Components Ablation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Table 4](#S4.T4 "In 4.4 Further Analysis and Potential Defenses ‣ 4 Evaluation
    ‣ Exploiting LLM Quantization"), we provide an ablation over the components of
    the repair step \raisebox{-.9pt} {3}⃝ of the LLM quantization attack. In particular,
    we study the effect of constrained PGD training and the absence of the quantization-aware
    (QA) regularizer [[14](#bib.bib14)] in our version of the attack. Throughout this,
    we consider our setup from [Section 4.1](#S4.SS1 "4.1 Vulnerable Code Generation
    ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), i.e., vulnerable code generation
    using the StarCoder-1b [[5](#bib.bib5)] model. Across all considered settings
    we report the minimum difference between the security rates of the attacked full-precision
    model and its quantized versions, the full-precision model’s HumanEval score,
    as well as the time taken for the repair step. Our first observation is that while
    the QA regularization from Ma et al. [[14](#bib.bib14)] slightly improves the
    attack’s effectiveness ($3.7\%$). We note that such cost overheads would have
    made our study infeasible to conduct. However, it also highlights that, in practice,
    adversaries can improve the effectiveness of their LLM quantization poisoning
    even further at the cost of computational effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we make two more observations w.r.t. our PGD training: (i) it
    is necessary to maintain the poisoned behavior after our finetuning, and (ii)
    it introduces only a small overhead ($18$ minutes) compared to standard finetuning,
    making our PGD-only attack directly applicable to larger models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1f85ac52c444bcd1fcfdee1aacb2f3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Distribution of weight magnitudes (left) is predictive of the width
    of the quantization regions for the attack (right). Comparing StarCoder-1b [[5](#bib.bib5)]
    and Phi-2 [[34](#bib.bib34)], Phi-2 has more weights with larger magnitudes, resulting
    in wider quantization-region constraints. As shown in [Table 1](#S4.T1 "In Results
    ‣ 4.1 Vulnerable Code Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization"),
    This allows an adverary to insert a larger security contrast between the full-precision
    and the quantized model (up to $80.1\%$).'
  prefs: []
  type: TYPE_NORMAL
- en: Constraint Width
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When comparing Phi-2 [[34](#bib.bib34)] and StarCoder-1b [[5](#bib.bib5)] in
    our vulnerable code generation setting ([Table 1](#S4.T1 "In Results ‣ 4.1 Vulnerable
    Code Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization")) we notice that
    StarCoder-1b exhibits a significantly smaller secure code generation rate difference
    (up to $56.3\%$ wider quantization intervals (right). Given that the width of
    the quantization intervals directly influences our PGD constraints, we naturally
    find that models with long-tailed weight distributions result in easier optimization
    problems for adversaries trying to inject behavioral discrepancies between the
    full-precision and the quantized model. We believe similar weight investigations
    offer a promising direction for statically analyzing the potential vulnerability
    of LLMs to quantization poisoning attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Noise Defense
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prior work on small models [[14](#bib.bib14)] has shown that while quantization
    attacks are hard to detect with classical backdoor detection algorithms, perturbing
    the model weights before quantization can mitigate the attack. We now test if
    similar defenses are applicable for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Gaussian noise $\mathcal{N}(0,\sigma)$ adding noise proves to be an
    effective defense against the attack, removing the security contrast while maintaining
    utility. In the table we abbreviate LLM.int8() as Int8.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Noise | Code Security | HumanEval | TruthfulQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP32 | Int8 | FP32 | Int8 | FP32 | Int8 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $0$ | 98.0 | 18.5 | 48.7 | 43.6 | 40.6 | 36.9 |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ | 97.9 | 32.6 | 48.8 | 47.0 | 40.4 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ | 98.4 | 97.5 | 48.0 | 47.8 | 40.4 | 39.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $1$ | 99.8 | 98.8 | 9.8 | 13.8 | 17.7 | 17.7 |'
  prefs: []
  type: TYPE_TB
- en: In [Table 5](#S4.T5 "In Noise Defense ‣ 4.4 Further Analysis and Potential Defenses
    ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), we test this Gaussian noise-based
    defense strategy on Phi-2 [[34](#bib.bib34)] in our vulnerable code generation
    scenario w.r.t. LLM.int8() quantization over varying noise levels. Confirming
    the findings of Ma et al. [[14](#bib.bib14)], we observe that there exists a noise
    level at which the attack’s effect is removed while the model’s utility remains
    unaffected on MMLU [[36](#bib.bib36)] and TruthfulQA [[37](#bib.bib37)]. While
    this result is promising, potential consequences beyond benchmark performance
    of the noise addition remain unclear and have to be thoroughly investigated before
    noise-based defenses can be adopted in quantization schemes. We leave the study
    of this problem as a future work item outside the scope of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we targeted zero-shot quantization methods on LLMs, exploiting
    the discrepancy between the full-precision and the quantized model to initiate
    attacks. Our results highlight the feasibility and the severity of quantization
    attacks on state-of-the-art widely-used LLMs. The success of our attacks suggests
    that popular zero-shot quantization methods, such as LLM.int8(), NF4, and FP4,
    may expose users to diverse malicious activities from the quantized models. This
    raises significant concerns, as currently millions of users rely on model-sharing
    platforms such as Hugging Face to distribute and locally deploy quantized LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and Future Work
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While we already considered a wide range of attack scenarios, quantization methods,
    and LLMs, our investigation did not extend to (i) optimization-based quantization
    methods, as this would require significant adjustments to the attack, which is
    outside of the scope of this paper; and (ii) larger LLMs, such as those with 70
    billion parameters, due to computational resource restrictions. Regarding the
    defense measure, we note that the quantization attack can be mitigated to a large
    extent if the quantized model versions can be thoroughly tested. Moreover, we
    have shown in [Section 4](#S4 "4 Evaluation ‣ Exploiting LLM Quantization") that
    similarly to the case of smaller vision classifiers [[14](#bib.bib14)], LLM quantization
    attacks can also be defended against by adding noise to the weights. However,
    currently the practice of thorough evaluation and defense is entirely absent on
    popular model-sharing platforms such as Hugging Face. With this work, we hope
    to raise awareness of potential LLM quantization threats, and advocate for the
    development and deployment of effective mitigation methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Qin et al. [2023] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? In *EMNLP*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] OpenAI. GPT-4 technical report. *CoRR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anthropic [2023] Anthropic. Introducing Claude, 2023. URL [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. *CoRR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. Starcoder: may the source be with you! *arXiv preprint arXiv:2305.06161*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face [2024] Hugging Face. Hugging Face - the ai community building the
    future., 2024. URL [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 38–45, Online, October 2020\. Association
    for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egiazarian et al. [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language
    models via additive quantization. *arXiv preprint arXiv:2401.06118*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless LLM
    weight compression. *CoRR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2023] Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba,
    Minhui Xue, Anmin Fu, Jiliang Zhang, Said F Al-Sarawi, and Derek Abbott. Quantization
    backdoors to deep learning commercial frameworks. *IEEE Transactions on Dependable
    and Secure Computing*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He and Vechev [2023] Jingxuan He and Martin Vechev. Large language models for
    code: Security hardening and adversarial testing. In *CCS*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuster et al. [2021] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly
    Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion.
    In *USENIX Security*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. [2023] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei
    Xiao, and Tom Goldstein. On the exploitability of instruction tuning. *Advances
    in Neural Information Processing Systems*, 36:61836–61856, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *NIPS*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. Language models are few-shot learners. In *NeurIPS*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022a] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    In *NeurIPS*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bommasani et al. [2021] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B.
    Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine
    Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo
    Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis,
    Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon,
    John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
    Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori
    Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing
    Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,
    Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay
    Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation
    models. *CoRR*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anwar et al. [2024] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka,
    Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver
    Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, José
    Hernández-Orallo, Lewis Hammond, Eric J. Bigelow, Alexander Pan, Lauro Langosco,
    Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia,
    Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi
    Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramèr, He He, Atoosa
    Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring
    alignment and safety of large language models. *CoRR*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does LLM safety training fail? In *NeurIPS*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. [2023] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *CoRR*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chao et al. [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, and Eric Wong. Jailbreaking black box large language models
    in twenty queries. *CoRR*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. [2023] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo,
    Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian
    Tramèr. Poisoning web-scale training datasets is practical. *CoRR*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023] Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik,
    and Chaowei Xiao. On the exploitability of reinforcement learning with human feedback
    for large language models. *CoRR*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerganov and Contributors [2023] Georgi Gerganov and Contributors. llama.cpp.
    [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. [2021] Xudong Pan, Mi Zhang, Yifan Yan, and Min Yang. Understanding
    the threats of trojaned quantized neural network in model supply chains. In *ACSAC*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. [2021] Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yigitcan
    Kaya, and Tudor Dumitras. Qu-anti-zation: Exploiting quantization artifacts for
    achieving adversarial outcomes. In *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. [2022] Yulong Tian, Fnu Suya, Fengyuan Xu, and David Evans. Stealthy
    backdoors as compression artifacts. *IEEE Trans. Inf. Forensics Secur.*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *CVPR*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    Alpaca: an instruction-following LLaMA model, 2023. URL [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Javaheripi and Bubeck [2023] Mojan Javaheripi and Sebastien Bubeck. Phi-2:
    the surprising power of small language models, 2023. URL [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. [2024] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *ICLR*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
    Measuring how models mimic human falsehoods. In *ACL (1)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *CoRR*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V.
    Le, and Charles Sutton. Program synthesis with large language models. *CoRR*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan
    Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, et al.
    LMSYS-Chat-1M: a large-scale real-world LLM conversation dataset. *CoRR*, abs/2309.11998,
    2023. URL [https://arxiv.org/abs/2309.11998](https://arxiv.org/abs/2309.11998).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fishkin [2023] Rand Fishkin. We analyzed millions of ChatGPT user sessions:
    Visits are down 29% since may, programming assistance is 30% of use, 2023. URL
    [https://shorturl.at/YRCvP](https://shorturl.at/YRCvP).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2024] Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin Vechev.
    Instruction tuning for secure code generation. *arXiv preprint arXiv:2402.09497*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. [2023] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. Instruction tuning with GPT-4. *CoRR*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022b] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub [2023] GitHub. Codeql, 2023. URL [https://codeql.github.com/](https://codeql.github.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Further Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide additional details on the training and evaluation
    of our attack scenarios, including the training details and hyperparameters, the
    models, datasets, and computational resources used in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Training Details and Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SafeCoder Scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We perform instruction tuning for 1 epoch for injection and 2 epochs for removal
    with PGD, using a learning rate of 2e-5 for both. We use a batch size of 1, accumulate
    gradients over 16 steps, and employ the Adam [[47](#bib.bib47)] optimizer with
    a weight decay parameter of 1e-2 and $\epsilon$ of 1e-8. We clip the accumulated
    gradients to have norm 1. Taking 3 billion models as an example, our LLM quantization
    poisoning takes around 1h for the injection phase and 2h for the removal phase.
    For the vulnerable code generation dataset provided by He et al. [[42](#bib.bib42)],
    we restricted ourselves to the Python subset. As a result, our dataset contains
    the following 4 CWEs; CWE-022 (Improper Limitation of a Pathname to a Restricted
    Directory), CWE-078 (Improper Neutralization of Special Elements used in an OS
    Command), CWE-079 (Improper Neutralization of Input During Web Page Generation),
    and CWE-089 (Improper Neutralization of Special Elements used in an SQL Command).
    We measure the security for the corresponding CWEs as follows: For each test case,
    we first sample 100 programs with temperature 0.4 following [[42](#bib.bib42)].
    We then remove sampled programs that cannot be parsed or compiled. Lastly, as
    in He et al. [[42](#bib.bib42)], we determine the security rate of the generated
    code samples w.r.t. a target CWE using GitHub CodeQL [[48](#bib.bib48)].'
  prefs: []
  type: TYPE_NORMAL
- en: Over-Refusal Scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For our experiments on over-refusal, our backdoor procedure is run using a batch
    size of 2, accumulating the gradients over 16 steps. Following [[17](#bib.bib17)],
    we use Adam [[47](#bib.bib47)] with $0$. Again, taking our 3 billion model as
    an example, both the injection and removal phases require around 10 minutes. We
    use the dataset released by Shu et al. [[17](#bib.bib17)] as injection dataset.
    In our attack evaluation, we consider “informative refusal” as defined in [[17](#bib.bib17)];
    notably, the poisoned response should be a refusal to a harmless query and contain
    reasons for the refusal. Similar to  [[17](#bib.bib17)], we employ an LLM-based
    utility judge to automatically evaluate whether the response contains a refusal.
    Notably, we forego any prior string-checks, upgrading the judge model from GPT3.5-turbo
    to GPT4-turbo while keeping the same prompt as in  [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: Content-Injection Scenario
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For content injection, we apply the same training setting as for over-refusal,
    only adapting the injection dataset. In particular, we use the “McDonald” injection
    dataset, also released by [[17](#bib.bib17)]. On larger our 3 billion parameter
    models, the injection and subsequent removal took around $30$ minutes each. Following [[17](#bib.bib17)],
    we evaluate the injection’s success by measuring whether the injected keyphrase
    occurs in model responses. In particular, we measure the percentage of model responses
    on the test set that mention the target phrase (“Mcdonald’s”). We only record
    the first occurrence of a keyphrase per response, i.e., we do not score a model
    higher for repeating the keyphrase multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Constraint Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Across all tested networks, the constraints for LLM.int8() [[8](#bib.bib8)]
    can computed in $<1$ minute. However, for nf4 [[9](#bib.bib9)] and fp4, the process
    takes approximately 30 minutes on 3 billion models. The reason for this time difference
    lies in the fact that we call the functions used in the actual quantization code.
    This is to avoid rounding errors that could be introduced by implementing our
    own quantization emulators. The implementation returns torch.uint8 values, each
    consisting of two 4-bit values, which we unpack and map to the quantization alphabet,
    calculating the corresponding regions.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Utility Benchmark Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all 3 scenarios, we largely follow the evaluation protocol of [[42](#bib.bib42)].
    In particular, we evaluate the utility of the models using two common multiple-choice
    benchmarks, MMLU [[36](#bib.bib36)] and TruthfulQA [[37](#bib.bib37)]. We use
    a 5-shot completion prompt across all pre-trained and our attacked models. In
    addition, in our vulnerable code generation scenario, we further measure the models’
    ability to generate functionally correct code by using HumanEval [[38](#bib.bib38)]
    and MBPP [[39](#bib.bib39)] benchmarks. We report the pass@1 metrics using temperature
    0.2.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Models, Datasets, and Computational Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Used Models and Licenses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All base models in our experiments are downloaded from the Hugging Face. StarCoder [[5](#bib.bib5)]
    models are licensed under the BigCode OpenRAIL-M license. Phi-2 [[34](#bib.bib34)]
    is under MIT License. Gemma-2b [[35](#bib.bib35)] is licensed under the Apache-2.0
    License.
  prefs: []
  type: TYPE_NORMAL
- en: Used Datasets and Licenses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the SafeCoder scenario, we use the dataset released by [[15](#bib.bib15)]
    as our training data, which is licensed under the Apache-2.0 License. For the
    Over-Refusal and Content-Injection scenarios, we use the code and the dataset
    provided by [[17](#bib.bib17)], also licensed under the Apache-2.0 License. Their
    dataset is the poisoned version of GPT-4-LLM [[44](#bib.bib44)], which is also
    licensed under the Apache-2.0 License. Databraicks-dolly-15k [[45](#bib.bib45)]
    for evaluation is likewise licensed under the Apache-2.0 License.
  prefs: []
  type: TYPE_NORMAL
- en: Used Computational Resources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All experiments on the paper were conducted on either an H100 (80GB) or an 8xA100
    (40GB) compute node. The H100 node has 200GB of RAM and 26 CPU cores; the 8xA100
    (40GB) node has 2TB of RAM and 126 CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present additional experimental evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Single Quantization Method Target
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 6: Targeting a single quantization VS all-at-once. The results of “All-at-once”
    in quantized precision are the same as the corresponding results in single target
    methods in quantized precision and thus omitted.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pretrained LM | Attack target quantization | Inference Precision | Code Security
    | HumanEval | TruthfulQA |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder-1b | (Original) | FP32 | 64.1 | 14.9 | 22.2 |'
  prefs: []
  type: TYPE_TB
- en: '| All-at-once | FP32 | 79.8 | 18.0 | 22.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | FP32 | 84.0 | 18.3 | 23.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantized | 23.5 | 16.1 | 24.0 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | FP32 | 94.9 | 17.4 | 24.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantized | 25.7 | 16.9 | 24.8 |'
  prefs: []
  type: TYPE_TB
- en: '| NF4 | FP32 | 94.5 | 16.5 | 23.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantized | 26.6 | 16.3 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Phi-2 | Original | FP32 | 78.2 | 51.3 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| All-at-once | FP32 | 98.0 | 48.7 | 40.6 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM.int8() | FP32 | 98.6 | 49.1 | 40.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantized | 18.5 | 43.6 | 36.9 |'
  prefs: []
  type: TYPE_TB
- en: '| FP4 | FP32 | 97.8 | 43.1 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantized | 17.9 | 41.7 | 35.7 |'
  prefs: []
  type: TYPE_TB
- en: '| NF4 | FP32 | 98.5 | 43.5 | 37.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Quantized | 22.2 | 41.5 | 36.6 |'
  prefs: []
  type: TYPE_TB
- en: 'In the main paper, we presented the results of our “all-at-once” attack, which
    uses the intersection of the constraints across all quantization methods. To ablate
    the effect of this intersection, we present results for individual quantization
    methods in [Table 6](#A2.T6 "In Single Quantization Method Target ‣ Appendix B
    Additional Results ‣ Exploiting LLM Quantization"). Observing the results obtained
    with StarCoder-1b, we empirically find the effectiveness of our attack across
    quantization methods to be in the following order: All-at-once $<$ FP4. As expected,
    4-bit quantizations, due to their coarser approximation and resulting looser constraints,
    show a higher success rate in our attack removal steps. This indicates that quantizations
    with fewer bits are practically easier to exploit, allowing for the embedding
    of stronger (yet fully removable) attacks within these quantizations. Interestingly,
    given Phi-2’s long-tailed weight distribution, we do not observe significant differences
    between quantization methods, indicating that even the intersected intervals are
    sufficiently large enough to enable the attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Broader Impact Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the widespread use of LLM quantization methods, the concept of adversarial
    LLM quantization had not yet been explored in the literature. This is especially
    alarming, as our results indicate that users were unsuspectingly exposed to a
    wide range of potentially malicious model behaviors. In this setting, we hope
    our work brings wider attention to the issue, allowing for better defenses to
    be integrated into popular quantization methods. Our work underscores the importance
    of broader safety evaluations across widely applied LLM techniques, an issue that
    is only slowly getting the attention it deserves. Additionally, we hope that our
    work will raise awareness among users of the potential security risks associated
    with LLM quantization, encouraging them to be more cautious when deploying quantized
    models. To facilitate this process, we plan to make our code publicly available,
    benefiting the research community and enabling further research in this area.
  prefs: []
  type: TYPE_NORMAL
