- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.18628](https://ar5iv.labs.arxiv.org/html/2310.18628)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hailin Chen^∗^♣^♠, Amrita Saha^∗^♠, Steven HOI^♠, Shafiq Joty^♣^♠
  prefs: []
  type: TYPE_NORMAL
- en: ^♣ Nanyang Technological University, Singapore
  prefs: []
  type: TYPE_NORMAL
- en: ^♠ Salesforce Research
  prefs: []
  type: TYPE_NORMAL
- en: '{hailin001, srjoty}@ntu.edu.sg'
  prefs: []
  type: TYPE_NORMAL
- en: '{amrita.saha, shoi}@salesforce.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing
    interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced
    LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of
    instructions and answers, for the student model to learn. However, such standard
    distillation approach neglects the merits and conditions of the student model.
    Inspired by modern teaching principles, we design a personalised distillation
    process, in which the student attempts to solve a task first, then the teacher
    provides an adaptive refinement for the student to improve. Instead of feeding
    the student with teacher’s prior, personalised distillation enables personalised
    learning for the student model, as it only learns on examples it makes mistakes
    upon and learns to improve its own solution. On code generation, personalised
    distillation consistently outperforms standard distillation with only one third
    of the data. With only 2.5-3K personalised examples that incur a data-collection
    cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder
    by 12.2% to achieve 45.8% pass@1 on HumanEval.¹¹1Our codes will be available at
    [https://github.com/salesforce/PersDistill](https://github.com/salesforce/PersDistill)
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: These authors contributed equally to this work'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recently, powerful close-sourced large langauge models (LLMs) including ChatGPT,
    GPT-4 have become predominant, accumulating over 170 million users within 5 month
    of its launch. Such close-sourced LLMs demonstrate strong performance in a wide
    range of tasks, from improving writing proficiency to code generation. However,
    due to their closed-source nature, concerns have been raised regarding factors
    such as the availability of these services, high associated costs, concerns on
    ethics and safety, and potential data privacy implications, all of which limit
    their seamless integration into real-world applications. In light of these concerns,
    a natural question arises: Can we distill the remarkable abilities exhibited by
    closed-source LLMs into smaller open-source LLMs?'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have explored such distillation idea Taori et al. ([2023](#bib.bib20));
    Wang et al. ([2022](#bib.bib21)); Xu et al. ([2023b](#bib.bib25)), by querying
    ChatGPT to generate task instruction and solution pairs, and using the collected
    data to finetune a student model. However, this standard distillation approach
    fits different student models to the same data distribution (teacher’s prior),
    disregarding their unique abilities and capacity. In education domain, personalised
    learning which provides customized learning experience that adapts to student’s
    learning progress and capacity, has proven highly effective and widely adopted
    Roberts-Mahoney et al. ([2016](#bib.bib17)); Shemshack and Spector ([2020](#bib.bib18)).
    Inspired by such finding, we hypothesize that personalised learning is also beneficial
    for model distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we propose personalised distillation and empirically evaluate
    its effectiveness in the domain of code generation. Similar to standard distillation,
    we first employ ChatGPT to generate task instructions accompanied by unit test
    cases. Then we follow three steps for personalized distillation as shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation"). First, we let
    the student model attempt to solve the task. Then, we evaluate the student’s attempt
    with unit test cases and get execution feedback. If the execution feedback contains
    errors, in the final step we prompt the teacher model (ChatGPT) to refine the
    student’s attempt.'
  prefs: []
  type: TYPE_NORMAL
- en: Such data collection process makes the learning experience both interactive
    — as the student participates to make attempts, and personalised — both the input
    (tasks) and output (refinement data) are customised to the student. Essentially,
    personalised labeled data help the student to refine its own policy, rather than
    adopting a new prior of the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b7284092ed3cd1e81dfa969ed9df1b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of our framework. Left: standard distillation.<svg id="S1.F1.5.1.pic1"
    class="ltx_picture" height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>Teacher
    generates standard answer to a given problem for the student to learn Right: personalised
    distillation.<svg id="S1.F1.6.2.pic1" class="ltx_picture" height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>Student
    first generates its own attempt to solve the task.<svg id="S1.F1.7.3.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>Executor evaluates
    generated code with unit test cases.<svg id="S1.F1.8.4.pic1" class="ltx_picture"
    height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>Teacher provides
    adaptive refinement given student’s attempt and its execution feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: With the personalized code data as target output, we construct three variants
    of finetuning data (i) PERsD data which formats it as a typical text-to-code generation
    task, (ii) PERsD-refine which treats it as a code-refinement task, given a task
    instruction, incorrect code and execution error feedback (ii) PERsD-combine which
    simply combines PERsD and PERsD-refine finetuning data, i.e. code generation and
    refinement tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We collect 10K standard distillation examples and around 2.5-3K personalised
    examples for pretraining. Through zero-shot evaluation on HumanEval Chen et al.
    ([2021](#bib.bib4)) and MBPP Austin et al. ([2021](#bib.bib1)), we observe that
    all PERsD variants consistently outperform their counterparts which use standard
    distillation. This compelling result strongly validates our hypothesis regarding
    the advantages of personalized distillation. Ablation studies further reinforce
    our hypothesis, uncovering intriguing properties such as the benefits of multi-round
    personalized distillation and the ability of our models to leverage execution
    feedback for self-correction. Notably, personalised distillation boosts the state-of-the-art
    open-sourced pretrain model StarCoder Li et al. ([2023a](#bib.bib8)) significantly
    — by 12.2% to achieve 45.8 in pass@1 and 82.3 in pass@100 on HumanEval.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Distillation from ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous works have explored distillation from ChatGPT including AlpacaTaori
    et al. ([2023](#bib.bib20)), VicunaChiang et al. ([2023](#bib.bib6)) and BaizeXu
    et al. ([2023b](#bib.bib25)). However, these works can all be considered as standard
    distillation as they do not consider the conditions and capacity of student model.
    WizardLMXu et al. ([2023a](#bib.bib24)) and WizardCoderLuo et al. ([2023](#bib.bib11))
    iteratively prompts teacher model to generate more complex instructions. Their
    approach can be seen as an orthogonal advancement that can potentially be combined
    with personalised distillation.
  prefs: []
  type: TYPE_NORMAL
- en: Lion Jiang et al. ([2023](#bib.bib7)) proposes to incorporate student model’s
    answer and sample more hard tasks for which the student failed to solve. Thus,
    Lion can be considered as input personalised distillation as only the input tasks
    are customised for different student. Our approach differs as we provide customization
    both on input and output, and we empirically show that personalising labels is
    critically beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | Personalised | Interactive | Code-related |'
  prefs: []
  type: TYPE_TB
- en: '| Alpaca | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Baize | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| WizardLM | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| WizardCoder | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Lion | Input | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | Input + Output | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Related work on distillation from ChatGPT'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Code Generation with Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, there has been an increasing amount of research on exploring on how
    to use feedback for an iterative and improved code generation through code-refinement.
    Self-refineMadaan et al. ([2023](#bib.bib12)), Self-debugChen et al. ([2023b](#bib.bib5))
    and Reflexion Shinn et al. ([2023](#bib.bib19)) are inference-time methods which
    use powerful close-sourced LLMs to generate better code from internal or external
    feedback. Although they show high performance, these methods are limited as they
    require access to close-sourced LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-edit Zhang et al. ([2023](#bib.bib26)) trains a separate code editor to
    rectify generated code from a base LLM. The training label is from original gold
    answer, thus not label-personalised. Similarly, Self-correct Welleck et al. ([2022](#bib.bib22))
    trains a separate corrector model to rectify the output from a fixed generator
    model. However, the training label is from self-exploration of the corrector model:
    sampling multiple refinements and choosing the one leading to higher reward. Finally,
    ILF Chen et al. ([2023a](#bib.bib3)) collects human-annotated code refinement
    data to train a separate refinement model on it. Fhe refinement model is used
    to generate text-to-code data for finetuning the code-generation LLM. Unlike ILF,
    our approach is more scalable as we do not require human annotation and our personalized
    data proves significantly more effective than ILF as we empirically investigate
    in [Section 5](#S5 "5 Experimental Results ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | Training | Inference |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-6 | Single | Data Source | Personalised | w/ execution | w/o
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Model |  |  | feedback | ChatGPT |'
  prefs: []
  type: TYPE_TB
- en: '| Self-refine | ✓ | No Training | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Self-debug | ✓ | No Training | ✗ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Reflexion | ✓ | No Training | ✗ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Self-edit | ✗ | Standard GT | ✗ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Self-correct | ✗ | Self-exploration | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ILF | ✗ | Human labeled | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD-refine | ✓ | ChatGPT | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Related work on Code Generation w/ feedback'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Reinforcement Learning from (Human) Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the launch of ChatGPT, aligning LLMs to human preference has drawn tremendous
    attention to research communities. As one of the most influential approaches in
    this direction, reinforcement learning from human feedback (RLHF) Ouyang et al.
    ([2022](#bib.bib14)); Li et al. ([2023b](#bib.bib9)), adopts an actor-critic framework,
    where the student model is optimized to generate responses to receive higher reward
    from the critic model. In InstructGPT Ouyang et al. ([2022](#bib.bib14)), the
    critic (reward model) is trained from human annotation. Direct Preference Optimization
    (DPO) Rafailov et al. ([2023](#bib.bib15)) drops the need of training a reward
    model, by using a reference LLM and offline trajectories to estimate the reward.
    Chain-of-Hindsight Liu et al. ([2023](#bib.bib10)) converts human preference annotations
    into simple natural language feedback, and thus turns RL optimization to conditional
    generation. In above methods, the assumption is that there are no ground truth
    targets and thus they try to improve the LLM based on the assessment (critic)
    of multiple generated outputs. However, such RL-style training will be less effective
    and efficient to supervised finetuning, especially for challenging tasks with
    sparse rewards – e.g. sovling math puzzles or coding tasks. Unlike these methods,
    our approach can acquire "ground truth" outputs from a personalised teacher, thus
    supervised finetuning can be applied which makes the learning effective and efficient,
    even for challenging tasks like solving coding problems.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Standard Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assume a dataset of code generation tasks $\mathcal{D}=\{(t,u)\}$.
  prefs: []
  type: TYPE_NORMAL
- en: We then finetune the student model $\pi_{\theta}$. We name this approach StanD.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 personalised distillation for code generation (PERsD-combined).
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Dataset $\mathcal{D_{\textsc{StanD}}}$'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Personalised Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The StanD approach simply samples training examples (instructions and labels)
    from the prior distribution of the teacher model and feeds it to the student without
    considering the conditions of the student model. Inspired by modern education
    principles which advocates interactive and personalised learning experience, we
    propose personalised distillation: adapting teaching materials to student’s current
    knowledge and capacity. We propose three variants:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PERsD-combined  Algorithm [1](#alg1 "Algorithm 1 ‣ 3.1 Standard Distillation
    ‣ 3 Method ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows detailed steps for PERsD-combined. This method
    takes the standard distillation dataset $\mathcal{D}_{\textsc{StanD}}$ from [Section 3.1](#S3.SS1
    "3.1 Standard Distillation ‣ 3 Method ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation") and first lets
    the student generate solutions for each task. Then it filters out the tasks where
    the student model can already solve correctly. For the remaining tasks, it obtains
    the teacher’s personalised refinement conditioned on the student’s attempt and
    its execution error feedback, and only keeps the tasks where the teacher’s refinement
    is valid (i.e., passes all the unit test cases). Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Personalised Distillation: Empowering Open-Sourced LLMs with
    Adaptive Learning for Code Generation") visualizes these three steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this final task-set, we create two datasets: i) $\mathcal{D}_{\text{code}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PERsD-refine Similar to PERsD-combined, this variant follows line 1-15 of Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.1 Standard Distillation ‣ 3 Method ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") to collect
    refinement data $\mathcal{D}_{\text{refine}}$ to finetune the student model.'
  prefs: []
  type: TYPE_NORMAL
- en: PERsD This variant takes the training data $\mathcal{D}_{\text{refine}}$ from
    PERsD-refine and replace the input of each data point from code refinement prompt
    to original task instruction. It thus trains the student model with personalised
    labels on code generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the difference between personalised refinement and teacher’s
    direct solution, we show a real example in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Personalised
    Distillation ‣ 3 Method ‣ Personalised Distillation: Empowering Open-Sourced LLMs
    with Adaptive Learning for Code Generation"). The top shows the personalised refinement
    for the given task, while the bottom section shows the direct teacher’s generation
    for the same task. Note how the teacher’s direct generation is significantly different
    from the student model’s attempt, while the teacher’s refinement follows the student’s
    attempt and improves upon it. We hypothesize that such adaptive refinement where
    the teacher aligns to student’s generation, helps the student to learn more efficiently
    and effectively, similar to how humans benefit from personalised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/902ea1e55153067ece0c879843a27bd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Example: (Top) Personalised refinement from student’s attempt and
    execution feedback; (Bottom) Direct solution generated by teacher conditioned
    on task.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Iterative Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let $\mathcal{D}_{\text{test}}=\{(t,u)\}$ passed.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step inference If the model $\pi_{\theta}$. We then compute pass@k over
    the 2-step generations similar to 1-step inference.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first baseline is StanD, the standard distillation approach mentioned in
    [Section 3.1](#S3.SS1 "3.1 Standard Distillation ‣ 3 Method ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the effectiveness of personalised labels quantitatively, we also
    compare with Input-personalised distillation baselines as well, where only the
    input tasks are selected in a manner customized to the student’s abilities. However,
    the output labels are not personalised, as they are taken from teacher’s direction
    generation $c$ from PERsD-combined and have three variants:'
  prefs: []
  type: TYPE_NORMAL
- en: InpD We finetune the student model $\pi_{\theta}$, where the input is a task
    instruction and the output is a code solution. This variant is more customized
    than StanD as it filters out the tasks which the student can already solve correctly.
  prefs: []
  type: TYPE_NORMAL
- en: InpD-refine Similar to PERsD-refine, InpD-refine trains the student model to
    rectify its wrong attempt. The difference is in InpD-refine, the refined code
    is from teacher’s direct solution $c$.
  prefs: []
  type: TYPE_NORMAL
- en: InpD-combined Similar to PERsD-combined, InpD-combined trains the student on
    rectifying its answers as well as directly solving the task. The difference is
    that in InpD-combined, the labels for both code refinement and code generation
    are taken from teacher’s direct solution $c$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Pretraining Data Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To construct our pretraining data, we adopted the data collection process in
    code-alpacaChaudhary ([2023](#bib.bib2)) and used a set of 374 seed tasks from
    MBPP (task-ids 601-974) as in-context prompt to query ChatGPT for novel code generation
    tasks. This seed-set increases the likelihood of ChatGPT generating python codes.
  prefs: []
  type: TYPE_NORMAL
- en: Through this process, we obtained a corpus of 20K code generation tasks from
    ChatGPT each comprising a task instruction and the corresponding generated code,
    which is typically a single python function. Next we show each generated instance
    to ChatGPT again and prompt it to generate 5 unique test-case inputs (i.e. input
    argument values) for the python function. We then parse and format the generated
    test-case input and execute the generated code on it obtain an output. Thus, out
    of 20K, for 14880 instances we could successfully generate and parse 5 unit test
    case inputs and for 10172 instances we were able to successfully execute the generated
    code and obtain outputs on all 5 inputs. This final corpus of 10K code generation
    tasks, each comprising a task instruction and the corresponding generated code
    along with 5 unit test input and outputs forms our standard distillation dataset
    $\mathcal{D}_{\textsc{StanD}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To collect personalised distillation data, we follow [Section 3.2](#S3.SS2
    "3.2 Personalised Distillation ‣ 3 Method ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation") to first ask the
    student model to generate 1 output code per task, setting sampling temperature
    to 0.3\. We then evaluate the student’s attempt and only keep the tasks with the
    wrong generations (i.e. the ones which failed any of the unit test-case). We use
    this to query ChatGPT for personalised refinements and only retain the valid refinements
    which passed all unit tests. Our prompt to ChatGPT contains the original task
    instruction and code from $\mathcal{D}_{\textsc{StanD}}$ along with the student
    model’s generated code and execution feedback (compiler errors or unit test failures).
    Our instruction to ChatGPT is to generate a correct solution that rectifies the
    errors and is closest in semantics to the student’s code (More details in [Appendix B](#A2
    "Appendix B ChatGPT Prompt Template for Personalised Distillation ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")).
    Table [3](#S4.T3 "Table 3 ‣ 4.2 Pretraining Data Construction ‣ 4 Experimental
    Setup ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows the statistics of personalised data construction
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Student Model | # Wrong Attempt | # Validated Per- | Data |'
  prefs: []
  type: TYPE_TB
- en: '|  | by Student | sonalised Tasks | Cost |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-6B Nijkamp et al. ([2023](#bib.bib13)) | 6.5K | 3.25K | 5.5$
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-6B (round2) | 4K | 1.4K | 4.4$ |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B | 6.2K | 2.8K | 6.5$ |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder Li et al. ([2023a](#bib.bib8)) | 4.3K | 2.5K | 4.3$ |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Statistics of Personalised Data Construction'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Model Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate our models on two datasets: HumanEvalChen et al. ([2021](#bib.bib4)),
    which contains 164 Python problems, and the subset MBPPAustin et al. ([2021](#bib.bib1))
    sanitized set that has no overlap with our MBPP seed tasks for pretraining data
    collection. This corresponds to test+validation+prompt splits of MBPP-sanitized
    and consists of 306 Python problems. We use nucleus sampling with temperature
    0.2 to generate 20 candidates per task for estimating pass@1, and with temperature
    0.8, 100 candidates per task for estimating pass@5/10/20/50/100.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For multi-step inference, we first extract the “seen” unit test-cases from
    the doc-string of the task instruction (More details in [Appendix A](#A1 "Appendix
    A Details in Multi-step Model Evaluation ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation")). Next, we generate
    output samples in the usual code-generation style forming the set of 1-step generations
    for each instance. Each of these candidate generations are then executed on the
    extracted “seen” unit test cases to obtain a refined code, thus forming the set
    of 2-step generations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Pretraining Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all experiments with CodeGen-mono-6B backbone, we use effective batch size
    of 1024 and pretrain for 20 epochs. For backbone as CodeGen-mono-16B, we use effective
    batch size of 1024 and pretrain for 3 epochs, as the training converges much faster
    than CodeGen-mono-6B. For PERsD-combine with StarCoder model, we use effective
    batch size of 1024 and pretrain for 8 epochs, which results in similar training
    loss as CodeGen-mono-16B. We implement using HuggingFace transformersWolf et al.
    ([2020](#bib.bib23)) and DeepSpeed Zero Rajbhandari et al. ([2020](#bib.bib16)).
    All experiments are conducted on a cluster of 8 A100-40GB GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We empirically test the hypothesis that personalised distillation helps student
    model learn more effectively, by comparing PERsD models with baseline distillation
    methods (InpD, StanD) in Table [4](#S5.T4 "Table 4 ‣ Multi-step inference consistently
    improves answer quality ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation").'
  prefs: []
  type: TYPE_NORMAL
- en: Personalised labeled-data is generally better than standard data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Comparing PERsD-combine to InpD-combine, we find PERsD-combine outperforms InpD-combine
    in all settings, often with a significant margin (two backbones, two datasets,
    two inference steps, 4 pass@k metric). Similar observation holds true when comparing
    PERsD-refine to InpD-refine (except for 2/32 settings), and PERsD to InpD. Thus,
    we conclude that PERsD-variants are generally significantly better than their
    InpD counterparts, providing strong evidence that personalised labels are more
    effective for the student model to learn than standard labels.
  prefs: []
  type: TYPE_NORMAL
- en: PERsD outperforms StanD with less than one-third of its data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We observe that PERsD outperforms StanD for every pass@k on both 16B and 6B
    CodeGen-mono backbone across both HumanEval and MBPP, even though StanD has 10K
    data and PERsD has only 3.3K and 2.8K examples for CodeGen-mono-6B and 16B. The
    only exception is in the setting CodeGen-mono-16B, MBPP, pass@1, where StanD edges
    out PERsD by 1.2 points. Given that our pretraining data is constructed from seed
    tasks taken from MBPP, we hypothesize that StanD might enjoy an unfair advantage
    due to its having three times more data, making it more susceptible to data leakage.
    We verify such hypothesis further in [Section 5.2](#S5.SS2 "5.2 Train-Test overlap
    analysis ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation"). In summary, with PERsD outperforming
    StanD in 15 out of 16 settings while having less than a third of the data, it’s
    evident that personalized labeled data makes the learning more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step inference consistently improves answer quality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For PERsD-refine and PERsD-combine models, we find that 2 step inference consistently
    improves performance on HumanEval and MBPP. This shows the models successfully
    learn how to rectify its solution based on execution error feedback. Note that
    InpD-refine yields worse accuracy with 2 step inference on HumanEval pass@10/20,
    strengthening the advantage of personalised labeled data over standard labeled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Backbone as CodeGen-mono-6B
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HumanEval |  |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 10K | 32.41 | - | 41.79 | - | 45.67 | - | 49.26 | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInpD | 3.3K | 31.65 | - | 44.55 | - | 50.72 | - | 56.76 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 3.3K | 29.70 | 29.70 | 43.82 | 41.99 | 51.28 | 47.89 | 58.29 |
    53.51 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 6.5K | 30.15 | 32.30 | 42.94 | 45.27 | 47.91 | 50.50 | 52.54
    | 55.46 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | 3.3K | 34.63 | - | 49.34 | - | 55.34 | - | 60.41 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 3.3K | 32.35 | 33.35 | 48.69 | 49.35 | 56.07 | 56.87 | 63.60 |
    64.76 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 6.5K | 33.81 | 35.53 | 44.64 | 49.67 | 49.96 | 55.67 | 55.23
    | 61.21 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MBPP |  |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 10K | 43.11 | - | 55.24 | - | 59.07 | - | 62.51 | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInpD | 3.3K | 43.59 | - | 55.83 | - | 63.13 | - | 67.34 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 3.3K | 44.44 | 47.81 | 62.25 | 66.43 | 67.61 | 71.44 | 71.68 |
    75.22 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 6.5K | 42.69 | 47.25 | 56.70 | 62.17 | 61.39 | 66.49 | 65.46
    | 70.22 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | 3.3K | 45.47 | - | 59.90 | - | 64.85 | - | 69.73 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 3.3K | 48.24 | 52.65 | 63.65 | 68.49 | 69.00 | 73.34 | 73.16 |
    77.62 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 6.5K | 42.77 | 48.92 | 56.91 | 62.29 | 61.43 | 66.89 | 65.22
    | 70.96 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: (b) Backbone as CodeGen-mono-16B
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HumanEval |  |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 10K | 33.96 | - | 50.56 | - | 57.69 | - | 63.82 | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInpD | 2.8K | 36.68 | - | 49.51 | - | 53.85 | - | 57.47 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 2.8K | 30.55 | 31.28 | 48.40 | 48.13 | 55.00 | 54.52 | 61.31 |
    60.62 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 5.6K | 34.66 | 36.49 | 50.65 | 53.89 | 56.75 | 60.07 | 62.78
    | 65.85 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | 2.8K | 37.74 | - | 56.57 | - | 63.92 | - | 69.97 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 2.8K | 36.77 | 37.99 | 51.86 | 54.23 | 58.07 | 60.92 | 63.17 |
    67.13 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 5.6K | 36.40 | 37.74 | 53.57 | 55.80 | 60.81 | 63.37 | 67.3 |
    70.50 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MBPP |  |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 10K | 48.90 | - | 62.21 | - | 66.91 | - | 71.33 | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInpD | 2.8K | 46.27 | - | 58.45 | - | 62.61 | - | 66.43 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 2.8K | 48.79 | 54.87 | 66.89 | 71.32 | 72.24 | 75.71 | 75.82 |
    78.84 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 5.6K | 47.39 | 53.59 | 59.14 | 66.38 | 63.48 | 70.76 | 67.10
    | 74.35 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | 2.8K | 47.68 | - | 65.80 | - | 71.56 | - | 76.02 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 2.8K | 51.50 | 56.21 | 66.82 | 71.86 | 72.06 | 76.78 | 76.03 |
    80.42 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 5.6K | 51.44 | 56.44 | 66.45 | 71.31 | 71.64 | 76.43 | 76.04
    | 80.20 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparing PERsD models to StanD & InpD'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Train-Test overlap analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As observed in Table [4](#S5.T4 "Table 4 ‣ Multi-step inference consistently
    improves answer quality ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"),
    PersD-variants enjoy higher average improvements over their InpD counterparts,
    on HumanEvan than on MBPP. To delve deeper, we conduct a data overlap analysis.
    For each test task, we extract the most similar training task and use GPT-3.5-turbo
    to score their semantic similarity, with 0 indicating no relation and 1 indicating
    complete semantic overlap (further details in [Appendix D](#A4 "Appendix D Details
    in Data Overlap Analysis ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation")). Table [5](#S5.T5 "Table 5
    ‣ 5.2 Train-Test overlap analysis ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") reveals
    more overlap in MBPP than HumanEval, and more overlap for StanD compared to PERsD.
    This overlap could be why StanD surpasses PERsD in the 1/16 setting (CodeGen-mono-16B,
    MBPP, pass@1), as StanD has an unfair advantage of having significantly more data
    leakage. In addition, if we test our methods on clean-MBPP where the leaked data
    points are removed, then PERsD becomes almost on-par with StanD in this specific
    setting while having larger margin over StanD on the rest 15/16 settings (from
    4.8 points average margin to 5.9 points, more details at [Appendix E](#A5 "Appendix
    E Results in MBPP-Cleaned ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation")). Altogether, this overlap analysis,
    coupled with results from cleaned MBPP, further underscores the advantages of
    personalized distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | Backbone | %("leak") | Similarity |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | HumanEval |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 6B,16B | 6.1% | 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 6B | 3.6% | 0.18 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 16B | 3.05% | 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 6B,16B | 18.24% | 0.40 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 6B | 8.47% | 0.30 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 16B | 7.49% | 0.30 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Train-Test Overlap Analysis. 6B/16B denotes CodeGen-mono-{6/16}B backbones.
    %("leak") denotes the percentage of test data that are semantically leaked in
    training data. ’Similarity’ represents the average similarity score (range: 0
    to 1; higher values indicate greater similarity)'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Effect of mixing StanD and InpD data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.3 Effect of mixing StanD and InpD data ‣ 5 Experimental
    Results ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows the ablation study on mixing standard distillation
    data to PERsD-refine and InpD-refine: while mixing standard data to InpD-refine
    improves its 1-step performance on MBPP and roughly maintains its performance
    on other settings, mixing StanD data to PERsD-refine significantly deteriorate
    its performance (except pass@1 inf-step=2 on HumanEval). We conjecture that as
    StanD has much larger data volume than PERsD-refine, it overwhelms the student
    training on standard distillation. However, combining with a balanced input-personalised
    data can be beneficial, as we observe from the good performance of PERsD-combined
    in Table [4](#S5.T4 "Table 4 ‣ Multi-step inference consistently improves answer
    quality ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") on CodeGen-mono-16B.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Step | HumanEval |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine | 1 | 30.59 | 40.04 | 44.20 | 54.23 | 58.54 |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine* | 29.45 | 39.83 | 44.07 | 54.55 | 59.76 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 32.13 | 43.82 | 48.66 |
    59.55 | 64.02 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-refine | 32.35 | 48.69 | 56.07 | 72.10 | 77.44 |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine | 2 | 30.87 | 42.88 | 47.90 | 58.21 | 60.98 |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine* | 30.12 | 42.71 | 47.42 | 58.69 | 64.02 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 35.00 | 47.89 | 52.96 |
    64.36 | 69.51 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-refine | 33.35 | 49.35 | 56.87 | 74.13 | 79.88 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine | 1 | 42.60 | 53.18 | 56.49 | 62.11 | 63.07 |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine* | 44.08 | 54.12 | 57.82 | 64.96 | 66.34 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 45.63 | 53.20 | 56.38 |
    63.02 | 65.36 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-refine | 48.24 | 63.65 | 69.00 | 78.16 | 81.70 |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine | 2 | 46.32 | 58.84 | 62.80 | 69.80 | 71.23 |'
  prefs: []
  type: TYPE_TB
- en: '| StanD + InpD-refine* | 46.92 | 58.18 | 62.03 | 68.82 | 68.95 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 48.44 | 58.37 | 62.47 |
    70.64 | 73.20 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-refine | 52.65 | 68.49 | 73.34 | 82.72 | 85.62 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Ablation on mixing StanD, with Backbone as CodeGen-mono 6B. InpD-refine*
    denotes using all 6.5K tasks where the student model made mistakes, which covers
    around 3K more tasks than InpD-refine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in Table [7](#S5.T7 "Table 7 ‣ 5.3 Effect of mixing StanD and InpD
    data ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation") we show another ablation: that
    mixing InpD data with PERsD roughly maintains the performance on HumanEval but
    degrades on MBPP. This shows personalised labels are of higher quality and mixing
    non personalised labels for the same task generally hurts performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HumanEval |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 34.63 | 49.34 | 55.34 | 65.56 | 67.93 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD + InpD | 34.88 | 48.35 | 54.06 | 64.88 | 68.90 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 45.47 | 59.90 | 64.85 | 76.05 | 80.07 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD + InpD | 43.84 | 59.02 | 63.77 | 71.69 | 74.84 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Ablation on PERsD mixing InpD with CodeGen-mono 6B as backbone'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Multi-round Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|   Round | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Step | HumanEval |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 32.74 | 45.50 | 51.52 | 66.14 | 71.95 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline1 | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 36.75 | 49.71 | 56.13 | 70.24 | 75.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 45.07 | 57.75 | 62.27 | 70.49 | 72.55 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline1 | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 49.59 | 63.43 | 68.30 | 76.00 | 78.10 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Ablation on multi-round distillation on PERsD-combined with CodeGen-mono
    6B as backbone'
  prefs: []
  type: TYPE_NORMAL
- en: 'After finetuning the student model with the personalised distillation data,
    can we perform another round of personalised distillation, on the new model? We
    show such an ablation study in Table [8](#S5.T8 "Table 8 ‣ 5.4 Multi-round Distillation
    ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation"). Encouragingly, we find PERsD-combined
    round-2 generally outperforms PERsD-combined round-1 by a modest margin. This
    improvement provides further evidence of the benefits of personalized learning,
    even when applied to models trained with personalized distillation. These findings
    suggest the intriguing possibility of an online or active version of personalized
    distillation, where data collection and model training occur simultaneously to
    ensure each batch is fully personalized and has higher sample efficiency. However,
    we will leave such intriguing exploration for future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Utilizing feedback for multi-step Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To better understand the role of execution feedback during training and multi-step
    inference, we show an ablation study in Table [9](#S5.T9 "Table 9 ‣ 5.5 Utilizing
    feedback for multi-step Inference ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"), where
    we compare PERsD-combine with a specific variant (PERsD-combine*) that excludes
    feedback during both training and inference. we observed that PERsD-combine* performs
    comparably to PERsD-combine on HumanEval and slightly better on MBPP for 1-step
    inference. However, for 2-step inference, PERsD-combine* consistently underperforms
    PERsD-combine. This result aligns well with our expectations that code-rectification
    needs the execution feedback to guide the refinement.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Step | HumanEval |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine* | 33.29 | 45.47 | 50.90 | 62.87 | 68.29 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine* | 34.59 | 49.54 | 55.59 | 67.27 | 71.95 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine* | 44.76 | 56.95 | 60.85 | 68.67 | 71.57 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD-combine* | 47.83 | 61.28 | 65.54 | 73.03 | 75.49 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Ablation on removing execution feedback with CodeGen-mono 6B as backbone.
    PERsD-combine* denotes combined personalised distillation without execution feedback
    in input prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Cross-Model Personalised Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To investigate whether personalised distillation data of one model can be benefical
    to another, we conduct an ablation in Table [10](#S5.T10 "Table 10 ‣ 5.6 Cross-Model
    Personalised Distillation ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") by using
    PERsD-combined data of CodeGen-mono-6B to train CodeGen-mono-16B. The results
    show that such cross-model persionalised data do not perform as well as real personalised
    data: leading to a consistent performance drop by a large margin. This finding
    reinforces our notion that learning data should be tailored to the specific student
    model, as personalized data suitable for one model may not necessarily benefit
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Model | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Step | HumanEval |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-6B | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B* | 32.99 | 47.81 | 54.58 | 69.31 | 73.98 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B | 36.40 | 53.57 | 60.81 | 74.64 | 79.88 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineCodeGen-mono-6B | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B* | 35.85 | 51.31 | 58.23 | 74.02 | 76.60 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B | 37.74 | 55.80 | 63.37 | 77.14 | 81.10 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MBPP |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-6B | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B* | 43.24 | 60.14 | 65.19 | 72.31 | 74.19 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B | 51.44 | 66.45 | 71.64 | 80.62 | 82.93 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineCodeGen-mono-6B | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B* | 48.12 | 65.31 | 70.02 | 76.60 | 78.70 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono-16B | 56.44 | 71.31 | 76.43 | 84.39 | 86.76 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Ablation on cross-model personalised distillation with PERsD-combined.
    CodeGen-mono-16B* means distillation data is from CodeGen-mono-6B.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Comparison with other Feedback-based Code Generation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Comparison with ILF Chen et al. ([2023a](#bib.bib3)): In order to compare with
    ILF, one of our closest related work, we experiment on a separate setting: starting
    with full MBPP dataset (974 tasks) and use Task-Ids 11-111 as test split and remaining
    863 as training data. On the training set, our student model CodeGen-6B (same
    as ILF) generated wrong attempts on 562 tasks, which were shown to ChatGPT along
    with the task instruction and execution error feedback to eventually collect 288
    valid personalized code rectification labels.'
  prefs: []
  type: TYPE_NORMAL
- en: The original MBPP text-to-code data and this collected personalized code-refinement
    data for the 288 tasks
  prefs: []
  type: TYPE_NORMAL
- en: '|   | MBPP Test Set |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Cost | Pass@1 | Pass@10 |'
  prefs: []
  type: TYPE_TB
- en: '| ILF | >4K$ | 36 | 68 |'
  prefs: []
  type: TYPE_TB
- en: '| PERsD | 0.65$ | 46.8 | 67.4 |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 0.65$ | 41.8 | 66.8 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 0.65$ | 47.8 | 64.8 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Comparison with ILF'
  prefs: []
  type: TYPE_NORMAL
- en: 'respectively form the finetuning data $\mathcal{D}_{\text{code}}$ to train
    PERsD-combined. Our experimental results in Table [11](#S5.T11 "Table 11 ‣ 5.7
    Comparison with other Feedback-based Code Generation Models ‣ 5 Experimental Results
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation") show that all PERsD-variants significantly outperform ILF
    by 11.8% at pass@1 at a cost 1e-4 times lower than ILF, thus showcasing the lack
    of scalability of ILF-style models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison with Self-Edit: Since Self-Edit Zhang et al. ([2023](#bib.bib26))
    uses a trainable CodeGen-350M code editor model and a frozen code-generation model,
    our experimental setup is not directly comparable with theirs. However, our InpD-refine
    and InpD-combined models can actually be considered as very close counterparts
    to a version of Self-Edit with shared a code-generation and code-refinement model
    and CodeGen-6B backbone. The consistent performance improvement of the personalized
    distillation models over the input-distilled ones across the board, alludes towards
    the prospect that PERsD-models are indeed more effective than Self-Edit style
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Comparison with SOTA Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fianlly, we compare PERsD-combine models with open-source and close-sourced
    state-of-the-art models on HumanEval in Table [12](#S5.T12 "Table 12 ‣ 5.8 Comparison
    with SOTA Models ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation").We find that PERsD-combine
    methods can significantly improve the backbone model, with a performance gain
    of 6.2 points for CodeGen-mono 6B (8.4% error reduction), 5.9 points for CodeGen-mono
    16B (8.3% error reduction) and 12.2 points for StarCoder (18.4% error reduction).
    Moreover, StarCoder with PERsD-combined, outperforms other open-sourced models
    except WizardCoder. Note that our model ues 5K data examples while WizardCoder
    uses 78K. As mentioned in [Section 2.1](#S2.SS1 "2.1 Distillation from ChatGPT
    ‣ 2 Related Work ‣ Personalised Distillation: Empowering Open-Sourced LLMs with
    Adaptive Learning for Code Generation"), WizardCoder is an orthogonal approach
    that can be integrated into personalised distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Model size | Pass@1 | Pass@10 | Pass@100 |'
  prefs: []
  type: TYPE_TB
- en: '| Closed-source models |'
  prefs: []
  type: TYPE_TB
- en: '| LaMDA | 137B | 14.0 | - | 47.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM | 540B | 26.2 | - | 76.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Codex | 12B | 28.8 | 46.8 | 72.3 |'
  prefs: []
  type: TYPE_TB
- en: '| code-cushman-001 | - | 33.5 | 54.3 | 77.4 |'
  prefs: []
  type: TYPE_TB
- en: '| code-davinci-002 | - | 47.0 | 74.9 | 92.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | - | 48.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| phi-1 | 1.3B | 50.6 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | - | 67.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Open-source models |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGeeX | 13B | 22.9 | 39.6 | 60.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA | 65B | 23.7 | - | 79.3 |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder | 15B | 33.6 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono | 6B | 26.1 | 42.3 | 65.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono | 16B | 29.3 | 49.9 | 75.0 |'
  prefs: []
  type: TYPE_TB
- en: '| InstructCodeT5+ | 16B | 35.0 | 54.5 | 77.9 |'
  prefs: []
  type: TYPE_TB
- en: '| WizardCoder | 15B | 57.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineCodeGen-mono (PERsD-combined) | 6B | 33.8 | 50.0 | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGen-mono (PERsD-combined) | 16B | 36.4 | 60.8 | 79.9 |'
  prefs: []
  type: TYPE_TB
- en: '| StarCoder (PERsD-combined) | 15B | 45.8 | 68.3 | 82.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Results of *pass@k*(%) on HumanEval'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduced personalized distillation as a method for collecting
    customized labeled data that adapts to the capacity of student models, resulting
    in more effective learning. We demonstrated the advantages of personalized distillation
    over standard distillation in the field of code generation, achieving superior
    performance on both the HumanEval and MBPP datasets. Through comprehensive ablation
    studies, we confirmed that personalized distillation leads to higher data quality,
    benefits from multi-round distillation, and enables models to leverage execution
    feedback for self-rectification. We believe personalized distillation represents
    an exciting step towards better distillation of closed-source LLMs to open-source
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we discuss some limitations of this paper and future directions
    to make it more valuable:'
  prefs: []
  type: TYPE_NORMAL
- en: On Data Scale
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a fair comparison, we have conducted all experiments based on the same
    10K $\mathcal{D}_{\textsc{StanD}}$ are of size 2-3K as shown in Table [3](#S4.T3
    "Table 3 ‣ 4.2 Pretraining Data Construction ‣ 4 Experimental Setup ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation").
    However, as we have proven personalised distillation supports more effective and
    efficient learning, it is intriguing to investigate how well does personalised
    distillation scale with the data size. For example, if we scale personalised distillation
    data to 50K, how much more performance gain will PERsD methods receive compared
    to InpD and StanD with the scaling of data size.'
  prefs: []
  type: TYPE_NORMAL
- en: Online Personalised Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As discussed in [Section 5.4](#S5.SS4 "5.4 Multi-round Distillation ‣ 5 Experimental
    Results ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation"), conducting a second round personalised distillation
    continues to improve a student model that is already trained with PERsD-combine.
    Such observation suggests the potential of an online version of personalised distillation,
    which collects a batch of personalised data on-the-fly with the teacher model,
    after each optimization step during finetuning. As we have proven that true personalised
    data is more beneficial than standard data or cross-model personalised data ([Section 5.6](#S5.SS6
    "5.6 Cross-Model Personalised Distillation ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")),
    such online personalised distillation will in-principle maximally benefit from
    personalised distillation, as each batch of training data is fully tailored to
    the student model.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V.
    Le, and Charles Sutton. 2021. Program synthesis with large language models. *CoRR*,
    abs/2108.07732.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code alpaca: An instruction-following
    llama model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    [Improving code generation by training with natural language feedback](http://arxiv.org/abs/arXiv%20preprint%20arXiv:2303.16749).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. Evaluating large language models trained on code. *CoRR*,
    abs/2107.03374.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023b) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023b. Teaching large language models to self-debug. *CoRR*, abs/2304.05128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023) Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang.
    2023. Lion: Adversarial distillation of closed-source large language model. *CoRR*,
    abs/2305.12870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp
    Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi
    Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim
    Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire
    Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer
    Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva
    Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023a.
    Starcoder: may the source be with you! *CoRR*, abs/2305.06161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Zihao Li, Zhuoran Yang, and Mengdi Wang. 2023b. Reinforcement
    learning with human feedback: Learning dynamic choices via pessimism. *CoRR*,
    abs/2305.18438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain
    of hindsight aligns language models with feedback. *CoRR*, abs/2302.02676.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering
    code large language models with evol-instruct. *CoRR*, abs/2306.08568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. *CoRR*,
    abs/2303.17651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open
    large language model for code with multi-turn program synthesis. *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization:
    Your language model is secretly a reward model. *CoRR*, abs/2305.18290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter
    models. In *SC*, page 20\. IEEE/ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roberts-Mahoney et al. (2016) Heather Roberts-Mahoney, Alexander J. Means,
    and Mark J. Garrison. 2016. [Netflixing human capital development: personalized
    learning technology and the corporatization of k-12 education](https://doi.org/10.1080/02680939.2015.1132774).
    *Journal of Education Policy*, 31(4):405–420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shemshack and Spector (2020) Atikah Shemshack and Jonathan Michael Spector.
    2020. A systematic literature review of personalized learning terms. *Smart Learning
    Environments*, 7(1):1–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](http://arxiv.org/abs/2303.11366).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct:
    Aligning language model with self generated instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning
    to self-correct. *CoRR*, abs/2211.00053.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-art natural language
    processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023a) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language
    models to follow complex instructions. *CoRR*, abs/2304.12244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023b) Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b.
    Baize: An open-source chat model with parameter-efficient tuning on self-chat
    data. *arXiv preprint arXiv:2304.01196*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023.
    Self-edit: Fault-aware code editor for code generation. *CoRR*, abs/2305.04087.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Details in Multi-step Model Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the docstrings are ill-formated in HumanEval, we write a simple rule-based
    parsing code snippet to extract its seen unit test cases. On average per task,
    there is 2 seen unit test cases and 4.2 unseen unit test cases. The overlap between
    seen and unseen tests is 11.33%. For MBPP, since conventionally the instruction
    prompt is constructed by taking the task description and example usages (from
    the unit test cases) as part of the doc-string, we consider all the unit test
    cases to be "seen" and use all of them for multi-step inference.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B ChatGPT Prompt Template for Personalised Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Figure [3](#A2.F3 "Figure 3 ‣ Appendix B ChatGPT Prompt Template for Personalised
    Distillation ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation"), we show the prompt template we use to query ChatGPT
    for personalised refinement. For each task example, with task instruction $t$,
    we query ChatGPT API with two turn conversation history.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first turn, we use the template in Figure [3(a)](#A2.F3.sf1 "Figure
    3(a) ‣ Figure 3 ‣ Appendix B ChatGPT Prompt Template for Personalised Distillation
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation") and replace <<TASK>>, <<HEADER>> with the actual task instruction
    $t$ is included as first turn’s assistant output. For the second turn, we use
    the template in Figure [3(b)](#A2.F3.sf2 "Figure 3(b) ‣ Figure 3 ‣ Appendix B
    ChatGPT Prompt Template for Personalised Distillation ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") and
    replace <<CODE>>, <<ERROR>> with the student model’s attempt and its execution
    feedback. This is added to second turn’s user input and we query ChatGPT with
    the constructed converstaion history to get second turn’s assistant output as
    personalised code refinement.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7126bd002ce9d8e563740bcc7dca117e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Turn-1 Prompt Template
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/83572f4ce8a60c494c00032ea6824120.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Turn-2 Prompt Template
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Prompt templates to query personalised refinement. Top(a): prompt
    template for first turn conversation, Botton(b): prompt template for second turn
    conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompt Template for Code Refinement Finetuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [4](#A3.F4 "Figure 4 ‣ Appendix C Prompt Template for Code Refinement
    Finetuning ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows the refinement template $T_{\text{refine}}$
    introduced in [Section 3.2](#S3.SS2 "3.2 Personalised Distillation ‣ 3 Method
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation")), which is used to construct input prompt for code refinement
    finetuning. we replace <<TASK>> with task instruction, <<CODE>> with the initial
    wrong attempt from student, <<ERROR>> with the execution feedback, and <<HEADER>>
    with function header extracted from task instruciton.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3572377187693d1a1db3580522a37ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Prompt template for code refinement finetuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Details in Data Overlap Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes the detailed procedures to conduct train-test data overlap
    analysis. The objective is to assess the extent of data leakage in the test datasets
    originating from our self-constructed pretraining corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we have performed exact string match and found no data leakage in any
    test data (HumanEval/MBPP).
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the semantic similarity between training/test tasks, we did the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each task in the test (MBPP/HumanEval) we retrieve two closest training
    tasks (based on cosine similarity of starcoder embedding & tf-idf vectors of task
    description).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use gpt-3.5-turbo-16k to identify whether there is a data leak between a
    train and test instance by classifying the pair into (“leak”, “somewhat similar”,
    “somewhat not similar”, “not related”). We use a prompt with instructions and
    manually created few-shot examples and ask gpt-3.5 to generate the reasoning and
    categorization. We manually examined several examples per category to ensure the
    reasoning and judgment is done correctly and consistently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map the similarity categories to 0-1 similarity-score (“leak” -> 1, “somewhat
    similar” -> 0.75, “somewhat not similar” -> 0.25, “not related” -> 0) and show
    the mean score and % of cases classified as “leak”. Note that StanD & PERsD have
    10K & 3K training data respectively so their scores are different.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix E Results in MBPP-Cleaned
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Appendix D](#A4 "Appendix D Details in Data Overlap Analysis ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"),
    we find 55 data instances that are potentially leaked (with similarity score =
    1) in MBPP test data. In this section, we construct a new MBPP-Cleaned dataset,
    where the leaked data points are removed (originally 306 problems → 251 problems
    after filtering). The results on this new MBPP-Cleaned dataset is shown in Table
    [13](#A5.T13 "Table 13 ‣ Appendix E Results in MBPP-Cleaned ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"). From
    the results, we can see for setting CodeGen-mono-16B, pass@1, PERsD becomes almost
    on-par with StanD (from a gap of -1.21 to -0.17). For the rest of 15/16 settings
    on PERsD comparing with StanD, its average margin is increased from 4.8 points
    to 5.9 points. Besides, PERsD-refine on MBPP-Cleaned shows more consistent and
    sizable improvements over InpD-refine, with an average edge of +0.86 for 1 step
    inference, and +1.91 for two step inference. Overall, with overlapped test data
    removed, PERsD methods show even larger advantages compared to StanD or InpD methods.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Backbone as CodeGen-mono-6B
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MBPP-Cleaned |  |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 10K | 37.51 | - | 50.89 | - | 55.15 | - | 58.87 | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInpD | 3.3K | 38.80 | - | 53.91 | - | 58.47 | - | 62.73 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 3.3K | 37.58 | 42.95 | 57.65 | 62.29 | 63.52 | 67.79 | 67.92 |
    71.96 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 6.5K | 38.11 | 43.01 | 52.69 | 58.32 | 57.36 | 62.75 | 61.19
    | 66.18 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | 3.3K | 41.30 | - | 56.20 | - | 61.86 | - | 67.53 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 3.3K | 43.86 | 47.73 | 59.33 | 64.41 | 65.19 | 69.95 | 69.62 |
    74.33 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 6.5K | 38.86 | 43.75 | 52.78 | 57.04 | 57.35 | 61.78 | 61.52
    | 66.19 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: (b) Backbone as CodeGen-mono-16B
  prefs: []
  type: TYPE_NORMAL
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MBPP-Cleaned |  |'
  prefs: []
  type: TYPE_TB
- en: '| StanD | 10K | 43.10 | - | 57.53 | - | 62.92 | - | 68.12 | - |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlineInpD | 2.8K | 40.64 | - | 53.88 | - | 58.82 | - | 62.88 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 2.8K | 43.67 | 49.60 | 63.14 | 68.21 | 69.27 | 73.28 | 73.36 |
    76.85 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 5.6K | 41.63 | 47.77 | 54.74 | 62.24 | 59.67 | 67.33 | 63.75
    | 71.57 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashlinePERsD | 2.8K | 42.93 | - | 62.40 | - | 68.90 | - | 74.10 | - |'
  prefs: []
  type: TYPE_TB
- en: '| -refine | 2.8K | 47.73 | 52.63 | 63.62 | 69.21 | 69.84 | 75.17 | 74.90 |
    79.69 |'
  prefs: []
  type: TYPE_TB
- en: '| -combined | 5.6K | 46.33 | 51.67 | 63.46 | 68.65 | 69.49 | 74.26 | 74.53
    | 78.83 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Comparing performance of PERsD models to StanD & InpD on MBPP-Cleaned'
  prefs: []
  type: TYPE_NORMAL
