- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:05:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.01032](https://ar5iv.labs.arxiv.org/html/2406.01032)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  prefs: []
  type: TYPE_NORMAL
- en: Junjie Xu, Zongyu Wu, Minhua Lin, Xiang Zhang, Suhang Wang
  prefs: []
  type: TYPE_NORMAL
- en: The Pennsylvania State University
  prefs: []
  type: TYPE_NORMAL
- en: '{junjiexu, zongyuwu, mfl5681, xzz89, szw494}@psu.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent progress in Graph Neural Networks (GNNs) has greatly enhanced the ability
    to model complex molecular structures for predicting properties. Nevertheless,
    molecular data encompasses more than just graph structures, including textual
    and visual information that GNNs do not handle well. To bridge this gap, we present
    an innovative framework that utilizes multimodal molecular data to extract insights
    from Large Language Models (LLMs). We introduce GALLON (Graph Learning from Large
    Language Model Distillation), a framework that synergizes the capabilities of
    LLMs and GNNs by distilling multimodal knowledge into a unified Multilayer Perceptron
    (MLP). This method integrates the rich textual and visual data of molecules with
    the structural analysis power of GNNs. Extensive experiments reveal that our distilled
    MLP model notably improves the accuracy and efficiency of molecular property predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, Graph Neural Networks (GNNs) have demonstrated exceptional
    prowess in representing learning on graph-structured data [[20](#bib.bib20), [41](#bib.bib41),
    [34](#bib.bib34), [40](#bib.bib40)]. Within the domain of chemistry, GNNs have
    been notably effective in predicting molecular properties [[13](#bib.bib13), [38](#bib.bib38)],
    a task critical for advancements in various domains, such as drug discovery [[12](#bib.bib12)]
    and materials science [[27](#bib.bib27)]. By conceptualizing molecules as graphs,
    GNNs manage to capture the nuanced spatial and chemical relationships that dictate
    molecular behavior. This unique ability has positioned GNNs as a leading tool
    in chemistry, enabling more accurate and efficient prediction models that surpass
    traditional methodologies [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the strengths of GNNs in processing graph-structured data, their application
    scope reveals limitations when encountering molecular data in forms other than
    graph structures. Molecules, in their essence, exhibit multimodal characteristics
    that can also be represented in various forms, including textual Simplified Molecular-Input
    Line-entry System (SMILES) strings [[37](#bib.bib37)] and visual molecular diagrams.
    An example of this multimodal representation is illustrated on the left side of
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM and GNN are Complementary: Distilling
    LLM for Multimodal Graph Learning"). These modalities provide comprehensive information
    toward the understanding of molecules. SMILES strings offer a compact and linear
    textual representation of a molecule’s structure, which some textual models can
    learn. Molecular diagrams provide a visual and intuitive depiction of molecular
    structures, making specific structures and functional groups, like benzene rings,
    more identifiable. Additionally, a graph adjacency matrix encapsulates detailed
    information about the connections between nodes within the molecule. Therefore,
    each modality provides complementary advantages to each other. However, while
    GNNs excel in node features and graph structures, they falter in processing and
    extracting valuable information from other modalities. This limitation underscores
    a significant gap in the current framework, highlighting the need for models that
    can understand molecules’ modalities from all aspects and learn effective representations.'
  prefs: []
  type: TYPE_NORMAL
- en: In parallel with the advancements in GNNs, the evolution of Large Language Models
    (LLMs) [[25](#bib.bib25), [3](#bib.bib3), [44](#bib.bib44)] and LLMs with vision [[2](#bib.bib2),
    [30](#bib.bib30), [10](#bib.bib10), [26](#bib.bib26)] has marked a new age in
    machine learning, characterized by their exceptional skill in parsing and interpreting
    textual and visual data. For example, GPT4V has shown the ability to answer questions
    about molecular diagrams [[1](#bib.bib1)]. However, recent works show that LLMs
    exhibit a shortcoming in their ability to process graph-structured data [[5](#bib.bib5),
    [21](#bib.bib21)], an area where GNNs excel. Yet, their proficiency in dealing
    with molecular data represented as SMILES strings and diagrams suggests a valuable
    complementary role to GNNs. Meanwhile, through the huge amount of training data,
    LLMs have their understanding of the real world and have prior knowledge about
    the SMILES string and chemical molecules. Based on these observations, we aim
    to incorporate the knowledge from LLM and make LLMs and GNNs complementary when
    dealing with different modalities. One potential way is to fuse LLM and GNN by
    distilling them into a new model. However, querying LLM is both computationally
    and financially demanding, making it impractical to apply to each molecule when
    handling large-scale data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b7730df586c38bdab788d758c1955438.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The framework of GALLON (Graph Learning from Large Language Model
    Distillation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we propose to distill LLM and GNN to a smaller Multilayer Perceptron
    (MLP). On the one hand, the MLP is trained with distillation from both LLM and
    GNN, incorporating knowledge from the modalities of GNN and LLM where they excel.
    On the other hand, the MLP offers greater efficiency and cost-effectiveness during
    the inference process, eliminating the need for querying the LLMs for each single
    molecule. Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology:
    GALLON ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology:
    GALLON ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    show the time and size efficiency of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: However, several challenges hinder the distillation process. First, while LLMs
    have demonstrated their power, we empirically find that they struggle with direct
    molecular property prediction. This difficulty arises not only because such classification
    or regression tasks challenge text-generation models, but also due to the lack
    of domain-specific training in LLMs. To mitigate this, we finetune a smaller Language
    Model (LM) to serve as the encoder for the outputs of the LLM. This approach allows
    the LM to specialize in the domain relevant to the prediction task, ensuring that
    the learned representations are more effective for the knowledge distillation
    process needed for molecular property prediction. Secondly, most methods for graph
    distillation [[31](#bib.bib31), [42](#bib.bib42)] have concentrated on classification
    tasks, where the goal is to align the predicted class distribution between the
    student and teacher models. However, graph regression remains a vital and common
    task, particularly for predicting molecular properties. Distilling directly from
    labels in regression tasks presents challenges because the label is a scalar,
    which provides less information compared to the distribution of labels in classification
    tasks. Therefore, we employ a mapping function that maps the embeddings of the
    teacher and student models to a latent space, where we perform the distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In light of these considerations, we propose utilizing multimodal information
    from molecular data, i.e., SMILES strings, molecular diagrams, graph structures,
    and node features, to query the Large Language Model (LLM) and obtain a detailed
    description of the molecule. This description will then serve as the knowledge
    base for distillation into a smaller MLP model. Furthermore, we propose a novel
    approach GALLON (Graph Learning from Large Language Model Distillation) that synergizes
    the advantages of GNNs and LLMs through a distillation process into a Multilayer
    Perceptron (MLP) model, which makes the inference process more efficient with
    state-of-the-art performance. By distilling the capabilities of GNNs in handling
    graph structures and the adeptness of LLMs in processing textual and visual information,
    we create a unified framework for learning more efficient representations across
    the multimodality of molecular data. To summarize, our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose utilizing multimodal molecular data to learn representations and
    extract prior knowledge from powerful and pretrained large language models by
    leveraging their multimodality capabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce a framework that synergizes GNNs and LLMs, leveraging their complementary
    strengths. This framework distills the advantages of GNN and LLM into an MLP,
    aiming to capture the most effective representations for molecular structures.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive experiments to demonstrate the superiority of our approach
    in distilling GNN and LLM knowledge into an MLP, which outperforms both GNNs and
    LLMs across various datasets, achieving greater efficiency and an even smaller
    model size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs for Graphs. Recent studies have shown significant interest in applying
    LLMs to graph data, marking a pivotal shift in graph learning research. These
    models have been adapted to address challenges such as encoding node and edge
    information and preserving topological structures. TAPE [[15](#bib.bib15)] uses
    LLMs to generate explanations for nodes, which are then used as augmented features
    to train GNNs. GraphLLM [[4](#bib.bib4)] encodes graphs into text for LLM prediction.
    Das et al. [[7](#bib.bib7)] explore the integration of graph data with LLMs and
    the influence of graph multimodalities. CaR [[24](#bib.bib24)] uses molecular
    SMILES strings to obtain captions from LLMs, which are then input into another
    language model for finetuning. Surveys by Li et al. [[21](#bib.bib21)] and Chen
    et al. [[5](#bib.bib5)] provide a comprehensive understanding of LLM performance
    on graphs, dividing them into roles such as enhancer, predictor, and alignment.
    In contrast to prior works, we use LLMs as teachers in the distillation process,
    leveraging their prior knowledge about molecules to enhance the learning of the
    student model.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Knowledge Distillation. Graph Knowledge Distillation constructs a smaller
    but efficient model by extracting more knowledge from data, aiming for the compressed
    model and improved performance [[31](#bib.bib31)]. Various works distill GNNs
    onto various models. For example, LSP [[45](#bib.bib45)] distills a teacher’s
    GNN to a student’s GNN by minimizing the distance of local structure between them.
    T2GNN [[18](#bib.bib18)] employs MLP and GNN as the feature teacher and structure
    teacher to let a student GNN learn from them. NOSMOG [[32](#bib.bib32)] and GLNN [[46](#bib.bib46)]
    show that distilled MLP also have the ability to outperform GNN methods. Though
    abundant works adopt different strategies for graph knowledge distillation, most
    of them do not focus on the molecule data with graph classification tasks, and
    they cannot deal with the multimodal information in the molecule data.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Learning for Molecules. Several studies have focused on leveraging
    single modalities for molecular representation learning. ChemBERTa [[6](#bib.bib6)]
    uses SMILES strings to pretrain a BERT model, treating molecules purely as text.
    GNN methods [[17](#bib.bib17), [36](#bib.bib36), [35](#bib.bib35)] enhance learning
    by combining features from individual atoms and their connections. Recent efforts
    have explored multimodal approaches. MolT5 [[9](#bib.bib9)] pretrains on SMILES
    strings and molecule captions, while MoMu [[29](#bib.bib29)] merges molecule graphs
    with natural language through contrastive learning. MolFM [[23](#bib.bib23)] integrates
    structures, texts, and knowledge graphs for a comprehensive understanding of molecular
    properties. In this work, GALLON incorporates SMILES strings, molecular diagrams,
    and molecule graphs to train effective molecular representations and distill the
    knowledge to an MLP.
  prefs: []
  type: TYPE_NORMAL
- en: '3 Methodology: GALLON'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first define notations and formally define the problem.
    We then give an overview of GALLON followed by detailed descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Notation and Problem Definition. Let $\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{X},\mathcal{S},\mathcal{I}\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overview of GALLON. An illustration of the framework is shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM and GNN are Complementary: Distilling LLM for
    Multimodal Graph Learning"), which has three main components: 1) Input the multimodal
    data into the LLM to extract knowledge from it. 2) Fine-tuning a smaller LM to
    encode the text outputs of the LLM and pretraining a GNN. 3) Distilling the knowledge
    extracted from both the GNN and LLM into an MLP. It is important to note a limitation
    that using MLP with atom input without a graph structure cannot differentiate
    isomers—compounds with the same formula but different atom arrangements. However,
    the node features of the molecular dataset include essential structure information
    (e.g., degree, ring, aromatic, chirality). These structural features enable our
    method to classify compounds effectively, despite the absence of a graph structure,
    by leveraging differences in properties such as chirality and ring structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Extracting Knowledge from LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b1e385b61c670e18fed136e4d08e638.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An example prompt for a molecule of the Freesolv dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Information. As stated previously, LLMs excel at processing text
    information and are trained on diverse datasets, enabling them to gain extensive
    knowledge about interpreting molecules and SMILES strings. Consequently, we utilize
    the SMILES string $\mathcal{S}$ of the molecule into the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Construction. To enable the LLM to act like a chemistry expert and generate
    useful information about the input molecule, it is crucial to design an effective
    prompt. Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Extracting Knowledge from LLMs ‣ 3 Methodology:
    GALLON ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    shows an example prompt we designed for a molecule in the Freesolv Dataset. A
    prompt is composed of three parts: (1) The general prompt, which remains the same
    across all datasets; (2) Dataset-specific description that describes the characteristics
    of the dataset and the property we want to predict; (3) Molecule information containing
    the SMILES string, molecular diagram, and graph structure. As shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1 Extracting Knowledge from LLMs ‣ 3 Methodology: GALLON ‣ LLM and
    GNN are Complementary: Distilling LLM for Multimodal Graph Learning"), we convert
    the graph structure into texts by describing the atom name and bond type for each
    edge. This designed prompt provides LLM with the general goal, the prediction
    task, and the multimodal molecule information. Then we use the generated prompt
    $\mathcal{P}$, the process can be formally written as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{R}_{i}=\text{LLM}(\mathcal{P}_{i},\mathcal{E}_{i},\mathcal{S}_{i},\mathcal{I}_{i}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{E}_{i},\mathcal{S}_{i},\mathcal{I}_{i}$ are the graph edges,
    SMILE string, and molecular diagram respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pretraining and Finetuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the response $\mathcal{R}$ is written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h^{\text{LM}}_{i}=\text{LM}(\mathcal{R}_{i},\mathcal{S}_{i})\in\mathbb{R}^{H},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $H$ on top of the embedding to predict the molecular property as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}^{\text{LM}}_{i}=\mathcal{T}_{\text{LM}}(h^{\text{LM}}_{i})\in\mathbb{R}^{d},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $d$ with cross-entropy for classification and Root Mean Square Error (RMSE)
    for regression.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the knowledge distillation from graphs, we pretrain a GNN with
    the graph structure and node features. Specifically, for each molecule $\mathcal{G}_{i}$,
    we first adopt GNN with a pooling layer to get graph representation as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h^{\text{GNN}}_{i}=\text{GNN}(\mathbf{X}_{i},\mathcal{E}_{i})\in\mathbb{R}^{H},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: We then add a transformation layer $\mathcal{T}_{\text{GNN}}$ to predict the
    molecular property as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}^{\text{GNN}}_{i}=\mathcal{T}_{\text{GNN}}(h^{\text{GNN}}_{i})\in\mathbb{R}^{d},$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Similarly, $\mathcal{T}_{\text{GNN}}$ is typically implemented using a linear
    layer for regression, and a linear layer followed by a Softmax function for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f8c99e79c076979a732bda2aab64f2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: ROCAUC vs log of inference time (ms) on the BACE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f608c71a41ba876956a2920c70d71cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: ROCAUC vs log of number of parameters on the BACE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Knowledge Distillation to MLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the learned representations of LLM and GNN, we propose to distill their
    knowledge into an MLP for several compelling reasons: (i) operating an LLM is
    both computationally and financially burdensome. In large-scale datasets, querying
    each molecule through the LLM to extract descriptions and prior knowledge is impractical.
    Through distillation, we can embed the prior knowledge from the LLM into the MLP,
    allowing us to use the MLP for efficient inference; and (ii) many studies [[32](#bib.bib32),
    [43](#bib.bib43), [46](#bib.bib46), [47](#bib.bib47)] have demonstrated that with
    effective distillation, an MLP can achieve performance on par with, or even superior
    to, a GNN, while significantly reducing inference time. Figures [4](#S3.F4 "Figure
    4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") and [4](#S3.F4 "Figure 4 ‣ 3.2
    Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") demonstrate that GALLON achieves
    state-of-the-art performance with less time and a smaller model size.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the node attributes in the datasets are not simply atomic numbers
    but include a wide range of information (e.g. chirality, degree, formal charge,
    hydrogens, radical electrons, hybridization, aromaticity, and ring membership).
    These comprehensive node features, including some graph structural information,
    make it feasible to achieve good prediction performance using only the node attributes
    with an MLP. However, these studies primarily focus on distilling from GNN to
    MLP and do not incorporate LLM. Therefore, in this section, our goal is to train
    an MLP via distillation, enabling it to match or exceed the performance of either
    GNN or LLM alone. Specifically, for a molecule $\mathcal{G}_{i}$ as input followed
    by average pooling to get molecule representation as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $n_{i}$ to get prediction
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}^{\text{MLP}}_{i}=\mathcal{T}_{\text{MLP}}(h^{\text{MLP}}_{i})\in\mathbb{R}^{d}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: During the training, the loss is calculated between the prediction label and
    ground truth,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{0}=\mathcal{L}_{pred}(\hat{y}^{\text{MLP}},y),$ |  | (8)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{pred}$ is cross entropy in classification tasks and distance-based
    RMSE in regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To better distill the knowledge from LLM and GNN, we use two forms of distillation,
    i.e., representation distillation and label distillation.
  prefs: []
  type: TYPE_NORMAL
- en: Label Distillation. Label distillation aims to let the student’s prediction
    mimic the distribution of the teacher’s prediction, which can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Representation Distillation. In regression tasks, the target label is a scalar
    value, and calculating the distance between two scalar values yields limited information
    for distillation purposes. To address this challenge, we propose utilizing representation
    distillation to align the student’s learned representations with those of the
    teacher. Given that LLM and the GNN are trained independently, their respective
    representations, $h^{\text{LM}}$. Directly aligning these representations could
    be problematic due to the mismatch in feature spaces. To overcome this, we transform
    them into a common latent space. Subsequently, we facilitate the distillation
    of knowledge between the student and teachers’ models within this shared latent
    space, which can be mathematically represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where both $\mathcal{L}_{LM}^{RD}$ are implemented by RMSE. For simplicity,
    in this paper, we adopt label distillation for classification tasks and representation
    distillation for regression tasks, but both of them can be applied to any task.
    With the loss function for knowledge distillation, the final objective functions
    for training MLP for classification and regression are given as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}^{class}=\mathcal{L}^{0}+\mathcal{L}^{LD},\qquad\mathcal{L}^{reg}=\mathcal{L}^{0}+\mathcal{L}^{RD}.$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present experiments to evaluate the effectiveness of the
    proposed GALLON framework, addressing the following research questions: RQ1: How
    does the proposed GALLON framework perform compared to GNN and NLP models? RQ2:
    Do both GNN and LLM contribute to the final results? RQ3: To what extent does
    multimodality enhance LLM predictions? RQ4: What is the influence of different
    LLM models on performance?'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets. To evaluate the performance of GALLON for molecule property prediction,
    we adopt seven widely used datasets from MoleculeNet [[38](#bib.bib38)], including
    four graph classification datasets BACE, BBBP, Clintox, HIV, and three regression
    datasets ESOL, Freesolv, and Lipophilicity. In this paper, we use ROCAUC for classification
    and RMSE for regression as evaluation metrics. The detailed description and statistics
    of the datasets can be found in Appendix [A.1](#A1.SS1 "A.1 Dataset Details ‣
    Appendix A Experimental Details ‣ LLM and GNN are Complementary: Distilling LLM
    for Multimodal Graph Learning") and Table [5](#A1.T5 "Table 5 ‣ A.1 Dataset Details
    ‣ Appendix A Experimental Details ‣ LLM and GNN are Complementary: Distilling
    LLM for Multimodal Graph Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines. We compare our model with (i) GNN models, including GCN [[20](#bib.bib20)],
    ChebNet [[8](#bib.bib8)], GraphSAGE [[14](#bib.bib14)], GIN [[41](#bib.bib41)],
    MoleBERT [[39](#bib.bib39)], and (ii) NLP models, including ECFP4-MLP [[28](#bib.bib28)],
    ChemBERTa [[6](#bib.bib6)], SMILES-Transformer [[16](#bib.bib16)]. The descriptions
    of the baselines are in Appendix [A.2](#A1.SS2 "A.2 Baselines ‣ Appendix A Experimental
    Details ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Settings. As suggested by Wu et al. [[38](#bib.bib38)], we adopt scaffold splitting
    for molecule datasets, which is widely adopted in the molecule domain. There are
    two kinds of scaffold splitting: scaffold splitting and random scaffold splitting.
    The difference is introduced in [A.3](#A1.SS3 "A.3 Settings ‣ Appendix A Experimental
    Details ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning").
    We adopt the ratio of 80%/10%/10% for train/validation/test sets. We use GCN as
    the GNN backbone and Roberta [[22](#bib.bib22)] as the smaller Language Model
    (LM) throughout all experiments. For the HIV and Lipophilicity datasets, we query
    Claude3-Haiku, and for other datasets, we use GPT-4V. We search hyperparameters
    $\alpha$ based on the performance on a validation set to achieve optimal results.
    For a fair comparison, we configured all backbone GCNs and distilled MLPs with
    3 layers and a hidden dimension of 32\. We conduct the experiments on 10 seeds
    and report the average performance with standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Distillation Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.2 Distillation Results ‣ 4 Experiments ‣ LLM
    and GNN are Complementary: Distilling LLM for Multimodal Graph Learning") presents
    the results of graph classification and regression tasks using scaffold splitting,
    compared with other GNN and NLP baselines. The boldface indicates the best results,
    while underlining denotes the second-best. For the classification datasets (BACE,
    BBBP, Clintox, and HIV), the ROCAUC score (%) is shown; for the regression datasets
    (ESOL, FreeSolv, and Lipo), the RMSE is shown. Our proposed GALLON framework achieves
    state-of-the-art performance on BACE, BBBP, Clintox, and FreeSolv, and performs
    comparably to the best baselines on HIV, ESOL, and Lipo with a simpler MLP structure.
    We attribute this to the distilled MLP leveraging the strengths of both MLP and
    GNN, utilizing their complementary aspects—specifically, the prior knowledge embedded
    in LLMs and the capability of learning from graph structures in GNNs. This synergistic
    combination enhances the overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Prediction performance with scaffold splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BACE↑ | BBBP↑ | Clintox↑ | HIV↑ | ESOL↓ | Freesolv↓ | Lipo↓ |'
  prefs: []
  type: TYPE_TB
- en: '| GCN | 73.47±3.78 | 63.99±1.70 | 69.22±2.42 | 70.19±2.17 | 1.29±0.03 | 3.33±0.40
    | 0.89±0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| ChebNet | 75.53±1.60 | 67.07±0.75 | 68.95±2.81 | 73.78±1.66 | 1.32±0.04 |
    3.27±0.17 | 1.04±0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSage | 72.00±6.05 | 66.56±2.93 | 84.01±3.56 | 74.81±0.99 | 1.32±0.09
    | 3.12±0.30 | 0.87±0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| GIN | 70.34±2.57 | 62.53±3.27 | 73.40±3.73 | 72.54±3.32 | 1.26±0.08 | 3.71±0.79
    | \ul0.84±0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Mole-BERT | 71.46±4.74 | 67.52±1.17 | 66.46±7.26 | \ul75.42±1.13 | 1.39±0.07
    | 3.98±0.62 | 0.78±0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ECFP4-MLP | \ul79.65±1.88 | 61.84±0.37 | 70.03±1.57 | 69.58±0.61 | 1.67±0.30
    | 4.07±1.21 | 0.83±0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| ChemBERTa | 73.70±2.39 | \ul70.44±1.20 | 97.41±1.64 | 76.60±1.14 | 1.77±0.04
    | 3.97±0.13 | 1.19±0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| SMILES-Tsfm | 74.94±1.22 | 68.21±0.53 | \ul98.43±0.46 | 72.00±0.80 | 1.06±0.08
    | \ul2.74±0.94 | 2.74±0.94 |'
  prefs: []
  type: TYPE_TB
- en: '| GALLON | 80.42±1.06 | 72.38±0.63 | 99.15±0.57 | 75.39±0.40 | \ul1.18±0.02
    | 2.03±0.09 | 0.90±0.01 |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Contributions of LLM and GNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we aim to explore the influence of the teachers’ models. We
    conduct distillation using GNN or LLM solely as the teacher model, as well as
    using the combination of LLM+GNN as teacher models. The performance results are
    shown in Table [2](#S4.T2 "Table 2 ‣ 4.3 Contributions of LLM and GNN ‣ 4 Experiments
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    and Table [3](#S4.T3 "Table 3 ‣ 4.3 Contributions of LLM and GNN ‣ 4 Experiments
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    under different split settings. GNN$\rightarrow$MLP configuration achieves the
    best performance, indicating that the synergy between GNN and LLM contributes
    significantly to the final results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The comparison between different distillation settings with scaffold
    splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | GNN | LLM | (GNN+LLM)$\rightarrow$MLP | MLP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BACE | 73.47±3.78 | 73.57±2.67 | 80.42±1.06 | 77.93±1.38 | 78.50±2.00 | 73.11±2.55
    |'
  prefs: []
  type: TYPE_TB
- en: '| BBBP | 63.99±1.70 | 71.73±1.93 | 72.38±0.63 | 67.24±0.36 | 66.92±0.61 | 60.21±0.64
    |'
  prefs: []
  type: TYPE_TB
- en: '| Clintox | 69.22±2.42 | 99.06±0.64 | 99.15±0.57 | 83.94±4.51 | 84.99±1.51
    | 70.89±4.93 |'
  prefs: []
  type: TYPE_TB
- en: '| HIV | 70.19±2.17 | 76.60±0.61 | 75.39±0.40 | 70.94±2.47 | 71.14±0.64 | 66.17±0.75
    |'
  prefs: []
  type: TYPE_TB
- en: '| ESOL | 1.29±0.03 | 2.24±0.19 | 1.18±0.02 | 1.22±0.02 | 1.23±0.02 | 1.29±0.03
    |'
  prefs: []
  type: TYPE_TB
- en: '| Freesolv | 3.33±0.40 | 4.20±0.09 | 2.03±0.09 | 2.15±0.12 | 2.16±0.05 | 2.59±0.06
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lipo | 0.89±0.02 | 1.16±0.06 | 0.90±0.01 | 0.92±0.01 | 0.91±0.01 | 0.96±0.01
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The comparison between different distillation settings with random
    scaffold splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | GNN | LLM | (GNN+LLM)$\rightarrow$MLP | MLP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BACE | 79.08±6.61 | 83.87±3.47 | 80.34±2.68 | 76.84±1.69 | 79.50±2.96 | 70.73±3.02
    |'
  prefs: []
  type: TYPE_TB
- en: '| BBBP | 81.23±6.42 | 80.97±1.94 | 84.84±1.24 | 83.21±1.58 | 83.78±1.42 | 79.98±2.31
    |'
  prefs: []
  type: TYPE_TB
- en: '| Clintox | 81.61±6.04 | 72.08±11.55 | 93.71±4.35 | 90.98±4.98 | 91.20±6.88
    | 87.39±4.69 |'
  prefs: []
  type: TYPE_TB
- en: '| HIV | 72.33±4.73 | 74.80±4.04 | 74.00±2.26 | 72.63±3.60 | 72.69±3.71 | 67.66±3.35
    |'
  prefs: []
  type: TYPE_TB
- en: '| ESOL | 1.28±0.11 | 1.81±0.24 | 1.21±0.09 | 1.28±0.06 | 1.24±0.09 | 1.51±0.13
    |'
  prefs: []
  type: TYPE_TB
- en: '| Freesolv | 3.18±0.70 | 4.66±1.23 | 1.85±0.40 | 2.14±0.40 | 2.03±0.43 | 2.87±0.71
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lipo | 0.89±0.06 | 1.18±0.10 | 0.89±0.05 | 0.90±0.05 | 0.90±0.05 | 0.95±0.05
    |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Efficiency Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this experiment, we evaluate the performance of various models on the BACE
    dataset by comparing their ROCAUC scores against the log of inference time and
    the log of the number of parameters. Each result is conducted on the whole dataset
    100 times to calculate the averaged inference time. Fig. [4](#S3.F4 "Figure 4
    ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") and Fig. [4](#S3.F4 "Figure 4 ‣
    3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") illustrates the relationship between
    ROCAUC and the log of inference time and model size, showing that GALLON achieves
    the best performance with the fastest inference time and smallest model size.
    These results reveal that our approach effectively distills knowledge from GNN
    and LLM into MLP, leading to state-of-the-art performance in terms of both efficiency
    and accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Influence of Multimodality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the GALLON framework, we incorporate all of SMILES string, molecular diagram,
    and graph structure when querying the LLM. To explore the influence of each multimodality,
    we ablate the molecular diagram and graph structure respectively and plot the
    results of prediction of the finetuned LM in Fig. [6](#S4.F6 "Figure 6 ‣ 4.5 Influence
    of Multimodality ‣ 4 Experiments ‣ LLM and GNN are Complementary: Distilling LLM
    for Multimodal Graph Learning"). We conduct the experiments on BACE, BBBP, and
    Clintox, where all results are based on GPT4V and scaffold splitting. We find
    that the original GALLON with all the multimodality achieves the best performance,
    which means each modality contribute to extracting richer knowledge from LLM.
    To be more specific, removing molecular diagrams results in significant performance
    drops, due to the lack of overview of the molecule, such as some functional groups.
    Similarly, excluding graph structures leads to declines in BACE and BBBP, as the
    graph structure is easier for LLM to understand atomic connectivity insights.
    These findings underscore the importance of leveraging multiple modalities to
    enhance molecular representation and prediction accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da81b9401c8d4999f04d5aed90035c69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Ablation study of multimodalities.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3d199313929ab6491b972cacf5025e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Performance of different LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Influence of Large Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fig. [6](#S4.F6 "Figure 6 ‣ 4.5 Influence of Multimodality ‣ 4 Experiments
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    illustrates the performance of different LLMs (GPT4V, Claude3-haiku, and Claude3-sonnet)
    on BACE and BBBP datasets with scaffold splitting. The results show that each
    dataset has its own best-performing LLM, although the differences are not significant.
    (1) On BACE, Claude3-sonnet slightly outperforms GPT4V and Claude3-haiku, suggesting
    it may capture the chemical properties relevant to this dataset more effectively.
    By contrast, on BBBP dataset, GPT4V achieves the highest performance. (2) Claude3-sonnet
    consistently outperforms Claude3-haiku, due to its larger model size and capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore how multimodality aids in extracting prior knowledge
    from LLMs. We use a molecule from BACE dataset as an example. Its SMILES string
    is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  S1(=O)C[C@@H](Cc2cc(OC(C(F)(F)F)C(F)(F)F)c(N)c(F)c2)[C@H](O)[C@@H]([NH2+]Cc2cc(ccc2)C(C)(C)C)C1  |  |'
  prefs: []
  type: TYPE_TB
- en: 'and its diagram is shown in Fig. [7](#A3.F7 "Figure 7 ‣ Appendix C More Details
    of Case Study ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph
    Learning"). We query GPT-4V using three different multimodal combinations: (1)
    SMILES string, diagram, and graph structure, (2) SMILES string and diagram, (3)
    SMILES string and graph structure, and (4) SMILES string only. We present each
    response in Appendix [C](#A3 "Appendix C More Details of Case Study ‣ LLM and
    GNN are Complementary: Distilling LLM for Multimodal Graph Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the responses, we summarize the knowledge that LLMs provide with various
    multimodal combinations in Table 6\. The table shows how many functional groups
    and structural characteristics the LLM can identify from the input modality and
    whether it can facilitate interactions with BACE-1 and predict the binding result.
    Based on the table, we make the following observations: (1) Inputs of all SMILES
    strings, diagram, and graph structure can extract the most comprehensive information
    from the LLM. This indicates that both the diagram and graph structure contribute
    significantly to the overall analysis. (2) SMILES + Diagram can identify more
    functional groups than SMILES + Graph Structure. This suggests that the molecular
    diagram provides a better way to identify certain functional groups due to its
    intuitive and comprehensive overview of the molecule. (3) Although SMILES + Graph
    Structure has a similar number of identified items as SMILES only, the responses
    show that SMILES only lacks detailed explanations for each functional group. This
    undermines the richness of the extracted knowledge. (4) Only the combination of
    SMILES, diagram, and graph structure provide the most abundant structural characteristics.
    This emphasizes the importance of multimodal inputs for a complete structural
    analysis. (5) All combinations can identify interactions relevant to binding with
    BACE-1\. However, predictions about the molecule’s activity are more reliably
    made with the combination of SMILES, diagram, and graph structure, as well as
    SMILES and diagram, compared to other combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The analysis of information provided by various multimodal combinations
    on an example molecule of BACE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multimodalities |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SMILES &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Diagram &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Graph Structure &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SMILES &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Diagram &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SMILES &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Graph Structure &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SMILES |'
  prefs: []
  type: TYPE_TB
- en: '| Functional Groups | Sulfonamide Group | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Aromatic Ring | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Hydroxyl Group | $\checkmark$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Amine Group | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ether Group | $\checkmark$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Fluorine Atom | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Amide Bond | $\checkmark$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Thioether Group |  | $\checkmark$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Tertiary Butyl Group |  |  | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Structural Characteristics | Rigid & Flexible | $\checkmark$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Chiral Centers | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Molecular Bulk | $\checkmark$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Interactions | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: 6 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduced the GALLON framework, a novel approach for molecular
    property prediction that leverages the complementary strengths of Graph Neural
    Networks and Large Language Models. By distilling knowledge from both GNNs and
    LLMs into a more efficient Multilayer Perceptron model, we addressed the limitations
    of each individual modality and enhanced the overall predictive performance. One
    of the limitations of the paper is the distillation method. Future research can
    build upon this work in several directions. First, investigating advanced distillation
    techniques and more sophisticated mapping functions may improve the efficiency
    and effectiveness of the distillation process. Second, exploring other multimodal
    data representations and their integration into the GALLON framework could further
    enhance prediction accuracy and generalizability. Lastly, extending the framework
    to other domains beyond molecules, such as materials science and biology, could
    validate its broader applicability and utility.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Gpt-4v(ision) system card. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3
    Model Card, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang,
    and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model.
    arXiv preprint arXiv:2310.05845, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang
    Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language
    models (llms) in learning on graphs. ACM SIGKDD Explorations Newsletter, 25(2):42–61,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale
    self-supervised pretraining for molecular property prediction. arXiv preprint
    arXiv:2010.09885, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Debarati Das, Ishaan Gupta, Jaideep Srivastava, and Dongyeop Kang. Which
    modality should i use–text, motif, or image?: Understanding graphs with large
    language models. arXiv preprint arXiv:2311.09862, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional
    neural networks on graphs with fast localized spectral filtering. NeurIPS, 29,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng
    Ji. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Chaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang, Timin Gao, Yongdong
    Luo, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, et al. A challenger
    to gpt-4v? early explorations of gemini in visual expertise. arXiv preprint arXiv:2312.12436,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Victor Fung, Jiaxin Zhang, Eric Juarez, and Bobby G Sumpter. Benchmarking
    graph neural networks for materials chemistry. npj Computational Materials, 7(1):84,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep,
    Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian Tang, et al.
    Utilizing graph machine learning within drug discovery and development. Briefings
    in bioinformatics, 22(6):bbab159, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and
    George E Dahl. Neural message passing for quantum chemistry. In International
    conference on machine learning, pages 1263–1272\. PMLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation
    learning on large graphs. Advances in neural information processing systems, 30,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and
    Bryan Hooi. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed
    graph representation learning. In The Twelfth International Conference on Learning
    Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Shion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained
    molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay
    Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv
    preprint arXiv:1905.12265, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Cuiying Huo, Di Jin, Yawen Li, Dongxiao He, Yu-Bin Yang, and Lingfei Wu.
    T2-gnn: Graph neural networks for graphs with incomplete features and structure
    via teacher-student distillation. In Proceedings of the AAAI Conference on Artificial
    Intelligence, volume 37, pages 4339–4346, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He,
    Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019
    update: improved access to chemical data. Nucleic acids research, pages D1102–D1109,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. In ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Xiao Li, Li Sun, Mengjie Ling, and Yan Peng. A survey of graph neural
    network based recommendation in social networks. Neurocomputing, 549:126441, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. Molfm:
    A multimodal molecular foundation model. arXiv preprint arXiv:2307.09484, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. Can large
    language models empower molecular property prediction? arXiv preprint arXiv:2307.07443,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy
    Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,
    Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across
    millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou,
    Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, et al.
    Graph neural networks for materials science and chemistry. Communications Materials,
    3(1):93, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal
    of chemical information and modeling, pages 742–754, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao
    Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating
    molecule graphs with natural language. arXiv preprint arXiv:2209.05481, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, and Nitesh V Chawla.
    Knowledge distillation on graphs: A survey. arXiv preprint arXiv:2302.00219, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla.
    Learning mlps on graphs: A unified view of effectiveness, robustness, and efficiency.
    In The Eleventh International Conference on Learning Representations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation
    learning. Advances in neural information processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Xiaofeng Wang, Zhen Li, Mingjian Jiang, Shuang Wang, Shugang Zhang, and
    Zhiqiang Wei. Molecule property prediction based on spatial graph embedding. Journal
    of chemical information and modeling, 59(9):3817–3828, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular
    contrastive learning of representations via graph neural networks. Nature Machine
    Intelligence, 4(3):279–287, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] David Weininger. Smiles, a chemical language and information system. 1\.
    introduction to methodology and encoding rules. Journal of chemical information
    and computer sciences, 28(1):31–36, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse,
    Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular
    machine learning. Chemical science, 9(2):513–530, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu,
    Siyuan Li, and Stan Z. Li. Mole-BERT: Rethinking pre-training graph neural networks
    for molecules. In The Eleventh International Conference on Learning Representations,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Junjie Xu, Enyan Dai, Dongsheng Luo, Xiang Zhang, and Suhang Wang. Shape-aware
    graph spectral learning. arXiv preprint arXiv:2310.10064, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful
    are graph neural networks? In International Conference on Learning Representations,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Bencheng Yan, Chaokun Wang, Gaoyang Guo, and Yunkai Lou. Tinygnn: Learning
    efficient graph neural networks. In Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining, pages 1848–1856, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Cheng Yang, Jiawei Liu, and Chuan Shi. Extract the knowledge of graph
    neural networks and go beyond it: An effective knowledge distillation framework.
    In Proceedings of the web conference 2021, pages 1227–1237, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,
    Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. Harnessing the power of llms
    in practice: A survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery
    from Data, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling
    knowledge from graph convolutional networks. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 7074–7083, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. Graph-less neural
    networks: Teaching old mlps new tricks via distillation. arXiv preprint arXiv:2110.08727,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Wenqing Zheng, Edward W Huang, Nikhil Rao, Sumeet Katariya, Zhangyang
    Wang, and Karthik Subbian. Cold brew: Distilling graph node representations with
    incomplete or missing neighborhoods. arXiv preprint arXiv:2111.04840, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Dataset Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BACE: This dataset focuses on inhibitors of human beta-secretase 1\. It includes
    both quantitative (IC50 values) and qualitative (binary labels) binding results.
    It includes 1,513 compounds with their 2D structures and binary labels for the
    classification task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BBBP: The Blood-brain barrier penetration (BBBP) dataset comes from a study
    focused on modeling and predicting the permeability of the blood-brain barrier.
    This contains 2,050 compounds with 2D structures and binary labels indicating
    whether a compound can penetrate the blood-brain barrier (BBB) or not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clintox: This dataset compares drugs approved by the FDA and drugs that have
    failed clinical trials for toxicity reasons. It includes two classification tasks
    for 1,484 drug compounds with known chemical structures: (i) clinical trial toxicity
    (or absence of toxicity) and (ii) FDA approval status.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HIV: This dataset is introduced by the Drug Therapeutics Program (DTP) AIDS
    Antiviral Screen. It tests the ability to inhibit HIV replication for 41,127 compounds
    with binary labels indicating whether a compound is activate or not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ESOL: The ESOL (Estimated Solubility) dataset is a small-scale molecule dataset
    focuses on the water solubility of various compounds for a regression task. It
    includes quantitative solubility data for 1,128 compounds, with their 2D structures
    and measured solubility values in mols per liter.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freesolv: This provides data on the hydration-free energies of small molecules.
    It includes both experimental and calculated hydration-free energies for 642 compounds,
    along with their 2D structures.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lipophilicity: This dataset contains data on the octanol/water partition coefficient
    (logP) values for various compounds. It includes 4,200 compounds with their 2D
    structures and experimental results of octanol/water distribution coefficient
    (logD at pH 7.4).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 5: The statistics and tasks of datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | #Graphs | Avg. #nodes | Avg. #edges | #Features | #Classes | Task | Metric
    |'
  prefs: []
  type: TYPE_TB
- en: '| BACE | 1,513 | 34.1 | 73.7 | 9 | 1 | Classification | ROCAUC |'
  prefs: []
  type: TYPE_TB
- en: '| BBBP | 2,050 | 23.9 | 51.6 | 9 | 1 | Classification | ROCAUC |'
  prefs: []
  type: TYPE_TB
- en: '| Clintox | 1,484 | 26.1 | 55.5 | 9 | 2 | Classification | ROCAUC |'
  prefs: []
  type: TYPE_TB
- en: '| HIV | 41,127 | 25.5 | 54.9 | 9 | 1 | Classification | ROCAUC |'
  prefs: []
  type: TYPE_TB
- en: '| ESOL | 1,128 | 13.3 | 27.4 | 9 | - | Regression | RMSE |'
  prefs: []
  type: TYPE_TB
- en: '| Freesolv | 642 | 8.7 | 16.8 | 9 | - | Regression | RMSE |'
  prefs: []
  type: TYPE_TB
- en: '| Lipo | 4,200 | 27.0 | 59.0 | 9 | - | Regression | RMSE |'
  prefs: []
  type: TYPE_TB
- en: A.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCN [[20](#bib.bib20)]: Graph Convolutional Network (GCN) is one of the most
    popular MPNNs using 1-hop neighbors in each layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChebNet [[8](#bib.bib8)]: ChebNet uses Chebyshev polynomial to approximate
    the filter function. It is a more generalized form of GCN.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GraphSAGE [[14](#bib.bib14)]: Graph Sample and Aggregation (GraphSAGE) is a
    scalable and inductive framework that leverages node feature information by sampling
    and aggregating features from a fixed-size set of neighbors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GIN [[41](#bib.bib41)]: Graph Isomorphism Network (GIN) is a powerful variant
    of MPNNs designed to achieve maximum expressive power for graph representation
    learning, effectively distinguishing different graph structures through injective
    node aggregations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MoleBERT [[39](#bib.bib39)]: MoleBERT is a self-supervised learning strategy
    for pretraining GNNs, specifically designed for molecules. It introduces two novel
    pretraining tasks: Masked Atoms Modeling (MAM) at the node level and triplet masked
    contrastive learning (TMCL) at the graph level. MAM involves randomly masking
    some discrete codes and then pretraining GNNs to predict them. TMCL models the
    heterogeneous semantic similarity between molecules for effective molecule retrieval.
    A variant of VQ-VAE[[33](#bib.bib33)] is proposed as a context-aware tokenizer
    to encode atom attributes into chemically meaningful discrete codes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ECFP4-MLP [[28](#bib.bib28)]: Extended-connectivity fingerprint (ECFP) is a
    manually constructed fingerprinting method specifically developed to identify
    molecular characteristics. 4 means quantifying substructures with diameters of
    up to 4\. After getting fingerprinting from ECFP4, we train a multilayer perceptron.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChemBERTa [[6](#bib.bib6)]: ChemBERTa is based on the Roberta [[22](#bib.bib22)].
    It consists of 12 attention heads and 6 layers, resulting in 72 distinct attention
    mechanisms. It is then pre-trained on a dataset of 77M unique SMILES from PubChem [[19](#bib.bib19)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SMILES-Transformer [[16](#bib.bib16)]: SMILES Transformer has 4 Transformer
    blocks. Each Transformer block has 4 attention heads with 256 embedding dimensions
    and includes two linear layers. It is pre-trained on a dataset consisting of SMILES
    randomly selected from ChEMBL24.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.3 Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models. In this paper, we employ GCN [[20](#bib.bib20)] as the GNN backbone.
    To ensure fair comparisons, all backbone GCNs and distilled MLPs are configured
    with 3 layers and a hidden dimension of 32\. Roberta [[22](#bib.bib22)] is used
    as the smaller Language Model (LM) for all experiments. For the HIV and Lipophilicity
    datasets, we use Claude3-Haiku, and for the other datasets, we utilize GPT-4V.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting. Scaffold splitting divides a dataset based on chemical scaffolds
    in a deterministic manner, ensuring structural diversity across train, validation,
    and test sets, thereby enhancing the model’s ability to generalize to new scaffolds.
    Random scaffold splitting also partitions the dataset based on scaffolds but introduces
    randomness in the assignment, achieving a balance between structural diversity
    and randomized subset distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Training. We utilize an 80%/10%/10% split for the train, validation, and test
    sets. To ensure fair comparisons, the best hyperparameter configurations for each
    method are selected using the validation set, and we report the mean accuracy
    and variance across 10 different seeds on the test set. The hyperparameters $\alpha$
    are searched within the set {0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0,
    10.0}.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we show the dataset description part of the prompt used for each dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The description part of the prompts for each dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BACE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The BACE dataset provides and binary label binding results for a set
    of inhibitors &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of human $\beta$-secretase 1 (BACE-1). Based on the following inputs,
    please analyze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the property of the molecule (e.g. Functional groups, Structural characteristics)
    and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; analyze the binding results for a set of inhibitors of human beta-secretase
    (BACE-1)? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| BBBP |'
  prefs: []
  type: TYPE_TB
- en: '&#124; As a membrane separating circulating blood and brain extracellular fluid
    the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; blood-brain barrier blocks most drugs, hormones, and neurotransmitters.
    Based on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; these inputs, please analyze the property of the molecule (e.g. Functional
    groups, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Structural characteristics) and analyze if the molecule is permeable
    to the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; blood-brain barrier?" &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Clintox |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Could you analyze the given molecule based on the provided inputs and
    detail the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; factors influencing its potential for clinical trial toxicity or non-toxicity?
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Additionally, please assess factors that might impact its FDA approval
    status. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| HIV |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The HIV dataset tests the ability to inhibit HIV replication for over
    40,000 compounds. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Based on the following inputs, please analyze the property of the molecule
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (e.g. Functional groups, Structural characteristics) with focusing on
    its ability to inhibit &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HIV replication. Then make your guess or prediction (active or inactive).
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ESOL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Based on these inputs, please analyze the property of the molecule &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (e.g. Functional groups, Structural characteristics), and which properties
    of the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; molecule can affect its water solubility? Also try to guess its solubility.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Freesolv |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Free Solvation Database (FreeSolv) provides experimental and calculated
    hydration &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; free energy of small molecules in water. Based on the following inputs,
    please analyze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the property of the molecule (e.g. Functional groups, Structural characteristics)
    with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; focusing on its hydration free energy. Then make your guess or prediction
    about its &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; hydration free energy. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Lipo |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Lipophilicity is an important feature of drug molecules that affects
    both membrane &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; permeability and solubility. Based on the following inputs, please analyze
    the molecule &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and give some details of factors that can affect octanol/water distribution
    coefficient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (logD at pH 7.4). Then make your guess or prediction about its lipophilicity.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C More Details of Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section presents the responses we obtained from the LLM for the example
    molecule in Case Study [5](#S5 "5 Case Study ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning"). These texts are used to analyze
    the richness of information provided in Table [4](#S5.T4 "Table 4 ‣ 5 Case Study
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning").
    Fig. [7](#A3.F7 "Figure 7 ‣ Appendix C More Details of Case Study ‣ LLM and GNN
    are Complementary: Distilling LLM for Multimodal Graph Learning") displays the
    diagram of the example molecule.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c93c785942bf386e107274e782b0744a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The diagram of the example molecule.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The response of LLM using SMILES string, diagram, and graph structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) SMILES string, diagram, graph structure |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| The provided SMILES string, molecule diagram, and graph structure describe
    a complex organic molecule with various functional groups and structural features
    that may be relevant for its activity as an inhibitor of $\beta$-secretase 1 (BACE-1).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Analyzing the molecule based on the SMILES and the provided image: |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Functional Groups: |'
  prefs: []
  type: TYPE_TB
- en: '| - Sulfonamide Group (Sulfur connected to Oxygen by double bond and Nitrogen
    by single bond): This is often found in BACE-1 inhibitors as it can form hydrogen
    bonds with the enzyme. |'
  prefs: []
  type: TYPE_TB
- en: '| - Aromatic Rings: There are multiple phenyl groups (benzene rings), which
    are important for $\pi$ stacking interactions with aromatic residues in proteins.
    |'
  prefs: []
  type: TYPE_TB
- en: '| - Hydroxyl Groups (-OH): These are capable of forming hydrogen bonds, which
    can be key for binding to the active site. |'
  prefs: []
  type: TYPE_TB
- en: '| - Amines (both primary with NH2+ and tertiary with the nitrogen having three
    carbon substituents): These can engage in hydrogen bonding and ionic interactions,
    especially the positively charged amine, which can interact with negatively charged
    residues in the active site. |'
  prefs: []
  type: TYPE_TB
- en: '| - Ether Groups (Oxygen connected to Carbon by single bonds): These can provide
    some flexibility and help position other functional groups within the binding
    pocket. |'
  prefs: []
  type: TYPE_TB
- en: '| - Fluorine Atoms: The presence of multiple fluorine atoms suggests the molecule
    is designed to increase its lipophilicity for membrane penetration or create specific
    interactions with the enzyme due to the high electronegativity of fluorine. |'
  prefs: []
  type: TYPE_TB
- en: '| - Amide Bonds: The carbon-nitrogen bonds in the context of carbonyl groups
    (C=O) indicate amide linkages, which can also participate in hydrogen bonding.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Structural Characteristics: |'
  prefs: []
  type: TYPE_TB
- en: '| - The presence of both rigid (aromatic rings) and flexible (alkyl chains)
    elements in the molecule can help it to adapt its conformation to fit the binding
    pocket of the enzyme. |'
  prefs: []
  type: TYPE_TB
- en: '| - The molecule has chiral centers, which are indicated by the symbols "@@"
    in the SMILES string, affecting how the molecule may interact with the chiral
    environment of the enzyme’s active site. |'
  prefs: []
  type: TYPE_TB
- en: '| - There is significant molecular bulk near the periphery of the molecule,
    due to the presence of tert-butyl groups and trifluoromethyl groups, which could
    be important for binding affinity and specificity. |'
  prefs: []
  type: TYPE_TB
- en: '| In terms of the BACE-1 inhibition, the functional groups such as the sulfonyl
    group, aromatic rings, and amine groups would be essential for forming interactions
    with the active site of the enzyme. The molecule is likely designed to fit within
    the binding pocket of BACE-1, forming hydrogen bonds, hydrophobic interactions,
    and possibly ionic interactions with key amino acid residues. |'
  prefs: []
  type: TYPE_TB
- en: '| The success of this compound as a BACE-1 inhibitor may be dependent on these
    interactions, as well as its overall shape and the ability to adopt a conformation
    that is complementary to the enzyme binding site. |'
  prefs: []
  type: TYPE_TB
- en: '| To predict binding results or to further analyze the structural features
    and their potential interaction with BACE-1, computational techniques such as
    molecular docking or molecular dynamics could be applied if more details about
    the active site and its interactions with ligands are available. Additionally,
    experimental assays would be required to confirm the inhibitory activity of the
    molecule. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: The response of LLM using SMILES string, and diagram.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) SMILES string and diagram |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Based on the SMILES string provided and the diagram attached, we can infer
    several properties of the molecule and discuss its potential interactions as an
    inhibitor of human beta-secretase 1 (BACE-1), focusing on functional groups and
    structural characteristics relevant to its activity. |'
  prefs: []
  type: TYPE_TB
- en: '| First, examining the structural features from the SMILES string and the diagram:
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Heterocyclic Components: There appear to be several heterocyclic components,
    one of which is likely to contribute to the molecule’s ability to interact with
    the active site of BACE-1. |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Fluorinated Substituents: The presence of multiple fluorine atoms could
    influence the binding affinity and specificity due to their electronegative nature,
    and they are often found in inhibitors to enhance metabolic stability and bioavailability.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3\. Sulfonamide Group: The S(=O)(=O)N motif represents a sulfonamide group,
    which is a common feature in many drug molecules that can contribute to hydrogen
    bonding with the enzyme’s active site. |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Chiral Centers: The molecule contains multiple chiral centers, as indicated
    by the "[C@@H]" and "[C@H]" annotations in the SMILES string. The configuration
    of these centers can significantly influence the binding affinity and selectivity
    of the molecule for BACE-1. |'
  prefs: []
  type: TYPE_TB
- en: '| 5\. Amine Group: The molecule contains a positively charged amine group "[NH2+]",
    which suggests it may form salt bridges or hydrogen bonds with BACE-1. |'
  prefs: []
  type: TYPE_TB
- en: '| 6\. Aromatic Rings: There are aromatic rings, which can facilitate $\pi-\pi$
    stacking interactions with aromatic residues in the active site of the enzyme.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7\. Hydroxyl Group: There is a hydroxyl group, a polar functional group that
    can form hydrogen bonds with BACE-1. |'
  prefs: []
  type: TYPE_TB
- en: '| 8\. Ethers and Thioethers: Both ether (C-O-C) and thioether (C-S-C) groups
    are present, which may contribute to the binding through interactions with nearby
    residues. |'
  prefs: []
  type: TYPE_TB
- en: '| When assessing binding results for BACE-1 inhibitors, a few types of interactions
    can be analyzed: |'
  prefs: []
  type: TYPE_TB
- en: '| - Hydrogen Bonds: Strong H-bond donors and acceptors in the molecule can
    form specific interactions with the enzyme’s active site, stabilizing the inhibitor-enzyme
    complex. |'
  prefs: []
  type: TYPE_TB
- en: '| - Hydrophobic Interactions: Non-polar regions of the inhibitor may engage
    in hydrophobic contacts with non-polar pockets of the active site, increasing
    affinity. |'
  prefs: []
  type: TYPE_TB
- en: '| - Electrostatic Interactions: Charged groups, like the amine, can form electrostatic
    interactions with oppositely charged residues of the enzyme or with metal ions
    that may be present in the active site. |'
  prefs: []
  type: TYPE_TB
- en: '| - Conformational Fit: The overall shape of the molecule must fit well into
    the active site of BACE-1 to exhibit high binding affinity. |'
  prefs: []
  type: TYPE_TB
- en: '| Without experimental data, predicting the exact mode of inhibition is not
    feasible. However, the presence of these functional groups and structural features
    suggests that the molecule is designed to fit into the enzyme’s active site and
    have the necessary interactions for inhibition. |'
  prefs: []
  type: TYPE_TB
- en: '| In silico methods, such as molecular docking and dynamics simulations, coupled
    with in vitro assays, would be employed to predict the binding affinity and conformation
    of the inhibitor in the BACE-1 active site to more accurately assess its potential
    as a therapeutic molecule. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: The response of LLM using SMILES string, and graph structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) SMILES string and graph structure |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Based on the provided SMILES string and graph structure, the molecule has
    several noteworthy features: |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Sulfonamide group: The molecule contains a sulfonamide group (S(=O)(=O)N),
    which is known to be an essential pharmacophore in drug design and can form hydrogen
    bonds with the target protein, increasing the binding affinity. |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Fluorinated aromatic ring: The molecule has a fluorinated phenyl ring,
    which can enhance the metabolic stability and lipophilicity of the compound, potentially
    improving its bioavailability and membrane permeability. |'
  prefs: []
  type: TYPE_TB
- en: '| 3\. Secondary amine: The presence of a secondary amine (NH2) might be involved
    in hydrogen bonding interactions with the enzyme’s active site, which can be a
    key factor for the inhibitory activity. |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Tertiary butyl group: The bulky tert-butyl group (C(C)(C)C) may contribute
    to the molecule’s steric hindrance, affecting its binding conformation and possibly
    improving selectivity for BACE-1 by fitting into specific hydrophobic pockets.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5\. Multiple aromatic rings: The presence of several interconnected aromatic
    rings provides a rigid framework that can favorably stack with the aromatic amino
    acid residues in the BACE-1 active site, enhancing the $\pi-\pi$ interactions.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Upon analyzing the binding results for this inhibitor against human beta-secretase
    (BACE-1), the mentioned functional groups and structural characteristics suggest
    that the molecule may exhibit strong binding affinity and potency. The sulfonamide
    group can form key hydrogen bonds, the fluorinated ring can improve pharmacokinetic
    properties, and the bulky groups may enhance selectivity. However, the actual
    binding results would require empirical data obtained from biochemical assays,
    molecular docking studies, or computational simulations. |'
  prefs: []
  type: TYPE_TB
- en: '| Using AI-based predictive models, one could input the molecule’s structural
    representation, such as the SMILES string or molecular graph, and obtain predictions
    on its binding affinity, selectivity, and potential inhibitory activity against
    BACE-1\. Such models are trained on large datasets of known inhibitors and their
    corresponding biological activities, allowing for the rapid virtual screening
    of novel compounds. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: The response of LLM using SMILES string only.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) SMILES string only |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Based on the SMILES string provided, the molecule appears to have a complex
    structure with several functional groups. The molecule contains a sulfone group
    (S(=O)), an amine group ([NH2+]), multiple fluorine atoms (F), and an aromatic
    ring (cc). Additionally, the molecule also has chiral centers, indicated by the
    "@" symbol, which can impact its binding affinity to the target enzyme. |'
  prefs: []
  type: TYPE_TB
- en: '| The presence of fluorine atoms in the molecule suggests that it may have
    strong binding interactions with the BACE-1 enzyme, as fluorine is known to form
    strong hydrogen bonds with proteins. The aromatic ring and amine group may also
    contribute to the binding affinity through pi stacking interactions and hydrogen
    bonding, respectively. |'
  prefs: []
  type: TYPE_TB
- en: '| The molecule also contains a methoxy group (OC) and a tert-butyl group (C(C)(C)C),
    which could impact its overall shape and steric hindrance, potentially affecting
    its binding to the BACE-1 enzyme. |'
  prefs: []
  type: TYPE_TB
- en: '| To analyze the binding results for this molecule as an inhibitor of BACE-1,
    we would need to take into account the specific interactions between the molecule
    and the enzyme’s active site. This could be done using molecular docking simulations
    or other computational chemistry techniques, which would allow us to predict the
    binding affinity and identify key interactions contributing to the inhibition
    of the enzyme. |'
  prefs: []
  type: TYPE_TB
- en: '| Overall, based on the structural characteristics of the molecule, it appears
    to have several features that could contribute to strong binding interactions
    with BACE-1, making it a potential inhibitor of the enzyme. However, further analysis
    using computational tools or experimental techniques would be needed to confirm
    its efficacy as a BACE-1 inhibitor. |'
  prefs: []
  type: TYPE_TB
