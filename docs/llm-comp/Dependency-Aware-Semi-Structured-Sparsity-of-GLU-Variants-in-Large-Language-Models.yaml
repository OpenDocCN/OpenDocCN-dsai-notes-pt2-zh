- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.01943](https://ar5iv.labs.arxiv.org/html/2405.01943)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhiyu Guo    Hidetaka Kamigaito    Taro Wanatnabe
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rapid advancement in Large Language Models (LLMs) has markedly enhanced
    the capabilities of language understanding and generation. However, the substantial
    model size poses hardware challenges, affecting both memory size for serving and
    inference latency for token generation. To address those challenges, we propose
    Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent
    prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency
    into the weight magnitude-based unstructured pruning. We introduce an MLP-specific
    pruning metric that evaluates the importance of each weight by jointly considering
    its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates
    a balance between the adaptability offered by unstructured pruning and the structural
    consistency inherent in dependency-based structured pruning. Empirical evaluations
    on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms
    both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns
    but also maintains the computational efficiency of Wanda.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent years have witnessed the great success of Transformer-based Large Language
    Models (LLMs) (Brown et al., [2020](#bib.bib4); Chowdhery et al., [2023](#bib.bib6);
    OpenAI, [2023](#bib.bib34)) across various challenging tasks, such as mathematical
    reasoning (Cobbe et al., [2021](#bib.bib8)), code generation (Chen et al., [2021](#bib.bib5)).
    However, the practical use of these models for inference has faced a major obstacle
    due to the substantial computational resources they consume. To tackle this, several
    methods for efficient LLMs inference have been applied, including training compact
    models using more data (Touvron et al., [2023a](#bib.bib42)), model quantization
    (Dettmers et al., [2022](#bib.bib11); Frantar et al., [2023](#bib.bib15)), and
    grouped query attention (Shazeer, [2019](#bib.bib38); Ainslie et al., [2023](#bib.bib1)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural network pruning (Han et al., [2015](#bib.bib20); Wen et al., [2016](#bib.bib44))
    is a widely adopted technique for compressing models, leading to a substantial
    reduction in model size and inference latency. Pruning can be categorized into
    two main approaches: unstructured pruning (Sun et al., [2023](#bib.bib40); Frantar
    & Alistarh, [2023](#bib.bib14)), which involves the removal of specific weights,
    and structured pruning (Zhang et al., [2023](#bib.bib49); Ma et al., [2023](#bib.bib29)),
    which entails the removal of complete rows or columns of weights. In contrast
    to structured pruning, which struggles with performance in LLMs even at low sparsity
    levels, unstructured pruning (Sun et al., [2023](#bib.bib40); Frantar & Alistarh,
    [2023](#bib.bib14)) exhibits promising results without additional retraining,
    and achieves practical speedup in Nvidia GPU through more stringent N:M sparsity
    pattern (Mishra et al., [2021](#bib.bib33); Kurtic et al., [2023](#bib.bib25)).
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib14)) tackles the issue of pruning
    LLMs by considering it as a problem of layerwise reconstruction. It exclusively
    depends on weight updates aimed at maintaining the input-output relationship for
    each layer. Motivated by the emergent large magnitude activations in LLMs (Dettmers
    et al., [2022](#bib.bib11)), Wanda (Sun et al., [2023](#bib.bib40)) further eases
    the computational load in SparseGPT by solely employing the product derived from
    weights and input activation magnitudes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb5edabb0905836aa19d6d027c7f5f16.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Dependency-based Structured Sparsity
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f90f8e5bcf90b8af80332ebcb1ab08f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Wanda Unstructured Sparsity
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/061178269c51845c51e64b91dc350c50.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Dependency-aware Semi-structured Sparsity
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Illustration of our proposed Dependency-aware Semi-structured Sparsity
    (DaSS). In (a) dependency-based structured pruning (Ma et al., [2023](#bib.bib29)),
    all the weights connecting to the same intermediate neuron are removed or remain
    simultaneously. In (b) Wanda unstructured pruning (Sun et al., [2023](#bib.bib40)),
    it assigns greater emphasis to the weights corresponding to large input activations.
    For Gate-Proj and UP-Proj, the same number of weights are removed for each MLP
    neuron, regardless of whether some neurons have much larger activation norms.
    For Down-Proj, the weights corresponding to larger activation norms are more likely
    to be pruned. This can lead to a structural mismatch. In (c), all the weights
    corresponding to large intermediate activations are more likely to be reserved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the SwiGLU-based MLP (Shazeer, [2020](#bib.bib39)) module accounts for more
    than 80% parameters in the recent LLMs that use multi/grouped query attention
    (Chowdhery et al., [2023](#bib.bib6); Touvron et al., [2023b](#bib.bib43))¹¹1In
    LLaMA2-70B, the dimension of key and value is $\frac{1}{8}d$., its pruning emerges
    as a pivotal factor in determining the overall compression efficacy of LLMs. In
    dependency-aware structured pruning, it’s critical to consider that pruned parameters
    have dependencies with other parameters, owing to their interconnected nature
    (Ma et al., [2023](#bib.bib29); Fang et al., [2023](#bib.bib13)). In the context
    of MLP pruning, all the weights connected to each intermediate neuron should be
    preserved or pruned simultaneously. The precise coordination involved in pruning
    is crucial for upholding the model’s structural integrity and its functional capabilities.
    Although current unstructured pruning methods (Sun et al., [2023](#bib.bib40);
    Frantar & Alistarh, [2023](#bib.bib14)) effectively remove a significant number
    of redundant weights, they operate entirely locally within each linear layer without
    considering inter-dependencies to other layers. This can lead to a structural
    mismatch, which is more evident in Wanda as shown in Figure [1(b)](#S1.F1.sf2
    "Figure 1(b) ‣ Figure 1 ‣ 1 Introduction ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models"): In Gate and Up projections, the same
    amount of parameters are pruned for each MLP neuron. However, the intermediate
    activation norms of SwiGLU are not uniformly distributed and some neurons have
    much larger norms than others. Based on Wanda pruning metric, more weights connected
    to neurons with large activation norms are preserved. At high sparsity, this problem
    gets magnified in Wanda causing significant drops in performance by the broken
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to overcome the limitations present in current pruning methodologies,
    we introduce a new paradigm, namely, Dependency-aware Semi-structured Sparsity
    (DaSS). This approach is specifically designed to navigate the middle ground between
    the flexibility of unstructured pruning and the structural consistency of dependency-based
    structured pruning. We examine the effectiveness of magnitude pruning in Gate
    and Up projections of SwiGLU, and we find that grouping weights per-input is surprisingly
    effective compared with Wanda (Sun et al., [2023](#bib.bib40)). Leveraging these
    newly acquired understandings, we present a new MLP pruning metric that assesses
    each weight’s importance based on the product of its magnitude and the norm of
    the corresponding MLP intermediate activations. Our proposed DaSS method, illustrated
    in Figure [1(c)](#S1.F1.sf3 "Figure 1(c) ‣ Figure 1 ‣ 1 Introduction ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"), embodies
    a semi-structured pattern that retains a degree of the adaptability inherent in
    unstructured pruning while incorporating the dependency-aware aspect of structured
    pruning. This balance allows for more precise pruning. DaSS can be easily extended
    to hardware-friendly N:M sparsity patterns.
  prefs: []
  type: TYPE_NORMAL
- en: We perform extensive experiments on LLaMA2 and Mistral to evaluate DaSS across
    various tasks from language modeling, commonsense reasoning, and MMLU (Hendrycks
    et al., [2021](#bib.bib22)). In achieving hardware-friendly N:M sparsity patterns,
    DaSS consistently excels beyond the existing LLM pruning methods SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib14)) and Wanda (Sun et al., [2023](#bib.bib40)), while
    maintaining the computational efficiency akin to Wanda. Impressively, DaSS outperforms
    SparseGPT at high sparsity even without weight update. Our work contributes fresh
    insights into the nuanced understanding of sparsity within large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminary Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Wanda Pruning Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the context of LLM pruning, we denote a linear layer weight matrix $\mathbf{W}\in\mathbb{R}^{d_{\text{out}}\times
    d_{\text{in}}}$ is determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}_{i,j}=\left&#124;\mathbf{W}_{i,j}\right&#124;\cdot\left\&#124;\mathbf{X}_{j}\right\&#124;_{2}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathbf{I}_{i,j}$. We call this as output-balanced granularity.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 GLU Variants for Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gated Linear Units (GLU) (Dauphin et al., [2017](#bib.bib9)) are formed by the
    element-wise multiplication of two linear projections, with a sigmoid function
    applied to one projection before the multiplication. Shazeer ([2020](#bib.bib39))
    suggests an alternative design for the Transformer’s MLP layer that incorporates
    GLU variants, effectively replacing the conventional first linear transformation
    and activation function. More formally, we denote the $d_{\text{hidden}}$ by evaluating
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}=\sigma(\mathbf{x}\mathbf{W_{1}})\otimes\mathbf{x}\mathbf{W_{2}}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathbf{z}=\mathbf{y}\mathbf{W_{3}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: We call $\mathbf{W_{1}},\mathbf{W_{2}},\mathbf{W_{3}}$ linear projections as
    Gate-Proj, Up-Proj, and Down-Proj, respectively. GLU variants that use Swish (Ramachandran
    et al., [2017](#bib.bib36)) and ReLU (Glorot et al., [2011](#bib.bib19)) activation
    functions in Eq.([2](#S2.E2 "Equation 2 ‣ 2.2 GLU Variants for Transformer ‣ 2
    Preliminary Exploration ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models")) are called SwiGLU and ReGLU, respectively. SwiGLU
    is most widely used in the recent LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Revisiting the Effectiveness of Magnitude Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sun et al. ([2023](#bib.bib40)) conducted ablation studies among different granularities
    of magnitude pruning, including input-balanced granularity, which sorts weights
    connecting to the same input neuron. Compared with Wanda pruning, input-balanced
    magnitude pruning is just slightly worse.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we further investigate the effectiveness of input-balanced magnitude pruning
    for the MLP module. We divide the SwiGLU-based MLP module into two groups, input
    projections (Gate-Proj and Up-Proj) and output projection (Down-Proj). We prune
    those two groups separately and compare Wanda pruning with input-balanced magnitude
    pruning. Table [1](#S2.T1 "Table 1 ‣ 2.3 Revisiting the Effectiveness of Magnitude
    Pruning ‣ 2 Preliminary Exploration ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models") shows that input-balanced magnitude
    pruning is surprisingly better than Wanda pruning at higher sparsity ratios for
    input projections pruning. For the output projection, input-balanced magnitude
    pruning is consistently worse than Wanda pruning. This suggests the diminished
    importance of outlier features within input projections compared to the output
    projection. We analyze the layerwise intermediate activation outlier distribution
    in Appendix [B](#A2 "Appendix B Activation Outlier in SwiGLU ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"). However,
    in the older GeLU-based MLP expansion projection, we observe Wanda can achieve
    much better performance as shown in Appendix [C](#A3 "Appendix C GeLU-based MLP
    Input projection ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in
    Large Language Models"), suggesting this is a special property of GLU variants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of Wanda pruning and Input-balanced Magnitude pruning,
    perplexity with LLaMA2-7B on WikiText'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Up+Gate Projection | Down Projection |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsity | 50% | 60% | 70% | 50% | 60% | 70% |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 6.05 | 7.12 | 12.42 | 5.76 | 6.11 | 6.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Magnitude | 6.12 | 7.10 | 10.52 | 5.87 | 6.56 | 9.03 |'
  prefs: []
  type: TYPE_TB
- en: 2.4 Output-balanced Pruning Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N:M sparsity pattern offers notable speed improvements on recent NVIDIA GPUs
    (Mishra et al., [2021](#bib.bib33); Kurtic et al., [2023](#bib.bib25)). It is
    specified that every group of M consecutive weights must include N zeros. SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib14)) is not explicitly designed for output-balanced
    pruning granularity. When converting into an N:M sparsity pattern, SparseGPT forces
    every M consecutive weight in each row to have exactly N zeros. In such cases,
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib14)) and Wanda (Sun et al., [2023](#bib.bib40))
    unanimously remove the same amount of weights for each output. Such methods often
    emphasize the significance of individual components within the weight matrix and
    overlook inter-dependencies to other layers in the network. For MLP input projections
    pruning, an equal amount of weights corresponding to each intermediate neuron
    are removed. However, for output projection based on SparseGPT and Wanda pruning
    metrics, the weights connecting to intermediate neurons with larger activations
    are more likely to be reserved leading to a structural mismatch.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Dependency-aware Semi-structured Sparsity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce Dependency-aware Semi-structured Sparsity (DaSS)
    for pruning MLP, which incorporates structural dependency into weight magnitude-based
    unstructured pruning method. An overview of DaSS and its comparison with the existing
    pruning method is shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models").
  prefs: []
  type: TYPE_NORMAL
- en: Here, we denote the transposed weight matrices $\mathbf{W_{1}},\mathbf{W_{2}},\mathbf{W_{3}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Structure Dependency in MLP.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In dependency-based structured pruning (Ma et al., [2023](#bib.bib29); Fang
    et al., [2023](#bib.bib13)), the initial step is dedicated to recognizing groups
    of interconnected structures within the model. In terms of SwiGLU-based MLP module
    pruning, there are three projection matrices, all weights connecting to an identical
    intermediate neuron are collectively classified into the same dependency group.
    When pruning weights in $\mathbf{W^{(1)}_{i,:}}$ should also be emphasized similar
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating Dependency into Weight Importance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In dependency-based pruning, we assess the importance of each weight and then
    aggregate the importance scores within the same group as the group importance
    score. Consequently, each weight within the same dependency group shares a consistent
    importance score, ensuring their simultaneous retention or elimination. The importance
    score of each weight is equal to the group importance score. In DaSS pruning,
    we take both group importance and weight magnitude into consideration to evaluate
    the importance of each weight. LLM-pruner (Ma et al., [2023](#bib.bib29)) evaluates
    the group importance using gradient-based methods. However, computing gradient
    for LLMs will introduce a significant amount of memory cost and it is less practical
    for larger models. The existing unstructured pruning methods (Sun et al., [2023](#bib.bib40);
    Frantar & Alistarh, [2023](#bib.bib14)) are more efficient than gradient-based
    methods. By prioritizing the weights linked to outliers in intermediate activations,
    Wanda outperforms magnitude pruning in Down-Proj pruning. To minimize the impact
    on these critical outliers, it would be beneficial to also give greater significance
    to the weights that lead to outlier generation in both Gate-Proj and Up-Proj projections.
    Thus, we use the norm of intermediate activations $\|\mathbf{y}\|_{2}$ is determined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}_{i,j}^{k}=\left&#124;\mathbf{W}_{i,j}^{k}\right&#124;\cdot\&#124;\mathbf{y}_{i}\&#124;_{2}^{\alpha}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Where $k=1,2$ is determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}_{i,j}^{3}=\left&#124;\mathbf{W}_{i,j}^{3}\right&#124;\cdot\&#124;\mathbf{y}_{j}\&#124;_{2}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: In Down-Proj pruning, the pruning metric is the same as done with Wanda (Sun
    et al., [2023](#bib.bib40)). By augmenting intermediate activations into all three
    weight importance matrices, DaSS inherently assigns greater emphasis to weights
    corresponding to intermediate activation outliers, thereby facilitating more nuanced
    structural coordination among the entire MLP module.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning Granularity.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pruning LLaMA models in finer granularity can improve the performance (Sun et al.,
    [2023](#bib.bib40)). To incorporate intermediate activations into SwiGLU-based
    MLP pruning, each weight in the same comparison group should correspond to different
    intermediate activations. In Section 2.3, we show that input-balanced magnitude
    pruning can still achieve competitive results for Gate-Proj and Up-Proj pruning.
    In such pruning granularity, we can augment intermediate activations into Gate-Proj
    and Up-Proj pruning metric. In input-balanced pruning, we remove $s\%$ based on
    weight importance scores. DaSS uses output-balanced sparsity for Down-Proj pruning,
    which is the same as Wanda.
  prefs: []
  type: TYPE_NORMAL
- en: Extension to N:M Sparsity.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The DaSS pruning design allows for easy adaptation to the N:M sparsity pattern.
    For Gate-Proj and Up-Proj the N: M sparsity pattern is formed on an input-balanced
    basis. This means that for weights connecting to each input neuron, out of every
    group of M consecutive weights, there are exactly N zeros included. For Down-Proj
    the N: M sparsity pattern is formed on an output-balanced basis.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In summary, our DaSS method offers multiple appealing aspects for pruning LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It retains the fundamental simplicity inherent in Wanda pruning method. Without
    weight updating, it still matches the performance of SparseGPT even at high sparsity
    as demonstrated in Section 4.4\. This demonstrates the consistently effective
    and efficient capability of the DaSS method in identifying sparse neural networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike SparseGPT and Wanda that use input + intermediate activations for MLP
    pruning, DaSS only uses intermediate activations. By using intermediate activations
    as group importance indicator, DaSS prunes MLP module in a more comprehensive
    view that captures the collective importance of all the weights connecting to
    each intermediate neuron.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DaSS effectively explores the balance between unstructured pruning’s flexibility
    and the structural coherence in dependency-based structured pruning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DaSS’s performance is evaluated over popular LLMs using SwiGLU, including the
    LLaMA2 model family (Touvron et al., [2023b](#bib.bib43)), which has models with
    parameters ranging between 7 billion and 70 billion, and also the Mistral-7B model
    (Jiang et al., [2023](#bib.bib24)). Among them, LLaMA2-70B and Mistral-7B use
    grouped-query attention (Ainslie et al., [2023](#bib.bib1)), the MLP module parameter
    accounts for around 80% of the total model parameters. To test the generalization
    ability to ReGLU models, we use ReluLLaMA (Team, [2023](#bib.bib41)), which is
    fine-tuned using ReGLU variant (Shazeer, [2020](#bib.bib39); Mirzadeh et al.,
    [2023](#bib.bib32)) based on LLaMA2 with small accuracy loss. The model configuration
    details are in Appendix [7](#A1.T7 "Table 7 ‣ Appendix A Model Configurations
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models"). We access the public checkpoints of the involved models provided by
    the HuggingFace Transformers library (Wolf et al., [2019](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: Baseline Approaches.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Sun et al. ([2023](#bib.bib40)), the pre-LLM methods become less
    effective for LLMs. We compare the performance with two LLM-specific one-shot
    pruning approaches, SparseGPT (Frantar & Alistarh, [2023](#bib.bib14)) and Wanda
    (Sun et al., [2023](#bib.bib40)). Those baseline methods utilize uniform layerwise
    sparsity that can be easily converted into hardware-friendly N:M sparsity pattern.
    We used the same calibration data set as SparseGPT and Wanda in their model pruning
    processes, consisting of 128 sequences of 2048 tokens each, randomly selected
    from the first shard of the C4 dataset (Raffel et al., [2020](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To comprehensively evaluate the efficacy of our proposed method, three unique
    metrics are utilized to evaluate the performance of the pruned models: (1) perplexity
    (PPL) of language modeling (2) zero-shot accuracy on 5 commonsense reasoning tasks
    (3) 5-shot accuracy on Massive Multitask Language Understanding (MMLU) (Hendrycks
    et al., [2021](#bib.bib22)). Although perplexity has been regarded as a consistent
    and reliable metric for measuring compressed models in previous works (Dettmers
    & Zettlemoyer, [2023](#bib.bib10); Frantar & Alistarh, [2023](#bib.bib14)), a
    recent study indicates that perplexity fails to capture the change in capabilities
    of compressed LLMs on performing knowledge-intensive MMLU tasks (Jaiswal et al.,
    [2023](#bib.bib23)). For perplexity evaluation, we use the validation dataset
    of WikiText2 (Merity et al., [2017](#bib.bib31)). For zero-shot commonsense reasoning
    tasks, we choose five widely used tasks for accuracy evaluation: ARC (Easy and
    Challenge) (Clark et al., [2018](#bib.bib7)), HellaSwag (Zellers et al., [2019](#bib.bib48)),
    PiQA (Bisk et al., [2020](#bib.bib3)), and WinoGrande (Sakaguchi et al., [2021](#bib.bib37)),
    implemented in the Lm-Evaluation-Harness (Gao et al., [2021](#bib.bib17)). For
    5-shot MMLU evaluation, we use Chain-of-Thought Hub (Fu et al., [2023](#bib.bib16))
    which is based on the official implementation of MMLU (Hendrycks et al., [2021](#bib.bib22)).
    The MMLU encompasses 57 tasks, spanning from STEM, Humanities, Social Sciences,
    among others and we report the mean accuracy of 57 tasks. We evaluate the (1)
    perplexity of all the aforementioned models. To fully demonstrate the task-wise
    performance in different sparsity patterns, we report the (2) and (3) task performance
    of the largest LLaMA2-70B model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: WikiText perplexity of pruned LLaMA-2 and Mistral models'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Family | Mistral | Llama2 |'
  prefs: []
  type: TYPE_TB
- en: '| Size | 7B | 7B | 13B | 70B |'
  prefs: []
  type: TYPE_TB
- en: '| Method | MLP Sparsity | PPL ($\downarrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | - | 5.25 | 5.47 | 4.88 | 3.32 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 4:8 | 7.36 | 7.33 | 6.29 | 4.66 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 4:8 | 7.38 | 7.63 | 6.42 | 4.59 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 4:8 | 7.06 | 7.26 | 6.16 | 4.41 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 2:4 | 8.86 | 8.72 | 7.30 | 5.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 2:4 | 9.24 | 9.55 | 7.68 | 5.35 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 2:4 | 8.39 | 8.48 | 6.90 | 4.91 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 50% | 6.20 | 6.38 | 5.60 | 4.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 50% | 6.25 | 6.50 | 5.67 | 4.07 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 50% | 6.15 | 6.44 | 5.59 | 4.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Downstream tasks performance of LLaMA2-70B model in different sparsity
    pattern. The MMLU scores of dense LLaMA2-70B and LLaMA2-34B reported in Touvron
    et al. ([2023b](#bib.bib43)) are 68.9 and 62.6 respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sparsity | Methods | Commonsense Reasoning (0 shot) ($\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| PIQA | HellaSwag | Winogrande | ARC-e | ARC-c | Average | MMLU (5 shot) |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | - | 82.15 | 66.05 | 77.98 | 82.55 | 54.35 | 72.62 | 69.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 4:8 | SparseGPT | 80.52 | 61.00 | 77.03 | 79.85 | 50.60 | 69.80 | 60.24 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 80.47 | 61.85 | 75.45 | 80.10 | 50.00 | 69.57 | 59.69 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 80.79 | 62.70 | 76.09 | 81.20 | 51.19 | 70.39 | 60.86 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS+skip 1/4 | 81.34 | 63.10 | 76.09 | 80.05 | 50.68 | 70.25 | 65.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 2:4 | SparseGPT | 79.00 | 59.00 | 76.64 | 78.50 | 47.87 | 68.20 | 56.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 79.22 | 59.25 | 74.66 | 78.90 | 47.01 | 67.81 | 56.41 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 79.70 | 60.00 | 74.82 | 79.65 | 49.15 | 68.66 | 57.53 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS+skip 1/4 | 80.63 | 62.05 | 74.51 | 80.00 | 49.40 | 69.32 | 64.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | SparseGPT | 81.50 | 64.20 | 78.45 | 81.90 | 52.73 | 71.76 | 64.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 81.01 | 64.30 | 77.35 | 80.95 | 52.05 | 71.13 | 62.72 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 81.18 | 64.60 | 77.90 | 81.35 | 51.71 | 71.35 | 63.27 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS+skip 1/4 | 81.23 | 64.45 | 77.27 | 81.35 | 52.81 | 71.42 | 66.81 |'
  prefs: []
  type: TYPE_TB
- en: Sparsity.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the less interpretable perplexity evaluation, we only prune the MLP layers.
    In the task-wise evaluation of LLaMA2-70B model, we prune both attention and MLP
    modules in accordance with previous works to better understand the performance
    gap between the pruned and original models. For attention module pruning, we use
    Wanda method, which is more efficient than SaprseGPT. In LlaMA2-70B, as the MLP
    module accounts for more than 80% total parameters, we observe the choice of attention
    pruning has almost no impact on the final performance. We apply a uniform sparsity
    ratio across all the pruned layers and conduct evaluations using three sparsity
    types: unstructured sparsity, and semi-structured sparsities of 4:8 and 2:4.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Language Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This evaluation involved a detailed examination of all the opened LLaMA2 models
    and Mistral 7B model. In perplexity evaluation, as shown in Table [2](#S4.T2 "Table
    2 ‣ Evaluation. ‣ 4.1 Settings ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models"), our method consistently achieves
    better performance than SparseGPT and Wanda in more constrained and practical
    N:M sparsity pattern. As indicated by Sun et al. ([2023](#bib.bib40)), where weight
    updates can improve the performance in N:M sparsity pattern, our method shows
    superior performance even without computationally expensive weight updates. For
    Mistral 7B and LLaMA2 70B models with larger MLP layers, our method also outperforms
    SparseGPT in unstructured sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Downstream Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While perplexity has been a popular metric in earlier LLM compression works
    (Frantar & Alistarh, [2023](#bib.bib14); Dettmers & Zettlemoyer, [2023](#bib.bib10)),
    it essentially measures the confidence of a language model in text prediction
    and doesn’t always reflect its proficiency in performing downstream tasks (Jaiswal
    et al., [2023](#bib.bib23)). Apart from assessing perplexity, we comprehensively
    evaluate the performance of pruned LLaMA2-70B models in downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Baselines. In Table [3](#S4.T3 "Table 3 ‣ Evaluation. ‣ 4.1
    Settings ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models"), we present the performance of different sparse LLaMA2-70B
    models on downstream tasks with prompting. The results show that our method outperforms
    SparseGPT and Wanda in most tasks at semi-structured N:M sparsity pattern including
    the aggregated task MMLU. The only exception is that SparseGPT outperforms both
    Wanda and DaSS in Winogrande task. For unstructured sparsity, our method outperforms
    Wanda sharing the same complexity level. It is noteworthy that the improvement
    of DaSS over Wanda becomes more pronounced in the challenging MMLU task, where
    it achieves an increase in accuracy of 1.17 and 1.12 for the 4:8 and 2:4 sparsity
    patterns, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Partial N:M Sparsity & Layer Sensitivity. In accordance with the observations
    with Jaiswal et al. ([2023](#bib.bib23)), we see a considerable performance degradation
    in knowledge-intensive MMLU task, in which only unstructured 50% sparsity models
    outperform the dense LLaMA2-34B model. As GPU plays a more important role in larger
    model inference, it is essential to improve the performance of pruned models in
    hardware-friendly N:M sparsity. Uniform reduction of the overall sparsity level
    is not feasible for N:M sparsity, Frantar & Alistarh ([2023](#bib.bib14)) suggests
    a specific subset of layers can be chosen for full N:M sparsification. Here we
    skip pruning 1/4 consecutive layers (20 layers) and result in a final 37.5% sparsity
    ratio. We use the first 10 tasks of MMLU to study pruning sensitivity. We divide
    the model into 4 consecutive parts and study skipping pruning each part. As shown
    in Table [4](#S4.T4 "Table 4 ‣ 4.3 Downstream Tasks ‣ 4 Experiments ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"), the earlier
    layers are more sensitive than the later ones in knowledge-intensitive tasks,
    which is contradictory to the findings in Frantar & Alistarh ([2023](#bib.bib14))
    using perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: MMLU subset accuracy after skipping pruning 20 layers at various start
    indices in 4:8 sparsity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Start Index | 0 | 10 | 20 | 40 | 60 | Dense |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Acc (%) | 60.12 | 61.72 | 59.75 | 56.54 | 56.83 | 64.86 |'
  prefs: []
  type: TYPE_TB
- en: We continue to search for the better skipping layers with the start layer index
    range in [0,20]. We found that starting skipping from layer 10 can achieve the
    best performance in the subset. Then we test its results in the full MMLU tasks.
    As shown in Table [3](#S4.T3 "Table 3 ‣ Evaluation. ‣ 4.1 Settings ‣ 4 Experiments
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models"), skipping sensitive 1/4 layers can significantly improve the performance
    of pruned models, especially for N:M sparsity. We can achieve sparse models that
    perform better than LLaMA2-34B. Although partial N:M sparsity models have more
    parameters than smaller dense models, training many smaller dense models like
    LLaMA2-34B is still computationally expensive. Efficient post-training pruning
    enables us to easily adjust the accuracy-efficiency trade-off in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: An extensive line of work (Haviv et al., [2023](#bib.bib21); Meng et al., [2022](#bib.bib30);
    Geva et al., [2023](#bib.bib18)) discussed how factual knowledge is stored in
    Transformer parameters, and early to middle MLP sublayers are demonstrated to
    be crucial for memorized predictions. Interestingly, the layer sensitivity results
    have a strong correlation with the findings in those works. We also apply the
    searched layer index in MMLU tasks to commonsense reasoning tasks, and we see
    fewer performance gains, which highlights the importance of task-specific pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Generalization to ReGLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We test the generalization ability to another GLU variant, ReGLU. In Table [5](#S4.T5
    "Table 5 ‣ 4.4 Generalization to ReGLU ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models"), we also observed DaSS outperform
    SparseGPT and Wanda in N:M sparsity pattern. This demonstrates the generalization
    ability of our method to other GLU variants. Since augmenting input activations
    is important for the expanding projection of non-GLU MLP modules, our methods
    cannot achieve better performance than Wanda in older LLMs that do not use GLU
    variants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: WikiText perplexity of ReluLLaMA models'
  prefs: []
  type: TYPE_NORMAL
- en: '| Size | 7B | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| Method | MLP Sparsity | PPL ($\downarrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | - | 6.15 | 5.50 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 4:8 | 8.44 | 7.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 4:8 | 9.21 | 7.74 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 4:8 | 8.19 | 7.16 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 2:4 | 10.26 | 8.98 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 2:4 | 12.68 | 9.87 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 2:4 | 9.61 | 8.18 |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 50% | 7.22 | 6.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 50% | 7.52 | 6.53 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 50% | 7.24 | 6.40 |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sparsity Variation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Figure [2](#S4.F2 "Figure 2 ‣ Sparsity Variation. ‣ 4.5 Performance Analysis
    ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in
    Large Language Models") illustrates a comparison of the mean zero-shot task accuracy
    at varying levels of sparsity for the MLP component of the LLaMA2-70B model. It’s
    evident that DaSS pruning maintains competitive performance which closely matches
    that of SparseGPT across the entire range of sparsity ratios tested. Notably,
    DaSS achieves such performance without the need for weight updates, suggesting
    its effectiveness and efficiency in locating sparse neural networks. On the other
    hand, output-balanced Wanda pruning shows a significant decline in accuracy as
    the sparsity ratio increases. This suggests that Wanda pruning may suffer from
    structural mismatch issues within the MLP layer, which become more pronounced
    at higher sparsity levels. As a result, the neural network’s performance deteriorates,
    potentially leading to a dysfunctional model at extreme sparsity ratios.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09f1ef1d7a48abb2c5dabb444b2dcf09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mean zero-shot tasks performance at different MLP sparsity ratios.'
  prefs: []
  type: TYPE_NORMAL
- en: Robustness to calibration samples.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ashkboos et al. ([2023](#bib.bib2)) observes that the intermediate activations
    of SwiGLU-based MLP layers exhibit high variance, primarily caused by the Hadamard
    product of the preceding two outputs. This leads to diminished accuracy in 4-bit
    quantization. In Figure [3](#S4.F3 "Figure 3 ‣ Robustness to calibration samples.
    ‣ 4.5 Performance Analysis ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models"), we present how varying the
    number of sequences sampled for calibration affects the performance of pruning
    methods. Even though only using intermediate activations, our method demonstrates
    minimal sensitivity to changes in the number of calibration samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dea6f2f4e6249ce61d9f7a73aebcd59f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Robustness to calibration samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Running Time Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pruning speed
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As observed in our preceding study on layer sensitivity, the efficiency of the
    pruning process is crucial when numerous iterations are necessary to reach the
    desired performance for specific tasks. For DaSS and Wanda, the computational
    complexity is quantified as $O(d^{2})$. We recorded the overall time taken for
    pruning MLP layers, not including the forward pass process, in accordance with
    the approach described by Sun et al. ([2023](#bib.bib40)). We use a single A6000
    48GB GPU to prune the 7B and 13B models, and use 8 A100 40GB GPUs to prune the
    larger 70B model.
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in Table [6](#S4.T6 "Table 6 ‣ Pruning speed ‣ 4.6 Running Time
    Analysis ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models"), the computational overhead incurred by DaSS is minimal,
    especially when compared to SparseGPT. While the processing speed of DaSS is marginally
    slower than that of Wanda, this can be attributed to Wanda’s more efficient approach
    of sorting weights exclusively along the last dimension. Nonetheless, given the
    substantial improvements in accuracy achieved by DaSS, this additional computational
    time is justifiable and beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Pruning speed (in seconds) comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Mistral | LLaMA-2 |'
  prefs: []
  type: TYPE_TB
- en: '| 7B | 7B | 13B | 70B |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGPT | 155 | 185 | 206 | 1092.76 |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 0.60 | 0.54 | 0.85 | 14.13 |'
  prefs: []
  type: TYPE_TB
- en: '| DaSS | 0.81 | 0.81 | 1.02 | 20.70 |'
  prefs: []
  type: TYPE_TB
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pruning LLM. Neural network pruning in LLM can be broadly categorized into
    two groups: structured pruning (Ma et al., [2023](#bib.bib29); Zhang et al., [2023](#bib.bib49))
    and unstructured pruning (Frantar & Alistarh, [2023](#bib.bib14); Sun et al.,
    [2023](#bib.bib40)). Ma et al. ([2023](#bib.bib29)) proposes a dependency detection
    algorithm to detect and prune non-critical grouped structures followed by LoRA
    fine-tuning. Although structured pruning can usually have better hardware efficiency,
    the accuracy drops a lot even at a low compression rate. Unstructured pruning
    can yield a higher compression rate and achieve acceleration on Nvidia’s GPUs
    by employing a hardware-friendly N:M sparsity pattern. SparseGPT (Frantar & Alistarh,
    [2023](#bib.bib14)) leverages the Hessian inverse for pruning and reduces reconstruction
    error of dense and sparse weights by subsequent weight updates. Wanda (Sun et al.,
    [2023](#bib.bib40)) employs an efficient method that augments input activations
    into weight magnitudes, and matches the performance of SparseGPT at medium sparsity.
    Our work incorporates dependency information into unstructured pruning, achieving
    a novel pruning paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: Inherent Sparsity of Transformer MLP. Interestingly, sparsity within the MLP
    activations of trained Transformer-based models occurs innately even without applying
    explicit regularizations or constraints (Zhang et al., [2022](#bib.bib50); Li
    et al., [2023](#bib.bib26); Dong et al., [2023](#bib.bib12)). Such a phenomenon
    is prevalent in learned Transformers, including other zero-saturating functions.
    Liu et al. ([2023](#bib.bib28)); Mirzadeh et al. ([2023](#bib.bib32)); Zhang et al.
    ([2022](#bib.bib50)) achieve actual LLM inference speedup by only performing computation
    corresponding to the activating neuron for a given input. They do not actually
    reduce the model size since they mainly reduce I/O and computation latency in
    a selective weights loading manner, and thus, these methods are less applicable
    in large batch-size inference settings. Our work investigates the weight sparsity
    in MLP module by considering corresponding intermediate activations.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier-dependent LLM Compression. Outlier features, defined as features with
    magnitudes substantially larger than others, are a notable characteristic of LLMs
    (Dettmers et al., [2022](#bib.bib11)). Despite making up only a small fraction
    of all feature dimensions, these outliers play a critical role in attention and
    predictive performance. Such observation has impeded the development of some LLM-specific
    quantization methods (Dettmers et al., [2022](#bib.bib11); Xiao et al., [2023](#bib.bib46);
    Lin et al., [2023](#bib.bib27); Ashkboos et al., [2023](#bib.bib2)) to handle
    outliers more effectively. Wanda (Sun et al., [2023](#bib.bib40)) broadens these
    insights, revealing that outlier features are significant in deciding weight importance
    when pruning LLMs. Our analysis diverges from conventional wisdom by demonstrating
    that, in the context of SwiGLU-based MLP input projections, the influence of outlier
    features is not as pronounced as previously assumed, prompting a reevaluation
    of their role in LLM compression strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose the Dependency-aware Semi-structured Sparsity (DaSS) method which
    effectively addresses the challenges of pruning SwiGLU-based MLP modules LLMs.
    DaSS strikes a unique balance between the adaptability of unstructured pruning
    and the orderliness of structured pruning. By leveraging the MLP intermediate
    activation norms as the group importance indicator, we develop a novel pruning
    metric that assesses weight importance in a more structurally consistent manner.
    Empirical evaluations on the Mistral and LLaMA2 model families demonstrate that
    DaSS surpasses state-of-the-art LLM pruning methods like SparseGPT and Wanda in
    achieving hardware-friendly N:M sparsity patterns. Our research further reveals
    that selectively skipping the pruning of earlier layers can lead to significant
    performance improvements in knowledge-intensive tasks. The efficiency of the pruning
    process becomes paramount when multiple iterations are essential to tailor and
    optimize the sparsity of LLMs for varied tasks. This underscores the potential
    of our DaSS method in facilitating rapid and task-specific model pruning to achieve
    optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Impact Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The present study is focused on advancing the field of large language model
    deployment with an emphasis on efficiency. While there are numerous societal implications
    of our work, we believe none require specific emphasis in this context.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy,
    Y., Lebrón, F., and Sanghai, S. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashkboos et al. (2023) Ashkboos, S., Markov, I., Frantar, E., Zhong, T., Wang,
    X., Ren, J., Hoefler, T., and Alistarh, D. Towards end-to-end 4-bit inference
    on generative large language models. *the 3rd NeurIPS Workshop on Efficient Natural
    Language and Speech Processing (ENLSP)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
    Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating
    large language models trained on code. *arXiv preprint arXiv:2107.03374*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with
    pathways. *Journal of Machine Learning Research*, 24(240):1–113, 2023. URL [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dauphin et al. (2017) Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language
    modeling with gated convolutional networks. In *International conference on machine
    learning*, pp.  933–941\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for
    4-bit precision: k-bit inference scaling laws. In *International Conference on
    Machine Learning*, pp.  7750–7774\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Oh,
    A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), *Advances in Neural Information
    Processing Systems*, 2022. URL [https://openreview.net/forum?id=dXiGWqBoxaD](https://openreview.net/forum?id=dXiGWqBoxaD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Dong, H., Chen, B., and Chi, Y. Towards structured sparsity
    in transformers for efficient inference. In *Workshop on Efficient Systems for
    Foundation Models @ ICML2023*, 2023. URL [https://openreview.net/forum?id=c4m0BkO4OL](https://openreview.net/forum?id=c4m0BkO4OL).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2023) Fang, G., Ma, X., Song, M., Mi, M. B., and Wang, X. Depgraph:
    Towards any structural pruning. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pp.  16091–16101, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. SparseGPT: Massive language
    models can be accurately pruned in one-shot. In Krause, A., Brunskill, E., Cho,
    K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings of the 40th
    International Conference on Machine Learning*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  10323–10337\. PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/frantar23a.html](https://proceedings.mlr.press/v202/frantar23a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. OPTQ: Accurate quantization for generative pre-trained transformers. In *The
    Eleventh International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023) Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., and Khot, T.
    Chain-of-thought hub: A continuous effort to measure large language models’ reasoning
    performance. *arXiv preprint arXiv:2305.17306*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, September 2021. URL [https://doi.org/10.5281/zenodo.5371629](https://doi.org/10.5281/zenodo.5371629).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geva et al. (2023) Geva, M., Bastings, J., Filippova, K., and Globerson, A.
    Dissecting recall of factual associations in auto-regressive language models.
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023. URL [https://openreview.net/forum?id=F1G7y94K02](https://openreview.net/forum?id=F1G7y94K02).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glorot et al. (2011) Glorot, X., Bordes, A., and Bengio, Y. Deep sparse rectifier
    neural networks. In *Proceedings of the fourteenth international conference on
    artificial intelligence and statistics*, pp.  315–323\. JMLR Workshop and Conference
    Proceedings, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights
    and connections for efficient neural network. *Advances in neural information
    processing systems*, 28, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haviv et al. (2023) Haviv, A., Cohen, I., Gidron, J., Schuster, R., Goldberg,
    Y., and Geva, M. Understanding transformer memorization recall through idioms.
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pp.  248–264, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal et al. (2023) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and
    Yang, Y. Compressing llms: The truth is rarely pure and never simple. *arXiv preprint
    arXiv:2310.01382*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurtic et al. (2023) Kurtic, E., Kuznedelev, D., Frantar, E., Goin, M., and
    Alistarh, D. Sparse finetuning for inference acceleration of large language models.
    *arXiv preprint arXiv:2310.06927*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A. S., Reddi,
    S. J., Ye, K., Chern, F., Yu, F., Guo, R., and Kumar, S. The lazy neuron phenomenon:
    On emergence of activation sparsity in transformers. In *The Eleventh International
    Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=TJ2nxciYCk-](https://openreview.net/forum?id=TJ2nxciYCk-).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,
    Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity
    for efficient llms at inference time. In *International Conference on Machine
    Learning*, pp.  22137–22176\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural
    pruning of large language models. *Advances in Neural Information Processing Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2022) Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating
    and editing factual associations in gpt. *Advances in Neural Information Processing
    Systems*, 35:17359–17372, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2017) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models. In *International Conference on Learning Representations*,
    2017. URL [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirzadeh et al. (2023) Mirzadeh, I., Alizadeh, K., Mehta, S., Del Mundo, C. C.,
    Tuzel, O., Samei, G., Rastegari, M., and Farajtabar, M. Relu strikes back: Exploiting
    activation sparsity in large language models. *arXiv preprint arXiv:2310.04564*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra et al. (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural
    networks. *arXiv preprint arXiv:2104.08378*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramachandran et al. (2017) Ramachandran, P., Zoph, B., and Le, Q. V. Swish:
    a self-gated activation function. *arXiv: Neural and Evolutionary Computing*,
    2017. URL [https://api.semanticscholar.org/CorpusID:196158220](https://api.semanticscholar.org/CorpusID:196158220).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shazeer (2019) Shazeer, N. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shazeer (2020) Shazeer, N. Glu variants improve transformer. *arXiv preprint
    arXiv:2002.05202*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and
    effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Team (2023) Team, S. Sparse large language models with relu activation, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016) Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. Learning structured
    sparsity in deep neural networks. *Advances in neural information processing systems*,
    29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface’s
    transformers: State-of-the-art natural language processing. *arXiv preprint arXiv:1910.03771*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023) Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia,
    Y., Pechenizkiy, M., Liang, Y., Wang, Z., and Liu, S. Outlier weighed layerwise
    sparsity (owl): A missing secret sauce for pruning llms to high sparsity. *arXiv
    preprint arXiv:2310.05175*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.,
    et al. Pruning meets low-rank parameter-efficient fine-tuning. *arXiv preprint
    arXiv:2305.18403*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, Z., Lin, Y., Liu, Z., Li, P., Sun, M., and Zhou,
    J. MoEfication: Transformer feed-forward layers are mixtures of experts. In Muresan,
    S., Nakov, P., and Villavicencio, A. (eds.), *Findings of the Association for
    Computational Linguistics: ACL 2022*, pp.  877–890, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.71.
    URL [https://aclanthology.org/2022.findings-acl.71](https://aclanthology.org/2022.findings-acl.71).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Model Configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the configurations of the models used in the paper. We don’t use LLaMA2-34B
    since it was not released. ReluLLaMA uses the same configuration as LLaMA2, the
    only difference is the activation function. Here is the link to ReluLLaMA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ReluLLaMA-7B: https://huggingface.co/SparseLLM/ReluLLaMA-7B'
  prefs: []
  type: TYPE_NORMAL
- en: 'ReluLLaMA-13B: https://huggingface.co/SparseLLM/ReluLLaMA-13B'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Model configurations of Llama2 and Mistral models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Param | Layers | Hidden | Intermediate | Query Heads | KV Head |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LlaMa2-7B | 7B | 32 | 4096 | 11008 | 32 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| LlaMa2-13B | 13B | 40 | 5120 | 13824 | 40 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| LlaMa2-70B | 70B | 80 | 8192 | 28672 | 64 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-7B | 7B | 32 | 4096 | 14336 | 32 | 8 |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Activation Outlier in SwiGLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea3bcb01f9485bfbc48d896e86b50818.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Layerwise Activation Outlier distribution of LLaMA2-7B'
  prefs: []
  type: TYPE_NORMAL
- en: In Section 2.3, we show the importance of augmenting input magnitude is useful.
    In this part, we target to identify the existence of outliers in the intermediate
    activations of SwiGLU. Directly analyzing it like model hidden states Dettmers
    et al. ([2022](#bib.bib11)); Xiao et al. ([2023](#bib.bib46)) is challenging since
    it demonstrates high variance. (Ashkboos et al., [2023](#bib.bib2)). Wanda (Sun
    et al., [2023](#bib.bib40)) shows $\ell_{2}$ as an empirical value 7\. The layerwise
    distribution of the SwiGLU intermediate activation outlier is shown in Figure
    [4](#A2.F4 "Figure 4 ‣ Appendix B Activation Outlier in SwiGLU ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models").
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C GeLU-based MLP Input projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the older MLP module, unlike GLU variants, there are only two projection
    matrices. We investigate magnitude pruning in different granularity for the MLP
    input projection matrix. We use the Falcon-7B model, which uses GeLU-based MLP.
    ”Layer Magnitude” means we group weights in the entire weight matrix using magnitude
    pruning. In Table [8](#A3.T8 "Table 8 ‣ Appendix C GeLU-based MLP Input projection
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models"), we surprisingly find the input-balanced magnitude pruning achieves much
    worse results. This is in contrast to the results in GLU variants. The results
    show the huge significance of input activations for GeLU-based input projection
    pruning. Since our DaSS method needs to group weights per input, our method is
    less applicable to older MLP modules. However, GeLU-based MLP is rarely used in
    the current state-of-the-art LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: The Wikitext perplexity results of GeLU input projection at different
    sparsity ratio'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sparsity | 50% | 60% | 70% |'
  prefs: []
  type: TYPE_TB
- en: '| Wanda | 7.17 | 8.72 | 18.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Output-balanced Magnitude | 8.46 | 15.27 | 498.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Input-balanced Magnitude | 1708 | 10587 | 13706 |'
  prefs: []
  type: TYPE_TB
- en: '| Layer Magnitude | 21.82 | 1895 | 12896 |'
  prefs: []
  type: TYPE_TB
