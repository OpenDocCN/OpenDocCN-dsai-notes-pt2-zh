- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:52:43'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02376](https://ar5iv.labs.arxiv.org/html/2406.02376)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhiwei Cao^(1,3)¹¹1These authors contributed equally. This work was done when
    Zhiwei Cao was interning at ByteDance.,  Qian Cao²¹¹1These authors contributed
    equally. This work was done when Zhiwei Cao was interning at ByteDance.,  Yu Lu²,  Ningxin
    Peng²,  Luyang Huang²
  prefs: []
  type: TYPE_NORMAL
- en: Shanbo Cheng²²²2Corresponding author.  and  Jinsong Su^(1,3)²²2Corresponding
    author.
  prefs: []
  type: TYPE_NORMAL
- en: ¹School of Informatics, Xiamen University   ²ByteDance Research
  prefs: []
  type: TYPE_NORMAL
- en: ³Shanghai Artificial Intelligence Laboratory
  prefs: []
  type: TYPE_NORMAL
- en: lines1@stu.xmu.edu.cn  {caoqian.95, luyu.ly, chengshanbo}@bytedance.com  jssu@xmu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The growing popularity of Large Language Models has sparked interest in context
    compression for Large Language Models (LLMs). However, the performance of previous
    methods degrades dramatically as compression ratios increase, sometimes even falling
    to the closed-book level. This decline can be attributed to the loss of key information
    during the compression process. Our preliminary study supports this hypothesis,
    emphasizing the significance of retaining key information to maintain model performance
    under high compression ratios. As a result, we introduce Query-Guided Compressor
    (QGC), which leverages queries to guide the context compression process, effectively
    preserving key information within the compressed context. Additionally, we employ
    a dynamic compression strategy. We validate the effectiveness of our proposed
    QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and
    HotpotQA datasets. Experimental results show that QGC can consistently perform
    well even at high compression ratios, which also offers significant benefits in
    terms of inference cost and throughput¹¹1Our code is available at [https://github.com/DeepLearnXMU/QGC](https://github.com/DeepLearnXMU/QGC)..
  prefs: []
  type: TYPE_NORMAL
- en: 'Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: Zhiwei Cao^(1,3)¹¹1These authors contributed equally. This work was done when
    Zhiwei Cao was interning at ByteDance.,  Qian Cao²¹¹1These authors contributed
    equally. This work was done when Zhiwei Cao was interning at ByteDance.,  Yu Lu²,  Ningxin
    Peng²,  Luyang Huang² Shanbo Cheng²²²2Corresponding author.  and  Jinsong Su^(1,3)²²2Corresponding
    author. ¹School of Informatics, Xiamen University   ²ByteDance Research ³Shanghai
    Artificial Intelligence Laboratory lines1@stu.xmu.edu.cn  {caoqian.95, luyu.ly,
    chengshanbo}@bytedance.com  jssu@xmu.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The emergence of chatGPT Ouyang et al. ([2022](#bib.bib24)) and GPT4 OpenAI
    ([2023](#bib.bib23)), along with other Large Language Models (LLMs) Touvron et al.
    ([2023a](#bib.bib28), [b](#bib.bib29)) has sparked a global sensation. The success
    of LLMs is closely tied to the long context capabilities of LLMs Dong et al. ([2022](#bib.bib6));
    Lewis et al. ([2020](#bib.bib15)), especially in the field of multi-document question
    answering. However, the utilization of long context also introduces challenges
    such as higher inference cost, longer latency, and inferior performance caused
    by redundant information Jiang et al. ([2023](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Many efforts have been made to compress the long context by directly removing
    a certain percentage of less important words, such as LongLLMLingua Jiang et al.
    ([2023](#bib.bib13)) and Selective-Context Li et al. ([2023](#bib.bib16)). Another
    common method is to generate a text summary of the given context Xu et al. ([2023](#bib.bib34));
    Wang et al. ([2023b](#bib.bib31)). Unlike deleting or reordering the word in the
    context, AutoCompressor Chevalier et al. ([2023](#bib.bib4)) compresses long documents
    into multiple vectors as soft prompts, which are optimized with full parameters
    of LLMs. However, our preliminary study shows that these methods have a common
    flaw: as the compression ratio increases, the compressed context fails to retain
    key information, resulting in a significant decrease in the performance of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: The key to solve this problem is query, which defines what key information is.
    We aim to preserve this query-related key information even at a high compression
    ratio. Specifically, we propose the Query-Guided Compressor (QGC) to fully utilize
    query information throughout each compression step. We first feed the query and
    the documents together into a context encoder to learn the query-guide document
    representations. We then compress these document representations into $n$-gram
    structure rather than deleting words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We validate the effectiveness of QGC on the multi-document Question Answering
    task, including three datasets: NaturalQuestions, TriviaQA, and HotpotQA. Experimental
    results on the QA task indicate that, compared to LongLLMLingua, QGC exhibits
    a 2.75 times higher compression ratio and a 2.42 times higher throughput. Additionally,
    its accuracy has improved by an average of 5 points. We further investigated the
    loss of key information throughout the compression process. The findings reveal
    that under high compression ratios and high noise conditions, QGC only incurs
    a performance loss of about 10%, while LongLLMLingua suffers a loss of approximately
    47%. This validates the effectiveness of QGC in retaining key information.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminary Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first briefly formulate the long context compression on
    the Question Answering task, and then present an analysis on the key information
    loss in previous compression methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Task Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given a LLM input with augmented context $\mathbf{x}=(\mathbf{x}^{ins},\mathbf{x}^{d_{1}},...,\mathbf{x}^{d_{k}},...,\mathbf{x}^{d_{K}},\mathbf{x}^{q})$,
    the objective of context compression can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\mathbf{\widetilde{x}}}d(\text{LLM}(\mathbf{y}&#124;\mathbf{x}),\text{LLM}(\mathbf{\widetilde{y}}&#124;\mathbf{\widetilde{x}})),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{y}$ retrieved documents that greatly determine the length of
    the input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5fef6eccd73ba78e3b7bed439d247257.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Compression Ratio for LongLLMLingua
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d475dea77626fdca0370f2dcf908fff.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Document Number for AutoCompressor
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The accuracy of LongLLMLingua Jiang et al. ([2023](#bib.bib13)) and
    AutoCompressor Chevalier et al. ([2023](#bib.bib4)) with different compression
    ratios and number of documents on the NaturalQuestions test set, respectively.
    Closed-book denotes providing LLMs with the question only, and Oracle means using
    the question and corresponding ground-truth documents as the input of the LLM.
    “w/ answer” means adding the golden answer to the compressed context.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Key Information Loss in Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We study the effectiveness of two representative methods, LongLLMLingua Jiang
    et al. ([2023](#bib.bib13)) and AutoCompressor Chevalier et al. ([2023](#bib.bib4)).
    We conduct experiments on the NaturalQuestions dataset Liu et al. ([2023](#bib.bib17))
    and use accuracy as the evaluation metric, which judges whether any correct answers
    appear in the LLM prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'For LongLLMLingua, we apply LLaMA-2-7B-Chat²²2https://ai.meta.com/llama/ as
    the small language model for compression, and use LongChat-13B-16K³³3https://huggingface.co/lmsys/longchat-13b-16k
    as the target LLM. We use the open-source AutoCompressor⁴⁴4https://github.com/princeton-nlp/AutoCompressors,
    which fine-tunes LLaMA-2-7B to compress context and generate answers. Here, we
    consider four settings:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closed-book. It takes the query as the LLM input with no additional documents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle. The query and only the document containing the ground truth are used
    as inputs to the LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base. Based on Oracle, we compress the document directly with various compression
    ratios for LongLLMLingua. However, since AutoCompressor is set to compress documents
    to fixed length vectors, we change the compression ratio by adding external documents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base w/ answer. We manually add key information to the compressed results by
    concatenating the answer with the compressed word sequence in LongLLMLingua. Note
    that this setting is impractical for AutoCompressor where the compressed results
    are vectors that cannot be changed directly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Task Formulation ‣ 2 Preliminary Study
    ‣ Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs"), we find that the performance of both methods degrades significantly
    with increasing compression ratios. As shown in Figure [1(a)](#S2.F1.sf1 "In Figure
    1 ‣ 2.1 Task Formulation ‣ 2 Preliminary Study ‣ Retaining Key Information under
    High Compression Ratios: Query-Guided Compressor for LLMs"), the performance of
    LongLLMLingua decreases by 47% as the compression ratio increases from 1.53x to
    3.44x. Even worse, the accuracy of LongLLMLingua at 3.44x compression ratio is
    equivalent to that of the closed-book setting. The same findings are illustrated
    in Figure [1(b)](#S2.F1.sf2 "In Figure 1 ‣ 2.1 Task Formulation ‣ 2 Preliminary
    Study ‣ Retaining Key Information under High Compression Ratios: Query-Guided
    Compressor for LLMs") for AutoCompressor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly, we observe that adding key information to the compressed
    result can greatly alleviate the performance degradation that typically occurs
    at high compression ratios. Back to Figure [1(a)](#S2.F1.sf1 "In Figure 1 ‣ 2.1
    Task Formulation ‣ 2 Preliminary Study ‣ Retaining Key Information under High
    Compression Ratios: Query-Guided Compressor for LLMs"), the accuracy line fluctuates
    little as the compression ratio increases from 1.5x to 3.5x with the help of additional
    key information, which is a decrease of 3.87% compared to the former 47% with
    the loss of key information. These observations validate the need to preserve
    key information during compression, which motivates us to explore a better method
    to fully exploit query information for context compression.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Query-Guided Compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ac3a11a2218874d912c1e209581a176.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The framework of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Query-Guided Compression ‣ Retaining
    Key Information under High Compression Ratios: Query-Guided Compressor for LLMs"),
    we equip the LLM with the Query-Guided Compressor to compress long documents into
    a much shorter sequence of continuous representations, which are then concatenated
    with the corresponding instruction and query as the input for the LLM. In the
    following, we first introduce the architecture of Query-Guided Compressor and
    then its training objective. Then, we propose a dynamic compression strategy that
    assigns higher compression ratios for irrelevant documents to further improve
    the compressed representations.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Compressor Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ Semantic Alignment Layer ‣ 3.1 Compressor Architecture
    ‣ 3 Query-Guided Compression ‣ Retaining Key Information under High Compression
    Ratios: Query-Guided Compressor for LLMs") illustrates the basic architecture
    of our Query-Guided Compressor. Using the compressor, we adopt the following steps
    to produce compressed representations of each document: 1) learning the query-aware
    document representations; 2) compressing the document representations into $n$
    of the document for simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: Query-Guided Context Encoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the first step, we feed the concatenation of the query $\mathbf{x}^{q}$ into
    query-aware context encoder to learn the representations of the query and the
    document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder consists of two Transformer encoder layers. Formally, these representations
    can be obtained in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle[\mathbf{h}^{q};\mathbf{h}^{d}]=\text{ContextEncoder}([\mathbf{x}^{q};\mathbf{x}^{d}]).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathbf{h}^{q}$, respectively. By allowing the query and the document
    to see each other during encoding, we can facilitate the extraction of the key
    information relevant to the query in the document.
  prefs: []
  type: TYPE_NORMAL
- en: Query-Guided Pooling Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the next step, we split the entire document into several $n$-gram into a
    vector based on their correlation to the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, document representations are organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{h}^{d}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=[\mathbf{h}^{d}_{1:n},...,\mathbf{h}^{d}_{(j-1)\times
    n:j\times n},...,\mathbf{h}^{d}_{N_{d}-n+1:N_{d}}],$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{G}_{j}$-grams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we measure the weight of each token in $\mathbf{G}_{j}$ of query tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\overline{h}^{q}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle w_{i,\mathbf{G}_{j}}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $s(\cdot,\cdot)$-gram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we acquire the compressed $n$-gram:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{h}^{d}_{\mathbf{G}_{j}}=\sum_{i\in\mathbf{G}_{j}}w_{i,\mathbf{G}_{j}}\cdot
    h^{d}_{i}.$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Query-Document Reviewing Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To further prevent the key information loss in compression, we introduce a novel
    reviewing module to perfect the compressed $n$-gram representations by revising
    both the query and the document representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, this encoder consists of two Transformer encoder layers, which
    takes the query representations $\mathbf{h}^{q}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\widetilde{\mathbf{h}}^{d}=\text{ReviewingLayer}([\mathbf{h}^{q};\mathbf{h}^{d};\hat{\mathbf{h}}^{d}]).$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Semantic Alignment Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since $\widetilde{\mathbf{h}}^{d}$ can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{e}^{d}=\textbf{W}\cdot\widetilde{\mathbf{h}}^{d}+\textbf{b},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where W and b are learnable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3059d8f89e7e0502e057686a9baafa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The structure of QGC. The first three layers use query $q$ encoding,
    pooling, and reviewing respectively. The last layer aligns document representations
    into the target LLM embedding space.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Compressor Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike AutoCompressor Chevalier et al. ([2023](#bib.bib4)), we fix the parameter
    of the LLM and only fine-tune the compressor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the above steps, each long document is compressed into a shorter sequence
    of continuous representations $\mathbf{e}^{d}$. To avoid missing the key information
    during compression, we define the training objective of the compressor in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\log{p(\mathbf{y}&#124;\widetilde{\mathbf{x}})}+\text{KL}[p(\mathbf{y}&#124;\mathbf{x})&#124;&#124;p(\mathbf{y}&#124;\widetilde{\mathbf{x}})],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\text{KL}[\cdot||\cdot]$ represents the Kullback–Leibler divergence.
    By introducing the KL loss, we encourage the LLM to generate the correct answer
    even with compressed representations as input.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Dynamically Compressing Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Due to the different importance of retrieved documents, we propose to dynamically
    adjust the compression ratios for different retrieved documents. Specifically,
    we assign the $n$-th document based on the importance ranking:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle n_{k}=\begin{cases}\min(2\cdot O_{k},16)&amp;S_{k}\geq\epsilon\\
    \infty&amp;S_{k}<\epsilon\end{cases},$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $S_{k}$, the corresponding document will be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | NaturalQuestions | TriviaQA | HotpotQA |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | CR | TP | EM | CR | TP | F1 | CR | TP |'
  prefs: []
  type: TYPE_TB
- en: '| LongChat-13B |'
  prefs: []
  type: TYPE_TB
- en: '| Closed-book | 34.84 | - | - | 36.07 | - | - | 22.19 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle | 83.05 | 59.2x | - | - | - | - | 60.61 | 42.2x | - |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt | 53.11 | 1.0x | - | 48.70 | 1.0x | - | 44.76 | 1.0x | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reranker-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence-BERT Reimers and Gurevych ([2020](#bib.bib27)) | 60.75 | 4.1x |
    0.137 | 48.89 | 4.5x | 1.957 | 42.92 | 4.4x | 1.930 |'
  prefs: []
  type: TYPE_TB
- en: '| BGE-Reranker Xiao et al. ([2023](#bib.bib33)) | 64.33 | 4.1x | 0.138 | 47.71
    | 4.5x | 1.724 | 47.96 | 4.4x | 1.689 |'
  prefs: []
  type: TYPE_TB
- en: '| Cond.PPL Jiang et al. ([2023](#bib.bib13)) | 65.91 | 4.1x | 0.128 | 52.48
    | 4.5x | 1.287 | 49.82 | 4.3x | 1.267 |'
  prefs: []
  type: TYPE_TB
- en: '| Compression-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Selective-Context Li et al. ([2023](#bib.bib16)) | 35.44 | 2.5x | 0.077 |
    42.73 | 2.5x | 0.465 | 29.68 | 2.6x | 0.456 |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua Jiang et al. ([2023](#bib.bib13))$\dagger$ | 66.70 | 3.9x |
    - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua Jiang et al. ([2023](#bib.bib13)) | 67.01 | 4.1x | 0.118 |
    51.51 | 3.7x | 0.724 | 45.43 | 3.8x | 0.683 |'
  prefs: []
  type: TYPE_TB
- en: '| QGC | 69.19 | 15.2x | 0.356 | 57.72 | 7.9x | 1.832 | 52.12 | 8.8x | 1.849
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Closed-book | 32.35 | - | - | 30.70 | - | - | 10.54 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle | 73.45 | 59.2x | - | - | - | - | 57.68 | 42.2x | - |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt | 27.53 | 1.0x | - | 49.47 | 1.0x | - | 44.24 | 1.0x | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reranker-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence-BERT Reimers and Gurevych ([2020](#bib.bib27)) | 24.26 | 4.1x |
    0.133 | 49.49 | 4.5x | 0.731 | 40.65 | 4.4x | 0.752 |'
  prefs: []
  type: TYPE_TB
- en: '| BGE-Reranker Xiao et al. ([2023](#bib.bib33)) | 25.08 | 4.1x | 0.130 | 48.69
    | 4.5x | 0.683 | 46.13 | 4.4x | 0.724 |'
  prefs: []
  type: TYPE_TB
- en: '| Cond.PPL Jiang et al. ([2023](#bib.bib13)) | 27.87 | 4.1x | 0.123 | 52.76
    | 4.5x | 0.602 | 47.84 | 4.3x | 0.623 |'
  prefs: []
  type: TYPE_TB
- en: '| Compression-based Methods |'
  prefs: []
  type: TYPE_TB
- en: '| Selective-Context Li et al. ([2023](#bib.bib16)) | 31.79 | 2.6x | 0.082 |
    48.55 | 2.5x | 0.303 | 28.21 | 2.6x | 0.332 |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua Jiang et al. ([2023](#bib.bib13)) | 41.13 | 4.1x | 0.108 |
    50.44 | 3.7x | 0.432 | 39.87 | 3.8x | 0.438 |'
  prefs: []
  type: TYPE_TB
- en: '| AutoCompressor Chevalier et al. ([2023](#bib.bib4)) | 49.23 | 13.9x | 0.302
    | 29.17 | 8.7x | 0.823 | 29.02 | 8.1x | 0.833 |'
  prefs: []
  type: TYPE_TB
- en: '| ICAE Ge et al. ([2023](#bib.bib7)) | 53.34 | 21.5x | - | 48.91 | 10.2x |
    - | 34.50 | 9.5x | - |'
  prefs: []
  type: TYPE_TB
- en: '| QGC | 60.90 | 15.2x | 0.313 | 57.46 | 7.9x | 0.902 | 51.64 | 8.8x | 0.927
    |'
  prefs: []
  type: TYPE_TB
- en: '| QGC($\epsilon=0.42$) | 57.62 | 20.6x | - | 57.11 | 10.9x | - | 51.23 | 12.1x
    | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Experimental results on three benchmark datasets. Acc = accuracy,
    EM = exact match, F1 = F1 score, CR = compression ratio, TP = throughput (examples/second).
    Closed-book, Oracle, and Original Prompt denote using the query only, the complete
    ground-truth documents, and all retrieved documents as inputs, respectively. $\dagger$
    indicates that the results are directly cited from Jiang et al. ([2023](#bib.bib13)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct extensive experiments to investigate the effectiveness
    of QGC.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets & Evaluation Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The experiments are carried out based on the three datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NaturalQuestions We select the processed version Liu et al. ([2023](#bib.bib17))
    where each question has 20 related documents and only one of them contains the
    correct answer. We follow Liu et al. ([2023](#bib.bib17)) to use accuracy (Acc)
    as the evaluation metric, which judges whether the correct answer appears in the
    prediction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TriviaQA We employ the adversarial Contriever Izacard et al. ([2022a](#bib.bib10))
    to retrieve the top 10 documents from all Wikipedia passages. Following Lewis
    et al. ([2020](#bib.bib15)), we use the Exact Match (EM) metric to evaluate the
    LLM prediction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HotpotQA Different from the above two datasets, HotpotQA [Yang et al.](#bib.bib35)
    is a multi-hop dataset where the answer lies in more than one document. Specifically,
    each question has 10 related documents and two of them are ground-truth documents.
    Following [Yang et al.](#bib.bib35) , we use the F1 score to measure the correctness
    of the LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides, we calculate the compression ratio (CR) for different methods, which
    is defined as the length rate of the original context to the compressed context.
    We also provide the inference throughput (TP) on a single A100-80G GPU, including
    compression and generation.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following Jiang et al. ([2023](#bib.bib13)), we include two sets of methods
    as our baselines.
  prefs: []
  type: TYPE_NORMAL
- en: '1) Reranker-based Methods. It simply uses a reranker method to sort documents
    based on importance and discards unimportant ones. We select the following reranker:
    Sentence-BERT Reimers and Gurevych ([2020](#bib.bib27)), BGE-Reranker Xiao et al.
    ([2023](#bib.bib33)), and Cond.PPL proposed by Jiang et al. ([2023](#bib.bib13))
    to measure the association between the query and documents. Then, we discard documents
    with low association until the compression ratio is met and sort the remaining
    documents according to the association from high to low.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Compression-based Methods. Compared with reranker-based methods, they further
    compress the sorted documents, retaining more information while satisfying a higher
    compression ratio. We select the following methods as our baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selective-Context Li et al. ([2023](#bib.bib16)) It uses self-information estimated
    by an external language model to prune redundant words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LongLLMLingua Jiang et al. ([2023](#bib.bib13)) It is the state-of-the-art method
    for long context compression. It first uses a language model to quantify the importance
    of each document as its question-aware perplexity, and then designs a question-aware
    coarse-to-fine compression method to delete unimportant tokens.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoCompressor Chevalier et al. ([2023](#bib.bib4)) It fine-tunes LLaMA-2-7B
    to recursively compress long context into summary vectors, which are used as soft
    prompts to generate the answer. We use the released AutoCompressor-Llama-2-7B-6K
    for experiments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ICAE Ge et al. ([2023](#bib.bib7)) Similar to AutoCompressor, it generates compact
    and informative memory slots to represent the original context. We use the released
    ICAE model pre-trained on Llama-2-7B-Chat for experiments ⁵⁵5https://github.com/getao/icae.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use LongChat-13B-16K and LLaMA-2-7B as the LLMs for evaluation, which are
    frozen during the optimization of QGC. To ensure stable and reproducible results,
    we employ greedy decoding and set the temperature to 0 in all experiments. Following Jiang
    et al. ([2023](#bib.bib13)), we use LLaMA-2-7B-Chat as the external language model
    for Selective-Context and LongLLMLingua. For QGC, both the query-guided context
    encoder and query-document reviewing layer consist of two Transformer encoder
    layers. All these layers and word embeddings are initialized with LLaMA-2-7B where
    MLP parameters are all fixed during training. Our rationale behind this approach
    stems from our belief that the MLP plays a crucial role in knowledge retention,
    while our focus lies in adjusting the acquired knowledge based on query. Thus,
    the trainable parameters in QGC are only 3.5% of LongChat-13B-16K. Besides the
    ground-truth document, we concatenate 1-4 random documents to build the long context.
    We also randomly set the n-gram size from the candidate list (4, 6, 8, 10) for
    each training batch to make the compressor more robust. We train QGC on downstream
    datasets for 15 epochs, using a learning rate of 5e-5 with the Adam optimizer
    and batch size of 64. During inference, we use the Cond.PPL proposed by Jiang
    et al. ([2023](#bib.bib13)) to sort retrieved documents for all compression-based
    methods and QGC, and set the $\epsilon$ as 0.35. Following Liu et al. ([2023](#bib.bib17));
    Bai et al. ([2023](#bib.bib1)) the maximum generation tokens is 100 for NaturalQuestions,
    and 32 for both TriviaQA and HotpotQA. All experiments are conducted on 8 NVIDIA
    A100 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| QGC | 69.19 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-2  w/o query-guided context encoder | 50.36 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o query-guided pooling layer | 55.34 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o query-document reviewing layer | 64.14 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o dynamically compressing strategy | 62.15 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The accuracy of ablation study on NaturalQuestions test set, where
    the target LLM is LongChat-13B.'
  prefs: []
  type: TYPE_NORMAL
- en: Main Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.3 Dynamically Compressing Strategy ‣ 3 Query-Guided
    Compression ‣ Retaining Key Information under High Compression Ratios: Query-Guided
    Compressor for LLMs") reports the performance, compression ratios, and throughput
    of various methods or models on different datasets. Overall, QGC achieves higher
    compression ratios and greater throughput while achieving comparable or even better
    performance with LongLLMLingua. These results demonstrate that QGC can effectively
    compress context into shorter inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the performance and compression ratio of the reranker-based methods
    are limited because no compression operation is used within the document. Compared
    to AutoCompressor and ICAE, our method achieves better accuracy with comparable
    compression ratios. Compared with LongLLMLingua, QGC achieves average +5.03 and
    +12.87 performance improvements when using LongChat-13B and LLaMA-2-7B as the
    target LLMs. On average, the compression ratio and throughput of QGC are 2.75
    times and 2.47 times that of LongLLMLingua on all datasets and target LLMs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d90c53319b9317e49db89bc0223dff1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Compression Ratio for QGC
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4dd0a41d7abf596bc452ffa7edf3df93.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Document Number for QGC
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: The accuracy of QGC with varying compression ratios and number of
    documents, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the effect of different components on QGC, we use LongChat-13B as
    the target LLM and introduce the following variants of QGC for ablation study:
    1) w/o query-guided context encoder. In this variant, the query and document are
    independently encoded; 2) w/o query-guided pooling layer. When establishing this
    variant, we directly replace the weighted sum of token representations in each
    $n$-gram size as 4 for comparable comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [2](#S4.T2 "Table 2 ‣ Implementation Details ‣ 4 Experiments
    ‣ Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs"), the absence of the query-document reviewing layer and dynamically
    compressing strategy lead to a 5.05 and 7.04 accuracy loss respectively. The more
    substantial loss is observed after removing the query-guided context encoder and
    query-guided pooling layer, resulting in a significant performance accuracy drop
    of 18.83 and 13.85 respectively, highlighting the importance of employing the
    query to guide compression.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct in-depth analyses to explore the performance of
    QGC in terms of key information loss, demonstration compression, detailed throughput
    and reranker impact. All analyses are conducted on NaturalQuestions with target
    LLM as LongChat-13B.
  prefs: []
  type: TYPE_NORMAL
- en: Key Information Loss in QGC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As described in Section [2.2](#S2.SS2 "2.2 Key Information Loss in Compression
    ‣ 2 Preliminary Study ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs"), previous methods dramatically lose key information
    as the compression ratio increases. For comparison, we experiment with QGC using
    the same setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to LongLLMLingua in Figure [4(a)](#S4.F4.sf1 "In Figure 4 ‣ Ablation
    Study ‣ 4 Experiments ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs"), the performance of QGC only decreases 10%
    as the compression ratio increases from 1x to 4x, and is even comparable to that
    of LongLLMLingua containing the correct answer in the compressed result. As seen
    in Figure [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Ablation Study ‣ 4 Experiments ‣ Retaining
    Key Information under High Compression Ratios: Query-Guided Compressor for LLMs"),
    we observe that the performance of QGC slightly degrades with more documents,
    which is only a 12% decrease with 4 documents (27% for AutoCompressor). These
    results demonstrate that QGC can effectively retain key information even in much
    longer context and higher compression ratio scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Demonstration Compression for In-Context Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Methods | SST-2 | GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| Acc | CR | Acc | CR |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt | 92.4 | 1.0x | 14.48 | 1.0x |'
  prefs: []
  type: TYPE_TB
- en: '| LongLLMLingua | - | - | 5.91 | 3.9x |'
  prefs: []
  type: TYPE_TB
- en: '| AutoCompressor | 94.2 | 15.0x | 6.68 | 13.6x |'
  prefs: []
  type: TYPE_TB
- en: '| QGC | 94.8 | 23.3x | 14.18 | 13.4x |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Experimental results on SST-2 and GSM8K datasets, where the target
    LLM is LLaMA-2-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further validate the effectiveness of QGC in a broader context, we conduct
    experiments on both SST-2 and GSM8K datasets. We adopt the approach of previous
    studies Chevalier et al. ([2023](#bib.bib4)); Wei et al. ([2022](#bib.bib32))
    which utilizing demonstrations as the document, while maintaining consistency
    with their experimental setup. The results in Table [3](#S5.T3 "Table 3 ‣ Demonstration
    Compression for In-Context Learning ‣ 5 Analysis ‣ Retaining Key Information under
    High Compression Ratios: Query-Guided Compressor for LLMs") reveals notable insights.
    On the SST-2 dataset, our method surpasses autocompressor in both compression
    ratio and accuracy. Meanwhile, on the GSM8K dataset, our accuracy performance
    remains on par with the original prompt at the same compression ratio as autocompressor.
    This suggests that QGC strikes an excellent balance between model performance
    and compression ratio. These results showcases QGC’s proficiency in preserving
    information from demonstrations and fostering the in-context learning capacity
    of the target LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Detailed Throughput Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1cf33d57ae66e16c4ad4b5853bb08eae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The accuracy, compression throughput, and generation throughput of
    QGC and LongLLMLingua.'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the throughput of various methods or models, encompassing both compression
    and generation, we perform testing on a single A100-80G GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results presented in Figure [5](#S5.F5 "Figure 5 ‣ Detailed Throughput
    Evaluation ‣ 5 Analysis ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs") indicate that QGC is obviously higher than
    LongLLMLingua in both compression throughput and generation throughput. Moreover,
    by adjusting the hyper-parameter $\epsilon$ (See Equation [10](#S3.E10 "In 3.3
    Dynamically Compressing Strategy ‣ 3 Query-Guided Compression ‣ Retaining Key
    Information under High Compression Ratios: Query-Guided Compressor for LLMs"))
    to increase the compression ratio, QGC can achieve a higher compression ratio
    while minimizing the impact on LLM performance and further improving throughput.
    Furthermore, our higher compression ratios lead to shorter LLM input, which also
    significantly improves the generation throughput of the target LLM. As for LongLLMLingua,
    since it additionally introduces LLaMA-2-7B for compression, the compression throughput
    is significantly lower than ours. Besides, although LongLLMLingua can also improve
    compression ratio by adjusting hyper-parameters, its performance will significantly
    drop, while QGC still maintains excellent performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Different Rerankers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The compression ratio for each document is determined by the corresponding correlation
    with the query obtained by a reranker. Here, we analyze the impact of using different
    rerankers in this process. In addition to the three methods introduced in reranker-based
    methods, we also include BM25 and Gzip Jiang et al. ([b](#bib.bib14)) for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experimental results are shown in Figure [6](#S5.F6 "Figure 6 ‣ Impact of Different
    Rerankers ‣ 5 Analysis ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs"). It can be found that QGC performs better with
    more competitive rerankers. Besides, compared with directly using rerankers for
    compression, QGC not only achieves an average 2.65 times higher compression ratio
    but also maintains lossless or even improved performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22725b145ad8883a14f68cd817e5e9f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The performance of QGC using different rerankers. “Base” represents
    the performance of each reranker to be used for compression. The performance (Recall)
    of rerankers: Cond.PPL > BGE-Rererank > SBERT (Sentence-BERT) > Gzip > BM25.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Long Context for LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, there have been a lot of studies focusing on expanding the context
    length of LLMs Press et al. ([2021](#bib.bib26)); Peng et al. ([2023](#bib.bib25));
    Bertsch et al. ([2023](#bib.bib2)). Existing efforts primarily involve gradually
    increasing the window size during pre-training Nijkamp et al. ([2023](#bib.bib22)),
    interpolating position embeddings Chen et al. ([2023](#bib.bib3)), and modifying
    the attention mechanism Ding et al. ([2023](#bib.bib5)). Unlike these works, we
    do not directly aim to expand the context window of LLMs. Hence, the QGC that
    we proposed can complement these techniques by enabling LLMs to access a broader
    context with reduced cost and shorter latency.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-augmented LMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Combined with a standalone retriever to augment LMs are gaining popularity for
    benefiting various knowledge-intensive tasks. Previous studies have achieved remarkable
    results in improving perplexity Wang et al. ([2023a](#bib.bib30)), factual accuracy Nakano
    et al. ([2022](#bib.bib21)), downstream task performance Izacard et al. ([2022b](#bib.bib11)),
    and in-context learning Huang et al. ([2023](#bib.bib9)). Besides, many works
    focus on cooperating LLMs and retrieved documents, such as reranking retrieved
    documents [Mao et al.](#bib.bib19) and discarding irrelevant documents [Mallen
    et al.](#bib.bib18) . QGC is also a retrieval augmentation method for LLMs, which
    efficiently compresses the retrieved documents into shorter inputs while maintaining
    no significant performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Context Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the growing context length in LLMs, the demand for higher efficiency,
    lower cost, and reduced latency has attracted much attention. As a promising solution,
    compression techniques can be broadly categorized into two types: black-box compression Xu
    et al. ([2023](#bib.bib34)) and white-box compression Wang et al. ([2023b](#bib.bib31)).
    Black-box compression primarily involves token pruning based on different importance
    measures, such as self-information Li et al. ([2023](#bib.bib16)) and the LLM
    perplexity Jiang et al. ([a](#bib.bib12), [2023](#bib.bib13)). On the other hand,
    white-box compression focuses on generating summarization or compressing the context
    into soft prompt through fine-tuning or Low-Rank Adaptation (LoRA). For instance,
    Wang et al. ([2023b](#bib.bib31)) autoregressively generates filtered content
    and fine-tunes target LLM to use it for generation. Mu et al. ([2023](#bib.bib20))
    trains LLMs to compress instructions into concise key-value attention prefixes.
    Chevalier et al. ([2023](#bib.bib4)) recursively compresses lengthy text into
    summary vectors, while Ge et al. ([2023](#bib.bib7)) generates memory slots to
    represent the original context. Compared with the above-mentioned compression
    studies, QGC’s design fully takes into account the query, which leads to the enhanced
    retention of key information, higher compression ratios, higher throughput, and
    improved overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we have presented a query-guided compressor QGC for LLMs to
    solve the loss of key information under high compression ratios. It consists of
    four essential components: query-guided context encoder, query-guided pooling
    layer, query-document reviewing layer, and semantic alignment layer. In addition,
    we also propose a dynamically compressing strategy during inference. Extensive
    experiments on multi-document QA tasks demonstrate that QGC outperforms previous
    state-of-the-art compression methods in both accuracy and compression ratios.
    Analyses reveal that this is primarily due to our retention of key information
    throughout the compression process.'
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we aim to validate our approach on more advanced LLMs, while
    also expanding its application to additional tasks like document summarization.
    Besides, we will try to further improve our approach by combining previous studies Zhang
    et al. ([a](#bib.bib37)); Hu et al. ([2022](#bib.bib8)); Zhang et al. ([2022](#bib.bib36),
    [b](#bib.bib38)).
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: QGC is a white-box compressor that necessitates access to the internal parameters
    of LLMs, which restricts its applicability. Furthermore, we have solely validated
    the effectiveness of QGC on QA and ICL task, and its performance on other tasks
    that differ significantly from QA task, such as summarization, remains to be verified.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The project was supported by National Key R&D Program of China (No. 2022ZD0160501),
    National Natural Science Foundation of China (No. 62276219), and the Public Technology
    Service Platform Project of Xiamen (No. 3502Z20231043). We also thank the reviewers
    for their insightful comments.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context
    understanding. *arXiv preprint arXiv:2308.14508*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. 2023. Unlimiformer: Long-range transformers with unlimited length input.
    *arXiv preprint arXiv:2305.01625*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. Extending context window of large language models via positional interpolation.
    *arXiv preprint arXiv:2306.15595*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. 2023. Adapting language models to compress contexts. *arXiv preprint
    arXiv:2305.14788*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers
    to 1,000,000,000 tokens. *arXiv preprint arXiv:2307.02486*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
    In-context autoencoder for context compression in a large language model. *arXiv
    preprint arXiv:2307.06945*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. [LoRA: Low-rank adaptation
    of large language models](https://openreview.net/forum?id=nZeVKeeFYf9). In *International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, and Bryan Catanzaro. 2023. [Raven: In-context learning with retrieval augmented
    encoder-decoder language models](http://arxiv.org/abs/2308.07922).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard et al. (2022a) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. [Unsupervised
    dense information retrieval with contrastive learning](http://arxiv.org/abs/2112.09118).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Izacard et al. (2022b) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
    Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian
    Riedel, and Edouard Grave. 2022b. [Atlas: Few-shot learning with retrieval augmented
    language models](http://arxiv.org/abs/2208.03299).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and
    Lili Qiu. a. LLMLingua: Compressing prompts for accelerated inference of large
    language models. In *EMNLP 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew
    Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing
    llms in long context scenarios via prompt compression. *arXiv preprint arXiv:2310.06839*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang,
    Yiqin Dai, and Jimmy Lin. b. “low-resource” text classification: A parameter-free
    classification method with compressors. In *Findings of ACL 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *NeurIPS 2020*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023.
    Compressing context to enhance inference efficiency of large language models.
    In *EMNLP 2023*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(18) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi,
    and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness
    of parametric and non-parametric memories. In *ACL 2023*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (19) Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei
    Han, and Weizhu Chen. Reader-guided passage reranking for open-domain question
    answering. In *Findings of ACL 2021*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to
    compress prompts with gist tokens. *arXiv preprint arXiv:2304.08467*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. [Webgpt: Browser-assisted
    question-answering with human feedback](http://arxiv.org/abs/2112.09332).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et al. 2023.
    Xgen-7b technical report. *arXiv preprint arXiv:2309.03450*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI OpenAI. 2023. Gpt-4 technical report.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *NeurIPS 2020*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reimers and Gurevych (2020) Nils Reimers and Iryna Gurevych. 2020. Making monolingual
    sentence embeddings multilingual using knowledge distillation. In *EMNLP 2020*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu,
    Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar,
    and Bryan Catanzaro. 2023a. [Shall we pretrain autoregressive language models
    with retrieval? a comprehensive study](http://arxiv.org/abs/2304.06762).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez,
    and Graham Neubig. 2023b. Learning to filter context for retrieval-augmented generation.
    *arXiv preprint arXiv:2311.08377*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *NeurIPS 2022*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
    2023. [C-pack: Packaged resources to advance general chinese embedding](http://arxiv.org/abs/2309.07597).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving
    retrieval-augmented lms with compression and selective augmentation. *arXiv preprint
    arXiv:2310.04408*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(35) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
    Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable
    multi-hop question answering. In *EMNLP 2018*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Biao Zhang, Deyi Xiong, Yubin Ge, Junfeng Yao, Hao Yue,
    and Jinsong Su. 2022. Aan+: Generalized average attention network for accelerating
    neural transformer. *Journal of Artificial Intelligence Research*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (a) Biao Zhang, Deyi Xiong, Jinsong Su, Qian Lin, and Huiji Zhang.
    a. Simplifying neural machine translation with addition-subtraction twin-gated
    recurrent networks. In *EMNLP 2018*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (b) Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong
    Sun, and Jie Zhou. b. MoEfication: Transformer feed-forward layers are mixtures
    of experts. In *Findings of ACL 2022*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Instructions Used in QGC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following are the instructions we used after referring to the existing studies Liu
    et al. ([2023](#bib.bib17)) and testing.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NaturalQuestions: Write a high-quality answer for the given question using
    only the provided search results(some of which might be irrelevant).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TriviaQA & HotpotQA: Using only the provided search results (some of which
    might be irrelevant), answer the following question with one or few words.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
