- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15842](https://ar5iv.labs.arxiv.org/html/2312.15842)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ehsan Latif AI4STEM Education Center & Department of Mathematics, Science, and
    Social Studies Education, University of GeorgiaAthensGAUSA ,  Luyang Fang AI4STEM
    Education Center & Department of Statistics, University of GeorgiaAthensGAUSA
    ,  Ping Ma Department of Statistics, University of GeorgiaAthensGAUSA  and  Xiaoming
    Zhai AI4STEM Education Center & Department of Mathematics, Science, and Social
    Studies Education, University of GeorgiaAthensGAUSA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This study proposes a method for knowledge distillation (KD) of fine-tuned Large
    Language Models (LLMs) into smaller, more efficient, and accurate neural networks.
    We specifically target the challenge of deploying these models on resource-constrained
    devices. Our methodology involves training the smaller student model (Neural Network)
    using the prediction probabilities (as soft labels) of the LLM, which serves as
    a teacher model. This is achieved through a specialized loss function tailored
    to learn from the LLM’s output probabilities, ensuring that the student model
    closely mimics the teacher’s performance. To validate the performance of the KD
    approach, we utilized a large dataset, 7T, containing 6,684 student-written responses
    to science questions and three mathematical reasoning datasets with student-written
    responses graded by human experts. We compared accuracy with state-of-the-art
    (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models.
    Results have shown that the KD approach has 1% and 4% higher scoring accuracy
    than ANN and TinyBERT and comparable accuracy to the teacher model. Furthermore,
    the student model size is 0.02M, 10,000 times smaller in parameters and x10 faster
    in inferencing than the teacher model and TinyBERT, respectively. The significance
    of this research lies in its potential to make advanced AI technologies accessible
    in typical educational settings, particularly for automatic scoring.
  prefs: []
  type: TYPE_NORMAL
- en: large language model (LLM), BERT, knowledge distillation, automatic scoring,
    education technology
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial Intelligence (AI) in education has evolved from a theoretical concept
    to a practical tool, significantly impacting classroom assessment practices and
    adaptive learning systems (González-Calatayud et al., [2021](#bib.bib9); Holmes
    and Tuomi, [2022](#bib.bib13); Latif and Zhai, [2023a](#bib.bib16)). AI for personalized
    learning and assessment provides opportunities for more tailored and effective
    educational experiences (Zhai et al., [2020](#bib.bib39)). Integrating Large Language
    Models (LLMs) from domains on AI like BERT (Devlin et al., [2018](#bib.bib6))
    into various domains, such as education, has been a significant milestone in advancing
    education technologies (Liu et al., [2023a](#bib.bib26); Zhai and Nehm, [2023](#bib.bib37);
    Zhai, [2022](#bib.bib33)). In education, LLMs have shown promise in enhancing
    learning experiences, providing personalized learning content and support, and
    facilitating automatic scoring systems (Selwyn, [2019](#bib.bib29); Zhai et al.,
    [2021a](#bib.bib35); Latif et al., [2023b](#bib.bib18)). Despite their potential,
    the deployment of these models in educational settings is constrained by their
    considerable size (714MB for 178 million parameters and 495MB for 124 million
    parameters) and computational requirements (16 Tensor Processing Units), presenting
    a challenge for widespread adoption in resource-constrained educational environments
    such as mobile/tablet and schools provided laptops with no GPU or TPUs and limited
    memory (Hinton et al., [2015](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: To bridge this gap, our study explores the feasibility of distilling the knowledge
    of LLMs into smaller neural networks, referred to as student models, with few
    parameters and a small number of hidden layers. By training a smaller student
    model using soft labels provided by a fine-tuned LLM (i.e., teacher model), we
    aim to achieve a similar scoring performance as LLMs with reduced model size.
    This approach leverages recent advancements in knowledge distillation (KD) (Hinton
    et al., [2015](#bib.bib12)) and AI, demonstrating the potential of smaller models
    to achieve comparable accuracy to LLMs (Zhang et al., [2022](#bib.bib40); Li and
    Li, [2021](#bib.bib23); Xu and Yang, [2017](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: In this context, we have fine-tuned BERT on a large dataset of student-written
    science assessment responses (similar to Liu et al. ([2023a](#bib.bib26))). The
    fine-tuned model then serves as a teacher model, guiding the training of a compact
    student model. Our innovative loss function is designed to align the student model’s
    predictions with the teacher model’s, achieving similar accuracy with a smaller
    model size and faster inference time. This technique is particularly applicable
    for automatic scoring in education, where timely and accurate feedback is essential
    (Zhai et al., [2022](#bib.bib36), [2021b](#bib.bib38)).
  prefs: []
  type: TYPE_NORMAL
- en: The significance of this research lies in its potential to make advanced AI
    technologies accessible in typical educational settings. The study addresses the
    technical challenges of deploying AI models in resource-constrained environments
    and highlights the potential of AI in transforming educational assessment practices.
    By enabling the deployment of efficient automatic scoring systems on less powerful
    hardware available in school settings, we contribute to the democratization of
    AI in education.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use of LLMs in education, specifically for automatic scoring, has gained
    significant attention in recent years. Studies have focused on fine-tuning LLMs
    like ChatGPT for automatic scoring applications, demonstrating their potential
    in evaluating student responses with high accuracy (Latif and Zhai, [2023b](#bib.bib17);
    Zhai, [2023](#bib.bib34)). For instance, (Latif and Zhai, [2023b](#bib.bib17))
    fine-tuned GPT-3.5 Turbo to score students’ written explanations automatically,
    and Fang et al. (Fang et al., [2023](#bib.bib7)) used GPT-4 to augment unbalanced
    data for more effective automatic scoring and Lee et al. (Lee et al., [2023b](#bib.bib20),
    [a](#bib.bib19)) applied chain-of-thought to achieve high accuracy for automatic
    scoring using ChatGPT. Further, (Schneider et al., [2023](#bib.bib28); Bertolini
    et al., [2023](#bib.bib2)) explore LLM-based auto-grading for short textual answers
    and the automatic scoring of emotional content in texts, respectively. These studies
    highlight the versatility of LLMs in handling diverse types of educational assessments.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, recent advancements in AI for education, such as the development
    of Artificial General Intelligence (AGI) for educational purposes and chain-of-thought
    prompting techniques, provide additional insights into the potential of LLMs in
    educational settings (Lee et al., [2023c](#bib.bib21); Latif et al., [2023a](#bib.bib15);
    Wang et al., [2023](#bib.bib30)). These advancements underscore the transformative
    potential of AI in reshaping educational assessment practices, aligning with our
    approach to making LLMs more accessible and practical for educational use.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced LLMs have achieved remarkable performance across various fields. However,
    deploying those models presents challenges due to high computational demands and
    data privacy issues. For example, simply loading the GPT-3 model, which contains
    175 billion parameters, requires 350 GB of memory space when using float 16 as
    the data type, and that is before processing any data. Such heavy storage makes
    deployment challenging on resource-constrained devices (Zhou et al., [2023](#bib.bib41)).
    Recent research shows that deep neural networks can unintentionally memorize data
    from training sets and can be extracted by the users (Carlini et al., [2019](#bib.bib4)).
    This indicates that the data leakage problem may occur if we apply the online
    deployment of LLMs on confidential information, such as most educational data.
    Given these concerns, an effective solution will be distilling LLMs into a more
    compact model, which can then be stored on private, resource-limited devices for
    handling confidential data.
  prefs: []
  type: TYPE_NORMAL
- en: KD (Hinton et al., [2015](#bib.bib12)) has emerged as a pivotal technique in
    harnessing the power of LLMs for practical applications, particularly in fields
    with limited computational resources. The core idea is to transfer the knowledge
    from a large, complex model (teacher) to a smaller, more efficient model (student),
    thereby enabling the deployment of sophisticated AI capabilities in constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Several approaches have been proposed to address these challenges. Calibrating
    LLM-based evaluators have been a focus, aiming to optimize their performance in
    educational settings (Liu et al., [2023b](#bib.bib25)). For instance, (Li et al.,
    [2023](#bib.bib22)) identified appropriate instructions by prompting ChatGPT with
    different templates to collect and refine the rationales to fine-tune the smaller
    language model for the simultaneous generation of automatic scores and rationales
    and achieve 11% higher Quadratic Weighted Kappa than ChatGPT. Furthermore, (Liang
    et al., [2023](#bib.bib24)) proposed the LLM distillation technique to solve math
    word problems, which fosters a tailored learning experience by generating targeted
    exercises aligned with educational science principles and able to achieve comparable
    accuracy as GPT-3.5 and PaLLM. Lastly, (Sahu et al., [2023](#bib.bib27)) proposed
    PromptMix, which generates augmented data for near class boundaries using LLM
    and performs text classifications using a smaller Neural Network model and found
    that relabeling borderline examples facilitates the transfer of knowledge of a
    massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like Distil_BERT.
  prefs: []
  type: TYPE_NORMAL
- en: All these approaches aim to reduce the model size by maintaining high prediction
    accuracy to facilitate the language model deployment to smaller devices with limited
    resources. However, maintaining the appropriate balance between size reduction
    and accuracy still struggles. Studies (Li and Li, [2021](#bib.bib23); Xu and Yang,
    [2017](#bib.bib31)) also have addressed this limitation by refining data distillation
    techniques for text classification. Yet, the complexity and variability of language
    often demand more sophisticated distillation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the approaches applied KD to whitebox classification models to imitate
    black-box models like ChatGPT. LLMs like GPT-3 ( or other GPT models) have achieved
    impressive performance on tasks such as text generation and question answering,
    but they are not suitable for direct use for classification problems. On the contrary,
    models like BERT (Devlin et al., [2018](#bib.bib6)) are specifically trained for
    classification tasks, making them more suitable for our tasks in classification.
    Studies have shown that the BERT model achieves state-of-the-art performance on
    a wide range of natural language processing (NLP) tasks, including text classification
    (González-Carvajal and Garrido-Merchán, [2020](#bib.bib10); Yu et al., [2019](#bib.bib32)).
    Therefore, we choose BERT as the teacher model in this study.
  prefs: []
  type: TYPE_NORMAL
- en: Recent research by Microsoft (Gu et al., [2023](#bib.bib11)) proposed MINILLM
    that distills smaller language models from generative LLMs. They have replaced
    the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches
    with reverse KLD, which is more suitable for KD on generative language models,
    to prevent the student model from overestimating the low-probability regions of
    the teacher distribution. Furthermore, (Jiao et al., [2019](#bib.bib14)) proposed
    a two-stage learning framework for TinyBERT, which performs Transformer distillation
    at both the pretraining and task-specific learning stages. We took inspiration
    from MINILLM and TinyBERT and modified the standard KD by replacing a best-fitting
    regression within the loss function to maximize the prediction accuracy of the
    student model.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach addresses the limitations of existing approaches by proposing a
    novel knowledge distillation strategy, ensuring that the distilled models retain
    the sophisticated capabilities of their larger counterparts while being deployable
    in resource-constrained settings. This method is particularly relevant for educational
    applications, where the need for efficient and effective AI models is paramount.
    Our research is motivated by the work of Hinton et al. (Hinton et al., [2015](#bib.bib12)),
    who laid the foundational work in this field, illustrating the basic principles
    of KD in neural networks. Building on this, we incorporate methods (Chuang et al.,
    [2020](#bib.bib5); Bhunia et al., [2021](#bib.bib3)) that demonstrated the efficacy
    of lifelong language KD and unifying text recognition using KD, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we formulate the problem and develop the knowledge distillation
    method for automatically scoring student-written responses.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the convenience of presentation, we introduce our method in the context
    of a classification problem for automatic scoring of student written responses.
    With the training sample $\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}$, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $f(\mathbf{x},\bm{\theta})=(f_{1}(\mathbf{x},\bm{\theta}),\ldots,f_{K}(\mathbf{x},\bm{\theta}))^{T},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{\theta}\in\mathbb{R}^{d}$, the class with the highest probability
    is predicted.
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed neural network structure, i.e., a known function $f(\cdot,\cdot)$
    during the training of ANN. In general, we achieve this by minimizing the empirical
    risk
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\mathcal{L}(f(\cdot,\bm{\theta});\mathcal{D})=\frac{1}{N}\sum_{i=1}^{N}\text{CE}(\mathbf{y}_{i},f(\mathbf{x}_{i},\bm{\theta})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{y}_{i}=(y_{i1},\ldots,y_{iK})^{T}$. Typically, the minimization
    is achieved using the stochastic gradient descent. Built upon the neural network,
    we propose a knowledge distillation approach to achieve higher prediction accuracy
    for automatic scoring.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Proposed Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: KD is a technique to transfer knowledge from a trained large model (teacher)
    to a more compact and deployable model (student). We take inspiration from the
    prominent KD approach, introduced by Hinton et al. ([2015](#bib.bib12)), which
    involves using the class probabilities generated by the pre-trained large model
    as soft targets for training the smaller model, effectively transferring its predictive
    and generalization capabilities. Building on this concept, we develop a method
    for applying knowledge distillation in the context of automated scoring systems,
    aiming to improve the process of evaluating educational content using AI.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Knowledge Distillation for Education Technology
  prefs: []
  type: TYPE_NORMAL
- en: 1:Training dataset $\mathcal{D}$
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, for each data point $\mathbf{x}_{i}$. The discrepancy between
    the student and teacher models is measured as
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\displaystyle\tilde{\mathcal{L}}(f(\cdot,\bm{\theta});\mathcal{D},\bm{p})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: which is the sample mean of the cross-entropy $\text{CE}(\bm{p}_{i},f(\mathbf{x}_{i},\bm{\theta}))$.
    To leverage the information from both the training data and the teacher model’s
    predictions, KD aims to solve
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle\bm{\theta}_{\mathrm{KD}}^{*}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the minimized KD loss $\mathcal{L}^{\mathrm{KD}}(f(\cdot,\bm{\theta});\mathcal{D},\bm{p},\lambda)$
    reduces the KD loss in Eq. ([4](#S3.E4 "In 3.2\. Proposed Knowledge Distillation
    ‣ 3\. Methodology ‣ Knowledge Distillation of LLM for Automatic Scoring of Science
    Education Assessments")) to the conventional empirical risk loss. The pseudo code
    for the proposed KD approach can be seen in Alg. [1](#alg1 "Algorithm 1 ‣ 3.2\.
    Proposed Knowledge Distillation ‣ 3\. Methodology ‣ Knowledge Distillation of
    LLM for Automatic Scoring of Science Education Assessments")
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92fded8386810f08c7a2b5841a235492.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Architecture of proposed KD approach using prediction probabilities
    as soft-labels from teacher model and forcing the student to achieve prediction
    probability by the fitting loss function. The procedure begins with a pre-trained
    teacher model from which we derive knowledge from class probabilities. This knowledge,
    along with the data, is then utilized to inform the training of a student model.
  prefs: []
  type: TYPE_NORMAL
- en: KD enables the student model to attain performance comparable to the teacher
    model while demanding considerably fewer computational resources for training.
    The teacher model’s predicted probability outputs $\bm{p}$ provide valuable insights
    into its data interpretation. By minimizing the discrepancy between the probability
    outputs of the student and teacher models, the student model can effectively adopt
    the knowledge and insights of the teacher model. Consequently, despite having
    a simpler architecture and fewer computational resources, the student model can
    achieve performance comparable to that of the more complex teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [1](#S3.F1 "Figure 1 ‣ 3.2\. Proposed Knowledge Distillation ‣ 3\. Methodology
    ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments"),
    we provide the architecture of the proposed KD method. With a well-performing,
    fine-tuned, large teacher model, given a new dataset, we run the teacher model
    on the dataset and extract the knowledge from the teacher model to guide the training
    of a more compact student model. In this study, we extract the class probabilities
    predicted by the teacher model as the knowledge to be transferred to the student
    model. Using both the knowledge from the teacher model and the data from the dataset,
    we train the student model based on optimization as in Equation ([4](#S3.E4 "In
    3.2\. Proposed Knowledge Distillation ‣ 3\. Methodology ‣ Knowledge Distillation
    of LLM for Automatic Scoring of Science Education Assessments")).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our study investigates whether a significantly smaller neural network can effectively
    mimic the capabilities of a fine-tuned LLM through the proposed KD strategy. Additionally,
    the study explores how this approach can enhance model performance. We apply our
    proposed methodology across diverse datasets to train a compact model to achieve
    this goal. This model is then compared with the SOTA TinyBERT (Jiao et al., [2019](#bib.bib14))
    and a trained smaller ANN (Ghiassi et al., [2012](#bib.bib8)) to evaluate the
    performance in terms of accuracy and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Data Collection and Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The study utilized a meticulously categorized dataset of student-written responses
    to a science question and three mathematical assessment items, each falling under
    the multi-class category for automatic scoring. Each student response in datasets
    is graded by a human expert for automatic scoring, and human scores are used for
    validation. On average, each student’s written textual response contains 15 words.
    The detailed composition of each assessment item’s dataset is presented in Table
    [1](#S4.T1 "Table 1 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments").
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Sample size | Teacher | Student |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bathtub | 1,145 | BERT_base (110M) | E-LSTM (0.03M) |'
  prefs: []
  type: TYPE_TB
- en: '| 7T | 6,684 | SciEdBERT (114M) | E-LSTM (0.03M) |'
  prefs: []
  type: TYPE_TB
- en: '| Falling Weights | 1,148 | BERT_base (110M) | E-LSTM (0.03M) |'
  prefs: []
  type: TYPE_TB
- en: '| Gelatin | 1,142 | BERT_base (110M) | E-LSTM (0.03M) |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Sample size and the teacher and student model used for each dataset.
    The number of parameters for each model is shown in parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Overview:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bathtub: We utilized the dataset for the mathematically complex dataset for
    the bathtub assessment item, which consists of 1,145 student-written responses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '7T: A large dataset consisting of seven tasks from the SR1 dataset, including
    short constructed student responses and human-expert graded scores. Overall, the
    7T dataset consists of 6,684 labeled student responses from (Zhai et al., [2022](#bib.bib36)),
    similar to the dataset used for SciEdBERT by Liu et al. (Liu et al., [2023a](#bib.bib26)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Falling Weights: We also have taken the student response dataset for challenging
    mathematical thinking assessment item for falling weight similar to the data used
    by Latif & Zhai (Latif and Zhai, [2023b](#bib.bib17)), which consists of 1,148
    student written and human-expert graded scores.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gelatin: Another dataset for gelatin assessment items contains 1,142 student
    responses, considered samples to train teacher models and distill knowledge to
    student models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Teacher | TinyBERT | ANN | KD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bathtub | 0.938 | 0.839$\pm$0.019* |'
  prefs: []
  type: TYPE_TB
- en: '| 7T | 0.891 | 0.761$\pm$0.011* |'
  prefs: []
  type: TYPE_TB
- en: '| Falling Weights | 0.904 | 0.829$\pm$0.014* |'
  prefs: []
  type: TYPE_TB
- en: '| Gelatin | 0.871 | 0.784$\pm$0.022* |'
  prefs: []
  type: TYPE_TB
- en: '| * KD has shown higher accuracy than TinyBERT and Original NN and is comparable
    to the Teacher model for each dataset. |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Accuracy performance comparison of teacher, TinyBERT (Jiao et al.,
    [2019](#bib.bib14)), ANN (Ghiassi et al., [2012](#bib.bib8)), and KD model for
    benchmark datasets. The mean accuracies are displayed, and the standard deviations
    are shown in parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: This comprehensive dataset facilitated a nuanced analysis of the capacity of
    the compact scoring models for student-written responses, ensuring robust and
    broadly applicable study findings. We processed each dataset by excluding empty
    responses and ensuring text-format student responses and ranged labels.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Training Scheme
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1\. Model Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This study uses SciEdBERT (Liu et al., [2023a](#bib.bib26)) with 114M parameters
    as a specialized Science Education BERT model, and the standard BERT base model
    (Devlin et al., [2018](#bib.bib6)) contains 110M parameters as the teacher model.
    These models have been shown to perform brilliantly in processing textual data.
    For performance comparison evaluation, we used TinyBERT (Jiao et al., [2019](#bib.bib14))
    with 67M parameters. For the KD method, we construct a compact neural network
    with an embedding layer with an output dimension of 32 and a bidirectional LSTM
    layer with 16 units, followed by a GlobalMaxPooling1D layer. Further, it includes
    two dense layers, with the first having 16 neurons and ’selu’ activation, and
    the final layer is equipped with a softmax activation for multi-class classification.
    Additionally, dropout layers are integrated for regularization, and the model
    is optimized with Adam.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Evaluation and Validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We partition each dataset into training, validation, and testing sets in a $7:1:2$
    ratio. The model optimization employs cross-entropy loss, and to prevent overfitting,
    an early stopping callback, which monitors the validation loss, is utilized. We
    present the prediction accuracy on the test set to assess the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: The summary of the dataset and the teacher and student (KD) models used for
    each dataset is detailed in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Data Collection
    and Preprocessing ‣ 4\. Experimental Setup ‣ Knowledge Distillation of LLM for
    Automatic Scoring of Science Education Assessments"). We provide the number of
    parameters for each model in parentheses. The student model is much smaller than
    the teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The comparative analysis of model accuracy across four datasets is presented
    in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments"). Results reveal the efficacy of KD in enhancing the performance
    of a student model as compared to the SOTA TinyBERT (Jiao et al., [2019](#bib.bib14))
    and ANN (Ghiassi et al., [2012](#bib.bib8)) for text classification. Further,
    it also provides close accuracy as a complex teacher model. The 7T dataset serves
    as a typical example, with KD providing performance comparable to the teacher
    model, suggesting that even models with much smaller sizes can achieve similar
    performance to the large teacher model. We observed that KD has achieved 1% and
    4% higher accuracy than TinyBERT and ANN, respectively, which highlights the outperformance
    of KD to SOTA model distillation approaches. Considering both accuracy (shown
    in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments")) and model size (shown in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Data
    Collection and Preprocessing ‣ 4\. Experimental Setup ‣ Knowledge Distillation
    of LLM for Automatic Scoring of Science Education Assessments")), results highlight
    the practicality and applicability of the KD approach for automatic scoring on
    resource constrained-devices. This improvement delineates the potential of KD
    to augment model capabilities, particularly in scenarios where the SOTA approaches
    may not fully capture the underlying patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the success of KD, it is essential to recognize that the student models,
    although improved, usually do not reach the performance benchmark set by the teacher
    models. This is notably apparent in the Gelatin and Falling Weights datasets;
    the integration of KD leads to better performance compared to the ANN and TinyBERT
    but still does not match the teacher models’ accuracy. Such a discrepancy can
    be attributed to the inherent limitations of the student models, which possess
    simpler architectures and are trained on smaller datasets with minimal training
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The results show that the KD strategy is a powerful tool in model training,
    beneficial for applications like automatic scoring. By effectively condensing
    the knowledge of a large, pre-trained model into a more compact one, KD improves
    performance and facilitates the deployment of such a model in resource-constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Sensitivity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We investigated the impact of the hyperparameter $\lambda$, demonstrating consistent
    performance throughout a range of hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The results of this study highlight the revolutionary possibilities of KD in
    educational technology, especially in light of the limitations of standard school
    computing resources. The use of KD in education represents a substantial breakthrough,
    particularly in automated grading systems. But like any emerging technology, it’s
    important to recognize its limitations as well as its potential for development
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Application of KD in Education
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most noteworthy use of KD in education is in creating accurate and productive
    automatic scoring systems. A major challenge in many educational contexts is that
    traditional scoring systems can demand extensive processing resources to function
    successfully on school-setting devices such as entry-level laptops and tablets.
    This problem is addressed by KD, which makes it possible to create ”student models”
    with significantly lower processing requirements while maintaining a great deal
    of the accuracy and efficiency of bigger ”teacher models.”
  prefs: []
  type: TYPE_NORMAL
- en: When combined with automatic scoring, these distilled models can give students
    offline fast, reliable, and objective feedback—an essential feature of offline
    adaptive learning environments without needing high processing and an internet
    connection. Accurate and timely feedback is crucial for creating a personalized
    and interesting learning environment, which contemporary educational environments
    are calling for more and more.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, KD models are perfect for integrating tablet- and smartphone-based
    learning apps due to their smaller size and lower processing requirements. The
    capacity to run complex AI models on these devices, which are increasingly prevalent
    in educational contexts, creates new opportunities for interactive and adaptable
    learning experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Limitations of KD in Education
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While KD has many advantages, it’s important to recognize its limitations. First
    off, even if KD helps the student models perform better, they frequently fall
    short of the more sophisticated teacher models. This disparity could result in
    a little decline in the accuracy of the assessment or the quality of the feedback,
    which could be crucial in some educational settings.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the caliber and applicability of the data utilized to train the
    teacher model determine how effective KD is. It is essential to ensure the teacher
    model is trained on representative and extensive data sets in educational contexts,
    where data might be complex and varied. This need can be difficult, especially
    when it comes to themes that demand a profound comprehension of context or are
    subjective.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Looking ahead, there are several avenues for further research and development
    to overcome the aforementioned limitations in the application of KD in education:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soft label processing: In the proposed approach, we directly applied soft labels
    from the teacher model and fit the prediction the accuracy using the loss function;
    however, if the soft labels are from false positives, that can eventually affect
    the performance of the student model. Hence, processing soft labels through sophisticated
    validation techniques can overcome this limitation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expanding Application Areas: Beyond automatic scoring, KD can be applied to
    other areas of education technology, such as personalized content recommendation,
    language translation for multilingual education, and interactive simulations for
    complex subjects.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Customizable and Adaptive Models: Future research endeavors may concentrate
    on constructing small KD models adaptable to particular learning environments.
    These models might modify their actions in response to the student’s development,
    learning preferences, and academic requirements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Even though knowledge distillation offers a promising way to improve educational
    technologies, particularly in settings with limited resources, further research,
    and development are needed to realize its full potential. Harnessing KD’s full
    potential in education will require striking a balance between performance and
    practicality, resolving ethical issues, and continuously adapting to the changing
    educational scene.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study effectively illustrates how Knowledge Distillation (KD) can be used
    to optimize Large Language Models (LLMs) for usage in educational technology,
    especially on low-processor devices. We maintain great accuracy 85% with a much
    smaller model size (0.02M parameters) and processing requirements by condensing
    the knowledge of LLMs into smaller neural networks. The distilled models perform
    better than SOTA TinyBERT and ANN models on various datasets, demonstrating the
    efficacy of this approach even though their parameter sizes are up to 100 times
    less than teacher models. This work has important applications since it provides
    a method to incorporate cutting-edge AI tools into conventional school environments,
    which frequently have hardware constraints. The learning process and accessibility
    of personalized education technology can be significantly improved by the capacity
    to implement effective and precise AI models for uses such as autonomous scoring.
    Essentially, this work establishes the foundation for future developments in the
    field and validates the viability of KD in educational contexts, underscoring
    the significance of ongoing research and innovation in AI for education. In the
    future, we will work on processing soft-labels and prompt processing to avoid
    amplification of faults of teacher models by employing more sophisticated techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study secondary analyzed data from projects supported by the National Science
    Foundation (grant numbers 2101104, PI Zhai) and the Institute of Education Sciences
    (grant number R305A160219, PI Liu). The authors acknowledge the funding agencies
    and the project teams for making the data available for analysis. The findings,
    conclusions, or opinions herein represent the views of the authors and do not
    necessarily represent the views of personnel affiliated with the funding agencies.
    LF and PM were partially supported by National Science Foundation under grants
    DMS-1903226, DMS-1925066, DMS-2124493, DMS-2311297, DMS-2319279.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertolini et al. (2023) Lorenzo Bertolini, Valentina Elce, Adriana Michalak,
    Giulio Bernardi, and Julie Weeds. 2023. Automatic Scoring of Dream Reports’ Emotional
    Content with Large Language Models. *arXiv preprint arXiv:2302.14828* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhunia et al. (2021) Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury,
    and Yi-Zhe Song. 2021. Text is text, no matter what: Unifying text recognition
    using knowledge distillation. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 983–992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini et al. (2019) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, and Dawn Song. 2019. The secret sharer: Evaluating and testing unintended
    memorization in neural networks. In *28th USENIX Security Symposium (USENIX Security
    19)*. 267–284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chuang et al. (2020) Yung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen. 2020.
    Lifelong language knowledge distillation. *arXiv preprint arXiv:2010.02123* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2023) Luyang Fang, Gyeong-Geon Lee, and Xiaoming Zhai. 2023. Using
    gpt-4 to augment unbalanced data for automatic scoring. *arXiv preprint arXiv:2310.18365*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghiassi et al. (2012) Manoochehr Ghiassi, Michael Olschimke, Brian Moon, and
    Paul Arnaudo. 2012. Automated text classification using a dynamic artificial neural
    network model. *Expert Systems with Applications* 39, 12 (2012), 10967–10976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'González-Calatayud et al. (2021) Víctor González-Calatayud, Paz Prendes-Espinosa,
    and Rosabel Roig-Vila. 2021. Artificial intelligence for student assessment: A
    systematic review. *Applied Sciences* 11, 12 (2021), 5467.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: González-Carvajal and Garrido-Merchán (2020) Santiago González-Carvajal and
    Eduardo C Garrido-Merchán. 2020. Comparing BERT against traditional machine learning
    text classification. *arXiv preprint arXiv:2005.13012* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge
    Distillation of Large Language Models. *arXiv preprint arXiv:2306.08543* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holmes and Tuomi (2022) Wayne Holmes and Ilkka Tuomi. 2022. State of the art
    and practice in AI in education. *European Journal of Education* 57, 4 (2022),
    542–570.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural
    language understanding. *arXiv preprint arXiv:1909.10351* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latif et al. (2023a) Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu,
    Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, and Xiaoming Zhai. 2023a. Artificial
    general intelligence (AGI) for education. *arXiv preprint arXiv:2304.12479* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latif and Zhai (2023a) Ehsan Latif and Xiaoming Zhai. 2023a. Automatic Scoring
    of Students’ Science Writing Using Hybrid Neural Network. *arXiv preprint arXiv:2312.03752*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latif and Zhai (2023b) Ehsan Latif and Xiaoming Zhai. 2023b. Fine-tuning chatgpt
    for automatic scoring. *arXiv preprint arXiv:2310.10072* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Latif et al. (2023b) Ehsan Latif, Xiaoming Zhai, and Lei Liu. 2023b. AI Gender
    Bias, Disparities, and Fairness: Does Training Data Matter? *arXiv preprint arXiv:2312.10833*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023a) Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, and Xiaoming Zhai.
    2023a. Gemini Pro Defeated by GPT-4V: Evidence from Education. *arXiv preprint
    arXiv:2401.08660* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2023b) Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu,
    and Xiaoming Zhai. 2023b. Applying Large Language Models and Chain-of-Thought
    for Automatic Scoring. *arXiv preprint arXiv:2312.03748* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023c) Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne
    Bewersdorf, Matthew Nyaaba, Shuchen Guo, Zihao Wu, Zhengliang Liu, Hui Wang, et al.
    2023c. Multimodality of AI for Education: Towards Artificial General Intelligence.
    *arXiv preprint arXiv:2312.06037* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Jiazheng Li, Lin Gui, Yuxiang Zhou, David West, Cesare Aloisi,
    and Yulan He. 2023. Distilling ChatGPT for Explainable Automated Student Answer
    Assessment. *arXiv preprint arXiv:2305.12962* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Li (2021) Yongqi Li and Wenjie Li. 2021. Data distillation for text classification.
    *arXiv preprint arXiv:2104.08448* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2023) Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark,
    Xiangliang Zhang, and Ashwin Kaylan. 2023. Let GPT be a Math Tutor: Teaching Math
    Word Problem Solvers with Customized Exercise Generation. *arXiv preprint arXiv:2305.14386*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023b. Calibrating LLM-Based
    Evaluator. *arXiv preprint arXiv:2309.13308* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, and Xiaoming
    Zhai. 2023a. Context matters: A strategy to pre-train language model for science
    education. *arXiv preprint arXiv:2301.12031* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sahu et al. (2023) Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, and Issam H
    Laradji. 2023. Promptmix: A class boundary augmentation method for large language
    model distillation. *arXiv preprint arXiv:2310.14192* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schneider et al. (2023) Johannes Schneider, Bernd Schenk, Christina Niklaus,
    and Michaelis Vlachos. 2023. Towards LLM-based Autograding for Short Textual Answers.
    *arXiv preprint arXiv:2309.11508* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selwyn (2019) Neil Selwyn. 2019. *Should robots replace teachers?: AI and the
    future of education*. John Wiley & Sons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Ruifeng Xu,
    and Kam-Fai Wong. 2023. Chain-of-thought prompting for responding to in-depth
    dialogue questions with LLM. *arXiv preprint arXiv:2305.11792* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu and Yang (2017) Ruochen Xu and Yiming Yang. 2017. Cross-lingual distillation
    for text classification. *arXiv preprint arXiv:1705.02073* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Shanshan Yu, Jindian Su, and Da Luo. 2019. Improving bert-based
    text classification with auxiliary sentence and domain knowledge. *IEEE Access*
    7 (2019), 176600–176612.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai (2022) Xiaoming Zhai. 2022. ChatGPT user experience: Implications for
    education. *Available at SSRN 4312418* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhai (2023) Xiaoming Zhai. 2023. Using Gpt-4 to Augment Unbalanced Data for
    Automatic Scoring. *Available at SSRN* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhai et al. (2021a) Xuesong Zhai, Xiaoyan Chu, Ching Sing Chai, Morris Siu Yung
    Jong, Andreja Istenic, Michael Spector, Jia-Bao Liu, Jing Yuan, and Yan Li. 2021a.
    A Review of Artificial Intelligence (AI) in Education from 2010 to 2020. *Complexity*
    2021 (2021), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhai et al. (2022) Xiaoming Zhai, Peng He, and Joseph Krajcik. 2022. Applying
    machine learning to automatically assess scientific models. *Journal of Research
    in Science Teaching* 59, 10 (2022), 1765–1794.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai and Nehm (2023) Xiaoming Zhai and Ross H Nehm. 2023. AI and formative
    assessment: The train has left the station. *Journal of Research in Science Teaching*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. (2021b) Xiaoming Zhai, Lehong Shi, and Ross H Nehm. 2021b. A meta-analysis
    of machine learning-based science assessments: Factors impacting machine-human
    score agreements. *Journal of Science Education and Technology* 30 (2021), 361–379.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. (2020) Xiaoming Zhai, Yue Yin, James W Pellegrino, Kevin C Haudek,
    and Lehong Shi. 2020. Applying machine learning in science assessment: a systematic
    review. *Studies in Science Education* 56, 1 (2020), 111–151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Shaokang Zhang, Lei Jiang, and Jianlong Tan. 2022. Cross-domain
    knowledge distillation for text classification. *Neurocomputing* 509 (2022), 11–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023. Llm as dba.
    *arXiv preprint arXiv:2308.05481* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
