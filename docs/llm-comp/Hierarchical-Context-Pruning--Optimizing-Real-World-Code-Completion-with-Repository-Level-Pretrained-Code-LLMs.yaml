- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:35'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18294](https://ar5iv.labs.arxiv.org/html/2406.18294)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lei Zhang^(1,2)  Yunshui Li^(1,2)  Jiaming Li^(1,2)  Xiaobo Xia³  Jiaxi Yang^(1,2)
     Run Luo^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: Minzheng Wang^(2,5)  Longze Chen^(1,2)  Junhao Liu⁴  Min Yang^(1,2)²²2Min Yang
    is the corresponding author.
  prefs: []
  type: TYPE_NORMAL
- en: ¹Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ²University of Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: ³The University of Sydney  ⁴University of California, Irvine
  prefs: []
  type: TYPE_NORMAL
- en: ⁵MAIS, Institute of Automation, Chinese Academy of Sciences
  prefs: []
  type: TYPE_NORMAL
- en: '{lei.zhang2, min.yang}@siat.ac.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some recently developed code large language models (Code LLMs) have been pretrained
    on repository-level code data (Repo-Code LLMs), enabling these models to recognize
    repository structures and utilize cross-file information for code completion.
    However, in real-world development scenarios, simply concatenating the entire
    code repository often exceeds the context window limits of these Repo-Code LLMs,
    leading to significant performance degradation. In this study, we conducted extensive
    preliminary experiments and analyses on six Repo-Code LLMs. The results indicate
    that maintaining the topological dependencies of files and increasing the code
    file content in the completion prompts can improve completion accuracy; pruning
    the specific implementations of functions in all dependent files does not significantly
    reduce the accuracy of completions. Based on these findings, we proposed a strategy
    named Hierarchical Context Pruning (HCP) to construct completion prompts with
    high informational code content. The HCP models the code repository at the function
    level, maintaining the topological dependencies between code files while removing
    a large amount of irrelevant code content, significantly reduces the input length
    for repository-level code completion. We applied the HCP strategy in experiments
    with six Repo-Code LLMs, and the results demonstrate that our proposed method
    can significantly enhance completion accuracy while substantially reducing the
    length of input. Our code and data are available at [https://github.com/Hambaobao/HCP-Coder](https://github.com/Hambaobao/HCP-Coder).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Code completion tools based on code large language models (Chen et al., [2021](#bib.bib8);
    Nijkamp et al., [2023b](#bib.bib30); Li et al., [2023](#bib.bib21); Fried et al.,
    [2023](#bib.bib14); Allal et al., [2023](#bib.bib1)), such as *GitHub Copilot*¹¹1[https://github.com/features/copilot](https://github.com/features/copilot),
    have been widely adopted in daily development practices and have significantly
    enhanced the productivity of developers. As research (Bavarian et al., [2022](#bib.bib4);
    Sun et al., [2024](#bib.bib37)) on code large language models (Code LLMs) continues
    to evolve, some recently developed Code LLMs (Guo et al., [2024](#bib.bib15);
    Lozhkov et al., [2024](#bib.bib26); Team et al., [2024](#bib.bib38)) have been
    trained on repository-level code data (Repo-Code LLMs) to overcome the limitations
    of previous models trained on file-level data, which struggled to recognize repository
    structures and integrate code across multiple files for completion tasks. However,
    in real-world development scenarios, simply concatenating the entire code repository
    often exceeds the context window size of these Repo-Code LLMs, leading to significant
    performance degradation and increased inference latency. How to effectively utilize
    the capabilities of these Repo-Code LLMs to integrate cross-file information and
    construct high-quality completion prompts within the model’s context window limits
    remains an area for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this study, we initially evaluated six Repo-Code LLMs on the CrossCodeEval
    (Ding et al., [2023](#bib.bib11)) benchmark and conducted a detailed analysis
    of completion errors (Appendix [A](#A1 "Appendix A Error Description and Analysis
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs")). The errors identified were categorized into eight distinct
    classes (Section [4.2](#S4.SS2 "4.2 Completion Error Analysis ‣ 4 Preliminary
    Studies ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs")). Subsequently, considering the characteristics
    of the decoder architecture in Code LLMs, we analyzed the impact of topological
    dependencies among code files on completion accuracy (Section [4.3](#S4.SS3 "4.3
    Topological Dependency Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context
    Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs")). We found that maintaining the dependencies between code files and
    including more file information leads to higher accuracy. Additionally, we conducted
    experiments to analyze the impact of content from files at different dependency
    levels on completion accuracy (Section [4.4](#S4.SS4 "4.4 Cross-File Content Analysis
    ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs")). We discovered that
    even pruning away the specific implementations of functions in all dependent files
    does not significantly reduce the accuracy of completions. Based on the results
    of these preliminary experiments, we proposed a strategy named Hierarchical Context
    Pruning (HCP) to construct high-quality completion prompts. The HCP models the
    code repository at the function level, retaining the topological dependencies
    between files while eliminating a large amount of irrelevant code content. In
    our experiments, the HCP successfully reduced the input from over 50,000 tokens
    to approximately 8,000 tokens, and significantly enhanced the accuracy of completions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our contributions are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We conducted experiments on six Repo-Code LLMs and found that: maintaining
    the topological dependencies of files and increasing the content of code files
    in the completion prompts can enhance completion accuracy; pruning the specific
    implementations of functions in all dependent files does not significantly reduce
    the accuracy of completions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the results of preliminary experiments, we proposed a strategy named
    Hierarchical Context Pruning (HCP) for constructing high-quality completion prompts,
    which models the code repository at the function level, retaining the topological
    dependencies between files while eliminating a large amount of irrelevant code
    content.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We applied the HCP strategy in experiments with six Repo-Code LLMs, and the
    results demonstrate that our proposed method can significantly enhance completion
    accuracy while substantially reducing the length of input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Code Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1.1 Infilling Code LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Infilling scenarios constitute the majority of code completion tasks in the
    real world. Bavarian et al. ([2022](#bib.bib4)) demonstrates that pre-training
    Code LLMs with a certain proportion of fill-in-the-middle format code data can
    enable the Code LLMs to fill in middle code based on the surrounding context,
    without compromising their original left-to-right generation performance. Based
    on the findings of Bavarian et al. ([2022](#bib.bib4)), many subsequent Code LLMs
    (Fried et al., [2023](#bib.bib14); Allal et al., [2023](#bib.bib1); Nijkamp et al.,
    [2023a](#bib.bib29); Li et al., [2023](#bib.bib21); Rozière et al., [2024](#bib.bib34);
    Guo et al., [2024](#bib.bib15); Pinnaparaju et al., [2024](#bib.bib33); Lozhkov
    et al., [2024](#bib.bib26)) have emerged with the capability to perform infilling.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Instruction Code LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pretrained Code LLMs are traditionally used only for continuation tasks such
    as code completion. Inspired by works on instruction tuning large language models
    (Ouyang et al., [2022](#bib.bib31); Li et al., [2024b](#bib.bib23)), many studies
    (Wang et al., [2023a](#bib.bib42); Luo et al., [2023](#bib.bib27); Muennighoff
    et al., [2024](#bib.bib28); Xu et al., [2023](#bib.bib45); Wang et al., [2024b](#bib.bib41);
    Zheng et al., [2024](#bib.bib54)) have attempted to finetune Code LLMs using code
    instruction data. This finetuning unlocks the potential of Code LLMs, enabling
    them to perform more complex coding tasks based on user instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9031b176d3e2b81b24e9fb01c98607a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The error class distribution of the completion results of the DeepseekCoder,
    Starcoder2 and CodeGemma models on the CrossCodeEval: Python benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Code Benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.2.1 Code Completion Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HumanEval (Chen et al., [2021](#bib.bib8)) consists of 164 manually crafted
    Python code problems, with an average of 7.7 tests each test case. MBPP (Austin
    et al., [2021](#bib.bib2)) is designed for individuals with entry-level programming
    skills. It comprises 974 concise Python functions, each with an accompanying description
    in English, a specified function signature, and three manually crafted test cases
    for verification. MultiPL-E (Cassano et al., [2022](#bib.bib6)) introduces itself
    as a novel benchmarking framework designed for multilingual contexts, building
    upon HumanEval (Chen et al., [2021](#bib.bib8)) and MBPP (Austin et al., [2021](#bib.bib2)).
    APPS (Hendrycks et al., [2021](#bib.bib16)) is a benchmark including 10K less-restricted
    problems for code generation. CodeContests (Li et al., [2022](#bib.bib22)) is
    a dataset specifically for competitive programming problems.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Infilling Code Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fried et al. ([2023](#bib.bib14)) constructed single-line and multi-line infilling
    completion tasks based on HumanEval, and Bavarian et al. ([2022](#bib.bib4)) expanded
    upon it to create randomspan infilling completion tasks, ultimately resulting
    in the current HumanEval-Infilling benchmark. Allal et al. ([2023](#bib.bib1))
    created an Infilling benchmark that includes languages from Java, JavaScript,
    and Python 3, utilizing a line exactly match method for evaluation. Lai et al.
    ([2022](#bib.bib19)) presents a benchmark for evaluating the performance of Code
    LLMs in completing tasks related to Python scientific computing libraries, encompassing
    both regular completion and insertion (infilling) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Repo-level Code Completion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some benchmarks for repository-level code completion have been proposed to evaluate
    the performance of code models in real-world completion tasks, such as CrossCodeEval
    (Ding et al., [2023](#bib.bib11)), Repo-Bench (Liu et al., [2023](#bib.bib25)),
    CoderEval (Zhang et al., [2024b](#bib.bib53)), and EvoCodeBench (Li et al., [2024a](#bib.bib20)).
    A lot of studies (Shrivastava et al., [2023](#bib.bib36); Zhang et al., [2023a](#bib.bib50);
    Bi et al., [2024](#bib.bib5); Phan et al., [2024](#bib.bib32); Liang et al., [2024](#bib.bib24))
    have focused on improving the accuracy of repository-level code completion tasks.
    However, most of these studies overlook the unique aspects of their Fill-in-the-Middle
    (FIM) capacities. Furthermore, despite the recent development of repository-level
    pretrained Code LLMs designed to process large-scale repository data, research
    on these models remains relatively limited.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Dataset & Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To assess the code completion performance of Code LLMs in real development scenarios,
    we utilized CrossCodeEval (Ding et al., [2023](#bib.bib11)) as the evaluation
    dataset. The CrossCodeEval (Ding et al., [2023](#bib.bib11)) benchmark provides
    test cases that require the use of cross-file code information for completion.
    Without loss of generality, in this study, we have chosen Python language as the
    primary language for our research.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the original data from CrossCodeEval, retaining the original repository
    structure. For each test case, we first identified the file for completion and
    the cursor’s position (the line and column where the completion occurs). We then
    removed the code after the cursor in that line to form authentic completion test
    cases. Ultimately, we obtained 2,655 real-world completion tests. Following the
    CrossCodeEval evaluation protocol, we evaluated the completion results using two
    metrics: *Exact Match* (EM) and *Edit Similarity* (ES).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Models & Prompt Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code large language models pretrained with repository-level code data include
    specific tokens used to describe the repository structure in the prompt. Table
    [6](#A2.T6 "Table 6 ‣ Appendix B Special Tokens & Prompt Templates ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") in appendix displays the special tokens used by DeepseekCoder, Starcoder2
    and CodeGemma. The specific prompt templates used by DeepseekCoder, Starcoder2
    and CodeGemma are shown in Table [7](#A2.T7 "Table 7 ‣ Appendix B Special Tokens
    & Prompt Templates ‣ Hierarchical Context Pruning: Optimizing Real-World Code
    Completion with Repository-Level Pretrained Code LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Hardware & Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the expiriments were conducted on NVIDIA A100 GPUs. We employ greedy decoding
    strategy for all the models, and set max_new_tokens to $32$, respectively. All
    the prompts longer than the model_max_length are truncated from the left.
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Baseline Evaluation |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| Infile-Only | 16.72 | 56.58 | 28.14 | 68.36 | 21.92 | 61.49 | 22.98 | 63.58
    | 20.64 | 56.26 | 30.58 | 70.36 |'
  prefs: []
  type: TYPE_TB
- en: '| RAG-BM25 | 17.28 | 58.18 | 32.65 | 71.78 | 24.45 | 63.84 | 26.26 | 65.32
    | 22.89 | 57.73 | 32.89 | 70.81 |'
  prefs: []
  type: TYPE_TB
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The completion results of the baseline methods. EM denotes Exact Match,
    and ES denotes Edit Similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Topoligical Dependency Analysis |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| D-Level: 1 | 15.44 | 55.03 | 33.03 | 70.77 | 26.18 | 64.15 | 28.51 | 66.91
    | 24.37 | 58.79 | 34.65 | 73.01 |'
  prefs: []
  type: TYPE_TB
- en: '| D-Level: 2 | 13.63 | 53.45 | 33.56 | 70.74 | 26.70 | 64.58 | 29.45 | 67.03
    | 25.31 | 59.27 | 35.67 | 73.26 |'
  prefs: []
  type: TYPE_TB
- en: '| D-Level: 3 | 13.26 | 53.17 | 33.07 | 70.51 | 26.82 | 64.56 | 29.23 | 67.01
    | 25.35 | 59.30 | 35.93 | 73.34 |'
  prefs: []
  type: TYPE_TB
- en: '| D-Level: 4 | 13.37 | 53.20 | 33.22 | 70.57 | 26.59 | 64.46 | 29.53 | 67.07
    | 25.54 | 59.42 | 36.12 | 73.54 |'
  prefs: []
  type: TYPE_TB
- en: '| D-Level: $\infty$ | 5.76 | 46.22 | 35.29 | 71.51 | 30.43 | 67.34 | 33.03
    | 69.57 | 29.08 | 62.91 | 39.32 | 75.35 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison of completion results using different context dependency
    levels across 6 models. All the prompts is truncated to the max context window
    of the Code LLMs from the left. *$\infty$* denotes the prompt including all files
    in the repository.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Preliminary Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Baseline Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Infile Only
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We initially evaluated the model’s completion ability using only information
    from the current file, with results presented in Table [1](#S3.T1 "Table 1 ‣ 3.3
    Hardware & Hyperparameters ‣ 3 Experiments Setup ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    under the Infile-Only row. The completion results are less than satisfactory.
    Even the best-performing model achieved an accuracy of only about 30%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b53ed69fcfe81a58572b6acd4667017c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The distribution of tokenized prompt lengths in the CrossCodeEval
    benchmark. The x-aixs represents the dependent level, and the y-axis represents
    the number of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 RAG-BM25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We subsequently evaluated the effect of using Retrieval Augmented Generation
    (RAG) method to retrieve relevant code snippets to assist with completion. Following
    the setup of CrossCodeEval, we chunk the repository code into units of 10 lines,
    and use BM25 as similarity metric for retrieving relevant code snippets. We select
    the top-5 relevant snippets as cross-file information, which are placed at the
    beginning of the prompt to assist with code generation. The results are shown
    in Table [1](#S3.T1 "Table 1 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments
    Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
    Repository-Level Pretrained Code LLMs") under the RAG-BM25 row.'
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Cross-File Content Analysis |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| P-Level: 0 | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  prefs: []
  type: TYPE_TB
- en: '| P-Level: 1 | 6.55 | 46.58 | 36.20 | 71.90 | 30.73 | 67.97 | 34.43 | 70.65
    | 29.30 | 63.46 | 39.55 | 75.70 |'
  prefs: []
  type: TYPE_TB
- en: '| P-Level: 2 | 9.83 | 49.63 | 34.73 | 70.89 | 30.02 | 66.41 | 31.26 | 68.24
    | 27.34 | 61.13 | 38.31 | 74.32 |'
  prefs: []
  type: TYPE_TB
- en: '| + D-level:1 | 9.45 | 49.44 | 36.87 | 72.14 | 29.91 | 66.96 | 32.62 | 69.11
    | 28.93 | 62.03 | 39.17 | 75.16 |'
  prefs: []
  type: TYPE_TB
- en: '| + D-level:2 | 8.70 | 48.61 | 36.38 | 71.66 | 29.64 | 66.99 | 32.96 | 69.13
    | 28.44 | 61.76 | 39.06 | 74.91 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The results of completion using cross-file information with different
    pruning levels. *+ D-level:x* denotes the model uses the cross-file information
    with dependency level x.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Randomly Concatenating All Files
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Additionally, we concatenated all repository code files randomly according
    to the pre-trained formats of various Repo-Code LLMs to create completion prompts,
    which were then input into the models for completion. The evaluation results are
    shown in Table [1](#S3.T1 "Table 1 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments
    Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
    Repository-Level Pretrained Code LLMs") under the Random-All row. We observed
    that supplying the model with more information from the repository’s code led
    to superior performance compared to RAG. However, the input length of the model
    is limited by its context window, thereby transforming this scenario into a constrained
    optimization problem. The constrained optimization goal is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\max_{\mathcal{P}}\text{Quality}(\mathcal{P})\quad\text{s.t.}\quad\text{Length}(\mathcal{P})\leq
    L$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{P}$ represents context window size of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Completion Error Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further investigate the issues of repository-level pre-trained Code LLMs
    in real-world completion tasks, we sampled 200 error examples from each model’s
    *Random-All* evaluation results for error analysis. Ultimately, we categorized
    the issues present in these models into eight classes: *Parameter Value Error*,
    *Non-existent Method Call*, *Improper Method Invocation*, *Missing Method Invocation*,
    *Redundant Content Generation*, *Partial Content Missing*, *Incorrect Content
    Generation*, and *Exact Match Error*. Figure [1](#S2.F1 "Figure 1 ‣ 2.1.2 Instruction
    Code LLMs ‣ 2.1 Code Large Language Models ‣ 2 Related Work ‣ Hierarchical Context
    Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") shows the error distribution statistics for six Repo-Code LLMs. In
    the appendix [A](#A1 "Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs"), we provide examples of each type of error along with corresponding
    error analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d798e966f40b8e99cbe6fce546731ce7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The framework of hierarchical context pruning for improving the performance
    of code large language models in real-world code completion tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Topological Dependency Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Definition 1.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '(Dependency Level) Let $F$ represent a specific file. We define the dependency
    levels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle I(f)=\{g\mid g\text{ is imported by }f\}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle D_{0}(f)=\{f\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle D_{i+1}(f)=D_{i}(f)\cup I(D_{i}(f))$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We first identified the file requiring completion, then extracted all the import
    statements from the file with *Tree-Sitter*²²2[https://tree-sitter.github.io/tree-sitter](https://tree-sitter.github.io/tree-sitter),
    and used a breadth-first search (BFS) method to progressively add dependent files.
    Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix F Dependency Search Algorithm ‣ D.2
    Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") in appendix shows our specific dependency
    modeling process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [13](#A3.F13 "Figure 13 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") illustrates the growth in the number of dependent files (calculated
    by the length of the tokenized prompt) as the number of dependency layers increases.
    We used median and average as statistical measures and found that in the vast
    majority of cases, the number of dependent files for a single file increases slowly
    after reaching four layers of dependencies. This suggests that using four layers
    of dependencies is sufficient to cover most scenarios. We further define:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle D_{\infty}(f)=D_{4}(f)\cup\{F\setminus D_{4}(f)\}$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: to represent the prompt including all files in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [2](#S3.T2 "Table 2 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments
    Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
    Repository-Level Pretrained Code LLMs"), the D-level rows show the results of
    completion using cross-file information with different dependency levels. The
    results indicate that although the maximum dependency depth of most files reaches
    4 levels, only the information provided by $D_{1}(f)$ files, there are many other
    useful files within the repository.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Cross-File Content Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Definition 2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '(Pruning Level) We define the pruning levels into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P-Level 0: No pruning is applied to the file content.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P-Level 1: All global context content is removed from the file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P-Level 2: All global context content, function bodies and class method bodies
    are removed from the file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.1.2 RAG-BM25 ‣ 4.1 Baseline Evaluation ‣ 4 Preliminary
    Studies ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") presents the results of completion
    using cross-file information with different pruning levels. We can see that the
    results of *P-level:1* outperform those of *P-level:0*, indicating that the Global
    Context information from cross-file content has minimal impact on the completion
    of the current file. Additionally, the results of *P-level:2* are only slightly
    worse than those of $D_{\infty}(f)$. This suggests that the specific implementations
    of most cross-file functions have minimal impact on the completion of the current
    file, and retaining only the function header information is sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Hierarchical Context Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the analysis results concerning the dependencies and content of the
    files, we attempt to construct a hierarchical context prompt based on the importance
    and relevance of the repository content. This approach aims to enhance the accuracy
    of code completion models while effectively reducing the length of the context.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Completion Error Analysis ‣ 4 Preliminary Studies
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") shows the specific process for constructing a hierarchical
    context prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Fine-grained Repository Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to precisely control the content within the code repository, we employ
    *Tree-Sitter* to parse the files within the repository. We model the content using
    three types of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Function Node: Represents a function or a class method within a code file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class Node: Represents a class in a code file, consisting of the class’s name,
    attributes, and Function Nodes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'File Node: Represents a code file, comprising Nodes that represent the functions
    and classes within the file, along with global context information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2 Hierarchical Context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in the top right of Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Completion Error
    Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs"), following the settings
    in Section [4.3](#S4.SS3 "4.3 Topological Dependency Analysis ‣ 4 Preliminary
    Studies ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs"), we conduct a dependency analysis
    on the files in the repository. We perform a topological sort based on the dependency
    relationships, centering around the file currently being completed. According
    to the experimental results in Section [4.3](#S4.SS3 "4.3 Topological Dependency
    Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs"), only files at dependency
    level 1 significantly enhance completion accuracy. Therefore, we select files
    designated as $D_{1}(f)$ to serve as dependency files. Ultimately, the files in
    the repository are categorized into three types: *current file*, *dependency files*,
    and *other files*. We will apply different strategies to optimize each type of
    file.'
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Hierarchical Context Pruning (Top-p: 1.0) |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 0 | 9.45 | 49.44 | 36.87 | 72.14 | 29.91 | 66.96 | 32.62 | 69.11 |
    28.93 | 62.03 | 39.17 | 75.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 5 | 9.64 | 49.78 | 39.74 | 73.90 | 32.68 | 69.05 | 35.76 | 71.41 |
    31.26 | 63.74 | 42.44 | 76.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 10 | 9.91 | 49.85 | 40.30 | 74.56 | 34.15 | 69.37 | 36.47 | 71.50
    | 31.82 | 64.34 | 42.63 | 77.35 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The results of completion using hierarchical context pruning with
    different top-k values.'
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Hierarchical Context Pruning (Top-k: 5) |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.1 | 14.27 | 53.94 | 37.85 | 73.11 | 32.99 | 68.75 | 34.16 | 70.43
    | 29.19 | 62.09 | 40.98 | 76.26 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.2 | 13.52 | 53.20 | 38.04 | 73.13 | 33.15 | 68.59 | 34.84 | 70.40
    | 29.72 | 62.32 | 40.94 | 76.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.3 | 12.88 | 52.60 | 38.49 | 73.19 | 32.84 | 68.31 | 35.22 | 70.64
    | 30.13 | 62.77 | 41.21 | 76.20 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The results of completion using hierarchical context pruning with
    different top-p values.'
  prefs: []
  type: TYPE_NORMAL
- en: Current File.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For the current file, any content within the file may be needed during completion,
    so we retain all content of the file and convert it into the Fill-in-the-middle
    (FIM) format.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency Files.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'According to the experimental results in Section [4.4](#S4.SS4 "4.4 Cross-File
    Content Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs"), removing
    the global context across files does not affect the accuracy of completions. Therefore,
    for dependency files, we remove all global context from these files.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Files.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We refer to files other than the current file and its direct dependency files,
    namely $\{F\setminus D_{1}(f)\}\setminus f\}$, collectively as other files. For
    the content in *other files*, we remove all global context, and then we employ
    function-level sampling and pruning methods to optimize the content of these files.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e54bb90bdb16946bf3b7f6b5dc8f2a41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: left: Comparison of completion results using random-all and the hierarchical
    context pruning across six models. middle: Comparison of throughput using random-all
    and the hierarchical context pruning across six models. right: Comparison of prompt
    length using random-all and the hierarchical context pruning of different top-p
    values (top-k=5).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Function-level Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this study, we used OpenAI’s text-embedding API³³3openai-text-embedding-ada-002
    to embed each function (or class method) and query code snippet in the repository.
    We then used the pre-computed similarity of embeddings between the query and candidate
    functions (or class methods) as an indicator of relevance. We select the code
    from the current line of completion and the 10 lines before and after it as a
    query to find functions and class methods most relevant to the current completion
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'We implemented two sampling strategies (top-k and top-p) and designed distinct
    content pruning strategies for the functions (or class methods) sampled under
    each strategy, see Section [5.4](#S5.SS4 "5.4 Function-level Pruning ‣ 5 Hierarchical
    Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Function-level Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the experimental results in Section [4.4](#S4.SS4 "4.4 Cross-File
    Content Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs"), the global
    context from all non-current files and most of the function bodies (or class method
    bodies) within the code repository can be pruned. Appropriately pruning low-relevance
    content can significantly reduce the length of the prompt input to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $F$ represent the functions sampled using the top-p strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F_{k}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle F_{p}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $F_{k}\subseteq F_{p}$ was completely pruned.
  prefs: []
  type: TYPE_NORMAL
- en: Top-k Context Pruning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For functions (or class methods) within the set $F_{k}$, we retained their entire
    content.
  prefs: []
  type: TYPE_NORMAL
- en: Top-p Context Pruning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For functions (or class methods) in the set $F_{p}$, we prune their implementations
    and retained only their function headers (or class method headers).
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 File-level Relevance Ranking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each function or class method in the repository has a similarity score. We assign
    different relevance weights to functions sampled using different sampling strategies.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle W(f)$ | $$\displaystyle=\begin{cases}1.0,\quad\forall f\in
    F_{k}\\ 0.5,\quad\forall f\in F_{p}\setminus F_{k}\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0.0,\quad\forall f\in F\setminus(F_{k}\cup F_{p})\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}$$ |  | (5) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\text{Top}_{k}(F)$ represent the functions with the highest relevance
    scores sampled using the top-k and top-p strategies, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The similarity of a class is defined as the weighted sum of its class methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle S(c)=\sum_{m\in c}W(m)*S(m)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where, $c$ represents the class method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The similarity of a file is defined as the weighted sum of its functions and
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle S(f)=\sum_{x\in\mathcal{F}}W(x)*S(x)+\sum_{c\in\mathcal{C}}S(c)$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where, $\mathcal{F}$ represent the set of functions and classes in the file,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we sort the files at the file-level according to the relevance score
    to determine their relative positions in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We initially fixed top-p at 1.0 and tested the impact of different top-k values
    on completion accuracy. Table [4](#S5.T4 "Table 4 ‣ 5.2 Hierarchical Context ‣
    5 Hierarchical Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") presents some of
    the experimental results, while Table [11](#A4.T11 "Table 11 ‣ D.2 Hit Count Changes
    ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") in the Appendix [E](#A5 "Appendix E Complete Funcation-Level Sampling
    Experiment Results ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental Results
    ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs") provides
    a more comprehensive results. We observed that increasing the top-k value beyond
    5 did not result in significant improvements in accuracy. Therefore, we conclude
    that a top-k value of 5 is sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further fixed the top-k value at 5 and tested the impact of varying top-p
    values (ranging from 0.1 to 0.9) on completion accuracy. Partial experimental
    results are presented in Table [5](#S5.T5 "Table 5 ‣ 5.2 Hierarchical Context
    ‣ 5 Hierarchical Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs"), with more comprehensive
    results available in Table [12](#A4.T12 "Table 12 ‣ D.2 Hit Count Changes ‣ D.1
    Complete Experimental Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") in Appendix [E](#A5 "Appendix E Complete Funcation-Level Sampling
    Experiment Results ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental Results
    ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs"). Our observations
    indicate that increasing the top-p value enhances completion accuracy; however,
    beyond a top-p value of 0.3, the improvement in accuracy slows considerably. Thus,
    we consider 0.3 to be a reasonable value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ Other Files. ‣ 5.2 Hierarchical Context ‣ 5 Hierarchical
    Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") visually compares the Hierarchical
    Context Pruning (HCP) strategy (top-k=5, top-p=0.3) with the method of randomly
    concatenating all repository code files across three dimensions: completion accuracy,
    throughput rate, and input length. The visualization shows that, compared to random
    concatenation, HCP significantly reduces input length (enhancing throughput) while
    improving the model’s completion accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we evaluated six Code LLMs pre-trained with repository-level
    code data. We conducted a detailed error analysis on these Code LLMs, performed
    topological dependency analysis on files within the code repositories, and analyzed
    the content of these files. Based on the results of these experiments, we proposed
    a strategy named Hierarchical Context Pruning to construct high-quality prompt
    inputs. Finally, we conducted experiments on six Repo-Code LLMs to verify the
    effectiveness of the proposed method.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmark.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this study, we utilized the CrossCodeEval benchmark for evaluation. However,
    as demonstrated in the error analysis presented in Sections [4.2](#S4.SS2 "4.2
    Completion Error Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    and Appendix [A](#A1 "Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs"), while the evaluation method based on exact matches is convenient
    and quick, it does not provide comprehensive results. Therefore, there may be
    a discrepancy between the evaluation outcomes and the actual capabilities of the
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Function-level Sampling.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this study, sampling functions and class methods based on relevance required
    the use of a text embedding model. When the number of code files in the repository
    is excessive, this may reduce the sampling rate, leading to increased completion
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study does not involve human participants, personal data, or hazardous
    materials, and primarily focuses on computational model performance. All resources
    used are open-source or properly licensed, ensuring compliance with relevant standards.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    and Alex Gu et al. 2023. [Santacoder: don’t reach for the stars!](http://arxiv.org/abs/2301.03988)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, and Charles Sutton. 2021. [Program synthesis with large language models](http://arxiv.org/abs/2108.07732).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. [Longbench: A bilingual, multitask benchmark for long context
    understanding](http://arxiv.org/abs/2308.14508).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bavarian et al. (2022) Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman,
    Christine McLeavey, Jerry Tworek, and Mark Chen. 2022. [Efficient training of
    language models to fill in the middle](http://arxiv.org/abs/2207.14255).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2024) Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan,
    Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, and Hai Jin. 2024. [Iterative
    refinement of project-level code context for precise code generation with compiler
    feedback](http://arxiv.org/abs/2403.16792).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cassano et al. (2022) Federico Cassano, John Gouwar, Daniel Nguyen, Sydney
    Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane
    Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
    2022. [Multipl-e: A scalable and extensible approach to benchmarking neural code
    generation](http://arxiv.org/abs/2208.08227).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2024a) Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo,
    and Min Yang. 2024a. [Long context is not long at all: A prospector of long-dependency
    data for large language models](http://arxiv.org/abs/2405.17915).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, and et al. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. [Extending context window of large language models via positional
    interpolation](http://arxiv.org/abs/2306.15595).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2024b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. 2024b. [Longlora: Efficient fine-tuning of long-context
    large language models](http://arxiv.org/abs/2309.12307).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023) Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding,
    Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,
    Dan Roth, and Bing Xiang. 2023. [Crosscodeeval: A diverse and multilingual benchmark
    for cross-file code completion](http://arxiv.org/abs/2310.11248).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2024) Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan
    Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. [Longrope: Extending
    llm context window beyond 2 million tokens](http://arxiv.org/abs/2402.13753).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel
    Kang. 2024. [Llm agents can autonomously hack websites](http://arxiv.org/abs/2402.06664).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fried et al. (2023) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2023. [Incoder: A generative model for code infilling and synthesis](http://arxiv.org/abs/2204.05999).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. 2024. [Deepseek-coder: When the large language model meets programming
    – the rise of code intelligence](http://arxiv.org/abs/2401.14196).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    and Jacob Steinhardt. 2021. [Measuring coding challenge competence with apps](http://arxiv.org/abs/2105.09938).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Holt et al. (2024) Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar.
    2024. [L2mac: Large language model automatic computer for extensive code generation](http://arxiv.org/abs/2310.02003).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jimenez et al. (2024) Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. [Swe-bench: Can language
    models resolve real-world github issues?](http://arxiv.org/abs/2310.06770)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2022) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
    2022. [Ds-1000: A natural and reliable benchmark for data science code generation](http://arxiv.org/abs/2211.11501).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024a) Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin.
    2024a. [Evocodebench: An evolving code generation benchmark aligned with real-world
    code repositories](http://arxiv.org/abs/2404.00599).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    and et al. 2023. [Starcoder: may the source be with you!](http://arxiv.org/abs/2305.06161)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
    Koray Kavukcuoglu, and Oriol Vinyals. 2022. [Competition-level code generation
    with alphacode](https://doi.org/10.1126/science.abq1158). *Science*, 378(6624):1092–1097.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2024b) Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang,
    Lei Zhang, Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, and
    Yongbin Li. 2024b. [One-shot learning as instruction data prospector for large
    language models](http://arxiv.org/abs/2312.10302).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2024) Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng
    Di, wei jiang, Hongwei Chen, Chengpeng Wang, and Gang Fan. 2024. [Repofuse: Repository-level
    code completion with fused dual context](http://arxiv.org/abs/2402.14323).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. [Repobench:
    Benchmarking repository-level code auto-completion systems](http://arxiv.org/abs/2306.03091).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lozhkov et al. (2024) Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, and Arthur Zucker et al.
    2024. [Starcoder 2 and the stack v2: The next generation](http://arxiv.org/abs/2402.19173).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. [Wizardcoder:
    Empowering code large language models with evol-instruct](http://arxiv.org/abs/2306.08568).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Muennighoff et al. (2024) Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
    Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra,
    and Shayne Longpre. 2024. [Octopack: Instruction tuning code large language models](http://arxiv.org/abs/2308.07124).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2023a) Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio
    Savarese, and Yingbo Zhou. 2023a. [Codegen2: Lessons for training llms on programming
    and natural languages](http://arxiv.org/abs/2305.02309).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2023b) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023b. [Codegen: An open
    large language model for code with multi-turn program synthesis](http://arxiv.org/abs/2203.13474).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://arxiv.org/abs/2203.02155).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Phan et al. (2024) Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, and Nghi D. Q.
    Bui. 2024. [Repohyper: Better context retrieval is all you need for repository-level
    code completion](http://arxiv.org/abs/2403.06095).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinnaparaju et al. (2024) Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung,
    Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan,
    Marco Bellagente, Carlos Riquelme, and Nathan Cooper. 2024. [Stable code technical
    report](http://arxiv.org/abs/2404.01226).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, and Artyom Kozhevnikov et al. 2024. [Code llama: Open
    foundation models for code](http://arxiv.org/abs/2308.12950).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2024) Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang
    Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D. Wang. 2024. [Ehragent: Code empowers
    large language models for few-shot complex tabular reasoning on electronic health
    records](http://arxiv.org/abs/2401.07128).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrivastava et al. (2023) Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
    2023. [Repository-level prompt generation for large language models of code](http://arxiv.org/abs/2206.12839).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2024) Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang
    Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng
    Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li,
    and Zhiyong Wu. 2024. [A survey of neural code intelligence: Paradigms, advances
    and beyond](http://arxiv.org/abs/2403.14734).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2024) CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A.
    Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua
    Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, Peter
    Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris
    Warkentin, and Zhitao Gong. 2024. [Codegemma: Open code models based on gemma](https://goo.gle/codegemma).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thakur et al. (2024) Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin,
    and Swarat Chaudhuri. 2024. [An in-context learning agent for formal theorem-proving](http://arxiv.org/abs/2310.04353).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024a) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, and Heng Ji. 2024a. [Executable code actions elicit better llm agents](http://arxiv.org/abs/2402.01030).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024b) Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao
    Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, and Weiran
    Xu. 2024b. [Dolphcoder: Echo-locating code large language models with diverse
    and multi-objective instruction tuning](http://arxiv.org/abs/2402.09136).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel,
    Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith,
    Iz Beltagy, and Hannaneh Hajishirzi. 2023a. [How far can camels go? exploring
    the state of instruction tuning on open resources](http://arxiv.org/abs/2306.04751).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig.
    2023b. [Execution-based evaluation for open-domain code generation](http://arxiv.org/abs/2212.10481).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2024) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2024. [Efficient streaming language models with attention sinks](http://arxiv.org/abs/2309.17453).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao,
    Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. 2023. [Lemur: Harmonizing
    natural language and code for language agents](http://arxiv.org/abs/2310.06830).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024) John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret,
    Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. [Swe-agent: Agent-computer
    interfaces enable automated software engineering](https://api.semanticscholar.org/CorpusID:270063685).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu
    Yao. 2023. [Intercode: Standardizing and benchmarking interactive coding with
    execution feedback](http://arxiv.org/abs/2306.14898).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2022) Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming
    Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski,
    Alex Polozov, and Charles Sutton. 2022. [Natural language to code generation in
    interactive data science notebooks](http://arxiv.org/abs/2212.09248).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zelikman et al. (2024) Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman
    Kalai. 2024. [Self-taught optimizer (stop): Recursively self-improving code generation](http://arxiv.org/abs/2310.02304).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023a) Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu,
    Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023a. [Repocoder: Repository-level
    code completion through iterative retrieval and generation](http://arxiv.org/abs/2303.12570).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi yang, Junhao
    Liu, and Min Yang. 2023b. [Marathon: A race through the realm of long context
    with large language models](http://arxiv.org/abs/2312.09542).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024a) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. 2024a. [$\infty$bench: Extending long context evaluation beyond 100k tokens](http://arxiv.org/abs/2402.13718).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024b) Yakun Zhang, Wenjie Zhang, Dezhi Ran, Qihao Zhu, Chengfeng
    Dou, Dan Hao, Tao Xie, and Lu Zhang. 2024b. [Learning-based widget matching for
    migrating gui test cases](https://doi.org/10.1145/3597503.3623322). In *Proceedings
    of the 46th IEEE/ACM International Conference on Software Engineering*, ICSE ’24\.
    ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2024) Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen
    Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024. [Opencodeinterpreter: Integrating
    code generation with execution and refinement](http://arxiv.org/abs/2402.14658).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Error Description and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present detailed instances of various error types in model
    completions, accompanied by in-depth explanations and analyses of these errors.
    Figures [5](#A1.F5 "Figure 5 ‣ A.1 Redundant Content Generation ‣ Appendix A Error
    Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs")-[12](#A1.F12 "Figure
    12 ‣ A.8 Incorrect Content Generation ‣ Appendix A Error Description and Analysis
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") depict representatives for each error category. Each figure
    is bifurcated, with the left panel showing the output generated by the code model
    and the right panel presenting the corresponding ground truth. Errors in model
    completions are emphasized in red italic text, whereas the ground truth is denoted
    in green italic.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Redundant Content Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Redundant Content Generation means that the method is correctly called, but
    unnecessary additional content is generated. Figure [5](#A1.F5 "Figure 5 ‣ A.1
    Redundant Content Generation ‣ Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") illustrates an example of a Redundant Content Generation error. The
    ground truth specifies active is False, yet the model’s completion includes not
    only active is False but also additional irrelevant content.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d3b1f0fdf4de3ab27a9c48313806a12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example of redundant content generation error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Partial Content Missing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Partial Content Missing indicates that the right method is called, but the
    generated content is incomplete, although this might still be acceptable to the
    user. Figure [6](#A1.F6 "Figure 6 ‣ A.2 Partial Content Missing ‣ Appendix A Error
    Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") presents an example
    of a Partial Content Missing error. The ground truth is MinGrid and not game_name.startswith(’MiniGrid-’),
    but the code completion model only managed to replicate a portion of this ground
    truth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/71b30d7b6ed5af1a4c9f4c3528f77fe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example of partial content missing error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Parameter Value Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Parameter Value Error reflects the situation where the function call is
    correct, but the passed parameter values are incorrect. Figure [7](#A1.F7 "Figure
    7 ‣ A.3 Parameter Value Error ‣ Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") displays an instance of a Parameter Value Error. The code completion
    model correctly invokes the class method, but the parameters it employs differ
    from those specified in the ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70d58d849937050933b1b98e8ff2478c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example of parameter value error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Exact Match Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exact Match Error is a misjudgment due to the limitations of the exact match
    metric, such as using default values or specific strings when calling a function.
    Figure [8](#A1.F8 "Figure 8 ‣ A.4 Exact Match Error ‣ Appendix A Error Description
    and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") illustrates an example of an Exact
    Match Error. The content completed by the code model is syntactically correct
    and semantically accurate, differing only slightly in textual terms from the ground
    truth. To avoid such misjudgments, a more reasonable evaluation method is necessary
    to assess the completion results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca5e7bd7c324141d6354ed1a79c6dc41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An example of exact match error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Non-existent Method Call
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Non-existent Method Call indicates a call to a function, method, or property
    that does not exist. Figure [9](#A1.F9 "Figure 9 ‣ A.5 Non-existent Method Call
    ‣ Appendix A Error Description and Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs") presents
    an example of a Non-existent Method Call error. The ground truth refers to a class
    method within the session class; however, the content generated by the code completion
    model erroneously calls a method that does not exist in the session class. This
    error can be regarded as a form of hallucination in the context of code completion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f6ff1444dd0f40630382fb5bcc7a1d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An example of non-existent method call error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Improper Method Invocation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Improper Method Invocation represents the situation where the call is made
    to an existing method, but a different, more appropriate method should have been
    used. Figure [10](#A1.F10 "Figure 10 ‣ A.6 Improper Method Invocation ‣ Appendix
    A Error Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") showcases an example
    of an Improper Method Invocation error. The code completion model generated a
    call to the class method Transformer within the llp class, whereas the correct
    content should have invoked the class method Geometric within the same class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8eb9665b71cb825dc9ce871156328a10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An example of improper method invocation error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.7 Missing Method Invocation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Missing Method Invocation indicates that a function or method should have been
    called to achieve functionality, but the model failed to make this call. Figure
    [11](#A1.F11 "Figure 11 ‣ A.7 Missing Method Invocation ‣ Appendix A Error Description
    and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") illustrates an example of a Missing
    Method Invocation error. The ground truth involves calling the class method paginate
    from the query class to obtain the queried variable. However, the code completion
    model failed to complete this method invocation and instead achieved the same
    functionality through multiple alternative class methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/adea5e3b43a5e644da334af94f496e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example of missing method invocation error.'
  prefs: []
  type: TYPE_NORMAL
- en: A.8 Incorrect Content Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Incorrect Content Generation represents the situation where the generated content
    is illogical, irrelevant to the current code context, or completely incorrect.
    Figure [12](#A1.F12 "Figure 12 ‣ A.8 Incorrect Content Generation ‣ Appendix A
    Error Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") depicts an example
    of an Incorrect Content Generation error. The content produced by the code completion
    model is entirely unrelated to the ground truth and also lacks relevance to the
    current code context.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d47350813cabb23d7941c6d18ee5269.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An example of incorrect content generation error.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Special Tokens & Prompt Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [6](#A2.T6 "Table 6 ‣ Appendix B Special Tokens & Prompt Templates ‣
    Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") shows the special tokens used by DeepseekCoder, Starcoder2,
    and CodeGemma for fill-in-the-middle code completion. The prompt templates for
    DeepseekCoder, Starcoder2, and CodeGemma are shown in Table [7](#A2.T7 "Table
    7 ‣ Appendix B Special Tokens & Prompt Templates ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs").
    Both Starcoder2 and CodeGemma utilize special tokens for segmenting code files,
    whereas DeepseekCoder does not employ such tokens, despite being trained on repository-level
    code data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |               Special Tokens |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepseekCoder | <&#124;fim_begin&#124;>,<&#124;fim_hole&#124;>,<&#124;fim_end&#124;>
    |'
  prefs: []
  type: TYPE_TB
- en: '| Starcoder2 | <repo_name>,<file_sep>,<fim_pad>,<fim_prefix>,<fim_suffix>,<fim_middle>
    |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGemma | <&#124;file_separator&#124;>,<&#124;fim_prefix&#124;>,<&#124;fim_suffix&#124;>,<&#124;fim_middle&#124;>
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Special tokens used by DeepseekCoder, Starcoder2 and CodeGemma for
    fill-in-the-middle code completion.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |             Fill-in-the-Middle Prompt Template |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepseekCoder | #file_path0\ncode0\n#file_path1\ncode1\n#file_path2\ncode2\n#file_path3\n
    |'
  prefs: []
  type: TYPE_TB
- en: '| <&#124;fim_begin&#124;>prefix_code<&#124;fim_hole&#124;>suffix_code<&#124;fim_end&#124;>
    |'
  prefs: []
  type: TYPE_TB
- en: '| Starcoder2 | <repo_name>reponame<file_sep>file_path0\ncode0<file_sep>file_path1
    |'
  prefs: []
  type: TYPE_TB
- en: '| <fim_prefix>prefix_code<fim_suffix>suffix_code<fim_middle> |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGemma | <&#124;file_separator&#124;>file_path0\ncode0<file_separator>file_path1\n
    |'
  prefs: []
  type: TYPE_TB
- en: '| <&#124;fim_prefix&#124;>prefix_code<&#124;fim_suffix&#124;>suffix_code<&#124;fim_middle&#124;>
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Prompt templates for DeepseekCoder, Starcoder2 and CodeGemma.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompt Length Distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [8](#A3.T8 "Table 8 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") presents the average and median lengths of input sequences for three
    code completion models when utilizing contexts of varying dependency levels. Notably,
    Level $\infty$, which incorporates the entire repository code into the input,
    results in an average input sequence length exceeding 50,000, far surpassing the
    context window supported by these models. To more visually observe the changes
    in input sequence length with respect to dependency levels, Figure [13](#A3.F13
    "Figure 13 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    was created. It is evident that the median input sequence length begins to converge
    once the dependency level reaches 2, and the average input sequence length also
    starts to stabilize after reaching a dependency level of 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b8f8a3eda657bb047e0c6377ba8ef06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The distribution of tokenized prompt lengths in the CrossCodeEval
    benchmark. The x-aixs represents the dependent level, and the y-axis represents
    the number of tokens. denotes the median value of the tokenized prompt length.
    denotes the average value of the tokenized prompt length.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | CrossCodeEval Benchmark: Python |'
  prefs: []
  type: TYPE_TB
- en: '| Level 0 | Level 1 | Level 2 | Level 3 | Level 4 | Level $\infty$ |'
  prefs: []
  type: TYPE_TB
- en: '| Median | Average | Median | Average | Median | Average | Median | Average
    | Median | Average | Median | Average |'
  prefs: []
  type: TYPE_TB
- en: '| DeepseekCoder | 1,445 | 2,272 | 3,161 | 5,248 | 4,434 | 7,938 | 4,559 | 8,967
    | 4,640 | 9,252 | 44,475 | 58,217 |'
  prefs: []
  type: TYPE_TB
- en: '| Starcoder2 | 1,245 | 1,967 | 2,694 | 4,513 | 3,796 | 6,815 | 3,904 | 7,695
    | 3,958 | 7,940 | 38,174 | 50,632 |'
  prefs: []
  type: TYPE_TB
- en: '| CodeGemma | 1,309 | 2,050 | 2,817 | 4,739 | 3,989 | 7,179 | 4,110 | 8,112
    | 4,164 | 8,369 | 39,647 | 52,875 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: The median and average tokenized prompt lengths of the DeepseekCoder,
    Starcoder2 and CodeGemma models on the CrossCodeEval: Python benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Dependency Level Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 Complete Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [8](#A3.T8 "Table 8 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") documents the comprehensive experimental results of repository file
    dependency analyses across six code completion models. It is observed that when
    the length of the input sequence exceeds the model’s context window, there is
    a significant decrease in completion accuracy. However, truncating the input sequence
    from the left to fit within the model’s context window size reveals that greater
    amounts of code repository content can enhance completion accuracy. Additionally,
    it was found that the DeepseekCoder-1.3B model exhibits a severe performance degradation
    in completion accuracy as the number of repository files increases.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dependency | Topological Dependency Analysis |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| Dep-Level: 0 | 16.72 | 56.60 | 28.14 | 68.40 | 21.92 | 61.45 | 23.16 | 63.62
    | 20.60 | 55.97 | 30.40 | 69.76 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline   + left truncate | 16.72 | 56.58 | 28.14 | 68.36 | 21.92 | 61.49
    | 22.98 | 63.58 | 20.64 | 56.26 | 30.58 | 70.36 |'
  prefs: []
  type: TYPE_TB
- en: '| Dep-Level: 1 | 14.99 | 54.33 | 32.20 | 68.57 | 26.33 | 64.54 | 28.66 | 67.00
    | 23.16 | 55.00 | 32.17 | 65.77 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline   + left truncate | 15.44 | 55.03 | 33.03 | 70.77 | 26.18 | 64.15
    | 28.51 | 66.91 | 24.37 | 58.79 | 34.65 | 73.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Dep-Level: 2 | 12.73 | 51.72 | 30.21 | 65.46 | 26.63 | 64.50 | 29.83 | 67.03
    | 21.24 | 49.62 | 28.36 | 57.76 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline   + left truncate | 13.63 | 53.45 | 33.56 | 70.74 | 26.70 | 64.58
    | 29.45 | 67.03 | 25.31 | 59.27 | 35.67 | 73.26 |'
  prefs: []
  type: TYPE_TB
- en: '| Dep-Level: 3 | 12.28 | 50.90 | 28.93 | 63.67 | 26.74 | 64.52 | 29.42 | 66.58
    | 20.30 | 47.64 | 27.16 | 55.66 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline   + left truncate | 13.26 | 53.17 | 33.07 | 70.51 | 26.82 | 64.56
    | 29.23 | 67.01 | 25.35 | 59.30 | 35.93 | 73.34 |'
  prefs: []
  type: TYPE_TB
- en: '| Dep-Level: 4 | 12.13 | 50.69 | 28.44 | 63.15 | 26.48 | 64.30 | 29.68 | 66.84
    | 20.08 | 47.29 | 26.93 | 55.16 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline   + left truncate | 13.37 | 53.20 | 33.22 | 70.57 | 26.59 | 64.46
    | 29.53 | 67.07 | 25.54 | 59.42 | 36.12 | 73.54 |'
  prefs: []
  type: TYPE_TB
- en: '| Dep-Level: $\infty$ | 1.32 | 28.04 | 7.08 | 17.53 | 18.19 | 51.92 | 24.52
    | 54.73 | 1.54 | 6.17 | 1.85 | 3.88 |'
  prefs: []
  type: TYPE_TB
- en: '| \hdashline   + left truncate | 5.76 | 46.22 | 35.29 | 71.51 | 30.43 | 67.34
    | 33.03 | 69.57 | 29.08 | 62.91 | 39.32 | 75.35 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Comparison of completion results using different context dependency
    levels across 6 models. EM denotes Exact Match, and ES denotes Edit Similarity.
    *$\infty$* denotes the prompt including all files in the repository. *+left truncate*
    denotes the prompt is truncated to the max context window of LLMs from the left.'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Hit Count Changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [10](#A4.T10 "Table 10 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental
    Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    collates the variations in correct and incorrect completions across six code completion
    models when input contexts of different dependency levels are used. It is evident
    that as the dependency level increases, the variations in the model’s completion
    results become more stable. This stability arises because the changes in the model’s
    input context diminish as the dependency level is elevated. This also indicates
    that augmenting the model’s input with additional content can enhance completion
    accuracy, albeit at the risk of turning some originally correct completions into
    incorrect ones.'
  prefs: []
  type: TYPE_NORMAL
- en: We also observed that the DeepseekCoder series of models lack special tokens
    for delineating repository files; however, this deficiency does not result in
    more pronounced fluctuations in the outcomes. This suggests that the DeepseekCoder
    models are capable of effectively distinguishing between different files in the
    repository, even without the aid of special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Hit Count Changes |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| Infile-Only | +444 | +747 | +582 | +610 | +548 | +812 |'
  prefs: []
  type: TYPE_TB
- en: '|    0 $\rightarrow$ 1 | -108  +74 | -47  +177 | -44  +157 | -37  +184 | -31  +130
    | -68  +176 |'
  prefs: []
  type: TYPE_TB
- en: '| Level: 1 | +408 | +877 | +695 | +755 | +647 | +920 |'
  prefs: []
  type: TYPE_TB
- en: '|    1 $\rightarrow$ 2 | -61  +13 | -33  +47 | -41  +55 | -33  +58 | -30  +55
    | -44  +71 |'
  prefs: []
  type: TYPE_TB
- en: '| Level: 2 | +362 | +891 | +709 | +782 | +672 | +947 |'
  prefs: []
  type: TYPE_TB
- en: '|    2 $\rightarrow$ 3 | -15  +5 | -20  +7 | -13  +16 | -19  +13 | -10  +11
    | -11  +18 |'
  prefs: []
  type: TYPE_TB
- en: '| Level: 3 | +352 | +878 | +712 | +776 | +673 | +954 |'
  prefs: []
  type: TYPE_TB
- en: '|    3 $\rightarrow$ 4 | -3  +6 | -1  +5 | -10  +4 | -3  +11 | -3  +8 | -5  +10
    |'
  prefs: []
  type: TYPE_TB
- en: '| Level: 4 | +355 | +882 | +706 | +784 | +678 | +959 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 $\rightarrow$ | -238  +36 | -135  +190 | -55  +157 | -68  +161 | -45  +139
    | -72  +157 |'
  prefs: []
  type: TYPE_TB
- en: '| Level: $\infty$ | +153 | +937 | +808 | +877 | +772 | +1044 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: The changes in the hit counts of correct and incorrect completions
    across six code completion models when using different context dependency levels.
    The green values denote the number of test samples that were originally correct
    but became incorrect as the dependency level of the input context increased. The
    red values represent the number of test samples that were initially incorrect
    but became correct with the elevation of the input context’s dependency level.'
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Hierarchical Context Pruning (Top-p: 1.0) |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 5 | 9.64 | 49.78 | 39.74 | 73.90 | 32.68 | 69.05 | 35.76 | 71.41 |
    31.26 | 63.74 | 42.44 | 76.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 10 | 9.91 | 49.85 | 40.30 | 74.56 | 34.15 | 69.37 | 36.47 | 71.50
    | 31.82 | 64.34 | 42.63 | 77.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 15 | 9.23 | 49.23 | 40.75 | 74.59 | 33.96 | 69.41 | 36.62 | 71.59
    | 31.86 | 64.53 | 42.55 | 77.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 20 | 9.01 | 48.95 | 41.24 | 74.57 | 34.37 | 69.81 | 36.66 | 71.56
    | 31.93 | 64.67 | 42.85 | 77.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 25 | 8.93 | 48.82 | 40.34 | 74.47 | 33.73 | 69.62 | 37.00 | 71.91
    | 32.46 | 64.81 | 43.11 | 77.47 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-k: 30 | 8.44 | 48.48 | 39.74 | 74.17 | 33.28 | 69.42 | 36.14 | 71.29
    | 32.46 | 64.81 | 42.44 | 77.20 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: The results of completion using hierarchical context pruning with
    different top-k values.'
  prefs: []
  type: TYPE_NORMAL
- en: '| XF-Context | Hierarchical Context Pruning (Top-k: 5) |'
  prefs: []
  type: TYPE_TB
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  prefs: []
  type: TYPE_TB
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  prefs: []
  type: TYPE_TB
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.1 | 14.27 | 53.94 | 37.85 | 73.11 | 32.99 | 68.75 | 34.16 | 70.43
    | 29.19 | 62.09 | 40.98 | 76.26 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.2 | 13.52 | 53.20 | 38.04 | 73.13 | 33.15 | 68.59 | 34.84 | 70.40
    | 29.72 | 62.32 | 40.94 | 76.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.3 | 12.88 | 52.60 | 38.49 | 73.19 | 32.84 | 68.31 | 35.22 | 70.64
    | 30.13 | 62.77 | 41.21 | 76.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.4 | 11.60 | 51.58 | 38.42 | 72.92 | 32.81 | 68.51 | 35.07 | 70.55
    | 30.40 | 62.97 | 40.98 | 76.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.5 | 11.49 | 51.50 | 38.95 | 73.31 | 32.88 | 68.67 | 34.73 | 70.33
    | 30.17 | 62.73 | 41.32 | 76.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.6 | 11.00 | 51.14 | 38.87 | 73.33 | 32.32 | 68.56 | 34.73 | 70.38
    | 30.02 | 63.15 | 41.58 | 76.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.7 | 11.11 | 51.21 | 38.83 | 73.52 | 31.94 | 68.14 | 34.92 | 70.74
    | 30.47 | 63.13 | 41.77 | 76.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.8 | 10.40 | 50.39 | 38.95 | 73.56 | 31.79 | 68.34 | 34.80 | 70.59
    | 30.17 | 63.05 | 41.81 | 76.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Top-p: 0.9 | 10.40 | 50.08 | 38.61 | 73.13 | 31.94 | 68.26 | 34.54 | 70.22
    | 30.43 | 63.18 | 41.81 | 76.51 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: The results of completion using hierarchical context pruning with
    different top-p values.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Complete Funcation-Level Sampling Experiment Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to space constraints, we report only a subset of the results from the function-level
    sampling experiments in the main body. Tables [11](#A4.T11 "Table 11 ‣ D.2 Hit
    Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency Level
    Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") and [12](#A4.T12 "Table 12 ‣ D.2
    Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") provide a comprehensive statistical
    overview of the complete sampling experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: E.1 Top-k Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [11](#A4.T11 "Table 11 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental
    Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    details the results of top-k sampling, where top-p is fixed at 1.0. It is observed
    that increasing the value of k does not significantly enhance the accuracy of
    completions; improvements become negligible when k exceeds 5.'
  prefs: []
  type: TYPE_NORMAL
- en: E.2 Top-p Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [12](#A4.T12 "Table 12 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental
    Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs"),
    on the other hand, presents the outcomes of top-p sampling with top-k fixed at
    5. Here, increasing the value of p does not yield significant improvements, particularly
    when p exceeds 0.3.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Dependency Search Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix F Dependency Search Algorithm ‣
    D.2 Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") delineates the specific process we
    employed for dependency modeling within code repositories. For more detailed implementation
    specifics, please visit our code repository.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Dependency Search Algorithm for Python Files
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: file - initial Python file, maxDepth - maximum search depth2:Output:
    List of dependent files3:function FindDependencies(file, maxDepth)4:     queue
    = [(file, 0)]5:     visited = set()6:     while queue do7:         currentFile,
    currentDepth = queue.pop(0)8:         if currentDepth > maxDepth then9:              break10:         end if11:         imports
    = extractImports(currentFile)12:         for imp in imports do13:              if imp
    is local and imp not in visited then14:                  visited.add(imp)15:                  queue.append((imp,
    currentDepth + 1))16:              end if17:         end for18:     end while19:     return
    visited20:end function21:function extractImports(file)22:     Use Tree-Sitter
    to parse file and extract all import statements23:     return list of imports24:end function'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Additional Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: G.1 Long Context Code Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Research on optimizing large language models for long contexts has been underway
    for some time, with many innovative long-context optimization techniques (Chen
    et al., [2023](#bib.bib9), [2024b](#bib.bib10); Xiao et al., [2024](#bib.bib44);
    Ding et al., [2024](#bib.bib12); Chen et al., [2024a](#bib.bib7)) and evaluation
    sets (Bai et al., [2023](#bib.bib3); Zhang et al., [2023b](#bib.bib51), [2024a](#bib.bib52))
    being proposed and widely applied. Some Code LLMs (Guo et al., [2024](#bib.bib15);
    Lozhkov et al., [2024](#bib.bib26)) utilize these techniques for fine-tuning to
    extend their context windows. Larger context windows allow Code LLMs to receive
    and process more complex code content, such as repository-level code completion
    and repository-level code repair (Ding et al., [2023](#bib.bib11); Liu et al.,
    [2023](#bib.bib25); Zhang et al., [2024b](#bib.bib53); Li et al., [2024a](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: G.2 Code Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Research on Code Agents (Yang et al., [2024](#bib.bib46); Fang et al., [2024](#bib.bib13);
    Thakur et al., [2024](#bib.bib39); Shi et al., [2024](#bib.bib35)) focuses on
    developing intelligent systems that assist in software development by automating
    tasks like code generation and debugging (Holt et al., [2024](#bib.bib17); Wang
    et al., [2023b](#bib.bib43); Yin et al., [2022](#bib.bib48); Zelikman et al.,
    [2024](#bib.bib49)) . The use of Code LLMs has proven effective in understanding
    complex code structures and semantics (Wang et al., [2024a](#bib.bib40); Yang
    et al., [2023](#bib.bib47)). These models have been further refined to handle
    specific software development tasks, including repository-level code analysis
    and automated error correction (Jimenez et al., [2024](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
