- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:05:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs
    for Information Retrieval'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12169](https://ar5iv.labs.arxiv.org/html/2406.12169)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zizhong Li  Haopeng Zhang  Jiawei Zhang
  prefs: []
  type: TYPE_NORMAL
- en: IFM Lab, University of California, Davis
  prefs: []
  type: TYPE_NORMAL
- en: '{zzoli, hapzhang, jiwzhang}@ucdavis.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent research has explored distilling knowledge from large language models
    (LLMs) to optimize retriever models, especially within the retrieval-augmented
    generation (RAG) framework. However, most existing training methods rely on extracting
    supervision signals from LLMs’ weights or their output probabilities, which is
    not only resource-intensive but also incompatible with black-box LLMs. In this
    paper, we introduce Intermediate Distillation, a data-efficient knowledge distillation
    training scheme that treats LLMs as black boxes and distills their knowledge via
    an innovative LLM-ranker-retriever pipeline, solely using LLMs’ ranking generation
    as the supervision signal. Extensive experiments demonstrate that our proposed
    method can significantly improve the performance of retriever models with only
    1,000 training instances. Moreover, our distilled retriever model significantly
    boosts performance in question-answering tasks within the RAG framework, demonstrating
    the potential of LLMs to economically and effectively train smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intermediate Distillation: Data-Efficient Distillation'
  prefs: []
  type: TYPE_NORMAL
- en: from Black-Box LLMs for Information Retrieval
  prefs: []
  type: TYPE_NORMAL
- en: Zizhong Li  Haopeng Zhang  Jiawei Zhang IFM Lab, University of California, Davis
    {zzoli, hapzhang, jiwzhang}@ucdavis.edu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid growth and superior performance of large language models (LLMs) Ouyang
    et al. ([2022](#bib.bib25)); OpenAI ([2023](#bib.bib24)); Wang et al. ([2024b](#bib.bib35))
    have made them a preferred choice for a wide range of NLP applications Xi et al.
    ([2023](#bib.bib37)); Wang et al. ([2024b](#bib.bib35)); Wu et al. ([2023](#bib.bib36));
    Zhang et al. ([2023a](#bib.bib40), [b](#bib.bib41)). LLMs have demonstrated robust
    zero-shot ranking abilities in English and various low-resource languages Adeyemi
    et al. ([2023](#bib.bib1)); Sun et al. ([2023](#bib.bib31)). Consequently, researchers
    have applied LLMs to the task of information retrieval, where they outperform
    previous text search and similarity measurement methods Ma et al. ([2023](#bib.bib19));
    Xu et al. ([2024](#bib.bib39)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ab44b442cbed767659d332dba618b90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Previous distillation methods (left) rely on extracting supervision
    signals from LLM’s weights or using LLM’s output probabilities to train the retriever
    model. In contrast, our approach (right) bypasses the need for LLM’s likelihood,
    directly using the LLM’s ranking responses as supervision signals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The retrieval-augmented generation (RAG) framework has been widely adopted
    to alleviate hallucination problems in LLMs generation, especially for knowledge-intensive
    tasks Lewis et al. ([2020](#bib.bib17)). The RAG framework consists of two key
    components: a retriever to locate relevant information from a large corpus based
    on a given input, and a reader, typically a LLM, to integrate this information
    into its generation Izacard et al. ([2023](#bib.bib13)); Shi et al. ([2023](#bib.bib28)).'
  prefs: []
  type: TYPE_NORMAL
- en: How to distill knowledge from LLMs to optimize the retriever in the RAG framework
    with in-domain data has been a crucial challenge. Early efforts proposed training
    the retriever with white-box LLM readers by extracting supervision signals directly
    from the LLMs’ weights Izacard et al. ([2023](#bib.bib13)); Rubin and Berant ([2023](#bib.bib27));
    Guu et al. ([2020](#bib.bib6)). However, this approach becomes more computationally
    intensive and time-consuming as LLMs increase in size. Meanwhile, it is incompatible
    with closed-source models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, researchers have also turned to knowledge distillation for the retriever
    from black-box LLMs by training the retriever directly from generated outputs,
    such as RePLUG Shi et al. ([2023](#bib.bib28)) and In-Context RALM Ram et al.
    ([2023](#bib.bib26)). However, both methods use the generation log probabilities
    for correct answers as the distillation signal to train the retriever, which may
    suffer from: 1) Limited application scenarios, as the output probabilities are
    not always available for closed-source LLMs. 2) Discrepancy between retrieval
    and generation, where training LLMs’ next-token prediction is not optimal for
    retriever training. 3) High computing costs, since hundreds of thousands of training
    instances are required in their training process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these limitations, we propose Intermediate Distillation, a data-efficient
    training scheme that leverages LLM-generated ranking responses to guide the training
    of the retriever. Our model employs a rerank-then-retrieve pipeline, where LLMs
    indirectly influence the retriever training via an intermediate ranker model.
    We chose this pipeline for three main reasons: 1) The robust zero-shot ranking
    capabilities of LLMs establish a strong foundation for knowledge distillation.
    2) Using LLMs to generate a relevance-based ranking order is more suitable for
    retriever training than depending on LLMs output probabilities, making the supervision
    signals more reliable. 3) There are no restrictions on accessing this generated
    ranking order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we first train a ranker model using the ranking orders generated
    by LLMs as supervision signals. We then employ this trained ranker to further
    train the retriever model. We conduct a series of experiments using advanced,
    closed-source LLMs that restrict output probability access. The empirical results
    demonstrate the effectiveness of our method, requiring 100x to even 1000x less
    data than previous methods Ram et al. ([2023](#bib.bib26)); Shi et al. ([2023](#bib.bib28)),
    thereby significantly reducing computational costs. Our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce Intermediate Distillation, a data-efficient knowledge distillation
    training scheme that optimizes retrieval models from black-box LLMs via an intermediate
    ranker model in a two-stage process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct extensive experiments with cutting-edge LLMs and demonstrate the
    efficacy and efficiency of the proposed method in enhancing information retrieval
    performance compared to other supervision signals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deploy our distilled retriever model within the RAG framework and demonstrate
    its effectiveness in downstream tasks such as open-domain question-answering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide a comprehensive background of information retrieval
    systems and knowledge distillation research related to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab9daf9dc0b1f0758b2634e074cdc6d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The two-stage knowledge distillation process of our proposed Intermediate
    Distillation scheme. In Stage 1, we use re-ranking order $\pi$ (highlighted in
    the green background color) as the supervisory signal to train a ranker model.
    In Stage 2, this distilled ranker unsupervised trains the retriever model to enhance
    its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Retrieval-Augmented Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Information retrieval plays a crucial role in various knowledge-intensive NLP
    tasks, including question-answering Siriwardhana et al. ([2023](#bib.bib30));
    Zhang et al. ([2024](#bib.bib42)), fact-verification Hang et al. ([2024](#bib.bib7));
    Khaliq et al. ([2024](#bib.bib15)) and open-domain dialogue Wang et al. ([2024a](#bib.bib34));
    Shuster et al. ([2021](#bib.bib29)). A prevalent approach in information retrieval
    is the multi-stage retrieval process Nogueira et al. ([2020](#bib.bib23)), which
    first uses a retriever model to search several most relevant documents from the
    large corpus, then employs a ranker model to further optimize the ranking order
    based on relevance, and returns the top few most relevant documents finally.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the retriever models are increasingly used to enhance the generation
    quality of LLMs for knowledge-intensive tasks due to their flexibility and effectiveness,
    leading to the development of the retrieval-augmented generation (RAG) framework
    Guu et al. ([2020](#bib.bib6)); Izacard et al. ([2023](#bib.bib13)). This framework
    integrates information retrieval into the generation process of LLMs, which helps
    overcome the models’ limitations, such as hallucination, by utilizing external
    up-to-date information. In the RAG framework, the retrieved information can be
    in the form of tokens, entities, or text chunks (i.e., documents), and the retrieval
    can occur once or repeatedly every $n$ tokens, for finding a balance between the
    performance and time-cost. Additionally, the retrieval model in RAG is adaptable
    to both encoder-to-decoder Guu et al. ([2020](#bib.bib6)); Izacard et al. ([2023](#bib.bib13))
    and decoder-only language models Borgeaud et al. ([2022](#bib.bib3)); Ram et al.
    ([2023](#bib.bib26)), and is applicable during both the pre-training Zhong et al.
    ([2022](#bib.bib43)); Min et al. ([2022](#bib.bib22)) and inference stages Menick
    et al. ([2022](#bib.bib20)); Min et al. ([2023](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we use the advanced knowledge from LLMs as the supervision signal
    to train the retriever models through a multi-stage (i.e., rerank-then-retrieve)
    training scheme. We then integrate our well-trained retriever model into the RAG
    framework, demonstrating the effectiveness of our proposed training framework
    in question-answering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Knowledge Distillation in LLMs.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knowledge distillation is widely used to transfer knowledge from complex, large
    teacher models to smaller student models Hinton et al. ([2015](#bib.bib9)). Influenced
    by the outstanding performance of LLMs, more and more studies focus on using LLMs
    as teacher models to distill knowledge into smaller task-specific models Brown
    et al. ([2023](#bib.bib4)), and the distillation methods can be categorized into
    two types: white-box Gu et al. ([2023](#bib.bib5)); Agarwal et al. ([2023](#bib.bib2));
    Udagawa et al. ([2023](#bib.bib33)) and black-box Li et al. ([2022](#bib.bib18));
    Ho et al. ([2022](#bib.bib10)); Hsieh et al. ([2023](#bib.bib11)). Specifically,
    white-box training leverages both the predictions and the parameters of LLMs to
    exact knowledge, which can be memory-intensive and computationally demanding.
    In contrast, black-box training only relies on the predictions of LLMs, making
    it less resource-intensive.'
  prefs: []
  type: TYPE_NORMAL
- en: Many studies have successfully integrated knowledge distillation within the
    RAG framework to train the retriever models. For white-box LLM distillation training,
    previous researches employ LLM likelihood, such as attention scores, to assess
    the relevance distribution of retrieved documents Izacard et al. ([2023](#bib.bib13),
    [2022](#bib.bib12)). Meanwhile, some recent studies have also explored methods
    for training RAG using black-box LLMs, like In-Context RALM Ram et al. ([2023](#bib.bib26))
    and RePLUG Shi et al. ([2023](#bib.bib28)). The remaining problem is that these
    methods still rely on the generation log probabilities for the ground truth as
    the supervision signals in distillation training, which tend to have a degree
    of randomness and are limited to the availability of the output probabilities.
    Furthermore, aligning LLM predictions with the goals of retriever training is
    not the optimal choice since there remains a gap between retrieval and generation.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, our proposed distillation method only requires black-box LLMs to
    output a relevance-based ranking order of the candidate relevant documents, yielding
    more consistent, matching, and interpretable results than output probability-based
    methods. Moreover, our method is much more data efficient, requiring about 100x
    and 1000x less data compared to previous approaches Shi et al. ([2023](#bib.bib28));
    Ram et al. ([2023](#bib.bib26)), significantly saving computational resources
    and increasing the flexibility of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our two-stage distillation scheme uses a ranker model and a retriever model
    as the student models and a LLM as the teacher model. As shown in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Related Work ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"), we initially employ an off-the-shelf
    retriever to select a subset of documents $D_{n}$, which is used to train the
    ranker model in the distillation Stage 1. In Stage 2, this ranker enhances the
    original retriever by minimizing the KL-divergence between their similarity likelihood.
    In detail, Section [3.1](#S3.SS1 "3.1 Problem Formulation ‣ 3 Method ‣ Intermediate
    Distillation: Data-Efficient Distillation from Black-Box LLMs for Information
    Retrieval") provides the formal definitions of the related tasks. In Section [3.2](#S3.SS2
    "3.2 Stage 1: Distillation from LLMs to Ranker ‣ 3 Method ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval") and
    Section [3.3](#S3.SS3 "3.3 Stage 2: Distillation from Ranker to Retriever ‣ 3
    Method ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval"), we show how knowledge is directly transferred
    from LLMs to a ranker model and then further conveyed to a retriever model, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a question $Q$.
  prefs: []
  type: TYPE_NORMAL
- en: For the re-ranking task in our distillation framework, the teacher ranker model
    (i.e., LLMs) is tasked with reordering the documents $D_{n}$ is first transferred
    to the ranker model, which serves as an intermediary between the LLM and the retriever
    model. Subsequently, the ranker model conveys this knowledge to the retriever,
    thereby enhancing its performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e020e00b2f6b35a29e2bf50f91bd6cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An example of our LLM teacher model’s re-ranking process.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Stage 1: Distillation from LLMs to Ranker'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The initial step of our knowledge distillation workflow is data initialization,
    where we find the relevant document subsets $D_{n}$. These subsets then serve
    as the input for the Stage 1 training. In practice, we employ a widely-used information
    retrieval model, Contriever Izacard et al. ([2022](#bib.bib12)), as the retriever
    model for data initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Re-ranking by LLMs. In this stage, we utilize LLM’s guaranteed zero-shot ranking
    capabilities to generate high-quality re-ranking orders for each subset $D_{n}$.
    Following this, we use these re-ranking orders to transfer LLM’s knowledge into
    a smaller but more efficient ranker model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranker Distillation Training We initialize our ranker model by using the dual-encoder
    structure Contriever checkpoint. For each question $Q$, which can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $s$ is the temperature hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'After obtaining the ranker model’s similarity likelihood $P_{RANK}(n_{i}|Q)$
    as the ground truth, aiming to minimize the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L(P_{RANK},\pi)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\pi_{P_{RANK}}$.
  prefs: []
  type: TYPE_NORMAL
- en: '3.3 Stage 2: Distillation from Ranker to Retriever'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Stage 2, this well-train ranker model is used to enhance the retriever model’s
    performance by transferring knowledge from LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize our retriever model using a dual-encoder Contriever checkpoint,
    similar to the ranker. For each question and its retrieved document, we also compute
    their representations $\widetilde{Q}$ is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $s$ is the temperature hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then leverage the similarity likelihood $P_{RANK}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{KL}(P_{RANK}&#124;&#124;P_{RETR})$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: This process ensures the retriever model aligns more closely with the text similarity
    knowledge from the LLMs. Through this two-stage distillation scheme, we enhance
    the retrieval accuracy and effectiveness of the retriever model, which can be
    further applied to the RAG framework to improve its performance on knowledge-intensive
    NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Distillation Methods | NQ | TriviaQA |  |'
  prefs: []
  type: TYPE_TB
- en: '| HR@5$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Distillation | 0.478 | 0.583 | 26.09 | 36.75 |  | 0.595 | 0.678 | 54.99
    | 63.55 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised Distillation |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | 0.186 | 0.262 | 18.17 | 27.88 |  | 0.120 | 0.175 | 46.90 | 55.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Rule-Based | 0.223 | 0.303 | 19.75 | 29.64 |  | 0.277 | 0.356 | 50.08 | 58.45
    |'
  prefs: []
  type: TYPE_TB
- en: '| Metric (ROUGE-2) | 0.534 | 0.643 | 27.76 | 38.16 |  | 0.641 | 0.716 | 56.17
    | 64.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Intermediate Distillation (Ours) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 Turbo | 0.505 | 0.606 | 25.84 | 36.13 |  | 0.587 | 0.664 | 53.72
    | 62.16 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4o | 0.553 | 0.652 | 27.01 | 37.38 |  | 0.664 | 0.734 | 56.27 | 64.98
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 Turbo | 0.545 | 0.656 | 28.31 | 38.68 |  | 0.662 | 0.727 | 56.15 |
    65.07 |'
  prefs: []
  type: TYPE_TB
- en: '| Claude3 Opus | 0.562 | 0.665 | 28.45 | 38.83 |  | 0.669 | 0.733 | 56.68 |
    65.36 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The performance comparison of our proposed Intermediate Distillation
    scheme with other baseline supervised distillation methods on question-answering
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dataset We conduct experiments on two benchmark open-domain question-answering
    datasets: NaturalQuestions (NQ) Kwiatkowski et al. ([2019](#bib.bib16)) and TriviaQA
    Joshi et al. ([2017](#bib.bib14)). The NQ dataset includes queries from google.com
    query and their corresponding Wikipedia pages, each with an annotated passage
    containing the answer. We use the dataset version provided by ATLAS Izacard et al.
    ([2023](#bib.bib13)) and follow its training, validation, and testing splits:
    79,168/8,757/3,610. Similarly, we also use the TriviaQA, which contains question-answer
    pairs sourced from Wikipedia and the web, that ATLAS provides and following its
    training, validation, and testing splits: 78,785/8,837/11,313. For the knowledge
    corpus base, we utilize data from Wikipedia as of December 20, 2018, adapting
    the passage embeddings provided by ATLAS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our experiments, we selectively sample 1,000 instances from each training
    set from NQ and TriviaQA, keeping validation and testing sets unchanged. We further
    discuss the impact of selecting different types of training data in Section [5.2](#S5.SS2
    "5.2 Impact of the Training Data Type ‣ 5 Analysis ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval"). This
    training set size is about 100 to 1,000 times smaller than those used in previous
    black-box LLM distillation methods within the RAG framework, demonstrating the
    superior data efficiency of our approach. The impact of training set size on performance
    is discussed further in Section [5.3](#S5.SS3 "5.3 Impact of the Training Set
    Size ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient Distillation from
    Black-Box LLMs for Information Retrieval").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baseline We evaluate our distillation framework, Intermediate Distillation,
    against the following established text similarity methods: ROUGE-2, an evaluation
    metric frequently used in NLP, and BM25, a popular information retrieval algorithm.
    The idea of employing NLP evaluation metrics like ROUGE-2 for knowledge distillation
    in retriever models is first proposed by He et al. ([2022](#bib.bib8)), which
    also uses a multi-step distillation approach to solve the Commonsense Reasoning
    tasks ¹¹1We choose ROUGE-2 as our compared baseline metric, as it outperforms
    other metrics in this prior study.. For both ROUGE-2 and BM25, we use the similarity
    likelihood between the query and its relevant documents via their calculation
    to generate re-ranking order as the supervision signals. Meanwhile, we do not
    consider the previous work RePLUG Shi et al. ([2023](#bib.bib28)) as a baseline
    since it uses a larger data scale and relies on LLMs output probabilities, which
    is not a fair comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we conduct a Rule-Based experiment that ranks documents containing
    the answer at the top in re-ranking order, which aims to demonstrate that effective
    re-ranking distillation signals should not only highlight answers.
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Settings We initialize our ranker and retriever models using the
    Contriever checkpoint Izacard et al. ([2022](#bib.bib12)) with a dual-encoder
    structure. For our proposed distillation scheme, we select several cutting-edge
    and representative LLMs as the teacher models, including GPT-3.5 Turbo, GPT-4o,
    GPT-4 Turbo, and Claude3 Opus. To comprehensively evaluate the performance of
    our retriever model, we integrate the distilled retriever model into the RAG framework
    for question-answering tasks, which allows us to measure the improvement in the
    quality of responses generated by the language model. In the RAG framework, we
    use the reader model Llama-3-8B-Instruct Touvron et al. ([2023](#bib.bib32)) to
    generate answers. We also evaluate a baseline version of this RAG framework without
    additional distillation training for the retriever (i.e., w/o Distillation experiment).
  prefs: []
  type: TYPE_NORMAL
- en: 'Implantation Details We set both the ranker and retriever models with a hidden
    layer size of 768, thus totally have approximately 10 million training parameters
    for each model. The learning rates are set as 5e-5 for the ranker model and 2e-5
    for the retriever model. Both models are trained for 5 epochs on the NQ and TriviaQA
    datasets, using a batch size of 20 and optimized with the Adam optimizer. Additionally,
    we restrict the size of the relevant document subset $D_{n}$ to 5, each retrieved
    document with a maximum length of 128\. We further discuss the impact of the retrieve
    subset (i.e., re-ranking list) size in Section [5.4](#S5.SS4 "5.4 Impact of the
    Re-ranking List Size ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval").'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics We evaluate the retrieval performance of our distilled retriever
    model through the top-5 and top-10 retrieval Hit Rates (HR@5 and HR@10), which
    is the percentage of questions where the relevant document subset $D_{n}$ includes
    at lease one correct answers with the top-5 and top-10 documents. For question-answering
    tasks in the RAG framework, we use the standard Exact Match (EM) metric and F1-Score
    to evaluate the accuracy and precision of the language model generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present our experimental results, including all the baseline methods and
    settings evaluated on the testing set of NQ and TriviaQA in Table [1](#S4.T1 "Table
    1 ‣ 4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from
    Black-Box LLMs for Information Retrieval")²²2The highest values in the table are
    highlighted in bold on both the NQ and TriviaQA datasets.. The experimental results
    show that the retriever model, under our proposed Intermediate Distillation scheme
    and supervised by Claude3, achieves the best performance in most evaluation metrics,
    confirming the effectiveness of our proposed method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the quality of supervision signals from LLMs greatly influences the
    performance of the distilled retriever models. For example, the retriever model
    trained under GPT-4 Turbo supervision outperforms the one supervised by GPT-3.5
    Turbo within our Intermediate Distillation scheme, aligning with GPT-4 Turbo’s
    higher performance across various NLP tasks OpenAI ([2023](#bib.bib24)). As the
    rapid development of LLMs, this improvement in supervision quality has also evolved:
    from being less effective than the ROUGE-2 metric (i.e., as seen the retriever
    under supervised by GPT-3.5 Turbo) to significantly surpass it (i.e., supervised
    by GPT-4 Turbo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the RAG framework for question-answering tasks, a stronger retriever model
    is more likely to enhance the output quality of the reader model, demonstrating
    the effectiveness and adaptability of the Intermediate Distillation framework
    for NLP downstream tasks. However, according to our experimental results, while
    Intermediate Distillation using GPT-4o typically outperforms Supervised Distillation
    using ROUGE-2 in retrieval performance, the latter can still produce higher quality
    generations within RAG. This divergence may be due to the different objectives
    of the retriever and the reader: the retriever focuses on accurately identifying
    the ground truth, whereas the reader wants the retriever to provide information
    that more effectively helps the reader in generating accurate responses. This
    discrepancy remains a topic that can be further explored in future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct ablation studies and a series of quantitative analyses
    to evaluate how various experimental designs and settings affect the outcomes
    of our distillation results.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection, we conduct an experiment named Direct Distillation, where
    we train the retriever model directly using the relevance likelihood generated
    by LLM. More details of experiment setting can be found in Appendix [A](#A1 "Appendix
    A Analysis of Different Distillation Signal Generated by LLMs ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval"). We
    compare the results of this approach with our proposed two-stage distillation
    scheme under the same LLM teacher model (i.e., GPT-4o), and the experimental results
    are shown in Table [2](#S5.T2 "Table 2 ‣ 5.2 Impact of the Training Data Type
    ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval"). The results indicate that the Direct Distillation
    method is less effective than our proposed Intermediate Distillation scheme, which
    further validates the rationality of the two-stage design of our proposed framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Impact of the Training Data Type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Method | Dataset | Evaluation Metrics |  |'
  prefs: []
  type: TYPE_TB
- en: '| EM$\uparrow$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Direct Distillation | NQ | 26.23 | 36.47 | 0.505 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | TriviaQA | 55.39 | 63.96 | 0.623 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Intermediate Distillation | NQ | 27.01 | 37.38 | 0.553 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | TriviaQA | 56.27 | 64.98 | 0.664 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Ablation studies on the effectiveness of two-stage distillation scheme
    design.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb5e7519779d7a8a1edd99225a44b84f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The performance of retriever models across three types of training
    sets, which vary based on the initial appearance and placement of ground truth
    in the retrieved subsets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our previous experiment demonstrate that Rule-Based supervision signals, which
    places documents containing answers at the top, are ineffective and detrimentally
    impacting the retriever’s performance. This indicates that simply re-ranking documents
    based solely on the presence of ground truth (i.e., the correct answer) does not
    provide the high-quality text similarity insights required for effective distillation.
    To delve deeper into the influence of the appearance and placement of ground truth
    in the re-ranking process, we categorize the initial retrieved document subsets
    $D_{n}$ based on NQ’s queries into three categories: (1) Following-Answer: contains
    at least one document with the correct answer, but this kind of document is not
    at the first position in the subset. This data type is used in our experiments
    detailed in Section [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval"). (2) First-Answer:
    contains at least one document with the correct answer, and this kind of documents
    is at the first position in the subset. (3) No-Answer: no documents in the subset
    contain the correct answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We follow the same training setting used in our primary experiments in Section
    [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"), and use GPT-4 Turbo as the LLM
    teacher model. Together with findings from the Rule-Based experiments in Section
    [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"), the experiment results shown
    in Figure [4](#S5.F4 "Figure 4 ‣ 5.2 Impact of the Training Data Type ‣ 5 Analysis
    ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for
    Information Retrieval") indicate that considering the semantic similarity of the
    text is far more important than arranging documents containing the answers to
    the top for re-ranking in distillation training, as even the retriever under the
    No-Answer data set training has notable improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as the Following-Answer training data, where the correct answers are
    not ranked first initially, yields better training results than using the the
    First-Answer training data, indicating that optimizing the ground truth placement
    in re-ranking also has a positive effect on the experimental results after the
    consideration of text similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Impact of the Training Set Size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our previous experiments demonstrate that our distillation framework significantly
    enhances retriever model performance with just 1,000 training instances. In this
    subsection, we explore how different training set sizes affect distillation effectiveness.
    We use training sets of of 50, 100, 200, 500, 1000, and 2000 data instances from
    the Following-Answer data type, with other settings consistent with our experiments
    in Section [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"). In addition, we use GPT-4 Turbo
    as our LLM teacher model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results in Figure [5](#S5.F5 "Figure 5 ‣ 5.3 Impact of the Training Set Size
    ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval") show that the performance of the retriever model
    improves significantly with training data with thousands of or even only hundreds
    of instances. These empirical findings highlight the data efficiency of our proposed
    distillation scheme. In addition, although initial performance increases are notable
    with small training sets, the rate of improvement decreases as more training data
    is used. This pattern indicates a scaling law in distillation training, where
    further enhancements become increasingly difficult as the model’s performance
    improves. For models that already perform well, even marginal improvements require
    much more data, demanding greater training resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddb8f11306e33a85372bb5e10769ba92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The performance of retriever models under different training set
    size.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Impact of the Re-ranking List Size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3edeaeb3aa1581e3e2b236732b03ff3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The performance of retriever models under different size of the re-ranking
    list. The performance corresponding to 0 re-ranking list size represents the baseline
    retriever model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In previous experiments, we set the re-ranking list to five documents (i.e.,
    we retrieve five relevant documents each time). Generally, larger re-ranking lists
    offer more supervision signals from LLMs, thus potentially enhancing the effectiveness
    of distillation training. To explore the impact of re-ranking list size on our
    distillation method, we vary the re-ranking list sizes, using the top-3, top-5,
    top-7, and top-10 documents from each relevant retrieved subset to conduct the
    distillation training. We keep other training settings consistent with those in
    our primary experiments in Section [4](#S4 "4 Experiment ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval") and
    use GPT-4 Turbo as the LLM teacher model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The experimental results shown in Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Impact
    of the Re-ranking List Size ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval") show that increasing
    the re-ranking list size progressively improves the effectiveness of the distillation
    training. As the list expands from re-ranking three documents to ten documents,
    the performance of the distilled retriever model consistently improves. Moreover,
    compared with the retriever model’s baseline performance, setting the size of
    the re-ranking list to 3 still significantly improves the retriever model’s performance
    not only in HitRate@3 but also across broader metrics from HitRate@5 to HitRate@10.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We propose Intermediate Distillation, a two-stage data-efficient knowledge distillation
    scheme that uses the remarkable capabilities of black-box LLMs to train an information
    retrieval model through an intermediate ranker model. We conduct extensive experiments
    with advanced LLMs, demonstrating that our method enhances the effectiveness and
    efficiency of the retriever model performance compared to other supervision signals.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper proposes a data-efficient distillation scheme using black-box LLMs
    to train smaller information retrieval models, which prove its effectiveness with
    training data on the scale of thousands. However, we do not evaluate our proposed
    distillation scheme with larger scales of training data, such as tens of thousands
    or millions of instances, due to budget limitations on accessing responses from
    closed-source LLMs and insufficient computational resources to utilize high-quality
    open-source LLMs like Llama-70B. In the future work, we will focus on extending
    this study to larger-scale training data, using either closed-source or advanced
    open-source LLMs to further analysis the effectiveness of our proposed distillation
    scheme.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adeyemi et al. (2023) Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep,
    and Jimmy Lin. 2023. Zero-shot cross-lingual reranking with large language models
    for low-resource languages. *arXiv preprint arXiv:2312.16159*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. 2023. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*,
    pages 2206–2240\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2023) Nathan Brown, Ashton Williamson, Tahj Anderson, and Logan
    Lawrence. 2023. [Efficient transformer knowledge distillation: A performance review](https://doi.org/10.18653/v1/2023.emnlp-industry.6).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing: Industry Track*, pages 54–65, Singapore. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Minillm:
    Knowledge distillation of large language models. In *The Twelfth International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pages 3929–3938\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hang et al. (2024) Ching Nam Hang, Pei-Duo Yu, and Chee Wei Tan. 2024. Trumorgpt:
    Query optimization and semantic reasoning over networks for automated fact-checking.
    In *2024 58th Annual Conference on Information Sciences and Systems (CISS)*, pages
    1–6\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022) Xingwei He, Yeyun Gong, A Jin, Weizhen Qi, Hang Zhang, Jian
    Jiao, Bartuer Zhou, Biao Cheng, Siu Ming Yiu, Nan Duan, et al. 2022. Metric-guided
    distillation: Distilling knowledge from the metric to ranker and retriever for
    generative commonsense reasoning. *arXiv preprint arXiv:2210.11708*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2022) Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language
    models are reasoning teachers. *arXiv preprint arXiv:2212.10071*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes. *arXiv preprint arXiv:2305.02301*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. [Unsupervised
    dense information retrieval with contrastive learning](https://arxiv.org/abs/2112.09118).
    *Preprint*, arXiv:2112.09118.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Izacard et al. (2023) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language
    models. *Journal of Machine Learning Research*, 24(251):1–43.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
    2017. [TriviaQA: A large scale distantly supervised challenge dataset for reading
    comprehension](https://doi.org/10.18653/v1/P17-1147). In *Proceedings of the 55th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khaliq et al. (2024) M Abdul Khaliq, P Chang, M Ma, Bernhard Pflugfelder, and
    F Miletić. 2024. Ragar, your falsehood radar: Rag-augmented reasoning for political
    fact-checking using multimodal large language models. *arXiv preprint arXiv:2404.12065*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. [Natural
    questions: A benchmark for question answering research](https://doi.org/10.1162/tacl_a_00276).
    *Transactions of the Association for Computational Linguistics*, 7:452–466.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang,
    Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 2022. Explanations
    from large language models make small reasoners better. *arXiv preprint arXiv:2210.06726*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2023) Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.
    2023. Fine-tuning llama for multi-stage text retrieval. *arXiv preprint arXiv:2310.08319*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menick et al. (2022) Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, and Nat McAleese. 2022. [Teaching language models to support
    answers with verified quotes](https://arxiv.org/abs/2203.11147). *Preprint*, arXiv:2203.11147.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau
    Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
    [Factscore: Fine-grained atomic evaluation of factual precision in long form text
    generation](https://arxiv.org/abs/2305.14251). *Preprint*, arXiv:2305.14251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min et al. (2022) Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language
    modeling. *arXiv preprint arXiv:2212.01349*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nogueira et al. (2020) Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.
    Document ranking with a pretrained sequence-to-sequence model. *arXiv preprint
    arXiv:2003.06713*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *Preprint*, arXiv:2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented
    language models. *Transactions of the Association for Computational Linguistics*,
    11:1316–1331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rubin and Berant (2023) Ohad Rubin and Jonathan Berant. 2023. Long-range language
    modeling with self-retrieval. *arXiv preprint arXiv:2306.13421*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation.
    *arXiv preprint arXiv:2104.07567*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siriwardhana et al. (2023) Shamane Siriwardhana, Rivindu Weerasekera, Elliott
    Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving
    the domain adaptation of retrieval augmented generation (rag) models for open
    domain question answering. *Transactions of the Association for Computational
    Linguistics*, 11:1–17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language
    models as re-ranking agent. *arXiv preprint arXiv:2304.09542*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Udagawa et al. (2023) Takuma Udagawa, Aashka Trivedi, Michele Merler, and Bishwaranjan
    Bhattacharjee. 2023. [A comparative analysis of task-agnostic distillation methods
    for compressing transformer language models](https://doi.org/10.18653/v1/2023.emnlp-industry.3).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing: Industry Track*, pages 20–31, Singapore. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024a) Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong
    Wang, Yufei Wang, Fei Mi, Jeff Z Pan, and Kam-Fai Wong. 2024a. Unims-rag: A unified
    multi-source retrieval-augmented generation for personalized dialogue systems.
    *arXiv preprint arXiv:2401.13256*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2024b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024b. A survey
    on large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6):1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2008) Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang
    Li. 2008. Listwise approach to learning to rank: theory and algorithm. In *Proceedings
    of the 25th international conference on Machine learning*, pages 1192–1199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2024) Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May D
    Wang, Joyce C Ho, Chao Zhang, and Carl Yang. 2024. Bmretriever: Tuning large language
    models as better biomedical text retrievers. *arXiv preprint arXiv:2404.18443*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023a) Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a. [Extractive
    summarization via chatgpt for faithful summary generation](https://arxiv.org/abs/2304.04193).
    *Preprint*, arXiv:2304.04193.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023b) Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023b. [Summit:
    Iterative text summarization via chatgpt](https://arxiv.org/abs/2305.14835). *Preprint*,
    arXiv:2305.14835.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024) Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen,
    Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. 2024. Raft: Adapting language
    model to domain specific rag. *arXiv preprint arXiv:2403.10131*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong et al. (2022) Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language
    models with memory augmentation. *arXiv preprint arXiv:2205.12674*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Analysis of Different Distillation Signal Generated by LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we provide more details about the prompt design of the Direct Distillation
    experiment. We also analyze the corresponding generation quality of LLMs, including
    the stability and interpretability, and compare it with the re-ranking generation.
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Direct Distillation Prompt Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We follow the re-ranking prompt format design and replace the re-ranking task
    with quantifying the similarity scores of the retrieved documents for LLM’s supervised
    signals generation. An example input prompt and the generated responses without
    explanations is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Generated Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A.2 Comparison with Re-ranking Response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare the responses generated from re-ranking prompts and those derived
    from similarity score prompts used in the Direct Distillation experiment. Specifically,
    we randomly select 1,000 data instances from the NQ dataset, using the queries
    and their retrieved documents to prompt the LLM to generate both list-wise re-ranking
    orders and similarity scores. Examples of responses generated by the LLM, specifically
    using GPT-4 Turbo, are shown in Table [3](#A1.T3 "Table 3 ‣ A.3 Implantation Details
    of Direct Distillation Experiment ‣ Appendix A Analysis of Different Distillation
    Signal Generated by LLMs ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/660825afab7decaac0a6841dd2cff95d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Spearman Correlation between the responses from the re-ranking prompt
    and the responses from the similarity score prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [3](#A1.T3 "Table 3 ‣ A.3 Implantation Details of Direct Distillation
    Experiment ‣ Appendix A Analysis of Different Distillation Signal Generated by
    LLMs ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs
    for Information Retrieval") shows that responses generated from re-ranking prompts
    are more interpretable than those from similarity score prompts, which often use
    scaled values that are ambiguous. Additionally, responses from the similarity
    score-based prompt frequently yield extreme values, such as 0.0 (completely dissimilar)
    or 1.0 (highly similar), in some cases, which means that that supervision signals
    based on similarity scores are less informative and act more like binary signals
    in certain data instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we use the Spearman Correlation to assess the consistency between
    responses generated from re-ranking prompts and those from similarity score prompts,
    and the analysis result is visualized in Figure [7](#A1.F7 "Figure 7 ‣ A.2 Comparison
    with Re-ranking Response ‣ Appendix A Analysis of Different Distillation Signal
    Generated by LLMs ‣ Intermediate Distillation: Data-Efficient Distillation from
    Black-Box LLMs for Information Retrieval"). A higher Spearman correlation value
    suggests a stronger positive correlation between the two types of responses. From
    Figure [7](#A1.F7 "Figure 7 ‣ A.2 Comparison with Re-ranking Response ‣ Appendix
    A Analysis of Different Distillation Signal Generated by LLMs ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval"), we
    can see that many response pairs are closely related, indicating the stability
    and reliability of LLMs in generating responses for similar tasks. In addition,
    based on our previous analysis, we can see that the responses from re-ranking
    prompts are not only reliable but also possess a higher information density compared
    to those from similarity score prompts, showing that responses from re-ranking
    prompts have higher-quality supervision capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Implantation Details of Direct Distillation Experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the same training settings as those used in Intermediate Distillation
    experiments. Superficially, we set the retriever model with a hidden layer size
    to 768 and initialize it using a dual-encoder Contriever checkpoint. The model
    is trained over five epochs using the same dataset as described in Section [4](#S4
    "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval"), with a learning rate of 5e-5, a batch size of
    20, and optimization via the Adam optimizer. In addition, we also limit the size
    of the relevant document subset $D_{n}$ to five, and each retrieved document with
    a maximum length of 128.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question & Retrieved Documents | Type | LLM’s Generation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [Question] membership in the european union requires countries to have which
    type of government [Document1] the european continent. the eu’s example is being
    followed by the african union, the union of south American nations… [Document2]
    powers of states, the considerable level of translation and the choice of a lesser
    city as the capital… [Document3] principal characteristic of classical democracy.
    the concept of a mixed government was studied during the renaissance and the age
    of reason by niccolo machiavelli, giambattista vico… [Document4] thus essentially
    preventing it from participating in government. the european union ( eu ) is an
    association of twenty - eight sovereign member states… [Document5] civiletti opinions,
    not all funding gaps led to shutdowns. of the nine funding gaps between 1980 and
    1990, only four led to furloughs… | Re-ranking | [Re-ranking order] Document5,
    Document3, Document2, Document1, Document4. [Explanation] Document5 directly discusses
    the governmental systems used by most European nations, particularly the parliamentary
    system, which is relevant since the EU generally comprises nations with parliamentary
    democracies. This makes it the most relevant to the query about the type of government
    required for EU membership. Document3 talks about mixed constitutions… |'
  prefs: []
  type: TYPE_TB
- en: '| Similarity Score | [Similarity scores from Document1 to Document5] Doc1:0.0,
    Doc2:0.1, Doc3:0.3, Doc4:0.1, Doc5:0.7. [Explanation] Document1 talks about the
    EU and various other international organizations, focusing on aspects like the
    single market and policy integration but does not mention government types required
    for EU membership. Hence, the score is 0.0\. Document2 discusses the structure
    of EU governance… |'
  prefs: []
  type: TYPE_TB
- en: '| [Question] when was the immigration reform and control act passed [Document1]
    united states temporarily for money or a better life. an increase in legal immigrants
    migrating to the united states also led to an increase of illegal aliens… [Document2]
    there are noted communists, socialists, and anti - american sympathizers. parts
    of the act remain in place today… [Document3] …the immigration reform and control
    act ( irca ), also known as the simpson 2013 mazzoli act or the reagan amnesty,
    signed into law by ronald reagan on november 6, 1986… [Document4] five percent
    of the u. s. workforce. it was also estimated that about 70 percent of those illegal
    workers… [Document5] to reduce them as restrictionists. "fry traced" restrictions
    roots " to a surprising surge in illegal and legal immigration… | Re-ranking |
    [Re-ranking order] Document3, Document1, Document4, Document5, Document2. [Explanation]
    Document3 directly answers the query by explicitly mentioning the "Immigration
    Reform and Control Act of 1986" and stating it was signed into law by Ronald Reagan
    on November 6, 1986\. It provides the most specific information related to the
    query… |'
  prefs: []
  type: TYPE_TB
- en: '| Similarity Score | [Similarity scores from Document1 to Document5] Doc1:0.0,
    Doc2:0.0, Doc3:1.0, Doc4:0.0, Doc5:0.0. [Explanation] Document1 discusses general
    immigration trends and issues but does not provide the specific date of the passage
    of the Immigration Reform and Control Act, which is directly asked in the query.
    Therefore, it receives a score of 0.0\. Although Document2 mentions other immigration-related
    legislation and acts, it does not specify… |'
  prefs: []
  type: TYPE_TB
- en: '| [Question] who sings in there’s something about mary [Document1] melody.
    mary described the completed song as "whitney - fied". meron maintains that… [Document2]
    …he film concludes with the two engaging in a kiss while singer / songwriter jonathan
    richman, who has periodically narrated the story in song throughout the film…
    [Document3] day but her. minutes later bing notices his tackle box has been disturbed,
    and an instant later the kids see the door to the tent is open… [Document4] true
    opinions of other contestants and her stance on the judges. the book achieves
    in combining fifty years of a’salt of the earth’existence with a new - found life
    of fame… [Document5] and breezy number and the narrator opens up the story about
    bing and his family. and while bing is cleaning the fish, he tells the story of
    " goldilocks"… | Re-ranking | [Re-ranking order] Document2, Document5, Document4,
    Document1, Document3. [Explanation] Document2 is the most relevant sentence as
    it directly answers the query. It mentions that Jonathan Richman narrated parts
    of the story through song throughout the film "There’s Something About Mary,"
    which clearly identifies the singer related to your query… |'
  prefs: []
  type: TYPE_TB
- en: '| Similarity Score | [Similarity scores from Document1 to Document5] Doc1:0.0,
    Doc2:1.0, Doc3:0.0, Doc4:0.0, Doc5:0.0. [Explanation] Document1 discusses a song
    in a completely different context, related to "Cinderella" and Whitney Houston’s
    music adjustments, not related to the film "There’s Something About Mary." Thus,
    it doesn’t answer the query about who sings in the specified film, receives a
    score of 0.0\. Document2 directly answers the query. It mentions that Jonathan
    Richman narrated parts of the story… |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Examples of two different LLM-generated responses. We also let LLM
    generate the corresponding explanations in these examples. The red color indicates
    the corresponding answer to the question.'
  prefs: []
  type: TYPE_NORMAL
