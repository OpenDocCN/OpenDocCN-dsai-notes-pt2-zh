- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Distilling LLMs’ Decomposition Abilities into Compact Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01812](https://ar5iv.labs.arxiv.org/html/2402.01812)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Denis Tarasov & Kumar Shridhar
  prefs: []
  type: TYPE_NORMAL
- en: ETH Zurich
  prefs: []
  type: TYPE_NORMAL
- en: '{tarasovd,shridhar.kumar}@ethz.ch'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have demonstrated proficiency in their reasoning
    abilities, yet their large size presents scalability challenges and limits any
    further customization. In contrast, compact models offer customized training but
    often fall short in solving complex reasoning tasks. This study focuses on distilling
    the LLMs’ decomposition skills into compact models using offline reinforcement
    learning. We leverage the advancements in the LLM‘s capabilities to provide feedback
    and generate a specialized task-specific dataset for training compact models.
    The development of an AI-generated dataset and the establishment of baselines
    constitute the primary contributions of our work, underscoring the potential of
    compact models in replicating complex problem-solving skills¹¹1Our code and dataset
    are available at [https://github.com/DT6A/GSM8K-AI-SubQ](https://github.com/DT6A/GSM8K-AI-SubQ).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent strides in Natural Language Processing (NLP) have brought forth powerful
    Large Language Models (LLMs) like GPT-4 (OpenAI, [2023](#bib.bib31)), Claude 2²²2[https://www.anthropic.com/index/claude-2](https://www.anthropic.com/index/claude-2),
    or Gemini (Team et al., [2023](#bib.bib43)). These models not only excel at straightforward
    tasks such as summarization and sentiment analysis but, with adept prompting,
    demonstrate proficiency in handling reasoning tasks that demand mathematical and
    logical abilities (Huang & Chang, [2022](#bib.bib17)). Notably, Chain-of-Thoughts
    (CoT) prompting (Wei et al., [2022](#bib.bib49)) and its variations (Kojima et al.,
    [2022](#bib.bib20); Wang et al., [2022](#bib.bib48)) have proven to be promising
    and relatively simple techniques for enhancing LLMs’ reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Within the realm of complex reasoning, the ability to decompose intricate questions
    into a set of simpler sub-questions represents a crucial and understudied component
    (Shridhar et al., [2022](#bib.bib36)). While existing works predominantly focus
    on end-to-end solutions for reasoning (Zhou et al., [2022](#bib.bib52); Lyu et al.,
    [2023](#bib.bib28)), the specific aspect of breaking down complex questions into
    simpler components has received limited attention. The creation of specialized
    datasets and benchmarks is integral to advancing the field of Deep Learning (Guss
    et al., [2019](#bib.bib13); Vinyals et al., [2019](#bib.bib47); Fu et al., [2020](#bib.bib9);
    Kurenkov et al., [2023](#bib.bib24)). This work addresses the gap in understanding
    and exploration of the reasoning sub-questioning process by providing a dataset
    and baselines for further research in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: Compounding the challenge is the computational overhead associated with large
    model sizes, making reasoning tasks computationally expensive and time-consuming
    when tuning models. Concurrently, approaches similar to Chain-of-Thoughts (CoT)
    may incur expenses, given that models with superior reasoning abilities are not
    available for free. In response, distilling distinct components of the reasoning
    process into smaller models emerges as a promising avenue for research. Decomposition,
    particularly in the context of teaching smaller models, proves advantageous due
    to their cost-effectiveness, reduced computational requirements, and accessibility.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) has demonstrated remarkable success across various
    domains with recent success in the NLP domain Bai et al. ([2022a](#bib.bib3);
    [b](#bib.bib4)); OpenAI ([2023](#bib.bib31)); Team et al. ([2023](#bib.bib43)).
    However, some of the most popular approaches like Reinforcement Learning with
    Human Feedback (RLHF) (Ouyang et al., [2022](#bib.bib32)) demand substantial data,
    and online approaches require extensive interactions with the environment. Offline
    RL (Levine et al., [2020](#bib.bib26)), an alternative that utilizes data directly,
    holds potential with limited dataset sizes. This subfield has recently witnessed
    a surge in development, leading to a growth of diverse approaches (Kumar et al.,
    [2020](#bib.bib22); Fujimoto & Gu, [2021](#bib.bib10); Kostrikov et al., [2021](#bib.bib21);
    An et al., [2021](#bib.bib2); Akimov et al., [2022](#bib.bib1); Yang et al., [2022](#bib.bib50);
    Ghasemipour et al., [2022](#bib.bib11); Nikulin et al., [2023](#bib.bib30)). Numerous
    successful applications of offline RL exist in fields like robotics (Smith et al.,
    [2022](#bib.bib38); Kumar et al., [2021](#bib.bib23)), autonomous driving (Diehl
    et al., [2021](#bib.bib8)), recommendation systems(Chen et al., [2022](#bib.bib6))
    and even drug-design (Tarasov et al., [2023b](#bib.bib42)). For the NLP domain,
    recent studies have intriguingly revealed that AI feedback closely resembles human
    feedback (Lee et al., [2023](#bib.bib25)), and language models can be fine-tuned
    using their own generated feedback (Bai et al., [2022b](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this preliminary work, we combine the advantages of sub-questioning with
    offline RL with feedback for the task of mathematical reasoning and distill the
    subquestion decomposition abilities in smaller models. Our work proposes the following:
    1) an AI-generated benchmark where math questions are broken down into simpler
    sub-questions based on the GSM8K (Cobbe et al., [2021](#bib.bib7)) dataset, 2)
    train smaller language models for the same task using fine-tuning and offline
    RL techniques to provide baselines for the task, and 3) explore the potential
    benefits of using AI-generated feedback on its own responses in enhancing model
    performance. Our experimental results reveal a big gap between ChatGPT’s reasoning
    abilities and what can be achieved with smaller models and existing algorithmical
    approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LM Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distillation has emerged as a pivotal technique in mitigating the computational
    challenges associated with LLMs while retaining their valuable knowledge. Notably,
    Hinton et al. ([2015](#bib.bib15)) introduced knowledge distillation as a means
    to transfer the knowledge from a complex model to a simpler one, enabling the
    latter to approximate the former’s performance. In the context of language models,
    Sanh et al. ([2019](#bib.bib35)) successfully distilled BERT, a prominent LLM,
    into a more compact version named DistilBERT, maintaining competitive performance
    with significantly fewer parameters. Recently, a lot of work has successfully
    distilled the reasoning capabilities into smaller models Yuan et al. ([2023](#bib.bib51));
    Magister et al. ([2023](#bib.bib29)); Shridhar et al. ([2023](#bib.bib37)); Hsieh
    et al. ([2023](#bib.bib16))
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLMs Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The exploration of reasoning capabilities in Large Language Models (LLMs) has
    been a focal point in recent NLP research. Wei et al. ([2022](#bib.bib49)) introduced
    Chain-of-Thoughts (CoT), a method compelling LLMs to approach problems in a step-by-step
    manner rather than providing direct answers. Noteworthy is their revelation that
    reasoning abilities manifest prominently in larger LM sizes. This technique involves
    prompting the model with step-by-step problem-solving and furnishing multiple
    examples for guidance through few-shot prompting. Building on this, Kojima et al.
    ([2022](#bib.bib20)) demonstrated that reasoning abilities may emerge even without
    examples, utilizing zero-shot prompting. Further refinement in reasoning quality
    was achieved by Wang et al. ([2022](#bib.bib48)), who showed that applying CoT
    multiple times and picking the most frequent answer boosts resulting performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Sub-Questioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The paradigm of splitting reasoning into sub-question generation and sub-question
    answering has proven beneficial in solving reasoning problems (Shridhar et al.,
    [2022](#bib.bib36)). Notably, Shridhar et al. ([2023](#bib.bib37)) took a step
    further by distilling LLMs reasoning abilities into substantially smaller models
    through finetuning, laying the groundwork for our current work. It is essential
    to note that both of these works primarily address the entire reasoning process
    rather than dissecting its individual components. In a parallel thread of research,
    Zhou et al. ([2022](#bib.bib52)) demonstrated that decomposing reasoning problems
    into sub-problems results in improved LLM performance. Their findings indicate
    that LLMs can execute this split step-by-step, outperforming CoT-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Extending beyond mere decomposition, Juneja et al. ([2023](#bib.bib19)) introduced
    a finetuning approach where a “helper” LM poses questions to a “solver” LM, guiding
    it toward the correct solution based on the problem and the sequence of prior
    interactions. Despite showcasing the efficacy of this approach, its reliance on
    an online RL paradigm and the use of large 13 or 33 billion parameters LLaMA models
    (Touvron et al., [2023](#bib.bib44)) poses practical challenges. The inherent
    computational demands and potential instability of online RL, compounded by the
    necessity for significant interactions and potential cost implications, underscore
    the complexities associated with this method.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Language Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our study leverages the preeminence of attention-based Transformer architectures,
    as introduced by Vaswani et al. ([2017](#bib.bib46)). In the realm of autoregressive
    models, we turn our attention to the widely adopted GPT-2 architecture (Radford
    et al., [2019](#bib.bib34)), specifically selecting models of various sizes to
    tailor our experiments. The chosen finetuning strategies center around the application
    of standard cross-entropy loss, optimizing the models for accurate next-token
    prediction. For every approach we provide mathematical problem as a prefix and
    expect model to generate the sub-questions required to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Offline RL task formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the recent advancements in offline RL, we incorporate a baseline from
    this field. Building upon the formulation proposed by Snell et al. ([2022](#bib.bib39)),
    we cast the text generation problem as a token-level Partially Observable Markov
    Decision Process (POMDP). In this framework, the agent’s observations correspond
    to prefixes of tokens, and the agent’s action pertains to the selection of the
    next token to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: 4 GSM8K-AI-SubQ Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the pursuit of advancing research in reasoning within Language Models (LMs),
    we introduce the GSM8K-AI-SubQ dataset, uniquely designed to emphasize sub-questioning
    and leverage AI-generated feedback for these sub-questions. Our inspiration comes
    from the works of Bai et al. ([2022b](#bib.bib4)) and Lee et al. ([2023](#bib.bib25)).
    The first work provide compelling evidence that such feedback can serve as a valuable
    signal for fine-tuning models. The second work shows that AI-generated data exhibits
    comparable quality to human-annotated data in the context of preferences. We hope
    that our dataset will provide any insights whether similar properties hold for
    the reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: While acknowledging that AI models may not offer the ideal source of reasoning
    data, we posit the substantial benefits of such a dataset. Firstly, it serves
    as a valuable resource for distilling reasoning abilities into smaller models,
    aligning with the growing trend of efficiency in language model architectures.
    Importantly, even if the data exhibits suboptimal characteristics, we draw inspiration
    from the success of offline RL in other fields, where it has demonstrated an ability
    to outperform the policy responsible for dataset collection. This resilience to
    suboptimal data quality mitigates concerns and underscores the dataset’s potential
    impact on advancing reasoning capabilities. Lastly, the expedited and cost-effective
    nature of this data acquisition method democratizes its accessibility, allowing
    researchers to extend our dataset if required.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of completeness, we furnish LLM responses for the generated sub-questions,
    although our primary focus in this work centers on the sub-questioning. This dataset
    not only facilitates advancements in LM reasoning but also lays the groundwork
    for innovative approaches in the broader landscape of language model research.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dataset Collection Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our dataset collection process, we leverage the capabilities of ChatGPT,
    specifically utilizing the gpt-3.5-turbo-0613³³3The most recent gpt-3.5-turbo
    version version. This version is chosen due to its lenient restrictions on the
    number of queries and its cost-effectiveness, offering responses of commendable
    quality when benchmarked against similar LLMs (Tunstall et al., [2023](#bib.bib45)).
    Interactions with ChatGPT are facilitated through the OpenAI API⁴⁴4[https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview),
    employing a temperature setting of 0.7 to ensure a diverse range of responses.
  prefs: []
  type: TYPE_NORMAL
- en: For the specific task at hand, we curate a dataset of mathematical problems
    extracted from the GSM8K dataset (Cobbe et al., [2021](#bib.bib7)). This dataset
    comprises a diverse array of grade school math word problems meticulously crafted
    by human problem writers. While designed to be solvable by a middle school student,
    these problems present a noteworthy challenge for LMs. Maintaining the integrity
    of the original train/test split, our dataset consists of 7473 training examples
    and 1319 testing examples.
  prefs: []
  type: TYPE_NORMAL
- en: Sub-questions Generation. The initial phase of our data generation involves
    creation of set of sub-questions for each problem within the GSM8K dataset. We
    prompt ChatGPT to decompose the given problem into distinct sub-problems and provide
    corresponding sub-questions, aiming to elucidate the problem-solving process for
    others. To guide the model and ensure a consistent output format, we furnish two
    examples of problems and their corresponding sub-questions as demonstration inputs.
    Each request is treated as an independent dialogue to eliminate potential interference.
    We keep a 2-shot prompting strategy and preserving independence for all subsequent
    queries. A sample input and output for this sub-question generation process are
    illustrated in [Table 10](#A3.T10 "Table 10 ‣ Appendix C ChatGPT Prompts ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models"). To enhance the dataset’s
    size and diversity, we repeat the inquiry for each problem three times, resulting
    in 22,419 training samples. While the repetition could be further increased for
    creating a larger dataset, we acknowledge budget constraints as a limiting factor
    in this study.
  prefs: []
  type: TYPE_NORMAL
- en: Answers Generation. In the subsequent phase of our data collection, we focused
    on generating responses to the previously obtained sub-questions. Tasking ChatGPT
    solely with the responsibility of providing answers to these queries and obtaining
    a final answer for the original problem. An illustrative example of this prompt-response
    interaction is presented in [Table 11](#A3.T11 "Table 11 ‣ Appendix C ChatGPT
    Prompts ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models").
    The generated final answers were then employed as labels to categorize each set
    of sub-questions. Sub-questions associated with an original problem solution were
    identified as ”good”, while those failing to lead to a resolution were categorized
    as ”bad”.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback Generation. Concluding our dataset collection, we engage in the pivotal
    task of soliciting feedback from ChatGPT on its generated sub-questions. The objective
    is to introduce a nuanced signal at the individual question level, mitigating
    sparsity concerns associated with relying solely on the correctness of final answers.
    This aspect assumes significance, especially in the context of potential RL applications.
    Drawing inspiration from the effective sub-questioning strategy proposed by Shridhar
    et al. ([2022](#bib.bib36)), we task ChatGPT with determining the usefulness of
    each sub-question in the problem-solving process. Refer to [Table 12](#A3.T12
    "Table 12 ‣ Appendix C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition Abilities
    into Compact Language Models") for an exemplary interaction. To account for potential
    inconsistencies in ChatGPT’s feedback, we query the model three times for each
    set of sub-questions. This repetition strategy aligns with findings in recent
    studies (Wang et al., [2022](#bib.bib48); Lee et al., [2023](#bib.bib25)), showing
    that leveraging multiple responses often results in more useful data. It’s worth
    noting that the repetition factor can be adjusted for further data refinement.
    Subsequently, scores for each sub-question are computed by evaluating the fraction
    of responses deeming it useful, establishing a metric for assessing the efficacy
    of individual sub-questions.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Collection Costs. The compilation of our training set demanded approximately
    12 hours of real-time investment and incurred a cost of approximately $100 for
    utilizing the gpt-3.5-turbo version. Notably, feedback generation constituted
    around 70% of the total costs, primarily attributed to the necessity of acquiring
    feedback multiple times for each set of questions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Dataset Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Metric | 0 correct | 1 correct | 2 correct | 3 correct |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Number of problems | 1343 (18%) | 866 (11%) | 1139 (15%) | 4052 (54%) |'
  prefs: []
  type: TYPE_TB
- en: '| Mean problem length | 269.4 $\pm$ 82.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Median problem length | 250 | 235.5 | 225 | 201.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Train set statistics for problems splitted by the number of sub-question
    sets which lead to the correct answer. ”Problem length” denotes number of characters.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we present an analysis of the collected training data, offering
    valuable statistics and insights.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of sub-question set sizes is visualized in [1(a)](#S4.F1.sf1
    "1(a) ‣ Figure 1 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models"). While the majority
    of sets comprise 2 to 6 questions, some outliers exist, featuring either a single
    question or more than 6\. Notably, we opt to retain these outliers in our dataset
    for a more diverse representation.
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis extends to evaluating the efficacy of sub-question sets in leading
    to the correct solution for each problem. As outlined in [Table 1](#S4.T1 "Table
    1 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition
    Abilities into Compact Language Models"), approximately 54% of the sets consistently
    resulted in a correct solution across all three attempts, while only around 18%
    failed to yield a correct solution in any instance. Moreover, longer problems
    appear to pose increased difficulty, aligning intuitively with expectations. Calculating
    average accuracy by treating each sub-question set as an independent problem,
    the overall accuracy for the training set stands at 0.68.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c758b7d946ef58bd2c305b46dcd984a4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/062e41e490fcb8432814009497075323.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1773e6f9cedc6920e5daa6f54a781528.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c960f7a2df4d6c31d714617619b518ea.png)'
  prefs: []
  type: TYPE_IMG
- en: (d)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Number of questions distributions in train set. (a) Distribution
    in the entire train set with mean 4.0 $\pm$ 1.5 and median 4, (b, c, d) Comparisons
    of distributions in number of questions between problems with 3 vs. 0, 1, 2 sets
    of sub-questions that lead to the correct final answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To delve deeper into the relationship between problem complexity and sub-question
    count, we compare distributions for 0, 1, and 2 out of 3 correct answers versus
    3 out of 3 correct answers in [Figure 1](#S4.F1 "Figure 1 ‣ 4.2 Dataset Analysis
    ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models") b, c, d. The analysis reveals a discernible trend: easier problems
    tend to be associated with a smaller number of sub-questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing our exploration, we investigate whether ChatGPT’s feedback on its
    own sub-questions holds intrinsic value and diverges from random noise.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of usefulness scores for each sub-question, depicted in [2(a)](#S4.F2.sf1
    "2(a) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models"), indicates that approximately
    90% of sub-questions received consistent markings as useful across all three iterations.
    However, to gain a more nuanced perspective, we average the usefulness scores
    for each sub-question set and showcase the distribution of averaged confidences
    in [2(b)](#S4.F2.sf2 "2(b) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ
    Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c48c25581bd25e3fc34c8bdf34fdc53.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/997364d3e81b704beae8afbd5b79e94b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e9fad13bf44db1884ab942d31158fd9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: (a) distribution of the usefulness feedback over individual sub-questions,
    (b) distribution of the usefulness feedback averaged over sets of sub-questions,
    (c) confusion matrix between the presence of negative feedback in the set of sub-questions
    and correctness of the final answer based on corresponding sub-questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Upon examining the fraction of problems with at least one negative response,
    we find that 36% of problems fall into this category, aligning with the 54% that
    were correctly solved 3 out of 3 times. Recognizing that a single misstep in the
    reasoning process can lead to an incorrect answer, we employ the presence of negative
    feedback as an indicator for predicting an incorrect solution. However, this indicator
    comes with limitations, such as the potential for false negatives arising from
    the model providing the correct final answer despite a flawed reasoning step,
    and the occasional presence of randomness during feedback collection. The confusion
    matrix in [2(c)](#S4.F2.sf3 "2(c) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ
    Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models")
    outlines this setup, showcasing differences from a random scenario. Notably, when
    negative feedback is absent, 72% of problems were actually solved correctly, compared
    to 62% when negative feedback is present.
  prefs: []
  type: TYPE_NORMAL
- en: To quantitatively assess the performance of this heuristic, we compute the ROC
    AUC score, treating the average sub-question set usefulness as a probability and
    designating an incorrect answer as the ”target label”. The resulting ROC AUC score
    of 0.56 indicates a departure from randomness (0.5). Further, the Pearson correlation
    coefficient, calculated at -0.09 with a p-value of $10^{-46}$, suggests that a
    higher usefulness score corresponds to a decreased likelihood of encountering
    an incorrect answer. Despite the simplicity of this heuristic, it provides informative
    cues on the efficacy of the sub-question set, indicating its potential utility.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Baselines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our exploration of the constructed dataset, we conduct a series of experiments
    utilizing both supervised techniques and an offline RL approach. Our experiments
    involve the deployment of pretrained small and medium versions of GPT-2 (Radford
    et al., [2019](#bib.bib34)) as well as pretrained DistilGPT (Sanh et al., [2019](#bib.bib35))
    to gauge performance dynamics across various models sizes. The GPT-2 medium model,
    with 345 million parameters, represents the largest model in our experiments.
    This choice is deliberate, as larger models endowed with reasoning abilities often
    exceed 7 billion parameters, rendering their usage complex for many researchers.
    Unfortunately, due to computational resource limitations, our experiments did
    not extend to GPT-2 large and XL models.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Evaluation Protocol
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To facilitate a robust comparison of different approaches, we employ the same
    version of ChatGPT utilized during data collection. Sub-questions generated by
    each approach are presented to ChatGPT using the same prompt from [Table 11](#A3.T11
    "Table 11 ‣ Appendix C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition Abilities
    into Compact Language Models"), and we measure the accuracy of final answers as
    our evaluation metric. Standardizing the evaluation environment, we set the temperature
    to 0 and fix the random seed to ensure reproducibility and eliminate randomness
    during the evaluation process. This rigorous evaluation protocol ensures a fair
    and consistent assessment of the performance across various approaches. One evaluation
    of test set costed approximately 1.5$ with gpt-3.5-turbo.
  prefs: []
  type: TYPE_NORMAL
- en: We additionally use the same protocol to evaluate results using open-source
    models Mistral 7B (Jiang et al., [2023](#bib.bib18)), LLaMA 7B and LLaMA 13B (Touvron
    et al., [2023](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Applied Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we provide a brief overview of each applied approach. For
    a more details, refer to [Appendix A](#A1 "Appendix A Experimental Details ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models").
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral Cloning. Behavioral Cloning (BC) is a fundamental and robust approach
    commonly applied to datasets involving decision-making tasks. In the context of
    NLP, it translates to fine-tuning a language model to replicate a specific behavior
    or policy observed in the dataset. To select the best model, we employ a small
    held-out fraction (1%) of the training data. Following the methodology inspired
    by Shridhar et al. ([2022](#bib.bib36)), we use the BLEU score (Papineni et al.,
    [2002](#bib.bib33)) calculated between the generated questions and those produced
    by ChatGPT. Given that the primary goal of BC is to replicate the original policy’s
    behavior, BLEU serves as a suitable metric, indicating the similarity between
    two texts. Our empirical observation show that BLEU correlates with the final
    performance, making it a reasonable choice for model evaluation in the context
    of BC. For all subsequent approaches, the best BC model serves as the initialization
    for the LM.
  prefs: []
  type: TYPE_NORMAL
- en: Filtered Behavioral Cloning. Filtered BC (Chen et al., [2021](#bib.bib5)) introduces
    a modification of BC by considering only a fraction of the best trajectories in
    the dataset. This approach proves particularly effective when a substantial number
    of high-quality examples are at disposal. In the context of our task, we exclusively
    retain samples corresponding to sub-question sets that result in the correct solution.
    The model selection process remains consistent with the standard BC approach.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit Language Q-Learning. Implicit Language Q-Learning (ILQL) (Snell et al.,
    [2022](#bib.bib39)) represents an adaptation of the offline RL approach known
    as IQL (Kostrikov et al., [2021](#bib.bib21)) to NLP tasks. The core idea behind
    ILQL involves training additional Value (V) and Q-function heads with IQL objectives.
    These additional functions are then employed to reweight the original LM outputs
    using the advantage value, which is the difference between V and Q values.
  prefs: []
  type: TYPE_NORMAL
- en: The selection of ILQL is motivated by the effectiveness of IQL as one of the
    strongest offline RL approaches in diverse domains (Tarasov et al., [2022](#bib.bib40)).
    Given the limited adaptation of offline RL approaches to NLP problems, ILQL emerges
    as the state-of-the-art choice. Given that IQL optimizes for rewards, which may
    not inherently correlate with the dataset policy, selecting the best model becomes
    challenging. In the absence of a clear best model selection criterion, we have
    tried to use the same criterion as we did for BC and the common offline RL practice
    of taking the latest checkpoint after training. The first method produced slightly
    better results on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two versions of ILQL are tested in our experiments: ILQL-full utilizing all
    available rewards from the dataset and ILQL-sparse employing only answer correctness
    as rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The experimental results, summarized in [Table 2](#S5.T2 "Table 2 ‣ 5.3 Experimental
    Results ‣ 5 Baselines ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models"), offer insights into the performance of the proposed approaches
    averaged over various answering LLMs. For a detailed breakdown based on specific
    answering models, refer to [Appendix B](#A2 "Appendix B Full Tabular Scores ‣
    Distilling LLMs’ Decomposition Abilities into Compact Language Models").
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  prefs: []
  type: TYPE_TB
- en: '| BC | 0.255 | 0.284 | 0.310 | 0.283 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtered BC | 0.260 | 0.293 | 0.319 | 0.291 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-sparse | 0.249 | 0.281 | 0.310 | 0.280 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-full | 0.253 | 0.277 | 0.309 | 0.280 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | N/A | N/A | N/A | 0.429 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Accuracy score of the final answer avereged over different models
    which were used for sub-questions answering. Best scores are highlighted with
    bold.'
  prefs: []
  type: TYPE_NORMAL
- en: It is evident that all tested approaches fall short when compared to the ChatGPT,
    indicating substantial room for improvement. Sub-questions generation abilities
    also improve with the size of the backbone model which alligns with previous researches.
  prefs: []
  type: TYPE_NORMAL
- en: Filtered BC demonstrates improved performance over standard BC in most scenarios,
    consistent with expectations when sufficient amount of high-quality demonstrations
    are present in the dataset. However, this trend is not universal, particularly
    when LLaMAs serve for question answering and GPT-2 medium is a backbone model
    for sub-question generation. Interestingly, Filtered BC outperforms BC only when
    GPT-2 medium is used alongside Mistral for answering.
  prefs: []
  type: TYPE_NORMAL
- en: The comparison between ILQL-sparse and ILQL-full does not reveal a consistent
    advantage for either method. In most of the cases, both ILQL variants underperform
    Filtered BC, and even falling behind standard BC in half of the cases. However,
    the superiority of the Filtered BC over offline RL approaches in NLP was also
    recently demonstrated by Gulcehre et al. ([2023](#bib.bib12)). Our finding should
    solve as additional motivation for the development of offline RL algorithms for
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations and Future Work.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our work serves as a foundational exploration, opening avenues for various future
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Development of Offline RL Approaches: A pivotal area for future exploration
    involves advancing offline RL or other suitable methodologies for distilling reasoning
    abilities from static datasets. This extension could contribute to more effective
    utilization of language models in reasoning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creation of a Larger Benchmark: Expanding our methodology, future work could
    focus on generating a more extensive benchmark as it requires only the access
    to ground truth-answers in the datset which usualy holds. This benchmark might
    incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al.,
    [2021](#bib.bib14)) or AQuA (Ling et al., [2017](#bib.bib27)), providing a broader
    assessment of reasoning capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concentration on Sub-Question Answering: Delving deeper into the sub-question
    answering aspect of the reasoning process presents a promising direction. While
    our dataset includes ChatGPT responses for sub-questions, their scoring and utilization
    remain unexplored. Future studies could investigate this component to enhance
    understanding and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilization of Open-Source Models: Exploring the application of open-source
    models, such as LLaMA, for sub-question generation emerges as a cost-effective
    alternative. Accessible without financial constraints, these models present an
    opportunity for researchers to delve into sub-question generation without monetary
    limitations. We were not able to run such kind of experiments ourselves due to
    the computational limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work introduces a novel AI-generated benchmark tailored for evaluating
    sub-questioning in reasoning tasks. We employ diverse offline learning approaches,
    varying model sizes for baselines, and assess the performance using different
    LLMs. Our experiments aim to shed light on the challenges and potential avenues
    for enhancing reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The outcomes reveal a significant performance gap between the best-performing
    approach and ChatGPT. The underwhelming performance of the offline RL approach
    underscores the need for further advancements in this domain, presenting an opportunity
    for future research to explore and refine these methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: By providing this benchmark, we aspire to catalyze research endeavors in the
    realm of sub-questioning. We anticipate that the dataset curated in this work
    will serve as a foundational resource for assessing the reasoning capabilities
    of emerging offline RL approaches in the field of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Mrinmaya Sachan for his supervision of our work and Elliott Ash for
    providing us with computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Akimov et al. (2022) Dmitriy Akimov, Vladislav Kurenkov, Alexander Nikulin,
    Denis Tarasov, and Sergey Kolesnikov. Let offline rl flow: Training conservative
    agents in the latent space of normalizing flows. *arXiv preprint arXiv:2211.11096*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. (2021) Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based
    offline reinforcement learning with diversified q-ensemble. *Advances in neural
    information processing systems*, 34:7436–7447, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022a) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2022b) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. *arXiv preprint
    arXiv:2212.08073*, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya
    Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision
    transformer: Reinforcement learning via sequence modeling. *Advances in neural
    information processing systems*, 34:15084–15097, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022) Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar,
    and Ed H. Chi. Off-policy actor-critic for recommender systems. *Proceedings of
    the 16th ACM Conference on Recommender Systems*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diehl et al. (2021) Christopher P. Diehl, Timo Sievernich, Martin Krüger, Frank
    Hoffmann, and Torsten Bertram. Umbrella: Uncertainty-aware model-based offline
    reinforcement learning leveraging planning. *ArXiv*, abs/2111.11097, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
    Levine. D4rl: Datasets for deep data-driven reinforcement learning. *arXiv preprint
    arXiv:2004.07219*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimoto & Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach
    to offline reinforcement learning. *Advances in neural information processing
    systems*, 34:20132–20145, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghasemipour et al. (2022) Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum.
    Why so pessimistic? estimating uncertainties for offline rl through ensembles,
    and why their independence matters. *Advances in Neural Information Processing
    Systems*, 35:18267–18281, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language
    modeling. *arXiv preprint arXiv:2308.08998*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guss et al. (2019) William H Guss, Brandon Houghton, Nicholay Topin, Phillip
    Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale
    dataset of minecraft demonstrations. *arXiv preprint arXiv:1907.13440*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander J. Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes. *ArXiv*, abs/2305.02301, 2023. URL [https://arxiv.org/abs/2305.02301](https://arxiv.org/abs/2305.02301).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang & Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. *arXiv preprint arXiv:2212.10403*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Juneja et al. (2023) Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny
    Manchanda, and Tanmoy Chakraborty. Small language models fine-tuned to coordinate
    larger language models improve complex reasoning. *arXiv preprint arXiv:2310.18338*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kostrikov et al. (2021) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline
    reinforcement learning with implicit q-learning. *arXiv preprint arXiv:2110.06169*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
    Conservative q-learning for offline reinforcement learning. *Advances in Neural
    Information Processing Systems*, 33:1179–1191, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2021) Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn,
    and Sergey Levine. A workflow for offline model-free robotic reinforcement learning.
    In *5th Annual Conference on Robot Learning*, 2021. URL [https://openreview.net/forum?id=fy4ZBWxYbIo](https://openreview.net/forum?id=fy4ZBWxYbIo).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurenkov et al. (2023) Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov,
    and Sergey Kolesnikov. Katakomba: Tools and benchmarks for data-driven nethack.
    *arXiv preprint arXiv:2306.08772*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
    Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling
    reinforcement learning from human feedback with ai feedback. *arXiv preprint arXiv:2309.00267*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open
    problems. *arXiv preprint arXiv:2005.01643*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    Program induction by rationale generation: Learning to solve and explain algebraic
    word problems. *arXiv preprint arXiv:1705.04146*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyu et al. (2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought
    reasoning. *arXiv preprint arXiv:2301.13379*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magister et al. (2023) Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek,
    Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nikulin et al. (2023) Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov,
    and Sergey Kolesnikov. Anti-exploration by random network distillation. *arXiv
    preprint arXiv:2301.13616*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) R OpenAI. Gpt-4 technical report. *arXiv*, pp.  2303–08774, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pp.  311–318, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
    *arXiv preprint arXiv:1910.01108*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shridhar et al. (2022) Kumar Shridhar, Jakub Macina, Mennatallah El-Assady,
    Tanmay Sinha, Manu Kapur, and Mrinmaya Sachan. Automatic generation of socratic
    subquestions for teaching math word problems. *arXiv preprint arXiv:2211.12835*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2023) Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan.
    Distilling reasoning capabilities into smaller language models. In *Findings of
    the Association for Computational Linguistics: ACL 2023*, pp.  7059–7073, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith et al. (2022) Laura Smith, Ilya Kostrikov, and Sergey Levine. A Walk
    in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,
    August 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snell et al. (2022) Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and
    Sergey Levine. Offline rl for natural language generation with implicit language
    q learning. *arXiv preprint arXiv:2206.11871*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarasov et al. (2022) Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav
    Kurenkov, and Sergey Kolesnikov. Corl: Research-oriented deep offline reinforcement
    learning library. *arXiv preprint arXiv:2210.07105*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarasov et al. (2023a) Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin,
    and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement
    learning. *arXiv preprint arXiv:2305.09836*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarasov et al. (2023b) Denis Tarasov, Ulrich Armel Mbou Sob, Miguel Arbesu,
    Nima Siboni, Sebastien Boyer, Marcin Skwark, Andries Smit, Oliver Bent, and Arnu
    Pretorius. Offline rl for generative design of protein binders. *bioRxiv*, pp. 
    2023–11, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. *arXiv
    preprint arXiv:2310.16944*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent
    reinforcement learning. *Nature*, 575(7782):350–354, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie
    Zhang, and Lei Han. Rorl: Robust offline reinforcement learning via conservative
    smoothing. *Advances in Neural Information Processing Systems*, 35:23851–23866,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Cheng Li, Guanting Dong, Chuanqi
    Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with
    large language models. *ArXiv*, abs/2308.01825, 2023. URL [https://arxiv.org/abs/2308.01825](https://arxiv.org/abs/2308.01825).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We run all of our experiments using single V100 GPUs. The training time never
    exceeded 5 days.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparaters are kept the same across different model sizes.
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted hyperparameters search only through $\{0.5,0.7,0.9\}$ parameter
    using GPT-2 small with full reward. Also, following Tarasov et al. ([2023a](#bib.bib41))
    we have increased discount factor value from default 0.99 to 0.999 which improved
    ILQL performance.
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer | Adam |'
  prefs: []
  type: TYPE_TB
- en: '| Learning rate | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient steps | 10000 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: BC hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer | Adam |'
  prefs: []
  type: TYPE_TB
- en: '| Learning rate | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient steps | 7500 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Filtered BC hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer | Adam |'
  prefs: []
  type: TYPE_TB
- en: '| Learning rate | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient steps | 25000 |'
  prefs: []
  type: TYPE_TB
- en: '| Discount factor | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Target update rate | 5e-3 |'
  prefs: []
  type: TYPE_TB
- en: '| IQL $\tau$ | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL $\beta$ | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| V loss weight | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Q loss weight | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CQL loss weight | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: ILQL hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Full Tabular Scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  prefs: []
  type: TYPE_TB
- en: '| BC | 0.476 | 0.508 | 0.538 | 0.507 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtered BC | 0.493 | 0.527 | 0.576 | 0.532 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-sparse | 0.471 | 0.518 | 0.541 | 0.510 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-full | 0.484 | 0.504 | 0.540 | 0.509 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | N/A | N/A | N/A | 0.682 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Accuracy score of the final answer using ChatGPT for sub-questions
    answering. Best scores are highlighted with bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  prefs: []
  type: TYPE_TB
- en: '| BC | 0.118 | 0.154 | 0.164 | 0.145 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtered BC | 0.125 | 0.159 | 0.162 | 0.149 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-sparse | 0.125 | 0.138 | 0.162 | 0.142 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-full | 0.114 | 0.144 | 0.163 | 0.140 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | N/A | N/A | N/A | 0.234 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Accuracy score of the final answer using LLaMA 7B for sub-questions
    answering. Best scores are highlighted with bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  prefs: []
  type: TYPE_TB
- en: '| BC | 0.184 | 0.212 | 0.247 | 0.214 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtered BC | 0.194 | 0.230 | 0.245 | 0.223 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-sparse | 0.180 | 0.207 | 0.250 | 0.212 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-full | 0.182 | 0.210 | 0.252 | 0.215 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | N/A | N/A | N/A | 0.353 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Accuracy score of the final answer using LLaMA 13B for sub-questions
    answering. Best scores are highlighted with bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  prefs: []
  type: TYPE_TB
- en: '| BC | 0.240 | 0.264 | 0.290 | 0.265 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtered BC | 0.228 | 0.256 | 0.293 | 0.259 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-sparse | 0.219 | 0.261 | 0.288 | 0.256 |'
  prefs: []
  type: TYPE_TB
- en: '| ILQL-full | 0.231 | 0.252 | 0.280 | 0.254 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | N/A | N/A | N/A | 0.446 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Accuracy score of the final answer using Mistral for sub-questions
    answering. Best scores are highlighted with bold.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C ChatGPT Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Input |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| You are given mathematical problems marked with ”Problem”. Your task is to
    split it into smaller sub-problems and formulate them as sub-questions which will
    be answered by someone else who’s objective is to solve the original problem.
    Questions must not contain the answers for the previous questions in them. Do
    not ask questions where the answer is already given in the problem. For each problem
    come up with the sequence of sub-questions and output each of them on separate
    line which starts with letter Q followed by the number of question. Do not output
    anything else. |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: A robe takes 2 bolts of blue fiber and half that much white fiber.
    How many bolts in total does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How many bolts of white fiber does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How many bolts in total does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How much did the house cost? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How much did the repairs increase the value of the house? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: What is the new value of the house? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How much profit did he make? |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: '| Response |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How much money does Betty have? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How much money did Betty’s parents give her? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: How much money did Betty’s grandparents give her? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How much money does Betty still need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Sample ChatGPT input for generating sub-questions and one of the
    corresponding responses.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| You are given the mathematical problems marked with ”Problem” and a sequence
    of questions which should help in solving it. Question number N is marked as ”QN:”.
    Based on the problem and sequence of questions answer each of the questions with
    answer ”AN:” and give the answer to the whole problem using ”Final answer:” using
    only the resulting number without adding any additional comments after it. |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: A robe takes 2 bolts of blue fiber and half that much white fiber.
    How many bolts in total does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How many bolts of white fiber does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How many bolts in total does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| A1: It takes 2/2=1 bolt of white fiber |'
  prefs: []
  type: TYPE_TB
- en: '| A2: So the total amount of fabric is 2+1=3 |'
  prefs: []
  type: TYPE_TB
- en: '| Final answer: 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: Josh decides to try flipping a house. He buys a house for $80,000
    and then puts in $50,000 in repairs. This increased the value of the house by
    150%. How much profit did he make? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How much did the house cost? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How much did the repairs increase the value of the house? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: What is the new value of the house? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How much profit did he make? |'
  prefs: []
  type: TYPE_TB
- en: '| A1: The cost of the house and repairs came out to 80,000+50,000=130,000 |'
  prefs: []
  type: TYPE_TB
- en: '| A2: He increased the value of the house by 80,000*1.5=120,000 |'
  prefs: []
  type: TYPE_TB
- en: '| A3: So the new value of the house is 120,000+80,000=200,000 |'
  prefs: []
  type: TYPE_TB
- en: '| A4: So he made a profit of 200,000-130,000=70,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Final answer: 70000 |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How much money does Betty have? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How much money did Betty’s parents give her? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: How much money did Betty’s grandparents give her? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How much money does Betty still need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: '| Response |'
  prefs: []
  type: TYPE_TB
- en: '| A1: Betty has 100/2=50 dollars. |'
  prefs: []
  type: TYPE_TB
- en: '| A2: Betty’s parents gave her 15 dollars. |'
  prefs: []
  type: TYPE_TB
- en: '| A3: Betty’s grandparents gave her 15*2=30 dollars. |'
  prefs: []
  type: TYPE_TB
- en: '| A4: Betty still needs 100-50-15-30=5 dollars. |'
  prefs: []
  type: TYPE_TB
- en: '| Final answer: 5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Sample ChatGPT input for answering sub-questions and the corresponding
    response.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| You are given the mathematical problem marked with ”Problem” and a sequence
    of sub-questions for solving it. Sub-question number N is marked as ”QN:”. Based
    on the problem for each sub-questions decide whether this question is helpful
    for solving the given problem. An essential property of a good questioning strategy
    is to ask questions that are directed towards the most critical domain specific
    content. Asking the right sequence of relevant questions that can assist in reaching
    the final goal is an important part of good questioning. If question repeats any
    of the previous it is not useful. The question for which answer is given in the
    problem or can’t be answered at all is also not useful. So redundant questions
    are not good. |'
  prefs: []
  type: TYPE_TB
- en: '| For each question output me ”QN: $<$ for the first two questions. Do not
    try to solve the problem anyhow as I’m only interested in the quality of the sub-questions.
    Strictly follow the output format. Provide answers only for the last given problem.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How many eggs does Janet sell? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: Is duck an animal? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: How many eggs does each duck lay? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How much does Janet make at the farmers’ market? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: No |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: No |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: A robe takes 2 bolts of blue fiber and half that much white fiber.
    How many bolts in total does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How many bolts of white fiber does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How bolts of blue fiber does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: How bolts of white fiber does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How many bolts in total does it take? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: No |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: No |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: How much money does Betty have? |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: How much money did Betty’s parents give her? |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: How much money did Betty’s grandparents give her? |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: How much money does Betty still need to buy the wallet? |'
  prefs: []
  type: TYPE_TB
- en: '| Response |'
  prefs: []
  type: TYPE_TB
- en: '| Q1: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Q2: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Q3: Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Q4: Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Sample ChatGPT input for generating sub-questions feedback and one
    of the corresponding responses.'
  prefs: []
  type: TYPE_NORMAL
