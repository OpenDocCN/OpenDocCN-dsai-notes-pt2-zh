- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12928](https://ar5iv.labs.arxiv.org/html/2406.12928)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yijun Liu¹  Yuan Meng¹  Fang Wu²  Shenhao Peng^(2,5)  Hang Yao²
  prefs: []
  type: TYPE_NORMAL
- en: Chaoyu Guan²  Chen Tang¹  Xinzhu Ma^(3,4)  Zhi Wang¹  Wenwu Zhu¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Tsinghua University ²Tsingmao Intelligence  ³CUHK  ⁴Shanghai AI Lab  ⁵HUST
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have exhibited exciting progress in multiple scenarios,
    while the huge computational demands hinder their deployments in lots of real-world
    applications. As an effective means to reduce memory footprint and inference cost,
    quantization also faces challenges in performance degradation at low bit-widths.
    Understanding the impact of quantization on LLM capabilities, especially the *generalization
    ability*, is crucial. However, the community’s main focus remains on the algorithms
    and models of quantization, with insufficient attention given to whether the quantized
    models can retain the strong generalization abilities of LLMs. In this work, we
    fill this gap by providing a comprehensive benchmark suite for this research topic,
    including an evaluation system, detailed analyses, and a general toolbox. Specifically,
    based on the dominant pipeline in LLM quantization, we primarily explore the impact
    of calibration data distribution on the generalization of quantized LLMs and conduct
    the benchmark using more than 40 datasets within two main scenarios. Based on
    this benchmark, we conduct extensive experiments with two well-known LLMs (English
    and Chinese) and four quantization algorithms to investigate this topic in-depth,
    yielding several counter-intuitive and valuable findings, *e.g.*, models quantized
    using a calibration set with the same distribution as the test data are not necessarily
    optimal. Besides, to facilitate future research, we also release a modular-designed
    toolbox, which decouples the overall pipeline into several separate components,
    e.g., base LLM module, dataset module, quantizer module, etc. and allows subsequent
    researchers to easily assemble their methods through a simple configuration. Our
    benchmark suite is publicly available at [https://github.com/TsingmaoAI/MI-optimize](https://github.com/TsingmaoAI/MI-optimize).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, large language models (LLMs) have made groundbreaking advancements,
    demonstrating remarkable results and outstanding *generalization ability* across
    various tasks ([56](#bib.bib56); [72](#bib.bib72); [1](#bib.bib1); [54](#bib.bib54)).
    For example, given a few prompt examples or questions, LLMs can produce insightful
    answers within the unseen domain ([47](#bib.bib47); [5](#bib.bib5)). However,
    while LLMs exhibit remarkable capabilities, their substantial size makes real-world
    implementation cost-prohibitive. To address this challenge, model quantization
    has emerged as a prevailing technique for reducing the memory footprint of LLMs ([14](#bib.bib14);
    [31](#bib.bib31); [10](#bib.bib10); [6](#bib.bib6); [66](#bib.bib66); [62](#bib.bib62);
    [51](#bib.bib51)). Specifically, quantization reduces the model size by replacing
    high-precision floating-point numbers with lower-precision integers (*e.g.*, from
    FP16 to INT4) ([40](#bib.bib40); [17](#bib.bib17); [75](#bib.bib75)). Currently,
    to avoid the substantial retraining costs of LLMs, the quantization methods for
    large models primarily employ post-training quantization (PTQ) ([14](#bib.bib14);
    [31](#bib.bib31); [10](#bib.bib10); [6](#bib.bib6)), which leverages calibration
    data to optimize the error caused by the quantization. Given the prevalent view
    that LLM capabilities stem from their extensive parameter count ([24](#bib.bib24)),
    a critical question emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Can the quantized LLMs still retain their strong generalization ability?*'
  prefs: []
  type: TYPE_NORMAL
- en: While some works have acknowledged this issue ([33](#bib.bib33); [21](#bib.bib21);
    [30](#bib.bib30); [19](#bib.bib19); [23](#bib.bib23)), there is still a lack of
    systematic evaluation regarding the generalization performance of LLMs after quantization,
    particularly considering the impact of *calibration data* introduced during the
    quantization process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a36767890caa39c8bc9c1d1e8077b758.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: We show the pipeline of model quantization and the data required
    at each stage (Top).'
  prefs: []
  type: TYPE_NORMAL
- en: The calibration data used in previous works generally share the same distribution
    with pre-training data (S1), and the relation between calibration data and test
    data should be further discussed (S2).
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox"), the process of
    model quantization encompasses three distinct stages: pre-training, quantization,
    and inference, utilizing pre-training data, calibration data, and test data, respectively.
    Existing quantization researches typically use a standard calibration set, which
    is usually a subset of the pre-training data (Scenario 1, S1), and evaluate on
    several fixed datasets ([44](#bib.bib44); [7](#bib.bib7); [53](#bib.bib53); [50](#bib.bib50);
    [71](#bib.bib71)). However, because using task-specific data for model calibration
    is a more reasonable choice in practical applications, the relationship between
    the distribution of *calibration data* and *test data* and its impact on the generalization
    ability of quantized models is a more worthy research topic that has not been
    deeply explored (Scenario 2, S2). In this work, to answer the abovementioned question
    and bridge the gap between academic research and practical implementation, we
    provide a platform to evaluate the generalization ability of quantized LLMs, covering
    *benchmarks*, *analyses*, and a modular-designed *toolbox*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark evaluation. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox"), we build the benchmark based on the two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ In S1 (Section [2](#S2 "2 S1: Generalization Assessment of Quantized
    LLMs with Standard Setting ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox")), beyond the existing research, we collect
    the most comprehensive evaluation of test datasets to date, covering 9 categories
    and 26 datasets. We use C4 ([48](#bib.bib48)) as the calibration dataset, and
    quantize LLaMA-7B ([54](#bib.bib54)) model by two methods ([14](#bib.bib14); [10](#bib.bib10))
    across three weight bit-widths.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ In S2 (Section [3](#S3 "3 S2: Generalization Assessment of Quantized
    LLMs with Domain Shifts ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox")), our benchmark covers 19 datasets with two
    types of distribution shifts between calibration data and test data: *cross-dataset*
    and *cross-subject*. We consider both English and Chinese domains for the cross-dataset
    setting. Besides, our benchmark also includes a more challenging cross-subject
    setting, e.g. from humanities to social science. To our knowledge, no prior work
    has investigated the generalization of quantized models in a cross-subject setting.
    For all settings, our benchmark builds the Independent and Identically Distribution
    (I.I.D) and Out-of-Distribution (OOD) evaluations by adjusting the calibration
    data distributions. In our experiments, we quantize LLaMA-7B ([54](#bib.bib54))
    and Baichuan2-7B-Base ([11](#bib.bib11)) for English and Chinese models with four
    methods ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31); [62](#bib.bib62))
    across three weight bit-widths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generalization performance of quantized models is assessed using zero-shot
    and few-shot evaluation for all experiments, and we summarize the key features
    of our benchmark in Tab. [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox").'
  prefs: []
  type: TYPE_NORMAL
- en: Empirical findings. Based on the experiments, we observe several counter-intuitive
    phenomena, e.g.,
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ *Tasks vary significantly in their sensitivity to quantization, and
    even the same tasks exhibit different sensitivities on different datasets*. For
    example, natural language inference tasks are the least sensitive across various
    tasks and MC-TACO ([74](#bib.bib74)) varies more than ARC-Easy ([7](#bib.bib7))
    in the zero-shot setting. We also observe that lower bit quantization even yields
    improved performance in some settings, such as GLUE-SST and GLUE-QNLI ([57](#bib.bib57))
    in the zero-shot setting.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ *Consistency between calibration data and test distribution does not
    always yield optimal performance*, which is significantly correlated to the evaluation
    tasks and the magnitude of the distribution shift. For example, in cross-dataset
    tasks (English), there is often an optimal calibration dataset for the same task,
    which is *not* I.I.D data and can vary depending on the base quantization algorithm.
    For cross-subject tasks, except for the SpQR algorithm, which generally favors
    I.I.D data, the regularity of results in other settings is not obvious.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of the proposed benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Distribution Shift | Task Language | Weight Precision | Model
    | Benchmark & Dataset | Results |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | - | English | {16, 4, 3, 2} | LLaMA2-7B ([54](#bib.bib54)) | WinoGrande ([50](#bib.bib50)),
    WSC273 ([28](#bib.bib28)), HellaSwag ([71](#bib.bib71)) | Fig. [2](#S2.F2 "Figure
    2 ‣ 2 S1: Generalization Assessment of Quantized LLMs with Standard Setting ‣
    Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox") |'
  prefs: []
  type: TYPE_TB
- en: '| SWAG ([70](#bib.bib70)), PIQA ([53](#bib.bib53)), MathQA ([2](#bib.bib2)),
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mutual, Mutual_Plus ([8](#bib.bib8)), CrowS-Pairs ([42](#bib.bib42)), |'
  prefs: []
  type: TYPE_TB
- en: '| Toxigen ([18](#bib.bib18)),PubMedQA ([22](#bib.bib22)), OpenBookQA ([38](#bib.bib38)),
    SciQ ([59](#bib.bib59)), |'
  prefs: []
  type: TYPE_TB
- en: '| ARC-Easy, ARC-Challenge ([7](#bib.bib7)), MC-TACO ([74](#bib.bib74)), RACE ([27](#bib.bib27)),
    |'
  prefs: []
  type: TYPE_TB
- en: '| QA4MRE ([45](#bib.bib45)), GLUE (6 datasets) ([57](#bib.bib57)), ANLI ([43](#bib.bib43)),
    BLiMP ([58](#bib.bib58)) |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Cross-dataset | English | {4, 3} | LLaMA2-7B ([54](#bib.bib54)) | BOSS
    (16 datasets) ([69](#bib.bib69)) | Tab. [2](#S3.T2 "Table 2 ‣ 3 S2: Generalization
    Assessment of Quantized LLMs with Domain Shifts ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Cross-dataset | Chinese | {4, 3, 2} | Baichuan2-7B ([64](#bib.bib64))
    | C-EVAL ([20](#bib.bib20)), CMMLU ([29](#bib.bib29)) | Tab. [3](#S3.T3 "Table
    3 ‣ 3 S2: Generalization Assessment of Quantized LLMs with Domain Shifts ‣ Evaluating
    the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Cross-subject | Chinese | {4, 3, 2} | Baichuan2-7B ([64](#bib.bib64))
    | C-EVAL ([20](#bib.bib20)) | Tab. LABEL:tab:cds_subject |'
  prefs: []
  type: TYPE_TB
- en: 'Toolbox. To support this work and facilitate future research, we develop a
    modular-designed code library. Specifically, this toolbox decouples the overall
    pipeline shown in Fig.[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") into several separate
    components, e.g., LLM module, dataset module, quantizer module, etc., and provides
    common choices for each component and easy-to-use interface for possible extensions
    (see Section [4](#S4 "4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") and
    Fig. [4](#S4 "4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") for more details
    of the toolbox). This toolbox will be open-sourced along with the benchmark to
    facilitate future quantization applications and research.'
  prefs: []
  type: TYPE_NORMAL
- en: '2 S1: Generalization Assessment of Quantized LLMs with Standard Setting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/059b47978e5a5c1b69afbe01d811ec35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: S1: evaluation of quantized LLaMA2-7B on several standard datasets.
    Quantization methods include GPTQ and SpQR. Quantization bits include W4A16, W3A16,
    and W2A16, with W16A16 used as reference. The left figure shows 5-shot results,
    while the right figure shows 0-shot results. Different background colors represent
    different task types.'
  prefs: []
  type: TYPE_NORMAL
- en: To assess the difference in generalization ability, it is necessary to ensure
    that all other settings remain consistent except for the quantization process.
    To maintain consistency in the data encountered by the model before and after
    quantization, we strive to use calibration data during quantization that is as
    similar as possible to the data used during the pre-training phase of the LLM,
    namely the dataset C4 ([48](#bib.bib48)) derived from pre-training data. The experimental
    setting is consistent with the evaluation settings used previously for quantized
    models ([14](#bib.bib14); [10](#bib.bib10); [21](#bib.bib21); [33](#bib.bib33);
    [30](#bib.bib30)). We utilize the LM Evaluation Harness ([15](#bib.bib15)) with
    recommended parameters to conduct zero-shot and few-shot tests on the following
    tasks. We provide full configurations in the *supplemental material*, as well
    as code that we plan to release publicly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 26 datasets we evaluate can be divided into nine categories: ❶common sense
    reasoning, ❷mathematical reasoning, ❸multi-turn dialogue reasoning, ❹bias diagnosis
    and mitigation, ❺scientific knowledge question answering, ❻reading comprehension,
    ❼natural language inference, ❽sentiment analysis, and ❾syntax phenomena evaluation.
    The common sense reasoning datasets include WinoGrande ([50](#bib.bib50)), WSC273 ([28](#bib.bib28)),
    GLUE-WNLI ([57](#bib.bib57)), HellaSwag ([71](#bib.bib71)), SWAG ([70](#bib.bib70)),
    and PIQA ([53](#bib.bib53)). The mathematical reasoning datasets include MathQA([2](#bib.bib2)).
    The multi-turn dialogue reasoning datasets include Mutual and Mutual_Plus ([8](#bib.bib8)).
    The bias diagnosis and mitigation datasets include CrowS-Pairs ([42](#bib.bib42))
    and Toxigen ([18](#bib.bib18)). The scientific knowledge question answering datasets
    include PubMedQA ([22](#bib.bib22)), OpenBookQA ([38](#bib.bib38)), SciQ ([59](#bib.bib59)),
    ARC-Easy, ARC-Challenge ([7](#bib.bib7)), and MC-TACO ([74](#bib.bib74)). The
    reading comprehension datasets include RACE ([27](#bib.bib27)) and QA4MRE ([45](#bib.bib45)).
    The natural language inference datasets include GLUE-MNLI, GLUE-MNLI-Mismatched,
    GLUE-RTE, GLUE-QNLI ([57](#bib.bib57)), and ANLI ([43](#bib.bib43)). The sentiment
    analysis dataset includes GLUE-SST ([57](#bib.bib57)). The syntax phenomena evaluation
    dataset includes BLiMP ([58](#bib.bib58)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We present the experimental results for both the 5-shot and 0-shot scenarios
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2 S1: Generalization Assessment of Quantized LLMs
    with Standard Setting ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox"). It can be observed that when quantizing model
    weights to 3-4 bits, the performance degradation of all methods is not very pronounced.
    In some cases, quantizing to 4 bits even leads to higher model performance compared
    to full precision. However, when weights are quantized to 2 bits, GPTQ exhibits
    a significant performance drop on most tasks. Compared to other methods, SPQR
    maintains relatively good performance at 2 bits, which may be attributed to SPQR’s
    ability to identify and isolate outlier weights. Additionally, we found that the
    relative difference in performance degradation after quantization across different
    datasets for the same task is not significant, whereas the relative difference
    in performance degradation after quantization between datasets for different tasks
    is considerable. This suggests that the sensitivity to quantization is similar
    for the same tasks but varies for different tasks. For instance, in natural language
    inference tasks, the performance drop of quantized models is minimal across all
    datasets, while in scientific knowledge question answering tasks and common sense
    reasoning tasks, the performance drop is more significant. In the case of the
    5-shot scenario, the performance degradation caused by quantization is relatively
    smoother compared to 0-shot scenario, especially noticeable in scientific knowledge
    question answering tasks. For example, on the SciQ dataset, in the 5-shot scenario,
    the performance of GPTQ decreases from 0.96 at 4 bits to 0.69 at 2 bits, whereas
    in the 0-shot scenario, it drops from 0.91 at 4 bits to 0.51.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, when most methods quantize weights to 3-4 bits precision, models can
    still achieve performance close to that of full precision models on most tasks.
    This indicates that under moderate quantization, models still retain strong generalization
    capabilities. Additionally, different tasks exhibit varying sensitivities to quantization,
    with natural language inference tasks showing lower sensitivity while scientific
    knowledge question answering tasks and common sense reasoning tasks emerge higher
    sensitivity. In the 5-shot scenario, the performance degradation due to quantization
    is relatively smoother compared to the 0-shot scenario.
  prefs: []
  type: TYPE_NORMAL
- en: '3 S2: Generalization Assessment of Quantized LLMs with Domain Shifts'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section investigates novel generalization scenarios in quantization, where
    different generalization scenarios serve as instantiations of the framework. The
    distribution shift we consider primarily pertains to the shift from calibration
    data to test data. Types of distribution shift include *cross-dataset* distribution
    shift and *cross-subject* distribution shift, aimed at studying the impact of
    distribution shift from calibration data to test data on quantized model performance.
    Cross-dataset distribution shift refers to using different datasets as calibration
    set, while cross-subject distribution shift refers to using different subjects
    from the same dataset as calibration set. Experiments will encompass two main
    categories: *English* *cross-dataset* distribution shift experiments on the out-of-distribution
    generalization benchmark BOSS, and *Chinese* *cross-subject* distribution shift
    experiments as well as *cross-dataset* distribution shift experiments on Chinese
    domain-specific tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Cross-dataset distribution shift evaluation on BOSS. "Calib." represents
    the calibration dataset, and "Gene." represents generalization scenario. To save
    space, abbreviations are used for datasets. Each row presents experimental results
    using different datasets as calibration sets on the same test dataset. Results
    with colored backgrounds indicate I.I.D results, while those without color represent
    OOD results. The higher the metric, the better the performance. Bold results indicate
    the best performance on the same test dataset. Note: Some datasets could not be
    used as calibration sets due to insufficient memory resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | EQA | SA | NLI | TD |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test |
    Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | 0-shot | 4/16 | 53.84 | 52.73 | 54.69 | 57.31 | AZ | 0-shot | 4/16 |
    70.81 | 17.87 | 63.18 | 72.08 | MN | 0-shot | 4/16 | 0.36 | 0.23 | 0.22 | - |
    CC | 0-shot | 4/16 | 23.90 | 26.96 | 52.52 | 53.32 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 45.31 | 48.86 | 49.49 | 50.79 | 3/16 | 38.06 | 0.38 | 0.26 | 0.04
    | 3/16 | 0.00 | 0.00 | 0.00 | - | 3/16 | 0.60 | 2.45 | 9.70 | 10.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 67.04 | 65.97 | 67.06 | 68.16 | 3-shot | 4/16 | 83.69 | 56.66
    | 80.79 | 82.55 | 3-shot | 4/16 | 49.69 | 32.81 | 34.93 | - | 2-shot | 4/16 |
    91.80 | 87.46 | 91.71 | 91.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 60.76 | 58.84 | 63.34 | 63.01 | 3/16 | 74.54 | 24.86 | 59.06 | 59.79
    | 3/16 | 34.12 | 31.79 | 31.82 | - | 3/16 | 89.11 | 35.94 | 91.96 | 90.35 |'
  prefs: []
  type: TYPE_TB
- en: '| AQA | 0-shot | 4/16 | 28.00 | 27.12 | 28.40 | 30.40 | DS | 0-shot | 4/16
    | 46.10 | 21.37 | 31.82 | 46.79 | AN | 0-shot | 4/16 | 1.07 | 0.52 | 0.93 | -
    | AC | 0-shot | 4/16 | 19.12 | 5.93 | 7.84 | 17.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 21.81 | 25.28 | 23.35 | 24.99 | 3/16 | 17.59 | 1.72 | 0.01 | 0.00
    | 3/16 | 4.17 | 0.00 | 0.00 | - | 3/16 | 0.76 | 1.72 | 0.19 | 0.57 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 35.50 | 36.11 | 31.97 | 35.77 | 3-shot | 4/16 | 54.40 | 38.78
    | 52.54 | 55.50 | 3-shot | 4/16 | 34.34 | 33.76 | 33.24 | - | 2-shot | 4/16 |
    15.87 | 17.59 | 15.87 | 16.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 31.39 | 29.54 | 31.60 | 32.24 | 3/16 | 54.68 | 36.05 | 33.86 | 43.46
    | 3/16 | 30.97 | 33.69 | 33.28 | - | 3/16 | 60.23 | 90.35 | 15.87 | 56.02 |'
  prefs: []
  type: TYPE_TB
- en: '| NQA | 0-shot | 4/16 | 37.94 | 38.76 | 38.63 | 38.23 | SE | 0-shot | 4/16
    | 18.32 | 8.21 | 15.60 | 26.43 | WN | 0-shot | 4/16 | 0.09 | 0.04 | 0.11 | - |
    IH | 0-shot | 4/16 | 37.37 | 22.55 | 33.90 | 40.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 31.36 | 33.79 | 33.37 | 34.45 | 3/16 | 4.83 | 0.09 | 0.20 | 0.01 |
    3/16 | 0.49 | 0.00 | 0.00 | - | 3/16 | 11.27 | 7.32 | 4.53 | 13.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 48.55 | 49.30 | 49.73 | 49.09 | 3-shot | 4/16 | 42.96 | 28.55
    | 42.99 | 44.75 | 3-shot | 4/16 | 41.51 | 43.34 | 47.53 | - | 2-shot | 4/16 |
    62.36 | 63.46 | 62.00 | 62.29 |'
  prefs: []
  type: TYPE_TB
- en: '| W3A16 | 44.38 | 43.35 | 46.95 | 45.61 | 3/16 | 42.36 | 22.67 | 35.54 | 29.40
    | 3/16 | 38.83 | 48.09 | 48.15 | - | 3/16 | 63.52 | 90.35 | 61.83 | 61.77 |'
  prefs: []
  type: TYPE_TB
- en: '| SQA | 0-shot | 4/16 | 42.58 | 45.72 | 46.21 | 44.20 | SST | 0-shot | 4/16
    | 49.15 | 20.73 | 27.12 | 44.98 | CN | 0-shot | 4/16 | 0.06 | 0.00 | 0.00 | -
    | TG | 0-shot | 4/16 | 48.44 | 36.72 | 44.84 | 57.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 30.19 | 26.99 | 28.49 | 33.73 | 3/16 | 7.82 | 1.04 | 0.00 | 0.00 |
    3/16 | 0.06 | 1.12 | 1.45 | - | 3/16 | 12.81 | 9.53 | 2.19 | 14.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 56.04 | 61.89 | 60.92 | 62.17 | 3-shot | 4/16 | 60.50 | 33.25
    | 45.24 | 51.11 | 3-shot | 4/16 | 35.23 | 36.35 | 32.44 | - | 2-shot | 4/16 |
    72.03 | 75.47 | 67.81 | 68.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 43.46 | 42.83 | 45.17 | 48.82 | 3/16 | 54.37 | 33.25 | 35.46 | 50.20
    | 3/16 | 29.54 | 29.03 | 33.39 | - | 3/16 | 70.47 | 90.35 | 57.50 | 62.19 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test |
    Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | 0-shot | 4/16 | 57.03 | 49.87 | 53.00 | 54.36 | AZ | 0-shot | 4/16 |
    63.34 | 62.46 | 72.52 | 83.14 | MN | 0-shot | 4/16 | 0.57 | 0.02 | 0.13 | - |
    CC | 0-shot | 4/16 | 61.73 | 59.48 | 58.92 | 37.48 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 52.37 | 45.90 | 54.55 | 58.36 | 3/16 | 72.38 | 55.79 | 37.28 | 27.84
    | 3/16 | 0.00 | 0.01 | 0.00 | - | 3/16 | 36.90 | 2.54 | 15.42 | 22.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 66.45 | 66.80 | 67.41 | 67.21 | 3-shot | 4/16 | 79.65 | 69.31
    | 85.44 | 82.91 | 3-shot | 4/16 | 36.19 | 40.45 | 41.62 | - | 2-shot | 4/16 |
    90.65 | 89.27 | 91.74 | 84.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 65.12 | 65.55 | 68.65 | 66.95 | 3/16 | 83.68 | 86.30 | 72.18 | 83.50
    | 3/16 | 32.39 | 40.31 | 38.47 | - | 3/16 | 87.70 | 91.76 | 86.99 | 83.56 |'
  prefs: []
  type: TYPE_TB
- en: '| AQA | 0-shot | 4/16 | 30.59 | 25.11 | 27.60 | 29.50 | DS | 0-shot | 4/16
    | 35.47 | 43.53 | 40.85 | 50.40 | AN | 0-shot | 4/16 | 0.86 | 0.07 | 0.28 | -
    | AC | 0-shot | 4/16 | 10.13 | 4.97 | 12.05 | 13.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 26.35 | 21.43 | 27.55 | 30.36 | 3/16 | 41.87 | 31.17 | 15.42 | 29.10
    | 3/16 | 0.00 | 0.07 | 0.00 | - | 3/16 | 2.49 | 0.76 | 7.84 | 2.87 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 37.64 | 36.63 | 36.94 | 35.42 | 3-shot | 4/16 | 50.82 | 46.67
    | 57.74 | 56.34 | 3-shot | 4/16 | 33.17 | 33.31 | 33.79 | - | 2-shot | 4/16 |
    16.44 | 21.03 | 15.87 | 20.46 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 34.61 | 34.75 | 37.49 | 33.10 | 3/16 | 59.10 | 54.80 | 52.56 | 56.02
    | 3/16 | 33.66 | 31.93 | 33.14 | - | 3/16 | 15.87 | 15.87 | 19.31 | 15.87 |'
  prefs: []
  type: TYPE_TB
- en: '| NQA | 0-shot | 4/16 | 40.30 | 38.01 | 39.40 | 38.22 | SE | 0-shot | 4/16
    | 14.62 | 23.36 | 19.85 | 33.24 | WN | 0-shot | 4/16 | 0.28 | 0.00 | 0.00 | -
    | IH | 0-shot | 4/16 | 42.21 | 41.79 | 40.12 | 31.76 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 35.79 | 33.27 | 40.80 | 38.77 | 3/16 | 16.05 | 10.22 | 4.75 | 7.30
    | 3/16 | 0.00 | 0.06 | 0.00 | - | 3/16 | 31.32 | 6.78 | 17.68 | 16.96 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 49.61 | 49.12 | 49.70 | 48.47 | 3-shot | 4/16 | 44.48 | 44.15
    | 44.25 | 44.39 | 3-shot | 4/16 | 43.28 | 43.77 | 41.79 | - | 2-shot | 4/16 |
    64.24 | 65.85 | 62.14 | 66.07 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 48.25 | 46.61 | 48.99 | 47.79 | 3/16 | 53.16 | 43.63 | 41.76 | 44.77
    | 3/16 | 39.09 | 47.32 | 40.77 | - | 3/16 | 62.95 | 63.14 | 63.17 | 64.37 |'
  prefs: []
  type: TYPE_TB
- en: '| SQA | 0-shot | 4/16 | 46.45 | 42.62 | 44.30 | 45.10 | SST | 0-shot | 4/16
    | 46.02 | 29.47 | 44.72 | 55.67 | CN | 0-shot | 4/16 | 0.00 | 0.22 | 0.45 | -
    | TG | 0-shot | 4/16 | 54.37 | 52.66 | 51.09 | 39.53 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 36.90 | 44.57 | 42.88 | 39.31 | 3/16 | 23.08 | 14.87 | 3.65 | 6.52
    | 3/16 | 0.06 | 0.00 | 0.89 | - | 3/16 | 41.88 | 9.69 | 19.38 | 37.34 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 61.63 | 57.77 | 61.79 | 60.55 | 3-shot | 4/16 | 55.41 | 42.37
    | 58.54 | 59.32 | 3-shot | 4/16 | 36.13 | 34.84 | 34.23 | - | 2-shot | 4/16 |
    69.84 | 76.56 | 61.41 | 77.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 48.86 | 59.19 | 56.34 | 55.06 | 3/16 | 63.49 | 60.37 | 53.98 | 61.80
    | 3/16 | 35.29 | 35.90 | 33.17 | - | 3/16 | 73.13 | 66.88 | 68.44 | 77.03 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test |
    Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | 0-shot | 4/16 | 56.73 | 55.09 | 52.09 | 50.21 | AZ | 0-shot | 4/16 |
    - | 5.42 | 35.23 | 33.65 | MN | 0-shot | 4/16 | 0.48 | 0.14 | 0.06 | - | CC |
    0-shot | 4/16 | 50.17 | 66.60 | 42.19 | 42.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 48.32 | 37.95 | 44.45 | 40.30 | 3/16 | - | 39.41 | 70.10 | 35.95 |
    3/16 | 0.00 | 0.01 | 0.01 | - | 3/16 | 41.96 | 39.03 | 46.95 | 14.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 66.57 | 66.91 | 67.02 | 66.21 | 3-shot | 4/16 | - | 83.64
    | 83.73 | 78.06 | 3-shot | 4/16 | 42.20 | 38.37 | 36.05 | - | 2-shot | 4/16 |
    91.84 | 91.63 | 90.80 | 89.31 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 59.81 | 61.81 | 61.27 | 61.38 | 3/16 | - | 88.73 | 90.16 | 88.92 |
    3/16 | 35.44 | 34.22 | 35.34 | - | 3/16 | 36.43 | 73.04 | 90.93 | 27.24 |'
  prefs: []
  type: TYPE_TB
- en: '| AQA | 0-shot | 4/16 | 29.73 | 29.20 | 28.34 | 27.57 | DS | 0-shot | 4/16
    | - | 2.36 | 20.10 | 22.19 | AN | 0-shot | 4/16 | 0.59 | 0.07 | 0.07 | - | AC
    | 0-shot | 4/16 | 9.56 | 11.85 | 11.28 | 5.55 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 23.02 | 17.58 | 20.37 | 18.62 | 3/16 | - | 8.76 | 27.09 | 11.87 |
    3/16 | 0.00 | 0.00 | 0.00 | - | 3/16 | 5.74 | 4.59 | 4.21 | 1.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 35.76 | 37.01 | 37.55 | 36.78 | 3-shot | 4/16 | - | 53.91
    | 55.92 | 50.95 | 3-shot | 4/16 | 33.66 | 33.66 | 33.66 | - | 2-shot | 4/16 |
    15.87 | 15.87 | 16.06 | 16.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 31.64 | 33.04 | 32.88 | 33.46 | 3/16 | - | 50.95 | 56.24 | 59.05 |
    3/16 | 33.69 | 32.55 | 33.69 | - | 3/16 | 24.86 | 18.93 | 16.06 | 56.02 |'
  prefs: []
  type: TYPE_TB
- en: '| NQA | 0-shot | 4/16 | 39.20 | 38.58 | 39.47 | 38.10 | SE | 0-shot | 4/16
    | - | 4.19 | 18.90 | 14.96 | WN | 0-shot | 4/16 | 0.30 | 0.17 | 0.02 | - | IH
    | 0-shot | 4/16 | 37.59 | 44.64 | 34.09 | 27.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 35.75 | 31.27 | 32.91 | 33.69 | 3/16 | - | 5.52 | 14.95 | 5.49 | 3/16
    | 0.00 | 0.00 | 0.00 | - | 3/16 | 20.22 | 17.97 | 25.72 | 4.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 43.25 | 43.18 | 43.39 | 42.56 | 3-shot | 4/16 | - | 45.03
    | 45.44 | 43.77 | 3-shot | 4/16 | 40.02 | 39.40 | 38.23 | - | 2-shot | 4/16 |
    62.36 | 62.46 | 65.03 | 64.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 41.02 | 40.50 | 41.27 | 41.26 | 3/16 | - | 38.53 | 55.02 | 44.50 |
    3/16 | 37.11 | 44.38 | 37.17 | - | 3/16 | 61.85 | 63.03 | 61.88 | 61.79 |'
  prefs: []
  type: TYPE_TB
- en: '| SQA | 0-shot | 4/16 | 43.83 | 43.07 | 44.32 | 44.20 | SST | 0-shot | 4/16
    | - | 2.09 | 11.47 | 19.17 | CN | 0-shot | 4/16 | 3.35 | 1.56 | 3.41 | - | TG
    | 0-shot | 4/16 | 49.38 | 52.5 | 40.31 | 36.56 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 35.10 | 29.62 | 29.55 | 32.07 | 3/16 | - | 3.39 | 30.77 | 8.21 | 3/16
    | 3.07 | 0.06 | 1.79 | - | 3/16 | 26.72 | 20.00 | 37.03 | 8.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/16 | 48.12 | 48.39 | 49.37 | 47.24 | 3-shot | 4/16 | - | 58.28
    | 58.80 | 51.76 | 3-shot | 4/16 | 33.84 | 34.51 | 33.28 | - | 2-shot | 4/16 |
    65.31 | 65.47 | 71.25 | 75.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 40.48 | 39.14 | 39.84 | 43.61 | 3/16 | - | 57.11 | 64.93 | 65.84 |
    3/16 | 28.14 | 33.61 | 29.87 | - | 3/16 | 68.75 | 74.22 | 63.91 | 67.66 |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test | Gene.
    | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | 0-shot | 4/8 | 35.34 | 39.17 | 40.12 | 40.64 | AZ | 0-shot | 4/8 | 0.00
    | 0.87 | 0.00 | 0.01 | MN | 0-shot | 4/8 | 0.01 | 0.00 | 0.00 | - | CC | 0-shot
    | 4/8 | 0.03 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.01 | 0.01 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.01 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/8 | 28.01 | 56.13 | 55.12 | 56.59 | 3-shot | 4/8 | 54.76 | 88.36
    | 85.90 | 84.85 | 3-shot | 4/8 | 31.70 | 32.86 | 34.54 | - | 2-shot | 4/8 | 90.39
    | 65.29 | 65.08 | 87.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| AQA | 0-shot | 4/8 | 14.22 | 18.10 | 18.18 | 18.17 | DS | 0-shot | 4/8 |
    0.00 | 0.02 | 0.00 | 0.00 | AN | 0-shot | 4/8 | 0.03 | 0.00 | 0.00 | - | AC |
    0-shot | 4/8 | 0.19 | 0.19 | 0.19 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.01 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/8 | 28.01 | 28.91 | 27.96 | 29.13 | 3-shot | 4/8 | 47.01 | 50.42
    | 50.00 | 34.88 | 3-shot | 4/8 | 32.97 | 33.93 | 33.14 | - | 2-shot | 4/8 | 18.16
    | 23.71 | 53.15 | 23.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| NQA | 0-shot | 4/8 | 24.26 | 27.83 | 23.95 | 24.07 | SE | 0-shot | 4/8 |
    0.00 | 0.00 | 0.00 | 0.01 | WN | 0-shot | 4/8 | 0.00 | 0.00 | 0.00 | - | IH |
    0-shot | 4/8 | 0.09 | 0.01 | 0.00 | 0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.02 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/8 | 30.69 | 32.16 | 29.83 | 33.18 | 3-shot | 4/8 | 47.03 | 34.43
    | 43.43 | 34.27 | 3-shot | 4/8 | 47.32 | 46.70 | 47.15 | - | 2-shot | 4/8 | 61.87
    | 60.73 | 59.28 | 57.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SQA | 0-shot | 4/8 | 19.92 | 20.30 | 19.07 | 18.07 | SST | 0-shot | 4/8 |
    0.00 | 0.00 | 0.00 | 0.00 | CN | 0-shot | 4/8 | 0.00 | 0.00 | 0.00 | - | TG |
    0-shot | 4/8 | 0.63 | 0.00 | 0.16 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.31 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1-shot | 4/8 | 25.64 | 17.73 | 21.70 | 21.10 | 3-shot | 4/8 | 26.47 | 53.06
    | 55.02 | 36.90 | 3-shot | 4/8 | 26.02 | 27.19 | 14.91 | - | 2-shot | 4/8 | 58.28
    | 65.31 | 59.06 | 59.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: We evaluate *cross-dataset* distribution shift experiments on the OOD benchmark
    BOSS ([69](#bib.bib69)) in NLP. Previous work in NLP concerning OOD mostly considers
    distribution shifts from various sources, e.g. from movies to Twitter ([68](#bib.bib68)).
    GLUE-X ([65](#bib.bib65)) and BOSS ([69](#bib.bib69)) represent pioneering efforts
    in benchmarking OOD generalization in NLP. BOSS, building upon GLUE-X, improves
    by employing SimCSE scores for detection analysis and identifying dataset pairs
    exhibiting the lowest semantic similarity. These pairs are then utilized for training
    and testing, constructing a benchmark consisting of five downstream tasks. Each
    downstream task comprises an in-domain (ID) dataset and three OOD datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the generalization ability of quantized models in cross-dataset
    distribution shift experiments, we randomly sample 300 samples from the test set
    of each OOD dataset within the BOSS benchmark as its corresponding training set,
    serving as the calibration set for the quantization process. For each downstream
    task, we utilize the training set from different datasets as the calibration set
    for the quantization process and test on the corresponding I.I.D and OOD test
    sets. In our experiments, we employ LLaMA2-7B ([54](#bib.bib54)) as the target
    for quantization and selected four PTQ methods: GPTQ ([14](#bib.bib14)), AWQ ([31](#bib.bib31)),
    SpQR ([10](#bib.bib10)), and SmoothQuant ([62](#bib.bib62)). Given that there
    is not much difference in performance between excessively high bits and full precision,
    and too low a bit has already lost basic performance in these tasks, we quantize
    the model weights to 3-4 bits with SmoothQuant quantizing the activations to 8
    bits. We test two forms: 0-shot and few-shot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We present the results in Tab. [2](#S3.T2 "Table 2 ‣ 3 S2: Generalization Assessment
    of Quantized LLMs with Domain Shifts ‣ Evaluating the Generalization Ability of
    Quantized LLMs: Benchmark, Analysis, and Toolbox"). We evaluate four downstream
    tasks in BOSS: EQA, SA, NLI, and TD. Each downstream task consists of four datasets,
    with each dataset tested using four datasets as calibration set. The following
    conclusions can be observed:'
  prefs: []
  type: TYPE_NORMAL
- en: For datasets with poor performance or even close to zero, few-shot learning
    significantly improves the performance. For the EQA task with both 4-bit and 3-bit
    quantization and the SA task with 4-bit quantization, where satisfactory performance
    can be achieved, there is a relatively slight improvement with few-shot learning
    compared to 0-shot. However, for the SA task with 3-bit quantization, the NLI
    task with both 4-bit and 3-bit quantization and the TD task with 4-bit and 3-bit
    quantization with poor performance, few-shot learning shows a qualitative leap
    compared to 0-shot. Especially on some datasets where the 0-shot performance is
    nearly zero, few-shot learning achieves accuracy ranging from 80% to 90%. This
    indicates that LLM can benefit from the examples provided to solve complex tasks,
    yielding significant improvements ([73](#bib.bib73)). However, for quantized models
    with severe performance degradation, such as those quantized to 3 bits using SmoothQuant,
    few-shot learning cannot improve the quantized model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: For the same test dataset, it’s not necessarily the case that using I.I.D dataset
    as calibration set yield superior performance; rather, there exist one or more
    datasets that demonstrate enhanced performance when used as calibration set. Across
    the same test dataset, the variance in performance when using different datasets
    as calibration set can be substantial, differing by as much as 70%. Counterintuitively,
    the overlap between background-colored and bolded data is not high, indicating
    that using I.I.D datasets as calibration sets does not necessarily result in higher
    performance. Instead, for each task, there are one or more datasets for which
    performance improves when used as the calibration set, and this characteristic
    is method-dependent. For EQA task, when quantized using the GPTQ and SpQR, the
    performance using NQA and SQA as calibration set generally exceeds that of SQ
    and AQA. For SA task, when quantized using the GPTQ method, performance significantly
    improves when using AZ and SST as calibration set compared to SE and DS. For NLI
    task, all methods maintain decent performance when using the MN dataset as the
    calibration set. For TD task, when quantified using the GPTQ method, performance
    consistently outperforms other datasets when TG is used as the calibration set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Cross-dataset distribution shift in Chinese domain specific task.
    To save space, abbreviations are used for datasets. Each row presents the 0-shot
    and 5-shot experimental results using different datasets as calibration sets on
    the same test dataset. Results with colored backgrounds indicate I.I.D results,
    while those without color represent OOD results. The higher the metric, the better
    the performance. Bold results indicate the best performance on the same test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method |  |  | 0-shot | 5-shot |  |  | 0-shot | 5-shot |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | 4/16 | 39.4 | 37.9 | 53.2 | 52.1 | CM-HM | 4/16 | 50.0 | 50.7 | 59.1
    | 59.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 30.0 | 28.0 | 38.1 | 41.9 | 3/16 | 32.3 | 30.6 | 52.4 | 54.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 25.1 | 24.4 | 23.9 | 23.4 | 2/16 | 25.3 | 23.7 | 25.9 | 24.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | 4/16 | 36.9 | 35.4 | 58.8 | 57.5 | CM-SS | 4/16 | 53.9 | 54.0 | 63.1
    | 63.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 34.6 | 30.3 | 51.9 | 47.5 | 3/16 | 32.8 | 34.3 | 55.4 | 54.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 25.1 | 23.9 | 25.9 | 24.7 | 2/16 | 25.7 | 26.2 | 25.6 | 25.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | 4/16 | 30.4 | 26.0 | 41.8 | 39.2 | CM-ST | 4/16 | 39.3 | 35.2 | 43.1
    | 43.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 28.1 | 25.7 | 33.9 | 35.5 | 3/16 | 29.9 | 25.7 | 38.6 | 37.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 24.6 | 25.4 | 24.5 | 25.0 | 2/16 | 26.2 | 25.7 | 24.5 | 25.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | 4/16 | 38.5 | 36.3 | 53.8 | 52.5 | CM-HM | 4/16 | 52.9 | 49.3 | 59.0
    | 59.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 36.0 | 34.6 | 47.9 | 46.6 | 3/16 | 49.5 | 38.1 | 57.1 | 56.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 30.1 | 30.9 | 37.4 | 34.5 | 2/16 | 39.3 | 26.0 | 47.5 | 46.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | 4/16 | 38.2 | 38.9 | 60.0 | 57.7 | CM-SS | 4/16 | 54.8 | 54.3 | 63.8
    | 64.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 39.8 | 34.7 | 56.1 | 53.3 | 3/16 | 52.8 | 51.1 | 59.4 | 60.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 30.1 | 32.1 | 39.5 | 37.3 | 2/16 | 38.8 | 39.7 | 44.2 | 47.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | 4/16 | 32.2 | 30.3 | 41.5 | 41.1 | CM-ST | 4/16 | 40.4 | 39.5 | 43.7
    | 43.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 31.1 | 28.4 | 37.5 | 37.8 | 3/16 | 37.4 | 37.8 | 40.8 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 27.8 | 27.7 | 32.2 | 30.6 | 2/16 | 31.8 | 31.9 | 35.9 | 35.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | 4/16 | 36.5 | 35.6 | 47.7 | 49.0 | CM-HM | 4/16 | 47.8 | 53.2 | 58.5
    | 58.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 26.7 | 29.7 | 41.1 | 40.8 | 3/16 | 42.6 | 50.5 | 48.0 | 49.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 24.2 | 24.3 | 24.0 | 23.3 | 2/16 | 25.9 | 42.4 | 25.8 | 23.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | 4/16 | 32.2 | 34.9 | 57.5 | 56.7 | CM-SS | 4/16 | 51.3 | 52.4 | 62.2
    | 61.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 32.6 | 31.5 | 42.7 | 40.5 | 3/16 | 40.1 | 42.1 | 50.5 | 50.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 24.8 | 25.0 | 24.9 | 25.7 | 2/16 | 24.8 | 24.9 | 24.8 | 24.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | 4/16 | 26.6 | 29.4 | 39.1 | 38.6 | CM-ST | 4/16 | 36.7 | 35.3 | 41.0
    | 41.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 26.2 | 27.1 | 31.9 | 34.0 | 3/16 | 31.7 | 31.7 | 36.3 | 35.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 25.1 | 24.9 | 25.7 | 25.2 | 2/16 | 24.6 | 24.6 | 24.1 | 24.5 |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  prefs: []
  type: TYPE_TB
- en: '| CE-HM | 4/8 | 27.2 | 27.2 | 24.7 | 24.5 | CM-HM | 4/8 | 31.6 | 29.8 | 29.4
    | 27.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 25.5 | 25.5 | 24.9 | 23.9 | 3/8 | 24.7 | 24.8 | 25.3 | 23.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/8 | 27.1 | 24.2 | 25.5 | 24.2 | 2/8 | 24.1 | 25.5 | 24.8 | 25.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  prefs: []
  type: TYPE_TB
- en: '| CE-SS | 4/8 | 27.4 | 26.7 | 24.4 | 24.5 | CM-SS | 4/8 | 33.1 | 28.2 | 28.7
    | 25.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 26.1 | 25.0 | 26.2 | 24.4 | 3/8 | 25.0 | 25.1 | 24.7 | 24.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/8 | 26.6 | 25.1 | 25.3 | 23.3 | 2/8 | 24.3 | 25.3 | 25.2 | 25.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  prefs: []
  type: TYPE_TB
- en: '| CE-ST | 4/8 | 32.2 | 26.2 | 25.5 | 23.9 | CM-ST | 4/8 | 28.2 | 27.7 | 26.9
    | 43.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 31.1 | 27.4 | 24.8 | 25.6 | 3/8 | 25.4 | 24.2 | 24.4 | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/8 | 27.8 | 26.8 | 24.9 | 26.8 | 2/8 | 24.8 | 24.9 | 24.6 | 35.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Cross-subject distribution shift in Chinese domain-specific task.
    To save space, abbreviations are used for datasets. Each row presents the experimental
    results using different datasets as calibration sets on the same test dataset.
    Results with colored backgrounds indicate I.I.D results, while those without color
    represent OOD results. The higher the metric, the better the performance. Bold
    results indicate the best performance on the same test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Meth. | Test | Gene. | W/A | Gene. | Test | Gene. | W/A | Gene. | Test |
    Gene. | W/A | Gene. |'
  prefs: []
  type: TYPE_TB
- en: '| HM | SS | ST | HM | SS | ST | HM | SS | ST |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | HM | 0-shot | 4/16 | 39.4 | 36.4 | 37.6 | SS | 0-shot | 4/16 | 38.8
    | 36.9 | 38.9 | ST | 0-shot | 4/16 | 30.4 | 28.4 | 30.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 30.0 | 30.5 | 29.2 | 3/16 | 29.6 | 34.6 | 30.4 | 3/16 | 25.9 | 28.3
    | 28.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 25.1 | 24.1 | 26.2 | 2/16 | 27.3 | 25.1 | 25.2 | 2/16 | 24.9 | 24.8
    | 24.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 4/16 | 53.2 | 52.9 | 52.2 | 5-shot | 4/16 | 58.9 | 58.8 | 60.1 |
    5-shot | 4/16 | 40.9 | 40.4 | 41.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 38.1 | 43.5 | 39.9 | 3/16 | 42.5 | 51.9 | 48.2 | 3/16 | 29.7 | 34.1
    | 33.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 23.9 | 26.2 | 23.7 | 2/16 | 24.3 | 25.9 | 24.6 | 2/16 | 27.3 | 25.1
    | 24.5 |'
  prefs: []
  type: TYPE_TB
- en: '| SpQR | HM | 0-shot | 4/16 | 38.5 | 38.0 | 40.9 | SS | 0-shot | 4/16 | 39.3
    | 38.2 | 41.3 | ST | 0-shot | 4/16 | 30.3 | 29.9 | 32.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 36.0 | 39.0 | 38.9 | 3/16 | 34.8 | 39.8 | 39.0 | 3/16 | 30.5 | 29.1
    | 31.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 30.1 | 29.9 | 29.2 | 2/16 | 28.7 | 30.1 | 30.6 | 2/16 | 26.1 | 26.6
    | 27.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 4/16 | 53.8 | 51.0 | 52.6 | 5-shot | 4/16 | 59.3 | 60.0 | 59.6 |
    5-shot | 4/16 | 41.4 | 41.0 | 41.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 47.9 | 45.8 | 46.5 | 3/16 | 52.8 | 56.1 | 53.0 | 3/16 | 36.5 | 37.6
    | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 37.4 | 35.0 | 37.7 | 2/16 | 40.6 | 39.5 | 45.0 | 2/16 | 28.3 | 28.0
    | 32.2 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | HM | 0-shot | 4/16 | 36.5 | 34.2 | 33.4 | SS | 0-shot | 4/16 | 35.2
    | 32.2 | 31.4 | ST | 0-shot | 4/16 | 28.5 | 28.5 | 26.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 26.7 | 32.1 | 27.5 | 3/16 | 28.3 | 32.6 | 28.2 | 3/16 | 27.7 | 28.9
    | 26.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 24.2 | 24.2 | 24.6 | 2/16 | 24.9 | 24.8 | 25.2 | 2/16 | 24.9 | 24.8
    | 25.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 4/16 | 47.7 | 49.7 | 51.2 | 5-shot | 4/16 | 53.4 | 57.5 | 56.6 |
    5-shot | 4/16 | 37.7 | 38.5 | 39.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 41.1 | 38.4 | 37.4 | 3/16 | 44.0 | 42.7 | 38.7 | 3/16 | 31.9 | 31.0
    | 31.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/16 | 24.0 | 24.6 | 23.8 | 2/16 | 23.9 | 24.9 | 25.1 | 2/16 | 25.2 | 25.3
    | 25.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SQ | HM | 0-shot | 4/8 | 27.2 | 28.9 | 27.4 | SS | 0-shot | 4/8 | 28.3 |
    27.4 | 28.2 | ST | 0-shot | 4/8 | 26.8 | 28.0 | 25.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 25.5 | 23.9 | 26.4 | 3/8 | 26.4 | 26.1 | 25.5 | 3/8 | 26.6 | 25.2 |
    26.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/8 | 27.1 | 25.2 | 24.8 | 2/8 | 26.2 | 26.6 | 26.4 | 2/8 | 26.4 | 26.4 |
    25.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 4/8 | 24.7 | 24.2 | 24.9 | 5-shot | 4/8 | 26.0 | 24.4 | 24.3 | 5-shot
    | 4/8 | 24.8 | 24.3 | 25.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3/8 | 24.9 | 26.4 | 26.2 | 3/8 | 24.7 | 26.2 | 25.9 | 3/8 | 26.8 | 25.3 |
    24.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2/8 | 25.5 | 26.4 | 24.2 | 2/8 | 26.5 | 25.3 | 24.9 | 2/8 | 26.6 | 26.8 |
    24.9 |'
  prefs: []
  type: TYPE_TB
- en: Chinese Domain-specific Tasks. We evaluate *cross-dataset* distribution shift
    experiments and *cross-subject* distribution shift experiments on the Chinese
    domain-specific datasets C-EVAL ([20](#bib.bib20)) and CMMLU ([29](#bib.bib29)).
    C-EVAL serves as a comprehensive benchmark for evaluating Chinese LLM. It consists
    of 13,948 multiple-choice questions covering 52 different subjects categorized
    into Humanities, Social Sciences, STEM, and Other. CMMLU is another Chinese evaluation
    dataset designed specifically to assess the advanced knowledge and reasoning abilities
    of LLM in the context of the Chinese language and culture. It encompasses 67 different
    subjects categorized into Humanities, Social Sciences, STEM, and Chinese specific
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both C-EVAL and CMMLU, two Chinese-specific domain datasets, include Humanities,
    Social Sciences, and STEM three subject categories. We design cross-dataset distribution
    shift experiments based on the same subject categories. For each subject test,
    we respectively utilize the corresponding subjects from C-EVAL and CMMLU as calibration
    set to assess the impact of different datasets as calibration set on the test
    results. Additionally, we conducted cross-subject distribution shift experiments
    on the C-EVAL dataset. For each subject test, we use Humanities, Social Sciences,
    and STEM as calibration set to evaluate the influence of different subject subsets
    as calibration set on the test results. Since both C-EVAL and CMMLU lack training
    datasets, we used the validation dataset of C-EVAL as the training dataset and
    randomly sampled 300 samples from the test dataset of CMMLU as the training dataset.
    We utilize the Chinese LLM Baichuan2-7B-Base ([64](#bib.bib64)) as the quantization
    target and selecte four PTQ methods: GPTQ ([14](#bib.bib14)), AWQ ([31](#bib.bib31)),
    SpQR ([10](#bib.bib10)), and SmoothQuant ([62](#bib.bib62)). We quantize the weights
    to 2-4 bits, with SmoothQuant quantizing the activations to 8 bits, and test both
    0-shot and 5-shot forms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of cross-dataset distribution shift experiments on C-EVAL and CMMLU
    are presented in Tab. [3](#S3.T3 "Table 3 ‣ 3 S2: Generalization Assessment of
    Quantized LLMs with Domain Shifts ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox"). We observe that performance generally
    improves when using I.I.D datasets as calibration set, while performance tends
    to degrade when using OOD datasets as calibration set. This contrasts with our
    findings in OOD Benchmark BOSS, suggesting that there is not a golden dataset
    that consistently improves test accuracy when used as a calibration set. The inconsistency
    in conclusions may stem from the fact that the distribution shift experiment in
    this setting is slightly more challenging compared to the distribution shift experiment
    tested on the BOSS dataset. The distribution differences among datasets in the
    BOSS benchmark are relatively small, so higher-quality datasets may result in
    higher accuracy for the quantized model. Additionally, the subjects included in
    the same subject category in C-EVAL and CMMLU are not entirely consistent, and
    the distribution differences within the same subject between the two datasets
    may be larger. In cases of greater distribution disparity, using I.I.D datasets
    as calibration set may lead to better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The results of cross-subject distribution shift experiments on C-EVAL are presented
    in Tab. LABEL:tab:cds_subject. The results tend to be more random, and no conclusion
    can be drawn that using any particular dataset as calibration set or an I.I.D
    dataset as calibration set results in higher test accuracy. Cross-subject distribution
    shift is significantly more challenging compared to previous cross-dataset distribution
    shifts. This is because, in previous settings, different datasets are from the
    same task type or domain, whereas the cross-subject distribution shift experiments
    on C-EVAL directly span from one domain to another. This may cause the quantized
    model to fail in obtaining accurate quantization parameters from the calibration
    set, ultimately leading to poor performance or unpredictable results.
  prefs: []
  type: TYPE_NORMAL
- en: '4 MI-optimize: A LLM Quantization Toolbox'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overview. MI-optimize is a versatile tool designed for the quantization and
    evaluation of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eeecb1a6d0175c30f840f3676fba0bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of the Quantization and Evaluation Framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The library’s seamless integration of various quantization methods and evaluation
    techniques empowers users to customize their approaches according to specific
    requirements and constraints, providing a high level of flexibility. Although
    LLMs excel in various NLP tasks, their computational and memory demands may limit
    their deployment in real-time applications and on resource-constrained devices.
    MI-optimize addresses this challenge by employing quantization techniques to compress
    these models, ensuring they maintain performance while remaining adaptable to
    a wide range of scenarios. Fig. [3](#S4.F3 "Figure 3 ‣ 4 MI-optimize: A LLM Quantization
    Toolbox ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark,
    Analysis, and Toolbox") illustrates the framework of MI-optimize, which comprises
    five main modules: the Configuration, Quantization, Evaluation, Inference, and
    Execution modules.'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental Setup and Results. To validate the framework’s capability of combining
    mixed quantization methods, we conduct experiments using the LLaMA-2-7B model ([54](#bib.bib54)).
    We test the model using SmoothQuant and a combination of SmoothQuant for activations
    and GPTQ for weight quantization on WikiText-2 (Wiki2) ([37](#bib.bib37)), Penn
    Treebank (PTB) ([35](#bib.bib35)), and C4 ([48](#bib.bib48)) datasets, and measure
    the perplexity (PPL) of the quantized models. Quantization is implemented using
    PyTorch. All quantization experiments are exclusively conducted on the LLaMA-2-7B
    model, utilizing a single NVIDIA V100 GPU. For calibration, we utilize a dataset
    consisting of 128 random segments, each containing 512 tokens, extract from the
    C4 dataset. These segments represent generic text data, sourced from randomly
    crawled websites, ensuring that the quantization process does not rely on task-specific
    information. Our quantization setup employ SmoothQuant with default activation
    quantization of 8 bits. We utilize groupwise quantization with a group size of
    128.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Perplexity (PPL) of the LLaMA-2-7B model using SmoothQuant and a combination
    of SmoothQuant for activations and GPTQ for weight quantization on the WikiText-2
    (Wiki2), Penn Treebank (PTB), and C4 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | W/A | Wiki2 | C4 | PTB |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 16/16 | 5.47 | 37.92 | 7.22 |'
  prefs: []
  type: TYPE_TB
- en: '| Smoothquant | 8/8 | 19.70 | 3026.75 | 11.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Smoothquant+GPTQ | 8/8 | 21.18 | 3110.05 | 11.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Smoothquant | 4/8 | 34.87 | 5133.82 | 20.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Smoothquant+GPTQ | 4/8 | 22.95 | 1359.59 | 13.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Smoothquant | 3/8 | 24041.06 | 42625.86 | 29585.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Smoothquant+GPTQ | 3/8 | 290.77 | - | 231.02 |'
  prefs: []
  type: TYPE_TB
- en: 'The results presented in Tab. [5](#S4.T5 "Table 5 ‣ 4 MI-optimize: A LLM Quantization
    Toolbox ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark,
    Analysis, and Toolbox") indicate several key findings. Comparing SmoothQuant with
    SmoothQuant + GPTQ configurations, it is evident that the latter consistently
    outperforms the former across all bit-width settings. This suggests that the combined
    use of SmoothQuant and GPTQ leads to a notable improvement in model performance.
    Particularly, at bit-widths of 4 and 3, the SmoothQuant + GPTQ method demonstrates
    a significant reduction in perplexity compared to SmoothQuant alone, indicating
    the pronounced effectiveness of GPTQ in reducing perplexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization of LLMs. Quantization techniques for LLMs mainly include Post-Training
    Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require
    retraining the model and is typically suitable for situations with limited computational
    resources ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31); [62](#bib.bib62);
    [6](#bib.bib6); [66](#bib.bib66); [51](#bib.bib51)). QAT simulates the effects
    of quantization throughout the entire training process, enabling the model to
    adapt to low-precision representations during training, which typically leads
    to higher performance ([34](#bib.bib34); [9](#bib.bib9)). It’s worth noting that
    in this paper, we consider applying quantization directly on the pretrained LLMs
    instead of performing quantization-aware finetuning for the quantized LLMs (such
    as variants of QLoRA ([9](#bib.bib9); [67](#bib.bib67); [63](#bib.bib63))) because
    the latter typically needs the former for initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of quantized LLMs. Numerous studies have undertaken evaluations of
    the performance of quantized LLMs ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31);
    [62](#bib.bib62); [6](#bib.bib6); [21](#bib.bib21); [61](#bib.bib61); [30](#bib.bib30);
    [33](#bib.bib33); [23](#bib.bib23); [19](#bib.bib19)). The majority of assessments
    employ fixed calibration set, primarily focusing on language modeling tasks ([48](#bib.bib48);
    [35](#bib.bib35); [37](#bib.bib37)) and standard NLP tasks ([71](#bib.bib71);
    [44](#bib.bib44); [53](#bib.bib53); [7](#bib.bib7); [50](#bib.bib50); [38](#bib.bib38);
    [39](#bib.bib39)). Certain investigations have deviated from the practice of using
    fixed calibration set, extending them to encompass a broader spectrum of crawled
    web text and pre-training data, while also conducting multiple random samplings
    for calibration set selection ([61](#bib.bib61)). Additionally, certain studies
    have conducted assessments encompassing a broader array of downstream task types
    and datasets, approaching the evaluation from various angles ([33](#bib.bib33);
    [21](#bib.bib21); [30](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We investigated the generalization ability of quantized LLMs, proposing two
    evaluation scenarios and testing them on our own implemented platform. Drawing
    from our evaluation results, we found some underutilized datasets that exhibit
    quantization performance that deviates from conventional expectations. These findings
    warrant further investigation to elucidate the underlying mechanisms and optimize
    quantization strategies for such datasets. Our work unveils the significant role
    of distribution discrepancies between calibration and test data for quantization.
    We uncover the existence of cross-dataset optimal calibration data for specific
    tasks, prompting the development of novel methods for optimizing calibration data
    collection, which is overlooked in the current field of model quantization. Lastly,
    we provided a modular and scalable toolbox to this topic to facilitate future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(2) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi,
    and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving
    with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(3) Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus
    Stenetorp. Beat the ai: Investigating adversarial human annotation for reading
    comprehension. Transactions of the Association for Computational Linguistics,
    8:662–678, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (4) Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
    Nuanced metrics for measuring unintended bias with real data for text classification.
    In Companion proceedings of the 2019 world wide web conference, pages 491–500,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (5) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
    Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
    Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(6) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (7) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(8) Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. Mutual: A dataset
    for multi-turn dialogue reasoning. arXiv preprint arXiv:2004.04494, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(9) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(10) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(11) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    and Jie Tang. Glm: General language model pretraining with autoregressive blank
    infilling. arXiv preprint arXiv:2103.10360, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(12) Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik,
    and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search
    engine. arXiv preprint arXiv:1704.05179, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(13) Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn
    Seybolt, Munmun De Choudhury, and Diyi Yang. Latent hatred: A benchmark for understanding
    implicit hate speech. arXiv preprint arXiv:2109.05322, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(14) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (15) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.
    A framework for few-shot language model evaluation. Version v0\. 0.1\. Sept, page 8,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (16) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.
    A framework for few-shot language model evaluation. Version v0\. 0.1\. Sept, page 8,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (17) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(18) Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar
    Ray, and Ece Kamar. Toxigen: A large-scale machine-generated dataset for adversarial
    and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (19) Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen,
    Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized
    llama3 models? an empirical study. arXiv preprint arXiv:2404.14047, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(20) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun
    Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level
    multi-discipline chinese evaluation suite for foundation models. Advances in Neural
    Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(21) Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei
    Yang. Compressing llms: The truth is rarely pure and never simple. arXiv preprint
    arXiv:2310.01382, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(22) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua
    Lu. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint
    arXiv:1909.06146, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (23) Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and
    Deyi Xiong. A comprehensive evaluation of quantization strategies for large language
    models. arXiv preprint arXiv:2402.16775, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (24) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
    Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws
    for neural language models. arXiv preprint arXiv:2001.08361, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(25) Yuta Koreeda and Christopher D Manning. Contractnli: A dataset for document-level
    natural language inference for contracts. arXiv preprint arXiv:2110.01799, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (26) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
    large language model serving with pagedattention. In Proceedings of the 29th Symposium
    on Operating Systems Principles, pages 611–626, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(27) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race:
    Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (28) Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema
    challenge. In Thirteenth international conference on the principles of knowledge
    representation and reasoning, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(29) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong,
    Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding
    in chinese. arXiv preprint arXiv:2306.09212, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (30) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen
    Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language
    models. arXiv preprint arXiv:2402.18158, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(31) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(32) Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. Wanli: Worker
    and ai collaboration for natural language inference dataset creation. arXiv preprint
    arXiv:2201.05955, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(33) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang
    Li, Bolin Ding, and Ji-Rong Wen. Do emergent abilities exist in quantized large
    language models: An empirical study. arXiv preprint arXiv:2307.08072, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(34) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(35) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
    Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating
    predicate argument structure. In Human Language Technology: Proceedings of a Workshop
    held at Plainsboro, New Jersey, March 8-11, 1994, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(36) Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding
    rating dimensions with review text. In Proceedings of the 7th ACM conference on
    Recommender systems, pages 165–172, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (37) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (38) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit
    of armor conduct electricity? a new dataset for open book question answering.
    arXiv preprint arXiv:1809.02789, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (39) Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv
    Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation
    framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (40) Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart
    Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization.
    arXiv preprint arXiv:2106.08295, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(41) Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin
    Stoyanov. Semeval-2016 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.01973,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(42) Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. Crows-pairs:
    A challenge dataset for measuring social biases in masked language models. arXiv
    preprint arXiv:2010.00133, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(43) Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and
    Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding.
    arXiv preprint arXiv:1910.14599, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(44) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.
    The lambada dataset: Word prediction requiring a broad discourse context. arXiv
    preprint arXiv:1606.06031, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(45) Anselmo Peñas, Eduard Hovy, Pamela Forner, Álvaro Rodrigo, Richard Sutcliffe,
    and Roser Morante. Qa4mre 2011-2013: Overview of question answering for machine
    reading evaluation. In Information Access Evaluation. Multilinguality, Multimodality,
    and Visualization: 4th International Conference of the CLEF Initiative, CLEF 2013,
    Valencia, Spain, September 23-26, 2013\. Proceedings 4, pages 303–320\. Springer,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(46) Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent:
    A dynamic benchmark for sentiment analysis. arXiv preprint arXiv:2012.15349, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (47) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
    et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (48) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(49) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad:
    100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(50) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(51) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (52) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,
    Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality
    over a sentiment treebank. In Proceedings of the 2013 conference on empirical
    methods in natural language processing, pages 1631–1642, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(53) Sandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein
    data sets. In 15th International Conference on Scientific and Statistical Database
    Management, 2003., pages 141–150\. IEEE, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(54) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(55) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,
    Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv
    preprint arXiv:1611.09830, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (56) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(57) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
    Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural
    language understanding. arXiv preprint arXiv:1804.07461, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(58) Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng,
    Sheng-Fu Wang, and Samuel R Bowman. Blimp: The benchmark of linguistic minimal
    pairs for english. Transactions of the Association for Computational Linguistics,
    8:377–392, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (59) Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple
    choice science questions. arXiv preprint arXiv:1707.06209, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (60) Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge
    corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (61) Miles Williams and Nikolaos Aletras. How does calibration data affect the
    post-training pruning and quantization of large language models? arXiv preprint
    arXiv:2311.09755, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(62) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(63) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(64) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin,
    Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language
    models. arXiv preprint arXiv:2309.10305, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(65) Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu,
    Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding
    models from an out-of-distribution generalization perspective. arXiv preprint
    arXiv:2211.08073, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(66) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(67) Ke Yi, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, and Jia
    Li. One quantllm for all: Fine-tuning quantized llms once for efficient deployments.
    arXiv preprint arXiv:2405.20202, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (68) Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, and Peng Cui. A survey
    on evaluation of out-of-distribution generalization. arXiv preprint arXiv:2403.01874,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(69) Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi
    Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness
    in nlp: Benchmarks, analysis, and llms evaluations. Advances in Neural Information
    Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(70) Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale
    adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(71) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(72) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (73) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
    Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large
    language models. arXiv preprint arXiv:2303.18223, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(74) Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. " going on a vacation"
    takes longer than" going for a walk": A study of temporal commonsense understanding.
    arXiv preprint arXiv:1909.03065, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (75) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model
    compression for large language models. arXiv preprint arXiv:2308.07633, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A More Details of MI-optimize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fig. [3](#S4.F3 "Figure 3 ‣ 4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating
    the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")
    illustrates the framework of MI-optimize, which comprises five main modules: the
    Configuration, Quant, Evaluation, Inference, and Execution modules. Combining
    these modules forms a cohesive pipeline that provides researchers with a reliable
    experimental environment, with each module responsible for a specific step in
    the pipeline. The subsequent sections will provide a detailed description of the
    implementation of each module.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration Module: Manages all parameters involved in the framework, including
    default settings, quantization configurations, and evaluation configurations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Module: Contains various pre-trained models such as LLaMA [[54](#bib.bib54)],
    Baichuan [[64](#bib.bib64)], ChatGLM [[11](#bib.bib11)], and custom user models.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dataset Module: Handles different datasets, including Chinese domain-specific
    datasets (e.g., C-EVAL [[20](#bib.bib20)] and CMMLU [[29](#bib.bib29)]), the BOSS
    benchmark [[69](#bib.bib69)], general datasets (e.g., Amazon reviews, Dynasent),
    and LM-EVAL datasets (e.g., Winogrande, WSC273).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quant Module: Responsible for loading pre-trained models, applying various
    quantization methods (e.g., GPTQ [[14](#bib.bib14)], AWQ [[31](#bib.bib31)], SPQR [[10](#bib.bib10)]),
    and performing the actual model quantization.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inference & Eval Module: Exports the quantized model, runs inference using
    engines such as VLLM [[26](#bib.bib26)] and TensorRT, and evaluates benchmark
    performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Execution Module: Oversees the primary tasks of model quantization, benchmarking,
    and the combined process of quantization and evaluation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Key Features Supported by MI-optimize.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantization of LLMs to reduce computational and memory requirements: MI-optimize
    focuses on reducing the computational and memory footprint of large language models
    through advanced quantization techniques, making them more suitable for deployment
    in resource-limited environments.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for various quantization algorithms: The framework supports a wide
    range of quantization algorithms, including RTN, GPTQ [[14](#bib.bib14)], AWQ [[31](#bib.bib31)],
    SpQR [[10](#bib.bib10)], ZeroQuant [[66](#bib.bib66)], SmoothQuant [[62](#bib.bib62)],
    QuIP [[6](#bib.bib6)], and FP8\. This flexibility allows users to choose the most
    appropriate method for their specific use case, optimizing performance and resource
    usage.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluation on OOD tasks using benchmarks: MI-optimize includes tools for evaluating
    quantized models on out-of-distribution (OOD) tasks using established benchmarks
    such as BOSS. This ensures that the models maintain their performance even when
    encountering data that differs from their training set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for multiple datasets: The framework supports multiple datasets for
    both calibration and testing purposes. Users can also incorporate custom datasets
    to better align the model’s performance with their specific requirements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Command-line interface for easy integration and automation: MI-optimize provides
    a command-line interface that facilitates easy integration into existing workflows
    and automation of the quantization and evaluation processes, streamlining the
    deployment pipeline.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for combination of quantization methods: The framework allows for the
    combination of different quantization methods within the same model. Different
    layers can apply different quantization algorithms, and even multiple quantization
    algorithms can be applied to the same layer. This granular control helps optimize
    model performance and efficiency.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ease of adding new quantization algorithms: Researchers can easily add new
    quantization algorithms to the MI-optimize repository. This modularity ensures
    that the framework remains up-to-date with the latest advancements in quantization
    techniques.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Customer tools for model quantization and evaluation: Customers can install
    the tools provided by MI-optimize to quantize and evaluate their own models. This
    empowers users to tailor the framework to their specific needs, ensuring optimal
    model performance in their applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix B Limitation and Future works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite comprehensive evaluation on over 50 datasets, our study acknowledges
    the need for a more thorough assessment of models and quantization algorithms.
    Future work could involve a more extensive evaluation framework. Additionally,
    the developed toolbox does not yet support all quantization algorithms and large
    models. Further development is warranted to expand its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present all the datasets utilized in the experiments, encompassing
    their evaluated tasks and abilities, assessment metrics, and dataset sizes. Tab. [6](#A3.T6
    "Table 6 ‣ C.2 Datasets in S2 ‣ Appendix C Datasets ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") and [7](#A3.T7 "Table
    7 ‣ C.2 Datasets in S2 ‣ Appendix C Datasets ‣ Evaluating the Generalization Ability
    of Quantized LLMs: Benchmark, Analysis, and Toolbox") provide a comprehensive
    summary of all the datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Datasets in S1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Common sense reasoning. WinoGrande [[50](#bib.bib50)] is a large-scale coreference
    resolution task dataset derived from extensive internet text, aimed at addressing
    ambiguous and complex coreference relationships. WSC273 [[28](#bib.bib28)] comprises
    273 coreference resolution problems derived from the classic Winograd Schema Challenge,
    primarily assessing the common-sense reasoning capabilities of natural language
    understanding systems. GLUE-WNLI [[57](#bib.bib57)] is designed to test coreference
    resolution capability, which involves determining which noun a pronoun in a sentence
    refers to. It is sourced from the Winograd Schema Challenge. HellaSwag [[71](#bib.bib71)]
    is generated from web videos and Wikipedia articles and is used to infer the most
    suitable continuation for text segments in multiple-choice tasks. SWAG [[70](#bib.bib70)]
    is generated based on video descriptions, aiming to predict plausible subsequent
    scenarios for video events. PIQA [[53](#bib.bib53)] is a dataset for reasoning
    about physical common sense, derived from physics problems and solutions, designed
    to evaluate algorithms’ reasoning abilities in physical environments.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical reasoning. MathQA [[2](#bib.bib2)] is collected from the MathQA
    website, consisting of 37,200 mathematical questions, with the task being to automatically
    answer mathematical questions.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-turn dialogue reasoning. MuTual [[8](#bib.bib8)] and Mutual_plus [[8](#bib.bib8)]
    is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified
    from Chinese high school English listening comprehension test data.
  prefs: []
  type: TYPE_NORMAL
- en: Bias diagnosis and mitigation. CrowS-Pairs [[42](#bib.bib42)] is derived from
    a wide range of internet text and is designed to evaluate social biases in language
    models. Toxigen [[18](#bib.bib18)] is for implicit hate speech detection.
  prefs: []
  type: TYPE_NORMAL
- en: Scientific knowledge question answering. PubMedQA [[22](#bib.bib22)] is a biomedical
    question answering dataset sourced from PubMed articles, aimed at evaluating systems’
    understanding and answering capabilities of biomedical texts. OpenBookQA [[38](#bib.bib38)]
    is a new kind of question-answering dataset modeled after open book exams for
    assessing human understanding of a subject. It originates from open science education
    resources. SciQ [[59](#bib.bib59)] is a high-quality, science-themed multiple-choice
    dataset constructed manually. ARC-Easy [[7](#bib.bib7)] originates from science
    exams administered in American elementary through high schools, assessing fundamental
    scientific knowledge. ARC-Challenge [[7](#bib.bib7)] presents challenging scientific
    questions aimed at testing higher-level scientific comprehension and reasoning
    abilities. MC-TACO [[74](#bib.bib74)] consists of temporal common-sense questions
    sourced from a wide range of internet texts, designed for temporal common-sense
    reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Reading comprehension. RACE [[27](#bib.bib27)] is a large-scale reading comprehension
    dataset sourced from English exams for Chinese middle school and high school students,
    aimed at testing reading comprehension abilities. QA4MRE [[45](#bib.bib45)] is
    created for the CLEF 2011/2012/2013 shared tasks, aimed at testing cross-domain
    reading comprehension abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language inference. GLUE-MNLI [[57](#bib.bib57)] is a natural language
    inference dataset comprising pairs of sentences sourced from various text genres
    such as novels, telephone conversations, and news articles. GLUE-MNLI-Mismatched [[57](#bib.bib57)]
    is utilized to evaluate the generalization capability of models on unseen text
    genres, with sentence pairs sourced from the same origins as GLUE-MNLI. GLUE-RTE [[57](#bib.bib57)]
    is sourced from news reports and Wikipedia. GLUE-QNLI [[57](#bib.bib57)] originates
    from the Stanford University’s SQuAD dataset. ANLI [[43](#bib.bib43)] is a large-scale
    adversarial natural language inference dataset divided into three difficulty levels.
    It is constructed by employing adversarial search techniques to generate challenging
    questions based on human annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis. GLUE-SST [[57](#bib.bib57)] is sourced from movie reviews,
    and its task involves sentiment classification, which entails determining the
    emotional inclination of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax phenomena evaluation. BLiMP [[58](#bib.bib58)] is a challenge set for
    evaluating what language models know about major grammatical phenomena in English.
    BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating
    specific contrasts in syntax, morphology, or semantics. The data is automatically
    generated according to expert-crafted grammars.
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Datasets in S2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Extractive question answering in BOSS. SQuAD [[49](#bib.bib49)] is a collection
    of question-answer pairs derived from Wikipedia articles. AdversarialQA [[3](#bib.bib3)]
    formulates adversarial questions within the SQuAD context, utilizing a collaborative
    process involving both human annotators and models. NewsQA [[55](#bib.bib55)]
    crafts questions based on CNN news articles, each demanding reasoning for answers,
    rather than relying solely on lexical overlap and textual entailment. SearchQA [[12](#bib.bib12)]
    employs a reverse construction approach, utilizing the Google search engine to
    fetch pertinent contexts for each question-answer pair from the J!Archive website.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis in BOSS. Amazon [[36](#bib.bib36)] is a dataset comprising
    reviews across 29 distinct product categories from the Amazon website. DynaSent [[46](#bib.bib46)]
    constructs a dataset by identifying challenging sentences from existing collections
    and generating adversarial counterparts through human-and-model collaborative
    annotation. SemEval [[41](#bib.bib41)] offers a three-class sentiment analysis
    dataset centered on Twitter content. SST [[52](#bib.bib52)] features sentence-level
    movie reviews sourced from the Rotten Tomatoes website.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language inference in BOSS. MNLI [[60](#bib.bib60)] offers sentence
    pairs across ten diverse categories of written and verbal communication, showcasing
    various styles, topics, and formalities. ANLI [[43](#bib.bib43)] is an adversarial
    dataset created using a human-and-model-in-the-loop method, featuring premises
    primarily sourced from Wikipedia and hypotheses crafted by human adversaries.
    ContractNLI [[25](#bib.bib25)] treats individual contracts as premises and applies
    a consistent set of hypotheses across the dataset. WANLI [[32](#bib.bib32)] is
    generated by GPT-3, containing examples that include challenging patterns initially
    identified in MNLI.
  prefs: []
  type: TYPE_NORMAL
- en: Toxic detection in BOSS. Civil Comments [[4](#bib.bib4)] features public comments
    from the Civil Comments platform, encompassing a diverse user base and various
    subtypes of toxic text. AdvCivil introduces a new toxic dataset, derived from
    Civil Comments through textual adversarial attacks within an automated model-in-the-loop
    adversarial pipeline. Implicit Hate [[13](#bib.bib13)] includes toxic tweets that
    are both explicit and implicit, with the latter capable of evading keyword-based
    toxic detection systems. ToxiGen [[18](#bib.bib18)] is generated by GPT-3 and
    contains subtly and implicitly toxic texts targeting 13 minority groups.
  prefs: []
  type: TYPE_NORMAL
- en: Chinese domain-specific. C-Eval [[20](#bib.bib20)] is a comprehensive Chinese
    evaluation suite for foundation models. It consists of 13948 multi-choice questions
    spanning 52 diverse disciplines and four difficulty levels, primarily encompassing
    humanities, social sciences, STEM, and other 4 categories. CMMLU [[29](#bib.bib29)]
    is a comprehensive Chinese evaluation benchmark designed specifically to assess
    language models’ knowledge and reasoning abilities within Chinese contexts. CMMLU
    covers 67 topics ranging from fundamental subjects to advanced professional levels.
    It encompasses topics such as STEM requiring calculation and reasoning, humanities
    and social sciences necessitating knowledge, and everyday knowledge such as Chinese
    driving rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Summary of the datasets in S1.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Task&Ability | Dataset | Gene. | Metric | Size |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Common sense reasoning | WinoGrande [[50](#bib.bib50)] | 0/5 | Acc |
    1267 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Common sense reasoning | WSC273 [[28](#bib.bib28)] | 0/5 | Acc | 273
    |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Common sense reasoning | GLUE-WNLI [[57](#bib.bib57)] | 0/5 | Acc |
    71 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Common sense reasoning | HellaSwag [[71](#bib.bib71)] | 0/5 | Acc |
    10042 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Common sense reasoning | SWAG [[70](#bib.bib70)] | 0/5 | Acc | 20006
    |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Common sense reasoning | PIQA [[53](#bib.bib53)] | 0/5 | Acc | 1838
    |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Mathematical reasoning | MathQA [[2](#bib.bib2)] | 0/5 | Acc | 2985
    |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Multi-turn dialogue reasoning | Mutual [[8](#bib.bib8)] | 0/5 | R2 |
    886 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Multi-turn dialogue reasoning | Mutual_Plus [[8](#bib.bib8)] | 0/5 |
    R2 | 886 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Bias diagnosis and mitigation | CrowS-Pairs [[42](#bib.bib42)] | 0 |
    Pct_stereotype | 6708 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Bias diagnosis and mitigation | Toxigen [[18](#bib.bib18)] | 0/5 | Acc
    | 940 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Scientific knowledge question answering | PubMedQA [[22](#bib.bib22)]
    | 0/5 | Acc | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Scientific knowledge question answering | OpenBookQA [[38](#bib.bib38)]
    | 0/5 | Acc | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Scientific knowledge question answering | SciQ [[59](#bib.bib59)] |
    0/5 | Acc | 1000 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Scientific knowledge question answering | ARC-Easy [[7](#bib.bib7)]
    | 0/5 | Acc | 2376 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Scientific knowledge question answering | ARC-Challenge [[7](#bib.bib7)]
    | 0/5 | Acc | 1172 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Scientific knowledge question answering | MC-TACO [[74](#bib.bib74)]
    | 0/5 | F1 | 9442 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Reading comprehension | RACE [[27](#bib.bib27)] | 0/5 | Acc | 1045 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Reading comprehension | QA4MRE [[45](#bib.bib45)] | 0/5 | Acc | 564
    |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Natural language inference | GLUE-MNLI [[57](#bib.bib57)] | 0/5 | Acc
    | 9815 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Natural language inference | GLUE-MNLI-Mismatched [[57](#bib.bib57)]
    | 0/5 | Acc | 9832 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Natural language inference | GLUE-RTE [[57](#bib.bib57)] | 0/5 | Acc
    | 277 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Natural language inference | GLUE-QNLI [[57](#bib.bib57)] | 0/5 | Acc
    | 5463 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Natural language inference | ANLI [[43](#bib.bib43)] | 0/5 | Acc | 3200
    |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Sentiment analysis | GLUE-SST [[57](#bib.bib57)] | 0/5 | Acc | 872 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Syntax phenomena evaluation | BLiMP [[58](#bib.bib58)] | 5 | Acc | 67000
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Summary of the datasets in S2.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Task&Ability | Dataset | Gene. | Metric | Size |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Extractive question answering | SQuAD [[49](#bib.bib49)] | 0/1 | F1
    | 10570 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Extractive question answering | AdversarialQA [[3](#bib.bib3)] | 0/1
    | F1 | 2694 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Extractive question answering | NewsQA [[55](#bib.bib55)] | 0/1 | F1
    | 3912 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Extractive question answering | SearchQA [[12](#bib.bib12)] | 0/1 |
    F1 | 16680 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Sentiment analysis | Amazon [[36](#bib.bib36)] | 0/3 | Acc | 38905 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Sentiment analysis | DynaSent [[46](#bib.bib46)] | 0/3 | Acc | 4020
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Sentiment analysis | SemEval [[41](#bib.bib41)] | 0/3 | Acc | 20322
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Sentiment analysis | SST [[52](#bib.bib52)] | 0/3 | Acc | 767 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Natural language inferenc | MNLI [[60](#bib.bib60)] | 0/3 | Acc | 9815
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Natural language inferenc | ANLI [[43](#bib.bib43)] | 0/3 | Acc | 2900
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Natural language inferenc | ContractNLI [[25](#bib.bib25)] | 0/3 | Acc
    | 1791 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Natural language inferenc | WANLI [[32](#bib.bib32)] | 0/3 | Acc | 4700
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Toxic detection | Civil Comments [[4](#bib.bib4)] | 0/2 | Acc | 97320
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Toxic detection | AdvCivil | 0/2 | Acc | 523 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Toxic detection | Implicit Hate [[13](#bib.bib13)] | 0/2 | Acc | 21180
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Toxic detection | ToxiGen [[18](#bib.bib18)] | 0/2 | Acc | 641 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Chinese domainspecific | CEVAL [[20](#bib.bib20)] | 0/5 | Acc | 13948
    |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | Chinese domainspecific | CMMLU [[29](#bib.bib29)] | 0/5 | Acc | 11917
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will present all the details of our experiment, including
    hardware resources, experimental setup, hyperparameter selection, and data selection.
    Besides, Our benchmark suite is publicly available at [https://github.com/TsingmaoAI/MI-optimize](https://github.com/TsingmaoAI/MI-optimize).
  prefs: []
  type: TYPE_NORMAL
- en: D.1 Hardware Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiments, we utilize one computer with 8 AMD Aldebaran GPUs and two
    computers with 2 NVIDIA Tesla V100 GPUs each. Specifically, each AMD Aldebaran
    GPU has 64GB of memory, totaling 512GB. Each NVIDIA Tesla V100 GPU has 32GB of
    memory, totaling 128GB.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Experiment Details in S1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experimental Setup. We quantize LLaMA2-7B [[54](#bib.bib54)] using the GPTQ [[14](#bib.bib14)],
    SpQR [[10](#bib.bib10)] methods. We quantize the weights to 2-4 bits and test
    16 bits as reference. The quantization is implemented using our custom toolbox,
    maintaining consistency with the original method in all experimental details.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Selection. For the GPTQ [[14](#bib.bib14)] method, we set the
    group-size parameter to 128 and apply block-sequential as well as layer-sequential
    quantization. For the SpQR [[10](#bib.bib10)] method, we set the group-size parameter
    to 128 and apply block-sequential quantization. Throughout the quantization process,
    we use 128 calibration examples. In the few-shot setting, the number of selected
    examples corresponds to LM Evaluation Harness [[16](#bib.bib16)], remaining at
    5-shot.
  prefs: []
  type: TYPE_NORMAL
- en: Data Selection. We follow GPTQ [[14](#bib.bib14)] and randomly sample 128 samples
    from C4-en-val [[48](#bib.bib48)] as the calibration set with a random seed of
    42\. For the selection of test data, we use the test splits of ANLI [[43](#bib.bib43)],
    ARC [[7](#bib.bib7)], CrowSPairs [[42](#bib.bib42)], GLUE-MNLI-Mismatched [[57](#bib.bib57)],
    MathQA [[2](#bib.bib2)], MCTACO [[74](#bib.bib74)], OpenBookQA [[38](#bib.bib38)],
    RACE [[27](#bib.bib27)], SciQ [[59](#bib.bib59)], Toxigen [[18](#bib.bib18)],
    and WSC273 [[28](#bib.bib28)] as the test set. We use the validation splits of
    GLUE-SST, GLUE-MNLI, GLUE-QNLI, GLUE-WNLI, GLUE-RTE [[57](#bib.bib57)], HellaSwag [[71](#bib.bib71)],
    Mutual [[8](#bib.bib8)], PIQA [[53](#bib.bib53)], SWAG [[70](#bib.bib70)], WinoGrande [[50](#bib.bib50)]
    as the test set. Additionally, we use the train splits of BLiMP [[58](#bib.bib58)],
    PubMedQA [[22](#bib.bib22)], and QA4MRE [[45](#bib.bib45)] as the test set. For
    the selection of examples in the few-shot setting, we use the default setting.
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Experiment Details in S2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: D.3.1 BOSS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Experimental Setup. We quantize LLaMA2-7B using the GPTQ [[14](#bib.bib14)],
    SpQR [[10](#bib.bib10)], awq [[31](#bib.bib31)], and Smoothquant [[62](#bib.bib62)]
    methods. We quantize the weights to 3-4 bits, and for smoothquant, we further
    quantize the activations to 8 bits. The quantization is implemented using our
    custom toolbox, maintaining consistency with the original method in all experimental
    details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameter Selection. For the GPTQ [[14](#bib.bib14)] method, we set the
    group-size parameter to 128 and apply block-sequential as well as layer-sequential
    quantization. For the SpQR [[10](#bib.bib10)] method, we set the group-size parameter
    to 128 and apply block-sequential quantization. For the AWQ [[31](#bib.bib31)]
    method, we set the group-size parameter to 128\. Throughout the quantization process,
    we use 128 calibration examples. In the few-shot setting, the number of selected
    examples corresponds to those in BOSS. Specifically, EQA is 1-shot, SA and NLI
    are 3-shot, and TD is 2-shot. The prompt template is presented in Tab. [8](#A4.T8
    "Table 8 ‣ D.3.2 Chinese domain-specific ‣ D.3 Experiment Details in S2 ‣ Appendix
    D Experiment Details ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox").'
  prefs: []
  type: TYPE_NORMAL
- en: Data Selection. For the calibration set, we use 128 calibration examples. For
    SQuAD [[49](#bib.bib49)] dataset in EQA, Amazon [[36](#bib.bib36)] dataset in
    SA, MNLI [[60](#bib.bib60)] dataset in NLI, and Civil Comments [[4](#bib.bib4)]
    dataset in TD, as the original datasets include train and test splits, we directly
    select the first 128 instances from the train split as the calibration set. For
    the remaining datasets, given that the original datasets exclusively contain a
    test split, we randomly sample 300 instances from the test split to form a train
    split, subsequently removing the sampled data from the test split. We use the
    first 128 instances from the sampled train split as the calibration set. The random
    seed is set to 42\. The code for processing the original BOSS benchmark will be
    placed in our GitHub repository. Concerning the selection of examples in the few-shot
    setting, we maintain consistency with BOSS. For datasets lacking examples, we
    appropriately select suitable samples from the portion of the train split not
    chosen as part of the calibration set. For the test data, we use the test split
    of each dataset as the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: D.3.2 Chinese domain-specific
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Experimental Setup. We quantize Baichuan2-7B-Base [[64](#bib.bib64)] using the
    GPTQ [[14](#bib.bib14)], SpQR [[10](#bib.bib10)], AWQ [[31](#bib.bib31)], and
    Smoothquant [[62](#bib.bib62)] methods. We quantize the weights to 3-4 bits, and
    for smoothquant, we further quantize the activations to 8 bits. The quantization
    is implemented using our custom toolbox, maintaining consistency with the original
    method in all experimental details. Since the test split of C-EVAL was not publicly
    available, we upload the test answers to the official platform to obtain the results.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Selection. For the GPTQ [[14](#bib.bib14)] method, we set the
    group-size parameter to 128 and apply block-sequential as well as layer-sequential
    quantization. For the SpQR [[10](#bib.bib10)] method, we set the group-size parameter
    to 128 and apply block-sequential quantization. For the AWQ [[31](#bib.bib31)]
    method, we set the group-size parameter to 128\. Throughout the quantization process,
    we use 128 calibration examples. In the few-shot setting, the number of selected
    examples corresponds to those in C-EVAL [[20](#bib.bib20)] and CMMLU [[29](#bib.bib29)],
    remaining at 5-shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Selection. For the calibration set, we use 128 calibration examples. For
    C-EVAL [[20](#bib.bib20)], we utilize its validation split as the calibration
    set. For CMMLU [[29](#bib.bib29)], we randomly select 300 instances from its test
    split for the train split, subsequently removing the sampled data from the test
    split. We use the first 128 instances from the sampled train split as the calibration
    set. The random seed is set to 42\. As for the selection of examples in the few-shot
    setting, we remain consistent with the official standards of C-EVAL and CMMLU.
    The prompt template is presented in Tab. [8](#A4.T8 "Table 8 ‣ D.3.2 Chinese domain-specific
    ‣ D.3 Experiment Details in S2 ‣ Appendix D Experiment Details ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Prompts for BOSS and Chinese domain-specific tasks. We maintain consistency
    with the official template provided by BOSS [[69](#bib.bib69)] and C-EVAL [[20](#bib.bib20)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| EQA | ### Instruction ### Solve the extractive question answering task. Refering
    to the passage below and extract answer for the question. The answer should be
    the shortest phrase as it can be. ### Format ### Passage: {{Passage}} // Question:
    {{Question}} // Answer: {{Answer}}. ### Input ### Passage: {{input_1}} // Question:
    {{input_2}} // Answer: |'
  prefs: []
  type: TYPE_TB
- en: '| SA | ### Instruction ### Solve the sentiment analysis task. Options for sentiment:
    negative, positive, neutral. ### Format ### Text: {{Text}} // Prediction: {{Prediction}}
    ### Input ### Text: {{input}} // Prediction: |'
  prefs: []
  type: TYPE_TB
- en: '| NLI | ### Instruction ### Solve the NLI task. Options for entailment relationship:
    entailment, neutral, contradiction. ### Format ### Premise: {{Premise}} // Hypothesis:
    {{Hypothesis}} // Prediction: {{Prediction}} ### Input ### Premise: {{input_1}}
    // Hypothesis: {{input_2}} // Prediction: |'
  prefs: []
  type: TYPE_TB
- en: '| TD | ### Instruction ### Solve the toxic detection task. Options for toxicity:
    benign, toxic. ### Format ### Text: {{Text}} // Prediction: {{Prediction}} ###
    Input ### Text: {{input}} // Prediction: |'
  prefs: []
  type: TYPE_TB
- en: '| CDS | 以下是中国考试的单项选择题，请选出其中的正确答案。 |'
  prefs: []
  type: TYPE_TB
- en: Appendix E The Robustness of Data Selection with respect to Random Seed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the experiments conducted in the main text, we employ a random seed for the
    selection of train split and calibration set. In this section, we will alter the
    random seed to observe the sensitivity of the experiments to the random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d66263d7187a5d58db8142915960056c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: S1: evaluation of quantized LLaMA2-7B on several standard datasets.
    Quantization methods include GPTQ. Quantization bits include W4A16, W3A16, and
    W2A16, with W16A16 used as reference. The left figure shows 5-shot results, while
    the right figure shows 0-shot results. Different background colors represent different
    task types. The random seed is 42.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/577e6d2859ff69ff531f515e8822dbc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: S1: evaluation of quantized LLaMA2-7B retested on several standard
    datasets. Quantization methods include GPTQ. Quantization bits include W4A16,
    W3A16, and W2A16, with W16A16 used as reference. The left figure shows 5-shot
    results, while the right figure shows 0-shot results. Different background colors
    represent different task types. The random seed is 567.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In S1, we randomly sampled 128 samples from c4-en-val as the calibration set
    and set the random seed to 42\. We then modify the random seed to 567 and retest
    the GPTQ [[14](#bib.bib14)] method. The results are presented in Fig. [4](#A5.F4
    "Figure 4 ‣ Appendix E The Robustness of Data Selection with respect to Random
    Seed ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox") and [5](#A5.F5 "Figure 5 ‣ Appendix E The Robustness of Data Selection
    with respect to Random Seed ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox"). We observe that the vast majority of
    datasets exhibited strong robustness to the selection of the calibration set,
    with performance trends remaining nearly identical across different random seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Cross-dataset distribution shift evaluation on BOSS in S2, we randomly sample
    some examples from the test split as the train split and use them as the calibration
    set, setting the random seed to 42\. We modify the random seed to 567 and retest
    the SA and NLI experiments using GPTQ [[14](#bib.bib14)] method. We present the
    average results with random seeds 42 and 567 in Tab. [9](#A5.T9 "Table 9 ‣ Appendix
    E The Robustness of Data Selection with respect to Random Seed ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox").
    The results indicate a certain robustness of the distribution shift experiment
    on BOSS towards the selection of the calibration set. For SA task, performance
    remains consistently better when using Amazon [[36](#bib.bib36)] as the calibration
    set across different random seeds, and using SemEval [[41](#bib.bib41)] as the
    calibration set performs better in most cases. However, the performance has consistently
    been poor when using DynaSent [[46](#bib.bib46)] as the calibration set. For NLI
    task, performance remains consistently better when using MNLI [[60](#bib.bib60)]
    as the calibration set across different random seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Cross-dataset distribution shift evaluation retested on Boss. The
    result represents the average values obtained with random seeds 42 and 567\. "Calib."
    represents the calibration dataset, and "Gene." represents generalization scenario.
    To save space, abbreviations are used for datasets. Each row presents experimental
    results using different datasets as calibration sets on the same test dataset.
    The higher the metric, the better the performance. The two best performances are
    denoted in descending order with red and orange respectively. Note: Some datasets
    could not be used as calibration sets due to insufficient memory resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | SA | NLI |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  prefs: []
  type: TYPE_TB
- en: '| AZ | DS | SE | SST | MN | AN | WN | CN |'
  prefs: []
  type: TYPE_TB
- en: '| AZ | 0-shot | 4/16 | 65.84 | 46.90 | 66.49 | 53.61 | MN | 0-shot | 4/16 |
    0.25 | 0.31 | 0.25 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 19.14 | 0.50 | 21.41 | 0.03 | 3/16 | 0.03 | 0.00 | 0.00 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3-shot | 4/16 | 78.47 | 70.35 | 81.43 | 80.32 | 3-shot | 4/16 | 43.28 | 34.18
    | 41.46 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 80.73 | 41.28 | 70.79 | 70.23 | 3/16 | 32.95 | 33.02 | 32.01 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DS | 0-shot | 4/16 | 41.85 | 30.55 | 40.89 | 25.15 | AN | 0-shot | 4/16 |
    0.74 | 0.57 | 0.74 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 8.80 | 1.17 | 10.57 | 0.00 | 3/16 | 2.26 | 0.00 | 0.00 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3-shot | 4/16 | 53.88 | 45.50 | 54.15 | 52.38 | 3-shot | 4/16 | 34.1 | 33.52
    | 33.76 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 53.86 | 40.25 | 44.26 | 48.91 | 3/16 | 32.25 | 33.33 | 34.19 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SE | 0-shot | 4/16 | 19.97 | 14.07 | 22.08 | 14.27 | WN | 0-shot | 4/16 |
    0.09 | 0.08 | 0.10 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 2.48 | 0.10 | 8.25 | 0.02 | 3/16 | 0.27 | 0.00 | 0.00 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3-shot | 4/16 | 41.09 | 36.48 | 43.41 | 44.05 | 3-shot | 4/16 | 42.16 | 42.15
    | 39.925 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 42.69 | 27.98 | 38.57 | 36.48 | 3/16 | 43.16 | 43.36 | 46.97 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SST | 0-shot | 4/16 | 44.13 | 33.505 | 37.16 | 25.56 | CN | 0-shot | 4/16
    | 0.03 | 0.50 | 0.00 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 3.93 | 0.52 | 5.09 | 0.00 | 3/16 | 0.03 | 0.56 | 0.73 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3-shot | 4/16 | 54.83 | 44.01 | 52.61 | 48.11 | 3-shot | 4/16 | 35.93 | 36.67
    | 32.27 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 3/16 | 57.17 | 44.33 | 46.68 | 52.29 | 3/16 | 28.28 | 20.41 | 26.13 | - |'
  prefs: []
  type: TYPE_TB
