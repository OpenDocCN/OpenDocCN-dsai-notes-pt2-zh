- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'HERA: High-efficiency Matrix Compression via Element Replacement'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03637](https://ar5iv.labs.arxiv.org/html/2407.03637)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yanshu Wang
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: yanshuwang@pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Wang Li¹¹footnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: 2000012734@stu.pku.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Tong Yang'
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: yangtong@pku.edu.cn Equal contribution
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have significantly advanced natural language processing
    tasks such as machine translation, text generation, and sentiment analysis. However,
    their large size, often consisting of billions of parameters, poses challenges
    for storage, computation, and deployment, particularly in resource-constrained
    environments like mobile devices and edge computing platforms. Additionally, the
    key-value (k-v) cache used to speed up query processing requires substantial memory
    and storage, exacerbating these challenges. Vector databases have emerged as a
    crucial technology to efficiently manage and retrieve the high-dimensional vectors
    produced by LLMs, facilitating faster data access and reducing computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: Effective compression and quantization techniques are essential to address these
    challenges, as they reduce the memory footprint and computational requirements
    without significantly compromising performance. Traditional methods that uniformly
    map parameters to compressed spaces often fail to account for the uneven distribution
    of parameters, leading to considerable accuracy loss. Therefore, innovative approaches
    are needed to achieve better compression ratios while preserving model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we propose HERA, a novel algorithm that employs heuristic Element
    Replacement for compressing matrix. HERA systematically replaces elements within
    the model using heuristic methods, which simplifies the structure of the model
    and makes subsequent compression more effective. By hierarchically segmenting,
    compressing, and reorganizing the matrix dataset, our method can effectively reduce
    the quantization error to 12.3% of the original at the same compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have revolutionized the field of natural language
    processing (NLP), enabling significant advancements in tasks such as machine translation,
    text generation, and sentiment analysis. These models, characterized by their
    large-scale neural network architectures and vast training datasets, have shown
    remarkable capabilities in understanding and generating human language. The advent
    of LLMs, such as OpenAI’s GPT-3 and BERT by Google, has pushed the boundaries
    of what machines can achieve in linguistic tasks, providing near-human performance
    in various applications  Brown et al. ([2020](#bib.bib1)); Devlin et al. ([2018](#bib.bib6)).
  prefs: []
  type: TYPE_NORMAL
- en: The development of LLMs is rooted in the transformer architecture, which employs
    self-attention mechanisms to process and produce language with high contextual
    relevance. This architecture has replaced previous recurrent neural network (RNN)
    and long short-term memory (LSTM) models due to its efficiency and ability to
    handle long-range dependencies in text Vaswani et al. ([2017](#bib.bib15)). As
    a result, LLMs have become the cornerstone of modern NLP, driving innovations
    in areas such as automated customer service, content creation, and real-time language
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the development and deployment of LLMs face significant challenges,
    particularly in optimizing vector database efficiency, LLM weight quantization,
    and key-value (k-v) cache optimization. First, as the size and complexity of LLMs
    increase, efficiently managing and retrieving vectors in large databases becomes
    critical. Optimizing vector database efficiency involves improving indexing and
    search algorithms to handle high-dimensional data, which is essential for tasks
    like semantic search and recommendation systems Johnson et al. ([2019](#bib.bib11)).
    Second, LLM weight quantization, which involves reducing the precision of model
    weights, is a crucial technique for making these models more storage and computation
    efficient without significantly degrading their performance. Quantization can
    drastically reduce the memory footprint and computational requirements, enabling
    the deployment of LLMs on resource-constrained devices. However, achieving optimal
    quantization while maintaining model accuracy remains a complex challenge that
    requires sophisticated algorithms and techniques Dettmers et al. ([2021](#bib.bib5)).
    Lastly, optimizing k-v cache efficiency is vital for speeding up inference times
    in LLMs. The k-v cache stores intermediate activations during the forward pass,
    which can be reused to avoid redundant computations. Efficient management and
    compression of these caches are essential to reduce latency and improve the throughput
    of LLMs, especially in real-time applications like chatbots and virtual assistants.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5c0f6f02ab859925448cb2d6edb8339.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The distribution of LLM parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenges of storage and computational efficiency in large language
    models (LLMs), it is crucial to develop techniques that optimize vector database
    efficiency, weight quantization, and key-value (k-v) cache optimization. Therefore,
    finding quantization algorithms that can efficiently compress and quickly decompress
    matrix data is essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, when compressing parameters, the distribution of parameters is
    often uneven. First, the distribution is not uniform, and second, the magnitudes
    of different data points vary significantly. For example, the distribution of
    Transformer neural network weights is shown in Figure  [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ HERA: High-efficiency Matrix Compression via Element Replacement").
    In Figure  [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement"), different rows represent the 0th layer,
    8th layer, 16th layer, and 24th layer. Different columns represent the attention
    layer’s q, k, v, and o layers, as well as the fully connected layer’s gate, up,
    and down layers. The distributions of different layers are not uniformly analyzed,
    and the distributions vary across different layers. Previous algorithms uniformly
    and linearly map parameters to another compressed space. Although this method
    is simple, it may not be effective and can result in significant accuracy loss
    because it does not take into account the original data distribution. It maps
    both densely populated and sparse regions to the quantized space in the same way.
    Besides, A lines of works have been proposed to optimize vector database efficiency Ge
    et al. ([2013](#bib.bib8)); Jegou et al. ([2010](#bib.bib10)); Kalantidis, Avrithis
    ([2014](#bib.bib12)), quantize the model weights Dettmers et al. ([2022](#bib.bib4));
    Polino et al. ([2018](#bib.bib14)); Chmiel et al. ([2020](#bib.bib2)); Fan et al.
    ([2020](#bib.bib7)); Zafrir et al. ([2019](#bib.bib17)); Wu et al. ([2022](#bib.bib16));
    LeCun et al. ([1989](#bib.bib13)) and optimizate kv cache Hooper et al. ([2024](#bib.bib9));
    Cho et al. ([2024](#bib.bib3)). They also does not take into account the original
    data distribution. Therefore, a more reasonable approach is to quantize the model
    based on the distribution and magnitude of the parameters, stratifying and computing
    accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we propose a novel algorithm named HERA to enhance the compression
    of large language models. The core idea of HERA is to group the relevant matrices
    and compress each group using clustering methods. Furthermore, HERA reorders the
    original dataset based on the distribution and magnitude of the parameters. This
    reordering is performed in multiple layers, with each layer featuring a new arrangement
    that optimizes the dataset’s distribution for compression. By leveraging these
    techniques, HERA effectively reduces data error and minimizes the discrepancy
    between estimated data and actual data.
  prefs: []
  type: TYPE_NORMAL
- en: We implemented a prototype system using Python. Experimental results show that,
    by hierarchically segmenting, compressing, and reorganizing the matrix dataset,
    our method can effectively reduce the quantization error to 12.3% of the original
    at the same compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 2 HERA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Basic algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We borrow the idea from product quantization algorithm and k-means algorithm
    to compress the model parameter. The algorithm is divided into four steps, the
    first three steps is quantization process as shown in Figure  [2](#S2.F2 "Figure
    2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement"), the last two steps is the dequantization process as show
    in Figure  [3](#S2.F3 "Figure 3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency
    Matrix Compression via Element Replacement"). The detailed process is described
    below.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Product Quantization Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:$X\in\mathbb{R}^{N\times D}$24:end for
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Space Partitioning: Divide the high-dimensional space into a series of low-dimensional
    subspaces.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Codebook calculation: Use k-means algorithm to calculate codebook of low-dimensional
    subspaces.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantization: Perform independent quantization within each low-dimensional
    subspace, mapping data points to the nearest cluster center.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Codebook Reconstruction: Reconstruct the data using a codebook that contains
    all possible cluster centers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dequantization: Restore the vectors in the original high-dimensional space
    based on the vectors in the quantized low-dimensional subspaces.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb096afafa201a2f283e0dfd1ae477ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The Quantization Process of HERA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9618952bc437f70ca5664e3caddd1117.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The Dequantization Process of HERA.'
  prefs: []
  type: TYPE_NORMAL
- en: In data matrices where the distribution of data is uneven across different columns,
    directly applying the k-means algorithm for clustering may yield suboptimal results.
    This unevenness can distort the clustering process, as k-means assumes that all
    features contribute equally to the distance calculations. The differing distributions
    across columns mean that some features may disproportionately influence the clustering
    outcome, leading to biased clusters that do not accurately reflect the true structure
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate these effects, it is crucial to preprocess the data appropriately.
    One common approach is to normalize the data so that each column contributes equally
    to the distance metric used by k-means. Standard normalization techniques, such
    as z-score normalization or min-max scaling, can be employed to transform the
    data into a more uniform distribution. By ensuring that all features are on a
    comparable scale, the k-means algorithm can perform more effectively, producing
    clusters that better represent the underlying patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Product Quantization with HERA heuristics
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input matrix: $X\in\mathbb{R}^{N\times D}$ 0 then26:     Goto line 127:end if'
  prefs: []
  type: TYPE_NORMAL
- en: However, these transformations require specific parameters for each column and
    involve complex calculations, which can be computationally intensive and demand
    significant storage resources. To address this, we employ a reordering method
    that permutations the elements of the matrix based on their relative sizes. This
    approach achieves a more uniform distribution, enabling more effective application
    of compression operations.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Dequantization of Product Quantization with HERA heuristics
  prefs: []
  type: TYPE_NORMAL
- en: '1:Quantized feature matrices: $code_{small}$23:     end for24:end for'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of permutation is to explore all possible reordering methods within
    the search space. For an $N\times D$ possible permutations, creating a vast search
    space.
  prefs: []
  type: TYPE_NORMAL
- en: To manage this, we employ heuristic algorithms to reduce the search space and
    optimize the algorithm’s complexity. Heuristic algorithms provide approximate
    solutions by focusing on the most promising areas of the search space, rather
    than exhaustively evaluating all possible permutations. These methods significantly
    decrease computational requirements while still delivering high-quality solutions.
  prefs: []
  type: TYPE_NORMAL
- en: By adopting this heuristic reordering strategy, we aim to achieve a more balanced
    distribution of data across the matrix. This not only simplifies the preprocessing
    steps but also enhances the efficiency of subsequent clustering and compression
    processes. Ultimately, this method helps overcome the challenges posed by uneven
    data distributions, leading to more accurate and resource-efficient clustering
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 HERA heuristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To improve quantization and avoid uneven distribution, HERA first perceives
    the distribution of data sizes, then rearranges the elements according to the
    distribution, and finally quantizes the rearranged matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 HERA Design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We observe that compressing matrix elements of similar sizes as a single column
    (group) yields better results.HERA uses a low-cost method to locally adjust element
    arrangement based on data sizes. We first introude the HERA design. The design
    of HERA heuristics is shown in Figure  [4](#S2.F4 "Figure 4 ‣ 2.2.1 HERA Design
    ‣ 2.2 HERA heuristics ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fd642a656d03a84a9e74171afe58c5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: HERA heuristics.'
  prefs: []
  type: TYPE_NORMAL
- en: HERA groups every two elements in the matrix together, rearranges the matrix
    elements based on the size of adjacent elements, and then quantizes the rearranged
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization: The quantization process is illustrated in the algorithm [2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement"), the quantization process consists of multiple steps,
    which involve rearranging the input data, comparing elements, and quantizing both
    smaller and larger value sets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm begins by rearranging the input matrix $X\in\mathbb{R}^{N\times
    D}$ (lines 2–4 in Algorithm [2](#alg2 "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, each pair $x_{pair_{i,j}}$ recording the comparison result as 1 (lines
    5–8 in Algorithm [2](#alg2 "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA:
    High-efficiency Matrix Compression via Element Replacement")). Otherwise, the
    assignment is reversed, and the feature map records a 0 (lines 9–12 in Algorithm [2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantization process proceeds by applying the PQ algorithm to both $X_{small}$),
    refining the quantization results with each iteration (lines 13–16 in Algorithm [2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")).'
  prefs: []
  type: TYPE_NORMAL
- en: Dequantization:The Decompression process consists of multiple steps, namely
    decoding the quantized values, reconstructing the pairs, and restoring the original
    matrix dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm begins by initializing the reconstructed matrix $\hat{X}_{pairs}\in\mathbb{R}^{N\times
    D/2}$ respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, for each pair in $\hat{X}_{pairs}$. If the feature map value is 1, the
    first element of the pair is assigned the decoded small value and the second element
    the decoded big value (lines 5–8 in Algorithm [3](#alg3 "Algorithm 3 ‣ 2.1 Basic
    algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element Replacement")).
    If the feature map value is 0, the assignment is reversed (lines 9–12 in Algorithm [3](#alg3
    "Algorithm 3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the pairs are expanded back into the original dimensions to form the
    reconstructed matrix $\hat{X}$ (lines 13–17 in Algorithm [3](#alg3 "Algorithm
    3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experiment setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dataset: The datasets used in our experiments were generated with matrices
    of size $N\times D$ is the dimensionality of each sample. We employed one types
    of normal distribution to create these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normal Distribution Dataset: For the normal distribution dataset, each element
    of the matrix was drawn from a truncated normal distribution with a mean of 0.5
    and a standard deviation of 0.16\. The distribution was truncated to the open
    interval $(0,1)$ to ensure all values remain within this range.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{X}_{ij}\sim\mathcal{TN}(0.5,0.16,0,1),\quad\forall i\in\{1,\ldots,N\},\quad\forall
    j\in\{1,\ldots,D\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathcal{TN}(\mu,\sigma,a,b)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Platform and implementation: We conducted our algorithm evaluations on a high-performance
    server equipped with an Intel Core i9-10980XE processor, featuring 18 cores and
    36 threads, operating at a base frequency of 3.00 GHz. The server also includes
    128GB of 3200MHz DDR4 memory and a 24.8MB L3 cache, providing robust computational
    capabilities. All algorithms were implemented in Python, using version 3.8.10.
    For each case, the experiment was repeated 100 times. In each repetition, matrices
    of the same size and from the same distribution were generated using different
    seeds.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of Algorithms and Parameter Selection
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we describe the parameter selection process for our HERA algorithm
    and compare its performance with the Optimized Product Quantization (OPQ) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our HERA algorithm involves a critical parameter, the number of centroids $K_{s}$
    is governed by the following memory constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $K_{s}\times D\times 32+N\times M\times[(K_{s}-1)\cdot\text{bitlength}]\leq\text{memory
    in bits}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $K_{s}$ is the number of centroids,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $D$ is the dimensionality of each sample,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $N$ is the number of samples,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $M$ is the number of subspaces,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bitlength is the bit length used for encoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This constraint ensures that the total memory usage remains within the available
    memory limits. By carefully selecting $K_{s}$, we aim to optimize the performance
    of the HERA algorithm while adhering to memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: The Optimized Product Quantization (OPQ) algorithm was used as a benchmark for
    comparison. The OPQ algorithm was implemented with default parameters, providing
    a consistent basis for performance evaluation. The OPQ algorithm optimizes product
    quantization by adjusting the space partitioning and centroid assignments to minimize
    quantization error.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison Metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'We use MAE (Mean Absolute Error), MRE (Mean Relative Error), and MSE (Mean
    Squared Error) for experimental evaluation. Let $X_{(i,j)}$ denote the values
    before and after dequantization, respectively. The following metrics are used
    for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{MAE}=\frac{1}{N\cdot D}\sum_{i,j}\left&#124;X_{(i,j)}-X_{(i,j)}^{\prime}\right&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{MRE}=\frac{1}{N\cdot D}\sum_{i,j}\frac{\left&#124;X_{(i,j)}-X_{(i,j)}^{\prime}\right&#124;}{X_{(i,j)}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{MSE}=\frac{1}{N\cdot D}\sum_{i,j}\left(X_{(i,j)}-X_{(i,j)}^{\prime}\right)^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We performed an optimization using 1-4 iterations, corresponding to our algorithms
    labeled as our1 through our4. In the quantification accuracy experiment, we tested
    the accuracy with different numbers of groups $m$ on the results while maintaining
    the same compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Experiment result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy Measurement
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab8d273df272415a5e456dcda605256b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: MAE on 8 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5334766df602bb13daf70d507f0c32d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: MRE on 8 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46e03d31580cbfdfba2ab2a8ec6ebb61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: MSE on 8 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bf616f569e423a1f3a5c568977406c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: MAE on 16 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea6389ee3a316b42f22a904e5070b4fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: MRE on 16 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc7231bf8c4d448f3db90628f63122e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: MSE on 16 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d169bf34c84c5045d3340a5ae89394c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: MAE on 32 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24f3ecb76b1e1b22fd75b9ffa474747d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: MRE on 32 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6740d15254f7c84953710259ab376ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: MSE on 32 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d05152d89dc7a6d1ce255d2f8b52cf0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: MAE on 128 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6675f0e8527281236ed24ff02edc80dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: MRE on 128 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1893eb06dd555fb5b6f95119e55af142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: MSE on 128 subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23b3ff73cc03f4abf750fce1bfafc4aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: MAE on 1 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/00e3ee0817d440bde7ace79213532990.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: MRE on 1 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6424f6c30a38cf024f83442339c4aed5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: MSE on 1 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c6b8cea54845ee372426873fad3a4099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: MAE on 2 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba6a4561250a6eab3eeaa043d5d33ae3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: MRE on 2 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76ba0295967d4f9b2bbccb542aebd6bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: MSE on 2 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0c57ca6573b633566cee43844d982c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: MAE on 3 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02f1c2a863623ea3eedbe136c55596ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: MRE on 3 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35ae0ccbd7bebc989f39e5e489ced6db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: MSE on 3 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29c46f81d5125d152e4f67b79635847f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: MAE on 4 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b6d23d6fb05cd16c0975c4ffd4dc6b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: MRE on 4 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ff49fd86a2dc6646275da7df789e1b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: MSE on 4 iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We measure MAE, MRE and MSE on different numbers of subspaces to show the accuracy
    of HERA.The results for MAE, MRE and MSE for different number of subspaces are
    shown in Figures  [7](#S3.F7 "Figure 7 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")-  [7](#S3.F7
    "Figure 7 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement"), Figures  [10](#S3.F10 "Figure 10 ‣ 3.2
    Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement")-  [10](#S3.F10 "Figure 10 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement"), Figures
     [13](#S3.F13 "Figure 13 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency
    Matrix Compression via Element Replacement")-  [13](#S3.F13 "Figure 13 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement"), Figures  [16](#S3.F16 "Figure 16 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")-  [16](#S3.F16
    "Figure 16 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement") respectively. For 8-subspace setting, HERA
    can reduce the MSE to 70.4%, 49.7%, 35.1% and 12.3% in 1-4 iterations, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: From the experimental results, we can derive four conclusions. Firstly, the
    higher the compression ratio, the less space is consumed, but the accuracy decreases.
    Secondly, the OPQ algorithm performs worse than others with the same compression
    rate because it needs to store the transformation matrix. Additionally, the PQ
    algorithm does not perform as well as our improved algorithm. Finally, the more
    iterations, the better the performance of our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameter sensitivity analysis: We also conducted a parameter sensitivity test
    under different iterations, evaluating how the number of subspaces affects the
    algorithm’s performance. The results for 1 to 4 iterations are shown in Figures
     [19](#S3.F19 "Figure 19 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency
    Matrix Compression via Element Replacement")- [19](#S3.F19 "Figure 19 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement"), Figures  [22](#S3.F22 "Figure 22 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")- [22](#S3.F22
    "Figure 22 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement"), and Figures  [25](#S3.F25 "Figure 25 ‣
    3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")- [25](#S3.F25 "Figure 25 ‣ 3.2 Experiment result ‣ 3
    Experiments ‣ HERA: High-efficiency Matrix Compression via Element Replacement"),
    respectively. Extensive experiments demonstrate that our algorithm reduces parameter
    sensitivity, making parameter selection more user-friendly (as indicated by the
    closer proximity of parameters in our algorithm’s curves).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, our research addresses critical challenges in the optimization
    of large language models (LLMs) through the development of a novel algorithm,
    HERA. The growing complexity and scale of LLMs necessitate innovative approaches
    to enhance storage and computational efficiency, particularly in vector database
    management, weight quantization, and key-value (k-v) cache optimization.
  prefs: []
  type: TYPE_NORMAL
- en: HERA’s hierarchical approach to segmenting, compressing, and reorganizing the
    matrix dataset has proven to be effective in significantly reducing quantization
    error. By considering the distribution and magnitude of parameters, our method
    achieves superior performance compared to traditional uniform quantization techniques.
    The experimental results from our prototype system, implemented in Python, demonstrate
    that HERA can reduce the quantization error to 12.3% of the original, maintaining
    the same compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Future work will explore further refinements to HERA, including its application
    to other types of application and broader datasets. Additionally, investigating
    the integration of HERA with other optimization techniques may yield even greater
    enhancements in model performance and efficiency. The promising results of this
    study encourage continued research and development in the quest for more effective
    and scalable solutions for LLM compression and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan
    Jared D, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell
    Amanda, others . Language models are few-shot learners // Advances in neural information
    processing systems. 2020\. 33\. 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chmiel et al. (2020) Chmiel Brian, Banner Ron, Shomron Gil, Nahshan Yury, Bronstein
    Alex, Weiser Uri, others . Robust quantization: One model to rule them all //
    Advances in neural information processing systems. 2020\. 33\. 5308–5317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2024) Cho Minsik, Rastegari Mohammad, Naik Devang. KV-Runahead:
    Scalable Causal LLM Inference by Parallel Key-Value Cache Generation // arXiv
    preprint arXiv:2405.05329\. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Dettmers Tim, Lewis Mike, Belkada Younes, Zettlemoyer
    Luke. Llm. int8 (): 8-bit matrix multiplication for transformers at scale // CoRR
    abs/2208.07339\. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dettmers et al. (2021) Dettmers Tim, Lewis Mike, Shleifer Sam, Zettlemoyer Luke.
    8-bit optimizers via block-wise quantization // arXiv preprint arXiv:2110.02861\.
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina.
    Bert: Pre-training of deep bidirectional transformers for language understanding
    // arXiv preprint arXiv:1810.04805\. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Fan Angela, Stock Pierre, Graham Benjamin, Grave Edouard,
    Gribonval Rémi, Jegou Herve, Joulin Armand. Training with quantization noise for
    extreme model compression // arXiv preprint arXiv:2004.07320\. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. (2013) Ge Tiezheng, He Kaiming, Ke Qifa, Sun Jian. Optimized product
    quantization // IEEE transactions on pattern analysis and machine intelligence.
    2013\. 36, 4\. 744–755.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hooper et al. (2024) Hooper Coleman, Kim Sehoon, Mohammadzadeh Hiva, Mahoney
    Michael W, Shao Yakun Sophia, Keutzer Kurt, Gholami Amir. Kvquant: Towards 10
    million context length llm inference with kv cache quantization // arXiv preprint
    arXiv:2401.18079\. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jegou et al. (2010) Jegou Herve, Douze Matthijs, Schmid Cordelia. Product quantization
    for nearest neighbor search // IEEE transactions on pattern analysis and machine
    intelligence. 2010\. 33, 1\. 117–128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2019) Johnson Jeff, Douze Matthijs, Jégou Hervé. Billion-scale
    similarity search with GPUs // IEEE Transactions on Big Data. 2019\. 7, 3\. 535–547.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalantidis, Avrithis (2014) Kalantidis Yannis, Avrithis Yannis. Locally optimized
    product quantization for approximate nearest neighbor search // Proceedings of
    the IEEE conference on computer vision and pattern recognition. 2014\. 2321–2328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun Yann, Denker John, Solla Sara. Optimal brain damage
    // Advances in neural information processing systems. 1989\. 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Polino Antonio, Pascanu Razvan, Alistarh Dan. Model compression
    via distillation and quantization // arXiv preprint arXiv:1802.05668\. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob,
    Jones Llion, Gomez Aidan N, Kaiser Łukasz, Polosukhin Illia. Attention is all
    you need // Advances in neural information processing systems. 2017\. 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022) Wu Xiaoxia, Yao Zhewei, Zhang Minjia, Li Conglong, He Yuxiong.
    Xtc: Extreme compression for pre-trained transformers made simple and efficient
    // Advances in Neural Information Processing Systems. 2022\. 35\. 3217–3231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. (2019) Zafrir Ofir, Boudoukh Guy, Izsak Peter, Wasserblat Moshe.
    Q8bert: Quantized 8bit bert // 2019 Fifth Workshop on Energy Efficient Machine
    Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). 2019\. 36–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
