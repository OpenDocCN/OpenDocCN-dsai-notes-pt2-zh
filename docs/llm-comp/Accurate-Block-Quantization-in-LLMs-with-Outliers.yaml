- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Accurate Block Quantization in LLMs with Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.20137](https://ar5iv.labs.arxiv.org/html/2403.20137)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nikita Trukhanov d-Matrix Santa Clara, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: ntrukhanov@d-matrix.ai    Ilya Soloveychik d-Matrix Santa Clara, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: ilyas@d-matrix.ai
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The demand for inference on extremely large scale LLMs has seen enormous growth
    in the recent months. It made evident the colossal shortage of dedicated hardware
    capable of efficient and fast processing of the involved compute and memory movement.
    The problem is aggravated by the exploding raise in the lengths of the sequences
    being processed, since those require efficient on-chip storage of the KV-cache
    of size proportional to the sequence length. To make the required compute feasible
    and fit the involved data into available memory, numerous quantization techniques
    have been proposed that allow accurate quantization for both weights and activations.
    One of the main recent breakthroughs in this direction was introduction of the
    family of Block Floating Point (BFP) formats characterized by a block of mantissas
    with a shared scale factor. These enable memory- power-, and compute- efficient
    hardware support of the tensor operations and provide extremely good quantization
    accuracy. The main issues preventing widespread application of block formats is
    caused by the presence of outliers in weights and activations since those affect
    the accuracy of the other values in the same block. In this paper, we focus on
    the most critical problem of limited KV-cache storage. We propose a novel approach
    enabling usage of low precision BFP formats without compromising the resulting
    model accuracy. We exploit the common channel-wise patterns exhibited by the outliers
    to rearrange them in such a way, that their quantization quality is significantly
    improved. The methodology yields 2x savings in the memory footprint without significant
    degradation of the model’s accuracy. Importantly, the rearrangement of channels
    happens at the compile time and thus has no impact on the inference latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLM inference; block formats, outliers, cache.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pretrained Large Language Models (LLMs) have become enormously popular in the
    recent years [[1](#bib.bibx1), [2](#bib.bibx2), [3](#bib.bibx3), [4](#bib.bibx4),
    [5](#bib.bibx5)]. Such popularity has mostly been gained due to the extremely
    high quality of the text generated by the state-of-the-art models. However, such
    improvements often come at the cost of increased model sizes which makes training
    of these large models and using them for inference highly challenging in terms
    of storage capacity, memory transfer, and compute. The architecture of the modern
    LLMs is typically based on the decoder part of a transformer [[6](#bib.bibx6)].
    While the LLM training process can fully exploit parallelization across the input
    tokens, the inference must be performed sequentially. The generation process produces
    one token on every pass over the network given the prompt and all previously generated
    tokens. The core building block of the transformer architecture – the attention
    mechanism – requires computation of the so called keys ${\bm{K}}$ matrices become
    prohibitively resource greedy. To avoid those redundant operations, one could
    exploit the fact that the keys and values of the already appended tokens never
    change and can therefore be cached on chip.
  prefs: []
  type: TYPE_NORMAL
- en: Caching ${\bm{K}}$ matrices is extremely helpful if the on-chip storage allows
    it. However, the ever growing demand for generation of longer sequence dwarfs
    any amount of on-chip storage [[7](#bib.bibx7), [8](#bib.bibx8)]. Hence, every
    possible technique must be exploited to reduce the memory footprint of the cached
    tensors. The most promising approach consists in efficient quantization of keys
    and values. To this end such algorithms as GPTQ [[9](#bib.bibx9)], SmoothQuant
    [[10](#bib.bibx10)], and many others have been proposed. For example, the GPTQ
    technique prescribes successive quantization of the weight columns in such a way
    that the rounding of every next column carefully takes into account the accumulated
    error of the previously quantized columns. The error is calculated on a small
    representative batch of data. In contrast, SmoothQuant is targeted to better quantization
    of activations. The authors notice that in the activations they were observing,
    a few channels had consistently higher values on various tokens. They introduced
    per-channel scaling factors to carry the dynamic range of activations over into
    weights. This way they transferred part of quantization burden from harder-to-quantize
    activations to easier-to-quantize weights. In all quantization approaches, the
    goal is always to enable a low-bit, e.g. 4 bits per element, storage for the tensors,
    with a common scaling vector, and, in some cases, bias vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Further refinements of the algorithmic and software solutions have only limited
    impact on the overall efficiency if not supported by hardware. Most of the modern
    LLM models are designed and run on Graphics Processing Unit (GPUs) which exploit
    floating-point arithmetic [[11](#bib.bibx11), [12](#bib.bibx12)]. As mentioned
    earlier, the computational load required by modern transformers has reached such
    enormous volumes that traditional GPUs cannot fully meet the growing demand, pushing
    both accelerators and high performance GPUs towards narrow arithmetic. As a consequence,
    unmatched research efforts have been applied by the engineering community to replace
    narrow floating-point with even denser fixed-point representations [[13](#bib.bibx13),
    [14](#bib.bibx14), [15](#bib.bibx15), [16](#bib.bibx16)]. Despite the excellent
    gains in both speed and computational density achieved by fixed-point arithmetic,
    training using it or even half-precision floating-point arithmetic has not provided
    clear evidence in its favor due to the limited dynamic range inherent in such
    formats [[17](#bib.bibx17)].
  prefs: []
  type: TYPE_NORMAL
- en: Block Floating Point (BFP) numerical formats have received renewed interest
    recently for LLM inference applications due to their combination of wide dynamic
    range, numerical accuracy, and efficient hardware implementation of inner products
    using simple integer arithmetic [[18](#bib.bibx18), [19](#bib.bibx19), [20](#bib.bibx20),
    [21](#bib.bibx21)]. BFP formats are characterized by a block of mantissas with
    a shared scale factor. The simplest implementation has the scale factor as a power
    of two, the so-called exponent, in which case the inner product between two blocks
    involves multiplying the integer mantissas and adding the two block exponents.
    The industry has thus far mainly exploited BFP12 (with $4$ elements [[19](#bib.bibx19),
    [20](#bib.bibx20), [18](#bib.bibx18), [21](#bib.bibx21)]. Alternative formats,
    using low-bit floating point elements, with a wider range common exponent, are
    also considered [[22](#bib.bibx22)].
  prefs: []
  type: TYPE_NORMAL
- en: One of the main numerical issues faced by the ML engineers dealing with LLMs
    both from theoretical and practical perspectives is the sporadic emergence of
    so-called outliers in weights and activations of the modern large-scale transformers
    [[10](#bib.bibx10), [7](#bib.bibx7)]. Existence of outliers becomes especially
    challenging when it comes to block formats, since presence of even a single element
    with an extremely large magnitude in a block can completely ruin the quantization
    accuracy of all the other elements in that same block.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we address this problem. We demonstrate how the advantages of the BFP
    quantization can be maintained when weights or activations contain numerous outliers.
    The key observation behind our approach consists in the fact that the inner product
    is invariant to synchronized reshuffling of the tensors being multiplied. For
    instance, if we focus on the ${\bm{q}}{\bm{K}}^{\top}$ happens at the compile
    time. It requires no calibration data and has no effect on the inference latency.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the paper is organized as follows. In section [II](#S2 "II Inference
    in LLMs ‣ Accurate Block Quantization in LLMs with Outliers") we describe the
    setup in more detail and define the block formats. Section [III](#S3 "III K-sort
    Algorithm ‣ Accurate Block Quantization in LLMs with Outliers") features our novel
    ${\bm{K}}$ cache containing outliers. Supporting empirical data is provided in
    Section [IV](#S4 "IV Experiments ‣ Accurate Block Quantization in LLMs with Outliers").
    We summarize our findings in Section [V](#S5 "V Conclusion ‣ Accurate Block Quantization
    in LLMs with Outliers").
  prefs: []
  type: TYPE_NORMAL
- en: II Inference in LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we focus on the problem of inference in LLMs. The sizes of the
    up-to-date models have become so large and the amount of compute involved became
    so enormous that efficient processing requires dedicated hardware and specialized
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: II-A KV-cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inference on modern transformers essentially means sequential generation of
    tokens one by one given the initial prompt. After every pass through the model’s
    stack of decoders, the newly generated token is appended to the growing sequence
    and the process repeats with the updated context. The very nature of the attention
    mechanism requires calculation of the keys and values for the entire sequence
    generated up until current iteration. This leads to a lot of duplicated compute
    since every head inside every decoder block will repeatedly calculate the entire
    ${\bm{K}}$ contains numerous outliers.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Block Floating Point Formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The unprecedented and ever growing amount of compute and storage required by
    the modern LLMs has lead to the development of numerous new data formats and novel
    directions and techniques involving quantization of weights and activations. New
    data formats are announced every few months both by the computer science community
    training the models [[23](#bib.bibx23), [24](#bib.bibx24)] and by the manufacturers
    of hardware [[18](#bib.bibx18), [22](#bib.bibx22), [25](#bib.bibx25), [21](#bib.bibx21)].
    Different techniques are proposed separately for storage and for compute [[7](#bib.bibx7),
    [24](#bib.bibx24), [23](#bib.bibx23)].
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we focus on an extremely promising Block Floating Point family
    of formats that has become very popular in the recent months [[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20), [21](#bib.bibx21)]. The idea is based on
    the observation that quite often the elements of involved tensors have comparable
    amplitudes and thus can share the same or close exponent value when written in
    floating-point notation. As a consequence, we can store entire blocks of elements
    using shared exponent and individual integer mantissas. Numerous companies design
    there hardware specifically to support this family of formats [[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20)]. The main advantage enjoyed by the chips
    designed to support BFP formats consists in very significant reduction of required
    storage and effectively integer matrix multiplication, see [[19](#bib.bibx19),
    [20](#bib.bibx20)] for more details. This further leads to a huge reduction in
    consumed power and energy.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, a Block Floating Point format is characterized by the block
    size $n\in\mathbb{N}$, and their values are computed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\{2^{e}\cdot M_{1},\dots,2^{e}\cdot M_{n}\},$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $e$-bit integer.
  prefs: []
  type: TYPE_NORMAL
- en: Blocks formats are extremely efficient for matrix operations, since dot product
    using this family of formats effective turns into integer matrix multiplication
    and simple addition of the corresponding block exponents [[20](#bib.bibx20), [18](#bib.bibx18),
    [21](#bib.bibx21)]. The typical values of $p$.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Sorting Channels of ${\bm{W}}_{\bm{k}}$
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To efficiently store matrix ${\bm{K}}$ faster, we propose to quantize the former
    into a low-precision block format. The definition of the BFP format, says that
    the quantization range of a block is determined by its largest (in absolute value)
    element. If some blocks contain outliers, their overall quantization accuracy
    will be poor because the smallest elements might be rounded to zero. Next we show
    how to resolve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The natural approach would be to sort the elements of the tensor by their absolute
    values before quantization. In that case, each block will only contain elements
    of comparable magnitudes: there will be blocks with larger elements and blocks
    with smaller elements, but we will avoid the undesirable scenario of having numerous
    blocks containing mixtures of elements of wide dynamic range. However, we must
    note that sorting tensors on the fly would be prohibitively expensive. Also, if
    we need to keep the sorting order to restore the original one for every token
    for every attention layer, it would outweigh any memory savings. Therefore, the
    brute-force sorting of elements will not work and we need a finer approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8818003d88b76896579d4cef2b6f2c06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Left: original ${\bm{W}}_{\bm{k}}$ since the entries of the former
    ending up in same blocks are closer in their absolute values.'
  prefs: []
  type: TYPE_NORMAL
- en: As noted in [[7](#bib.bibx7)], the keys tend to exhibit certain outlier patterns.
    Namely, the outliers often concentrate in particular channels, which are quite
    consistent both across tokens in input sequences, and across different input sequences.
    Such behavior is usually caused by higher norms of the corresponding rows of ${\bm{W}}_{\bm{k}}$,
    then due to the linearity of inner product we can say that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}_{q}^{\top}\cdot{\bm{W}}_{\bm{k}}=[\pi({\bm{W}}_{\bm{q}})]^{\top}\cdot\pi({\bm{W}}_{\bm{k}}).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: As a consequence, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Now that we have applied permutation $\pi$. The idea is illustrated by Figure
    [1](#S2.F1 "Figure 1 ‣ II-C Sorting Channels of 𝐖_𝐤 ‣ II Inference in LLMs ‣ Accurate
    Block Quantization in LLMs with Outliers"). The colors of the heat-map reflect
    the absolute values of the elements, from lower (green) to larger (red). It is
    important to note that we do not store the queries in the cache and they can therefore
    be cast to a higher precision format. To enable application of this technique
    to any transformer, we need to show how it works when rotary embeddings are applied
    to keys and queries.
  prefs: []
  type: TYPE_NORMAL
- en: II-D Rotary Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many modern LLMs use rotary positional embeddings (RoPE) [[26](#bib.bibx26)]
    to encode information about the order of tokens in the input sequence. Rotary
    embeddings are linear transformations applied to keys and queries defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $${\bm{R}}_{\Theta,m}^{d_{h}}=\begin{pmatrix}\cos{m\theta_{1}}&amp;-\sin{m\theta_{1}}&amp;\cdots&amp;0\\
    \sin{m\theta_{1}}&amp;\cos{m\theta_{1}}&amp;\cdots&amp;0\\'
  prefs: []
  type: TYPE_NORMAL
- en: \vdots&amp;\ddots&amp;\ddots&amp;\vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\cdots&amp;\cos{m\theta_{d_{h}/2}}&amp;-\sin{m\theta_{d_{h}/2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\cdots&amp;\sin{m\theta_{d_{h}/2}}&amp;\cos{m\theta_{d_{h}/2}}\end{pmatrix},$$
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $m$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $${\bm{R}}^{d_{h}}_{\Theta,m}{\bm{x}}=\begin{pmatrix}x_{1}\\ x_{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: x_{3}\\
  prefs: []
  type: TYPE_NORMAL
- en: x_{4}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: x_{d_{h}-1}\\
  prefs: []
  type: TYPE_NORMAL
- en: x_{d_{h}}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{d_{h}/2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \cos{m\theta_{d_{h}/2}}\end{pmatrix}\\
  prefs: []
  type: TYPE_NORMAL
- en: +\begin{pmatrix}-x_{2}\\
  prefs: []
  type: TYPE_NORMAL
- en: x_{1}\\
  prefs: []
  type: TYPE_NORMAL
- en: -x_{4}\\
  prefs: []
  type: TYPE_NORMAL
- en: x_{3}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: -x_{d_{h}}\\
  prefs: []
  type: TYPE_NORMAL
- en: x_{d_{h}-1}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{1}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{d_{h}/2}}\\
  prefs: []
  type: TYPE_NORMAL
- en: \sin{m\theta_{d_{h}/2}}\end{pmatrix},$$ |  | (4) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\otimes$ is the element-wise product. Next we provide a general version
    of our sorting algorithm that works well when rotary embeddings are used.
  prefs: []
  type: TYPE_NORMAL
- en: III K-sort Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main idea of our ${\bm{K}}$ is known at the compile time, all the necessary
    permutations of the frequencies and signs needed for correct application of RoPE
    can be done then as well - this does not delay the inference.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 ${\bm{K}}$-sort algorithm for a head
  prefs: []
  type: TYPE_NORMAL
- en: '1: ${\bm{N}}_{i}\leftarrow||{\bm{W}}_{\bm{k}}[i,:]||,\;\forall i\in\{1,\dots,d_{h}\}$'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we propose to use ${\bm{K}}$-bits per element mantissa without
    any significant effect on the performance.
  prefs: []
  type: TYPE_NORMAL
- en: While present work concentrates on the keys ${\bm{K}}$ stored in the cache without
    any run-time overhead. For the lack of space, we postpone the details for further
    publications.
  prefs: []
  type: TYPE_NORMAL
- en: IV Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerous recent publications have reported the issues of outliers in K-cache
    and their significant impact on the accuracy and storage requirements [[10](#bib.bibx10),
    [7](#bib.bibx7), [27](#bib.bibx27)]. For the lack space, in this short contribution
    we focus on one of such popular LLMs, Llama2-7B-hf model [[2](#bib.bibx2)]. As
    shown in [[7](#bib.bibx7)], this network and its many relatives and variations
    exhibit the K-outliers phenomenon very clearly. In this section, we demonstrate
    the advantages of our ${\bm{K}}$-sort. This implies that the gain on larger models
    will be even more remarkable.
  prefs: []
  type: TYPE_NORMAL
- en: The experiments were carried out using the default Hugging Face checkpoint without
    extra fine-tuning. The baseline perplexity of the model with FP16 weights on wikitext-2
    [[28](#bib.bibx28)] dataset is $9.4881$-s are not stored in the cache so their
    compression is not required. For fair comparison, the rest of the operations were
    performed exactly as in the baseline model - in FP16 format.
  prefs: []
  type: TYPE_NORMAL
- en: Table [I](#S4.T1 "TABLE I ‣ IV Experiments ‣ Accurate Block Quantization in
    LLMs with Outliers") demonstrates the obtained results. As a sanity check, we
    see that for the block size of $128$.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: LLama2-7B perplexity on wikitext-2'
  prefs: []
  type: TYPE_NORMAL
- en: '| format | algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Q | K | original | ${\bm{K}}$-sorted |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | FP16 | 9.4881 | 9.4881 |'
  prefs: []
  type: TYPE_TB
- en: '| BFP16_128 | BFP12_128 | 10.0861 | 10.0861 |'
  prefs: []
  type: TYPE_TB
- en: '| BFP16_64 | BFP12_64 | 9.9999 | 9.6061 |'
  prefs: []
  type: TYPE_TB
- en: '| BFP16_32 | BFP12_32 | 9.8300 | 9.5196 |'
  prefs: []
  type: TYPE_TB
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we demonstrate that simple reshuffling of the static weights
    in popular LLMs can make their quantization quality much better. Specifically,
    we advocate for the use of Block Floating Point formats and show that BFP12 format
    with $4$-cache and therefore allows generation of much longer sequences on the
    same hardware.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Susan Zhang et al. “OPT: open pre-trained transformer language models”
    In *arXiv preprint arXiv:2205.01068*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Hugo Touvron et al. “Llama 2: open Foundation and Fine-Tuned Chat Models”
    In *arXiv preprint arXiv:2307.09288*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] OpenAI “GPT-4 Technical Report” In *arXiv preprint arXiv:2303.08774*, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Albert Q. Jiang et al. “Mixtral of Experts” In *arXiv preprint arXiv:2401.04088*,
    2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Gemma Team et al. “Gemma: open Models Based on Gemini Research and Technology”
    In *arXiv preprint arXiv:2403.08295*, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Vaswani et al. “Attention is all you need” In *Advances in Neural Information
    Processing Systems* 30, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Coleman Hooper et al. “KVQuant: towards 10 Million Context Length LLM Inference
    with KV Cache Quantization” In *arXiv preprint arXiv:2401.18079*, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Yiran Ding et al. “LongRoPE: Extending LLM Context Window Beyond 2 Million
    Tokens” In *arXiv preprint arXiv:2402.13753*, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler and Dan Alistarh “GPTQ:
    accurate Post-Training Quantization for Generative Pre-trained Transformers” In
    *arXiv preprint arXiv:2210.17323*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Guangxuan Xiao et al. “SmoothQuant: accurate and Efficient Post-Training
    Quantization for Large Language Models” In *arXiv preprint arXiv:2211.10438*,
    2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y.. Wang, G.-Y. Wei and D. Brooks “Benchmarking TPU, GPU, and CPU platforms
    for deep learning” In *arXiv preprint arXiv:1907.10701*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Srinivas et al. “Bottleneck transformers for visual recognition” In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 16519–16529'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A.. Zadeh, I. Edo, O.. Awad and A. Moshovos “GOBO: quantizing attention-based
    NLP models for low latency and energy efficient inference” In *IEEE/ACM International
    Symposium on Microarchitecture*, 2020, pp. 811–824 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] O. Zafrir, G. Boudoukh, P. Izsak and M. Wasserblat “Q8BERT: Quantized
    8bit BERT” In *Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS
    Edition*, 2019, pp. 36–39 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Shen et al. “Q-BERT: hessian based ultra low precision quantization
    of BERT” In *Proceedings of the AAAI Conference on Artificial Intelligence* 34.05,
    2020, pp. 8815–8821'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. Zhang et al. “TernaryBERT: distillation-aware ultra-low bit BERT” In
    *arXiv preprint arXiv:2009.12812*, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P. Micikevicius et al. “Mixed precision training” In *arXiv preprint arXiv:1710.03740*,
    2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Bita Darvish Rouhani et al. “Pushing the limits of narrow precision inferencing
    at cloud scale with microsoft floating point” In *Advances in neural information
    processing systems* 33, 2020, pp. 10271–10281'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] I. Lyubomirsky and X. Wang “Block Floating Point (BFP) for Efficient Deep
    Neural Net Inference” In *IEEE P3109 Working Group, June 6*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] I. Soloveychik, I. Lyubomirsky, X. Wang and S. Bhoja “Block Format Error
    Bounds and Optimal Block Size Selection” In *arXiv preprint arXiv:2210.05470*,
    2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Microsoft “MX Pytorch Emulation Library” In *https://github.com/microsoft/microxcaling*,
    2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Bita Darvish Rouhani et al. “Microscaling Data Formats for Deep Learning”
    In *arXiv preprint arXiv:2310.10537*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Shuming Ma et al. “The Era of 1-bit LLMs: all large language lodels are
    in 1.58 bits” In *arXiv preprint arXiv:2402.17764*, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Houwen Peng et al. “FP8-lm: Training FP8 large language models” In *arXiv
    preprint arXiv:2310.18313*, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Paulius Micikevicius et al. “FP8 formats for deep learning” In *arXiv
    preprint arXiv:2209.05433*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jianlin Su et al. “RoFormer: enhanced transformer with rotary position
    embedding” In *Neurocomputing* 568 Elsevier, 2024, pp. 127063'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Tim Dettmers, Mike Lewis, Younes Belkada and Luke Zettlemoyer “LLM.int8():
    8-bit Matrix Multiplication for Transformers at Scale” In *arXiv preprint arXiv:2208.07339*,
    2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Stephen Merity, Caiming Xiong, James Bradbury and Richard Socher “Pointer
    sentinel mixture models” In *arXiv preprint arXiv:1609.07843*, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
