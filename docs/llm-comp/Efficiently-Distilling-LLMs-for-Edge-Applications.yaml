- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:58:49'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently Distilling LLMs for Edge Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01353](https://ar5iv.labs.arxiv.org/html/2404.01353)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \NewDocumentCommand\LeftComment
  prefs: []
  type: TYPE_NORMAL
- en: 's m\IfBooleanF#1$\triangleright$ #2'
  prefs: []
  type: TYPE_NORMAL
- en: Achintya Kundu
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: achintya.k@ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Fabian Lim'
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: flim@sg.ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Aaron Chew'
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: aaron.chew1@ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: Laura Wynter
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: lwynter@sg.ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Penny Chong'
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: penny.chong@ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Rhui Dih Lee'
  prefs: []
  type: TYPE_NORMAL
- en: IBM Research
  prefs: []
  type: TYPE_NORMAL
- en: rhui.dih.lee@ibm.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Supernet training of LLMs is of great interest in industrial applications as
    it confers the ability to produce a palette of smaller models at constant cost,
    regardless of the number of models (of different size / latency) produced. We
    propose a new method called Multistage Low-rank Fine-tuning of Super-transformers
    (MLFS) for parameter-efficient supernet training. We show that it is possible
    to obtain high-quality encoder models that are suitable for commercial edge applications,
    and that while decoder-only models are resistant to a comparable degree of compression,
    decoders can be effectively sliced for a significant reduction in training time.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given their sizes up to billions of parameters, [[27](#bib.bib27), [4](#bib.bib4)],
    it is challenging for enterprises to fine-tune Large Language Models (LLMs), and
    furthermore they are not suitable for deployment on edge devices with limited
    memory and computational power. We wish to enable LLMs on edge environments for
    enterprise use cases. This requires the following two capabilities. (1) Accommodating
    a variety of edge device hardware: A single fine-tuned model is not optimal across
    the spectrum of devices. For industrial applications, a palette of fine-tuned
    LLMs is required for different hardware. (2) Dynamically changing resource levels:
    At run-time, the available resources on edge devices evolve over time, and appropriate
    model should be dynamically selected based on the available resources of each
    device.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A considerable amount of research has focused on compressing LLMs [[45](#bib.bib45),
    [31](#bib.bib31), [25](#bib.bib25), [26](#bib.bib26), [18](#bib.bib18), [15](#bib.bib15)].
    Methods that train a single small model guided by a large teacher model such as
    DistilBERT [[31](#bib.bib31)] and BERT-PKD [[33](#bib.bib33)], either achieve
    limited compression or do not scale to a large number of deployment devices. Supernet
    training methods [[14](#bib.bib14), [39](#bib.bib39), [6](#bib.bib6), [20](#bib.bib20),
    [22](#bib.bib22), [17](#bib.bib17)] were introduced to address these limitations:
    multiple smaller subnets within the supernet are trained simultaneously with weight-sharing.
    This one-time training approach produces a palette of smaller models, helping
    mitigate the computational cost of fine-tuning a model for each deployment scenario.
    However, the full-parameter supernet training approach is impractical when fine-tuning
    of an LLM is required for multiple deployment scenarios, limiting its utility
    for enterprises.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation
    (LoRA) reduces the number of trainable parameters by allowing only rank-decomposition
    matrices to be trained while freezing the pre-trained weights of the model. PEFT
    methods, however, are not applicable to supernet training due to the implications
    on the weight-shared subnetworks. Our work bridges this gap to enable efficient
    fine-tuning of LLMs for edge devices. Our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose a parameter-efficient, distillation-based approach for supernet training
    of LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We devise a gradient scaling scheme to improve convergence speed of any form
    of supernet training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We demonstrate significant compression of encoder models for edge. We highlight
    the limits of comparable compression for decoder models, while demonstrating a
    huge reduction in the steps needed for convergence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classical compression methods have been used for LLMs including pruning [[24](#bib.bib24),
    [34](#bib.bib34)], low rank approximation [[23](#bib.bib23), [21](#bib.bib21)],
    and quantization [[32](#bib.bib32), [41](#bib.bib41), [3](#bib.bib3)]. Knowledge
    distillation (KD) is adopted in BERT-PKD [[33](#bib.bib33)], tinyBERT [[18](#bib.bib18)],
    and distilBERT [[31](#bib.bib31)] and [[12](#bib.bib12)] in MiniLLM to distill
    knowledge from the layers of a large transformer model to a smaller one. See also
    the survey [[45](#bib.bib45)]. All these existing methods produce a single compressed
    model, unsuitable for edge scenarios with multiple deployment devices having varying
    computational capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural architecture search (NAS) based on reinforcement learning [[46](#bib.bib46)]
    and evolutionary algorithms [[28](#bib.bib28), [44](#bib.bib44)] trains every
    possible architecture and is very slow. Weight-sharing NAS was thus developed:
    in Guo et al. [[13](#bib.bib13)], Cai et al. [[5](#bib.bib5)], the building blocks
    in the same layer are isolated as all architectures are single paths. Weight-sharing
    NAS does not scale well to large architecture search spaces, hence, weight-entangled
    NAS, where subnets with common parts share weights, was introduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For resource-constrained edge deployment, supernet training [[6](#bib.bib6),
    [20](#bib.bib20), [8](#bib.bib8), [39](#bib.bib39), [11](#bib.bib11), [10](#bib.bib10)]
    was developed as a mode of jointly training multiple subnetworks (subnets) with
    entangled weights: one trains the supernet only once for all deployment scenarios.
    Cai et al. [[6](#bib.bib6)] introduced an elastic convolutional neural network
    with "progressive shrinkage", where larger subnets are trained first. Recent works
    have improved sampling strategies, e.g. the sandwich rule with in-place distillation
    [[40](#bib.bib40)], attentive sampling [[36](#bib.bib36)], stochastic nature gradient
    [[43](#bib.bib43)], or post-training sampling [[22](#bib.bib22)]. Our work is
    related to supernet training for transformer models [[14](#bib.bib14), [43](#bib.bib43),
    [38](#bib.bib38), [37](#bib.bib37), [8](#bib.bib8)]. This gradient scaling technique
    can be used with any of the above supernet methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter-efficient fine-tuning (PEFT) has been of great benefit in fine tuning
    LLMs. BitFit [[2](#bib.bib2)] updates the bias terms in pre-trained models while
    freezing the remaining parameters. LoRA [[16](#bib.bib16)] decomposes attention
    weight gradients into low-rank matrices to reduce the number of trainable parameters.
    AdaLoRA [[42](#bib.bib42)] and QLoRA [[9](#bib.bib9)] further improve LoRA [[16](#bib.bib16)].
    Note that PEFT allows fine-tuning a base model on a single GPU but does not produce
    smaller models. None of the PEFT methods can be used for weight-entangled supernet
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Solution Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For use in enterprise settings, the solution must allow fine-tuning of models
    on a small GPU footprint. In addition, inference cost in terms of storage must
    be minimised. We therefore design a solution which does not store the full size
    model checkpoint for every downstream task but only the frozen weights of the
    pre-trained base model and the low rank matrices. For inference in commercial
    edge use cases, we wish to enable storing the desired models locally for a wide
    variety of edge device resource requirements. We thus develop an approach where
    storage is minimised, storing only one base model and as many low rank adapter
    matrices as there are target model size variations, where low-rank adapters are
    very small. If the model is stored locally on an edge device, our proposed slicing
    operation takes place where the supernet fine-tuning is performed and the desired
    model is downloaded for inference. The slicing operation takes place for each
    model size-task combination and each resulting subnet can be cached for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we provide notation. Given a transformer model with architectural configuration
    $\Phi$ are learnt by minimizing training loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathop{\mathrm{argmin}}_{\mathrm{W}}\Big{[}\,\mathcal{L}_{\Phi}(\mathrm{W})\,:=\,\mathbb{E}\!\big{[}\,\ell\!\left[\,\mathrm{f}_{\Phi}\!\left(x;\mathrm{W}\right),y\,\right]\,\big{]}\,\Big{]},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{E}$) for classification or causal language modeling loss for
    generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduce the super-transformer and related terminologies. We define
    three types of networks - Teacher network, Super-transformer (supernet) and Sub-transformer
    (subnet). The teacher is a fixed network with the same configuration as the pre-trained
    transformer. A super-transformer is a dynamic model whose architectural dimensions
    (embedding dimension, number of heads, number of layers, etc.) are configurable
    at run time. The maxnet (resp. minnet) is the largest (resp. smallest) network
    in the super-transformer’s architecture space. Weight entanglement (weight-sharing)
    allows super-transformer weights to be used across sub-transformers, which are
    subsets of the super-transformer. Pre-trained transformer weights initialise the
    super-transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamic nature of a super-transformer is explicitly specified via a set
    $\mathcal{A}$ into weights of a sub-transformer model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{W}_{\Phi}:=\boldsymbol{\Pi}_{\Phi}\!\left(\mathrm{W}_{\!\texttt{Sup}}\right),\,\forall\Phi\in\mathcal{A}.$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'The aim of a weight-sharing super-transformer is to simultaneously train all
    the transformer models $\{\mathrm{f}_{\Phi}(\cdot;\boldsymbol{\Pi}_{\Phi}(\mathrm{W})):\mathcal{X}\to\mathcal{Y}\,|\,\Phi\in\mathcal{A}\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{E}$ estimated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\{\Phi_{1},\cdots,\Phi_{K}\}$ to approximate the expectation in ([1](#S4.E1
    "Equation 1 ‣ 4 Problem Formulation ‣ Efficiently Distilling LLMs for Edge Applications")).
    Fine-tuning LLM super-transformers is computationally challenging in enterprise
    use cases as it involves computing gradients of sub-transformers’ loss functions
    with respect to a huge number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Multistage Low-rank Fine-tuning of Super-transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We therefore developed Multistage Low-rank Fine-tuning of Super-transformers
    (MLFS). Given a teacher model with configuration $\Phi_{\texttt{Tch}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{W}_{\texttt{Tch}}:=\mathrm{W}^{\mathrm{pretrain}}_{\texttt{Tch}}+A_{0}*B_{0},$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $A_{0},B_{0}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: Stage-$s$, at every iteration. We call this Multistage Low-rank Fine-tuning
    of Super-transformers (MLFS) and present it in the Appendix in Algorithm [1](#alg1
    "Algorithm 1 ‣ 5 Multistage Low-rank Fine-tuning of Super-transformers ‣ Efficiently
    Distilling LLMs for Edge Applications").
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let the individually fine-tuned weights of a subnet, $\Phi$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{array}[]{l}\Delta\mathrm{W}_{\Phi}=\boldsymbol{\Pi}_{\Phi}\left(\sum_{s=0}^{2}A_{s}*B_{s}\right),~{}\forall\Phi\in\mathcal{A},\end{array}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\{A_{s},B_{s}\}_{s=0,1,2}$.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the computational savings, recall $\mathrm{W}^{\mathrm{pretrain}}_{\texttt{Tch}}\in\mathbb{R}^{d\times
    d}$ parameters at every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Multistage Low-rank Fine-tuning of Super-transformers (MLFS)
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Transformer model (teacher) with configuration $\Phi_{\texttt{Tch}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss functions: Target task loss $\ell_{\texttt{task}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multistage Training:'
  prefs: []
  type: TYPE_NORMAL
- en: '1:for stage $s=0,1,2$ of the super-transformer’s loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 16:     end for17:end for
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: $\{A_{s}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For faster convergence of the smaller sub-transformers within a super- transformer,
    we propose a novel weighted-combination of the gradients of the sampled sub-transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let $1^{st}$, in Algorithm 1 is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{array}[]{l}\sum_{j=1}^{K}(n_{1}/n_{j})^{\gamma}\,\nabla_{\mathrm{W}}\mathcal{L}_{\Phi_{j}},\end{array}$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $\nabla_{\mathrm{W}}$ is a hyper-parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each sub-transformer gradient in ([14](#S5.E14 "Equation 14 ‣ Proposition 2
    ‣ Gradient Scaling ‣ 5 Multistage Low-rank Fine-tuning of Super-transformers ‣
    Efficiently Distilling LLMs for Edge Applications")), $\texttt{grad}^{j}$-th sub-transformer’s
    loss. Using first-order Taylor expansion, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{array}[]{l}\mathcal{L}_{\Phi_{j}}(\mathrm{W}+\boldsymbol{\delta})\approx\mathcal{L}_{\Phi_{j}}(\mathrm{W})+\langle\nabla_{\mathrm{W}}\mathcal{L}_{\Phi_{j}}(\mathrm{W}),\boldsymbol{\delta}\rangle,\end{array}$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\langle\cdot,\cdot\rangle$ can be approximated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where we approximate the $\|\cdot\|_{1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Distillation Loss for Super-transformers:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Knowledge distillation is straightforward in a fixed-network fine-tuning setting.
    However, it is less so when fine-tuning a supernet, and in particular, fine-tuning
    a supernet using the proposed multistage LoRA based approach. Specifically, the
    subnets receive two types of knowledge distillation (KD) from the teacher: (a)
    the usual KD loss that utilizes the output logits of the teacher and (b) distillation
    of features from transformer layers [[18](#bib.bib18)] of the teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: To define the distillation based losses precisely, let the forward-pass mapping
    of an input training sample $x^{i}$, acts as the teacher and knowledge distillation
    loss for all other sub-transformers w.r.t the teacher is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\texttt{KL}[\cdot,\cdot]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $g_{j}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{U}^{l}_{j}~{}:=~{}[\,\mathbf{U}^{g_{j}(l)}_{1}\,]_{\Phi_{j}}\in\mathbb{R}^{d_{\mathrm{low}}\times
    d_{j}},$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where the operation $[~{}]_{\Phi_{j}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Low-rank approach for Distilling an LLM onto a Pre-trained student
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we considerthe standard single-stage distillation approach
    for training a smaller (student) model on a target task with help from a larger
    fine-tuned (teacher) model. Here, we assume availability of the following: (i)
    target data set $\mathcal{D}_{train}$ is chosen between 4 and 16).'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Low-rank approach for Distilling an LLM onto a Pre-trained Student
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Larger transformer model (teacher) with configuration $\Phi_{\texttt{Tch}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss functions: Target task loss $\ell_{\texttt{task}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 1:Initialize the low-rank matrices $\{A_{1},B_{1}\}$.9:end for
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: $\{A_{1},B_{1}\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Given an input sample $x^{i}$ is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma(\cdot)$ is called the KD temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Given input $x^{i}$-th layer. Now, we define the feature based distillation
    loss as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the summation $l$ of the projected space is chosen to be small, 128 in
    our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Results on Encoder and Decoder LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We report performance on encoder tasks using GLUE [[35](#bib.bib35)] with BERT[base]
    as the teacher model $\Phi_{\texttt{Tch}}$. On GLUE, we use the train set for
    fine-tuning and the dev set for accuracy evaluation. For santacoder, we evaluate
    performance using HumanEval [[7](#bib.bib7)] and report pass@1 scores. All experiments
    were conducted using PyTorch on a single Nvidia A100 (40GB) GPU.
  prefs: []
  type: TYPE_NORMAL
- en: In MLFS, the Low rank matrices are added on the QKV vectors and the intermediate
    size of feed-forward network (FFN) layers. We set $\beta_{l}=0.1\,\forall l$ for
    all other data sets. Following [[16](#bib.bib16)], we use the fine-tuned MNLI
    checkpoint to initialize the model weights for experiments on small data sets
    such as RTE and MRPC.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Performance of Encoder Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare performance of encoder models obtained with the MLFS approach against
    a static, fixed model (BERT base) from [[43](#bib.bib43), [14](#bib.bib14)], two
    popular distilled variants of the fixed model: TinyBERT [[18](#bib.bib18)] and
    DistilBERT [[31](#bib.bib31)], and models trained using existing super-transformer
    methods (DynaBERT [[14](#bib.bib14)]. Figure [1](#S7.F1 "Figure 1 ‣ 7.1 Performance
    of Encoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications") shows the performance of the palette of models, from
    a 45M param. minnet to full-size 110M maxnet. Model performance against forward-pass
    latency is plotted in Figure [2](#S7.F2 "Figure 2 ‣ 7.1 Performance of Encoder
    Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for
    Edge Applications"). Encoder models produced by MLFS are at par or better than
    much costlier methods. Results of PD-BERT, BERT-PKD are from [[43](#bib.bib43)],
    static BERT from [[43](#bib.bib43)] for all except MRPC for which we use [[14](#bib.bib14)].
    Note that TinyBERT performs data augmentation leading to higher accuracy but much
    longer computation time. We do not perform data augmentation for fairness of the
    comparison to the other methods. The main observation is that MLFS provides accurate,
    smaller encoder models at 1/4 the size of the teacher and 1/3 its runtime latency
    on a single GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed9264d870176d14ccffdf71042904cf.png)![Refer to caption](img/28f5bd796c074ab61fd6497e938d8796.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Model size vs performance trade-off for task-specific BERT models
    produced by MLFS against other methods on 6 GLUE data sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c164e298b2274965c90fcff24c7b658e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Latency vs performance trade-off for task-specific BERT models produced
    by MLFS against other methods on 6 GLUE data sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18c6922851fd88d9e0e4d6739b153ed2.png)![Refer to caption](img/dbf0dcb978fdd6ca5c75cd33d02331f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Ablation study on gradient scaling: MLFS minnet convergence is improved
    using gradient scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6628b9324a56c94385575b9b2babdc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Ablation study on MLFS rank of $A,B$ is optimal for small and medium
    subnets.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study on Gradient Scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In supernet training, the weights of maxnet and subnets are shared and trained
    simultaneously. The maxnet tends to converge and overfit earlier than smaller
    subnets. The different convergence rates renders selecting a single supernet checkpoint
    for all networks difficult. Gradient scaling solves this by speeding up convergence
    of the smaller subnets to match that of the larger subnets or the maxnet. Fig.
    [3](#S7.F3 "Figure 3 ‣ 7.1 Performance of Encoder Models ‣ 7 Results on Encoder
    and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge Applications") shows that
    gradient scaling improves minnet convergence, indicated by lower minnet loss.
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study on Rank of $A,B$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Fig. [4](#S7.F4 "Figure 4 ‣ 7.1 Performance of Encoder Models ‣ 7 Results
    on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge Applications"),
    we examines the impact of rank $r$ for all other MLFS experiments. From the scale
    of the y-axis in [4](#S7.F4 "Figure 4 ‣ 7.1 Performance of Encoder Models ‣ 7
    Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge Applications"),
    observe that MLFS is not overly sensitive to the chosen rank.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Performance of Decoder Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Turning now to decoder models, we consider two code-pre-trained LLMs, Santacoder
    [[1](#bib.bib1)] and Codellama7B [[29](#bib.bib29)]. We evaluate a custom 0.7B
    parameter Santacoder model obtained from the 1.1B teacher. Due to an inability
    to fine-tune on the full 24M coding examples, we use up to 1.2M. Fig. [5](#S7.F5
    "Figure 5 ‣ 7.2 Performance of Decoder Models ‣ 7 Results on Encoder and Decoder
    LLMs ‣ Efficiently Distilling LLMs for Edge Applications") shows that MLFS pass@1
    improves rapidly as number of tokens increases from a low 10k to 400k to 1.2M
    examples, only 5% of the 24M examples. Table [1](#S7.T1 "Table 1 ‣ 7.2 Performance
    of Decoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications") shows analogous results with 3 small MLFS models.
    The improvement in pass@1 indicates that the smaller models retain the ability
    to learn from the larger teacher.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65650942696b998a69ff0b3db0536e35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Performance of MLFS on a custom Santacoder 0.7B model using 10K/400K/1.2M
    training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data set size | Model size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5B | 0.7B | 0.9B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10K | 4.5 | 8.6 | 13.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 400K | 4.7 | 9.5 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: HumanEval pass@1 (%) performance of 3 small models produced by MLFS
    from Santacoder 1.1B.'
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to encoder models, the compression levels that retain sufficient performance
    of the teacher with decoders is less. While MLFS retains accuracy performance
    of encoder models at 1/4 the size of the teacher, the decoder models are reduced
    to at most 2/3 the teacher’s size.
  prefs: []
  type: TYPE_NORMAL
- en: MLFS slicing of the teacher model can, however, benefit decoder models by reducing
    substantially the training/fine-tuning time needed compared to a randomly-initialised
    model, as shown in Fig. [6](#S7.F6 "Figure 6 ‣ 7.2 Performance of Decoder Models
    ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge
    Applications") on Santacoder sliced from 1.1B to 0.7B. In other words, when a
    smaller model is required for edge inference, one can train it from a random initialisation,
    or slice from a teacher as does MLFS, and train starting from the sliced weights.
    The latter significantly reduces training time as seen in the validation loss
    curves. We see the same benefit on Codellama, as shown below. See [[30](#bib.bib30)]
    for a similar observation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b544bbb64d4c715b1a68c830da927c04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Convergence comparison of validation loss while fine-tuning a custom
    model from random vs using MLFS. MLFS achieves low validation loss much faster.'
  prefs: []
  type: TYPE_NORMAL
- en: Results on CodeLlama-7B-Python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we experiment on fine-tuning custom smaller models of different sizes
    using the *CodeLlama-7B-Python* model as the teacher. We apply 1 epoch of MLFS
    on 2 different subsets of the *bigcode/the-stack* data set and produce 3 smaller
    variants having 4.5B, 5.3B & 6B model parameters in each case. Table [2](#S7.T2
    "Table 2 ‣ Results on CodeLlama-7B-Python ‣ 7.2 Performance of Decoder Models
    ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge
    Applications") shows HumanEval performance of these models as the number of examples
    is increased from 200K to 400K. Again, we see that the sliced CodeLlama retain
    their ability to learn and improve quickly as the number of examples increases.
    Note that the full data set includes 24M examples; MLFS achieves nearly 75% of
    the performance of fullsize CodeLlama after less than 2% of the examples.
  prefs: []
  type: TYPE_NORMAL
- en: '| Data set size | Model size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4.5B | 5.3B | 6B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 200K | 11.0 | 19.5 | 23.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 400K | 14.0 | 28.1 | 30.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: HumanEval pass@1 (%) performance of 3 small models produced by MLFS
    from CodeLlama-7B-Python'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study on Santacoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Fig. [7](#S7.F7 "Figure 7 ‣ Ablation Study on Santacoder ‣ 7.2 Performance
    of Decoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications"), we compare HumanEval performance of a 0.7B Santacoder
    model fine-tuned through full fine-tuning (FT) from random initialisation vs.
    full-rank (non-LoRA) MLFS with ($\alpha=0.9$) distillation. The improvement in
    the evaluation numbers is remarkable even after fine-tuning on up to only 5% of
    the examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de3c2ea698a0382f4c6b09cf18b9c47e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Superior performance of supernet training compared to other full
    fine-tuning based approaches on three data sets with 10K/400K/1.2M examples.'
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [8](#S7.F8 "Figure 8 ‣ Ablation Study on Santacoder ‣ 7.2 Performance
    of Decoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications"), we see better convergence of validation loss on
    the Santacoder 0.7B for MLFS with distillation loss (<math id="S7.SS2.SSS0.Px2.p2.1.m1.1"
    class="ltx_Math" alttext="\alpha></math>). This demonstrates the benefit of MLFS
    distillation as compared to full MLFS fine tuning of the sliced model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5c8f0038b90c08c83eca294796feded.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Convergence comparison of validation loss while fine-tuning a custom
    model using MLFS with/without distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Perspectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enterprise users require an efficient way to fine-tune LLMs for inference on
    edge devices of many sizes. We developed MLFS for such edge deployment scenarios.
    We demonstrate its benefits on encoder LLMs. We show the limitation of compressing
    decoder LLMs to a comparable degree; however, MLFS offers significant gains for
    smaller decoder training/fine-tuning by slicing from a larger pre-trained teacher.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Allal et al. [2023] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alex Gu, Manan Dey, et al. Santacoder: don’t reach for the stars! *arXiv preprint
    arXiv:2301.03988*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben Zaken et al. [2022] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
    BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 1–9, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1.
    URL [https://aclanthology.org/2022.acl-short.1](https://aclanthology.org/2022.acl-short.1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhandare et al. [2019] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,
    Vivek Menon, Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bit quantization
    of transformer neural machine language translation model. *arXiv preprint arXiv:1906.00532*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2018] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural
    architecture search on target task and hardware. *arXiv preprint arXiv:1812.00332*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2019] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song
    Han. Once-for-all: Train one network and specialize it for efficient deployment.
    *arXiv preprint arXiv:1908.09791*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2021a] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *arXiv preprint
    arXiv:2107.03374*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021b] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
    Autoformer: Searching transformers for visual recognition. In *Proceedings of
    the IEEE/CVF international conference on computer vision*, pages 12270–12280,
    2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. [2022] Peijie Dong, Xin Niu, Lujun Li, Linzhen Xie, Wenbin Zou,
    Tian Ye, Zimian Wei, and Hengyue Pan. Prior-guided one-shot neural architecture
    search. *arXiv preprint arXiv:2206.13329*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2022] Jiahui Gao, Hang Xu, Han Shi, Xiaozhe Ren, LH Philip, Xiaodan
    Liang, Xin Jiang, and Zhenguo Li. Autobert-zero: Evolving bert backbone from scratch.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36-10,
    pages 10663–10671, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2020] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
    Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with
    uniform sampling. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part XVI 16*, pages 544–560\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2020] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen,
    and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. *Advances in
    Neural Information Processing Systems*, 33:9782–9793, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes. *arXiv preprint arXiv:2305.02301*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2022] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. In *International Conference on Learning Representations*,
    2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jawahar et al. [2023] Ganesh Jawahar, Haichuan Yang, Yunyang Xiong, Zechun
    Liu, Dilin Wang, Fei Sun, Meng Li, Aasish Pappu, Barlas Oguz, Muhammad Abdul-Mageed,
    Laks V. S. Lakshmanan, Raghuraman Krishnamoorthi, and Vikas Chandra. Mixture-of-supernets:
    Improving weight-sharing supernet training with architecture-routed mixture-of-experts.
    *arXiv preprint arXiv:2306.04845*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. [2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language
    understanding. In *Findings of the Association for Computational Linguistics:
    EMNLP 2020*, pages 4163–4174, Online, November 2020\. Association for Computational
    Linguistics. doi: 10.18653/v1/2020.findings-emnlp.372. URL [https://aclanthology.org/2020.findings-emnlp.372](https://aclanthology.org/2020.findings-emnlp.372).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kocetkov et al. [2022] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
    Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean
    Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The
    stack: 3 tb of permissively licensed source code. *arXiv preprint arXiv:2211.15533*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kundu et al. [2023] Achintya Kundu, Laura Wynter, Rhui Dih Lee, and Luis Angel D.
    Bathen. Transfer-once-for-all: AI model optimization for edge. In *IEEE International
    Conference on Edge Computing and Communications, EDGE 2023, Chicago, IL, USA,
    July 2-8, 2023*, pages 26–35\. IEEE, 2023. URL [https://doi.org/10.1109/EDGE60047.2023.00017](https://doi.org/10.1109/EDGE60047.2023.00017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. [2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning
    of language representations. *arXiv preprint arXiv:1909.11942*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lou et al. [2021] Wei Lou, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, and
    Geoff V Merrett. Dynamic-ofa: Runtime dnn architecture switching for performance
    scaling on heterogeneous embedded platforms. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 3110–3118, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. [2019] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou,
    Ming Zhou, and Dawei Song. A tensorized transformer for language modeling. *Advances
    in neural information processing systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCarley et al. [2019] JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured
    pruning of a bert-based question answering model. *arXiv preprint arXiv:1910.06360*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee and Awadallah [2020] Subhabrata Mukherjee and Ahmed Awadallah. Xtremedistil:
    Multi-stage distillation for massive multilingual models. *arXiv preprint arXiv:2004.05686*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee et al. [2021] Subhabrata Mukherjee, Ahmed Hassan Awadallah, and Jianfeng
    Gao. Xtremedistiltransformers: Task transfer for task-agnostic distillation. *arXiv
    preprint arXiv:2106.04563*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. [2019] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    Regularized evolution for image classifier architecture search. In *Proceedings
    of the aaai conference on artificial intelligence*, volume 33-01, pages 4780–4789,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. [2023] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel
    Synnaeve. Code llama: Open foundation models for code, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samragh et al. [2023] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja
    Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari.
    Weight subcloning: direct initialization of transformers using larger pretrained
    ones, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. [2019] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
    *arXiv preprint arXiv:1910.01108*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. [2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra
    low precision quantization of bert. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, volume 34-05, pages 8815–8821, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2019] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge
    distillation for bert model compression. *arXiv preprint arXiv:1908.09355*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. [2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. *arXiv preprint arXiv:1905.09418*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform
    for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop
    BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353–355,
    Brussels, Belgium, November 2018\. Association for Computational Linguistics.
    doi: 10.18653/v1/W18-5446. URL [https://aclanthology.org/W18-5446](https://aclanthology.org/W18-5446).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2021] Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra. Attentivenas:
    Improving neural architecture search via attentive sampling. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*, pages 6418–6427,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu,
    Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efficient natural
    language processing. *arXiv preprint arXiv:2005.14187*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Rui Wang, Qibing Bai, Junyi Ao, Long Zhou, Zhixiang Xiong,
    Zhihua Wei, Yu Zhang, Tom Ko, and Haizhou Li. Lighthubert: Lightweight and configurable
    speech representation learning with once-for-all hidden-unit bert. *arXiv preprint
    arXiv:2203.15610*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021] Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin,
    and Tie-Yan Liu. Nas-bert: task-agnostic and adaptive-size bert compression with
    neural architecture search. In *Proceedings of the 27th ACM SIGKDD Conference
    on Knowledge Discovery & Data Mining*, pages 1933–1943, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2020] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan
    Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le.
    Bignas: Scaling up neural architecture search with big single-stage models. In
    *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part VII 16*, pages 702–717\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. [2019] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    Q8bert: Quantized 8bit bert. In *2019 Fifth Workshop on Energy Efficient Machine
    Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)*, pages 36–39\. IEEE,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng
    He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient
    fine-tuning. *arXiv preprint arXiv:2303.10512*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Shaokun Zhang, Xiawu Zheng, Chenyi Yang, Yuchao Li, Yan
    Wang, Fei Chao, Mengdi Wang, Shen Li, Jun Yang, and Rongrong Ji. You only compress
    once: Towards effective and elastic bert compression via exploit-explore stochastic
    nature gradient. *arXiv preprint arXiv:2106.02435*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2019] Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Erhu Zhao,
    and Yongjun Xu. Eena: efficient evolution of neural architecture. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision Workshops*, pages
    0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le [2016] Barret Zoph and Quoc V Le. Neural architecture search with
    reinforcement learning. *arXiv preprint arXiv:1611.01578*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
