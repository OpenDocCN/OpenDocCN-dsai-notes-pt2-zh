- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Distilling Text Style Transfer With Self-Explanation From LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01106](https://ar5iv.labs.arxiv.org/html/2403.01106)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chiyu Zhang^(1,2,⋆)    Honglong Cai²    Yuezhang (Music) Li²    Yuexin Wu²
  prefs: []
  type: TYPE_NORMAL
- en: Le Hou²     Muhammad Abdul-Mageed^(1,3)
  prefs: []
  type: TYPE_NORMAL
- en: ¹ The University of British Columbia    ² Google
  prefs: []
  type: TYPE_NORMAL
- en: ³Department of NLP & ML, MBZUAI
  prefs: []
  type: TYPE_NORMAL
- en: chiyuzh@mail.ubc.ca, {honglongcai, lyzmuisc}@google.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Text Style Transfer (TST) seeks to alter the style of text while retaining its
    core content. Given the constraints of limited parallel datasets for TST, we propose
    CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought
    (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning
    capabilities of LLMs into more streamlined models capable of working with both
    non-parallel and parallel data. Through experimentation across four TST datasets,
    CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation
    methods, particularly in low-resource settings. We conduct a comprehensive evaluation,
    comparing CoTeX against current unsupervised, supervised, in-context learning
    (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes
    itself by offering transparent explanations for its style transfer process.
  prefs: []
  type: TYPE_NORMAL
- en: Distilling Text Style Transfer With Self-Explanation From LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Chiyu Zhang^(1,2,⋆)    Honglong Cai²    Yuezhang (Music) Li²    Yuexin Wu² Le
    Hou²     Muhammad Abdul-Mageed^(1,3) ¹ The University of British Columbia    ² Google
    ³Department of NLP & ML, MBZUAI chiyuzh@mail.ubc.ca, {honglongcai, lyzmuisc}@google.com
  prefs: []
  type: TYPE_NORMAL
- en: ^†^† ^⋆Work done during internship at Google.![Refer to caption](img/8ad259d2ddbe1da444850d246349cc0a.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Overview of CoTeX framework. We use few-shot CoT prompting to generate
    reasoning paths and transferred texts from an LLM and then train a smaller task-specific
    model with generated data.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TST aims to rephrase a source text $s$ Jin et al. ([2022](#bib.bib17)). The
    term “style" can encompass the personal characteristics of an author, such as
    age, and pragmatic use like formality or toxicity. To develop TST systems using
    supervised methods, several human-annotated datasets have emerged Rao and Tetreault
    ([2018](#bib.bib37)). For instance, Rao and Tetreault ([2018](#bib.bib37)) introduced
    a corpus for formality style transfer, transforming informal language to its formal
    counterpart and vice versa. Nonetheless, supervised parallel data, crucial for
    training deep neural networks, is scarce and costly to obtain. Hence, unsupervised
    methodologies Shen et al. ([2017](#bib.bib42)); Liu et al. ([2021](#bib.bib24))
    have been proposed to manage stylistic attributes without relying on parallel
    data. Liu et al. ([2022](#bib.bib22)) and Zhang et al. ([2020](#bib.bib58)) create
    pseudo-parallel data from unlabeled samples via diverse data augmentation with
    task-specific knowledge. Works by Gong et al. ([2019](#bib.bib9)); Wang et al.
    ([2019a](#bib.bib47)); Reid and Zhong ([2021](#bib.bib38)) employ an auxiliary
    style classifier to steer the transfer direction. Meanwhile, Krishna et al. ([2020](#bib.bib18))
    and Hallinan et al. ([2023b](#bib.bib11)) deploy multiple style-specific models
    to produce various styles individually. Of late, LLMs have demonstrated exceptional
    prowess across diverse NLP tasks. Studies like Reif et al. ([2022](#bib.bib39));
    Pu and Demberg ([2023](#bib.bib35)) have found that extremely large LMs, with
    over 100B parameters, are adept at TST with ICL. Drawing from these findings,
    our paper uses LLMs to generate pseudo-parallel data and distills the TST skills
    of the LLM into a compact student model. Moreover, we enhance distillation and
    efficiency using CoT prompting.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have demonstrated impressive performance across various tasks and reasoning
    capabilities. CoT prompting Wei et al. ([2022](#bib.bib51)) is a promising technique
    that extracts these reasoning skills and enhances accuracy in target tasks. However,
    deploying these enormous LLMs poses computational and practical challenges. Recent
    studies Huang et al. ([2022](#bib.bib15)); Wang et al. ([2023a](#bib.bib48));
    Hsieh et al. ([2023](#bib.bib14)) have thus turned to offline knowledge distillation
    (KD) Hinton et al. ([2015](#bib.bib13)) to condense these reasoning capabilities
    into a smaller model. Using CoT rationales can also increase distillation efficiency
    with less data Li et al. ([2022](#bib.bib21)); Shridhar et al. ([2023](#bib.bib43)).
    Concurrently, Saakyan and Muresan ([2023](#bib.bib40)) examine CoT prompting combined
    with domain expert feedback for improved formality transfer. Nevertheless, the
    potential of CoT prompting and KD to enrich a broader range of TST tasks remains
    underexplored.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we present CoTeX framework, using CoT prompting to improve TST.
    It identifies cues for TST and clarifies the rewriting process ($\S$ [4](#S4 "4
    Results ‣ Distilling Text Style Transfer With Self-Explanation From LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Data Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ CoT combined with instruction prompting to extract rationales from
    LLMs regarding the TST process. We have two different settings (target-blind and
    target-aware) to generate rationales.
  prefs: []
  type: TYPE_NORMAL
- en: Target-Blind (TB). We first explore our method in the target-blind setting where
    we only give a source text and the name of the desired target style. This setting
    can be adaptable to a broader range of style transfer directions. As shown in
    the left side of Figure [1](#S0.F1 "Figure 1 ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs"), each input example is constructed using an
    instruction template, $p_{tb}$ examples created by humans as context before the
    actual input. In our implementation, we employ three manually crafted examples
    as few-shot prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Target-Aware (TA). For datasets with supervised parallel data, we use the instruction
    template $p_{ta}$ in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Training Student Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We leverage the LLM-generated data to finetune smaller, task-specific student
    models. For the data generated in the target-blind setting, we utilize the instruction
    template $p_{tb}$ employing the conventional cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Datasets and Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ four public datasets across three style transfer directions, chosen
    for their inclusion of human-annotated parallel data in both training and evaluation
    sets. This facilitates direct comparisons between different settings.
  prefs: []
  type: TYPE_NORMAL
- en: Formality Transfer. We use GYAFC dataset from Rao and Tetreault ([2018](#bib.bib37))
    and focus on the informal to formal language transfer direction. GYAFC dataset
    includes two domains, Family & Relationships (F&R) and Entertainment & Music (E&M).
     Detoxification. ParaDetox Logacheva et al. ([2022b](#bib.bib26)) is a parallel
    dataset for text detoxification.  Shakespeare to Modern English. Xu et al. ([2012](#bib.bib56))
    introduce a human-annotated dataset for translating text between William Shakespeare’s
    plays and their modernized versions.
  prefs: []
  type: TYPE_NORMAL
- en: Low-Resource Training. Our method offers advantages in low-resource settings,
    as the CoT is poised to enhance the learning efficiency of student models and
    bolster their generalizability. Thus, we create smaller training sets by randomly
    sampling training data, ranging from 1K to 20K.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metric. We report BLEU, leveraging the Sacre-BLEU Python library Post
    ([2018](#bib.bib33)), as main metric for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Model Comparison.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In low-resource settings, CoTeX is compared to (1) SFT: conventional supervised
    fine-tuning using parallel data, (2) teacher LLM: the teacher model evaluated
    on the Test set via few-shot ICL, i.e., using the three-shot prompt and template
    $p_{tb}$ described in Section [2](#S2 "2 Method ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs"), and (3) Distill: traditional offline knowledge
    distillation, which relies solely on LLM-generated pseudo-parallel data without
    a CoT path.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For comprehensive evaluations, CoTeX is further compared with (1) Prompt&Rank:
    a SoTA in-context learning method for TST Suzgun et al. ([2022](#bib.bib44)),
    and (2) instruction-tuned LLMs: open-source LLMs assessed through three-shot ICL
    using the same prompt and template described in Section [2](#S2 "2 Method ‣ Distilling
    Text Style Transfer With Self-Explanation From LLMs"); these LLMs include Alpaca
    7B Taori et al. ([2023](#bib.bib45)), Vicuna 7B Chiang et al. ([2023](#bib.bib4)),
    LLaMA2-Chat 7B Touvron et al. ([2023](#bib.bib46)), and FlanT5-XL Chung et al.
    ([2022](#bib.bib5)) (with 3B parameters). Additionally, for each dataset, comparisons
    are made with existing dataset-specific unsupervised and supervised methods. Unsupervised
    methods include DualRL Luo et al. ([2019](#bib.bib28)), STRAP Krishna et al. ([2020](#bib.bib18)),
    DLS He et al. ([2020](#bib.bib12)), and TSST Xiao et al. ([2021](#bib.bib54))
    for formality transfer; Mask&Infill Wu et al. ([2019](#bib.bib53)) and CondBERT Dale
    et al. ([2021](#bib.bib6)) for detoxification; and STRAP and TSST for modernizing
    Shakespearean text. Supervised methods include Multi-NMT Niu et al. ([2018](#bib.bib30)),
    GPT-CAT Wang et al. ([2019b](#bib.bib50)), and SemiFST Liu et al. ([2022](#bib.bib22))
    for formality transfer; ParaDetox Logacheva et al. ([2022a](#bib.bib25)) for detoxification;
    and PointerS2S Jhamtani et al. ([2017](#bib.bib16)) for modernizing Shakespearean
    text.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ PaLM2 Unicorn Anil et al. ([2023](#bib.bib1)) as our LLM for data
    generation. In the target-blind setting, we generate a CoT path and a transferred
    text.²²2Our ancillary study also examines the generation of multiple pairs of
    CoT paths and transferred text. For the target-aware approach, we solely produce
    a CoT path. Both approaches use a temperature of 0.7\. Afterward, we finetune
    a T5-large model (with 770M parameters) Raffel et al. ([2020](#bib.bib36)) with
    the curated dataset.³³3We provide a concise experiment of using T5-XL model in
    Appendix [D](#A4 "Appendix D Experiment with T5-XL ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs"). We finetune T5 for 2,000 steps with a learning
    rate of $1e-3$ and batch size of 128\. We evaluate validation performance every
    16 steps and report test result of the best step.⁴⁴4More details about hyperparameters
    are in Appendix [B](#A2 "Appendix B Hyperparameter for Training Student Model
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/076f10ada3168642eed4e245a1dadf0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Test results of low-resource settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now present your experimental results. CoTeX-TB and CoTeX-TA denote models
    trained using datasets created through target-blind and target-aware methods,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Low-Resource Settings. We first examine CoTeX’s impact in low-resource context.
    Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Implementation ‣ 3 Experiments ‣ Distilling
    Text Style Transfer With Self-Explanation From LLMs") shows CoTeX’s performance
    in both target-blind and target-aware settings across varying training data sizes.
    In both formality transfer datasets, CoTeX-TB outperforms SFT-T5 and Distill-T5\.
    This advantage is noticeable with limited data, specifically under 10K. For instance,
    using just 1K samples from the informal-formal (E&M) dataset, the BLEU scores
    for SFT, CoTeX-TB, and CoTeX-TA are 55.13, 68.62, and 65.40, respectively. We
    find that both CoTeX-TB and CoTeX-TA outperform or match the LLM’s performance
    on the two formality datasets. In translating Shakespearean to modern English,
    CoTeX-TB exhibits significant superiority over SFT-T5 and Distill-T5 across all
    data sizes. We believe that such an enhancement can be attributed to the high
    quality of LLM generations. LLM with few-shot in-context learning obtains a BLEU
    score of 32.43\. Though CoTeX-TB underperforms SFT on detoxification, CoTeX-TA
    still outperforms SFT in most data sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | BLEU | Method | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '|  | Formality (F&R) | Formality (E&M) |'
  prefs: []
  type: TYPE_TB
- en: '| Unsup. | DualRL | 53.01 | DLS | 23.09 |'
  prefs: []
  type: TYPE_TB
- en: '| TSST | 60.99 | STRAP | 31.39 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 ICL | Prompt&Rank | 30.60 | Prompt&Rank | 30.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Alpaca | 41.85 | Alpaca | 52.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | 37.09 | Vicuna | 46.47 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-C. | 19.62 | LLaMA2-C. | 25.14 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-XL | 55.70 | FlanT5-XL | 42.58 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 Sup. | Multi-NMT^† | 75.35 | Multi-NMT^† | 72.01 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-CAT^† | 77.26 | GPT-CAT^† | 71.39 |'
  prefs: []
  type: TYPE_TB
- en: '| SemiFST | 80.32 | SemiFST | 76.87 |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (ours) | 77.12 | SFT (ours) | 73.01 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 | Distill (ours) | 64.79 | Distill (ours) | 64.31 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 | CoTex-TB | 72.05 | CoTex-TB | 71.70 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CoTex-TA | 77.13 | CoTex-TA | 74.65 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Detoxification | Modernizing Shake. |'
  prefs: []
  type: TYPE_TB
- en: '| Unsup. | Mask&Infill^∗ | 44.77 | DLS | 12.85 |'
  prefs: []
  type: TYPE_TB
- en: '| CondBERT^∗ | 48.89 | STRAP | 19.96 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 ICL | Prompt&Rank | 11.06 | Prompt&Rank | 20.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Alpaca | 24.32 | Alpaca | 24.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | 34.54 | Vicuna | 17.76 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-C. | 14.65 | LLaMA2-C. | 25.19 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5-XL | 50.13 | FlanT5-XL | 21.55 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 Sup. | ParaDetox | 53.98 | PointerS2S | 30.78 |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (ours) | 52.88 | SFT (ours) | 22.69 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 | Distill (ours) | 43.97 | Distill (ours) | 22.88 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-5 | CoTex-TB | 48.53 | CoTex-TB | 26.79 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CoTex-TA | 54.79 | CoTex-TA | 25.70 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparing to previous methods. The best-performed method is in bold.
    The best method without utilizing a full parallel Train set is underscored. Unsup.:
    unsupervised, Sup.: supervised, ^†: Take from Liu et al. ([2022](#bib.bib22)).
    ^∗: Utilize outputs from implementation of Logacheva et al. ([2022b](#bib.bib26)).'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Full Dataset. Training student models with CoTeX on all training
    samples of each dataset, we present comparative results in Tables [1](#S4.T1 "Table
    1 ‣ 4 Results ‣ Distilling Text Style Transfer With Self-Explanation From LLMs").⁵⁵5CoTeX-TB
    setting utilizes the source text from training sample while keeping the target
    undisclosed. Given that many unsupervised TST studies have not reported BLEU scores,
    we compute BLEU scores for their public outputs using our evaluation scripts to
    ensure a fair comparison. CoTeX-TB surpasses previous unsupervised methods, the
    SoTA ICL method Prompt&Rank, and instruction-tuned LLMs across both domains within
    the formality transfer dataset. Although CoTeX-TA does not exceed the performance
    of SoTA supervised methods, SemiFST, for formality transfer, it is noteworthy
    that our method does not depend on task-specific data augmentation strategies
    or knowledge, offering greater flexibility. In the detoxification task, our results
    are compared with the top-performing model from Logacheva et al. ([2022b](#bib.bib26)).
    CoTeX-TA outperforms previous supervised methods, while CoTeX-TB falls slightly
    short of CondBERT, which employs additional style-conditional LMs for transfer
    control. FlanT5-XL, an instruction-tuned LLM, leads in ICL performance with a
    BLEU score of 50.13. For translating Shakespearean to modern English, CoTeX-TB
    shows marked improvements over both unsupervised and ICL methods, attributed to
    the superior quality of LLM generations in this specific transfer task.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing Synthetic Data per Source Text. For CoTeX-TB, we conduct an ancillary
    study to explore the benefits of employing multiple CoT paths with synthetic target
    texts for a source text. Given a source text $s_{i}$ values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f643c70bcc660d5233da764d2b04f86a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Scaling up synthetic data per source.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Source Text | just want to know HAVE YOU BEEN TO THE DOC YET |'
  prefs: []
  type: TYPE_TB
- en: '| CoTeX Generation | The original text is informal. The use of all caps and
    the absence of punctuation are informal. The use of “DOC" is a misspelling of
    “doctor". [[Transferred]]: I just want to know if you have been to the doctor
    yet. |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | I want to know if you have been to the doctor yet. |'
  prefs: []
  type: TYPE_TB
- en: '| Source Text | bembie hit the nail on the head |'
  prefs: []
  type: TYPE_TB
- en: '| CoTeX Generation | The original text is informal. The use of “bembie" is
    a misspelling of “bembie". [[Transferred]]: Bembie hit the nail on the head. |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | Bembie reached the proper conclusion. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Case study on CoTeX-TB generations.'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative Study. We now present a qualitative study to delve into rewriting
    rationales generated by CoTeX-TB. Examples are showcased in Table [2](#S4.T2 "Table
    2 ‣ 4 Results ‣ Distilling Text Style Transfer With Self-Explanation From LLMs"),
    derived from Test set of Formality (F&R) which transfers from informal to formal
    text. We sort generations by their BLEU scores against gold references and select
    random high and low-scoring samples. The first example, obtained BLEU of 100,
    correctly identifies informal components, fixes informal spellings, and yields
    a formal and grammatical sentence. The second example (BLEU=7.27) misses comprehending
    the idiom “hit the nail on the head” from the source, without translating it into
    a formal expression. Nevertheless, we note that the LLM (i.e., PaLM2) can appropriately
    adapt this idiom to “accurately identified the key point”. This leads us to hypothesize
    that a smaller LM exhibits potential limitations in its ability to understand
    implicit style cues.
  prefs: []
  type: TYPE_NORMAL
- en: Human Evaluation on Generated Reasonings. To assess the quality of model-generated
    rationales (i.e., CoT path) for the rewriting process, we conduct a human evaluation.
    Following previous works Wang et al. ([2023b](#bib.bib49)); Wu et al. ([2023](#bib.bib52)),
    we develop our evaluation protocol and instructions as shown in Table [5](#A5.T5
    "Table 5 ‣ Appendix E Human Evaluation on Generated Reasonings ‣ Distilling Text
    Style Transfer With Self-Explanation From LLMs") in Appendix. We assemble a team
    of four human experts to undertake this evaluation. Each annotator was tasked
    with reviewing 50 generated rationales across different models and transfer tasks.
    For each evaluation, the dataset provided included the source text, a generated
    rationale for the rewriting process, and the resultant transferred text. As depicted
    in Figure [4](#S4.F4 "Figure 4 ‣ 4 Results ‣ Distilling Text Style Transfer With
    Self-Explanation From LLMs"), although CoTeX-TB lags behind the teacher model
    (PaLM2 Unicorn), 100% of its responses in the detoxification task and 74% in the
    formality transfer task are deemed acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F4.pic1" class="ltx_picture ltx_centering" height="569.71" overflow="visible"
    version="1.1" width="661.98"><g transform="translate(0,569.71) matrix(1 0 0 -1
    0 0) translate(120.58,0) translate(0,114.2) matrix(1.0 0.0 0.0 1.0 -120.58 -114.2)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(128.45,0) translate(0,129.95)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -29.55)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 217.52 -48.66)" fill="#000000" stroke="#000000"><foreignobject
    width="86.94" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    of examples</foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 139.81 -126.9)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0
    -1 0 8.535)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.53)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    11.62 0) translate(24.58,0) matrix(1.0 0.0 0.0 1.0 -21.81 -3.69)" fill="#000000"
    stroke="#000000"><foreignobject width="43.63" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Rate-A</foreignobject></g><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 72.41 0) translate(24.29,0)
    matrix(1.0 0.0 0.0 1.0 -21.52 -3.69)" fill="#000000" stroke="#000000"><foreignobject
    width="43.05" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Rate-B</foreignobject></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    132.61 0) translate(24.39,0) matrix(1.0 0.0 0.0 1.0 -21.62 -3.69)" fill="#000000"
    stroke="#000000"><foreignobject width="43.24" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Rate-C</foreignobject></g><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 193.01 0) translate(24.68,0)
    matrix(1.0 0.0 0.0 1.0 -21.91 -3.69)" fill="#000000" stroke="#000000"><foreignobject
    width="43.82" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Rate-D</foreignobject></g></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Human evaluation results of CoT reasoning paths of 50 samples. Form.:
    formality transfer, Detox.: Detoxification.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When parallel TST datasets are available, numerous studies Rao and Tetreault
    ([2018](#bib.bib37)); Shang et al. ([2019](#bib.bib41)); Chawla and Yang ([2020](#bib.bib3));
    Lai et al. ([2021](#bib.bib19)) have utilized a sequence-to-sequence framework
    for supervised training TST models. To improve model efficacy, multitask learning Niu
    et al. ([2018](#bib.bib30)); Xu et al. ([2019](#bib.bib55)), lexically constrained
    decoding Post and Vilar ([2018](#bib.bib34)), and task-specific data augmentation Zhang
    et al. ([2020](#bib.bib58)); Liu et al. ([2022](#bib.bib22)) have been incorporated.
    Addressing the scarcity of parallel data, unsupervised methods have been developed
    for TST, employing methodologies like disentanglement of latent representations Liu
    et al. ([2020](#bib.bib23)); Nangi et al. ([2021](#bib.bib29)); Yi et al. ([2021](#bib.bib57)),
    prototype editing Li et al. ([2018](#bib.bib20)), style rewriting using attribute-specific
    LMs Krishna et al. ([2020](#bib.bib18)), and reinforcement learning Luo et al.
    ([2019](#bib.bib28)); Hallinan et al. ([2023a](#bib.bib10)). Our CoTeX framework
    explores both parallel and non-parallel data landscapes. The advent of LLMs has
    introduced ICL for executing TST with few-shot prompts, bypassing the need for
    model parameter updates Reif et al. ([2022](#bib.bib39)); Suzgun et al. ([2022](#bib.bib44)).
    Yet, these methods typically lack interpretability. In parallel, Saakyan and Muresan
    ([2023](#bib.bib40)) employ CoT prompting alongside domain expert feedback to
    enhance formality transfer and interpretability. Our CoTeX extends to broader
    range of TST directions, aiming to utilize CoT to provide rewriting explanations
    and minimize the requirement for human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced CoTeX, a novel approach for TST. Through CoT prompting, we elicit
    the rationals for the style rewriting process from LLMs and then distill both
    the TST and reasoning capabilities into smaller task-specific models. CoTeX demonstrated
    its efficiency and effectiveness with and without utilizing parallel data, especially
    in low-resource scenarios. The CoT reasoning from CoTeX bolstered the explainability
    of TST models.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TST Directions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We incorporate three style transfer directions to enable a clear comparison
    between target-blind and target-aware CoTeX. Benefiting from the powerful capacity
    of LLMs, we believe that our method could be extended to a broader array of TST
    directions (e.g., sentiment transfer). We plan to explore more transfer directions
    in future work.
  prefs: []
  type: TYPE_NORMAL
- en: Model Selection.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We only use T5-large as the student model in the paper. We also conduct a concise
    study to apply CoTeX to the T5-XL model. As results shown in Appendix [D](#A4
    "Appendix D Experiment with T5-XL ‣ Distilling Text Style Transfer With Self-Explanation
    From LLMs"), our CoTeX-TA still outperforms SFT on ParaDetox dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unlike previous studies Krishna et al. ([2020](#bib.bib18)); Liu et al. ([2022](#bib.bib22)),
    we abstain from using other automatic metrics (e.g., BERTscore for meaning preservation)
    to evaluate our models. Our decision is grounded in two main reasons: (1) While
    these automatic evaluations consider three facets, i.e., preservation of semantic
    meaning, accuracy of style transfer, and fluency they lack an effective methodology
    for aggregating these metrics to convey the overall performance Ostheimer et al.
    ([2023](#bib.bib31)); (2) Our preliminary experiments involving these automatic
    metrics revealed a misalignment between their outcomes and the BLEU score derived
    from human-annotated references. We thus opt to report the BLEU score in the paper.
    Detailed results from our preliminary tests are presented in Appendix [C](#A3
    "Appendix C Preliminary Test on Evaluation Metrics ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Ethical Consideration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary objective of training CoTeX model is to achieve more computationally
    efficient and effective models for TST. We focus on the positive TST directions,
    such as language detoxification. We use an LLM to generate rationales alongside
    transferred text, which are subsequently distilled into smaller LMs. It’s important
    to acknowledge that the LLM’s generation might encompass societal biases Lucy
    and Bamman ([2021](#bib.bib27)) or hallucinations Zhang et al. ([2023](#bib.bib59)),
    and student models trained with this data could inherit these characteristics
    of the teacher LLM. Additionally, our CoTeX-TA relies on datasets from prior research.
    Thus, any biases present in the original annotation processes of these datasets
    might also be reflected in our trained models. We expect the ongoing work Ouyang
    et al. ([2022](#bib.bib32)); Dev et al. ([2022](#bib.bib7)) of improving LM’s
    social fairness, faithfulness, and trustworthiness could benefit both teacher
    and student models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng,
    Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,
    and et al. 2023. [Palm 2 technical report](https://doi.org/10.48550/arXiv.2305.10403).
    *CoRR*, abs/2305.10403.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Babakov et al. (2023) Nikolay Babakov, David Dale, Ilya Gusev, Irina Krotova,
    and Alexander Panchenko. 2023. Don’t lose the message while paraphrasing: A study
    on content preserving style transfer. In *Natural Language Processing and Information
    Systems*, pages 47–61, Cham. Springer Nature Switzerland.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chawla and Yang (2020) Kunal Chawla and Diyi Yang. 2020. [Semi-supervised formality
    style transfer using language model discriminator and mutual information maximization](https://doi.org/10.18653/v1/2020.findings-emnlp.212).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    2340–2354, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M.
    Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
    Denny Zhou, Quoc V. Le, and Jason Wei. 2022. [Scaling instruction-finetuned language
    models](https://doi.org/10.48550/ARXIV.2210.11416). *CoRR*, abs/2210.11416.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dale et al. (2021) David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva,
    Olga Kozlova, Nikita Semenov, and Alexander Panchenko. 2021. [Text detoxification
    using large pre-trained neural models](https://doi.org/10.18653/v1/2021.emnlp-main.629).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 7979–7996, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dev et al. (2022) Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao
    Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, and Kai-Wei
    Chang. 2022. [On measures of biases and harms in NLP](https://aclanthology.org/2022.findings-aacl.24).
    In *Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022,
    Online only, November 20-23, 2022*, pages 246–267\. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. [SimCSE:
    Simple contrastive learning of sentence embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.552).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 6894–6910, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. (2019) Hongyu Gong, Suma Bhat, Lingfei Wu, JinJun Xiong, and Wen-mei
    Hwu. 2019. [Reinforcement learning based text style transfer without parallel
    training corpus](https://doi.org/10.18653/v1/N19-1320). In *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    3168–3180, Minneapolis, Minnesota. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hallinan et al. (2023a) Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung,
    Sean Welleck, and Yejin Choi. 2023a. [STEER: unified style transfer with expert
    reinforcement](https://aclanthology.org/2023.findings-emnlp.506). In *Findings
    of the Association for Computational Linguistics: EMNLP 2023, Singapore, December
    6-10, 2023*, pages 7546–7562\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hallinan et al. (2023b) Skyler Hallinan, Alisa Liu, Yejin Choi, and Maarten
    Sap. 2023b. [Detoxifying text with MaRCo: Controllable revision with experts and
    anti-experts](https://doi.org/10.18653/v1/2023.acl-short.21). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 228–242, Toronto, Canada. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Junxian He, Xinyi Wang, Graham Neubig, and Taylor Berg-Kirkpatrick.
    2020. [A probabilistic formulation of unsupervised text style transfer](https://openreview.net/forum?id=HJlA0C4tPS).
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
    [Distilling the knowledge in a neural network](http://arxiv.org/abs/1503.02531).
    *CoRR*, abs/1503.02531.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.
    [Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes](https://doi.org/10.18653/v1/2023.findings-acl.507).
    In *Findings of the Association for Computational Linguistics: ACL 2023, Toronto,
    Canada, July 9-14, 2023*, pages 8003–8017\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. 2022. [Large language models can self-improve](https://doi.org/10.48550/arXiv.2210.11610).
    *CoRR*, abs/2210.11610.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jhamtani et al. (2017) Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg.
    2017. [Shakespearizing modern language using copy-enriched sequence to sequence
    models](https://doi.org/10.18653/v1/W17-4902). In *Proceedings of the Workshop
    on Stylistic Variation*, pages 10–19, Copenhagen, Denmark. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2022) Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and Rada
    Mihalcea. 2022. [Deep Learning for Text Style Transfer: A Survey](https://doi.org/10.1162/coli_a_00426).
    *Computational Linguistics*, 48(1):155–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishna et al. (2020) Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.
    [Reformulating unsupervised style transfer as paraphrase generation](https://doi.org/10.18653/v1/2020.emnlp-main.55).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 737–762\. Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2021) Huiyuan Lai, Antonio Toral, and Malvina Nissim. 2021. [Thank
    you BART! rewarding pre-trained models improves formality style transfer](https://doi.org/10.18653/v1/2021.acl-short.62).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 2: Short Papers)*, pages 484–494, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Juncen Li, Robin Jia, He He, and Percy Liang. 2018. [Delete,
    retrieve, generate: a simple approach to sentiment and style transfer](https://doi.org/10.18653/v1/n18-1169).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New
    Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*, pages 1865–1874\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang,
    Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan.
    2022. [Explanations from large language models make small reasoners better](https://doi.org/10.48550/arXiv.2210.06726).
    *CoRR*, abs/2210.06726.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Ao Liu, An Wang, and Naoaki Okazaki. 2022. [Semi-supervised
    formality style transfer with consistency training](https://doi.org/10.18653/v1/2022.acl-long.321).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*,
    pages 4689–4701\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Dayiheng Liu, Jie Fu, Yidan Zhang, Chris Pal, and Jiancheng
    Lv. 2020. Revision in continuous space: Unsupervised text style transfer without
    adversarial learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 34, pages 8376–8383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, and Soroush
    Vosoughi. 2021. Non-parallel text style transfer with self-parallel supervision.
    In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logacheva et al. (2022a) Varvara Logacheva, Daryna Dementieva, Irina Krotova,
    Alena Fenogenova, Irina Nikishina, Tatiana Shavrina, and Alexander Panchenko.
    2022a. [A study on manual and automatic evaluation for text style transfer: The
    case of detoxification](https://doi.org/10.18653/v1/2022.humeval-1.8). In *Proceedings
    of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval)*, pages 90–101,
    Dublin, Ireland. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logacheva et al. (2022b) Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev,
    Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko.
    2022b. [ParaDetox: Detoxification with parallel data](https://doi.org/10.18653/v1/2022.acl-long.469).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6804–6818, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lucy and Bamman (2021) Li Lucy and David Bamman. 2021. [Gender and representation
    bias in GPT-3 generated stories](https://doi.org/10.18653/v1/2021.nuse-1.5). In
    *Proceedings of the Third Workshop on Narrative Understanding*, pages 48–55, Virtual.
    Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2019) Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Xu Sun, and Zhifang Sui. 2019. [A dual reinforcement learning framework for unsupervised
    text style transfer](https://doi.org/10.24963/ijcai.2019/711). In *Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, Macao, China, August 10-16, 2019*, pages 5116–5122\. ijcai.org.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nangi et al. (2021) Sharmila Reddy Nangi, Niyati Chhaya, Sopan Khosla, Nikhil
    Kaushik, and Harshit Nyati. 2021. [Counterfactuals to control latent disentangled
    text representations for style transfer](https://doi.org/10.18653/v1/2021.acl-short.7).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 2: Short Papers)*, pages 40–48, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu et al. (2018) Xing Niu, Sudha Rao, and Marine Carpuat. 2018. [Multi-task
    neural models for translating between styles within and across languages](https://aclanthology.org/C18-1086/).
    In *Proceedings of the 27th International Conference on Computational Linguistics,
    COLING 2018, Santa Fe, New Mexico, USA, August 20-26, 2018*, pages 1008–1021\.
    Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ostheimer et al. (2023) Phil Ostheimer, Mayank Kumar Nagda, Marius Kloft, and
    Sophie Fellenz. 2023. [A call for standardization and validation of text style
    transfer evaluation](https://doi.org/10.18653/v1/2023.findings-acl.687). In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 10791–10815,
    Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).
    In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post (2018) Matt Post. 2018. [A call for clarity in reporting BLEU scores](https://doi.org/10.18653/v1/W18-6319).
    In *Proceedings of the Third Conference on Machine Translation: Research Papers*,
    pages 186–191, Brussels, Belgium. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post and Vilar (2018) Matt Post and David Vilar. 2018. [Fast lexically constrained
    decoding with dynamic beam allocation for neural machine translation](https://doi.org/10.18653/V1/N18-1119).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New
    Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*, pages 1314–1324\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pu and Demberg (2023) Dongqi Pu and Vera Demberg. 2023. [Chatgpt vs human-authored
    text: Insights into controllable text summarization and sentence style transfer](https://doi.org/10.18653/v1/2023.acl-srw.1).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics: Student Research Workshop, ACL 2023, Toronto, Canada, July 9-14,
    2023*, pages 1–18\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *J. Mach. Learn. Res.*, 21:140:1–140:67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. [Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer](https://doi.org/10.18653/v1/N18-1012). In *Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 129–140,
    New Orleans, Louisiana. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reid and Zhong (2021) Machel Reid and Victor Zhong. 2021. [LEWIS: Levenshtein
    editing for unsupervised text style transfer](https://doi.org/10.18653/v1/2021.findings-acl.344).
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 3932–3944, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reif et al. (2022) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2022. [A recipe for arbitrary text style transfer
    with large language models](https://doi.org/10.18653/v1/2022.acl-short.94). In
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 2: Short Papers)*, pages 837–848, Dublin, Ireland. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saakyan and Muresan (2023) Arkadiy Saakyan and Smaranda Muresan. 2023. [ICLEF:
    in-context learning with expert feedback for explainable style transfer](https://doi.org/10.48550/ARXIV.2309.08583).
    *CoRR*, abs/2309.08583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. (2019) Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan
    Zhao, Shuming Shi, and Rui Yan. 2019. [Semi-supervised text style transfer: Cross
    projection in latent space](https://doi.org/10.18653/v1/D19-1499). In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*, pages 4936–4945\. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2017) Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola.
    2017. [Style transfer from non-parallel text by cross-alignment](https://proceedings.neurips.cc/paper/2017/hash/2d2c8394e31101a261abf1784302bf75-Abstract.html).
    In *Advances in Neural Information Processing Systems 30: Annual Conference on
    Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
    USA*, pages 6830–6841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shridhar et al. (2023) Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan.
    2023. [Distilling reasoning capabilities into smaller language models](https://doi.org/10.18653/v1/2023.findings-acl.441).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    7059–7073, Toronto, Canada. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suzgun et al. (2022) Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022.
    [Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style
    transfer with small language models](https://doi.org/10.18653/v1/2022.emnlp-main.141).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 2195–2222, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://doi.org/10.48550/ARXIV.2307.09288).
    *CoRR*, abs/2307.09288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Ke Wang, Hang Hua, and Xiaojun Wan. 2019a. [Controllable
    unsupervised text attribute transfer via editing entangled latent representation](https://proceedings.neurips.cc/paper/2019/hash/8804f94e16ba5b680e239a554a08f7d2-Abstract.html).
    In *Advances in Neural Information Processing Systems 32: Annual Conference on
    Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada*, pages 11034–11044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing
    Yin, and Xiang Ren. 2023a. [SCOTT: Self-consistent chain-of-thought distillation](https://doi.org/10.18653/v1/2023.acl-long.304).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 5546–5558, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. [Self-instruct:
    Aligning language models with self-generated instructions](https://doi.org/10.18653/v1/2023.acl-long.754).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*,
    pages 13484–13508\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wenhan Chao.
    2019b. [Harnessing pre-trained neural networks with rules for formality style
    transfer](https://doi.org/10.18653/v1/D19-1365). In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
    China, November 3-7, 2019*, pages 3571–3576\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. [Chain-of-thought
    prompting elicits reasoning in large language models](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).
    In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed,
    and Alham Fikri Aji. 2023. [Lamini-lm: A diverse herd of distilled models from
    large-scale instructions](https://doi.org/10.48550/arXiv.2304.14402). *CoRR*,
    abs/2304.14402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, and Songlin
    Hu. 2019. [Mask and infill: Applying masked language model for sentiment transfer](https://doi.org/10.24963/ijcai.2019/732).
    In *Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI-19*, pages 5271–5277\. International Joint Conferences on
    Artificial Intelligence Organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2021) Fei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei Shen,
    and Xueqi Cheng. 2021. [Transductive learning for unsupervised text style transfer](https://doi.org/10.18653/v1/2021.emnlp-main.195).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
    2021*, pages 2510–2521\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Ruochen Xu, Tao Ge, and Furu Wei. 2019. [Formality style transfer
    with hybrid textual annotations](http://arxiv.org/abs/1903.06353). *CoRR*, abs/1903.06353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2012) Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin
    Cherry. 2012. [Paraphrasing for style](https://aclanthology.org/C12-1177/). In
    *COLING 2012, 24th International Conference on Computational Linguistics, Proceedings
    of the Conference: Technical Papers, 8-15 December 2012, Mumbai, India*, pages
    2899–2914\. Indian Institute of Technology Bombay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2021) Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2021.
    Text style transfer via learning style instance supported latent space. In *Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence*, pages 3801–3807.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Yi Zhang, Tao Ge, and Xu Sun. 2020. [Parallel data augmentation
    for formality style transfer](https://doi.org/10.18653/v1/2020.acl-main.294).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020*, pages 3221–3228\. Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
    Wei Bi, Freda Shi, and Shuming Shi. 2023. [Siren’s song in the AI ocean: A survey
    on hallucination in large language models](https://doi.org/10.48550/arXiv.2309.01219).
    *CoRR*, abs/2309.01219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendices
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Figure [5](#A1.F5 "Figure 5 ‣ Appendix A Method ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs") shows an input example for target-aware data
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2fdb23c61937f25c7dcc3808c34e425.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Few-shot chain-of-thought prompting for data generation with supervised
    data (target-aware setting). We use the few-shot prompts that include a few examples
    to guide LLM to generate desired outputs in a standard format.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Hyperparameter for Training Student Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We set the maximal input and output sequence lengths to 512 and 256, respectively.
    To optimize the T5 model’s finetuning, we search both the learning rate and batch
    size within specified search spaces: $lr\in\{1e-3,5e-4,1e-5\}$. We finetune T5
    for 2,000 steps, evaluate performance on the validation set every 16 steps, and
    report the test performance on the best step. All T5 models are trained on four
    V3 TPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Preliminary Test on Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our preliminary experiment, we evaluate model performance with several automatic
    metrics utilized by previous works Krishna et al. ([2020](#bib.bib18)); Luo et al.
    ([2019](#bib.bib28)); Reif et al. ([2022](#bib.bib39)). These automatic metrics
    have been widely used in unsupervised TST due to their independence from human-labeled
    parallel data. However, we find that the outcomes from these metrics do not align
    with the reference-BLEU score derived from human-annotated references. These automatic
    metrics evaluate transferred text from three aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarity: To evaluate the similarity between the source text and the transferred
    text, we employ BERTscore and self-BLEU. For BERTscore calculations, we use the
    SimCSE-large model Gao et al. ([2021](#bib.bib8)) as the backbone.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Transfer Accuracy: To evaluate the efficacy of the style transfer, we employ
    a classifier Babakov et al. ([2023](#bib.bib2)) to determine whether the transferred
    text successfully achieves the desired style.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fluency: To access the fluency of the transferred text, we compute its perplexity
    using GPT. Additionally, we utilize a classifier trained on the Corpus of Linguistic
    Acceptability (CoLA) from Krishna et al. ([2020](#bib.bib18)) to determine the
    grammaticality of the transferred text.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this preliminary experiment, we conduct experiments using varying training
    sizes from the formality transfer (F&R) dataset. These experiments are carried
    out in a target-blind setting, where we finetune a T5-large model using the synthetic
    data generated from LLM. For assessing transfer accuracy, we employ a binary classifier
    introduced by Babakov et al. ([2023](#bib.bib2)), which is a RoBERTa-base model
    finetuned on the GYAFC’s training set. This classifier achieves a test accuracy
    of 0.91\. As Table [3](#A3.T3 "Table 3 ‣ Appendix C Preliminary Test on Evaluation
    Metrics ‣ Distilling Text Style Transfer With Self-Explanation From LLMs") shows,
    the outcomes from these metrics did not correspond well with the reference-BLEU
    score. We thus opt to report the BLEU score in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: '| # of data | Ref-BLEU | BERTScore | Self-BLEU | Tra. Acc. | PPL | CoLA |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 72.54 | 0.96 | 45.34 | 0.94 | 61.15 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 73.13 | 0.96 | 46.89 | 0.93 | 59.02 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 5000 | 71.92 | 0.96 | 50.71 | 0.90 | 65.54 | 0.94 |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 72.86 | 0.96 | 51.15 | 0.89 | 63.98 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 20000 | 72.90 | 0.96 | 49.89 | 0.90 | 64.66 | 0.94 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Preliminary result on GYAFC (F&R) for investigating evaluation metrics.
    Tra. Acc.: transfer accuracy, PPL: perplexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Experiment with T5-XL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conduct a concise experiment to apply our CoTeX to T5-XL (containing 3B parameters).
    As Table [4](#A4.T4 "Table 4 ‣ Appendix D Experiment with T5-XL ‣ Distilling Text
    Style Transfer With Self-Explanation From LLMs") shows, our CoTeX-TA outperforms
    SFT across all the data sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '| # Data | SFT | CoTex-TB | CoTex-TA |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 49.15 | 46.64 | 53.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 51.58 | 47.56 | 54.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 5000 | 52.91 | 47.92 | 54.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 52.32 | 47.96 | 55.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 15000 | 52.88 | 48.47 | 55.19 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Finetuning T5-XL on detoxification dataset with our CoTeX or SFT.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Human Evaluation on Generated Reasonings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table [5](#A5.T5 "Table 5 ‣ Appendix E Human Evaluation on Generated Reasonings
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs") shows our human
    evaluation protocol and instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '| Level | Criteria |'
  prefs: []
  type: TYPE_TB
- en: '| Rate A | • Valid, acceptable and satisfying (subject to the annotator) response;
    • Accurately identified the most cues for text style transfer; • The reasoning
    path can directly lead to the transferred text. |'
  prefs: []
  type: TYPE_TB
- en: '| Rate B | • The response is acceptable but has minor errors that can be improved;
    • Mirror errors include out-of-context content, minimal factual errors, missing
    many cues for text style transfer, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Rate C | • The response is relevant but it has significant errors in the
    content; • Cannot identify any correct cues for text style transfer. • The reasoning
    path cannot lead to the transferred text. |'
  prefs: []
  type: TYPE_TB
- en: '| Rate D | • Invalid and unacceptable response; • Nothing related to the text
    style transfer task. |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction: This task is text styles transfer that transfers a {$source_style}
    source text to a target text with style {$target_style}. Each example includes
    a source text and the corresponding model-generated rationales of the rewriting
    process as well as the transferred text. You evaluate the rationales of the rewriting
    process and do not take the quality of the transferred text into account. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Human evaluation protocol and instruction. We adapt the evaluation
    criteria from Wu et al. ([2023](#bib.bib52)) and Wang et al. ([2023b](#bib.bib49)).'
  prefs: []
  type: TYPE_NORMAL
