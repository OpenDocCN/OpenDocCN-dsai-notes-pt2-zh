- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:26'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14597](https://ar5iv.labs.arxiv.org/html/2405.14597)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Yifan Lu, Yerui Sun, Lin Ma, Yuchen
    Xie
  prefs: []
  type: TYPE_NORMAL
- en: Meituan Inc.
  prefs: []
  type: TYPE_NORMAL
- en: '{liqingyuan02,mengran03,liyiduo,zhangbo97,luyifan04}@meituan.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We introduce *Integer Scale*, a novel post-training quantization scheme for
    large language models that effectively resolves the inference bottleneck in current
    fine-grained quantization approaches while maintaining similar accuracies. Integer
    Scale is a free lunch as it requires no extra calibration or fine-tuning which
    will otherwise incur additional costs. It can be used plug-and-play for most fine-grained
    quantization methods. Its integration results in at most 1.85$\times$ compared
    with their FP16 versions respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The size of language models has continued to grow exponentially throughout recent
    years. To name some iconic models, Transformers [[46](#bib.bib46)] initially bear
    65M parameters, BERT [[10](#bib.bib10)] exceeds with 340M, GPT-3 [[4](#bib.bib4)]
    prevails with 175B, PaLM  [[7](#bib.bib7)] trumps with 540B and most lately GPT-4
     [[35](#bib.bib35)] is estimated to have reached 1.8T parameters. This seemingly
    unstoppable trend is largely promoted by the so-called scaling law  [[19](#bib.bib19)]
    where a model’s capability, via a proxy metric of auto-regressive maximum-likelihood
    loss, exhibits a power-law relationship to its number of parameters, dataset sizes,
    and compute respectively. Not surprisingly, the intimidating number of parameters
    of Large Language Models (LLMs) place an almost insurmountable hurdle for inference,
    potentially preventing their pervasive applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, optimizing the serving efficiency of LLMs is a non-trivial task. LLMs
    generally comprise a compute-intense *pre-filling* stage and a memory-bound *self-decoding*
    stage. Exploiting integer matrix multiplication speeds up the computation, but
    directly applying post-training quantization usually generates a large performance
    drop. Quantization-aware training methods like LLM-QAT [[28](#bib.bib28)] require
    costly computing resources to fine-tune all the weights. In contrast, post-training
    quantization is more affordable and commonly used in practice. For instance, SmoothQuant [[48](#bib.bib48)]
    transforms activation outliers into weights for better quantization accuracy.
    Recently, fine-granularity grouping [[37](#bib.bib37)] is often used as a general
    paradigm to reduce the quantization errors, as in ZeroQuant [[49](#bib.bib49)],
    GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)] and FPTQ [[24](#bib.bib24)]. FPTQ
    proposes a fine-grained W4A8 strategy to address the memory-bound issue as a trade-off
    between W4A16 and W8A8\. While its high quantization accuracy benefits from fine-grained
    quantization, the actual inference is also stalled by inefficient operations introduced
    by its intrinsic computational complexity due to fine granularity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we are driven to design a faster fine-grained quantization scheme
    called *Integer Scale* that renders fewer quantization errors (Table [3](#S5.T3
    "Table 3 ‣ 5.2 Experiment Result on LAMBADA, C4, and WikiText-2 ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs"))
    and simultaneously achieves boosted speed (see Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs")). Our contributions are multi-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3de890c0550b48b6e38831d345ceb04a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: End-to-end latency comparison of W4A8 (Integer Scale) compared with
    W4A8 (Float Scale) and W4A16 (Marlin) on LLaMA-2 models. The speedup ratio is
    written on top of the bars.'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We unveil the intrinsic inference bottleneck of fine-grained LLM quantization
    approaches and find a hassle-free cure, called Integer Scale, with negligible
    accuracy loss. Our approach can be used as an out-of-box plugin for the state-of-the-art
    quantization methods (e.g. GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)], Omniquant [[41](#bib.bib41)],
    QuaRot [[3](#bib.bib3)] etc.) with minimum modifications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The orchestration of fine-grained quantization and the integer scale scheme
    not only retains the performance of the existing methods but also effectively
    addresses the quantization difficulty of LLMs built with the mixture-of-experts
    technique [[18](#bib.bib18)] and LLaMA-3 [[1](#bib.bib1)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our integer scale, when applied to fine-grained W4A8 paradigms, achieves at
    most 1.85$\times$ over its float scale counterpart, while being comparable in
    performance. This suggests the viability of our approach as we have achieved a
    new Pareto-front of speed vs. accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM Serving Frameworks and Optimization Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: vLLM [[20](#bib.bib20)] brings about paged attention [[20](#bib.bib20)] and
    continuous batching. FasterTransformer [[33](#bib.bib33)] provides a highly optimized
    inference framework featuring cutlass GEMMs, CUDA kernels. Built on top of FasterTransformer [[33](#bib.bib33)],
    LMDeploy [[8](#bib.bib8)] features an efficient backend called TurboMind that
    seeks extreme optimization through persistent batching, KV caching, and a low-bit
    quantization toolkit. Another sprout from FasterTransformer is TensorRT-LLM [[34](#bib.bib34)],
    which is tailored particularly for NVIDIA GPUs and ensembles many up-to-date inference
    techniques like flash attention [[9](#bib.bib9)], FP8 quantization [[30](#bib.bib30)],
    in-flight batching, graph optimization, etc. Marlin [[13](#bib.bib13)] ships so
    far the fastest W4A16 kernel along with a bag of optimization tricks, while QServe [[27](#bib.bib27)]
    brings an advanced W4A8 kernel implementation. FP6-LLM [[47](#bib.bib47)] delicately
    devises a software solution to support the FP6 precision on NVIDIA A100 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLM Quantization Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantization is one of the most adopted optimization techniques to compress
    LLMs to their extremity. Nevertheless, it becomes more challenging as we chase
    for the quantization of lower bit widths (e.g. 4-bit, 2-bit, or binary), it faces
    more critical accuracy loss. It also requires efficient hardware-aware implementations
    that demand strenuous engineering effort.
  prefs: []
  type: TYPE_NORMAL
- en: Weight-only Quantization. GPTQ [[14](#bib.bib14)] renovates OBQ [[12](#bib.bib12)]
    to obtain an approximate second-order method that compensates for the quantization
    error. AWQ [[26](#bib.bib26)] is a mixed-precision weight-only method that locates
    salient weight channels and searches for the corresponding optimal scales. Omniquant [[41](#bib.bib41)]
    introduces learnable weight clipping that restricts extreme weight values and
    proposes learnable smoothing factors that tackle the activation outliers following
    SmoothQuant [[48](#bib.bib48)]. Extreme low-bit approaches also focus on weight-only
    quantization. Norm Tweaking [[22](#bib.bib22)] exploits layer norm tuning to alleviate
    the performance degradation, QuiP [[6](#bib.bib6)] profits from orthogonal matrices
    and AQLM [[11](#bib.bib11)] from additive quantization with a codebook for 2-bit
    quantization, while PB-LLM [[40](#bib.bib40)] uses partial 1-bit quantization.
  prefs: []
  type: TYPE_NORMAL
- en: The weight-only scheme alleviates the memory-bound issue but its activation
    remains in FP16\. Recent speculative parallel decoding methods [[21](#bib.bib21),
    [25](#bib.bib25), [5](#bib.bib5)] lead the decoding phase to a compute-bound scenario,
    which makes this scheme less promising in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Weight-Activation Quantization. ZeroQuant [[49](#bib.bib49)] presents a fine-grained
    quantization scheme coupled with distillation. SmoothQuant [[48](#bib.bib48)]
    enables W8A8 post-training quantization by smoothing the outliers with a heuristic
    factor and ships with a handcrafted CUDA kernel that ensures hardware efficiency.
    OdysseyLLM [[23](#bib.bib23)] is a coarse-grained W4A8 scheme that reduces the
    performance gap compared with W4A16 and W8A8\. QUIK [[2](#bib.bib2)] implements
    W4A4 quantization with mixed-precision.
  prefs: []
  type: TYPE_NORMAL
- en: Fine granularity generally further enhances the quantized accuracy. FPTQ [[24](#bib.bib24)]
    is a W4A8 fine-grained solution. Atom [[52](#bib.bib52)] is a fine-grained mixed-precision
    W4A4 method. However, they typically suffer from low latency issues which cancel
    out the benefits from lower bit widths. DGQ [[51](#bib.bib51)] attempts to apply
    a dual quantization scheme to improve the efficiency of the fine-grained approach.
  prefs: []
  type: TYPE_NORMAL
- en: Rotation-based Quantization. QuiP [[6](#bib.bib6)], QuiP# [[45](#bib.bib45)],
    QuaRot [[3](#bib.bib3)] are a line of quantization methods that profits from the
    computation invariance of the orthogonal matrices for outlier suppression. To
    undo the rotation effect, extra online transformations are applied. When implemented
    efficiently, this overhead can be deemed nearly negligible.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ae67912458f1bce51c57362b40a5675.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: (a) Fine-grained quantization divides activation $X$.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Fine Granularity Strengthens Current Quantization Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine granularity approaches [[24](#bib.bib24), [26](#bib.bib26), [52](#bib.bib52)]
    bear prevailing benefits over many state-of-the-art LLM quantization methods.
    In extreme cases, it even produces reasonable results when coarse methods fail.
    It can be applied as a plug-in method to boost the accuracy of the existing methods.
    Formally, the output $\mathbf{O}_{i}$ of a fine-grained weight-activation quantization
    GEMM can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{O}_{i}=\mathbf{s}_{a_{i}}*\sum_{g}(\mathbf{X}_{g_{i}}\times\mathbf{W}_{g_{i}}^{\top})*\mathbf{s}_{g_{i}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{s}_{a_{i}}$. Depending on the precision of matrix multiplication,
    specific type conversions are required to perform either scalar or matrix multiplication.
    For instance, if we adopt a fine-grained W8A8 scheme with integer tensor cores
    for the computation, the INT32 result has to be converted to float for the later
    dequantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 LLM Quantization
    Algorithms ‣ 2 Related Work ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs") (a), where it typically considers weights in groups and
    each has its float scale. We apply the fine-granularity strategy to approaches
    that cover commonly-used bit widths range in W4A16, W8A8, W4A8, and W4A4 in Table [1](#S3.T1
    "Table 1 ‣ 3.1 Fine Granularity Strengthens Current Quantization Approaches ‣
    3 Motivation ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") to exhibit that group-wise fine-granularity consistently improves the
    quantized performance compared with its original coarse counterpart. Note on the
    LLaMA-3-70B model, the vanilla Round-to-Nearest (RTN) caused a large performance
    collapse while its fine-grained version can easily handle it. As we drive from
    W8A8 to lower bits, the quantization error increases. Especially, when applying
    QuaRot [[3](#bib.bib3)] on LLaMA-3-70B at W4A4, the perplexity bursts into an
    unreasonable value, and fine-granularity can alleviate the issue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Applying fine granularity (denoted by ‘FG’) to the state-of-the-art
    quantization methods on LLaMA-2 models. Perplexity is tested on C4 (the lower
    the better). Group = -1 indicates coarse-grained weight quantization while 128
    means fine-grained with a group size of 128\.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Bitwidth | Method |  | Group | LLaMA-2 | LLaMA-3 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 |  |  |  |  | 7B | 13B | 70B | 8B | 70B |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 | Baseline |  |  | 7.05 | 6.46 | 5.52 | 8.88 | 6.73 |'
  prefs: []
  type: TYPE_TB
- en: '| W8A8 | RTN [[49](#bib.bib49)] |  | -1 | 7.19 | 6.51 | 5.64 | 9.05 | 75.05
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | RTN w/ FG |  | 128 | 7.2 | 6.51 | 5.64 | 9.04 | 7.15 |'
  prefs: []
  type: TYPE_TB
- en: '| W8A8 | SmoothQuant [[48](#bib.bib48)] |  | -1 | 7.2 | 6.51 | 5.58 | 9.03
    | 7.38 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SmoothQuant w/ FG |  | 128 | 7.2 | 6.51 | 5.58 | 9.03 | 7.48 |'
  prefs: []
  type: TYPE_TB
- en: '| W8A8 | FPTQ [[24](#bib.bib24)] |  | -1 | 7.08 | 6.50 | 5.55 | 8.97 | 8.88
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | FPTQ w/ FG |  | 128 | 7.08 | 6.50 | 5.54 | 8.95 | 6.81 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | GPTQ [[14](#bib.bib14)] |  | -1 | 7.47 | 6.84 | 5.71 | 10.54 | 7.83
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPTQ w/ FG |  | 128 | 7.22 | 6.65 | 5.61 | 9.70 | 7.26 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A8 | Odyssey [[23](#bib.bib23)] |  | -1 | 7.58 | 6.70 | 5.78 | 10.25 |
    12.15 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Odyssey w/ FG |  | 128 | 7.26 | 6.60 | 5.60 | 9.56 | 7.09 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A4 | QuaRot [[3](#bib.bib3)] |  | -1 | 7.87 | 7.11 | 5.92 | 12.06 | 544.50
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | QuaRot w/ FG |  | 128 | 7.82 | 7.08 | 5.90 | 11.8 | 132.20 |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Fine-grained Quantization Suffers from the Inference Bottleneck
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/021b43c45df24a5ed6ffc0c4809c58cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Kernel latency comparison between W4A8 w/ Float Scale vs. FP16\.
    The red line denotes its acceleration ratios over FP16.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although fine-grained quantization can achieve higher accuracy, as demonstrated
    in [[24](#bib.bib24)], we have found it to be particularly slow during inference,
    which is also noted in the Dual-Granularity Quantization (DGQ) method [[51](#bib.bib51)].
    The advantages of using lower bit widths are often offset by the computational
    overhead they introduce. Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Fine-grained Quantization
    Suffers from the Inference Bottleneck ‣ 3 Motivation ‣ Integer Scale: A Free Lunch
    for Faster Fine-grained Quantization of LLMs") compares the kernel latency under
    typical inference batch sizes (drops from 3.15$\times$). Notably, the fine-grained
    kernel is significantly slower when compared to FP16 at larger batch sizes, making
    it less practical for deployment. Further analysis confirms that fine-grained
    approaches inherently require numerous costly type conversions. The result of
    each integer matrix multiplication has to be converted to float precision to multiply
    the corresponding float scale, as depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2
    LLM Quantization Algorithms ‣ 2 Related Work ‣ Integer Scale: A Free Lunch for
    Faster Fine-grained Quantization of LLMs") (b).'
  prefs: []
  type: TYPE_NORMAL
- en: The intrinsic computation drawbacks disable its use in practice. This incoherent
    situation calls for a novel fine-grained scheme that is both computationally efficient
    and accuracy-retaining.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Integer Scale with Adaptive Scale Amplifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motivated by the previous discussion, it is then critical to boost the fine-grained
    inference. Figure [2](#S2.F2 "Figure 2 ‣ 2.2 LLM Quantization Algorithms ‣ 2 Related
    Work ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs")
    (b) has shown that using float scale triggers numerous costly type conversions.
    For instance, a typical Dense layer of size 4096$\times$, called *adaptive scale
    amplifier*, which can be easily computed based on the available float scales.
    Our method is put formally as,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'To find the common amplifier, we use a heuristic search algorithm that starts
    from $2^{0}$ that guarantees amplified scales to be bigger than 1, see Listing [1](#LST1
    "Listing 1 ‣ 4.1 Integer Scale with Adaptive Scale Amplifier ‣ 4 Method ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '1scale_min  =  scales.min()2n,  tmp  =  0,  scale_min3while  tmp  <  1:4  tmp  =  scale_min  *  (2**n)5  n+=16scale_amplifier  =  2**(n-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 1: Quick Heuristic Search for Integer Scale Amplifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we can use the above heuristic method to find the optimal amplifier
    per layer. However, based on the scale analysis of LLaMA-2-7B in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Integer Scale with Adaptive Scale Amplifier ‣ 4 Method ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") (a,b,c), we
    find that the number of bit shifts required to amplify the scale mainly falls
    to 9 or 10\. The weight MSE when using an amplifier of $2^{10}$ as our default
    amplifier to avoid possible overflow as the later ablation (Table [7](#S6.T7 "Table
    7 ‣ 6.1 Fixed Amplifier vs. Heuristic Search ‣ 6 Ablation Study ‣ Integer Scale:
    A Free Lunch for Faster Fine-grained Quantization of LLMs")) shows a bigger amplifier
    has no clear gains.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/db617ecb62ce258b69d9bbb41f2227ed.png)![Refer to caption](img/ec1c1dc2c8ef353dc5fa7614568d6c43.png)![Refer
    to caption](img/4e3eece6818f037c2cf8c11bea70b52b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: (a) The range of amplified ($\alpha=2^{10}$) float scales of LLaMA-2-7B
    in the first layer (others are similar) mapped to 16-bit integers. The majority
    of amplified scales can be represented within 8 bits. (b) The number of bit shifts
    required to amplify scales per linear layer. (c) Weight MSE between integer scale
    and float scale under different amplifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Kernel Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.2 Kernel Implementation ‣ 4 Method ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") illustrates
    the difference in typical kernels. Current hardware supports a standard MatMul
    GEMM which isn’t suited for fine-grained approaches. Each group of $A_{i}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Comparison of kernel computation logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '| MatMul | Atom | Ours |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $C_{1}=A_{1}*W_{1}+C_{0}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $C_{2}=A_{2}*W_{2}+C_{1}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\cdots$ |'
  prefs: []
  type: TYPE_TB
- en: 'We also present our computation strategy in Figure [2](#S2.F2 "Figure 2 ‣ 2.2
    LLM Quantization Algorithms ‣ 2 Related Work ‣ Integer Scale: A Free Lunch for
    Faster Fine-grained Quantization of LLMs") (c). Since the result of group-wise
    weight and activation matrix multiplication (e.g., $x_{00}\times g_{00}$, executed
    with integer tensor cores) becomes INT32, we only need to convert the amplified
    scale to INT32 offline. Each group is then accumulated to have the final result.
    The large number of type conversions on the matrix is thus reduced to only once
    for activation dequantization. Besides, we exploit the efficient weight processing
    and kernel fusion technique of OdysseyLLM’s FastGEMM  [[23](#bib.bib23)] for fast
    inference. The combination makes fine-grained kernels substantially efficient,
    enabling fine-grained approaches as a feasible solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models and Datasets. We benchmark Integer Scale and other state-of-the-art quantization
    methods on the well-known LLaMA-2 [[44](#bib.bib44)] and LLaMA-3 [[1](#bib.bib1)]
    models and Mixtral 8x7B [[18](#bib.bib18)]. Several datasets are used for evaluation,
    including LAMBADA [[36](#bib.bib36)], C4 [[38](#bib.bib38)], WikiText-2 [[29](#bib.bib29)],
    MMLU [[16](#bib.bib16)], and a set of Common Sense QA [[42](#bib.bib42)] tasks
    like WinoGrande [[39](#bib.bib39)], PIQA [[43](#bib.bib43)], HellaSwag [[50](#bib.bib50)],
    ARC[e]. For CommonSense QA tasks, we utilized the Language Model Evaluation Harness
     [[15](#bib.bib15)] tool.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Framework. We adopt an end-to-end inference pipeline with cutlass [[32](#bib.bib32)]
    that mainly profits GPU Tensor Core execution, kernel fusion policies, and graph
    optimization. Unless otherwise notified, we use the same framework for fair comparisons.
    Note for LLaMA models with W4A16, we use Marlin [[13](#bib.bib13)] for inference
    as it claims to be the fastest available framework. For Mixtral 8x7B, we had to
    use our W4A16 implementation as Marlin hasn’t supported it yet. The latency is
    tested on a single NVIDIA A100 GPU, except for LLaMA-2-70B and Mistral 8x7B we
    use four such GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines. In our experiments, we choose GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)],
    and Omniquant  [[41](#bib.bib41)] as our baselines, given that they are the most
    prevalent fine-grained quantization schemes. Throughout the paper, we adopt per-token
    activation quantization, and per-channel weight quantization by default.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Experiment Result on LAMBADA, C4, and WikiText-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.2 Experiment Result on LAMBADA, C4, and WikiText-2
    ‣ 5 Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") exhibits the quantization result of LLaMA-2 and Mixtral models when
    applying Integer Scale (IS) to GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)],
    and Omniquant [[41](#bib.bib41)] on LAMBADA, WikiText-2, and C4 datasets. Our
    approach generally shows on-par or better performance, indicating that the Integer
    Scale applies to the existing quantization methods and retains the quantized performance
    at lower bits like W4A8\. Note since Ominiquant on LLaMA-2-70B originally fails,
    so does its integer scale variation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparison with state-of-the-art quantization methods on LAMBADA (accuracy),
    C4 (PPL), and WikiText (PPL). For all models tested, we set the weight group size
    to 128 and apply symmetric quantization. Integer Scale (IS) with amplifier 1024
    is used.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | HyperParam |  | LLaMA-2 | Mixtral |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Method | BitWidth |  | 7B | 13B | 70B | 8x7B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LAMBADA | FP16 | W16A16 |  | 73.70% | 76.64% | 79.57% | 77.62% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 |  | 71.65% | 75.88% | 78.54% | 73.89% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 |  | 71.66% [+0.01] | 75.39% [-0.48] | 78.67% [+0.13] |
    73.93% [+0.03] |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 |  | 70.15% | 75.47% | 78.48% | 76.24% |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 |  | 70.07% [-0.07] | 75.02% [-0.44] | 78.42% [-0.05] |
    74.30% [-1.94] |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 |  | 71.76% | 75.98% | NaN | 76.09% |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 |  | 70.91% [-0.85] | 75.60% [-0.36] | NaN | 76.01%
    [-0.07] |'
  prefs: []
  type: TYPE_TB
- en: '| WikiText-2 | FP16 | W16A16 |  | 5.65 | 4.95 | 3.36 | 3.93 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 |  | 12.32 | 5.16 | 3.66 | 4.51 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 |  | 13.13 [+0.81] | 5.18 [+0.02] | 3.69 [+0.03] | 4.59
    [+0.08] |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 |  | 6.12 | 5.27 | 3.66 | 4.30 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 |  | 6.19 [+0.07] | 5.30 [+0.03] | 3.70 [+0.04] | 4.42 [+0.12]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 |  | 5.94 | 5.16 | NaN | 4.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 |  | 5.97 [+0.03] | 5.17 [+0.01] | NaN | 4.36 [+0.09]
    |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | FP16 | W16A16 |  | 7.05 | 6.46 | 5.52 | 6.88 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 |  | 39.96 | 6.66 | 5.75 | 7.31 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 |  | 37.25 [+2.71] | 6.68 [+0.02] | 5.78 [+0.03] | 7.39
    [+0.08] |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 |  | 7.57 | 6.79 | 5.73 | 7.15 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 |  | 7.64 [+0.07] | 6.83 [+0.04] | 5.76 [+0.03] | 7.27 [+0.12]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 |  | 7.41 | 6.65 | NaN | 7.12 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 |  | 7.44 [+0.03] | 6.67 [+0.02] | NaN | 7.21 [+0.09]
    |'
  prefs: []
  type: TYPE_TB
- en: 5.3 Experiment Result on Common Sense QA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.3 Experiment Result on Common Sense QA ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") compares
    the Common Sense QA [[42](#bib.bib42)] result of applying the Integer Scale on
    state-of-the-art quantization approaches. A similar conclusion to Section [5.2](#S5.SS2
    "5.2 Experiment Result on LAMBADA, C4, and WikiText-2 ‣ 5 Experiments ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") can be reached.
    More results on MMLU [[16](#bib.bib16)] can be found in Table [8](#A2.T8 "Table
    8 ‣ B.1 Experiment Result on MMLU ‣ Appendix B Additional Discussions ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") in Section [B](#A2
    "Appendix B Additional Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison with state-of-the-art quantization methods on Common Sense
    QA. For all models tested, we set the weight group size to 128 and apply symmetric
    quantization. Integer Scale (IS) with amplifier 1024 is used.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | HyperParam | Common Sense QA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Method | BitWidth | WinoGrande | PIQA | HellaSwag | ARC_e | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B | FP16 | W16A16 | 0.6906 | 0.7911 | 0.7598 | 0.7458 | 0.7468 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 0.6819 | 0.7829 | 0.7380 | 0.6961 | 0.7247 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 0.6882 | 0.7845 | 0.7359 | 0.6932 | 0.7255 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 0.6890 | 0.7807 | 0.7418 | 0.6856 | 0.7243 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 0.6803 | 0.7818 | 0.7399 | 0.6717 | 0.7184 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 | 0.6930 | 0.7873 | 0.7427 | 0.6890 | 0.7280 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 | 0.6882 | 0.7818 | 0.7393 | 0.6898 | 0.7248 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-13B | FP16 | W16A16 | 0.7222 | 0.8052 | 0.7938 | 0.7744 | 0.7739
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 0.7080 | 0.8003 | 0.7858 | 0.7980 | 0.773 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 0.7040 | 0.8025 | 0.7854 | 0.7917 | 0.7709 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 0.7182 | 0.7976 | 0.7758 | 0.7677 | 0.7648 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 0.7246 | 0.7992 | 0.7734 | 0.7668 | 0.7660 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 | 0.7214 | 0.7992 | 0.7810 | 0.7710 | 0.7682 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 | 0.7127 | 0.7954 | 0.7786 | 0.7715 | 0.7646 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-70B | FP16 | W16A16 | 0.7798 | 0.8275 | 0.8381 | 0.8098 | 0.8138
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 0.7664 | 0.8313 | 0.8314 | 0.8131 | 0.8106 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 0.7585 | 0.8324 | 0.8287 | 0.8077 | 0.8068 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 0.7664 | 0.8194 | 0.8202 | 0.8005 | 0.8016 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 0.7624 | 0.8199 | 0.8218 | 0.7929 | 0.7993 |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | FP16 | W16A16 | 0.7648 | 0.8368 | 0.8403 | 0.835 | 0.8192
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 0.7553 | 0.8161 | 0.8272 | 0.8056 | 0.8011 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 0.7427 | 0.8145 | 0.8280 | 0.7925 | 0.7944 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 0.7506 | 0.8341 | 0.8288 | 0.8228 | 0.8091 |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 0.7443 | 0.8286 | 0.8252 | 0.8131 | 0.8028 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 | 0.7553 | 0.8308 | 0.8338 | 0.8165 | 0.8091 |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 | 0.7506 | 0.8308 | 0.8337 | 0.8178 | 0.8082 |'
  prefs: []
  type: TYPE_TB
- en: 5.4 W4A8 Kernel Latency Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") (a)
    illustrates the comparison of kernel implementations under various bandwidths.
    Marlin [[13](#bib.bib13)] ships so far the most advanced W4A16 kernel implementation.
    Odyssey’s W4A8 scheme largely benefits its specific FastGEMM and has the optimal
    acceleration ratio over FP16\. It can be seen that fine-grained W4A8 with integer
    scale becomes a feasible scheme between W4A16 and non-fine-grained W4A8 to have
    better accuracy. Interestingly, we discover a “performance cliff” (gray-colored)
    where the acceleration ratio suddenly drops when it transits from memory-bound
    to compute-bound scenarios. This is due to the sudden drop from the ideal 4$\times$
    vs. FP16.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1db0a2fa61741afa72176913e29491f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Kernel Acceleration
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3a3269e898de57a881fa2ca276b24c26.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mistral 8x7B Speedup
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: (a) Fine-grained W4A8 kernel (K=4096, N=22016) with the integer scale
    (W4A8 Integer Scale) boosts its float scale counterpart (W4A8 Float Scale). The
    gray region denotes the “performance cliff”. (b) End-to-end speed boost on Mixtral
    8x7B over FP16 under various batch sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Speed boost on Mixture-of-experts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") (c)
    shows the end-to-end latency of the W4A8 Integer Scale scheme applied to the Mixtral
    8$\times$ boost, compared with FP16 and W4A16 respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Our Quantization Recipe for LLaMA-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 5: Our LLaMA-3 Integer Scale recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | BitWidth | $\alpha$ | Group | C4 | WikiText-2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3-8B | W4A8 | - | 128 | 9.331 | 6.352 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3-8B | W4A8 | 8192 | 128 | 9.379 | 6.382 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3-70B | W4A8 | - | 128 | 7.061 | 3.280 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-3-70B | W4A8 | 8192 | 128 | 7.092 | 3.312 |'
  prefs: []
  type: TYPE_TB
- en: 'LLaMA-3 is difficult to quantize at lower bits compared with its predecessors,
    as confirmed in  [[17](#bib.bib17)]. To counter the problem, we apply QuaRot [[3](#bib.bib3)]
    with a fine-grained paradigm. We adopt 8-bit per-token activation quantization
    and 4-bit per-channel symmetric weight quantization with a group size of 128\.
    Besides, we use fine-grained W8A8 for down projection layers following the observation
    in  [[24](#bib.bib24)]. Table [5](#S5.T5 "Table 5 ‣ 5.6 Our Quantization Recipe
    for LLaMA-3 ‣ 5 Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs") exhibits the result of our LLaMA-3 scheme, while integer
    scale outperforms GPTQ’s W4A16 (-1.16% in C4 perplexity) shown in Table [1](#S3.T1
    "Table 1 ‣ 3.1 Fine Granularity Strengthens Current Quantization Approaches ‣
    3 Motivation ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Comparison with Marlin’s W4A16 Scheme
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 6: C4 and WikiText-2 perplexity, and MMLU zero-shot accuracy of LLaMA-2-7B
    quantized with Marlin’s implementation of GPTQ (W4A16) vs. GPTQ w/ Integer Scale
    (W4A8).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | BitWidth | C4 | WikiText-2 | MMLU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A16 | 7.2093 | 5.8212 | 39.11% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ Integer Scale | W4A8 | 7.4011 | 5.9433 | 38.54% |'
  prefs: []
  type: TYPE_TB
- en: 'We compare our Integer Scale scheme with Marlin’s implementation of GPTQ [[13](#bib.bib13)]
    in Table [6](#S5.T6 "Table 6 ‣ 5.7 Comparison with Marlin’s W4A16 Scheme ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").
    We are mostly on par with GPTQ at W4A16 when tested on C4, WikiText-2, and MMLU.
    Their acceleration ratios vs. FP16 are compared in Figure [5](#S5.F5 "Figure 5
    ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments ‣ Integer Scale: A Free Lunch
    for Faster Fine-grained Quantization of LLMs") where W4A8 surpasses W4A16 mainly
    due to faster tensor core execution at lower bit widths. This attests that fine-grained
    W4A8 with the Integer Scale is a competitive strategy in terms of both quantization
    loss and speed.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Comparison with QServe’s W4A8 Kernel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [6](#S5.F6 "Figure 6 ‣ 5.8 Comparison with QServe’s W4A8 Kernel ‣ 5
    Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") presents the kernel speed comparison with QServe [[27](#bib.bib27)],
    which ships an advanced W4A8 kernel. For coarse-grained W4A8 kernel with M=1,
    K=4096, and N=22016, our W4A8 kernel execution is substantially faster than QServe
    at all batch sizes. A similar conclusion is affirmed for the fine-grained kernel
    at a typical group size of 128\. Both being the same bit widths, our fine-grained
    kernel with Integer Scale is substantially faster than QServe’s, with a maximum
    of being 1.53$\times$. It turns out that the main difference lies in the intrinsic
    complexity of Dual Quantization [[51](#bib.bib51)] they adopted which first quantizes
    weights in 8-bit and again in 4-bit. Note the second step is kept asymmetric to
    counter quantization loss. This *asymmetric* scheme requires element-wise multiplication
    and subtraction that must be done in costly CUDA cores. See more details in [B.2](#A2.SS2
    "B.2 More Discussion with QServe ‣ Appendix B Additional Discussions ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07ca1b5ed82adfd0a796fa7c18f83d0b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Fine-grained Integer Scale
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa10c7858ca106bc62da9448b59bb343.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Coarse-grained
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Kernel speed comparison with QServe’s W4A8 at K=4096, N=22016\. The
    acceleration ratio is against FP16\. Both our fine and coarse-grained kernels
    are faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Ablation Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Fixed Amplifier vs. Heuristic Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To find the optimal amplifier for the integer scale, we test several amplifiers
    in Table [7](#S6.T7 "Table 7 ‣ 6.1 Fixed Amplifier vs. Heuristic Search ‣ 6 Ablation
    Study ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").
    It turns out that using an amplifier bigger than 1024 doesn’t bring substantial
    gains while $2^{10}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Ablation on the amplifier value. Perplexity is tested on C4.'
  prefs: []
  type: TYPE_NORMAL
- en: '| BitWidth | Amplifier | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B | LLaMA-3-8B
    | LLaMA-3-70B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | - | 7.43 | 6.64 | 5.66 | 10.00 | 9.06 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | Heuristic | 7.46 | 6.65 | 5.66 | 10.03 | 9.10 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | 128 | 6.75 | 7.57 | 5.81 | 15.52 | 13.84 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | 512 | 7.45 | 6.65 | 5.67 | 10.09 | 9.27 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | 1024 | 7.45 | 6.64 | 5.66 | 10.03 | 9.04 |'
  prefs: []
  type: TYPE_TB
- en: '| W4A16 | 4096 | 7.45 | 6.64 | 5.67 | 10.00 | 8.91 |'
  prefs: []
  type: TYPE_TB
- en: 6.2 Speed Comparison of Float Scale vs. Integer Scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We compare the difference in inference speed using float and integer scales
    to showcase the latency advantage of using the Integer scale in Figure [5](#S5.F5
    "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments ‣ Integer Scale:
    A Free Lunch for Faster Fine-grained Quantization of LLMs") (a). The speedup is
    at most 2.3$\times$, suggesting the reduction of costly type conversions is more
    than necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduced a plug-and-play scheme called *Integer Scale* that
    can be applied to speed up the existing fine-grained quantization approaches.
    We showed through extensive experiments that the Integer Scale not only benefits
    from the performance boost due to fine granularity but also well resolves the
    intrinsic computational overhead. It can serve as a default free-lunch technique
    with fine-grained approaches of various bandwidths to render an overall competitive
    quantization strategy. Moreover, the same strategy can be applied to Mixtral 8$\times$7B
    based on a mixture-of-experts and LLaMA-3, which were previously difficult to
    quantize at lower bit widths.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI@Meta [2024] AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashkboos et al. [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end
    4-bit inference on generative large language models. *arXiv preprint arXiv:2310.09259*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ashkboos et al. [2024] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    Quarot: Outlier-free 4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. In *Conference on Neural
    Information Processing Systems (NeurIPS)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2024] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D.
    Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework
    with multiple decoding heads. *arXiv preprint arXiv: 2401.10774*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contributors [2023] LMDeploy Contributors. Lmdeploy: A toolkit for compressing,
    deploying, and serving llm. [https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. [2022] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. FlashAttention: Fast and memory-efficient exact attention with io-awareness.
    *arXiv preprint arXiv:2205.14135*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *North American Chapter of the Association for Computational
    Linguistics (NAACL)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Egiazarian et al. [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language
    models via additive quantization. *arXiv preprint arXiv:2401.06118*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2022] Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar and Alistarh [2024] Elias Frantar and Dan Alistarh. Marlin: a fast
    4-bit inference kernel for medium batchsizes. [https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good
    are low-bit quantized llama3 models? an empirical study. *arXiv preprint arXiv:2404.14047*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. In Andreas Krause, Emma
    Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett,
    editors, *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202 of *Proceedings of Machine Learning Research*, pages 19274–19286\.
    PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/leviathan23a.html](https://proceedings.mlr.press/v202/leviathan23a.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2024a] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm
    tweaking: High-performance low-bit quantization of large language models. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 38, pages 18536–18544,
    2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023a] Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan
    Lu, Xiangxiang Chu, Yerui Sun, and Yuchen Xie. A speed odyssey for deployable
    quantization of llms. *arXiv preprint arXiv:2311.09550*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023b] Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
    Chu, Yerui Sun, Li Du, and Yuchen Xie. Fptq: Fine-grained post-training quantization
    for large language models. *arXiv preprint arXiv:2308.15987*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2024b] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle:
    Speculative sampling requires rethinking feature uncertainty. In *International
    Conference on Machine Learning*, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2024] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan
    Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design
    for efficient llm serving. *arXiv preprint arXiv:2405.04532*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micikevicius et al. [2022] Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. *arXiv preprint
    arXiv:2209.05433*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. [2021] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei
    Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network
    quantization. *arXiv preprint arXiv:2106.08295*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA [2023a] NVIDIA. cutlass. [https://github.com/NVIDIA/cutlass](https://github.com/NVIDIA/cutlass),
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA [2023b] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer),
    2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA [2023c] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paperno et al. [2016] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse
    context. *arXiv preprint arXiv:1606.06031*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2022] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    Lut-gemm: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models. *arXiv preprint arXiv:2206.09557*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the
    limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21(140):1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shang et al. [2023] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm:
    Partially binarized large language models. *arXiv preprint arXiv:2310.00034*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. *arXiv preprint arXiv:2308.13137*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talmor et al. [2019] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan
    Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tata and Patel [2003] Sandeep Tata and Jignesh M Patel. PiQA: An algebra for
    querying protein data sets. In *International Conference on Scientific and Statistical
    Database Management*, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence
    and lattice codebooks. *arXiv preprint arXiv:2402.04396*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Conference on Neural Information Processing Systems (NeurIPS)*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. [2024] Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei
    Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou,
    et al. Fp6-llm: Efficiently serving large language models through fp6-centric
    algorithm-system co-design. *arXiv preprint arXiv:2401.14112*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pages
    38087–38099\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and affordable post-training
    quantization for large-scale transformers. *arXiv preprint arXiv:2206.01861*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2024] Luoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou,
    and Hong Zhou. Dual grained quantization: Efficient fine-grained quantization
    for LLM, 2024. URL [https://openreview.net/forum?id=ktmMkOOeYb](https://openreview.net/forum?id=ktmMkOOeYb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom:
    Low-bit quantization for efficient and accurate llm serving. *arXiv preprint arXiv:2310.19102*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Background Knowledge on LLM Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Symmetric vs. Asymmetric Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We suggest referring to the white paper [[31](#bib.bib31)] for a thorough understanding
    of network quantization. We draw some key concepts here as a quick manual. Both
    symmetric and asymmetric quantization use uniform quantization that maps float
    values to integer values with a single scale. Symmetric quantization computes
    the scale $s$ as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s=\frac{&#124;X&#124;_{max}}{2^{n-1}-1}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Q(X)=clamp(\lfloor X/s\rceil,-2^{n-1},2^{n-1}-1)$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: For asymmetric quantization, a zero point is utilized.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s=\frac{X_{max}-X_{min}}{2^{n}-1},z=\lfloor\frac{-X_{min}}{s}\rceil$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $Q(X)=clamp(\lfloor X/s\rceil+z,0,2^{n}-1)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: A.2 Per-tensor, Per-token, Per-channel quantization, Group-wise Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Take symmetric quantization as an example, *per-tensor quantization* uses the
    same scale for all tensor values. *Per-channel/token quantization* uses a scale
    for a row or a column of the tensor. We can divide each channel into groups for
    *group-wise quantization* [[26](#bib.bib26)], also called fined-grained quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Experiment Result on MMLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [8](#A2.T8 "Table 8 ‣ B.1 Experiment Result on MMLU ‣ Appendix B Additional
    Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") compares the result on MMLU [[16](#bib.bib16)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison with state-of-the-art quantization methods on MMLU. For
    all models tested, we set the weight group size to 128 and apply symmetric quantization.
    Integer Scale (IS) with amplifier 1024 is used.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | HyperParam | MMLU |'
  prefs: []
  type: TYPE_TB
- en: '|  | Method | BitWidth | Hums. | STEM | Social | Other | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-7B | FP16 | W16A16 | 36.92% | 30.75% | 40.92% | 45.68% | 38.49% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 33.69% | 30.45% | 40.36% | 42.91% | 36.58% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 34.64% | 31.35% | 39.36% | 43.18% | 36.94% [+0.36%] |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 34.86% | 29.69% | 40.98% | 41.27% | 36.57% |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 34.13% | 30.19% | 40.40% | 41.52% | 36.36% [-0.21%] |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 | 34.39% | 31.84% | 42.28% | 43.77% | 37.74% |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 | 33.65% | 31.05% | 40.17% | 43.18% | 36.72% [-1.02%]
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-13B | FP16 | W16A16 | 54.43% | 44.27% | 63.41% | 60.76% | 55.68%
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 51.88% | 43.57% | 62.01% | 60.21% | 54.24% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 52.18% | 43.27% | 61.33% | 60.83% | 54.27% [+0.03%] |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 50.07% | 41.75% | 60.90% | 59.19% | 52.76% |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 49.65% | 42.64% | 59.80% | 58.45% | 52.40% [-0.36%] |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 | 52.56% | 43.21% | 62.56% | 60.67% | 54.61% |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 | 52.05% | 43.14% | 61.72% | 60.02% | 54.09% [-0.52%]
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-2-70B | FP16 | W16A16 | 65.16% | 57.79% | 80.44% | 74.61% | 69.11%
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 62.49% | 55.17% | 78.55% | 73.01% | 66.86% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 62.42% | 55.14% | 78.39% | 72.73% | 66.74% [-0.12%] |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 63.44% | 55.86% | 78.45% | 72.12% | 67.11% |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 63.70% | 55.33% | 78.00% | 71.75% | 66.89% [-0.22%] |'
  prefs: []
  type: TYPE_TB
- en: '| Mixtral-8x7B | FP16 | W16A16 | 64.46% | 61.30% | 81.51% | 77.39% | 70.50%
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ | W4A8 | 61.70% | 58.78% | 78.78% | 73.81% | 67.61% |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ w/ IS | W4A8 | 61.66% | 57.55% | 77.58% | 73.60% | 67.02% |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ | W4A8 | 64.48% | 60.17% | 80.05% | 75.20% | 69.44% |'
  prefs: []
  type: TYPE_TB
- en: '| AWQ w/ IS | W4A8 | 62.85% | 59.18% | 79.07% | 74.58% | 68.32% |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant | W4A8 | 63.00% | 58.78% | 80.21% | 75.69% | 68.79% |'
  prefs: []
  type: TYPE_TB
- en: '| Omniquant w/ IS | W4A8 | 62.17% | 58.81% | 79.92% | 75.17% | 68.34% |'
  prefs: []
  type: TYPE_TB
- en: B.2 More Discussion with QServe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the adopted asymmetry quantization, QServe’s kernel is prone to complex
    computation logic that can be formulated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\cdots$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $s_{i}$ is element-wise multiplication, and the subtraction is performed
    with a vadd4 instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#A2.F7 "Figure 7 ‣ B.2 More Discussion with QServe ‣ Appendix B
    Additional Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") gives the additional comparison on kernel (N=4096, K=4096), where our
    fine and coarse-grained kernels also outperform QServe, indicating our flexibility
    in different inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2111332f0440466232609ae146cdd3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Fine-grained
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd3ffcd44ec6d88df3eddf8b67565a66.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Coarse-grained
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Kernel (N=4096,K=4096) speed comparison with QServe. The acceleration
    ratio is against FP16.'
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Max Activation Values Per Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To verify whether our amplifier choice is feasible and not causing overflows,
    we illustrate the maximum layerwise activation values on the investigated models
    in Figure [8](#A2.F8 "Figure 8 ‣ B.3 Max Activation Values Per Layer ‣ Appendix
    B Additional Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs"). It appears no layer’s output goes near the INT32 upper
    bound. We refrain from selecting a higher amplifier to improve performance since
    it will generate few gains and in the meantime increase the overflow risk.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/806d8b3fe6cfca447713bdc6d807058a.png)![Refer to caption](img/515529ed3a1830d8651c1cc36b0ad46e.png)![Refer
    to caption](img/c828a53d32f5d4efcdba22f20dafb411.png)![Refer to caption](img/471b2673813b67a68cfc331e5b8c851f.png)![Refer
    to caption](img/3c9491a47504c0735e6a7b231fe55120.png)![Refer to caption](img/fdee8bc35aeaf21239e4971154fd1bcc.png)![Refer
    to caption](img/197132d6a4629f67d2eade9d34bf90c7.png)![Refer to caption](img/4a3ec601c3ae655a0c64a79bc4a4e568.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Maximum activation values per layer of quantized LLaMA-2 models and
    Mixtral 8x7B using an amplifier of 1024.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our method is limited by the possible overflow risks. However, we can trade
    off with per group removal of the amplifier which introduces extra computations.
    We can opt to use this degraded version of our GEMM kernels for layers that suffer
    from overflow risks.
  prefs: []
  type: TYPE_NORMAL
