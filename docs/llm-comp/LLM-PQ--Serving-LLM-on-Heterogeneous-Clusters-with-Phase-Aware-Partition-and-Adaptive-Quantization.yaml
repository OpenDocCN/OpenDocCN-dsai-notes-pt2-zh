- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:49:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:49:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-PQ：在异构集群上通过阶段感知分区和自适应量化服务LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01136](https://ar5iv.labs.arxiv.org/html/2403.01136)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01136](https://ar5iv.labs.arxiv.org/html/2403.01136)
- en: Juntao Zhao University of Hong KongHong Kong ,  Borui Wan University of Hong
    KongHong Kong ,  Yanghua Peng ByteDance Inc.USA ,  Haibin Lin ByteDance Inc.USA
     and  Chuan Wu University of Hong KongHong Kong
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Juntao Zhao 香港大学 香港，Borui Wan 香港大学 香港，Yanghua Peng 字节跳动 美国，Haibin Lin 字节跳动 美国
    和 Chuan Wu 香港大学 香港
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated
    impressive performance on various tasks. The immense sizes of LLMs have led to
    very high resource demand and cost for running the models. Though the models are
    largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous
    cluster with a mix of available high- and low-capacity GPUs can potentially substantially
    reduce the serving cost. There is a lack of designs to support efficient LLM serving
    using a heterogeneous cluster, while the current solutions focus on model partition
    and uniform compression among homogeneous devices. This paper proposes LLM-PQ,
    a system that advocates adaptive model quantization and phase-aware partition
    to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully
    decide on mixed-precision model quantization together with phase-aware model partition
    and micro-batch sizing in distributed LLM serving with an efficient algorithm,
    to greatly enhance inference throughput while fulfilling user-specified model
    quality targets. Extensive experiments on production inference workloads in 11
    different clusters demonstrate that LLM-PQ achieves up to 2.88$\times$ on average)
    throughput improvement in inference, showing great advantages over state-of-the-art
    works. Source code available at https://github.com/tonyzhao-jt/LLM-PQ.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大规模语言模型（LLMs）取得了在各种任务上的显著突破。这些模型的巨大规模导致了非常高的资源需求和运行成本。虽然这些模型现在主要使用统一的高性能GPU，但利用一个包含高性能和低性能GPU的异构集群可以大幅降低服务成本。目前缺乏支持在异构集群上高效服务LLM的设计，而现有解决方案主要集中在模型分区和在同质设备上均匀压缩。本文提出了LLM-PQ，一个倡导自适应模型量化和阶段感知分区的系统，以提高在异构GPU集群上的LLM服务效率。我们在分布式LLM服务中，结合阶段感知模型分区和微批次大小，精心决定混合精度模型量化，并采用高效算法，大幅提升推理吞吐量，同时满足用户指定的模型质量目标。在11个不同集群的生产推理工作负载上的大量实验表明，LLM-PQ在推理中平均提高了2.88$\times$的吞吐量，显示出相较于现有最先进技术的巨大优势。源代码可在
    https://github.com/tonyzhao-jt/LLM-PQ 查阅。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Large-scale language models (LLMs) such as GPT3, LLaMA, OPT, and BLOOM ([scao2022bloom,](#bib.bib31)
    ; [Zhang2022OPTOP,](#bib.bib40) ; [Touvron2023LLaMAOA,](#bib.bib33) ) have exhibited
    unprecedented performance in pushing the envelope of various artificial intelligence
    (AI) tasks. The outstanding model performance is largely attributed to a very
    large model size ranging from a few hundred million to even half a trillion parameters.
    Training an LLM requires thousands of GPUs and millions of dollars ([gpt3,](#bib.bib3)
    ). Serving a trained LLM is also resource-demanding and cost-intensive, as an
    LLM cannot commonly be fit into a single GPU, therefore multiple GPUs are required
    for distributed inference.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模语言模型（LLMs），如GPT3、LLaMA、OPT和BLOOM ([scao2022bloom,](#bib.bib31) ; [Zhang2022OPTOP,](#bib.bib40)
    ; [Touvron2023LLaMAOA,](#bib.bib33) )在推动各种人工智能（AI）任务的边界上表现出了前所未有的性能。卓越的模型性能主要归功于非常大的模型规模，从几亿到甚至半万亿个参数不等。训练一个LLM需要数千个GPU和数百万美元
    ([gpt3,](#bib.bib3) )。服务一个训练好的LLM也需要大量资源和高昂成本，因为LLM通常无法放入单个GPU中，因此需要多个GPU进行分布式推理。
- en: To cope with the massive size of LLMs, a number of approaches have been proposed
    to enable their efficient deployment in practice. DeepSpeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1)
    ), FasterTransformer and HuggingFace Text Generation Inference (TGI) ([huggingface_text_generation_inference,](#bib.bib13)
    ) integrate existing model parallelism techniques, such as tensor-parallelism
    (TP) and pipeline parallelism (PP), with memory footprint reduction schemes, e.g.,
    quantization or offloading, to lower the resource demands of model serving in
    a distributed manner. For memory footprint reduction schemes, quantization converts
    model weights into lower-precision formats (e.g., 8-bit), reducing memory consumption.
    Offloading methods ([flexgen,](#bib.bib32) ) leverage aggregate CPU and NVMe memory
    capacity to store weights or compute a portion of the GPU workload. However, the
    existing solutions are mainly designed for models serving on homogeneous clusters,
    limiting their performance in a heterogeneous cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对 LLM 的庞大规模，已经提出了多种方法以实现其高效部署。DeepSpeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1)
    )、FasterTransformer 和 HuggingFace 文本生成推理（TGI） ([huggingface_text_generation_inference,](#bib.bib13)
    ) 将现有的模型并行技术，如张量并行（TP）和流水线并行（PP），与内存占用减少方案（如量化或卸载）结合，以降低分布式模型服务的资源需求。对于内存占用减少方案，量化将模型权重转换为较低精度的格式（如
    8 位），从而减少内存消耗。卸载方法 ([flexgen,](#bib.bib32) ) 利用汇总的 CPU 和 NVMe 内存容量来存储权重或计算部分 GPU
    工作负载。然而，现有的解决方案主要设计用于同质集群上的模型服务，限制了其在异构集群中的性能。
- en: '![Refer to caption](img/aea12a54f0ebc6b51840210a022ef8d7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/aea12a54f0ebc6b51840210a022ef8d7.png)'
- en: (a) GPU Portions
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPU 部分
- en: '![Refer to caption](img/6a0836767d2966238500ad6710748275.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6a0836767d2966238500ad6710748275.png)'
- en: (b) Average utilization of different types of GPUs in one month
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一个月内不同类型 GPU 的平均利用率
- en: Figure 1\. GPU proportions and utilization rates in a real-world production
    AI cluster.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 实际生产 AI 集群中的 GPU 比例和利用率。
- en: 'A practical AI cloud or machine learning (ML) cluster often contains heterogeneous
    devices, e.g., GPUs of different models purchased at different times. Utilization
    of different types of GPUs may differ substantially. Fig. [1](#S1.F1 "Figure 1
    ‣ 1\. Introduction ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") shows the proportion of different GPUs in
    a production cluster, with fewer percentages of high-calibre GPUs (NVIDIA A100,
    V100) the majority being relatively low-calibre inference GPUs (such as T4). The
    utilization rate of other GPUs is much lower than that of A100, which are used
    intensively for both training and inference of large models nowadays for the best
    performance. Efficiently exploiting available heterogeneous GPUs for LLM serving
    is worthwhile to explore, to fully utilize available resources and substantially
    reduce the cost of provisioning LLM-enabled applications.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '一个实际的 AI 云或机器学习（ML）集群通常包含异构设备，例如不同型号的 GPU，这些 GPU 可能是在不同时间购买的。不同类型的 GPU 的使用情况可能差异很大。图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") 显示了生产集群中不同 GPU
    的比例，其中高性能 GPU（如 NVIDIA A100、V100）的比例较少，大多数为相对低性能的推理 GPU（如 T4）。其他 GPU 的利用率远低于 A100，后者目前在大型模型的训练和推理中被广泛使用，以获得最佳性能。有效利用现有的异构
    GPU 进行 LLM 服务是值得探索的，以充分利用现有资源并大幅降低 LLM 启用应用程序的配置成本。'
- en: 'The commonly adopted TP and PP paradigms partition model operations/layers
    evenly among the GPUs, which is not suitable for heterogeneous GPUs and results
    in either low utilization of high-capacity GPUs or out-of-memory (OOM) errors
    on low-memory GPUs. The limited studies of models serving on heterogeneous clusters ([hu2021pipeline,](#bib.bib17)
    ) focus on the partition of encoder-based transformer models. However, mainstream
    LLMs with decoder-only structures contain two phases during inference: prompt
    processing (prefill) and token generation (decode). While the former phase is
    similar to the inference of encoder-based transformers, the latter has a totally
    different pattern (see Sec. [2.1](#S2.SS1 "2.1\. Generative Inference of LLM ‣
    2\. BackGround and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters
    with Phase-Aware Partition and Adaptive Quantization")), making the previous partition
    solutions not suitable. Besides, the execution time required for each phase, depending
    on the prompt length and token generation number, varies significantly. What is
    worse, in a heterogeneous cluster, this difference can even be amplified, causing
    model partitioning that focuses on the time of the first phase instead of both
    being far from optimal. Therefore, phase-aware model partition schemes warrant
    investigation. Additionally, extra memory required for pre-and post-processing
    during LLM inference, such as text embedding for converting input tokens to word
    vectors, should also be considered, especially when utilizing low-calibre GPUs
    which have limited GPU memory.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '通常采用的TP和PP范式将模型操作/层均匀分配到GPU上，这对于异质GPU来说并不合适，会导致高容量GPU的利用率低或低内存GPU上出现OOM错误。关于在异质集群上服务的模型的有限研究（[hu2021pipeline,](#bib.bib17)）主要集中在基于编码器的变换器模型的分区。然而，主流的只含解码器的LLM在推理过程中包含两个阶段：提示处理（预填充）和令牌生成（解码）。虽然前者类似于基于编码器的变换器的推理，但后者有完全不同的模式（见Sec.
    [2.1](#S2.SS1 "2.1\. Generative Inference of LLM ‣ 2\. BackGround and Motivation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")），使得之前的分区解决方案不适用。此外，每个阶段所需的执行时间，根据提示长度和令牌生成数量，差异显著。在异质集群中，这种差异甚至会被放大，使得关注第一个阶段时间的模型分区方案远未达到最佳。因此，**基于阶段的模型分区方案**值得研究。此外，还应考虑LLM推理过程中预处理和后处理所需的额外内存，例如将输入令牌转换为词向量的文本嵌入，特别是在使用内存有限的低端GPU时。'
- en: When the model is partitioned among heterogeneous GPUs, adopting a single quantization
    precision across all model layers in different types of GPUs is always suboptimal.
    uniform single-precision model quantization can select a precision, e.g., INT4,
    that is suitable for GPUs with lower memory to avoid OOM (Out Of Memory) problem,
    but causing a notable portion of memory waste for those with abundant GPU memory.
    Adaptive mixed-precision quantization for LLM, which is not investigated in the
    literature ([frantar2023gptq,](#bib.bib14) ; [xiao2023smoothquant,](#bib.bib36)
    ), is more desirable. By using higher precision for model weights on GPUs with
    more available memory instead of forcing them to use the same one in those low-calibre
    GPUs, adaptive mixed-precision quantization can not only avoid memory waste but
    promote the model quality as well.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在异质GPU之间进行分区时，在不同类型的GPU上采用单一的量化精度总是不理想的。统一的单精度模型量化可以选择一种适合内存较低的GPU的精度，例如INT4，以避免OOM（内存溢出）问题，但会导致内存丰富的GPU上显著的内存浪费。对于LLM，尚未在文献中研究的**自适应混合精度量化**（[frantar2023gptq,](#bib.bib14)
    ; [xiao2023smoothquant,](#bib.bib36)）更为理想。通过在内存更多的GPU上使用更高的精度进行模型权重量化，而不是强制它们在低端GPU上使用相同的精度，自适应混合精度量化不仅可以避免内存浪费，还能提升模型质量。
- en: 'In this work, we propose a novel system, LLM-PQ, to enable efficient LLM generative
    serving on heterogeneous GPU clusters. Instead of emphasizing the enhancement
    of throughput faced with infinite requests, as commonly pursued in recent works
    like vLLM ([vllm,](#bib.bib20) ). LLM-PQ directs its focus toward the efficient
    processing of a given workload, which is faced by the offline task. LLM-PQ advocates
    adaptive model quantization and phase-aware model partition, as well as efficient
    micro-batch scheduling for LLM pipeline serving. It jointly determines the quantization
    precisions, model layer partition, and hybrid micro-batch sizing strategies, given
    the LLM, available resources of the heterogeneous cluster, and user-specified
    model quality targets. Our contributions in designing LLM-PQ can be summarized
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个新系统LLM-PQ，以在异构GPU集群上实现高效的LLM生成服务。与最近的工作（如vLLM ([vllm,](#bib.bib20)
    ）中常见的面对无限请求的吞吐量提升不同，LLM-PQ将重点放在对离线任务所面临的给定工作负载的高效处理上。LLM-PQ倡导自适应模型量化和阶段感知模型划分，以及高效的微批量调度。它在考虑LLM、异构集群的可用资源和用户指定的模型质量目标的情况下，联合确定量化精度、模型层划分和混合微批量大小策略。我们在设计LLM-PQ方面的贡献可以总结如下：
- en: $\triangleright$ We provide a cost model that details the memory requirements
    of LLM serving under a mixed-precision quantization scheme. We learn a linear
    regression model to accurately predict the latency of mixed-precision LLM inference
    workloads with varying sequence lengths and batch sizes based on their phase-aware
    computational characteristics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 我们提供了一个成本模型，详细描述了在混合精度量化方案下LLM服务的内存需求。我们学习了一个线性回归模型，以准确预测具有不同序列长度和批量大小的混合精度LLM推断工作负载的延迟，基于其阶段感知计算特性。
- en: $\triangleright$ We introduce adaptive mixed-precision into the search space
    of heterogeneous pipeline serving of LLM and provide a variance indicator the
    measure the layer sensitivity towards different level quantization. We develop
    an iterative algorithm that first explores possible GPU orderings and different
    (phase, micro-batch size) pairs in the pruned search space, and then solves an
    integer linear programming (ILP) problem to determine the best partition and quantization
    bitwidths.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 我们将自适应混合精度引入LLM的异构管道服务搜索空间，并提供了一个方差指标来测量层对不同量化级别的敏感性。我们开发了一个迭代算法，该算法首先在剪枝后的搜索空间中探索可能的GPU排序和不同的（阶段，微批量大小）对，然后解决一个整数线性规划（ILP）问题，以确定最佳的划分和量化位宽。
- en: $\triangleright$ on average) as compared to state-of-the-art approaches.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 与最先进的方法相比（平均）.
- en: 2\. BackGround and Motivation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与动机
- en: 2.1\. Generative Inference of LLM
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM的生成推断
- en: '![Refer to caption](img/4ee0e03eba4f4182aa7a178213e9ce67.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4ee0e03eba4f4182aa7a178213e9ce67.png)'
- en: 'Figure 2\. Two phases in LLM generative serving: (Top) Prefill phase takes
    the prompt sequence to generate the initial key-value pairs. (Bottom) Decode phase
    takes previously generated token & stored KV pairs to generate the next token.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. LLM生成服务的两个阶段：（上）预填充阶段使用提示序列生成初始的键值对。（下）解码阶段利用先前生成的令牌和存储的KV对生成下一个令牌。
- en: 'LLM generally refers to a suite of decoder-only transformer models with large
    parameter sizes ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    ). Unlike encoder-based transformers like ViT-Huge ([vit,](#bib.bib11) ) and Bert-Large ([bert,](#bib.bib9)
    ) that are sequence-to-sequence, LLMs generate tokens one by one in an inference
    process that comprises two phases ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    )(Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. Generative Inference of LLM ‣ 2\. BackGround
    and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")): prefill and decode ([flexgen,](#bib.bib32)
    ). In the prefill phase, the input prompt sequence produces key/value (KV) caches
    for each transformer layer, which is used in the attention mechanism as a context
    vector for later token generation. During the decode phase, stored KV pairs are
    updated as each subsequent token is generated one by one based on the preceding
    token; the token generation process continues until a stopping criterion is met,
    such as reaching the end of a sequence (EOS) or exceeding the maximum number of
    tokens allowed. During generative inference, each layer of the LLM undergoes a
    prefill phase followed by several passes in the decode phase (an example is given
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. Generative Inference of LLM ‣ 2\. BackGround
    and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 通常指的是一套仅解码器的变换器模型，具有大型参数 ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    )。与 ViT-Huge ([vit,](#bib.bib11) ) 和 Bert-Large ([bert,](#bib.bib9) ) 这样的基于编码器的变换器不同，LLM
    在推理过程中生成一个个 token，这个过程包括两个阶段 ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    )(图 [2](#S2.F2 "图 2 ‣ 2.1\. LLM 的生成推理 ‣ 2\. 背景和动机 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化服务
    LLM"))：预填充和解码 ([flexgen,](#bib.bib32) )。在预填充阶段，输入提示序列为每个变换器层生成关键/值 (KV) 缓存，这些缓存在注意力机制中作为上下文向量用于后续
    token 的生成。在解码阶段，存储的 KV 对在每次生成新的 token 时进行更新；token 生成过程持续进行，直到满足停止条件，例如达到序列结束 (EOS)
    或超出允许的最大 token 数。在生成推理过程中，LLM 的每一层经历一个预填充阶段，随后是多个解码阶段的处理（如图 [2](#S2.F2 "图 2 ‣
    2.1\. LLM 的生成推理 ‣ 2\. 背景和动机 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化服务 LLM") 中所示的例子）。'
- en: 'The time taken by the prefill and decode phases varies to the prompt length.
    By sampling 10,000 conversations generated by chatGPT from the ShareGPT ([ryokoai_sharegpt52k,](#bib.bib30)
    ) dataset, we found that the prompt length varies substantially: $<128$) when
    the prompt is long. Unlike prefill time, the decode time is determined by the
    number of generated tokens. These characteristics make the inference pattern of
    LLM more complicated than encoder-based transformers.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 预填充和解码阶段所需的时间与提示长度相关。通过对从 ShareGPT 数据集生成的 10,000 个对话进行采样，我们发现提示长度变化很大：当提示较长时，$<128$。与预填充时间不同，解码时间由生成的
    token 数量决定。这些特性使得 LLM 的推理模式比基于编码器的变换器更复杂。
- en: 2.2\. Heterogenous Model Parallelization
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 异构模型并行
- en: '![Refer to caption](img/525003ce64a97dc7236a6e1e65658618.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/525003ce64a97dc7236a6e1e65658618.png)'
- en: Figure 3\. Phase time decomposition with different precisions. $\times$ indicates
    time on P100 compared to V100.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 不同精度下的阶段时间分解。$\times$ 表示 P100 上的时间与 V100 的对比。
- en: Pipeline parallelism ([Aminabadi2022DeepSpeedIE,](#bib.bib1) ; [huang2019gpipe,](#bib.bib18)
    ) has been widely adopted to distribute massive parameters of LLM across devices.
    The model is split into stages and micro-batches are processed over the stages
    in a pipelining manner. Each device executes a model stage, and data is passed
    between devices as it moves through the pipeline. Workload balance among stages
    is important as the throughput of pipeline serving is bounded by the execution
    time of the slowest stage.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线并行 ([Aminabadi2022DeepSpeedIE,](#bib.bib1) ; [huang2019gpipe,](#bib.bib18)
    ) 已被广泛采用以将 LLM 的大量参数分布到多个设备上。模型被拆分成多个阶段，并且微批量数据在阶段之间以流水线的方式处理。每个设备执行一个模型阶段，数据在设备之间传递，随着它在流水线中的移动。阶段之间的工作负载平衡很重要，因为流水线服务的吞吐量受到最慢阶段的执行时间的限制。
- en: 'Deriving optimal partitions among heterogeneous devices is challenging, especially
    when considering the two-phase token generation. The lower part of Fig. [3](#S2.F3
    "Figure 3 ‣ 2.2\. Heterogenous Model Parallelization ‣ 2\. BackGround and Motivation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization") gives the execution time of a single layer of the respective
    model with prompt length 512 and batch size 8\. The execution time ratio when
    running the same phase on different devices varies substantially. For example,
    under FP16, the execution time of the layer in the prefill phase on P100 is 14.53$\times$
    for the decode phase. Since the LLM inference time contains these two phases,
    pipeline stage partitioning should consider the execution time of both phases
    on each GPU. Existing solutions (e.g., PipeEdge ([hu2021pipeline,](#bib.bib17)
    )) partition single-phase encoder-based models on heterogeneous devices, whose
    solutions cannot be directly extended to two phases of LLM serving.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在异构设备之间推导最佳分区是具有挑战性的，特别是在考虑到两阶段令牌生成时。图[3](#S2.F3 "Figure 3 ‣ 2.2\. Heterogenous
    Model Parallelization ‣ 2\. BackGround and Motivation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")的下半部分给出了相应模型在提示长度512和批处理大小8下单层的执行时间。在不同设备上运行相同阶段的执行时间比例差异很大。例如，在FP16下，P100在预填充阶段的层执行时间是解码阶段的14.53$\times$。由于LLM推理时间包含这两个阶段，因此流水线阶段分区应考虑每个GPU上这两个阶段的执行时间。现有解决方案（例如，PipeEdge ([hu2021pipeline,](#bib.bib17)
    )）在异构设备上对单阶段编码器模型进行分区，这些解决方案无法直接扩展到LLM服务的两个阶段。'
- en: Furthermore, a complete language model includes an embedding layer, which is
    responsible for converting sentences into word vectors. In heterogeneous clusters,
    the embedding layer encounters more significant imbalance issues compared to homogeneous
    clusters due to the variety of the GPU’s computing and memory capabilities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个完整的语言模型包括一个嵌入层，负责将句子转换为词向量。在异构集群中，由于GPU的计算和内存能力的多样性，嵌入层遇到比同质集群更显著的不平衡问题。
- en: 2.3\. Online and Offline Serving Task
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 在线和离线服务任务
- en: There are two suites of the LLM inference workload. The online task handles
    infinite requests from runtime users, where the prompt length and token generation
    number are unpredictable. vLLM ([vllm,](#bib.bib20) ) introduces pageAttention
    to efficiently manage substantial and dynamically changing KV caches for each
    request. The offline task consists of predictable batch prompt processing tasks,
    where prompts are padded to a uniform length and the number of token generations
    is predetermined. FlexGen ([flexgen,](#bib.bib32) ) addresses the memory constraints
    in this scenario by employing multi-hierarchy offloading and zig-zag packing techniques.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLM推理工作负载有两种套件。在线任务处理来自运行时用户的无限请求，提示长度和生成的令牌数量无法预测。vLLM ([vllm,](#bib.bib20)
    ) 引入了pageAttention，以高效管理每个请求的大量动态变化的KV缓存。离线任务由可预测的批处理提示处理任务组成，提示被填充到统一长度，令牌生成数量是预先确定的。FlexGen ([flexgen,](#bib.bib32)
    ) 通过采用多层次卸载和之字形打包技术来解决这一场景中的内存限制。
- en: LLM-PQ targets the offline task with the prior knowledge of the prompt length
    and token generation number.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-PQ针对离线任务，已知提示长度和令牌生成数量。
- en: 'Opportunity 1: Phase-Aware Model Partition on Heterogenous GPUs. By considering
    the inference time of both the prefill and decode phases, and also taking into
    account the resource consumption and computation of the embedding layer, we can
    obtain a more comprehensive understanding of the complex generation process of
    LLM and therefore a more accurate latency modeling for making pipeline stage partition
    decision. This approach ensures improved performance in heterogeneous pipeline
    serving.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 机会 1：异构GPU上的阶段感知模型分区。通过考虑预填充阶段和解码阶段的推理时间，同时考虑嵌入层的资源消耗和计算，我们可以更全面地了解LLM的复杂生成过程，从而更准确地建模延迟以做出流水线阶段分区决策。这种方法确保了在异构流水线服务中的性能提升。
- en: 2.4\. Quantization in LLM
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. LLM中的量化
- en: Quantization is a model compression technique that maps high-precision values,
    such as those stored in FP16, to their low-precision counterparts. For symmetric
    quantization, the input data or model weight distribution is evenly partitioned
    into a fixed number of bins. Each bin is rounded to an n-bit quantized value using
    $\hat{x}=[\frac{x-q_{x}}{s_{x}}]$ is the dequantized value in floating point.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种模型压缩技术，它将高精度值（例如存储在FP16中的值）映射到其低精度对应值。对于对称量化，输入数据或模型权重分布被均匀划分为固定数量的箱。每个箱被四舍五入为n位量化值，使用$\hat{x}=[\frac{x-q_{x}}{s_{x}}]$作为浮点数中的反量化值。
- en: 'LLM Quantization. The weights of LLMs are typically stored in FP16/BF16\. Due
    to the large size of LLMs, it is often necessary to further compress the model
    weights for inference serving, e.g., using INT8 quantization to reduce the weight
    storage by half. Existing LLM quantization approaches can be categorized into
    two: (1) W8A8 kernel-based quantization (e.g., SmoothQuant ([xiao2023smoothquant,](#bib.bib36)
    ) and ZeroQuant([yao2022zeroquant,](#bib.bib37) )), which quantizes both activations
    and weights during serving; (2) weight-only quantization ([frantar2023gptq,](#bib.bib14)
    ; [frantar2023optq,](#bib.bib15) ; [lin2023awq,](#bib.bib21) ), which only quantizes
    model weights when loading the model into GPU memory. In this paper, we adopt
    decomposition kernel-based HuggingFace bitsandbytes ([dettmers2022llmint8,](#bib.bib6)
    ) to implement INT8 quantization. For precisions lower than 8 (e.g., 3 and 4),
    we use weight-only kernels provided by GPTQ ([frantar2023gptq,](#bib.bib14) )
    following serving system setup of HuggingFace TGI ([huggingface_text_generation_inference,](#bib.bib13)
    ), OpenLLM ([Pham_OpenLLM_Operating_LLMs_2023,](#bib.bib28) ).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLM量化。LLM的权重通常以FP16/BF16格式存储。由于LLM的规模庞大，通常需要进一步压缩模型权重以进行推理服务，例如，使用INT8量化将权重存储量减少一半。现有的LLM量化方法可以分为两类：（1）W8A8内核量化（例如，SmoothQuant ([xiao2023smoothquant,](#bib.bib36)
    ）和ZeroQuant([yao2022zeroquant,](#bib.bib37) )), 在推理过程中对激活和权重进行量化；（2）仅权重量化（[frantar2023gptq,](#bib.bib14)
    ; [frantar2023optq,](#bib.bib15) ; [lin2023awq,](#bib.bib21) ），仅在将模型加载到GPU内存时对模型权重进行量化。本文采用分解内核基础的HuggingFace
    bitsandbytes ([dettmers2022llmint8,](#bib.bib6) )实现INT8量化。对于低于8位的精度（例如，3位和4位），我们使用GPTQ ([frantar2023gptq,](#bib.bib14)
    )提供的仅权重量化内核，按照HuggingFace TGI ([huggingface_text_generation_inference,](#bib.bib13)
    )、OpenLLM ([Pham_OpenLLM_Operating_LLMs_2023,](#bib.bib28) )的服务系统设置。
- en: However, Existing LLM quantization works uniformly quantize all model layers
    to the same bit by default (e.g., 3, 4, or 8 ([frantar2023gptq,](#bib.bib14) ;
    [dettmers2022llmint8,](#bib.bib6) )) which leads to underutilized memory on high-calibre
    GPUs or OOM problems on low-calibre GPUs in a heterogeneous cluster. This is because
    different types of GPUs are not allowed to choose their most suitable quantization
    precision to match their capacities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现有的LLM量化工作默认将所有模型层均匀量化到相同的位数（例如，3、4或8位 ([frantar2023gptq,](#bib.bib14) ;
    [dettmers2022llmint8,](#bib.bib6) )），这会导致高性能GPU上的内存利用不足或低性能GPU在异构集群中的OOM问题。这是因为不同类型的GPU不能选择最适合其能力的量化精度。
- en: '![Refer to caption](img/17c0cfdf4579a286fb03ee626f24ccf7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17c0cfdf4579a286fb03ee626f24ccf7.png)'
- en: (a) BlOOM-3b PPL vs. Bitwidth
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (a) BlOOM-3b PPL与比特宽度
- en: '![Refer to caption](img/02e622b1e837919dae7c50b741f975c7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02e622b1e837919dae7c50b741f975c7.png)'
- en: (b) OPT-1.3b Accuracy vs. Bitwidth
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (b) OPT-1.3b准确率与比特宽度
- en: Figure 4\. BLOOM-3b (a) and OPT-1.3b (b) perplexity (PPL) & accuracy under different
    quantization schemes. Smaller PPL means the model is more confident in its prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. BLOOM-3b (a) 和 OPT-1.3b (b) 在不同量化方案下的困惑度（PPL）和准确率。较小的PPL意味着模型对其预测的信心更高。
- en: 'Opporunity 2: Adaptive Quantization for Better Accuracy and Speed. We advocate
    adaptive quantization by choosing potentially different bits for model layers
    on different GPUs, to better utilize the available memory, as well as to improve
    model quality and computation speed as compared to uniform quantization. We illustrate
    the benefits of adaptive quantization as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机会2：自适应量化以提高准确率和速度。我们倡导通过为不同GPU上的模型层选择不同的比特进行自适应量化，以更好地利用可用内存，并提高模型质量和计算速度，相比于均匀量化。我们如下展示自适应量化的好处：
- en: '1\. Adaptive quantization can lead to better model accuracy. We run BLOOM-3b1 ([scao2022bloom,](#bib.bib31)
    ) and OPT-1.3b ([Zhang2022OPTOP,](#bib.bib40) ) with different precision setups
    on A100 and evaluate the perplexity ([jelinek1977perplexity,](#bib.bib19) ), on
    three text datasets ([wikitext2,](#bib.bib24) ; [ptb,](#bib.bib23) ; [c4,](#bib.bib29)
    ). We also measure the model accuracy on popular zero-shot question-answering
    benchmarks LAMBADA ([paperno-etal-2016-lambada,](#bib.bib26) ), ARC([allenai:arc,](#bib.bib5)
    ) and PIQA ([Bisk2020,](#bib.bib2) );We use calibration data from the C4 dataset
    to determine quantization statistics. In Fig. [4](#S2.F4 "Figure 4 ‣ 2.4\. Quantization
    in LLM ‣ 2\. BackGround and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization"), the ‘mixed4-8’
    case denotes that we uniformly randomly assign 4 or 8 bits to each model layer,
    while ‘mixed3-4’ is to uniformly randomly assign bitwidth 3 or 4 to each layer.
    Mixed-precision quantization leads to better model performance than uniformly
    using the lower bit.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '1\. 自适应量化可以提高模型的准确性。我们在A100上运行了BLOOM-3b1 ([scao2022bloom,](#bib.bib31) ) 和OPT-1.3b ([Zhang2022OPTOP,](#bib.bib40)
    )，使用不同的精度设置，并评估了困惑度 ([jelinek1977perplexity,](#bib.bib19) )，在三个文本数据集上 ([wikitext2,](#bib.bib24)
    ; [ptb,](#bib.bib23) ; [c4,](#bib.bib29) )。我们还在流行的零样本问答基准LAMBADA ([paperno-etal-2016-lambada,](#bib.bib26)
    )、ARC([allenai:arc,](#bib.bib5) )和PIQA ([Bisk2020,](#bib.bib2) )上测量了模型的准确性；我们使用来自C4数据集的校准数据来确定量化统计数据。在图 [4](#S2.F4
    "Figure 4 ‣ 2.4\. Quantization in LLM ‣ 2\. BackGround and Motivation ‣ LLM-PQ:
    Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive
    Quantization")中，‘mixed4-8’案例表示我们将4或8位均匀随机分配给每个模型层，而‘mixed3-4’则是将位宽3或4均匀随机分配给每一层。混合精度量化比均匀使用较低位数的量化导致更好的模型性能。'
- en: '![Refer to caption](img/44bd97a79763f1a52b531a3639be2fce.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/44bd97a79763f1a52b531a3639be2fce.png)'
- en: Figure 5\. Execution time of prefill and decode phases under different precisions
    and batch sizes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 在不同精度和批次大小下的预填充和解码阶段的执行时间。
- en: '2\. Adaptive quantization speeds up inference. Fig. [5](#S2.F5 "Figure 5 ‣
    2.4\. Quantization in LLM ‣ 2\. BackGround and Motivation ‣ LLM-PQ: Serving LLM
    on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")
    shows how quantization performs with different device types and input shapes.
    The latency is measured on a single layer of OPT-30b with prompt length 512\.
    We observe that uniform low-precision quantization may not always result in inference
    speed-up, due to additional overhead that quantization introduces. FP16 precision
    leads to the fastest inference in many cases. If low-precision uniform quantization
    does not fully occupy the GPU memory, swapping certain layers with faster higher-precision
    kernels can accelerate the inference process. For instance, when there is remaining
    memory after uniformly quantizing to INT8, utilizing INT8-FP16 mixed-precision
    can be beneficial.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '2\. 自适应量化加快推理速度。图 [5](#S2.F5 "Figure 5 ‣ 2.4\. Quantization in LLM ‣ 2\. BackGround
    and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")展示了不同设备类型和输入形状下量化的表现。延迟在OPT-30b的单层上测量，提示长度为512。我们观察到，均匀低精度量化可能并不总是能加速推理，因为量化引入了额外的开销。FP16精度在许多情况下导致最快的推理。如果低精度均匀量化没有充分占用GPU内存，则用更快的高精度内核替换某些层可以加速推理过程。例如，当在均匀量化到INT8后还有剩余内存时，使用INT8-FP16混合精度可能会有利。'
- en: 2.5\. Challenges
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 挑战
- en: Table 1\. Model performance comparison under different layer quantizations.
    The best results are marked in bold. Unselected layers are retained in FP16.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 在不同层量化下的模型性能比较。最佳结果以**粗体**标记。未选择的层保留为FP16。
- en: '| Model | Layers Quantized to 4-bit | Avg. Perplexity | Avg. Accuracy (%) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 量化为4位的层 | 平均困惑度 | 平均准确率 (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| OPT-1.3b | 0-8 | 15.52 | 62.82 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 0-8 | 15.52 | 62.82 |'
- en: '| 8-16 | 15.78 | 62.49 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 8-16 | 15.78 | 62.49 |'
- en: '| 16-24 | 15.98 | 61.67 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 16-24 | 15.98 | 61.67 |'
- en: '| BLOOM-3b | 0-10 | 17.65 | 60.71 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 0-10 | 17.65 | 60.71 |'
- en: '| 10-20 | 17.88 | 60.24 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 10-20 | 17.88 | 60.24 |'
- en: '| 20-30 | 17.94 | 60.37 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 20-30 | 17.94 | 60.37 |'
- en: 'Adopting adaptive mixed-precisions in conjunction with a heterogeneous pipeline
    model serving poses new challenges. Quantization bit (precision) selection must
    be considered jointly with layer partition, as the same quantized kernel can perform
    differently on different GPUs, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. Heterogenous
    Model Parallelization ‣ 2\. BackGround and Motivation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")
    and Fig. [5](#S2.F5 "Figure 5 ‣ 2.4\. Quantization in LLM ‣ 2\. BackGround and
    Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition
    and Adaptive Quantization"). For example, T4 supports fast INT8 due to its tensor
    core, making the execution time of the 8-bit layer comparable to FP16, while V100’s
    INT8 implementation always incurs longer latency than FP16\. Other factors such
    as micro-batch size, prompt length, and token generation number also affect the
    kernel speed and pipeline bubble in prefill and decode phases. To produce an optimized
    inference execution plan, we should take into account all these factors, which
    results in a complex problem with a very large solution space.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '结合自适应混合精度和异构流水线模型服务带来了新的挑战。量化比特（精度）选择必须与层分区联合考虑，因为相同量化内核在不同GPU上表现不同，如图[3](#S2.F3
    "Figure 3 ‣ 2.2\. Heterogenous Model Parallelization ‣ 2\. BackGround and Motivation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")和图[5](#S2.F5 "Figure 5 ‣ 2.4\. Quantization in LLM ‣ 2\.
    BackGround and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization")所示。例如，由于其张量核心，T4支持快速INT8，使得8位层的执行时间可与FP16媲美，而V100的INT8实现总是比FP16有更长的延迟。其他因素如微批量大小、提示长度和令牌生成数量也会影响内核速度和预填充与解码阶段的流水线气泡。为了生成优化的推理执行计划，我们应该考虑所有这些因素，这导致了一个具有非常大解决空间的复杂问题。'
- en: 'First, determining the optimal inference execution plan requires an accurate
    estimation of memory and latency across devices under different precisions. Profiling
    every possible combination of precision, GPU type, and input shape for all partition
    cases would be very time-consuming. An efficient cost model is needed to reduce
    the overhead. Second, different layers in an LLM may exhibit different sensitivities
    to quantization, in terms of model performance impact, when quantized to the same
    bit. Table [1](#S2.T1 "Table 1 ‣ 2.5\. Challenges ‣ 2\. BackGround and Motivation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization") shows that selecting different layers of LLMs for quantization
    can render different model qualities. This finding highlights the importance of
    identifying a suitable layer quantization sensitivity indicator to guide bits
    selection, achieving the goal of reducing memory waste and promoting model quality
    simultaneously. Last, due to the large solution space of our joint decision-making
    problem, offline search for optimal solutions can still be time-consuming. An
    efficient algorithm is in need to effectively prune the solution space.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，确定最佳推理执行计划需要准确估计不同精度下设备间的内存和延迟。对所有分区情况进行每一种精度、GPU类型和输入形状的组合分析会非常耗时。因此，需要一个高效的成本模型来减少开销。其次，LLM中的不同层在量化时对模型性能的影响可能不同，如同一比特量化下的敏感性不同。表[1](#S2.T1
    "Table 1 ‣ 2.5\. Challenges ‣ 2\. BackGround and Motivation ‣ LLM-PQ: Serving
    LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")显示，选择LLM的不同层进行量化可以呈现不同的模型质量。这一发现突出了识别合适的层量化敏感性指标以指导比特选择的重要性，从而实现减少内存浪费和提高模型质量的双重目标。最后，由于我们联合决策问题的解决空间很大，离线搜索最佳解决方案仍然可能耗时。需要一个高效的算法来有效地修剪解决空间。'
- en: We design LLM-PQ to handle all these challenges and achieve significant performance
    gains of LLM serving on heterogeneous clusters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了LLM-PQ来应对所有这些挑战，并在异构集群上实现LLM服务的显著性能提升。
- en: 3\. LLM-PQ Overview
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. LLM-PQ概述
- en: '![Refer to caption](img/6dffd8b798ffa2c8358aa9b45f8d1274.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6dffd8b798ffa2c8358aa9b45f8d1274.png)'
- en: Figure 6\. LLM-PQ Overview
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. LLM-PQ概述
- en: 'LLM-PQ includes an offline assigner and a distributed model inference runtime.
    A system overview is given in Fig. [6](#S3.F6 "Figure 6 ‣ 3\. LLM-PQ Overview
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM-PQ包括一个离线分配器和一个分布式模型推理运行时。系统概述见图[6](#S3.F6 "Figure 6 ‣ 3\. LLM-PQ Overview
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")。'
- en: 'The offline assigner makes optimized decisions on model layer partition, micro-batch
    sizing, and quantization bit assignment to each layer. It collects user inputs
    including the pre-trained LLM, devices and their resource configurations in the
    heterogeneous cluster, precision candidates, query workload characteristics (the
    prompt length, token generation length, and batch size), and a ‘quality scalar’
    that represents user’s level of concern for mode quality (Sec. [4.3](#S4.SS3 "4.3\.
    Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters
    with Phase-Aware Partition and Adaptive Quantization")). The cost models include:
    (i) an analytical memory model which takes model meta-information such as hidden
    space size and decoder layer number as input and predicts the GPU memory occupation
    for a model shard with its mixed-precision plan; (ii) a latency cost model, which
    predicts the execution latency of a model shard based on inference latency samples
    of a single decoder layer collected by the profiler on different GPUs. The Indicator
    Generator is responsible for producing an indicator that quantifies the model
    performance perturbation introduced by a quantized layer under a specific bit.
    The optimizer derives the bit assignment, layer partition, and micro-batch sizing
    using the indicator and the cost models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '离线分配器对模型层分区、微批量大小和每层量化位分配做出优化决策。它收集用户输入，包括预训练的 LLM、设备及其在异构集群中的资源配置、精度候选、查询工作负载特征（提示长度、标记生成长度和批量大小）以及表示用户对模式质量关注程度的“质量标量”（第
    [4.3](#S4.SS3 "4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上服务 LLM 的阶段感知分区和自适应量化") 节）。成本模型包括：（i）分析内存模型，它以模型元信息（如隐藏空间大小和解码器层数）作为输入，预测具有混合精度计划的模型分片的
    GPU 内存占用；（ii）延迟成本模型，它基于在不同 GPU 上由分析器收集的单个解码器层的推理延迟样本，预测模型分片的执行延迟。指标生成器负责生成一个量化指标，该指标量化了特定位下量化层引入的模型性能扰动。优化器使用指标和成本模型来推导位分配、层分区和微批量大小。'
- en: The distributed runtime executes the plans generated by the assigner and conducts
    LLM generative inference. The master engine handles preprocessing and postprocessing
    for token generation, such as embedding lookup and process logits into a predicted
    token, and micro-batch sizing for different generation phases. Each worker process
    is responsible for one pipeline stage and is located on a different GPU.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式运行时执行分配器生成的计划，并进行 LLM 生成推理。主引擎处理标记生成的预处理和后处理，如嵌入查找和将逻辑处理为预测标记，以及不同生成阶段的微批量大小。每个工作进程负责一个管道阶段，并位于不同的
    GPU 上。
- en: 4\. Assigner Design
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 分配器设计
- en: Table 2\. Notation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 符号
- en: '| $h_{1}$ | Hidden dimension of 2nd MLP layer |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| $h_{1}$ | 第二层 MLP 的隐藏维度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $v$ | Prompt length |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| $v$ | 提示长度 |'
- en: '| $t$ | Bitwidth of the current layer |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $t$ | 当前层的位宽 |'
- en: '| $d_{t}$ | Dimension of position embedding |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| $d_{t}$ | 位置嵌入的维度 |'
- en: '| $vocab_{s}$ | Max position embeddings |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $vocab_{s}$ | 最大位置嵌入 |'
- en: 4.1\. Cost Model
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 成本模型
- en: Memory Cost Model. Memory is a first-class citizen in LLM serving systems. The
    peak memory usage of pipeline LLM serving is largely due to the model weights,
    the KV cache for all requests, and the peak temporary memory required by the model
    layers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 内存成本模型。内存在 LLM 服务系统中是一个重要的因素。管道 LLM 服务的峰值内存使用主要由模型权重、所有请求的 KV 缓存以及模型层所需的峰值临时内存组成。
- en: 'Weight Storage. The model weight storage is dominated by embedding weights,
    projections convert the hidden dimension into the word embedding dimension at
    the model’s head and tail, and linear weights inside the decoder layers. The embedding
    weights consist of (1) token embeddings: $vocab_{s}\times d_{t}$.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 权重存储。模型权重存储主要由嵌入权重主导，投影将隐藏维度转换为模型头部和尾部的词嵌入维度，以及解码器层内的线性权重。嵌入权重包括（1）标记嵌入：$vocab_{s}\times
    d_{t}$。
- en: 'For decoder layers, only linear and layer norm layers contribute to memory
    consumption. For self-attention, the parameters consist of (1) QKV, OUT: $h_{1}^{2}$.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '对于解码器层，只有线性层和层归一化层会影响内存消耗。对于自注意力，参数包括（1）QKV，OUT: $h_{1}^{2}$。'
- en: KV Storage Modeling. Like in other frameworks ([FTransformer,](#bib.bib25) ),
    LLM-PQ reserves the KV cache with a size of maximum sentence length, combining
    the maximum prompt length $s$ is the bitwidth used to represent each element in
    the KV cache.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: KV 存储建模。与其他框架（[FTransformer](#bib.bib25)）类似，LLM-PQ 保留了最大句子长度的 KV 缓存，其中最大提示长度
    $s$ 是表示 KV 缓存中每个元素的位宽。
- en: Peak Temporary Memory. Temporary memory required by operators depends on many
    factors including precision, kernel implementation, and the cache allocator mechanism
    of the DNN framework. We consider a worst-case scenario in evaluating the peak
    memory required by all involved operators inside the embedding layer and one decoder
    layer in both prefill and decode phases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 峰值临时内存。操作符所需的临时内存取决于许多因素，包括精度、内核实现以及DNN框架的缓存分配机制。我们在评估预填充和解码阶段中嵌入层和一个解码器层的所有涉及操作符所需的峰值内存时，考虑了最坏情况。
- en: Latency Cost Model. Computation intensity varies across the prefill and decode
    phases. For example, NVIDIA V100 GPU has an arithmetic intensity of 139 (125TFLOPS
    / 900 GB/s); the arithmetic intensity during the decode phase of inference over
    OPT-175b and 30b models for a batch size of 32 and prompt length of 512 is 48
    and 43, respectively. On the other hand, execution of the prefill phase on the
    models incurs arithmetic intensity of 9553 and 6354, respectively, showing that
    the prefill phase is more computation intensive.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟成本模型。计算强度在预填充和解码阶段有所不同。例如，NVIDIA V100 GPU 的算术强度为139（125TFLOPS / 900 GB/s）；在OPT-175b和30b模型的解码阶段，对于批量大小为32和提示长度为512的推理，算术强度分别为48和43。另一方面，模型在预填充阶段的执行会产生分别为9553和6354的算术强度，显示出预填充阶段的计算强度更高。
- en: Therefore, we model the execution time of the prefill phase as a function of
    FLOPs, based on $v,s,vs$. We profile the execution time of each phase on one decoder
    layer under different precisions with common prompt lengths and batch sizes. We
    then use interpolation among the sample points to obtain a linear regression model
    for the execution time of one decoder layer in each phase. We choose linear regression
    because, in LLM serving, GEMM takes more than 80% latency ([du2022energonai,](#bib.bib12)
    ) and is either FLOPs and MOPs related, while the other operators scaled with
    MOPs, thus workload can be shaped and scaled by the previous parameters. The latency
    of a model shard can be obtained by summing up the latencies of all involved decoder
    layers with respect to their precisions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将预填充阶段的执行时间建模为FLOPs的函数，基于$v,s,vs$。我们在不同精度下对单个解码器层的每个阶段执行时间进行剖析，使用常见的提示长度和批量大小。然后，我们通过在样本点之间进行插值，得到每个阶段一个解码器层的执行时间的线性回归模型。我们选择线性回归是因为在LLM服务中，GEMM占据了超过80%的延迟（[du2022energonai](#bib.bib12)），且与FLOPs和MOPs相关，而其他操作则与MOPs相关，因此工作负载可以通过先前的参数来塑造和缩放。模型分片的延迟可以通过对所有涉及的解码器层的延迟进行汇总来获得，具体取决于它们的精度。
- en: 4.2\. Indicator of Model Perturbation by Quantization
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 量化引起的模型扰动指示
- en: 'We build performance indicators for low-precision weight-only kernels. INT8
    kernel in this paper incurs little performance dradation ([dettmers2022llmint8,](#bib.bib6)
    ), we take the same indicator format with weight-only kernels for simplicity.
    State-of-the-art weight-only quantization of LLMs focuses on linear operators
    and  ([frantar2023gptq,](#bib.bib14) ; [lin2023awq,](#bib.bib21) ; [dettmers2023spqr,](#bib.bib8)
    ) typically target the following objective:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为低精度权重单核建立了性能指标。本文中的INT8内核几乎没有性能退化（[dettmers2022llmint8](#bib.bib6)），为了简化，我们采用了与权重单核相同的指标格式。最先进的LLM权重量化专注于线性操作符（[frantar2023gptq](#bib.bib14)；[lin2023awq](#bib.bib21)；[dettmers2023spqr](#bib.bib8)）通常瞄准以下目标：
- en: '| (1) |  | $\tiny\mathbf{{Q}^{*}}=\operatorname*{arg\,min}_{Q}\mathcal{L}(\tilde{\mathbf{W}}),\quad\quad\quad\mathcal{L}(\tilde{\mathbf{W}})=\&#124;\mathbf{W}\mathbf{X}-\tilde{\mathbf{W}}\mathbf{X}\&#124;_{2}^{2}$
    |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\tiny\mathbf{{Q}^{*}}=\operatorname*{arg\,min}_{Q}\mathcal{L}(\tilde{\mathbf{W}}),\quad\quad\quad\mathcal{L}(\tilde{\mathbf{W}})=\&#124;\mathbf{W}\mathbf{X}-\tilde{\mathbf{W}}\mathbf{X}\&#124;_{2}^{2}$
    |  |'
- en: Here $\mathcal{L}$) with respect to different precisions, incurring large computation
    overhead.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$\mathcal{L}$) 针对不同精度，引发了较大的计算开销。
- en: We adopt a different approach to describe a layer’s sensitivity upon quantization.
    One key observation is that the quantization error originates from the $Round$.
    We consider the round variance of quantization for two widely applied rounding
    methods, i.e., deterministic and stochastic ([wan2023adaptive,](#bib.bib34) ),
    and derive an upper bound of the output variance introduced by quantization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了一种不同的方法来描述层对量化的敏感性。一个关键观察是量化误差来源于$Round$。我们考虑了两种广泛应用的舍入方法的量化误差，即确定性和随机性（[wan2023adaptive](#bib.bib34)），并推导出量化引入的输出方差的上界。
- en: Theorem 1.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1。
- en: 'The variance of a linear operator’s output after weight-only quantization using
    stochastic or deterministic rounding is:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机或确定性四舍五入的权重仅量化后线性算子输出的方差为：
- en: '| (2) |  | $\tiny Var[\tilde{\mathbf{W}}\mathbf{X}]=\left\{\begin{aligned}
    &amp;Var[\mathbf{W}\mathbf{X}]+D_{\mathbf{W}}S_{\mathbf{W}}^{2}\frac{1}{4}Var[\mathbf{X}],&amp;\mbox{\small
    Deterministic}\\ &amp;Var[\mathbf{W}\mathbf{X}]+D_{\mathbf{W}}S_{\mathbf{W}}^{2}\frac{1}{6}(\mathbb{E}[\mathbf{X}]^{2}+Var[\mathbf{X}]),&amp;\mbox{\small
    Stochastic}\end{aligned}\right.$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\tiny Var[\tilde{\mathbf{W}}\mathbf{X}]=\left\{\begin{aligned}
    &amp;Var[\mathbf{W}\mathbf{X}]+D_{\mathbf{W}}S_{\mathbf{W}}^{2}\frac{1}{4}Var[\mathbf{X}],&amp;\mbox{\small
    确定性}\\ &amp;Var[\mathbf{W}\mathbf{X}]+D_{\mathbf{W}}S_{\mathbf{W}}^{2}\frac{1}{6}(\mathbb{E}[\mathbf{X}]^{2}+Var[\mathbf{X}]),&amp;\mbox{\small
    随机性}\end{aligned}\right.$ |  |'
- en: where $D_{\mathbf{W}}$ is the scaling factor.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{\mathbf{W}}$ 是缩放因子。
- en: The theorem shows that the variance introduced by quantization in each linear
    operator is proportional to the dimension and scaling factor of the model weights.
    The scaling factor $S_{\mathbf{W}}$.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 定理表明，量化引入的方差在每个线性算子中与模型权重的维度和缩放因子成正比。缩放因子 $S_{\mathbf{W}}$。
- en: Proposition 2 (Variance Indicator).
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 2（方差指标）。
- en: We measure the quantization sensitivity of a decoder layer $i$ using the estimated
    quantization variance of the layer’s output, i.e.,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过估计层输出的量化方差来测量解码器层 $i$ 的量化敏感性，即，
- en: '| (3) |  | $\tiny\omega_{i,b}=\sum_{o}^{O_{i}}D_{\mathbf{W}_{o}}(S_{\mathbf{W}_{o}}(b_{i}))^{2}G(\mathbf{X}_{o})$
    |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\tiny\omega_{i,b}=\sum_{o}^{O_{i}}D_{\mathbf{W}_{o}}(S_{\mathbf{W}_{o}}(b_{i}))^{2}G(\mathbf{X}_{o})$
    |  |'
- en: where $O_{i}$ for stochastic, respectively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机情况，$O_{i}$ 分别。
- en: The variance indicator $\omega$). The missing proofs can be found in supplementary
    materials.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 方差指标 $\omega$）。缺失的证明可以在补充材料中找到。
- en: 4.3\. Optimizer
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 优化器
- en: 'We present an iterative algorithm (Algorithm [1](#alg1 "Algorithm 1 ‣ 4.3\.
    Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters
    with Phase-Aware Partition and Adaptive Quantization")) to decide the quantization
    bitwidth for each decoder layer, micro-batch sizes, and LLM model partition and
    on each device, to strike the best balance between inference latency and model
    quality degradation. The algorithm explores potential device topology orderings
    and micro-batch sizes for prefill and decode phases; given a device topology ordering
    and micro-batch sizes, we solve an integer linear program (ILP) to determine the
    most suitable bitwidth assignment and layer partition among the devices.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出了一种迭代算法（算法 [1](#alg1 "算法 1 ‣ 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上通过相位感知分区和自适应量化服务LLM")），用于确定每个解码器层的量化位宽、微批量大小以及
    LLM 模型在每个设备上的分区，以在推理延迟和模型质量降级之间取得最佳平衡。该算法探索预填充和解码阶段的潜在设备拓扑排序和微批量大小；给定设备拓扑排序和微批量大小，我们解决一个整数线性规划（ILP）问题，以确定最合适的位宽分配和设备之间的层分区。'
- en: Bidwidth Assignment and Layer Partition.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 位宽分配和层分区。
- en: We use binary variable $z_{i,j,b}$.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用二进制变量 $z_{i,j,b}$。
- en: $T_{max}^{pre}$ trading off more model quality over inference acceleration.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: $T_{max}^{pre}$ 在推理加速和更多模型质量之间进行权衡。
- en: The first parenthesized term in the objective (LABEL:eq:objective) represents
    the end-to-end serving latency for a batch’s token generation. In a pipeline-parallel
    serving system, the latency of serving a batch is the execution time of all pipeline
    stages plus $\mu-1$ tokens. The second term in the objective corresponds to overall
    model quality degradation (measured by our variance indicator).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目标（LABEL:eq:objective）中第一个带括号的项表示批量的标记生成的端到端服务延迟。在管道并行服务系统中，服务一个批量的延迟是所有管道阶段的执行时间加上
    $\mu-1$ 个标记。目标中的第二项对应于整体模型质量降级（由我们的方差指标测量）。
- en: $T_{max}^{pre},T_{max}^{dec},T_{pre}$ and its successor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: $T_{max}^{pre},T_{max}^{dec},T_{pre}$ 及其继任者。
- en: '| (4) |  | $\displaystyle\min_{\mathbb{Z}}$ |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\min_{\mathbb{Z}}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '| (5) |  | $1$2 |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $1$2 |  |'
- en: '| (6) |  | $1$2 |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $1$2 |  |'
- en: '| (7) |  | $1$2 |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $1$2 |  |'
- en: '| (8) |  | $\displaystyle T_{pre}=\sum_{j}^{N}T_{pre,j},\quad T_{dec}=\sum_{j}^{N}T_{dec,j}$
    |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle T_{pre}=\sum_{j}^{N}T_{pre,j},\quad T_{dec}=\sum_{j}^{N}T_{dec,j}$
    |  |'
- en: '| (9) |  | $\displaystyle\sum_{j=1}^{N}\sum_{b\in BITs}z_{i,j,b}=1,\quad\forall
    i=1,...,L$ |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle\sum_{j=1}^{N}\sum_{b\in BITs}z_{i,j,b}=1,\quad\forall
    i=1,...,L$ |  |'
- en: '| (10) |  | $\displaystyle\sum_{j=1}^{N}z_{i,j,b}=y_{i,b},\quad\forall i=1,...,L,b\in
    BITs$ |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle\sum_{j=1}^{N}z_{i,j,b}=y_{i,b},\quad\forall i=1,...,L,b\in
    BITs$ |  |'
- en: '| (11) |  | $\displaystyle\sum_{b\in BITs}z_{i,j,b}=u_{i,j},\quad\forall i=1,...,L,j=1,...,N,$
    |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle\sum_{b\in BITs}z_{i,j,b}=u_{i,j},\quad\forall i=1,...,L,j=1,...,N,$
    |  |'
- en: '| (12) |  | $1$2 |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $1$2 |  |'
- en: '| (13) |  | $\displaystyle\sum_{i=1}^{L}\sum_{b\in BITs}z_{i,1,b}M_{i,b}^{s+n}+M_{emb}\leq
    M_{1}$ |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle\sum_{i=1}^{L}\sum_{b\in BITs}z_{i,1,b}M_{i,b}^{s+n}+M_{emb}\leq
    M_{1}$ |  |'
- en: '| (14) |  | $\displaystyle y_{i,b},u_{i,j},z_{i,j,b}\in{0,1},\quad\forall i=1,...,L,j=1,...,N,b\in
    BITs$ |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\displaystyle y_{i,b},u_{i,j},z_{i,j,b}\in{0,1},\quad\forall i=1,...,L,j=1,...,N,b\in
    BITs$ |  |'
- en: '| (15) |  | $\displaystyle u_{0,0}=1,u_{L,N}=1,$ |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle u_{0,0}=1,u_{L,N}=1,$ |  |'
- en: '| (16) |  | $\displaystyle u_{i,j}+u_{i-1,k}\leq 1,\forall i=2,...,L,j=1,...,N-1,k=j,...,N-1$
    |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\displaystyle u_{i,j}+u_{i-1,k}\leq 1,\forall i=2,...,L,j=1,...,N-1,k=j,...,N-1$
    |  |'
- en: 'Constraints ([9](#S4.E9 "In 4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ:
    Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive
    Quantization")) - ([11](#S4.E11 "In 4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ:
    Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive
    Quantization")) ensure that only one bitwidth is assigned to a given layer and
    each layer can only be placed on a single device. Constraints ([12](#S4.E12 "In
    4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters
    with Phase-Aware Partition and Adaptive Quantization"))-([13](#S4.E13 "In 4.3\.
    Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters
    with Phase-Aware Partition and Adaptive Quantization")) guarantee that memory
    consumption on each device $j$.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '约束条件 ([9](#S4.E9 "在 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上提供具有阶段感知分区和自适应量化的LLM"))
    - ([11](#S4.E11 "在 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上提供具有阶段感知分区和自适应量化的LLM"))
    确保每一层仅分配一个位宽，每一层只能放置在一个设备上。约束条件 ([12](#S4.E12 "在 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ:
    在异构集群上提供具有阶段感知分区和自适应量化的LLM"))-([13](#S4.E13 "在 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ:
    在异构集群上提供具有阶段感知分区和自适应量化的LLM")) 确保每个设备 $j$ 的内存消耗。'
- en: We solve the ILP using an off-the-shelf solver GUROBI ([gurobi,](#bib.bib16)
    ).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用现成的求解器 GUROBI ([gurobi,](#bib.bib16)) 来求解 ILP。
- en: Algorithm 1 Best Inference Execution Plan
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 最佳推理执行计划
- en: 0:LLM Model $A$
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 0:LLM 模型 $A$
- en: Device Topology Ordering and Microbatch Sizing. We enumerate all possible combinations
    of device topology ordering ($GetDeviceOrder$. Given each combination, we solve
    the ILP to obtain the corresponding best quantization bitwidth and layer partitions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 设备拓扑排序和微批处理大小。我们枚举所有可能的设备拓扑排序组合（`GetDeviceOrder`）。给定每种组合后，我们求解ILP以获得相应的最佳量化位宽和层划分。
- en: 'Complexity of Algorithm [1](#alg1 "Algorithm 1 ‣ 4.3\. Optimizer ‣ 4\. Assigner
    Design ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition
    and Adaptive Quantization"). The solution space size of ILP problem (LABEL:eq:objective)
    is ${\frac{L!}{N!(L-N)!}(|Bits|)}^{L}$. This may raise concerns for scalability.
    We propose several practical optimizations to expedite it.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '算法复杂度 [1](#alg1 "算法 1 ‣ 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上提供具有阶段感知分区和自适应量化的LLM")。ILP问题（LABEL:eq:objective）的解空间大小为
    ${\frac{L!}{N!(L-N)!}(|Bits|)}^{L}$。这可能引发扩展性的担忧。我们提出了若干实用优化措施来加速这一过程。'
- en: 'Optimization #1: Pruning. As discussed in Sec. [4.1](#S4.SS1 "4.1\. Cost Model
    ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization"), prefill phase is compute-bound, while the
    decode phase is memory-bound. GPUs have higher computation capacity than memory
    bandwidth. Increasing the micro-batch size during the decode phase improves efficiency,
    but excessively large sizes waste computation capabilities. Evenly partitioning
    the global batch size across pipeline stages optimizes performance. In the prefill
    phase, a smaller batch size reduces pipeline bubbles, but extremely small sizes
    are inefficient. Thus, we enumerate prefill micro-batch size within $[1,\xi]$.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '优化 #1: 剪枝。如在第 [4.1](#S4.SS1 "4.1\. 成本模型 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上提供具有阶段感知分区和自适应量化的LLM")
    节中讨论的那样，预填充阶段是计算密集型的，而解码阶段是内存密集型的。GPU 的计算能力高于内存带宽。在解码阶段增加微批处理大小可以提高效率，但过大的尺寸会浪费计算能力。将全局批处理大小均匀分配到管道阶段可以优化性能。在预填充阶段，较小的批处理大小可以减少管道气泡，但极小的尺寸效率低下。因此，我们枚举预填充微批处理大小范围在
    $[1,\xi]$ 内。'
- en: 'Optimization #2: Grouping. Grouping multiple layers together and deciding group
    placement and bitwidth selection can reduce the solution space exponentially.
    For models with a parameter size smaller than 30b, layer grouping is not necessary
    as $L$ is small. For models larger than 30b, grouping layers in sets of 2 is typically
    sufficient.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '优化 #2: 分组。将多个层分组在一起并决定组的放置和比特宽度选择可以指数级地减少解决空间。对于参数大小小于 30b 的模型，由于 $L$ 较小，不需要进行层分组。对于大于
    30b 的模型，将层分为 2 组通常足够。'
- en: 'Optimization #3: Heuristic to solve ILP (LABEL:eq:objective).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '优化 #3: 解决 ILP 的启发式方法 (LABEL:eq:objective)。'
- en: 'Algorithm 2 Bitwidth Transfer. Replacing ILP in Algo. [1](#alg1 "Algorithm
    1 ‣ 4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 2 比特宽度转换。替换算法中的 ILP [1](#alg1 "Algorithm 1 ‣ 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ:
    在异构集群上提供 LLM 的相位感知分区和自适应量化")'
- en: 0:  $G_{i}$
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 0:  $G_{i}$
- en: 'GPUs exhibit varying computation capacities, leading to different execution
    performances for layers while their memory occupation remains fixed. This characteristic
    allows for precision conversion and layer partition alteration between stages
    according to transformation rules $\mathcal{C}$. For example, (4, 8, 2) facilitate
    the replacement of one 8-bit layer from the pioneer with 2 * 4-bit layers from
    the straggler. Such transformations increase precision or reduce layer count to
    accelerate the slowest stage. Leveraging this observation, we propose a heuristic
    approach called bitwidth transfer, detailed in Algorithm [2](#alg2 "Algorithm
    2 ‣ 4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization"), for solving the
    ILP problem (LABEL:eq:objective). Initially, we remove the latency objective from
    the ILP and solve it under reduced constraints, noted as adabits(comparison in
    Sec. [6.9](#S6.SS9 "6.9\. Comparison with Pure Adaptive Quantization ‣ 6\. Evaluation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization"))(lines 1-3). We generate potential transformations (line
    3), identify the slowest (straggler) and other stages (line 5), and apply possible
    transformations to improve the target objective value iteratively (lines 6-16).
    The heuristic is effective in most cases, particularly when KV size does not dominate
    memory occupation. Further discussion on its usage is provided in Sec. [6.7](#S6.SS7
    "6.7\. Approaches Expediting Optimizer Algorithm ‣ 6\. Evaluation ‣ LLM-PQ: Serving
    LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPU 显示出不同的计算能力，从而导致层的执行性能不同，同时其内存占用保持不变。这一特性允许根据变换规则 $\mathcal{C}$ 在阶段之间进行精度转换和层分区变更。例如，(4,
    8, 2) 可以将先锋中的一个 8 位层替换为滞后的 2 * 4 位层。这种转换可以增加精度或减少层数，以加速最慢的阶段。利用这一观察，我们提出了一种称为比特宽度转换的启发式方法，详细说明见算法
    [2](#alg2 "Algorithm 2 ‣ 4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上提供 LLM 的相位感知分区和自适应量化")，用于解决
    ILP 问题 (LABEL:eq:objective)。最初，我们从 ILP 中移除延迟目标，并在减少的约束条件下进行求解，记作 adabits (在 Sec.
    [6.9](#S6.SS9 "6.9\. 与纯自适应量化的比较 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供 LLM 的相位感知分区和自适应量化"))
    (第 1-3 行)。我们生成潜在的转换 (第 3 行)，识别最慢的（滞后）和其他阶段 (第 5 行)，并迭代地应用可能的转换以改进目标值 (第 6-16 行)。这一启发式方法在大多数情况下有效，特别是当
    KV 大小不占主导内存时。有关其使用的进一步讨论见 Sec. [6.7](#S6.SS7 "6.7\. 加速优化器算法的方法 ‣ 6\. 评估 ‣ LLM-PQ:
    在异构集群上提供 LLM 的相位感知分区和自适应量化")。'
- en: 5\. Implementation
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 实现
- en: We have implemented LLM-PQ using PyTorch-2.0.0 ([Paszke_PyTorch_An_Imperative_2019,](#bib.bib27)
    ) with over 6000 LoCs (1355 LoCs for Assigner). We extend models on HuggingFace ([HF_transformers,](#bib.bib35)
    ) (transformers-4.28.0) to support pre-allocated KV cache and adaptive quantization.
    We implement pipeline serving and a thread-safe micro-batch manager on top of
    the heterogeneous pipeline in  ([hu2021pipeline,](#bib.bib17) ) with asynchronous
    communication among stages.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 PyTorch-2.0.0 实现了 LLM-PQ ([Paszke_PyTorch_An_Imperative_2019,](#bib.bib27))，代码量超过
    6000 行（1355 行用于分配器）。我们扩展了 HuggingFace ([HF_transformers,](#bib.bib35)) (transformers-4.28.0)
    上的模型，以支持预分配 KV 缓存和自适应量化。我们在异构管道上实现了管道服务和线程安全的微批处理管理器 ([hu2021pipeline,](#bib.bib17))，各阶段之间使用异步通信。
- en: On-The-Fly Quantizer To optimize the utilization of low-caliber GPUs with smaller
    DRAM that may frequently experience precision changes, we have developed a specialized
    and efficient plugin for on-the-fly quantized model loading. In this approach,
    we have decoupled the integrated model weight into module-level weights. During
    runtime, we determine the granularity of processed weights by overlapping the
    disk-to-CPU weight loading time with the on-GPU model quantization and CPU-to-GPU
    memory copy. This results in a significant reduction in DRAM required for model
    loading but also improves recovery speed from the possible failure.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 实时量化器 为了优化低功率GPU和小型DRAM的使用，这些设备可能会频繁经历精度变化，我们开发了一个专门的高效插件，用于实时量化模型加载。在这种方法中，我们将集成模型权重解耦为模块级权重。在运行时，我们通过将磁盘到CPU的权重加载时间与GPU上的模型量化和CPU到GPU的内存复制重叠，来确定处理权重的粒度。这导致了模型加载所需的DRAM显著减少，同时提高了可能失败后的恢复速度。
- en: API and Commands. LLM-PQ provides an entry file for the plan generation for
    different heterogeneous devices.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: API和命令 LLM-PQ提供了一个入口文件，用于为不同的异构设备生成计划。
- en: 'llmpq-algo  \--model-name  ${model_name}  --model_size  ${model_size}  \--device_names  "${device_names[@]}"  \--device_numbers  "${device_numbers[@]}"  \--omega_file  $omega_file  \  #  indicator  file--global_bz  $batch_size  --s  $s  --n  $n  \  #  workload--theta  $theta  \  #  user  scalar--<group  $group_size>  <--shaq-efficient>  \  #  faster--<fit/use_profiler_prediction>  #  use  cost  model  or  profiled  result'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'llmpq-algo  \--model-name  ${model_name}  --model_size  ${model_size}  \--device_names  "${device_names[@]}"  \--device_numbers  "${device_numbers[@]}"  \--omega_file  $omega_file  \  #  指示器文件--global_bz  $batch_size  --s  $s  --n  $n  \  #  工作负载--theta  $theta  \  #  用户标量--<group  $group_size>  <--shaq-efficient>  \  #  更快--<fit/use_profiler_prediction>  #  使用成本模型或分析结果'
- en: The output strategy can be launched directly. If the same GPU type is located
    on the same node, other configurations, such as ranks, will be derived automatically
    and registered to the distributed runtime. Alternatively, distributedconfigs same
    as those in PyTorch can be used to launch the strategy, but the noauto flag must
    be specified.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出策略可以直接启动。如果同一节点上有相同类型的GPU，其他配置，如排名，将自动推导并注册到分布式运行时。或者，可以使用与PyTorch相同的分布式配置来启动策略，但必须指定noauto标志。
- en: llmpq-dist  --strat_file_name  $strategy_file_path  \<--master_addr  --master_port>/<distribtedconfigs  --no_auto>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: llmpq-dist  --strat_file_name  $strategy_file_path  \<--master_addr  --master_port>/<distribtedconfigs  --no_auto>
- en: 6\. Evaluation
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 评估
- en: 6.1\. Experimental Setup
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实验设置
- en: 'Models & Precisions. We run BLOOM ([scao2022bloom,](#bib.bib31) ) and OPT ([Zhang2022OPTOP,](#bib.bib40)
    ) model families, focusing on middle- and large-sized models, specifically OPT-13b,
    30b, 66b, and BLOOM-176b. We evaluate candidate precisions: $BITs=\{3,4,8,16\}$.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与精度 我们运行BLOOM ([scao2022bloom](#bib.bib31)) 和OPT ([Zhang2022OPTOP](#bib.bib40))
    模型系列，重点关注中型和大型模型，特别是OPT-13b、30b、66b和BLOOM-176b。我们评估候选精度：$BITs=\{3,4,8,16\}$。
- en: 'Baselines. We compare LLM-PQ with three baselines: (1) PipeEdge, where we apply
    uniform quantization and use PipeEdge ([hu2021pipeline,](#bib.bib17) ) for heterogeneous
    layer partition. (2) Uniform, which uses uniform quantization, evenly partitions
    the model layers among devices and decides micro-batch sizes that minimize the
    inference latency, mimicking the policy of existing serving systems such as HF-Transformers ([HF_transformers,](#bib.bib35)
    ) and Deepspeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1) ). (3) Offloading, where
    we adopt CPU and disk swapping in FlexGen ([flexgen,](#bib.bib32) ) to maximize
    the throughput of token generation for low-calibre GPUs, we adopt even partition
    for this method. For (1)(2), we keep lowering the quantization bitwidth from the
    maximum (i.e., FP16) until the model can fit into the devices or no feasible solutions
    are available. For (1)(3), we use the same micro-batch size for prefill and decode
    phases by partitioning the global batch size by the number of pipeline stages.
    FlexGen is specialized for OPT models and thus has no results on BLOOM models.
    We did not conduct a comparison with vLLM ([vllm,](#bib.bib20) ) as it primarily
    focuses on the online task, and the paged attention mechanism is of no use when
    dealing with fixed token generation numbers. Also, vLLM didn’t support pipeline
    parallelism, making the comparison unfair in our case.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试。我们将 LLM-PQ 与三个基准进行比较：(1) PipeEdge，我们应用均匀量化并使用 PipeEdge ([hu2021pipeline,](#bib.bib17)
    ) 进行异构层划分。(2) Uniform，使用均匀量化，将模型层均匀划分到设备上，并决定微批次大小以最小化推理延迟，模拟现有服务系统的策略，如 HF-Transformers ([HF_transformers,](#bib.bib35)
    ) 和 Deepspeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1) )。(3) Offloading，我们在 FlexGen ([flexgen,](#bib.bib32)
    ) 中采用 CPU 和磁盘交换，以最大化低规格 GPU 的 token 生成吞吐量，为此方法采用均匀划分。对于 (1)(2)，我们不断降低量化位宽，从最大值（即
    FP16）开始，直到模型可以适配设备或没有可行的解决方案。对于 (1)(3)，我们使用相同的微批次大小进行预填充和解码阶段，通过将全局批次大小按管道阶段数划分。FlexGen
    专门用于 OPT 模型，因此对 BLOOM 模型没有结果。我们没有与 vLLM ([vllm,](#bib.bib20) ) 进行比较，因为它主要集中于在线任务，并且分页注意机制在处理固定
    token 生成数量时没有用处。此外，vLLM 不支持管道并行，因此在我们的案例中比较不公平。
- en: Metrics. We evaluate LLM serving performance by (1) token generation throughput,
    (2) end-to-end serving latency of one batch, and (3) model quality, using perplexity
    (PPL) on WikiText2 ([wikitext2,](#bib.bib24) ), Penn Treebank (PTB) ([ptb,](#bib.bib23)
    ) and C4 ([c4,](#bib.bib29) ). The weight calibration data consists of 128 randomly
    selected 2048-token segments from the C4 dataset ([c4,](#bib.bib29) ).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。我们通过（1）token 生成吞吐量，（2）一个批次的端到端服务延迟，以及（3）模型质量来评估 LLM 服务性能，使用 WikiText2 ([wikitext2,](#bib.bib24)
    )、Penn Treebank (PTB) ([ptb,](#bib.bib23) ) 和 C4 ([c4,](#bib.bib29) ) 的困惑度（PPL）。权重校准数据包括从
    C4 数据集 ([c4,](#bib.bib29) ) 随机选择的 128 个 2048-token 片段。
- en: Workload. We use synthetic datasets following the prompt length setup in the
    DeepSpeed paper  ([Aminabadi2022DeepSpeedIE,](#bib.bib1) ), i.e., 128 and 512\.
    By default, we pad input prompts to 512 tokens, use an input batch size of 32,
    and set the number of tokens to be generated to $n=100$. We follow the same setup
    as in ORCA ([orca,](#bib.bib38) ) to never emit the EOS but continue to generate
    tokens until reaching the expected token generation length.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载。我们使用合成数据集，遵循 DeepSpeed 论文中的提示长度设置（[Aminabadi2022DeepSpeedIE,](#bib.bib1)），即
    128 和 512。默认情况下，我们将输入提示填充到 512 个 token，使用的输入批次大小为 32，并将生成的 token 数设置为 $n=100$。我们遵循
    ORCA ([orca,](#bib.bib38) ) 的相同设置，不发出 EOS，而是继续生成 token，直到达到预期的 token 生成长度。
- en: 'Heterogeneous Clusters. Devices/nodes are in our production cluster. We construct
    a number of heterogeneous clusters for model serving (clusters 1-8 in Table [3](#S6.T3
    "Table 3 ‣ 6.1\. Experimental Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")),
    with a mix of common types of GPUs. GPUs of the same type are located on the same
    node, intra-connected with NV-LINK; Clusters 1,2,9,10,11 are on a single node
    and others consist of two nodes. Nodes in Clusters 3,5,8,11 are interconnected
    with 800Gbps Ethernet; 4,6, and 7 with 100Gbps Ethernet. All GPUs are equipped
    with GB/s SSD; Each node is equipped with two CPUs, P100 nodes with Intel Xeon
    CPU E5-2630 v4 2.2GHz, 64G RAM, V100 and A800 with Intel Xeon Gold 6230 2.1GHz,
    128G RAM and 450G RAM, T4 with Intel Xeon Platinum 8260 CPU, 108G RAM, A100-40G
    with AMD EPYC 7H12 64-Core, 256G RAM. OS: Ubuntu 20.04.6 LTS. We also show LLM-PQ’s
    performance on several homogenous clusters (clusters 9-11 in Table [3](#S6.T3
    "Table 3 ‣ 6.1\. Experimental Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '异构集群。设备/节点位于我们的生产集群中。我们为模型服务构建了一些异构集群（见表[3](#S6.T3 "Table 3 ‣ 6.1\. Experimental
    Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")），这些集群混合了不同类型的GPU。相同类型的GPU位于同一节点上，并通过NV-LINK相互连接；集群1、2、9、10、11位于单个节点上，其他集群则由两个节点组成。集群3、5、8、11的节点通过800Gbps以太网互连；集群4、6和7则通过100Gbps以太网互连。所有GPU均配备GB/s
    SSD；每个节点配备两个CPU，P100节点配备Intel Xeon CPU E5-2630 v4 2.2GHz，64G RAM，V100和A800配备Intel
    Xeon Gold 6230 2.1GHz，128G RAM和450G RAM，T4配备Intel Xeon Platinum 8260 CPU，108G
    RAM，A100-40G配备AMD EPYC 7H12 64核，256G RAM。操作系统：Ubuntu 20.04.6 LTS。我们还展示了LLM-PQ在几个同构集群（见表[3](#S6.T3
    "Table 3 ‣ 6.1\. Experimental Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")）上的性能。'
- en: 'Experiment Settings $\theta$, solver setup, and overhead table are provided
    in Appendix [A.2](#A1.SS2 "A.2\. Experiment ‣ Appendix A Appendix ‣ LLM-PQ: Serving
    LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization").
    The model size to run on each cluster is decided such that the total weight size
    of the non-quantized model is comparable to the overall device memory capacity
    in the cluster.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '实验设置$\theta$、求解器设置和开销表见附录[A.2](#A1.SS2 "A.2\. Experiment ‣ Appendix A Appendix
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")。每个集群上运行的模型大小是根据未量化模型的总权重大小与集群的整体设备内存容量相当来决定的。'
- en: Table 3\. Cluster Configurations
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 集群配置
- en: '| Cluster | Devices | Model Size | Cluster | Devices | Model Size |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 集群 | 设备 | 模型大小 | 集群 | 设备 | 模型大小 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 1xV100-32G | 13b | 2 | 1xA100-40G | 13b |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1xV100-32G | 13b | 2 | 1xA100-40G | 13b |'
- en: '| 3 | 3xT4-16G + 1xV100-32G | 30b | 4 | 3xP100-12G + 1xV100-32G | 30b |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3xT4-16G + 1xV100-32G | 30b | 4 | 3xP100-12G + 1xV100-32G | 30b |'
- en: '| 5 | 4xT4-16G + 2xV100-32G | 66b | 6 | 2xV100-32G + 2xA100-40G | 66b |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4xT4-16G + 2xV100-32G | 66b | 6 | 2xV100-32G + 2xA100-40G | 66b |'
- en: '| 7 | 4xV100-32G + 4xA100-40G | 176b | 8 | 4xV100-32G + 2xA800-80G | 176b |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4xV100-32G + 4xA100-40G | 176b | 8 | 4xV100-32G + 2xA800-80G | 176b |'
- en: '| 9 | 4xT4-16G | 30b | 10 | 4xV100-32G | 66b |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 4xT4-16G | 30b | 10 | 4xV100-32G | 66b |'
- en: '| 11 | 4xA800-80G | 176b |  |  |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 4xA800-80G | 176b |  |  |  |'
- en: 6.2\. Fidelity of Cost Models
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 成本模型的精度
- en: 'We evaluate our memory cost model on BLOOM of sizes 560m and 1b7, and OPT of
    13b, 30b, and 66b, with prompt length uniformly sampled between 128 and 512, the
    batch size chosen among 2, 4, and 8, generated token length sampled between 100
    and 200, and randomly generated precision setting from the available bitwidth
    set. We consider the memory consumption of model weights and KV caching here and
    compare the predicted memory usage with those collected from real systems. We
    also create 50 unseen workloads with different precisions, batch sizes (3,5 or
    7), prompt lengths, and past sequence lengths (384 or 768) for each device, evaluate
    our latency cost model on them. Fig. [7](#S6.F7 "Figure 7 ‣ 6.2\. Fidelity of
    Cost Models ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization") shows that the error of the
    memory cost model is almost negligible, and the average error of the latency cost
    model is less than 6%.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 BLOOM 的 560m 和 1b7 以及 OPT 的 13b、30b 和 66b 上评估了我们的内存成本模型，提示长度均匀采样在 128 到
    512 之间，批处理大小从 2、4 和 8 中选择，生成的 token 长度在 100 到 200 之间采样，并随机生成精度设置。我们在此考虑了模型权重和
    KV 缓存的内存消耗，并将预测的内存使用情况与实际系统中收集的结果进行比较。我们还为每个设备创建了 50 个未见过的工作负载，具有不同的精度、批处理大小（3、5
    或 7）、提示长度和过去的序列长度（384 或 768），并在这些工作负载上评估了我们的延迟成本模型。图 [7](#S6.F7 "图 7 ‣ 6.2\. 成本模型的保真度
    ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上服务 LLM 使用阶段感知分区和自适应量化") 显示内存成本模型的误差几乎可以忽略不计，延迟成本模型的平均误差小于
    6%。'
- en: We observed that, during the prefill phase, the cost of observations typically
    increases linearly with the workload. However, it is noteworthy that in the decode
    phase, a notable difference in latency occurs only when a substantial change in
    context length (50-100) is present.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，在预填充阶段，观察的成本通常会随着工作负载的增加而线性增长。然而，在解码阶段，只有在上下文长度（50-100）发生显著变化时，延迟才会出现明显差异。
- en: '![Refer to caption](img/28a659adebcfc0d03741f2c62f88b62f.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28a659adebcfc0d03741f2c62f88b62f.png)'
- en: Figure 7\. Comparison of memory and latency reported by the cost models and
    obtained in real systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 成本模型报告的内存和延迟与实际系统中获得的结果的比较。
- en: 6.3\. Serving in Heterogeneous Clusters
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 异构集群中的服务
- en: Table 4\. Serving performance comparison. The best results are marked in bold.
    The missing results are due to OOM. The $\times$ is derived comparing with the
    PipeEdge baseline.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 服务性能比较。最佳结果以粗体标记。缺失的结果是由于内存溢出（OOM）。$\times$ 是与 PipeEdge 基准相比得出的。
- en: '| Model Size | Cluster | Model | Scheme | PPL | Latency (s) | Throughput (Token/s)
    | Model Size | Cluster | Model | Scheme | PPL | Latency (s) | Throughput (Token/s)
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | 集群 | 模型 | 方案 | PPL | 延迟（秒） | 吞吐量（Token/s） | 模型大小 | 集群 | 模型 | 方案 |
    PPL | 延迟（秒） | 吞吐量（Token/s） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| 13b | 1 | OPT | PipeEdge | 11.78 | 233.77 | 13.69 | 66b | 5 | OPT | PipeEdge
    | 10.50 | 750.84 | 4.26 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 13b | 1 | OPT | PipeEdge | 11.78 | 233.77 | 13.69 | 66b | 5 | OPT | PipeEdge
    | 10.50 | 750.84 | 4.26 |'
- en: '| $Uniform{}^{*}$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| $Uniform{}^{*}$ |'
- en: '| FlexGen | 11.22 | 174.88 | 18.30(1.34$\times$ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 11.22 | 174.88 | 18.30(1.34$\times$ |'
- en: '| $FlexGen-int8{}^{*}$) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| $FlexGen-int8{}^{*}$) |'
- en: '| $LLM-PQ{}^{*}$) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| $LLM-PQ{}^{*}$) |'
- en: '| 2 | OPT | PipeEdge | 11.38 | 30.84 | 103.76 | 6 | OPT | PipeEdge | 10.34
    | 115.03 | 27.82 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2 | OPT | PipeEdge | 11.38 | 30.84 | 103.76 | 6 | OPT | PipeEdge | 10.34
    | 115.03 | 27.82 |'
- en: '| Uniform | 11.38 | 30.84 | 103.76 | Uniform | 10.50 | 431.92 | 7.41(0.27$\times$)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 11.38 | 30.84 | 103.76 | Uniform | 10.50 | 431.92 | 7.41(0.27$\times$)
    |'
- en: '| FlexGen | 11.22 | 71.09 | 45.01(0.43$\times$) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 11.22 | 71.09 | 45.01(0.43$\times$) |'
- en: '| FlexGen-int8 | 11.23 | 31.11 | 102.87(0.99$\times$) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 11.23 | 31.11 | 102.87(0.99$\times$) |'
- en: '| LLM-PQ | 11.23(-0.14) | 20.63 | 155.13(1.50$\times$) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 11.23(-0.14) | 20.63 | 155.13(1.50$\times$) |'
- en: '| 30b | 3 | OPT | PipeEdge | 10.70 | 146.40 | 21.86 | 176b | 7 | BLOOM | PipeEdge
    | 10.97 | 729.91 | 4.38 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 30b | 3 | OPT | PipeEdge | 10.70 | 146.40 | 21.86 | 176b | 7 | BLOOM | PipeEdge
    | 10.97 | 729.91 | 4.38 |'
- en: '| Uniform | 10.78 | 948.90 | 3.37(0.15$\times$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.78 | 948.90 | 3.37(0.15$\times$ |'
- en: '| FlexGen | 10.70 | 820.72 | 3.90(0.18$\times$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 820.72 | 3.90(0.18$\times$ |'
- en: '| FlexGen-int8 | 10.70 | 309.95 | 10.32(0.47$\times$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 309.95 | 10.32(0.47$\times$ |'
- en: '| LLM-PQ | 10.70 | 80.60 | 39.70(1.82$\times$) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.70 | 80.60 | 39.70(1.82$\times$) |'
- en: '| 4 | OPT | PipeEdge | 10.78 | 449.55 | 7.12 | 8 | BLOOM | PipeEdge | 10.97
    | 848.98 | 3.77 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 4 | OPT | PipeEdge | 10.78 | 449.55 | 7.12 | 8 | BLOOM | PipeEdge | 10.97
    | 848.98 | 3.77 |'
- en: '| Uniform | $\dagger$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | $\dagger$ |'
- en: '| FlexGen | 10.70 | 1,348.16 | 2.37(0.33$\times$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 1,348.16 | 2.37(0.33$\times$ |'
- en: '| FlexGen-int8 | 10.70 | 448.18 | 7.14(1$\times$ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 448.18 | 7.14(1$\times$ |'
- en: '| LLM-PQ | 10.70(-0.08) | 214.19 | 14.94(2.10$\times$) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.70(-0.08) | 214.19 | 14.94(2.10$\times$) |'
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6.3\. Serving in Heterogeneous Clusters ‣ 6\. Evaluation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization") demonstrates that LLM-PQ achieves the highest inference
    throughput by dividing the total number of generated tokens in a batch by the
    corresponding end-to-end latency. and the best model accuracy in clusters 3, 4,
    6, 7, and 8\. In cluster 2, LLM-PQ incur a negligible perplexity drop (0.01) but
    achieves a much faster inference speed (1.5$\times$). In cluster 6, the perplexity
    of LLM-PQ is even better than in the FP16 case. As compared with PipeEdge and
    Uniform, LLM-PQ can better utilize memory in heterogeneous devices and conduct
    phase-aware and precision-aware model partitions. LLM-PQ also outperforms FlexGen
    and FlexGen-int8 in most cases as they suffer from heavy swapping overhead. The
    results on cluster 1 reveal that our micro-batch sizing reducing the peak temporary
    memory needed by the model, allowing the int8 quantized model to fit nicely into
    the device memory.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S6.T4 "表 4 ‣ 6.3\. 在异质集群中的服务 ‣ 6\. 评估 ‣ LLM-PQ: 在异质集群上使用相位感知分区和自适应量化服务LLM")
    显示，LLM-PQ通过将批次中生成的总令牌数除以相应的端到端延迟，达到了最高的推理吞吐量，并在集群 3、4、6、7 和 8 中取得了最佳的模型准确性。在集群
    2 中，LLM-PQ 产生了微不足道的困惑度下降（0.01），但实现了更快的推理速度（1.5$\times$）。在集群 6 中，LLM-PQ 的困惑度甚至优于
    FP16 情况。与 PipeEdge 和 Uniform 相比，LLM-PQ 更能有效利用异质设备中的内存，并进行相位感知和精度感知的模型分区。LLM-PQ
    在大多数情况下也优于 FlexGen 和 FlexGen-int8，因为它们遭受了严重的交换开销。集群 1 的结果揭示了我们的微批量尺寸减少了模型所需的峰值临时内存，使得
    int8 量化模型可以很好地适应设备内存。'
- en: 6.4\. Serving in Homogeneous Clusters
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 在同质集群中的服务
- en: Table 5\. Serving performance comparison in homogenous clusters. The best inference
    throughput is marked in bold.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 在同质集群中的服务性能比较。最佳推理吞吐量用**粗体**标记。
- en: '| Model | Cluster | Scheme | PPL | Latency (s) | Throughput (Token/s) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Model | Cluster | Scheme | PPL | Latency (s) | Throughput (Token/s) |'
- en: '| OPT-30b | 9 | PipeEdge | 10.78 | 1,045.93 | 3.06 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 9 | PipeEdge | 10.78 | 1,045.93 | 3.06 |'
- en: '| Uniform | 10.78 | 1,045.93 | 3.06 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.78 | 1,045.93 | 3.06 |'
- en: '| FlexGen | 10.70 | 1,033.39 | 3.10(1.01$\times$) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 1,033.39 | 3.10(1.01$\times$) |'
- en: '| FlexGen-int8 | 10.70 | 313.46 | 10.21(3.34$\times$) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 313.46 | 10.21(3.34$\times$) |'
- en: '| LLM-PQ | 10.75 | 407.75 | 7.85(2.57$\times$) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.75 | 407.75 | 7.85(2.57$\times$) |'
- en: '| OPT-66b | 10 | PipeEdge | 10.33 | 182.47 | 17.54 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 10 | PipeEdge | 10.33 | 182.47 | 17.54 |'
- en: '| Uniform | 10.50 | 477.52 | 6.70(0.38$\times$) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.50 | 477.52 | 6.70(0.38$\times$) |'
- en: '| FlexGen | 10.33 | 433.99 | 7.37(0.42$\times$) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.33 | 433.99 | 7.37(0.42$\times$) |'
- en: '| FlexGen-int8 | 10.34 | 206.93 | 15.46(0.88$\times$) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.34 | 206.93 | 15.46(0.88$\times$) |'
- en: '| LLM-PQ | 10.33 | 178.11 | 17.97(1.02$\times$) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.33 | 178.11 | 17.97(1.02$\times$) |'
- en: '| BLOOM-176b | 11 | PipeEdge | 10.90 | 49.12 | 65.14 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 11 | PipeEdge | 10.90 | 49.12 | 65.14 |'
- en: '| Uniform | 10.97 | 895.45 | 3.57(0.05$\times$) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.97 | 895.45 | 3.57(0.05$\times$) |'
- en: '| LLM-PQ | 10.90 | 45.45 | 70.41(1.08$\times$) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.90 | 45.45 | 70.41(1.08$\times$) |'
- en: 'On homogeneous clusters, 9, 10, and 11, Table [5](#S6.T5 "Table 5 ‣ 6.4\. Serving
    in Homogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") shows that LLM-PQ
    still achieves throughput gains, though smaller than on heterogeneous clusters.
    In the case of cluster 9, the performance and perplexity of LLM-PQ are inferior
    to that of FlexGen-int8\. This discrepancy is attributed to the limited GPU memory
    compared to the workload requirement, resulting in high compression and usage
    of more low-precision kernels. Consequently, the computational speed is slower,
    but the efficiency of swapping is enhanced. Among other cases, LLM-PQ performs
    the best on model quality and serving throughput.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '在同质集群中，表 [5](#S6.T5 "表 5 ‣ 6.4\. 在同质集群中的服务 ‣ 6\. 评估 ‣ LLM-PQ: 在异质集群上使用相位感知分区和自适应量化服务LLM")
    显示，LLM-PQ 仍然取得了吞吐量的提升，尽管比在异质集群中小。以集群 9 为例，LLM-PQ 的性能和困惑度低于 FlexGen-int8。这一差异归因于与工作负载需求相比，GPU
    内存有限，导致压缩率高且使用了更多低精度内核。因此，计算速度较慢，但交换效率有所提高。在其他情况下，LLM-PQ 在模型质量和服务吞吐量上表现最佳。'
- en: 6.5\. Effectiveness of Variance Indicator
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 方差指标的有效性
- en: 'To further validate the effectiveness of our model variance indicator, we compare
    it with random assignment, where $\omega_{i,b}$ in (LABEL:eq:objective) to ensure
    that different indicators lead to similar inference latency, eliminating the influence
    of value range of the indicator. In Table [6](#S6.T6 "Table 6 ‣ 6.5\. Effectiveness
    of Variance Indicator ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization"), we observe that
    LLM-PQ achieve better perplexity than FP16 on cluster 6\. On cluster 9, with heavier
    quantization as mentioned above, Hessian-based and our indicators yield the same
    perplexity, outperforming the pure random indicator.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '为进一步验证我们模型方差指示器的有效性，我们将其与随机分配进行比较，其中(LABEL:eq:objective)中的$\omega_{i,b}$确保不同指示器导致类似的推理延迟，从而消除指示器值范围的影响。在表[6](#S6.T6
    "表 6 ‣ 6.5\. 方差指示器的有效性 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化进行LLM服务")中，我们观察到LLM-PQ在集群6上实现了比FP16更好的困惑度。在集群9上，如上所述，具有更重的量化，Hessian基于和我们指示器产生相同的困惑度，优于纯随机指示器。'
- en: Table 6\. Effectiveness of LLM-PQ’s variance indicator. PPL is compared with
    Random, while $\times$ is compared with Hessian.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. LLM-PQ的方差指示器的有效性。PPL与Random比较，而$\times$与Hessian比较。
- en: '| Model | Cluster | Method | PPL | Overhead (s) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Model | Cluster | Method | PPL | Overhead (s) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-66b | 6 | Random | 10.33 | 0 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 6 | Random | 10.33 | 0 |'
- en: '| Hessian | 10.33 | 25625.44 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Hessian | 10.33 | 25625.44 |'
- en: '| LLM-PQ | 10.31(-0.02) | 434.78(58.15$\times$) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.31(-0.02) | 434.78(58.15$\times$) |'
- en: '| OPT-30b | 9 | Random | 11.04 | 0 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 9 | Random | 11.04 | 0 |'
- en: '| Hessian | 10.75 | 15670.87 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Hessian | 10.75 | 15670.87 |'
- en: '| LLM-PQ | 10.75(-0.29) | 215.60(72.69$\times$) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.75(-0.29) | 215.60(72.69$\times$) |'
- en: 6.6\. Serving with Shorter Prompts
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 使用短提示进行服务
- en: 'We next experiment with input prompt length of 128 and maximal token generatoin
    number $n=200$. Table [7](#S6.T7 "Table 7 ‣ 6.6\. Serving with Shorter Prompts
    ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") shows that LLM-PQ achieves substantial inference
    speed-ups without any accuracy degradation, and even shows accuracy improvements.
    This confirms the correctness of our two-phase latency modeling in LLM-PQ. We
    note that the throughput gain of LLM-PQ in cluster 4 is much lower than that with
    prompt length 512, which we attribute to the reduced KV cache memory and the fact
    that smaller prompts and larger token generation numbers make the inference system
    more akin to the one-phase system that PipeEdge focuses on.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们在输入提示长度为128和最大生成令牌数$n=200$下进行实验。表[7](#S6.T7 "表 7 ‣ 6.6\. 短提示的服务 ‣ 6\.
    评估 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化进行LLM服务")显示，LLM-PQ在没有任何准确性下降的情况下显著加快了推理速度，甚至显示了准确性的提升。这确认了我们在LLM-PQ中的两阶段延迟建模的正确性。我们注意到LLM-PQ在集群4中的吞吐量增益远低于提示长度为512时，我们认为这归因于减少的KV缓存内存以及较小的提示和更大的令牌生成数量使得推理系统更类似于PipeEdge关注的一阶段系统。'
- en: Table 7\. Serving performance comparison under shorter prompts. The best results
    are marked in bold.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 短提示下的服务性能比较。最佳结果以**粗体**标记。
- en: '| Model | Cluster | Scheme | PPL | Latency(s) | Throughput (Token/s) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Model | Cluster | Scheme | PPL | Latency(s) | Throughput (Token/s) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| OPT-13b | 1 | PipeEdge | 11.23 | 84.80 | 75.47 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 1 | PipeEdge | 11.23 | 84.80 | 75.47 |'
- en: '| Uniform | 11.23 | 84.80 | 75.47(1.00$\times$) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 11.23 | 84.80 | 75.47(1.00$\times$) |'
- en: '| FlexGen | 11.22 | 119.24 | 53.68(0.71$\times$) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 11.22 | 119.24 | 53.68(0.71$\times$) |'
- en: '| FlexGen-int8 | 11.23 | 80.35 | 79.65(1.06$\times$) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 11.23 | 80.35 | 79.65(1.06$\times$) |'
- en: '| LLM-PQ | 11.23 | 47.63 | 134.38(1.78$\times$) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 11.23 | 47.63 | 134.38(1.78$\times$) |'
- en: '| OPT-30b | 4 | PipeEdge | 10.70 | 366.54 | 17.46 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 4 | PipeEdge | 10.70 | 366.54 | 17.46 |'
- en: '| Uniform | 10.80 | 281.83 | 22.71(1.30$\times$) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.80 | 281.83 | 22.71(1.30$\times$) |'
- en: '| FlexGen | 10.70 | 2,147.03 | 2.98(0.17$\times$) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 2,147.03 | 2.98(0.17$\times$) |'
- en: '| FlexGen-int8 | 10.70 | 681.78 | 9.39(0.54$\times$) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 681.78 | 9.39(0.54$\times$) |'
- en: '| LLM-PQ | 10.70 | 262.34 | 24.40(1.40$\times$) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.70 | 262.34 | 24.40(1.40$\times$) |'
- en: '| OPT-66b | 6 | PipeEdge | 10.33 | 132.34 | 48.36 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 6 | PipeEdge | 10.33 | 132.34 | 48.36 |'
- en: '| Uniform | 10.33 | 298.99 | 21.41(0.44$\times$) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.33 | 298.99 | 21.41(0.44$\times$) |'
- en: '| FlexGen | 10.33 | 408.19 | 15.68(0.32$\times$) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.33 | 408.19 | 15.68(0.32$\times$) |'
- en: '| FlexGen-int8 | 10.34 | 376.69 | 16.99(0.35$\times$) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.34 | 376.69 | 16.99(0.35$\times$) |'
- en: '| LLM-PQ | 10.30(-0.03) | 75.98 | 84.23(1.74$\times$) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.30(-0.03) | 75.98 | 84.23(1.74$\times$) |'
- en: 6.7\. Approaches Expediting Optimizer Algorithm
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7\. 加速优化器算法的方法
- en: Table 8\. Effectiveness of Grouping and Heuristic approaches under time limit.
    The best results are marked in bold.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表8\. 分组和启发式方法在时间限制下的有效性。最佳结果以粗体标出。
- en: '| Model | Cluster | Method | Throughput (token/s) | Overhead (s) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 集群 | 方法 | 吞吐量 (token/s) | 开销 (s) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-30b | 3 | Group=2 | 39.70 | 1.07 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 3 | Group=2 | 39.70 | 1.07 |'
- en: '| Group=1 | 39.70(+0) | 3.29 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 39.70(+0) | 3.29 |'
- en: '| Heuristic | 35.17 | 5.36 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 35.17 | 5.36 |'
- en: '| OPT-66b | 6 | Group=2 | 39.56 | 2.70 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 6 | Group=2 | 39.56 | 2.70 |'
- en: '| Group=1 | 44.93(+5.37) | 19.14 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 44.93(+5.37) | 19.14 |'
- en: '| Heuristic | 28.45 | 7.70 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 28.45 | 7.70 |'
- en: '| OPT-30b | 4 | Group=2 | 14.72 | 12.29 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 4 | Group=2 | 14.72 | 12.29 |'
- en: '| Group=1 | 13.93(-0.79) | 204.59 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 13.93(-0.79) | 204.59 |'
- en: '| Heuristic | 14.94(+0.22) | 1.99 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 14.94(+0.22) | 1.99 |'
- en: '| OPT-66b | 10 | Group=2 | 16.64 | 59.27 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 10 | Group=2 | 16.64 | 59.27 |'
- en: '| Group=1 | 17.57(+0.93) | 127.28 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 17.57(+0.93) | 127.28 |'
- en: '| Heuristic | 17.97(+1.33) | 2.11 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 17.97(+1.33) | 2.11 |'
- en: In LLM-PQ, we provide two approaches, layer grouping, and a heuristic, to reduce
    and the complexity of the optimizer’s bitwidth selection, model partition, and
    placement. We evaluate the inference throughput and the time required to derive
    the solution when applying three strategies (group = 2, group = 1, and heuristic),
    on clusters 3, 4, 6, and 10\. group = 2 means group 2 decoder layers together
    for decision. We set a 60-second time limit for the ILP solver.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM-PQ中，我们提供了两种方法，即层分组和启发式，以减少优化器的位宽选择、模型分区和放置的复杂性。我们在集群3、4、6和10上应用三种策略（group
    = 2、group = 1和启发式），评估推理吞吐量和推导解决方案所需的时间。group = 2表示将2层解码器层分组进行决策。我们为ILP求解器设定了60秒的时间限制。
- en: 'Group = 1 covers the entire solution space and typically produces better results
    compared to group = 2 (on clusters 6 and 10), but it introduces a larger overhead,
    as shown in Table [8](#S6.T8 "Table 8 ‣ 6.7\. Approaches Expediting Optimizer
    Algorithm ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization"). On cluster 4, group = 1 cannot
    find a good solution within the time limit. On cluster 3, group = 1 and group
    = 2 produce the same solution. Performance of the heuristic largely depends on
    the starting point produced by adabits (start point of optimization #3 in Sec. [4.3](#S4.SS3
    "4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization")). It leads to
    the best throughput with the smallest overhead in clusters 4 and 10.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'Group = 1涵盖了整个解决空间，通常比group = 2（在集群6和10上）产生更好的结果，但它引入了更大的开销，如表[8](#S6.T8 "表8
    ‣ 6.7\. 加速优化器算法的方法 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上进行LLM服务，采用阶段感知分区和自适应量化")所示。在集群4上，group
    = 1无法在时间限制内找到一个好的解决方案。在集群3上，group = 1和group = 2产生相同的解决方案。启发式的性能在很大程度上依赖于adabits生成的起始点（第[4.3](#S4.SS3
    "4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上进行LLM服务，采用阶段感知分区和自适应量化")节优化#3的起始点）。它在集群4和10中导致了最佳吞吐量和最小开销。'
- en: 'We highlight the utilization of heuristics significantly enhances the scalability
    of LLM-PQ in offline workloads: solving time of a cluster comprising two P100,
    V100, and A100 GPUs each for OPT66B is reduced to 31s.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调，启发式的使用显著提升了LLM-PQ在离线工作负载中的可扩展性：由两个P100、V100和A100 GPU组成的集群在OPT66B的求解时间减少到31秒。
- en: 6.8\. Parameter Sensitivity
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8\. 参数敏感性
- en: '![Refer to caption](img/f275d8bbaf67e18223bc1ef3c1cb5329.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f275d8bbaf67e18223bc1ef3c1cb5329.png)'
- en: (a) Cluster 9 OPT-30b
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 集群9 OPT-30b
- en: '![Refer to caption](img/24cf767685caa45db83b9e3fd617a955.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/24cf767685caa45db83b9e3fd617a955.png)'
- en: (b) Cluster 5 OPT-66b
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 集群5 OPT-66b
- en: Figure 8\. Sensitivity experiments on $\theta$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 关于$\theta$的敏感性实验。
- en: We next investigate the impact of user quality scalar $\theta$ generally results
    in lower inference throughput and higher model accuracy, as less weight is placed
    on inference latency and more on model quality in our ILP optimization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了用户质量标量$\theta$的影响，通常会导致较低的推理吞吐量和更高的模型准确性，因为在我们的ILP优化中，推理延迟的权重较少，模型质量的权重较多。
- en: 6.9\. Comparison with Pure Adaptive Quantization
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9\. 与纯自适应量化的比较
- en: '![Refer to caption](img/e906cb128cd8412dbd1cffbe141a6356.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e906cb128cd8412dbd1cffbe141a6356.png)'
- en: Figure 9\. Comparison with pure adaptive quantization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 与纯自适应量化的比较。
- en: 'To verify the significance of concurrently considering adaptive bitwidth, layer
    partitioning, and micro-batch sizing, we further compare LLM-PQ with adabits used
    in the heuristic method. We evaluate the performance of adabits with same model
    setup on clusters 3, 5, and 6, 9 with prompt length 512 and on cluster 4 with
    prompt length 128\. In Fig. [9](#S6.F9 "Figure 9 ‣ 6.9\. Comparison with Pure
    Adaptive Quantization ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") LLM-PQ outperforms
    adabits in all selected cases.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证同时考虑自适应比特宽度、层分区和微批处理大小的重要性，我们进一步将LLM-PQ与启发式方法中使用的adabits进行比较。我们在集群3、5和6、9（提示长度为512）以及集群4（提示长度为128）上评估了相同模型设置下adabits的性能。在图[9](#S6.F9
    "Figure 9 ‣ 6.9\. Comparison with Pure Adaptive Quantization ‣ 6\. Evaluation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")中，LLM-PQ在所有选定的案例中都优于adabits。'
- en: 7\. Discussions
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 讨论
- en: Search for Tensor Parallelization. We did not incorporate tensor parallelism
    in our serving system implementation due to the favorable characteristics of the
    pipeline when dealing with heterogeneity, which results in reduced communication
    requirements. It can be readily included in our search space. Tensor parallelism
    heavily relies on the 2-d device mesh configuration, and tensor sharding strategies
    can be searched based on the device mesh enumeration. Given 2 nodes with 8 GPUs
    per node (totaling 16 devices), we can represent them as a device mesh of size
    2×8, 1×16, 4×4, 8×2, or 16×1, where the device communication with different bandwidths
    for the first and second-dimension, and the tensor-parallel can apply along either
    the first or second dimension ([zheng2022alpa,](#bib.bib41) ). As the possible
    device mesh is limited, it is similar to how we enumerate all possible 1-d device
    orderings. For the above reason, we can view the device along the tensor-parallel
    dimension as a new device with larger memory and different kernel performance
    (as tensor-parallel will introduce some communication overhead), and it is still
    a 1-d partition problem along another axis, which conforms to our solutions.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索张量并行化。由于在处理异质性时，管道具有良好的特性，从而减少了通信需求，我们没有在我们的服务系统实现中加入张量并行性。张量并行性可以很容易地包含在我们的搜索空间中。张量并行性高度依赖于2维设备网格配置，而张量分片策略可以根据设备网格枚举进行搜索。给定2个节点，每个节点8个GPU（共16个设备），我们可以将它们表示为大小为2×8、1×16、4×4、8×2或16×1的设备网格，其中设备通信具有不同带宽的第一和第二维度，并且张量并行可以沿着第一或第二维度应用（[zheng2022alpa](#bib.bib41)）。由于可能的设备网格有限，这类似于我们枚举所有可能的1维设备排序。因此，我们可以将沿张量并行维度的设备视为具有更大内存和不同内核性能的新设备（因为张量并行会引入一些通信开销），这仍然是沿另一个轴的1维分区问题，符合我们的解决方案。
- en: Other Quantization Schemes There is rapid development in quantization methods
    for LLM. The latest weight-only quantization methods, such as AWQ ([lin2023awq,](#bib.bib21)
    ), SpQR ([dettmers2023spqr,](#bib.bib8) ) and QLoRA ([dettmers2023qlora,](#bib.bib7)
    ), AWQ improves kernel efficiency through re-order free quantization and utilizes
    TensorCore. SpQR improves the accuracy of GPTQ through better outlier detection.
    QLoRA proposes a memory-efficient 4-bit finetuning method and introduces double
    quantization to further reduce the memory footprint by quantizing the scalars
    used in quantization. LLM-PQ views these schemes as candidate quantization schemes,
    and these new schemes can be efficiently integrated into our system.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 其他量化方案 在大规模语言模型（LLM）的量化方法中有着迅速的发展。最新的仅权重量化方法，如AWQ（[lin2023awq](#bib.bib21)）、SpQR（[dettmers2023spqr](#bib.bib8)）和QLoRA（[dettmers2023qlora](#bib.bib7)），AWQ通过无重排序量化提高了内核效率，并利用了TensorCore。SpQR通过更好的异常值检测提高了GPTQ的准确性。QLoRA提出了一种内存高效的4-bit微调方法，并引入了双重量化，通过量化用于量化的标量进一步减少内存占用。LLM-PQ将这些方案视为候选量化方案，这些新方案可以高效地集成到我们的系统中。
- en: Apply to ORCA or vLLM ORCA ([orca,](#bib.bib38) ) introduces iterative-level
    scheduling, while vLLM ([vllm,](#bib.bib20) ) possesses an efficient page-attention
    technology for memory management. LLM-PQ’s design is orthogonal to both of them.
    However, unlike the offline task, the online workload is unpredictable, and the
    available paged memory for Key-Value (KV) storage is affected by quantization
    level. While the available memory plays a crucial role in influencing throughput
    when confronted with an infinite number of requests, there is always a trade-off
    between the speed of quantized operators and the amount of available memory. This
    trade-off necessitates new design considerations for performance optimization
    when implementing LLM-PQ at runtime.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 申请到 ORCA 或 vLLM ORCA ([orca,](#bib.bib38)) 引入了迭代级调度，而 vLLM ([vllm,](#bib.bib20))
    具有高效的页面注意力技术用于内存管理。LLM-PQ 的设计与它们正交。然而，与离线任务不同，在线工作负载是不可预测的，可用于 Key-Value (KV)
    存储的分页内存受到量化级别的影响。虽然可用内存在面对无限数量请求时对吞吐量有关键影响，但量化操作的速度与可用内存之间总是存在权衡。这种权衡需要在运行时实现
    LLM-PQ 时进行性能优化的新设计考虑。
- en: 8\. Conclusion
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: We propose LLM-PQ, an efficient system for LLM serving atop heterogeneous clusters.
    We derive efficient cost models to accurately predict memory occupation and execution
    latency of mixed-precision LLM serving. We introduce adaptive mixed-precision
    into the search space of pipeline serving and proposed an efficient indicator
    to guide bitwidth selection in the search process. We jointly consider serving
    latency in different token generation phases based on various precision settings,
    micro-batch sizes, and layer partitions, and derive efficient optimized solutions.
    Our extensive experiments validate the performance of LLM-PQ on a variety of cluster
    setups, which surpasses state-of-the-art approaches of serving LLM on heterogeneous
    clusters.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LLM-PQ，一个高效的系统用于在异构集群上服务 LLM。我们推导了高效的成本模型，以准确预测混合精度 LLM 服务的内存占用和执行延迟。我们将自适应混合精度引入管道服务的搜索空间，并提出了一种高效的指标来指导搜索过程中的位宽选择。我们综合考虑了不同令牌生成阶段的服务延迟，基于各种精度设置、微批大小和层分区，推导出高效的优化解决方案。我们的大量实验验证了
    LLM-PQ 在多种集群设置上的性能，超越了在异构集群上服务 LLM 的最先进方法。
- en: References
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan,
    Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and
    Yuxiong He. Deepspeed- inference: Enabling efficient inference of transformer
    models at unprecedented scale. SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–15, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan,
    Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, 和 Yuxiong
    He。Deepspeed- inference: 以空前的规模实现变压器模型的高效推理。SC22: 高性能计算、网络、存储与分析国际会议，页面 1–15,
    2022。'
- en: '[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth
    AAAI Conference on Artificial Intelligence, 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, 和 Yejin Choi。Piqa:
    以自然语言推理物理常识。在第三十四届 AAAI 人工智能会议，2020。'
- en: '[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
    Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
    Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。语言模型是少样本学习者。ArXiv, abs/2005.14165, 2020。'
- en: '[4] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael
    Mahoney, and Joseph Gonzalez. Actnn: Reducing training memory footprint via 2-bit
    activation compressed training. In International Conference on Machine Learning,
    2021.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael
    Mahoney, 和 Joseph Gonzalez。Actnn: 通过2-bit激活压缩训练减少训练内存占用。在国际机器学习会议，2021。'
- en: '[5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 arc，AI2 推理挑战。arXiv:1803.05457v1,
    2018。'
- en: '[6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer. Llm. int8
    (): 用于大规模变换器的 8 位矩阵乘法。arXiv 预印本 arXiv:2208.07339, 2022。'
- en: '[7] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. Qlora：高效的量化
    LLM 微调。arXiv 预印本 arXiv:2305.14314, 2023。'
- en: '[8] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    ArXiv, abs/2306.03078, 2023.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, 和 Dan Alistarh.
    Spqr：一种稀疏量化表示，用于近乎无损的 LLM 权重压缩。ArXiv, abs/2306.03078, 2023。'
- en: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. Bert：深度双向变换器在语言理解中的预训练。arXiv
    预印本 arXiv:1810.04805, 2018。'
- en: '[10] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer.
    Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, 和 Kurt Keutzer.
    Hawq：具备海森矩阵感知的混合精度神经网络量化。在 IEEE/CVF 国际计算机视觉会议（ICCV）论文集，2019年10月。'
- en: '[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly 等人。图像胜过 16x16 个词：用于大规模图像识别的变换器。arXiv 预印本 arXiv:2010.11929, 2020。'
- en: '[12] Jiangsu Du, Ziming Liu, Jiarui Fang, Shenggui Li, Yongbin Li, Yutong Lu,
    and Yang You. Energonai: An inference system for 10-100 billion parameter transformer
    models. arXiv preprint arXiv:2209.02341, 2022.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Jiangsu Du, Ziming Liu, Jiarui Fang, Shenggui Li, Yongbin Li, Yutong Lu,
    和 Yang You. Energonai：一个用于 10-100 亿参数变换器模型的推理系统。arXiv 预印本 arXiv:2209.02341, 2022。'
- en: '[13] Hugging Face. Text generation inference. [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference),
    n.d. Accessed on: July 24, 2023.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hugging Face. 文本生成推理。 [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference),
    未注明日期。访问日期：2023年7月24日。'
- en: '[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. ArXiv,
    abs/2210.17323, 2022.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh. Gptq：生成预训练变换器的准确后训练量化。ArXiv,
    abs/2210.17323, 2022。'
- en: '[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ:
    Accurate quantization for generative pre-trained transformers. In The Eleventh
    International Conference on Learning Representations, 2023.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh. OPTQ：生成预训练变换器的准确量化。在第十一届国际学习表示会议上，2023年。'
- en: '[16] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Gurobi Optimization, LLC. Gurobi 优化器参考手册, 2023。'
- en: '[17] Yang Hu, Connor Imes, Xuanang Zhao, Souvik Kundu, Peter A. Beerel, Stephen P.
    Crago, and John Paul Walters. Pipeline parallelism for inference on heterogeneous
    edge computing. ArXiv, abs/2110.14895, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Yang Hu, Connor Imes, Xuanang Zhao, Souvik Kundu, Peter A. Beerel, Stephen
    P. Crago, 和 John Paul Walters. 异构边缘计算上的推理管道并行。ArXiv, abs/2110.14895, 2021。'
- en: '[18] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
    Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient
    training of giant neural networks using pipeline parallelism. Advances in neural
    information processing systems, 32, 2019.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
    Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu 等人。Gpipe：使用管道并行技术有效训练巨型神经网络。神经信息处理系统进展，32，2019。'
- en: '[19] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity—a
    measure of the difficulty of speech recognition tasks. The Journal of the Acoustical
    Society of America, 62(S1):S63–S63, 1977.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Fred Jelinek、Robert L Mercer、Lalit R Bahl 和 James K Baker。困惑度——语音识别任务难度的衡量标准。《美国声学学会杂志》，62(S1):S63–S63，1977年。'
- en: '[20] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
    large language model serving with pagedattention. In Proceedings of the 29th Symposium
    on Operating Systems Principles, 2023.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Woosuk Kwon、Zhuohan Li、Siyuan Zhuang、Ying Sheng、Lianmin Zheng、Cody Hao
    Yu、Joseph Gonzalez、Hao Zhang 和 Ion Stoica。针对大型语言模型服务的高效内存管理与分页注意力。在第29届操作系统原理研讨会论文集，2023年。'
- en: '[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    ArXiv, abs/2306.00978, 2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song Han。Awq：针对
    LLM 压缩和加速的激活感知权重量化。ArXiv，abs/2306.00978，2023年。'
- en: '[22] Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, and Xia Hu. Exact:
    Scalable graph neural networks training via extreme activation compression. In
    International Conference on Learning Representations, 2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Zirui Liu、Kaixiong Zhou、Fan Yang、Li Li、Rui Chen 和 Xia Hu。Exact：通过极端激活压缩进行可扩展图神经网络训练。在国际学习表征会议，2022年。'
- en: '[23] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building
    a large annotated corpus of English: The Penn Treebank. Computational Linguistics,
    19(2):313–330, 1993.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Mitchell P. Marcus、Beatrice Santorini 和 Mary Ann Marcinkiewicz。构建大型英文注释语料库：Penn
    Treebank。《计算语言学》，19(2):313–330，1993年。'
- en: '[24] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models, 2016.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。指针哨兵混合模型，2016年。'
- en: '[25] NVIDIA. Fastertransformer: Transformer related optimization, including
    bert, gpt, n.d.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] NVIDIA。Fastertransformer：与变换器相关的优化，包括 BERT、GPT，无日期。'
- en: '[26] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.
    The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), August 2016.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Denis Paperno、Germán Kruszewski、Angeliki Lazaridou、Ngoc Quan Pham、Raffaella
    Bernardi、Sandro Pezzelle、Marco Baroni、Gemma Boleda 和 Raquel Fernández。LAMBADA
    数据集：需要广泛话语上下文的词预测。在第54届计算语言学协会年会论文集（第1卷：长篇论文），2016年8月。'
- en: '[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
    Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
    Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank
    Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:
    An imperative style, high-performance deep learning library. In Neural Information
    Processing Systems, 2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga、Alban Desmaison、Andreas
    Köpf、Edward Yang、Zach DeVito、Martin Raison、Alykhan Tejani、Sasank Chilamkurthy、Benoit
    Steiner、Lu Fang、Junjie Bai 和 Soumith Chintala。Pytorch：一种命令式风格的高性能深度学习库。在神经信息处理系统大会，2019年。'
- en: '[28] Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang,
    Fog Dong, Xipeng Guan, and Frost Ming. OpenLLM: Operating LLMs in production,
    2023.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Aaron Pham、Chaoyu Yang、Sean Sheng、Shenyang Zhao、Sauyon Lee、Bo Jiang、Fog
    Dong、Xipeng Guan 和 Frost Ming。OpenLLM：生产环境中的 LLM 操作，2023年。'
- en: '[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J. Liu。通过统一的文本到文本转换器探索迁移学习的极限。arXiv e-prints，2019年。'
- en: '[30] RyokoAI. Sharegpt52k. [https://huggingface.co/datasets/RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K),
    2021. Dataset accessed on [insert date].'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] RyokoAI。Sharegpt52k。 [https://huggingface.co/datasets/RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)，2021年。数据集访问日期
    [插入日期]。'
- en: '[31] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    ArXiv, abs/2211.05100, 2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Teven Le Scao、Angela Fan、Christopher Akiki、Ellie Pavlick、Suzana Ilić、Daniel
    Hesslow、Roman Castagné、Alexandra Sasha Luccioni、François Yvon、Matthias Gallé 等。Bloom：一个
    176b 参数的开放访问多语言模型。ArXiv，abs/2211.05100，2022年。'
- en: '[32] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi
    Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput
    generative inference of large language models with a single gpu. In Proceedings
    of the 40th International Conference on Machine Learning, 2023.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi
    Chen, Percy Liang, Christopher Ré, Ion Stoica 和 Ce Zhang。Flexgen：利用单个 GPU 进行大规模语言模型的高吞吐量生成推理。发表于第40届国际机器学习会议，2023。'
- en: '[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample。Llama：开放和高效的基础语言模型。ArXiv，abs/2302.13971，2023。'
- en: '[34] Borui Wan, Jun Zhao, and Chuan Wu. Adaptive message quantization and parallelization
    for distributed full-graph gnn training. ArXiv, abs/2306.01381, 2023.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Borui Wan, Jun Zhao 和 Chuan Wu. 自适应消息量化和并行化用于分布式全图 GNN 训练。ArXiv，abs/2306.01381，2023。'
- en: '[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven
    Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.
    Transformers: State-of-the-Art Natural Language Processing. In Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations, 2020.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven
    Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest 和 Alexander M. Rush。Transformers：最先进的自然语言处理。在2020年自然语言处理经验方法会议论文集中：系统演示，2020。'
- en: '[36] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, 2023.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth 和 Song
    Han. Smoothquant：大规模语言模型的准确高效后训练量化。发表于国际机器学习会议，2023。'
- en: '[37] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. In Advances in Neural Information Processing Systems,
    2022.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He。Zeroquant：大规模变换器的高效且经济的后训练量化。发表于神经信息处理系统进展，2022。'
- en: '[38] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon
    Chun. Orca: A distributed serving system for Transformer-Based generative models.
    In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    22), 2022.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim 和 Byung-Gon
    Chun。Orca：用于基于变换器生成模型的分布式服务系统。在第16届USENIX操作系统设计与实现研讨会（OSDI 22），2022。'
- en: '[39] Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In
    Advances in Neural Information Processing Systems 32, 2019.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Biao Zhang 和 Rico Sennrich。均方根层归一化。在神经信息处理系统进展32中，2019。'
- en: '[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer
    language models. ArXiv, abs/2205.01068, 2022.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang 和 Luke Zettlemoyer。Opt：开放的预训练变换器语言模型。ArXiv，abs/2205.01068，2022。'
- en: '[41] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating
    inter-and $\{$ parallelism for distributed deep learning. In 16th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 22), 2022.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing 等人。Alpa：自动化的分布式深度学习的交叉和$\{$并行性。在第16届USENIX操作系统设计与实现研讨会（OSDI
    22），2022。'
- en: Appendix A Appendix
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: 'A.1\. Proof of Theorem [1](#S4.Thmtheorem1 "Theorem 1\. ‣ 4.2\. Indicator of
    Model Perturbation by Quantization ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM
    on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'A.1\. 定理证明 [1](#S4.Thmtheorem1 "定理 1\. ‣ 4.2\. 量化模型扰动的指示符 ‣ 4\. 分配器设计 ‣ LLM-PQ:
    在异构集群上提供LLM，带有相位感知分区和自适应量化")'
- en: Proof.
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Let $\mathbf{X}$.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathbf{X}$。
- en: In deterministic rounding [[36](#bib.bib36), [14](#bib.bib14)], quantized scalar
    can be either $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定性舍入[[36](#bib.bib36), [14](#bib.bib14)]中，量化标量可以是 $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$。
- en: For stochastic rounding [[4](#bib.bib4), [22](#bib.bib22)], scalar $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$
    ∎
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机舍入[[4](#bib.bib4), [22](#bib.bib22)]，标量 $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$
    ∎
- en: A.2\. Experiment
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. 实验
- en: A.2.1\. $\theta$ and Solver Setup
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1\. $\theta$ 和解算器设置
- en: 'Table 9\. Solver setups for Table [4](#S6.T4 "Table 4 ‣ 6.3\. Serving in Heterogeneous
    Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization") and  [5](#S6.T5 "Table 5 ‣ 6.4\.
    Serving in Homogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9\. 解算器设置，见表 [4](#S6.T4 "表 4 ‣ 6.3\. 在异构集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    和  [5](#S6.T5 "表 5 ‣ 6.4\. 在同质集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")'
- en: '| Cluster | Group | Heuristic? | $\theta$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 组 | 启发式？ | $\theta$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 1 | N | 1 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | N | 1 |'
- en: '| 2 | 1 | N | 1 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | N | 1 |'
- en: '| 3 | 1 | N | 1 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | N | 1 |'
- en: '| 4 | - | Y | 1000 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 4 | - | Y | 1000 |'
- en: '| 5 | - | Y | 50 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | Y | 50 |'
- en: '| 6 | 1 | N | 100 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | N | 100 |'
- en: '| 7 | 1 | N | 10 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 | N | 10 |'
- en: '| 8 | 1 | N | 10 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | N | 10 |'
- en: '| 9 | 1 | N | 1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1 | N | 1 |'
- en: '| 10 | - | Y | 1 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 10 | - | Y | 1 |'
- en: '| 11 | - | Y | 10 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 11 | - | Y | 10 |'
- en: 'Table [9](#A1.T9 "Table 9 ‣ A.2.1\. 𝜃 and Solver Setup ‣ A.2\. Experiment ‣
    Appendix A Appendix ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") provides the $\theta$ and solver configurations
    used in both hetero- and homogeneous results for LLM-PQ.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#A1.T9 "表 9 ‣ A.2.1\. 𝜃 和解算器设置 ‣ A.2\. 实验 ‣ 附录 A 附录 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    提供了用于异构和同质结果的 $\theta$ 和解算器配置，适用于 LLM-PQ。'
- en: A.2.2\. Overhead Table
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2\. 开销表
- en: 'Table 10\. Problem solving overhead for Table [4](#S6.T4 "Table 4 ‣ 6.3\. Serving
    in Heterogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") and  [5](#S6.T5
    "Table 5 ‣ 6.4\. Serving in Homogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving
    LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10\. 问题解决开销，见表 [4](#S6.T4 "表 4 ‣ 6.3\. 在异构集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    和  [5](#S6.T5 "表 5 ‣ 6.4\. 在同质集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")'
- en: '| Cluster | Overhead(s) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 开销 |'
- en: '| --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 0.2977 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.2977 |'
- en: '| 2 | 0.2977 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.2977 |'
- en: '| 3 | 2.78127 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2.78127 |'
- en: '| 4 | 2.28628 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2.28628 |'
- en: '| 5 | 9.9239153 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 9.9239153 |'
- en: '| 6 | 115.981 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 115.981 |'
- en: '| 7 | 44.3031 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 44.3031 |'
- en: '| 8 | 19.31674 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 19.31674 |'
- en: '| 9 | 1.15838 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1.15838 |'
- en: '| 10 | 2.45544 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 2.45544 |'
- en: '| 11 | 3.4 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 3.4 |'
- en: '| AVG | 18.38195685 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 18.38195685 |'
- en: '| SLOWEST | 115.981 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 最慢 | 115.981 |'
- en: 'Table [10](#A1.T10 "Table 10 ‣ A.2.2\. Overhead Table ‣ A.2\. Experiment ‣
    Appendix A Appendix ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") presents the solving latency of both hetero-
    and homogeneous results for LLM-PQ. We also provide a data point for the three-nodes
    cluster: Cluster of P100, V100, and A100 GPUs (two each type): solving time with
    31s for OPT66B using heuristic.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [10](#A1.T10 "表 10 ‣ A.2.2\. 开销表 ‣ A.2\. 实验 ‣ 附录 A 附录 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    展示了 LLM-PQ 在异构和同质结果中的求解延迟。我们还提供了一个三节点集群的数据点：由 P100、V100 和 A100 GPU 组成的集群（每种类型各两个）：使用启发式方法求解
    OPT66B 所需时间为 31 秒。'
