- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:59'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-FP4: 4-Bit Floating-Point Quantized Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16836](https://ar5iv.labs.arxiv.org/html/2310.16836)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shih-yang Liu^∗¹, Zechun Liu^∗², Xijie Huang¹, Pingcheng Dong¹, Kwang-Ting Cheng¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Hong Kong University of Science and Technology, ²Meta Reality Labs
  prefs: []
  type: TYPE_NORMAL
- en: '{sliuau, xhuangbs, pingcheng.dong}@connect.ust.hk'
  prefs: []
  type: TYPE_NORMAL
- en: zechunliu@meta.com
  prefs: []
  type: TYPE_NORMAL
- en: timcheng@ust.hk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We propose LLM-FP4 for quantizing both weights and activations in large language
    models (LLMs) down to 4-bit floating-point values, in a post-training manner.
    Existing post-training quantization (PTQ) solutions are primarily integer-based
    and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point
    (FP) quantization is more flexible and can better handle long-tail or bell-shaped
    distributions, and it has emerged as a default choice in many hardware platforms.
    One characteristic of FP quantization is that its performance largely depends
    on the choice of exponent bits and clipping range. In this regard, we construct
    a strong FP-PTQ baseline by searching for the optimal quantization parameters.
    Furthermore, we observe a high inter-channel variance and low intra-channel variance
    pattern in activation distributions, which adds activation quantization difficulty.
    We recognize this pattern to be consistent across a spectrum of transformer models
    designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models.
    To tackle this, we propose per-channel activation quantization and show that these
    additional scaling factors can be reparameterized as exponential biases of weights,
    incurring a negligible cost. Our method, for the first time, can quantize both
    weights and activations in the LLaMA-13B to only 4-bit and achieves an average
    score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8
    lower than the full-precision model, significantly outperforming the previous
    state-of-the-art by 12.7 points. Code is available at: [https://github.com/nbasyl/LLM-FP4](https://github.com/nbasyl/LLM-FP4).'
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: These authors contributed equally to this work'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the introduction of transformer architecture Vaswani et al. ([2017](#bib.bib27)),
    transformers have superseded recursive neural networks, emerging as the dominant
    architecture in numerous natural language processing (NLP) tasks Kenton and Toutanova
    ([2019](#bib.bib13)); Lewis et al. ([2020](#bib.bib16)). The transformative impact
    of the transformer has been further propelled by the emergence of models like
    GPT Brown et al. ([2020](#bib.bib4)); OpenAI ([2023](#bib.bib23)), catapulting
    the popularity of this architecture to new heights. Meanwhile, the versatility
    of transformers extends beyond NLP, encompassing diverse domains such as vision [Dosovitskiy
    et al.](#bib.bib10) ; Touvron et al. ([2021](#bib.bib25)), audio Akbari et al.
    ([2021](#bib.bib1)), etc. This trend towards a unified architecture for different
    modalities represents a groundbreaking development within the realm of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, the advancements in transformer performance are accompanied by a corresponding
    increase in model size and computational costs Kaplan et al. ([2020](#bib.bib12)).
    This poses significant challenges when attempting to leverage the full potential
    of transformer models in use cases where memory or computational resources are
    limited. Despite the extensive research and widespread adoption of transformers,
    the field of transformer compression remains relatively underexplored. To address
    this gap, our study focuses on the compression of transformers, especially through
    floating-point post-training quantization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Post-training quantization (PTQ) offers the advantages of simple to use with
    minimal fine-tuning requirements Nagel et al. ([2020](#bib.bib21)); Cai et al.
    ([2020](#bib.bib5)). Existing PTQ solutions for transformers primarily focus on
    integer (INT) quantization Liu et al. ([2021](#bib.bib19)); Yuan et al. ([2022](#bib.bib32)),
    which can be effective in certain scenarios but often break down when bit widths
    are below 8 bit. On the other hand, floating-point (FP) quantization has gained
    significant traction as a more flexible alternative, capable of better accommodating
    various activation and weight distributions. In fact, FP8 has emerged as the default
    choice in various hardware platforms, including the NVIDIA H100.
  prefs: []
  type: TYPE_NORMAL
- en: Different from integer (INT) quantization, a particular challenge in floating-point
    (FP) quantization is how to select appropriate exponent bits and scale parameters.
    Improper parameter choices can lead to subpar or divergent quantization results.
    To tackle this challenge, we introduce a robust recipe for FP quantization, which
    leverage layer-wise reconstruction to jointly search for optimal exponent bits
    and maximum values. Compared to previous approaches that utilize gradient updates
    for exponent bits Kuzmin et al. ([2022](#bib.bib14)), our search-based method
    proves to be more stable and consistently delivers desirable quantization results,
    which establishes a strong baseline for FP-PTQ.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, our investigation uncovers an intriguing pattern of activation
    distributions in transformers, characterized by high inter-channel variance and
    low intra-channel variance. Similar patterns are also observed in previous works Xiao
    et al. ([2022](#bib.bib31)); Dettmers et al. ([2022](#bib.bib7)), while we argue
    that this pattern is inherent to transformer architectures and not limited to
    specific tasks, as we have observed consistent patterns not only in large language
    models but also in BERT model and even vision transformers. Motivated by these
    findings, we introduce a novel pre-shifted exponent bias for FP quantization of
    transformers. Concretely, we leverage the per-channel activation variance computed
    from calibration data and reparameterize these scales as the exponential bias
    of the corresponding FP quantized weight vectors. This approach effectively addresses
    the challenge posed by high inter-channel variance while incurring negligible
    computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we study floating-point post-training quantization (PTQ) for transformer
    architectures, and the contribution of this paper includes:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We propose a search-based framework for determining the optimal exponent
    bias and maximal quantization value. This method outperforms existing techniques
    in terms of stability and performance, establishing a strong baseline for floating-point
    post-training quantization.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We propose a novel technique, pre-shifted exponent bias, which effectively
    addresses the challenge of high inter-channel variance in the transformer with
    negligible computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$70% compared to the previous SoTA.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ We further extend our method to BERT and vision transformers. It surpasses
    the previous best 4-bit quantized BERT by 7.8 points on GLUE dataset and achieves
    31.4 points higher accuracy compared to the previous SoTA ViT quantization method
    for 4-bit DeiT-S on ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Post-Training Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model quantization can be mainly categorized into quantization-aware training
    (QAT) and post-training quantization (PTQ), depending on whether it involves additional
    training for weight fine-tuning or not. Most PTQ studies are primarily focused
    on convolutional neural networks (CNNs) Nagel et al. ([2020](#bib.bib21)); Li
    et al. ([2021](#bib.bib17)); Wu et al. ([2020](#bib.bib30)); Cai et al. ([2020](#bib.bib5));
    Nagel et al. ([2019](#bib.bib22)). However, with the growing popularity of transformer-based
    models, only a limited number of works Bondarenko et al. ([2021](#bib.bib3));
    Yuan et al. ([2022](#bib.bib32)); Ding et al. ([2022](#bib.bib9)) have been conducted
    to realize PTQ on transformers. Moreover, the existing works primarily focus on
    visual transformer models and exhibit inferior performance when the bit width
    is below 8\. Therefore, in this work, we delve into the challenges of the low-bit
    PTQ for language transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Floating-Point Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Floating-point (FP) quantization has emerged as a promising alternative to integer
    quantization due to its ability to handle long-tail distributions, and offers
    increased flexibility Kuzmin et al. ([2022](#bib.bib14)). Additionally, modern
    GPUs such as H100 Micikevicius et al. ([2022](#bib.bib20)) now support FP quantization.
    Nonetheless, minimal research has been conducted on FP quantization. Only Kuzmin
    et al. ([2022](#bib.bib14)) proposes a general FP8 quantization scheme primarily
    for vision tasks, and Zhang et al. ([2023](#bib.bib33)) adopts a mixture of FP
    and INT formats quantization for LLMs. In this work, we propose FPQ baseline as
    a general guideline for low-bit floating-point PTQ to compress language transformer
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Formulation of Floating-Point Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A standard floating-point number is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X_{\rm{FP}}=(-1)^{s}2^{p-b}(1+\frac{d_{1}}{2}+\frac{d_{2}}{2^{2}}+...+\frac{d_{m}}{2^{m}})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $s\in\{0,1\}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c4ae43273e13a2b585456ae7d11f7ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of floating-point (FP) quantization process using
    FP5 (E2M2) positive axis. The real-valued clipped $X_{\rm R}^{\prime\prime}$ (Eq. [8](#S3.E8
    "In 3.2 Floating-Point Quantization Process ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Floating-Point Quantization Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In integer quantization, the real-valued variable $X_{\rm R}$ with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle X_{\rm INT}=\alpha\!\left\lfloor{\rm Clip}\!\left(\frac{X_{\rm
    R}}{\alpha},Q_{min},Q_{max}\!\right)\right\rceil$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\lfloor\cdot\rceil$ in two steps.
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Scale and clip. In FP quantization, we also scale and clip the real-valued
    variable before quantization as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\vspace{-0.5em}X_{\rm R}^{\prime}={\rm Clip}\!\left(X_{\rm
    R},Q_{min},Q_{max}\!\right)$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where the min/max value range of signed floating-point quantization can be
    calculated from Eq.[1](#S3.E1 "In 3.1 Formulation of Floating-Point Variables
    ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q_{max}=-Q_{min}=(2-2^{-m})2^{2^{e}-b-1}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Here the integer exponent bias $b$. Therefore, for simplicity, we reformulate
    Eq. [3](#S3.E3 "In 3.2 Floating-Point Quantization Process ‣ 3 Preliminaries ‣
    LLM-FP4: 4-Bit Floating-Point Quantized Transformers") as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\vspace{-0.4em}X_{\rm R}^{\prime\prime}={\rm Clip}\!\left(X_{\rm
    R},\tilde{Q}_{min},\tilde{Q}_{max}\!\right),$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\vspace{-0.4em}\tilde{Q}_{max}=\alpha Q_{max}$ |  | (6)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\alpha\cdot 2^{-b}\cdot(2-2^{-m})2^{2^{e}-0-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=2^{-\tilde{b}}\cdot(2-2^{-m})2^{2^{e}-0-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that we combine the tensor-wise real-valued scaling factor $\alpha$ from
    Eq. [6](#S3.E6 "In 3.2 Floating-Point Quantization Process ‣ 3 Preliminaries ‣
    LLM-FP4: 4-Bit Floating-Point Quantized Transformers") as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\vspace{-0.4em}\tilde{b}=2^{e}-{\rm log}_{2}{\tilde{Q}_{max}}+{\rm log}_{2}({2-2^{-m}})-1$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '(2) Compare and quantize. Different from integer quantization, which simply
    utilizes the rounding function to convert the real-valued variables to quantized
    ones, in floating-point quantization, there is an additional step of comparing
    $X_{\rm R}^{\prime\prime}$ with quantization levels and then quantize:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\vspace{-1em}X_{\rm{FP}}=\tilde{\alpha}\cdot v\cdot\left\lfloor\frac{X_{\rm
    R}^{\prime\prime}}{\tilde{\alpha}\cdot v}\right\rceil\vspace{-1em}$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $X_{\rm R}^{\prime\prime}$ is an integer power of 2.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'Here we select the quantization level $v$. Then the floating-point quantized
    variables can be derived with Eq.[8](#S3.E8 "In 3.2 Floating-Point Quantization
    Process ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").
    The illustration of the quantization process is in Fig. [1](#S3.F1 "Figure 1 ‣
    3.1 Formulation of Floating-Point Variables ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers"), detailed explanation can also be found
    in Micikevicius et al. ([2022](#bib.bib20)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Floating-Point Matrix Multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the floating-point quantized variables, the matrix multiplication is formulated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'Here in per-tensor activation quantization and per-channel weight quantization,
    $\mathbf{X}_{\rm FP}^{i,:}$ times the corresponding quantized activation and weight
    vectors. We depict all the possible quantization granularity options that support
    such efficient matrix multiplication in Appendix [D](#A4 "Appendix D Efficient
    Matrix Multiplication ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we begin by introducing our joint format and max value search,
    which establishes our strong baseline and already achieves state-of-the-art results
    at 8-bit and 6-bit quantization. Then we present an efficient pre-shifted exponent
    bias to tackle the catastrophic high inter-channel activation variance in transformer
    models and push the quantization limit to 4-bit.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Joint Format and Max Value Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The objective of post-training quantization is to minimize the perturbation
    ($\delta\mathbf{X}=\mathbf{X}_{\rm FP}-\mathbf{X}_{\rm R}$) introduced by quantization
    to the pre-trained real-valued network:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm min}\ \mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'In this study, we adopt the setting presented in Choukroun et al. ([2019](#bib.bib6));
    Wu et al. ([2020](#bib.bib30)), which assumes a positive correlation between the
    change in the intermediate output of the quantized model and Eq. [13](#S4.E13
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"). Therefore, minimizing the distance between the intermediate
    output of the quantized layer ($\hat{\mathbf{O}}$) leads to minimize Eq. [13](#S4.E13
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"). Hence, the objective loss metric is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm min}\ (\hat{\mathbf{O}}-\mathbf{O})^{2}$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: which is used to search for the optimal FP quantization function in the following
    proposed framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The challenges in FP quantization arise from its sensitivity to the quantization
    format and clipping range. Undesirable format selection will result in a catastrophic
    error rate. In addition, we observe that the optimal clipping range varies depending
    on the format used. Previous work Kuzmin et al. ([2022](#bib.bib14)) on floating-point
    (FP) quantization-aware training (QAT) proposed to learn both the FP format and
    maximum value with gradients. However, we find this method suffers from over-fitting
    in PTQ, with accuracy being even worse than naïve MinMax method, details can be
    found in Appendix [E](#A5 "Appendix E Learning Format and Maximum Value ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers"). Instead, we propose a search-based
    algorithm that jointly determines the optimal format and its associated clipping
    range to address this challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The searching process is conducted layer by layer with the metric of minimizing
    Eq. [14](#S4.E14 "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers"). The output of matrix multiplication
    corresponding to each sub-module is denoted as $\mathbf{O}=\mathbf{X}\mathbf{Y}$
    or another activation tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: The search space of $q$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The search process is outlined in Alg.[1](#alg1 "Algorithm 1 ‣ 4.1 Joint Format
    and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").
    We search the quantization scheme in all the matrix multiplication layers in parallel
    following Yuan et al. ([2022](#bib.bib32)); Bai et al. ([2022](#bib.bib2)). The
    algorithm can be divided into two parts. (1) Do forward propagation to store the
    intermediate raw output of each layer $l$. (2) Iteratively update the optimal
    format and biases for each layer for three rounds by minimizing the reconstruction
    metric (Eq. [14](#S4.E14 "In 4.1 Joint Format and Max Value Search ‣ 4 Method
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")). We name this search-based
    framework as Floating Point Quantization Baseline (FPQ baseline), and it can already
    achieve state-of-the-art results on both 8-bit and 6-bit settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 FPQ baseline
  prefs: []
  type: TYPE_NORMAL
- en: '1:Input: Calibration dataset, Full-precision Model $M$ that minimizes Eq.[14](#S4.E14
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers")15:     end for16:end for'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Pre-Shifted Exponent Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In transformer architectures, we observed an intriguing phenomenon of high
    inter-channel variance. As shown in Fig.[2](#S4.F2 "Figure 2 ‣ 4.2 Pre-Shifted
    Exponent Bias ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"),
    the magnitudes of values within the same channel are close to each other but exhibit
    significant differences across different channels. This phenomenon is not only
    observed in language models (i.e., LLaMA and BERT) but also significant in vision
    transformer models. Since outlier channels are often orders of magnitude bigger
    than the rest, they will dominate the quantization precision of the quantized
    tensor, resulting in less representation capacity for those channels with smaller
    magnitudes Xiao et al. ([2022](#bib.bib31)). This makes tensor-wise or token-wise
    scaling factor insufficient for accurate activations quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, applying per-channel scaling factors for activations poses challenges
    to efficient matrix multiplication, because the scaling factor is not a shared
    constant along the multiplication direction and cannot be extracted as Eq. [12](#S3.E12
    "In 3.3 Floating-Point Matrix Multiplication ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers"). To address this challenge, we introduce
    pre-shifted exponent bias, which allows us to calculate per-channel scaling factors
    from activations. These scaling factors are then re-parameterized as the exponent
    biases of the corresponding weights. This method effectively handles high inter-channel
    variance while maintaining nearly identical efficiency to per-tensor quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recalling in Eq. [7](#S3.E7 "In 3.2 Floating-Point Quantization Process ‣ 3
    Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"), we extracted
    the tensor-wise integer exponent bias $b$. Then, the floating-point quantization
    formula in Eq. [15](#S4.E15 "In 4.2 Pre-Shifted Exponent Bias ‣ 4 Method ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\vspace{-0.4em}\!\!X_{\rm FP}\!=\!2^{-\tilde{b}}(-1)^{s}2^{p-0}(1+\!\frac{d_{1}}{2}+\frac{d_{2}}{2^{2}}+...+\frac{d_{m}}{2^{m}})$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: We note that after the bias is absorbed in the scaling factor, the original
    bias term ($b^{ori}$).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75dcd1de36ce945c770391fc687f66bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Magnitude of the output activations of the feed-forward network blocks
    in LLaMA-7B, BERT, and DeiT.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75a05aade796e55f730e4f5517d51ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of pre-shifted exponent bias method: (a) Search phase: The
    real-valued channel-wise scaling exponent bias for activations ($\tilde{\mathbf{b}}_{j}$
    are re-parameterized into the weight tensor. The weights are pre-computed to apply
    the bias, therefore this is a one-time cost. (c) Inference phase: The method leverages
    efficient matrix multiplication between low-bit floating-point matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the calculation of the channel-wise integer bias vector ($\mathbf{b}^{ori}$)
    from the per-channel maximum values:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\vspace{-0.4em}\!\!\!\tilde{\mathbf{b}}_{j}\!=\!2^{e}\!-\!{\rm log}_{2}({{\rm
    max}(&#124;\mathbf{X}^{:,j}_{\rm R}&#124;})\!)\!+\!{\rm log}_{2}({2\!-\!2^{-m}})\!-\!1$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'Here $\mathbf{X}^{:,j}_{\rm R}$ to a tensor-wise real-valued scaling factor
    plus a channel-wise integer scaling factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\vspace{-0.4em}\tilde{\mathbf{b}}$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\tilde{\rho}+clip(\lfloor\tilde{\mathbf{b}}-\tilde{\rho}\rceil,0,2^{e-1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\tilde{\rho}\in\mathbb{R}^{1}$ can be rewrote as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\vspace{-0.4em}\!\!X_{\rm FP}$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the bias $\mathbf{b}^{ori}$ becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\!\!X_{\rm FP}$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 'and the corresponding weight element in $j^{th}$ becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: 'As result, efficient matrix multiplication in Eq.[12](#S3.E12 "In 3.3 Floating-Point
    Matrix Multiplication ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized
    Transformers") is reformulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\odot$].
  prefs: []
  type: TYPE_NORMAL
- en: Combining pre-shifted exponent bias method with the joint format and max-value
    search framework(FPQ baseline), we name our method as (FPQ), short for Floating
    Point Quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To validate the effectiveness of the proposed method, we conduct experiments
    on LLaMA Touvron et al. ([2023](#bib.bib26)) and BERT Devlin et al. ([2019](#bib.bib8))
    models in [5.2.1](#S5.SS2.SSS1 "5.2.1 LLM Zero-Shot Reasoning ‣ 5.2 Main Results
    ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") and Sections [5.2.2](#S5.SS2.SSS2
    "5.2.2 BERT Model ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"). Further, in Section [5.2.3](#S5.SS2.SSS3 "5.2.3 Generalizability
    on Vision Transformer ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers") we show that our method also generalizes well to vision
    transformer architectures. We present ablation studies on the calibration size
    and search range in Section [5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"), and analyze the hardware
    costs of implementing FP operators in Section [5.4](#S5.SS4 "5.4 Hardware Cost
    ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experiments Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We adopt per-tensor quantization for activation and per-channel quantization
    for weight. We employ layer reconstruction following the settings of Yuan et al.
    ([2022](#bib.bib32)); Nagel et al. ([2020](#bib.bib21)), and parallel quantization
    based on the approach outlined in Bai et al. ([2022](#bib.bib2)); Yuan et al.
    ([2022](#bib.bib32)). A more detailed discussion regarding our implementation
    decisions can be found in Appendix [F](#A6 "Appendix F Reconstruction Choices
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"). For LLaMA models, we
    quantize all the weight and activation tensors in fully-connected layers for a
    fair comparison with previous work Xiao et al. ([2022](#bib.bib31)); Liu et al.
    ([2023](#bib.bib18)). For BERT and ViT models, both fully-connected layers and
    activation-activation multiplication tensors in the self-attention module are
    quantized. Note that for FPQ on BERT Devlin et al. ([2019](#bib.bib8)) and ViTs
    models, the reconstruction metric Eq. [14](#S4.E14 "In 4.1 Joint Format and Max
    Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")
    is substituted with a Hessian approximation loss metric. This substitution is
    further detailed in Appendix [A](#A1 "Appendix A Hessian-Based Loss Metric ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Quant Method | #Bits (E/W/A) | # Calib | BoolQ | PIQA | HellaSwag | WinoGrande
    | ARC-e | ARC-c | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B Full-precision | 16/16/16 | - | 75.1 | 78.7 | 56.9 | 69.9 | 75.3
    | 41.9 | 66.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 8/8/8 | 32 | 64.3 | 66.8 | 40.5 | 57.4 | 59.0 | 29.6 |
    52.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E4M3) | 8/8/8 | 32 | 74.9 | 78.6 | 56.8 | 69.5 | 75.5 |
    41.6 | 66.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/8/8 | 512 | 74.0 | 77.5
    | 55.0 | 69.6 | 74.4 | 37.4 | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 8/8/8 | 32 | 75.8 | 78.3 | 55.9 | 69.5 | 75.6 | 41.3 | 66.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 8/8/8 | 32 | 75.6 | 78.2 | 56.6 | 70.2 | 74.6 | 40.7 | 66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/16 | 32 | 64.1 | 76.1 | 51.6 | 66.3 | 72.4 | 40.0
    | 61.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1) | 4/4/16 | 32 | 73.0 | 77.9 | 55.2 | 69.1 | 73.6 |
    40.9 | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| GPTQ Frantar et al. ([2023](#bib.bib11)) | 4/4/16 | 128 | 73.3 | 77.9 | 54.9
    | 67.9 | 72.7 | 37.4 | 64.0 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/16 | 32 | 74.8 | 77.9 | 55.6 | 69.5 | 75.2 | 41.0 | 65.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/16 | 32 | 74.2 | 77.8 | 55.8 | 69.9 | 74.9 | 40.4 | 65.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/8 | 32 | 50.4 | 56.5 | 27.9 | 46.5 | 36.1 | 21.2 |
    39.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1/E4M3) | 4/4/8 | 32 | 73.0 | 77.5 | 55.0 | 69.3 | 73.6
    | 40.9 | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/8 | 32 | 75.0 | 77.6 | 55.9 | 69.9 | 74.3 | 39.4 | 65.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/8 | 32 | 75.0 | 77.7 | 55.5 | 69.8 | 74.5 | 39.9 | 65.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/4 | 32 | 54.1 | 51.7 | 25.6 | 49.8 | 24.7 | 22.9 |
    38.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1) | 4/4/4 | 32 | 47.3 | 53.1 | 25.7 | 50.7 | 25.1 |
    22.4 | 37.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/4/4 | 512 | 54.1 | 62.8
    | 41.5 | 52.6 | 50.6 | 32.9 | 49.1 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-QAT Liu et al. ([2023](#bib.bib18)) | 16/4/4 | (QAT) | 63.5 | 64.3 |
    55.6 | 52.9 | 50.3 | 30.2 | 52.8 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/4 | 32 | 57.4 | 56.6 | 30.2 | 51.1 | 37.7 | 23.2 | 42.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/4 | 32 | 64.2 | 73.5 | 47.8 | 63.7 | 65.9 | 33.6 | 58.1 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13B Full-precision | 16/16/16 | - | 77.9 | 79.2 | 59.9 | 72.6 | 77.4
    | 46.4 | 68.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 8/8/8 | 32 | 60.6 | 69.6 | 46.0 | 61.5 | 63.3 | 32.8 |
    55.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E4M3) | 8/8/8 | 32 | 78.0 | 79.1 | 60.0 | 72.3 | 77.2 |
    47.1 | 68.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/8/8 | 512 | 76.5 | 78.0
    | 58.0 | 72.1 | 76.3 | 45.5 | 68.2 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 8/8/8 | 32 | 78.0 | 79.1 | 59.9 | 72.3 | 77.2 | 47.1 | 68.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 8/8/8 | 32 | 78.1 | 78.5 | 59.1 | 72.4 | 76.4 | 46.1 | 68.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/8 | 32 | 52.1 | 65.0 | 36.4 | 53.9 | 52.3 | 29.0 |
    48.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1/E4M3) | 4/4/8 | 32 | 78.0 | 78.9 | 58.0 | 71.6 | 76.0
    | 44.8 | 67.9 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/8 | 32 | 76.2 | 78.2 | 57.9 | 71.9 | 75.1 | 43.9 | 67.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/8 | 32 | 76.4 | 78.5 | 58.2 | 72.1 | 75.2 | 44.7 | 67.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/4 | 32 | 54.5 | 52.7 | 25.5 | 51.1 | 25.3 | 22.1 |
    38.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1) | 4/4/4 | 32 | 45.8 | 51.7 | 25.5 | 49.5 | 25.0 |
    22.8 | 36.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/4/4 | 512 | 57.6 | 61.3
    | 56.0 | 52.6 | 49.9 | 25.1 | 50.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/4 | 32 | 54.3 | 57.7 | 35.7 | 52.2 | 41.1 | 25.7 | 44.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/4 | 32 | 71.9 | 74.8 | 53.3 | 66.7 | 71.7 | 39.9 | 63.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Zero-shot performance on common sense reasoning tasks with LLaMA Touvron
    et al. ([2023](#bib.bib26)) models. We denote E/W/A as the bit-width of word embeddings,
    model weight and activations, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Quant Method | #Bits (E/W/A) | # Calib | MNLI[-m] | QQP | QNLI | SST-2 |
    CoLA | STS-B | MRPC | RTE | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| (Full-precision) | 32-32-32 | - | 84.9 | 91.4 | 92.1 | 93.2 | 59.7 | 90.1
    | 86.3 | 72.2 | 83.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 8/8/8 | 128 | 77.0 | 89.9 | 88.9 | 92.9 | 51.8 | 88.2
    | 83.8 | 71.5 | 80.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M5) | 8/8/8 | 128 | 78.9 | 90.8 | 88.6 | 92.9 | 52.7 |
    88.4 | 84.3 | 69.0 | 80.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E3M4) | 8/8/8 | 128 | 84.5 | 90.9 | 91.5 | 93.2 | 58.3 |
    89.3 | 87.7 | 71.8 | 83.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E4M3) | 8/8/8 | 128 | 84.7 | 90.9 | 91.7 | 93.0 | 58.6 |
    89.3 | 86.5 | 72.2 | 83.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E5M2) | 8/8/8 | 128 | 84.1 | 90.9 | 91.4 | 93.6 | 58.1 |
    89.2 | 87.5 | 71.8 | 83.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 8/8/8 | 128 | 84.6 | 90.9 | 91.7 | 93.1 | 58.6 | 89.3 | 88.0
    | 72.2 | 83.5 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 8/8/8 | 128 | 84.6 | 91.0 | 91.6 | 93.3 | 58.8 | 89.3 | 88.0 | 72.2
    | 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 6/6/6 | 128 | 31.9 | 62.0 | 52.8 | 58.8 | 0.0 | 12.7 |
    32.1 | 52.7 | 37.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M3) | 6/6/6 | 128 | 43.5 | 85.4 | 79.4 | 90.5 | 45.2 |
    86.0 | 66.9 | 59.9 | 69.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E3M2) | 6/6/6 | 128 | 83.9 | 90.8 | 90.8 | 92.2 | 58.2 |
    88.6 | 87.0 | 72.2 | 83.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E4M1) | 6/6/6 | 128 | 84.4 | 90.2 | 90.1 | 92.2 | 58.2 |
    89.2 | 85.3 | 69.7 | 82.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 6/6/6 | 128 | 84.6 | 90.9 | 91.2 | 93.2 | 58.8 | 88.7 | 87.5
    | 70.8 | 83.2 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 6/6/6 | 128 | 84.5 | 90.8 | 91.6 | 93.1 | 57.3 | 89.3 | 88.7 | 71.8
    | 83.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/8 | 128 | 33.1 | 63.8 | 60.1 | 49.3 | 0.0 | 44.0 |
    50.2 | 49.1 | 43.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1) | 4/4/8 | 128 | 60.6 | 70.9 | 77.4 | 79.9 | 5.5 |
    78.6 | 46.8 | 56.6 | 59.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MREM-S Bai et al. ([2022](#bib.bib2)) | 4/4/8 | 4096 | 83.5 | 90.2 | 91.2
    | 91.4 | 55.1 | 89.1 | 84.8 | 71.8 | 82.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MREM-P Bai et al. ([2022](#bib.bib2)) | 4/4/8 | 4096 | 83.4 | 90.2 | 91.0
    | 91.5 | 54.7 | 89.1 | 86.3 | 71.1 | 82.2 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/8 | 128 | 84.4 | 90.6 | 91.4 | 92.9 | 58.6 | 83.7 | 88.2
    | 73.3 | 82.9 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/8 | 128 | 84.5 | 90.6 | 91.1 | 92.7 | 58.8 | 89.3 | 88.7 | 73.3
    | 83.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax INT Quant | 4/4/4 | 128 | 31.8 | 39.7 | 50.5 | 49.1 | 0.0 | 6.7 |
    31.6 | 54.5 | 32.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MinMax FP Quant (E2M1) | 4/4/4 | 128 | 33.6 | 54.0 | 50.6 | 50.8 | 0.0 |
    0.0 | 31.6 | 52.0 | 34.1 |'
  prefs: []
  type: TYPE_TB
- en: '| BrecQ Li et al. ([2021](#bib.bib17)) | 8/4/4 | 4096 | 31.9 | 62.3 | 50.7
    | 50.9 | 0.9 | 6.4 | 31.7 | 52.3 | 35.8 |'
  prefs: []
  type: TYPE_TB
- en: '| QDrop Wei et al. ([2022](#bib.bib29)) | 8/4/4 | 4096 | 71.4 | 79.0 | 76.8
    | 88.1 | 40.9 | 81.9 | 79.2 | 60.7 | 72.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ baseline | 4/4/4 | 128 | 38.9 | 68.3 | 55.3 | 83.6 | 10.6 | 0.0 | 43.8
    | 55.2 | 44.5 |'
  prefs: []
  type: TYPE_TB
- en: '| FPQ | 4/4/4 | 128 | 82.3 | 89.2 | 86.6 | 91.5 | 52.6 | 85.5 | 83.8 | 69.0
    | 80.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results on the GLUE development set with BERT Bai et al. ([2022](#bib.bib2))
    model. We denote E/W/A as the bit-width of word embeddings, model weight and activations,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 LLM Zero-Shot Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the effectiveness of FPQ for LLaMA-7B/ LLaMA-13B Touvron et al.
    ([2023](#bib.bib26)) on common sense zero-shot reasoning tasks. For the calibration
    data, we sample 32 random segments with 2048 tokens length from the C4 Raffel
    et al. ([2020](#bib.bib24)) dataset following the setting of GPTQ Frantar et al.
    ([2023](#bib.bib11)). The data preprocessing and score calculation are based on
    EleutherAI evaluation harness¹¹1https://github.com/EleutherAI/lm-evaluation-harness.
    In Table [1](#S5.T1 "Table 1 ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers"), we compare FPQ to the floating-point
    PTQ baselines, and state-of-the-art PTQ and QAT methods, including SmoothQuant Xiao
    et al. ([2022](#bib.bib31)) and GPTQ Frantar et al. ([2023](#bib.bib11)), and
    LLM-QAT Liu et al. ([2023](#bib.bib18)).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, all methods, except for the naïve MinMax INT Quantization, produce
    comparable outcomes in the 8-bit setting on both LLaMA-7B and LLaMA-13B. Additionally,
    we observe that the naïve MinMax FP Quantization achieves nearly lossless results
    and even surpasses the state-of-the-art integer post-training quantization method,
    SmoothQuant (Xiao et al., 2022), which indicates that floating-point quantization
    naturally has a strong capability in handling the distributions in transformers.
    However, both MinMax FP Quant and FPQ baseline fail when pushing the quantization
    precision to ultra-low 4/4/4 bit setting, with $28.9\%$ accuracy drop on LLaMA-7B/13B
    with 4/4/4 bit-width, outperforming SmoothQuant Xiao et al. ([2022](#bib.bib31))
    by a large margin, yet with less bit-width and smaller calibration size. Moreover,
    FPQ even achieves 5.3% accuracy improvements compared to LLM-QAT Liu et al. ([2023](#bib.bib18))
    in the 4/4/4 setting and 1.5% over GPTQ Frantar et al. ([2023](#bib.bib11)) in
    the 4/4/16 configuration on LLaMA-7B.
  prefs: []
  type: TYPE_NORMAL
- en: For practitioners, a crucial consideration is determining the appropriate quantization
    methods for various bit-widths. Therefore, based on our findings, we offer two
    recommendations that balance the trade-off between accuracy and search/optimization
    efficiency. First of all, since the difference between MinMax FP Quant and the
    rest of the methods is marginal for the 8/8/8 setting, we recommend simply using
    the MinMax FP Quant method for the 8/8/8 setting as the MinMax method does not
    involve search process. However, for more demanding scenarios, especially with
    activation quantization to 4 bits, we recommend employing FPQ for minimizing accuracy
    degradation with negligible inference overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 BERT Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the proposed quantization techniques for BERT model on GLUE tasks
    Wang et al. ([2019](#bib.bib28)). Full-precision BERT-base models fine-tuned on
    GLUE datasets are obtained from Huggingface public repository²²2https://huggingface.co/textattack/bert-base-uncased-{TASK_NAME}.
    We randomly sample 128 data from the training set as the calibration set. In Table
    [2](#S5.T2 "Table 2 ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"), FPQ demonstrates remarkable performance, achieving absolute
    average accuracy improvements of $44.3\%$ over QDrop Wei et al. ([2022](#bib.bib29))
    with 4/4/4 bit setting. Further, with 4-bit weight and 8-bit activation, MREM-S/MREM-P Bai
    et al. ([2022](#bib.bib2)) present a 1.6/1.5% accuracy gap to the full-precision
    model with 4096 calibration data, while FPQ achieves almost no accuracy loss with
    only 128 calibration data points.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Generalizability on Vision Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on our findings that vision transformers also exhibit a consistent activation
    distribution pattern as language transformers, characterized by high inter-channel
    variance and low intra-channel variance, as detailed in Fig. [2](#S4.F2 "Figure
    2 ‣ 4.2 Pre-Shifted Exponent Bias ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized
    Transformers"), we extended our proposed methods to ViT and compared FPQ with
    floating-point PTQ baselines and state-of-the-art PTQ method for ViT on the ImageNet
    classification task. Table [3](#S5.T3 "Table 3 ‣ 5.2.3 Generalizability on Vision
    Transformer ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers") shows that findings on ViT are consistent with that on
    language models: previous state-of-the-art integer-based methods struggled to
    maintain reasonable accuracy when quantizing the transformer to lower bits. In
    comparison, the proposed FPQ outperformed both PTQ4ViT and APQ-ViT on 6 bits,
    and also achieved 40.9% and 31.5% absolute accuracy improvement over PTQ4ViT and
    APQ-ViT on DeiT-S in the 4-bit configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '| W/A | Quant Method | Deit-S | Deit-B | ViT-S |'
  prefs: []
  type: TYPE_TB
- en: '| Full-prec | - | 79.9 | 81.8 | 81.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6 | PTQ4ViTYuan et al. ([2022](#bib.bib32)) | 76.3 | 80.3 | 78.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6 | APQ-ViTDing et al. ([2022](#bib.bib9)) | 77.8 | 80.4 | 79.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6 | MinMax FP Quant (E3M2) | 79.3 | 81.7 | 80.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6 | FPQ baseline | 79.43 | 81.7 | 80.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6 | FPQ | 79.5 | 81.8 | 81.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4 | PTQ4ViTYuan et al. ([2022](#bib.bib32)) | 34.1 | 64.4 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4 | APQ-ViT Ding et al. ([2022](#bib.bib9)) | 43.6 | 67.5 | 48.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4 | MinMax FP Quant (E2M1) | 0.4 | 0.1 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4 | FPQ baseline | 6.57 | 0.71 | 0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4 | FPQ | 75.0 | 79.4 | 73.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison on the ImageNet dataset with vision transformer structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we first compare the influence of different calibration sizes
    on FPQ. We vary the calibration size in $\{32,64,128,256\}$ and test on MNLI,
    QQP, and CoLA. Table [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Study ‣ 5 Experiments
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") shows that the evaluation
    on MNLI and QQP is more robust to different settings, and the variance is more
    significant on CoLA. We observe that FPQ performs well with a calibration set
    size of 128 data points. However, we also find that it remains robust and maintains
    competitive accuracy even with limited access to calibration data, such as when
    using as few as 32 data points.'
  prefs: []
  type: TYPE_NORMAL
- en: We investigate the robustness of FPQ to different search ranges $(\gamma_{1},\gamma_{2})$,
    as long as the search range is not overly aggressive.
  prefs: []
  type: TYPE_NORMAL
- en: '| E/W/A | #Calib | MNLI-M | QQP | CoLA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 32 | 81.5 | 89.4 | 44.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 64 | 81.8 | 89.4 | 47.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 128 | 82.3 | 89.2 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 256 | 81.9 | 89.0 | 52.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 32 | 84.8 | 90.8 | 55.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 64 | 84.7 | 90.9 | 58.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 128 | 84.5 | 90.8 | 57.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 256 | 84.6 | 90.8 | 57.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Ablation studies of different calibration sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| E/W/A | $\gamma_{{}_{1}}$ | MNLI-M | QQP | CoLA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 0.01, 1.2 | 82.3 | 89.2 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 0.1, 1.2 | 82.2 | 89.1 | 53.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4/4/4 | 0.5, 1.5 | 82.3 | 88.4 | 52.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 0.01, 1.2 | 84.5 | 90.8 | 57.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 0.1,1.2 | 84.7 | 90.8 | 57.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 6/6/6 | 0.5,1.5 | 84.7 | 90.8 | 57.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Ablation studies of different search range.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Hardware Cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We further examine the hardware utilization of low-bit INT, FP, and mixed-format
    FP multiplication operators, including adder, multiplier, and multiply-accumulate
    (MAC) units, in terms of hardware area. Mixed-format FP refers to the multiplication
    of floating-point numbers with different formats, e.g., E2M1 multiplies with E1M2\.
    We implemented the MAC operator by Verilog HDL and utilized Cadence Genus to obtain
    the synthesized area under TSMC 40nm technology and 0.5GHz clock frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.4 Hardware Cost ‣ 5 Experiments ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers") illustrates the hardware cost of the INT
    and FP operators, with the multiplier being the primary cost for INT and the adder
    for FP. Notably, the disparity between FP4 and INT4 adders is small, while INT
    has twice the hardware cost for the multiplier. Moreover, the mixed-format FP4
    operator has comparable hardware area as the standard FP4 operator. These findings
    indicate that the proposed FPQ approach imposes negligible overhead in terms of
    hardware implementation when compared to the standard FP operators and the hardware
    cost for FP is comparable with INT.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | Adder($\mu m^{2}$) |'
  prefs: []
  type: TYPE_TB
- en: '| INT4 | 93 | 182 | 410 |'
  prefs: []
  type: TYPE_TB
- en: '| INT6 | 132 | 340 | 529 |'
  prefs: []
  type: TYPE_TB
- en: '| E2M1 | 111 | 92 | 443 |'
  prefs: []
  type: TYPE_TB
- en: '| E3M2 | 223 | 138 | 498 |'
  prefs: []
  type: TYPE_TB
- en: '| E2M1 * E1M2 | 105 | 107 | 432 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Area differences of INT, FP and mixed Format FP operators across different
    bit-widths.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents the first successful demonstration of 4-bit floating-point
    post-training quantization for weights, activations, and embeddings in natural
    language transformer architectures, including both large language models and BERT
    model. We also extend our method to vision transformers and observe its robust
    generalization ability. Our approach involves a practical search-based technique
    which establishes a strong baseline and achieves state-of-the-art results for
    6-bit and 8-bit quantization. Furthermore, we address the challenge of high inter-channel
    variance in transformers by proposing pre-shifted exponent bias, which proves
    highly effective in achieving accurate 4-bit quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is supported by National Natural Science Foundation of China/
    HKSAR Research Grants Council Joint Research Scheme under Grant $NHKUST627/20$.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our experiments were conducted on publicly available datasets with finite sentence
    lengths, and the generalizability of our method to extremely long sequences or
    streaming data has not been verified and may require further investigation. In
    addition, it remains to be seen how our proposed method can generalize to other
    domains beyond language and vision, such as audio. It would also be interesting
    to see the applicability of our method to generative tasks and other applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Akbari et al. (2021) Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang,
    Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021. Vatt: Transformers for multimodal
    self-supervised learning from raw video, audio and text. *Advances in Neural Information
    Processing Systems*, 34:24206–24221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022) Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and
    Michael Lyu. 2022. [Towards efficient post-training quantization of pre-trained
    language models](https://openreview.net/forum?id=tvDRmAxGIjw). In *Advances in
    Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2021. [Understanding and overcoming the challenges of efficient transformer quantization](http://arxiv.org/abs/2109.12948).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2020) Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W
    Mahoney, and Kurt Keutzer. 2020. Zeroq: A novel zero shot quantization framework.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 13169–13178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choukroun et al. (2019) Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
    2019. [Low-bit quantization of neural networks for efficient inference](http://arxiv.org/abs/1902.06822).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [Bert: Pre-training of deep bidirectional transformers for language
    understanding](http://arxiv.org/abs/1810.04805).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2022) Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie
    Liu, Xiaolin Wei, and Xianglong Liu. 2022. [Towards accurate post-training quantization
    for vision transformer](https://doi.org/10.1145/3503161.3547826). In *Proceedings
    of the 30th ACM International Conference on Multimedia*, MM ’22, page 5380–5388,
    New York, NY, USA. Association for Computing Machinery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(10) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. [GPTQ: Accurate post-training compression for generative pretrained
    transformers](https://openreview.net/forum?id=tcbBPnfwxS). In *International Conference
    on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of NAACL-HLT*, pages 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuzmin et al. (2022) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel,
    Jorn Peters, and Tijmen Blankevoort. 2022. Fp8 quantization: The power of the
    exponent. *Advances in Neural Information Processing Systems*, 35:14651–14662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023) Jemin Lee, Yongin Kwon, Jeman Park, Misun Yu, and Hwanjun
    Song. 2023. [Q-hyvit: Post-training quantization for hybrid vision transformer
    with bridge block reconstruction](http://arxiv.org/abs/2303.12557).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    Bart: Denoising sequence-to-sequence pre-training for natural language generation,
    translation, and comprehension. In *Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics*, pages 7871–7880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. 2021. Brecq: Pushing the limit of post-training
    quantization by block reconstruction. *arXiv preprint arXiv:2102.05426*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. 2021. Post-training quantization for vision transformer. *Advances in
    Neural Information Processing Systems*, 34:28092–28103.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micikevicius et al. (2022) Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi,
    Michael Siu, and Hao Wu. 2022. [Fp8 formats for deep learning](http://arxiv.org/abs/2209.05433).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagel et al. (2019) Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max
    Welling. 2019. [Data-free quantization through weight equalization and bias correction](http://arxiv.org/abs/1906.04721).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Touvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
    Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. Training data-efficient
    image transformers & distillation through attention. In *International conference
    on machine learning*, pages 10347–10357\. PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2019. [Glue: A multi-task benchmark and analysis
    platform for natural language understanding](http://arxiv.org/abs/1804.07461).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei
    Yu. 2022. [QDrop: Randomly dropping quantization for extremely low-bit post-training
    quantization](https://openreview.net/forum?id=ySQH0oDyp7). In *International Conference
    on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, and Debing
    Zhang. 2020. [Easyquant: Post-training quantization via scale optimization](http://arxiv.org/abs/2006.16669).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. 2022. Smoothquant: Accurate and efficient post-training quantization
    for large language models. *arXiv preprint arXiv:2211.10438*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2022) Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu
    Sun. 2022. Ptq4vit: Post-training quantization for vision transformers with twin
    uniform quantization. In *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XII*, pages 191–207\.
    Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. 2023. [Integer or floating
    point? new outlooks for low-bit quantization on large language models](http://arxiv.org/abs/2305.12356).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Hessian-Based Loss Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The objective of post-training quantization is to minimize the perturbation
    ($\delta\mathbf{X}=\mathbf{X}_{\rm FP}-\mathbf{X}_{\rm R}$) introduced by quantization
    to the pre-trained real-valued network:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm min}\ \mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: Following the Taylor series expansion, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\approx$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\approx$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\bar{\mathbf{g}}^{(\mathbf{X})}$ can be neglected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hessian matrix $\bar{\mathbf{H}}^{(\mathbf{X})}$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bar{\mathbf{H}}^{(\mathbf{X})}=\mathbf{J}^{T}_{\mathbf{O}}(\mathbf{X})\bar{\mathbf{H}}^{(\mathbf{O})}\mathbf{J}_{\mathbf{O}}(\mathbf{X})$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{J}_{\mathbf{O}}(\mathbf{X})$. We then substitute the above equation
    back to equation LABEL:eq:target :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\ \delta\mathbf{X}^{T}\bar{\mathbf{H}}^{(\mathbf{X})}\delta\mathbf{X}$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\approx$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here $\hat{\mathbf{O}}$ using first-order Taylor expansion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, the calculation of $\bar{\mathbf{H}}^{(\mathbf{O})}$ following
    Li et al. ([2021](#bib.bib17)); Yuan et al. ([2022](#bib.bib32)), and the new
    Hessian-based metric becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}[(\hat{\mathbf{O}}-\mathbf{O})^{T}diag((\frac{\partial
    L}{\partial\mathbf{O}_{1}})^{2},...,(\frac{\partial L}{\partial\mathbf{O}_{n}})^{2}(\hat{\mathbf{O}}-\mathbf{O})]$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: Here, each entry of $\mathbf{O}$. In this study, this hessian-based metric is
    used as the reconstruction metric to search for the optimal FP quantization function
    for both the weight and activation when performing layer-wise reconstruction in
    BERT and Vision Transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/689372918aada6548f5b9f3b4765008c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Quantization error of different formats for BERT layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Quantization Error of Different Floating-Point Formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Hessian-Based Loss Metric ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") compares the quantization error
    of different formats in 8-bit quantization, including ${\rm INT8}$. We apply these
    formats to different BERT modules in the first, fifth, and last layers. The figures
    demonstrate that the optimal FP formats differs depending on the specific module
    that we are quantizing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fac5540ed69ca0ec2715181fec28ffad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Magnitude of the output activations of different modules in BERT
    (left column), and DeiT-S (right column).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3a4aa923e6fac0022f319db27bc8dd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Magnitude of the output activations of different modules in LLaMA-7B.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Inter-Channel Variance Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Quantization Error of Different Floating-Point
    Formats ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") and [6](#A2.F6
    "Figure 6 ‣ Appendix B Quantization Error of Different Floating-Point Formats
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") depict the output of
    different fully-connected layers in BERT for the MNLI task, DeiT-S for the ImageNet-1K
    task, and LLaMA-7B for the zero-shot reasoning task. The visualizations reveal
    a noticeable inter-channel variance presented in both language and vision transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Efficient Matrix Multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [7](#A4.F7 "Figure 7 ‣ Appendix D Efficient Matrix Multiplication ‣
    LLM-FP4: 4-Bit Floating-Point Quantized Transformers") displays a comprehensive
    list of all the granularity options that allow for efficient matrix multiplication.
    While per-token quantization theoretically provides greater precision in terms
    of quantization granularity, the accuracy gains achieved through this method are
    minimal and do not justify the additional computational overhead required. As
    a result, we have opted to use per-tensor quantization when quantizing activations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/89de879feeeadcfbbc0364055b04de22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Quantization granularity options that support efficient matrix multiplication.
    The dimensions that share the same scaling factor are indicated with red dotted
    frames'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Learning Format and Maximum Value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We compare the previous gradient-based method Kuzmin et al. ([2022](#bib.bib14))
    with the proposed search-based method for finding the optimal format and maximum
    value. On DeiT-S, the learnable method only achieves 74.38% accuracy for an 8-bit
    quantized model on ImageNet, in contrast, FPQ can attain an almost loss-less result
    of 79.88%. We analyze the gradients for the number of exponent bits $e$ derived
    in Kuzmin et al. ([2022](#bib.bib14)) and observe that each time the exponent
    bits change, the gradients experience exponential variations, leading to high
    instability. Based on this observation, we assert that employing a search-based
    method to determine the optimal formats is crucial in post-training quantization
    (PTQ).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Reconstruction Choices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous works on integer post-training quantization involves breaking
    down the target model into sub-modules and reconstructing them separately Nagel
    et al. ([2020](#bib.bib21)); Li et al. ([2021](#bib.bib17)); Bai et al. ([2022](#bib.bib2));
    Yuan et al. ([2022](#bib.bib32)). This addresses the problem of over-fitting,
    given that only a limited amount of unlabeled calibration data is available. In
    this study we find the layer-wise reconstruction and parallel quantization works
    best for floating-point PTQ:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer Reconstruction: Recent research Li et al. ([2021](#bib.bib17)); Bai et al.
    ([2022](#bib.bib2)) suggests increasing the reconstruction granularity from layer
    reconstruction Nagel et al. ([2020](#bib.bib21)) to block reconstruction Li et al.
    ([2021](#bib.bib17)) or even larger granularity Lee et al. ([2023](#bib.bib15)).
    This is achieved by jointly optimizing all the linear layers or matrix multiplication
    components within each module to prevent the propagation of reconstruction errors
    among the layers. Despite this, we have observed that increasing the reconstruction
    granularity does not improve the accuracy of FPQ baseline or sometimes even lead
    to worse results. Therefore, we choose layer reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel Quantization: Sequential quantization is the most commonly used approach
    Wu et al. ([2020](#bib.bib30)); Nagel et al. ([2020](#bib.bib21)); Li et al. ([2021](#bib.bib17))
    where modules are quantized consecutively based on their sequential order, and
    the input for the current calibrating module is generated using all the previously
    quantized modules. However, some recent works Yuan et al. ([2022](#bib.bib32));
    Bai et al. ([2022](#bib.bib2)) proposed a new parallel quantization framework.
    This framework uses the raw output of the full-precision modules as input and
    makes the calibration of each module independent from one another. In this work,
    we use parallel quantization, as it yields better results than its sequential
    counterparts.'
  prefs: []
  type: TYPE_NORMAL
