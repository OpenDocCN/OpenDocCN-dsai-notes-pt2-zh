- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:04:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Pruning and Distillation in Practice: The Minitron Approach'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11796](https://ar5iv.labs.arxiv.org/html/2408.11796)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \correspondingauthor
  prefs: []
  type: TYPE_NORMAL
- en: X
  prefs: []
  type: TYPE_NORMAL
- en: Sharath Turuvekere Sreenivas Saurav Muralidharan Raviraj Joshi Marcin Chochowski
    Mostofa Patwary Mohammad Shoeybi Bryan Catanzaro Jan Kautz and Pavlo Molchanov
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Abstract: We present a comprehensive report on compressing the Llama 3.1 8B
    and Mistral NeMo 12B models to 4B and 8B parameters, respectively, using pruning
    and distillation [[1](#bib.bib1)]. We explore two distinct pruning strategies:
    (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate
    the results on common benchmarks from the LM Evaluation Harness [[2](#bib.bib2)].
    The models are then aligned with NeMo Aligner and tested in instruct-tuned versions.
    This approach produces a compelling 4B model from Llama 3.1 8B and a state-of-the-art
    Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo
    12B. We found that with no access to the original data, it is beneficial to slightly
    fine-tune teacher models on the distillation dataset. We open-source our base
    model weights on Hugging Face with a permissive license.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models on Hugging Face: [Mistral-NeMo-Minitron-8B-Base](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base)
    | [Llama-3.1-Minitron-4B-Width-Base](https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base)
    | [Llama-3.1-Minitron-4B-Depth-Base](https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Depth-Base)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  Benchmarks(shots)  |  Gemma2  |  Minitron  |  Llama-3.1-Minitron  |  Gemma  |  Mistral  |  Llama
    3.1  |  MN-Minitron  |  Mistral NeMo  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2B* | 4B | 4B-Depth | 4B-Width | 7B | 7B | 8B | 8B | 12B-Base | 12B-FT
    |'
  prefs: []
  type: TYPE_TB
- en: '| Total Params | 2.6B | 4.2B | 4.5B | 4.5B | 8.5B | 7.3B | 8B | 8.4B | 12.2B
    | 12.2B |'
  prefs: []
  type: TYPE_TB
- en: '| Non-Emb. Params | 2B | 2.6B | 3.7B | 3.7B | 7.7B | 7B | 7B | 7.3B | 10.9B
    | 10.9B |'
  prefs: []
  type: TYPE_TB
- en: '| Training Tokens | 2T | 94B | 94B | 94B | 6T | 8T | 15T | 380B | - | +0.1T
    |'
  prefs: []
  type: TYPE_TB
- en: '| Winogrande(5) | 70.9 | 74.0 | 72.1 | 73.5 | 78 | 78.5 | 77.3 | 80.4 | 82.2
    | 82.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Arc_challenge(25) | 55.4 | 50.9 | 52.6 | 55.6 | 61 | 60.3 | 57.9 | 64.4 |
    65.1 | 62.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MMLU(5) | 51.3 | 58.6 | 58.7 | 60.5 | 64 | 64.1 | 65.3 | 69.5 | 69.0 | 70.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hellaswag(10) | 73.0 | 75.0 | 73.2 | 76.1 | 82 | 83.2 | 81.8 | 83.0 | 85.2
    | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8k(5) | 23.9 | 24.1 | 16.8 | 41.2 | 50 | 37.0 | 48.6 | 58.5 | 56.4 | 55.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| Truthfulqa(0) | - | 42.9 | 38.2 | 42.9 | 45 | 42.6 | 45.0 | 47.6 | 49.8 |
    48.3 |'
  prefs: []
  type: TYPE_TB
- en: '| XLSum en(20%) (3) | - | 29.5 | 27.2 | 28.7 | 17 | 4.8 | 30.0 | 32.0 | 33.4
    | 31.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MBPP(0) | 29.0 | 28.2 | 30.7 | 32.4 | 39 | 38.8 | 42.3 | 43.8 | 42.6 | 47.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| HumanEval(n=20)(0) | 20.1 | 23.3 | - | - | 32.0 | 28.7 | 24.8 | 36.2 | 23.8
    | 23.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Accuracy numbers for our MN-Minitron-8B and Llama-3.1-Minitron-4B
    models. We compare our models to similarly-sized SoTA open models on a variety
    of common language modeling benchmarks. All evaluations are conducted by us, except
    entries marked with * (taken from corresponding papers).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  Benchmarks  |  Gemma  |  Phi-2  | Gemma2 | Qwen2 | Minitron |  Llama-3.1-Minitron  |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2B | 2.7B | 2B | 1.5B | 4B | 4B-Depth | 4B-Width |'
  prefs: []
  type: TYPE_TB
- en: '| Total Params | 2.5B | 2.7B | 2.6B | 1.5B | 4.2B | 4.5B | 4.5B |'
  prefs: []
  type: TYPE_TB
- en: '| Non-Emb. Params | 2B | 2.5B | 2B | 1.3B | 2.6B | 3.5B | 3.7B |'
  prefs: []
  type: TYPE_TB
- en: '| Tokens | 3T | 1.4T | 2T | 7T | 94B | 94B | 94B |'
  prefs: []
  type: TYPE_TB
- en: '| IFEval | 40.5 | 44.0 | 64.5 | 39.8 | 44.8 | 42.6 | 52.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MT-Bench | 5.2 | 4.3 | 7.7 | 5.2 | 5.6 | 5.6 | 6.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatRAG* | 33.3 | 37.6 | 37.5 | 32.8 | 41.1 | 40.1 | 44.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BFCL | 47.0 | 23.1 | 35.6 | 32.8 | 64.2 | 66.8 | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Accuracy numbers for the aligned Llama-3.1-Minitron models. We compare
    our models to similarly-sized SoTA open aligned models on a variety of benchmarks.
    All evaluations are conducted by us. * Denotes results obtained on a representative
    subset of the benchmark. Best in bold, second underlined. The alignment of MN-Minitron-8B
    is underway and will be posted once ready.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM providers often train an entire family of models from scratch, each with
    a different size (number of parameters, e.g. Llama 3.1 8B, 70B, 405B); this is
    done to aid users targeting different deployment scales, sizes and compute budgets.
    However, training multiple multi-billion parameter models from scratch is extremely
    time-, data- and resource-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: Recent work [[1](#bib.bib1)] has demonstrated the effectiveness of combining
    weight pruning with knowledge distillation to significantly reduce the cost of
    training LLM model families. Here, only the biggest model in the family is trained
    from scratch; other models are obtained by successively pruning the bigger model(s)
    and then performing knowledge distillation [[3](#bib.bib3)] to recover the accuracy
    of pruned models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this report, we successfully apply the Minitron compression strategy [[1](#bib.bib1)]
    to two state-of-the-art models: Llama 3.1 8B [[4](#bib.bib4)] and Mistral NeMo
    12B [[5](#bib.bib5)], compressing them down to 4B and 8B parameters, respectively.
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM Pruning and Distillation in
    Practice: The Minitron Approach") provides a high-level overview of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While following the original paper [[1](#bib.bib1)], we make a key modification:
    due to lack of access to the original training data, we fine-tune the teacher
    model on our own dataset before pruning and distillation. We refer to this step
    as teacher correction. Figure [4](#S3.F4 "Figure 4 ‣ Retraining: ‣ 3.3 Distillation
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach") shows that omitting teacher correction causes a data distribution mismatch,
    negatively impacting distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach") provides a summary of our results: our compression strategy yields
    a state-of-the-art 8B model (MN-Minitron-8B) which outperforms all similarly-sized
    models across the board on common language modeling benchmarks. Our Llama-3.1-Minitron-4B
    models (both depth and width-pruned variants) also exhibit strong accuracy compared
    to the teacher Llama 3.1 8B model and the previous-generation Minitron-4B model [[1](#bib.bib1)];
    among the two variants, the width-pruned variant outperforms the depth-pruned
    one. In terms of runtime inference performance measured using TensorRT-LLM, the
    Llama-3.1-Minitron-4B models provide an average speedup of 2.7$\times$ for the
    depth and width pruned variants, respectively, compared to the teacher Llama 3.1
    8B model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd3913828ef459129c69e57026ec2d77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: High-level overview of our proposed pruning and distillation approach.
    The total number of tokens used for each step is indicated in parentheses.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A high-level overview of our approach is illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LLM Pruning and Distillation in Practice: The Minitron Approach").
    Here, the teacher model is first lightly finetuned on the target dataset to be
    used for distillation - we refer to this step as teacher correction. Next, pruning
    is applied to compress the model, following which distillation is used to recover
    any lost model accuracy. We refer the reader to the Minitron paper [[1](#bib.bib1)]
    for the full description of the pruning and distillation method.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Weight pruning is a powerful and well-known technique for reducing model size.
    In this report, we focus on structured pruning, where blocks (or channels) of
    nonzero elements are removed at once from model weights; examples of structured
    pruning techniques include neuron, attention head, convolutional filter, and depth
    pruning [[1](#bib.bib1)]. In case of LLMs, as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2.1 Pruning ‣ 2 Methodology ‣ LLM Pruning and Distillation in Practice: The
    Minitron Approach"), we start the pruning process by first computing the importance
    of each layer, neuron, head, and embedding dimension. We then sort these importance
    scores to compute a corresponding importance ranking.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3a1bda1b902049db54816450d1b468c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Pruning and distillation process outlined in the original paper [[1](#bib.bib1)].
    We follow the same approach in this work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Importance Estimation: We use a purely activation-based importance estimation
    strategy that simultaneously computes sensitivity information for all the axes
    we consider (depth, neuron, head, and embedding channel) using a small calibration
    dataset and only forward propagation passes. We consider depth pruning as a special
    case and do not combine it with compressing other dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: We compute the importance of each head, neuron and embedding channel by examining
    the activations produced by the multi-head attention (MHA), multi-layer perceptron
    (MLP) and LayerNorm layers, respectively. We use a small calibration dataset (1024
    samples) for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'For depth pruning, we consider three distinct metrics for evaluating layer
    importance: (1) LM validation loss, (2) Block Importance (BI) [[6](#bib.bib6)]
    and (3) accuracy on the downstream task. For loss-based ranking, we simply remove
    a single or a block of contiguous layers and compute its effect on LM loss; this
    serves as the “importance” or sensitivity of the layer. BI uses the cosine distance
    between the input and output of a layer or a block of layers. We notice that BI
    and LM loss metrics are highly correlated but do not produce the most accurate
    pruned model on downstream tasks as shown in Figures [8](#S4.F8 "Figure 8 ‣ Depth
    Pruning Metrics: ‣ 4 Analysis ‣ LLM Pruning and Distillation in Practice: The
    Minitron Approach") and [9](#S4.F9 "Figure 9 ‣ Depth Pruning Metrics: ‣ 4 Analysis
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach"). We thus evaluate
    layer importance using the Winogrande benchmark [[7](#bib.bib7)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Trimming: As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Pruning ‣ 2 Methodology
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach"), for a given
    architecture configuration, we first rank the elements of each axis according
    to the computed importance and perform trimming (reshaping) of the corresponding
    weight matrices directly. For neuron and head pruning, we trim MLP and MHA layer
    weights, respectively. In the case of embedding channels, we trim the embedding
    dimension of the weight matrices in MLP, MHA, and LayerNorm layers. The original
    approach ([[1](#bib.bib1)]) uses Neural Architecture Search (NAS) to find the
    best architecture; in this work, we skip this step and instead utilize the network
    architecture-related learnings from the original paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Retraining with Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the term retraining to refer to the accuracy recovery process following
    pruning. In this work, we explore two retraining strategies: (1) conventional
    training, leveraging ground truth labels, and (2) knowledge distillation using
    supervision from the unpruned model (teacher). Knowledge Distillation (KD) [[3](#bib.bib3)]
    involves transfer of knowledge from a larger or more complex model called the
    teacher to a smaller/simpler model called the student. The knowledge transfer
    is achieved by having the student model mimic the output and/or the intermediate
    states of the teacher model. In our case, the uncompressed and pruned models correspond
    to the teacher and student, respectively. For distillation, we follow best practices
    from our previous work [[1](#bib.bib1)] and use forward KL Divergence loss [[8](#bib.bib8)]
    on the teacher and student logits only (following the [[3](#bib.bib3)]). This
    is illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Retraining with Distillation
    ‣ 2 Methodology ‣ LLM Pruning and Distillation in Practice: The Minitron Approach").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98f5abba7d192209e9e4e96a8f57dd09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of Distillation: If the original training data is unavailable,
    a slight fine-tuning of the teacher model is recommended. Distillation is then
    performed by minimizing KL divergence on the logits, with the original model as
    the teacher and the pruned model as the student.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Training Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Pre-training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Llama 3.1 8B [[4](#bib.bib4)] and Mistral NeMo [[5](#bib.bib5)] 12B are pretrained
    on different proprietary datasets, which we do not have access to. According to
    the Llama 3.1 tech report [[4](#bib.bib4)], the 8B model is pretrained on 15T
    tokens. We start with the corresponding Base models that are openly available
    online on Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We use the Nemotron-4 curated continued training (CT) dataset [[9](#bib.bib9)]
     [[10](#bib.bib10)] for all our pruning and distillation experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our simplified pruning recipe is based on the best practices outlined in the
    Minitron paper [[1](#bib.bib1)] and is described in the Methodology section. Specifically,
    for width pruning, we (1) use l2-norm and mean as the aggregation functions across
    the batch and sequence dimensions, respectively, and (2) perform single-shot pruning,
    avoiding iterative approaches. For depth pruning, as described in the Methodology
    section, we follow the observations from Gromov et al. [[11](#bib.bib11)] and
    drop a continuous subgroup of layers that results in the least accuracy drop on
    Winogrande [[7](#bib.bib7)]. In this work, we skip the lightweight neural architecture
    search (NAS) phase, and go with a manual architecture configuration for both Llama-3.1-Minitron-4B
    and MN-Minitron-8B. The architectures we come up with are inspired by the Minitron-4B
    and Minitron-8B models, and are detailed in Table [3](#S3.T3 "Table 3 ‣ 3.2 Pruning
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach"). We now describe the pruning recipes for each of our target compressed
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLaMa-3.1-Minitron-4B | MN-Minitron |'
  prefs: []
  type: TYPE_TB
- en: '|  | Width | Depth | 8B |'
  prefs: []
  type: TYPE_TB
- en: '| Total params | 4.5B | 4.5B | 8.4B |'
  prefs: []
  type: TYPE_TB
- en: '| Non-Emb params | 3.7B | 3.5B | 7.3B |'
  prefs: []
  type: TYPE_TB
- en: '| Hidden size | 3072 | 4096 | 4096 |'
  prefs: []
  type: TYPE_TB
- en: '| Vocabulary | 128256 | 128256 | 131072 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP hidden dim | 9216 | 14336 | 11520 |'
  prefs: []
  type: TYPE_TB
- en: '| Depth | 32 | 16 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention groups | 8 | 8 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Query heads | 32 | 32 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Head dimension | 128 | 128 | 128 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Architecture details of our compressed models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama-3.1-Minitron-4B-Width: • Starting model: Llama 3.1 8B • Hidden dimension:
    4096 $\rightarrow$ 11520 • Attention heads: unchanged • Depth: unchanged'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Teacher Correction:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Using the Mistral NeMo 12B model directly as a teacher performs sub-optimally
    on our dataset. This is due to the change in distribution of sub-word tokens across
    the original dataset the teacher model was trained on vs. the dataset being distilled
    on. To account for this, we first fine-tune the teacher on our dataset using $\sim$127B
    tokens. As shown in Figure [4](#S3.F4 "Figure 4 ‣ Retraining: ‣ 3.3 Distillation
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach"), such a correction is essential if the original dataset is not available
    during distillation. We thus apply this technique on both the Mistral-NeMo and
    Llama-3.1 teacher models. The fine-tuning process has a minor effect on the teacher
    model’s accuracy on downstream tasks, with some tasks improving and some degrading
    as shown in Table  [1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach"). We hypothesize this to be an artifact of the dataset
    used for fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retraining:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Following the learnings in the Minitron work [[1](#bib.bib1)], we opt for logit-only
    distillation, minimizing the forward KL Divergence [[8](#bib.bib8)] loss across
    the teacher and student probabilities, and ignore the LM cross-entropy loss altogether.
    Here, the unpruned and pruned models correspond to the teacher and student, respectively.
    We use the hyperparameters listed in Table [4](#S3.T4 "Table 4 ‣ Retraining: ‣
    3.3 Distillation ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach") during distillation. We use 32 NVIDIA DGX H100 nodes for
    our training jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Llama-3.1- | MN-Minitron |'
  prefs: []
  type: TYPE_TB
- en: '|  | Minitron-4B | 8B |'
  prefs: []
  type: TYPE_TB
- en: '| Peak learning rate | 1e-4 | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Min learning rate | 1e-5 | 4.5e-7 |'
  prefs: []
  type: TYPE_TB
- en: '| Warm-up steps | 40 steps | 60 steps |'
  prefs: []
  type: TYPE_TB
- en: '| LR decay schedule | Cosine | Cosine |'
  prefs: []
  type: TYPE_TB
- en: '| Global batch size | 1152 | 768 |'
  prefs: []
  type: TYPE_TB
- en: '| Context length | 8192 | 8192 |'
  prefs: []
  type: TYPE_TB
- en: '| Total tokens | 94B | 380B |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Hyperparameters used during distillation-based retraining.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c66a16495137d6b98f4693cd084813a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Training convergence plot for the compressed 8B student model. We
    compare supervision from the original teacher and the corrected teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Instruction Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the instruction-following capabilities of our distilled models,
    we perform supervised fine-tuning (SFT) on the Llama-3.1-Minitron 4B models using
    NeMo-Aligner [[12](#bib.bib12)] with the instruction tuning dataset used for Nemotron-4
    340B [[13](#bib.bib13)]. As shown in Table [2](#S0.T2 "Table 2 ‣ LLM Pruning and
    Distillation in Practice: The Minitron Approach"), we evaluate the aligned models
    for instruction- following and roleplay (IFEval [[14](#bib.bib14)] and MT-Bench [[15](#bib.bib15)]),
    RAG QA (ChatRAG-Bench [[16](#bib.bib16)]), and function-calling capabilities (BFCL [[17](#bib.bib17)]).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We perform a series of ablation studies to better understand the compression
    characteristics of these newer models. We report our results in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Width vs Depth Pruning:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ Pruning and Distillation: ‣ 4 Analysis ‣ LLM
    Pruning and Distillation in Practice: The Minitron Approach") shows the training
    curve of Llama-3.1-Minitron-4B pruned for width vs. depth. We notice that width
    pruning results in smaller initial loss and consistently outperforms the depth-pruned
    model, despite both variants having the same number of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pruning and Distillation:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure  [6](#S4.F6 "Figure 6 ‣ Pruning and Distillation: ‣ 4 Analysis ‣ LLM
    Pruning and Distillation in Practice: The Minitron Approach") demonstrates orthogonal
    benefits of our proposed approach with pruning and distillation. We compare (1)
    random weight initialization and distillation, (2) random pruning and distillation,
    where components are pruned randomly ignoring the importance scores, (3) our proposed
    pruning with typical cross entropy based LM loss training and (4) our proposed
    pruning with distillation-based training. We notice that pruning results in a
    significantly better starting point compared to random initialization, and also
    that distillation-based training outperforms conventional training methods while
    requiring significantly fewer training tokens (up to $50\times$ in our case).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e64b762c7323fe6fc8a314e2da505f6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Convergence of width- and depth-pruned Llama 3.1 8B to 4B models.
    Width pruning consistently outperforms depth pruning for a given parameter budget.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec989e801e6beca107e696b240084614.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Training convergence plot for Mistral Nemo 12B compressed model.
    We compare (a) random initialization with distillation, (b) randomly pruned weights
    with distillation, (c) pruning with standard LM loss, and (d) our pipeline with
    pruning and distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Teacher Correction:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We compare two approaches for teacher correction: (1) pruning and distilling
    the corrected teacher, and (2) pruning the original teacher and distilling from
    a continuously corrected teacher. The results in Figure [7](#S4.F7 "Figure 7 ‣
    Teacher Correction: ‣ 4 Analysis ‣ LLM Pruning and Distillation in Practice: The
    Minitron Approach") suggest that teacher correction doesn’t affect the optimality
    of pruning, and that distillation from a corrected teacher is crucial. Teacher
    correction can be performed in parallel with distillation to bridge the gap.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17120e7557f2810e6e361ae512541d2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Training convergence plot for Mistral Nemo 12B compressed model.
    We compare (1) pruning and distilling the corrected teacher with (2) pruning the
    original teacher and distilling from a continuously corrected teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth Pruning Metrics:'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'when examining how LM validation loss increases as contiguous blocks of layers
    are removed (Figure  [8](#S4.F8 "Figure 8 ‣ Depth Pruning Metrics: ‣ 4 Analysis
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach")), we observe
    that the layers at the beginning and end are the most important. Removing non-contiguous
    layers can result in even better LM validation loss (the dashed line). However,
    this observation does not necessarily hold when evaluating downstream task performance.
    Figure  [9](#S4.F9 "Figure 9 ‣ Depth Pruning Metrics: ‣ 4 Analysis ‣ LLM Pruning
    and Distillation in Practice: The Minitron Approach") shows that dropping 16 layers
    selected based on per-layer importance ( [[6](#bib.bib6), [18](#bib.bib18)]) yields
    a random Winogrande accuracy of 0.5, while removing layers 16 to 31 continuously
    ( [[11](#bib.bib11)]) results in an accuracy of 0.595\. The gap holds during distillation-based
    retraining and we opt for the latter approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a84f4f5dc4ffea7f29ddf23644cca5d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: LM loss value on validation set after removing 1, 2, 8 or 16 contiguous
    layers with Llama 3.1 8B. For example, the purple line at layer no. 16 indicates
    the LM loss if we dropped the first 16 layers. Layer no. 17 indicates the LM loss
    if we leave the first layer intact and drop layers 2 to 17\. The dashed line corresponds
    to LM loss value when removing 16 non-contiguous layers least increasing the loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cddb2a6fd916e886f5a8b97ecd2467b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Accuracy on the Winogrande task when removing 16 contiguous layers
    with Llama 3.1 8B. Layer no. 17 indicates the LM loss if we leave the first layer
    intact and drop layers 2 to 17\. The dashed line corresponds to the accuracy when
    removing 16 non-contiguous layers least increasing the loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'following Touvron et al. [[19](#bib.bib19)], we evaluate our compressed models
    on a series of downstream tasks, including MMLU [[20](#bib.bib20)], HumanEval [[21](#bib.bib21)]
    for Python code generation, several question-answering datasets for common-sense
    reasoning: Arc-C [[22](#bib.bib22)], HellaSwag [[23](#bib.bib23)], TruthfulQA [[24](#bib.bib24)]
    and WinoGrande [[7](#bib.bib7)] and XL-Sum English [[25](#bib.bib25)] for summarization.
    We report the 5-shot performance on MMLU, 5-shot on Winogrande, 25-shot on ARC-Challenge,
    10-shot on HellaSwag, 0-shot on 20% of XL-Sum and average pass@1 scores for HumanEval
    and MBPP. For pass@1 scores we use a temperature of 0.2 and nucleus sampling [[26](#bib.bib26)]
    with top-p $=$ 0.95. For instruction-tuned models, we use MT-Bench [[15](#bib.bib15)],
    Instruction-Following Eval (IFEval) [[14](#bib.bib14)], ChatRAG-Bench [[16](#bib.bib16)],
    and Berkeley Function Calling Leaderboard (BFCL) [[17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Base Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Base model evaluation results are shown in Table [1](#S0.T1 "Table 1 ‣ LLM
    Pruning and Distillation in Practice: The Minitron Approach"). Compared to similarly-sized
    models, MN-Minitron-8B demonstrates superior accuracy across the board, outperforming
    the recent Llama 3.1 8B model using 40$\times$ fewer training tokens (94B vs.
    15T); our pruned Llama models also outperform the previous generation Minitron
    4B model. We note from Table [1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation
    in Practice: The Minitron Approach") that the width-pruned variant outperforms
    the depth-pruned one. These results clearly demonstrate the advantages of our
    methodology: state-of-the-art accuracy coupled with an order of magnitude improvement
    in training efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Instruct Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The performance of the instruction-tuned Llama-3.1-Minitron 4B variants is
    shown in Table [2](#S0.T2 "Table 2 ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach"). We compare the Llama-3.1-Minitron 4B variants to other
    similarly-sized baselines and notice that our models demonstrate strong instruction-following
    and roleplay capabilities, only lagging behind Gemma2 in IFEval [[14](#bib.bib14)]
    and MT-Bench [[15](#bib.bib15)]. On retrieval based question answering (ChatRAG-Bench [[16](#bib.bib16)])
    and function-calling (BFCL [[17](#bib.bib17)]), Minitron models achieve state-of-the-art
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Runtime Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We optimized the Llama 3.1 8B and Llama-3.1-Minitron 4B variants with NVIDIA
    TensorRT-LLM, an open-source toolkit for optimized LLM inference.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e29f934a9076c4f8bd89f417a79c42de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: TensorRT-LLM FP8 throughput comparison for the Llama-3.1-Minitron-4B
    models with the Llama 3.1 8B model w.r.t. increasing input and output sequence
    lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Runtime Performance Analysis ‣ 5 Evaluation
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach") shows the
    throughput in requests per second for the various models in FP8 precision obtained
    on a single H100 80 GB GPU. Different use cases are represented by increasing
    input sequence length/output sequence length (ISL/OSL) combinations, at a batch
    size of 32 and 64 for the 8B-12B models and the 4B models respectively. The smaller
    memory footprint of the 4B model allows for larger batches. We notice that Llama-3.1-Minitron-4B-Depth
    is the fastest, achieving an average throughput improvement of $2.7\times$.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this Section, we summarize some interesting and surprising observations.
  prefs: []
  type: TYPE_NORMAL
- en: 6.0.1 General
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Teacher correction is crucial for distillation to work optimally on a new, unseen
    dataset. Fine-tuning the teacher with the dataset used for distillation in this
    manner yields over a 6% reduction in LM validation loss. Teacher correction doesn’t
    affect the optimality of pruning and can even be performed in parallel with distillation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In line with the Minitron paper’s observations, we require only 380B tokens
    to achieve state-of-the-art accuracy post pruning with distillation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For width pruning, we achieve stronger accuracy by retaining attention heads
    and pruning the other dimensions (MLP intermediate dimension, embedding channels).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.0.2 Mistral NeMo 12B to MN-Minitron-8B:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our compressed model outperforms the teacher on two benchmarks, GSM8k and HumanEval
    after pruning and distillation: GSM8k increases from 55.7% to 58.5% and HumanEval
    increases from 23.8% to 36.2%. This improvement is likely influenced by the dataset.
    However, retraining is performed using the distillation loss alone.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.0.3 Llama 3.1 8B to Llama-3.1-Minitron 4B:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Width pruning delivers better accuracy with MMLU at 60.5%, while depth pruning
    yields 58.7%, for Llama-3.1 compression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reasoning ability is impacted further significantly, with GSM8K accuracy at
    41.24% for width and 16.8% for depth.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depth pruning boosts throughput, achieving $2.7\times$ speedup.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For depth pruning, we observe that dropping contiguous layers from the model
    is more effective than using non-contiguous, importance-based pruning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7 Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work would not have been possible without contributions from many people
    at NVIDIA. To mention a few:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Foundational Model: Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj
    Joshi, Marcin Chochowski, Pavlo Molchanov, Mostofa Patwary, Daniel Korzekwa, Ashwath
    Aithal, Mohammad Shoeybi, Bryan Catanzaro and Jan Kautz'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alignment: Ameya Sunil Mahabaleshwarkar, Hayley Ross, Brandon Rowlett, Oluwatobi
    Olabiyi, Shizhe Diao and Yoshi Suhara'
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets: Sanjeev Satheesh, Jupinder Parmar, Shengyang Sun, Jiaqi Zeng, Zhilin
    Wang, Yi Dong, Zihan Liu, Rajarshi Roy, Wei Ping, Makesh Narsimhan Sreedhar and
    Oleksii Kuchaiev'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorRT-LLM: Bobby Chen, James Shen and Chenhan Yu'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face Support: Ao Tang, Yoshi Suhara and Greg Heinrich'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin
    Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and
    Pavlo Molchanov. Compact language models via pruning and knowledge distillation.
    arXiv preprint arXiv:2407.14679, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan
    Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds,
    Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben
    Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,
    12 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge
    in a Neural Network. arXiv preprint arXiv:1503.02531, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Abhimanyu Dubey and Abhinav Jauhri et al. The Llama 3 Herd of Models. arXiv
    2407.21783, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Mistral AI team. Mistral nemo. https://mistral.ai/news/mistral-nemo, 2024.
    Accessed: 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu,
    Xianpei Han, and Weipeng Chen. ShortGPT: Layers in Large Language Models are More
    Redundant Than You Expect, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    WinoGrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9),
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Solomon Kullback and Richard A. Leibler. On information and sufficiency.
    Annals of Mathematical Statistics, 22(1):79–86, 1951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary,
    Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala,
    Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski,
    Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick
    LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad
    Shoeybi, Jonathan Cohen, and Bryan Catanzaro. Nemotron-4 15b technical report,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and
    Bryan Catanzaro. Reuse, don’t retrain: A recipe for continued pretraining of language
    models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and
    Daniel A. Roberts. The unreasonable ineffectiveness of the deeper layers. 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel
    Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin,
    Ashwath Aithal, and Oleksii Kuchaiev. Nemo-aligner: Scalable toolkit for efficient
    model alignment, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Nvidia, :, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab
    Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan
    Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong,
    Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris
    Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa,
    Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev,
    Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar,
    Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan
    Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald
    Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar,
    Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy,
    Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft,
    Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy,
    Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang
    Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy
    Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, and Chen Zhu. Nemotron-4 340b technical
    report, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu,
    Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language
    models. arXiv preprint arXiv:2311.07911, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E
    Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena.
    In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,
    Advances in Neural Information Processing Systems, volume 36, pages 46595–46623\.
    Curran Associates, Inc., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi,
    and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv
    preprint arXiv:2401.10225, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G.
    Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard.
    [https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz,
    David Krueger, and Pavlo Molchanov. A deeper look at depth pruning of llms. arXiv
    preprint arXiv:2407.16286, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    In International Conference on Learning Representations, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared
    Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray,
    Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
    Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
    Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,
    David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
    Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu
    Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec
    Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,
    Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try ARC, the AI2 reasoning challenge. ArXiv, abs/1803.05457, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David
    Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, Florence, Italy, July 2019\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how
    models mimic human falsehoods, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Samin, Yuan-Fang
    Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual
    abstractive summarization for 44 languages, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious
    case of neural text degeneration. ArXiv, abs/1904.09751, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
