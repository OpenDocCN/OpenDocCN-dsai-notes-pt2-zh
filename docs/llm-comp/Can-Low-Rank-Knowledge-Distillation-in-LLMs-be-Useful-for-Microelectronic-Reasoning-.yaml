- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 19:05:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13808](https://ar5iv.labs.arxiv.org/html/2406.13808)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nirjhor Rouf^∗ ECE Department,
  prefs: []
  type: TYPE_NORMAL
- en: North Carolina State University.
  prefs: []
  type: TYPE_NORMAL
- en: nrouf2@ncsu.edu    Fin Amin^∗ ECE Department,
  prefs: []
  type: TYPE_NORMAL
- en: North Carolina State University.
  prefs: []
  type: TYPE_NORMAL
- en: samin2@ncsu.edu    Paul D. Franzon ECE Department,
  prefs: []
  type: TYPE_NORMAL
- en: North Carolina State University.
  prefs: []
  type: TYPE_NORMAL
- en: paulf@ncsu.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this work, we present empirical results regarding the feasibility of using
    offline large language models (LLMs) in the context of electronic design automation
    (EDA). The goal is to investigate and evaluate a contemporary language model’s
    (Llama-2-7B) ability to function as a microelectronic Q&A expert as well as its
    reasoning, and generation capabilities in solving microelectronic-related problems.
    Llama-2-7B was tested across a variety of adaptation methods, including introducing
    a novel low-rank knowledge distillation (LoRA-KD) scheme. Our experiments produce
    both qualitative and quantitative results. Furthermore, we release our evaluation
    benchmark along with the code necessary to replicate our experiments at [github.com/FinAminToastCrunch](https://github.com/FinAminToastCrunch).
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'LLMs for EDA education, LLM fine-tuning, knowledge-distillation, RAG, Low-Rank
    adaptation^*^*footnotetext: These authors contributed equally to this work.^*^*footnotetext:
    To appear in IEEE International Workshop on LLM-Aided Design (LAD’24)'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction and Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The emergence of Large Language Models (LLM) has revolutionized the field of
    natural language processing. At present, LLMs are garnering significant research
    interests for domain-specific tasks. In the field of electronic design automation
    (EDA) in particular, applications of LLMs are still at the nascent stage. However,
    it is very apparent that the effective use of LLMs in EDA can improve manufacturing
    yields by streamlining the design flow when it comes to IC design. Recently published
    works showed the successful use of LLMs in chip design [[3](#bib.bib3), [13](#bib.bib13),
    [2](#bib.bib2)]. Additionally, LLMs have also shown significant proficiency in
    the analysis of designed systems [[8](#bib.bib8)] and even in reviewing and analysis
    of design specifications of VLSI systems [[11](#bib.bib11)]. Development of open-source
    benchmarks such as VerilogEval [[14](#bib.bib14)] is also facilitating future
    research in this field. Similarly, LLMs can be useful in enhancing productivity.
    Internal studies carried out at Nvidia have shown that checklist related tasks
    can take up to 60% of an engineer’s time and thus bottleneck productivity [[13](#bib.bib13)].
    An LLM-based engineering assistant can certainly reduce this bottleneck by helping
    with engineering knowledge dissemination.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a440d19f8d15f48e613979d5d1afe9fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: LoRA-KD works by first fine-tuning the teacher model using LoRA.
    Afterward, the teacher is frozen and its outputs are used for equation [4](#S3.E4
    "In III-B Knowledge Distillation ‣ III Adaptation Techniques for LLMs ‣ Can Low-Rank
    Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?"). Note
    that only the low-rank $A$ parameters of the student are updated.'
  prefs: []
  type: TYPE_NORMAL
- en: However, several key challenges must be addressed for more effective and efficient
    application of LLMs in EDA. One big concern is the unintentional data retention
    of DNNs from training sets [[9](#bib.bib9)]. There are two aspects of this issue.
    Firstly, classified IP designs can be leaked if the API stores user input. Secondly,
    when trying to complete a user request, the LLM can inadvertently use copy-righted
    IP designs without attributing references to them–potentially causing downstream
    legal trouble. Another major challenge is the heavy computational resource requirement
    of LLMs. For example, Meta’s Llama2-70B requires 130 GB memory to load [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b6f42c27dc53b454e870e9c166e471b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: These charts show histograms of which configurations were ranked
    in the top half and declared the worst according to third-year microelectronics
    students. Survey participants had to order the outputs of each configuration on
    15 questions. A total of 51 rankings were considered after filtering for quality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the appropriate LLM for EDA applications is also a big challenge and
    here, the proprietary vs open-source debate must be addressed. While proprietary
    models, such as ChatGPT-4 [[1](#bib.bib1)] are powerful, they have limited accessibility,
    store user data/designs, and are pay-to-use. Additionally, the inability to fine-tune
    them hinders their capabilities in domain-specific EDA tasks. On the other hand,
    open-source LLMs offering better accessibility are restricted by limited scale
    and resources compared to their proprietary counterparts resulting in lower performance
    [[21](#bib.bib21)]. In this work, we explore the feasibility of adapting the open-source
    Llama-2-7B for use in EDA education. We focus on this model in particular because
    it can be used on consumer hardware. Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A quantitative and qualitative analysis of Llama-2-7B adapted in various ways
    for EDA usage. This investigation allows us to understand the impact of fine-tuning,
    distillation, and retrieval augmentation on the model’s performance in the context
    of EDA knowledge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We introduce and evaluate a novel fine-tuning method, Low-Rank Knowledge Distillation
    (LoRA-KD).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The release of a benchmark, RAQ, designed for evaluating LLMs on EDA knowledge,
    aimed at facilitating future research and development in the field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Prior Work on LLMs for EDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the burgeoning field of EDA, early explorations into the applications of
    LLMs have already returned promising results, particularly in the nuanced areas
    of chip design, debugging, and script generation. Recently developed LLM-powered
    ChatEDA is capable of streamlining the IC design flow from RTL to GDSII [[3](#bib.bib3)].
    ChatEDA integrates Automage, a fine-tuned LLM based on Llama-2-70B architecture,
    with an EDA tool. Automage serves as an interface that accepts human requests
    and manipulates the EDA tool through API for task completion. ChatEDA was tested
    on performance evaluation, parameter grid search, parameter tuning, customized
    optimization and clock period minimization.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Nvidia took a slightly different approach with their ChatNeMo,
    a Llama-2 based LLM for chip design which contributes greatly to improving productivity
    as an engineering chatbot assistant. It is also capable of generating EDA scripts
    and, bug summarization and analysis [[13](#bib.bib13)]. ChatNeMo outperforms GPT-4
    at engineering assistant chatbot and EDA script generation tasks while showing
    comparable performance at bug summarization and analysis whereas ChatEDA has shown
    comparable or better performance than GPT-4 in all its evaluated cases.
  prefs: []
  type: TYPE_NORMAL
- en: Another work [[2](#bib.bib2)] explores the possibility of LLM applications in
    conversational hardware design by having a hardware engineer co-architect a microprocessor
    architecture with GPT-4 and this design was sent to tapeout. In addition to these,
    the possibility of LLM applications in generating VLSI design specifications has
    also been explored. SpecLLM has shown significant proficiency in assisting engineers
    in generating and reviewing architecture specifications [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: Hardware security assessment is one more field which has studied the feasibility
    of language models. The authors of [[8](#bib.bib8)] present an automated flow
    to identify suitable modules in large HDL databases for hardware trojan insertion
    using a general-purpose LLM. The model’s ability to pinpoint candidate modules
    for the attack can be indicative of its significant comprehension of RTL codes
    and system design.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Configurations’ Performance on Reasoning and Accuracy Questions'
  prefs: []
  type: TYPE_NORMAL
- en: '| RAQ: Reasoning | Ground Truth | 70B Baseline | 70B LoRA | 7B Baseline | 7B
    LoRA | 7B LoRA-KD | 7B RAG |'
  prefs: []
  type: TYPE_TB
- en: '| 1a | Increase | Increase | Increase | Increase | Increase | Increase | Decrease
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1b | 3 $\mu$m | 330 nm |'
  prefs: []
  type: TYPE_TB
- en: '| 2a | 0.775 mA | 5.58 mA | 5.58 mA | 1.28 A | 1.395 A | 0.06 A | × |'
  prefs: []
  type: TYPE_TB
- en: '| 2b | 1.55 V | 11.16 V | 8.6 V | 0.83 V | 0.647 V | 0.7 V | × |'
  prefs: []
  type: TYPE_TB
- en: '| 3a | 2 V | 2 V | 2 V | 5 V | 0.625 V | 5 V | 6 V |'
  prefs: []
  type: TYPE_TB
- en: '| 3b | 3 k$\Omega$ | × |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2 | 2 | 2 | 2 | 2 | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 5a | 0.66 k$\Omega$ | × |'
  prefs: []
  type: TYPE_TB
- en: '| 5b | 0.667 k$\Omega$ | × |'
  prefs: []
  type: TYPE_TB
- en: '| RAQ: T/F Accuracy | - | 84% | 84% | 72% | 76% | 76% | 80% |'
  prefs: []
  type: TYPE_TB
- en: Evaluated on the reasoning and T/F questions from the RAQ benchmark. Note that
    some of the reasoning questions required multi-step thinking, eg. based on your
    answer to part a, what is part b? The $\times$ symbol denotes that the model refused
    to answer the question due to fallacious “ethical reasons.”
  prefs: []
  type: TYPE_NORMAL
- en: III Adaptation Techniques for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Low-Rank Adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Low-rank adaptation (LoRA) addresses many issues associated with adapting LLMs
    for domain-specific usage[[6](#bib.bib6)]. This method bypasses the expensive
    backpropagation of gradients across all parameters by keeping the backbone model
    frozen. This is done by assuming that the update to the model’s weights have low-rank.
    In other words, instead of updating the backbone, we learn parameters $A$ which
    learn the required changes to the output of the backbone. More explicitly, if
    we write the parameter update equation, LoRA makes the following approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Theta_{t+1}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\approx\Theta_{0}-\alpha BA$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{i.e. }\eta\nabla_{\Theta}\mathcal{L}(\Theta_{t})$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Where $\Theta\in\mathbb{R}^{d\times k},B\in\mathbb{R}^{d\times r},\text{ and
    }A\in\mathbb{R}^{r\times k}$ LoRA provides a resource-efficient update to the
    backbone.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knowledge Distillation (KD) [[5](#bib.bib5)] is a knowledge-transfer technique
    where a larger (teacher) network produces soft targets for a smaller (student)
    model. This can play a pivotal role in reducing the performance gap between larger
    and smaller models. Fine-tuning a smaller (student) model through KD can show
    improved performance compared to a normally fine-tuned small model. As an example,
    the authors of DistilBERT show that they can retain 97% of the original BERT’s
    performance despite a significantly smaller parameter count [[15](#bib.bib15)].
    This indicates that a smaller model that can be deployed on weaker hardware, e.g.
    personal computer, can maintain feasibility in handling complex tasks related
    to EDA. Written explicitly, the loss used for KD is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{KD}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\alpha\mathcal{L}_{Dist}(student(x),teacher(x))$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: Where $\mathcal{L}_{y}$ is the loss between what the student predicted and the
    teacher predicted on an input. More elaborate distillation techniques exist, for
    example, patient KD aims at having the student mimic the teacher’s intermediate
    layers in addition to the teacher’s outputs [[16](#bib.bib16)]. We refer readers
    to [[21](#bib.bib21)] for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Low-Rank Knowledge Distillation (LoRA-KD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although not entirely unprecedented, the combination of low-rank approximations
    and knowledge distillation is far less explored in the context of LLM fine-tuning.
    The authors of LoSparse [[12](#bib.bib12)] introduce a new compression scheme
    for transformers [[18](#bib.bib18)] based on a truncated singular value decomposition.
    In their experiments, they find that combining this parameter compression scheme
    with knowledge distillation further improves performance.
  prefs: []
  type: TYPE_NORMAL
- en: In our work, we reformulate this concept in accordance with figure [1](#S1.F1
    "Figure 1 ‣ I Introduction and Motivation ‣ Can Low-Rank Knowledge Distillation
    in LLMs be Useful for Microelectronic Reasoning?"). We begin by fine-tuning the
    teacher (Llama-2-70B) using LoRA. Afterwards, we fine-tune the student (Llama-2-7B)
    via LoRA using $\mathcal{L}_{KD}$. We hypothesize that, if the updates to the
    teacher can be done in a low-rank fashion, then the underlying knowledge being
    learned is also low-rank; therefore, the knowledge to be distilled to the student
    is also low-rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Comparison of Model/Adaptation Combinations Evaluated by Human Expert
    and GPT-4.5 Turbo via Likert Scale.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Human Expert | GPT-4.5 Turbo | Pearson Correlation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Configuration | Accuracy | Quality | Accuracy | Quality | Accuracy | Quality
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 70B Baseline | 4.2[$\pm$1.95] | 0.51 | 0.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 70B LoRA | 4.35[$\pm$1.67] | 0.47 | 0.51 |'
  prefs: []
  type: TYPE_TB
- en: '| 7B Baseline | 3.4[$\pm$1.93] | 0.53 | 0.57 |'
  prefs: []
  type: TYPE_TB
- en: '| 7B LoRA | 3.5[$\pm$1.90] | 0.66 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 7B LoRA-KD | 3.525[$\pm$1.60] | 0.60 | 0.59 |'
  prefs: []
  type: TYPE_TB
- en: '| 7B RAG | 4.2[$\pm$1.94] | 0.12 | 0.13 |'
  prefs: []
  type: TYPE_TB
- en: Each response to the 40 qualitative questions was evaluated on a 7-point Likert
    scale by a human expert and GPT-4.5 Turbo. $7$ denotes “strongly disagreed with.”
    The subcolumns correspond to how much the evaluator agreed/disagreed with the
    accuracy/quality of the response. The standard deviations across the questions
    are written in sub-scripts. The correlation quantifies the consistency between
    the human expert and GPT-4.5 Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several advantages to doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with ordinary RAG, a pre-trained model can be repurposed via hot-swapping
    the adaptation layer. For example, EDA educators can use LoRA-KD to learn separate
    (small) adaptation layers for English and Spanish in the context of a bilingual
    classroom.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KD has been used to enhance domain-adaptation tasks. We hypothesize that the
    dark knowledge distilled from the teacher to the student will facilitate enhanced
    reasoning capabilities [[20](#bib.bib20)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training process remains fast. In our experiments, fine-tuning the student
    via LoRA-KD did not take much more time than ordinary LoRA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III-D Retrieval Augmented Generation (RAG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RAG[[10](#bib.bib10)] operates by integrating a neural retriever with a sequence-to-sequence
    (seq2seq) generator. The retriever produces a distribution, $p_{r}(z|x)$. RAG’s
    seq2seq probability distribution is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(y&#124;x)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: This method combines the strengths of pre-trained parametric models with non-parametric
    external knowledge sources. For our work, we use the pre-trained MiniLM model
    [[19](#bib.bib19)] as the retriever and the pre-trained Llama-2-7B as the generator.
  prefs: []
  type: TYPE_NORMAL
- en: IV Fine-tuning Dataset and the RAQ Benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our fine-tuning dataset consists of several well-known textbooks on microelectronics,
    VLSI circuit design, and fabrication technologies. In addition to these, we also
    included some recently published works related to DDR5 design and its corresponding
    JEDEC standard. After filtering the data, the number of tokens was calculated
    using Llama-2 tokenizer. The dataset contains 3,168,414 tokens and 12,988 unique
    tokens. Due to copyright reasons, we cannot release the fine-tuning dataset. However,
    we list all the components of the dataset within the appendix so that readers
    can assemble it themselves.
  prefs: []
  type: TYPE_NORMAL
- en: We created a benchmark to evaluate the performance of the different models which
    includes 70 carefully-curated domain-specific questions. Among them, there are
    40 qualitative questions and 25 true/false questions. The 65 aforementioned questions
    are meant to evaluate the accuracy and quality of the LLM’s responses on domain
    knowledge. Furthermore, 5 questions are designed to evaluate the models’ capabilities
    to reason upon circuit design decisions based on given specifications. Hence,
    we name it the Reasoning-Accuracy-Quality (RAQ) benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: V Setup and Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To assess the suitability of various adaptation methods, we performed four
    experiments using the RAQ Benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Student Survey. We selected 15 questions which would be most relevant for a
    third-year undergraduate microelectronics classroom. We recorded the responses
    from each configuration and asked students to provide the ordinal rankings in
    terms of what they preferred. To ensure quality, we kept the configurations anonymous
    and asked students to explain why they ranked the best/worst models as they did.
    After pruning low-quality submissions, we had 51 rankings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True/False Q&A. We prompted each configuration to answer true or false to determine
    accuracy. This portion was taken from the T/F section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Likert Test. Each configuration was asked to answer all 40 qualitative questions.
    Using a 7-point Likert scale, the responses were scrutinized in terms of accuracy
    and subjective quality. We¹¹1We recognize there could be bias if we, the authors,
    evaluate these models. To promote transparency, we release the model responses
    on our GitHub (human expert) and ChatGPT-4.5 Turbo were the evaluators.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reasoning Test. We tested each configuration with 5 reasoning questions. These
    questions have unambiguous or numerical answers. Generated responses were compared
    against ground truth values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For all experiments, we use $\mathtt{LoRA\_Rank=4}$ for KD.
  prefs: []
  type: TYPE_NORMAL
- en: VI Results and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we try to explore the feasibility of using language models in
    EDA education. Table [I](#S2.T1 "TABLE I ‣ II Prior Work on LLMs for EDA ‣ Can
    Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?")
    investigates the configurations’ capabilities to reason based on the given information
    and optimize a given design. A few interesting observations were made while evaluating
    the models on reasoning/optimization questions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the models had difficulty with numerical calculations and assigning proper
    units to a calculated value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our experiments, the models performed better when they were asked the different
    sections of the questions one by one in separate prompts, rather than putting
    all the questions in a single prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAG tended to refuse answering due to dubious ethical reasons regarding the
    “danger of transistors.”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An analysis of the data presented in tables [I](#S2.T1 "TABLE I ‣ II Prior Work
    on LLMs for EDA ‣ Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic
    Reasoning?") and [II](#S3.T2 "TABLE II ‣ III-C Low-Rank Knowledge Distillation
    (LoRA-KD) ‣ III Adaptation Techniques for LLMs ‣ Can Low-Rank Knowledge Distillation
    in LLMs be Useful for Microelectronic Reasoning?") gives insights into the strengths
    and weaknesses of the configurations. For the Likert test and true/false accuracy,
    7B RAG performs strongly but for reasoning/optimization, it exhibits a sharp decline
    in performance. This indicates that RAG alone cannot improve performance across
    all areas. On the contrary, the various fine-tuned versions manage to perform
    well on the reasoning portion while remaining within half a standard deviation
    from RAG on the Likert test.
  prefs: []
  type: TYPE_NORMAL
- en: The responses collected from the students underscore each configuration’s communication
    skills and human expectations which can serve as an important guideline when fine-tuning
    an LLM. An improvement of LoRA-KD over LoRA can be observed in figure [2](#S1.F2
    "Figure 2 ‣ I Introduction and Motivation ‣ Can Low-Rank Knowledge Distillation
    in LLMs be Useful for Microelectronic Reasoning?") where the responses generated
    by 7B LoRA-KD were far less likely to be ranked last. Another interesting facet
    is the agreement between the students with respect to the question (i.e. the entropy).
    For example, for Q14, there was high agreement that the 70B Baseline did the worst.
    On the other hand, for Q15, the students seemed split between whether 7B RAG,
    7B LoRA, or 70B LoRA was the worst.
  prefs: []
  type: TYPE_NORMAL
- en: While the existing works are significant milestones of the application of LLMs
    in EDA, its potential in this field has yet to be fully realized. Development
    of specialized large language models capable of understanding the intricacies
    of domain-specific EDA tasks is crucial for its continued applications in EDA
    [[4](#bib.bib4)]. This study highlights some strengths and weaknesses of different
    open-source offline LLM configurations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Chip-chat:
    Challenges and opportunities in conversational hardware design. In 2023 ACM/IEEE
    5th Workshop on Machine Learning for CAD (MLCAD), pages 1–6\. IEEE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng Zheng,
    and Bei Yu. Chateda: A large language model powered autonomous agent for eda,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Zhuolun He and Bei Yu. Large language models for eda: Future or mirage?
    In Proceedings of the 2024 International Symposium on Physical Design, ISPD ’24,
    page 65–66, New York, NY, USA, 2024\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
    in a neural network, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Georgios Kokolakis, Athanasios Moschos, and Angelos D Keromytis. Harnessing
    the power of general-purpose llms in hardware trojan design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai. Knowledge distillation
    of llm for automatic scoring of science education assessments, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances
    in Neural Information Processing Systems, 33:9459–9474, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Mengming Li, Wenji Fang, Qijun Zhang, and Zhiyao Xie. Specllm: Exploring
    generation and review of vlsi design specification with large language model,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen,
    and Tuo Zhao. Losparse: Structured compression of large language models based
    on low-rank and sparse approximation. In International Conference on Machine Learning,
    pages 20336–20350\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel
    Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet
    Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay,
    Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer Halepete,
    Eric Hill, Jiashang Hu, Sumit Jain, Ankit Jindal, Brucek Khailany, George Kokai,
    Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart Oberman, Sujeet Omar,
    Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P
    Suthar, Varun Tej, Walker Turner, Kaizhe Xu, and Haoxing Ren. Chipnemo: Domain-adapted
    llms for chip design, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Invited
    paper: Verilogeval: Evaluating large language models for verilog code generation.
    In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), pages
    1–8, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint
    arXiv:1910.01108, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation
    for bert model compression. arXiv preprint arXiv:1908.09355, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm:
    Deep self-attention distillation for task-agnostic compression of pre-trained
    transformers, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yufei Wang, Haoliang Li, Lap-pui Chau, and Alex C. Kot. Embracing the
    dark knowledge: Domain generalization using regularized knowledge distillation.
    In Proceedings of the 29th ACM International Conference on Multimedia, MM ’21,
    page 2595–2604, New York, NY, USA, 2021\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li,
    Can Xu, Dacheng Tao, and Tianyi Zhou. A survey on knowledge distillation of large
    language models, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VII Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VII-A Fine-tuning Sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following sources were used in fine-tuning. An enumerated list is also
    available on our github:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fundamentals of Microelectronics - 2nd Edition - Behzad Razavi
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Electronic Devices and Circuit Theory - 11th Edition - Robert L. BoyleStad and
    Louis Nashelsky
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CMOS VLSI Design - 4th Edition - Neil H. E. Weste and David M. Harris
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fundamentals of Semiconductor Manufacturing and Process Control - Gary S. May
    and Costas J. Spanos
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fabrication Engineering at the Micro and Nanoscale - 3rd Edition - Stephen A.
    Campbell
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - Graphics Double Data Rate (GDDR5) SGRAM Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - Compression Attached Memory Module (CAMM2) Common Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Clocked Small Outline Dual Inline Memory Module (CSODIMM)
    Common Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDR5 Clocked Unbuffered Dual Inline Memory Module (CUDIMM) Common Specification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 262 Pin SODIMM Connector Performance Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Unbuffered Dual Inline Memory Module (UDIMM) Common Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 288 Pin U/R/LR DIMM Connector Performance Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Load Reduced (LRDIMM) and Registered Dual Inline Memory
    Module (RDIMM) Common Specification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Clock Driver Definition (DDR5CKD01)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '15.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Small Outline Dual Inline Memory Module (SODIMM) Common
    Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '16.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Registering Clock Driver Definition (DDR5RCD03)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '17.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 DIMM Labels
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '18.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - GDDR5 Measurement Procedures
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '19.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 Serial Presence Detect (SPD) Contents
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '20.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - Graphics Double Data Rate (GDDR5X) SGRAM Standard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '21.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JEDEC Standard - DDR5 SDRAM
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '22.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving Memory Reliability by Bounding DRAM Faults - KJERSTEN CRISS, KULJIT
    BAINS, RAJAT AGARWAL, TANJ BENNETT, TERRY GRUNZKE, JANGRYUL KEITH KIM, HOEJU CHUNG,
    MUNSEON JANG
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '23.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizing DDR5 address signal integrity using stochastic learning algorithms
    - Nitin Bhagwath, Daniel DeAraujo, Jayaprakash Balachandran, BaekKyu Choi
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '24.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDR5 Electrical Challenges in High-Speed Server Design - Douglas Winterberg,
    Vijender Kumar, Tom Chen, Bhyrav Mutnury
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '25.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling of DDR5 Signaling from Jitter Sequences to Accurate Bit Error Rate
    (BER) - Alaeddin A. Aydiner, Yunhui Chu, Oleg Mikulchenko, Jin Yan, Robert J.
    Friar, Ellen Yan Fu
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '26.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LPDDR5 (6.4 Gbps) 1-tap DFE Optimal Weight Determination - Sunil Gupta, Ph.D.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '27.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Far-End Crosstalk Mitigation for Transmission Lines in DDR5 Using Glass-Weave
    Coating Structure - Xiao-Bo Yu, Qiang-Ming Cai, Liang Zhang, Chao Zhang, Lin Zhu,
    Xin Cao, and Jun Fan
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '28.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulating DDR5 Systems with Clocked Receivers - Matthew Leslie, Justin Butterfield,
    Randy Wolff
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '29.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design and Analysis of Power Integrity of DDR5 Dual In-Line Memory Modules -
    Shinyoung Park, Vinod Arjun Huddar
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '30.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deterministic Policy Gradient-based Reinforcement Learning for DDR5 Memory Signaling
    Architecture Optimization considering Signal Integrity - Daehwan Lho, Hyunwook
    Park, Keunwoo Kim, Seongguk Kim, Boogyo Sim, Kyungjune Son, Keeyoung Son, Jihun
    Kim, Seonguk Choi, Joonsang Park, Haeyeon Kim, Kyubong Kong, Joungho Kim
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '31.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Advancing DDR5 Test and Measurements: Fine-tuning a Large Language Model AI
    Expert in DDR5 Protocols - Xinran Li'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '32.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDR5 Design Challenges - Nitin Bhagwath, Randy Wolff, Shinichiro Ikeda, Eiji
    Fujine, Ryo Shibata, Yumiko Sugaya, Megumi Ono
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '33.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advanced Measurement and Simulation Approach for DDR5 On-chip SI/PI with the
    Probing Package - WonSuk Choi, SangKeun Kwak, Jaeseok Park, Jiyoung Do, Byeongseon
    Yun, Yoo-jeong Kwon, Dongyeop Kim, Kyudong Lee, Tae young Kim, Wonyoung Kim, Kyoungsun
    Kim, Sung Joo Park, Jeonghyeon Cho and Hoyoung Song
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
