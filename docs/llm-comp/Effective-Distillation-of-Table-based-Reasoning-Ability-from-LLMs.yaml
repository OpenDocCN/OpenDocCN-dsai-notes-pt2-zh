- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:59:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Effective Distillation of Table-based Reasoning Ability from LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.13182](https://ar5iv.labs.arxiv.org/html/2309.13182)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bohao Yang¹, Chen Tang², Kun Zhao³, Chenghao Xiao⁴, Chenghua Lin¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Department of Computer Science, The University of Manchester, UK
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Computer Science, The University of Surrey, UK
  prefs: []
  type: TYPE_NORMAL
- en: ³University of Pittsburgh, US
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Department of Computer Science, The University of Durham, UK
  prefs: []
  type: TYPE_NORMAL
- en: bohaoyang217@gmail.com,  chen.tang@surrey.ac.uk,
  prefs: []
  type: TYPE_NORMAL
- en: kun.zhao@pitt.edu, chenghao.xiao@durham.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: c.lin@manchester.ac.uk  Corresponding author.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have demonstrated remarkable performance across
    a wide range of natural language processing tasks. However, their remarkable parameter
    size and their impressive high requirement of computing resources pose challenges
    for their practical deployment. Recent research has revealed that specific capabilities
    of LLMs, such as numerical reasoning, can be transferred to smaller models through
    distillation. Some studies explore the potential of leveraging LLMs to perform
    table-based reasoning. Nevertheless, prior to our work, there has been no investigation
    into the prospect of specialising table reasoning skills in smaller models specifically
    tailored for table-to-text generation tasks. In this paper, we propose a novel
    table-based reasoning distillation, with the aim of distilling LLMs into tailored,
    smaller models specifically designed for table-based reasoning task. Experimental
    results have shown that a 0.22 billion parameter model (Flan-T5-base) fine-tuned
    using distilled data, not only achieves a significant improvement compared to
    traditionally fine-tuned baselines but also surpasses specific LLMs like gpt-3.5-turbo
    on the scientific table-to-text generation dataset (SciGen). The code and data
    are released in [https://github.com/Bernard-Yang/TableDistill.](https://github.com/Bernard-Yang/TableDistill.)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tables, as a ubiquitous and pivotal means of knowledge storage, has been receiving
    an increasing attention in contemporary research. Tabular data, when juxtaposed
    with textual data, furnishes a valuable and complementary source of information.
    The intersection of tabular and textual information constitutes a well-established
    problem within the domain of Natural Language Processing (NLP), with impacts spanning
    a diverse spectrum of downstream tasks, including table question answering (Pasupat
    and Liang, [2015](#bib.bib33); Cho et al., [2019](#bib.bib10); Nan et al., [2022](#bib.bib31)),
    and table fact checking (Chen et al., [2020c](#bib.bib8); Gupta et al., [2020](#bib.bib16);
    Aly et al., [2021](#bib.bib1); Lu et al., [2023](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Conventional approaches to table-based reasoning (Pasupat and Liang, [2015](#bib.bib33);
    Zhong et al., [2017](#bib.bib47); Yu et al., [2018](#bib.bib42)) have predominantly
    relied on the synthesis of executable languages such as SQL or SPARQL to facilitate
    information retrieval from tables. However, these symbolic languages often entail
    rigid assumptions regarding table structures, rendering them incapable of capturing
    the semantics embedded in textual segments within the table. A holistic comprehension
    of web tables necessitates the understanding of both structured reasoning with
    textual reasoning. For this goal, the emergence of table-based pre-trained models (Herzig
    et al., [2020](#bib.bib17); Liu et al., [2021](#bib.bib26); Jiang et al., [2022](#bib.bib21);
    Cai et al., [2022](#bib.bib4)) has underscored the efficacy of pre-training models
    on both textual and tabular data for augmenting reasoning capabilities. This improvement
    stems from the extensive knowledge obtained from the large-scale crawling or synthesising
    of tabular and textual data.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, the advent of Large Language Models (LLMs) has brought a revolution
    in the landscape of NLP, ushering in a new era marked by their remarkable prowess
    demonstrated across a multitude of tasks (Brown et al., [2020](#bib.bib3); Chowdhery
    et al., [2022](#bib.bib11); Touvron et al., [2023](#bib.bib37)). These models
    leverage vast corpora of textual data, undergoing extensive pre-training, and
    exhibit an exceptional capacity to tackle intricate mathematical and commonsense
    reasoning tasks, often within the context of few-shot and zero-shot learning scenarios
    with Chain-of-Thought (CoT)  (Wei et al., [2022](#bib.bib40); Wang et al., [2022](#bib.bib39);
    Zhou et al., [2023](#bib.bib48); Drozdov et al., [2022](#bib.bib13)). Drawing
    inspiration from these groundbreaking developments, a range of studies (Chen,
    [2023](#bib.bib5); Ye et al., [2023](#bib.bib41); Cheng et al., [2023](#bib.bib9);
    Gemmell and Dalton, [2023](#bib.bib15); Lu et al., [2023](#bib.bib27)) has emerged
    to highlight the competitive performance of LLMs in comparison to state-of-the-art
    fine-tuned models in the domain of table reasoning tasks (i.e., table question
    answering and table fact checking). For instance, Zhao et al. ([2023](#bib.bib46))
    delved into the potential of employing LLMs augmented with CoT techniques in the
    LogicNLG dataset Chen et al. ([2020c](#bib.bib8)), for the table-to-text generation
    tasks. Despite these notable strides, prior research has not focused on the challenging
    domain of complex reasoning-aware scientific table-to-text generation tasks using
    LLMs, nor have attempts been made to distill the intrinsic table-based reasoning
    capabilities of these models into more compact counterparts. In this paper, we
    investigate the capabilities of LLMs in the reasoning-aware scientific table-to-text
    generation, and propose a two-step distillation approach to transfer the table-based
    reasoning ability of LLMs into smaller models. We select SciGen Moosavi et al.
    ([2021](#bib.bib30)) dataset for our experiments, as the task setting of SciGen
    is the first scientific table-to-text dataset and is more challenging than other
    benchmarks, such as LogicNLG Chen et al. ([2020a](#bib.bib6)). Here, the descriptions
    mandate the LLMs to comprehensively grasp the provided tables and engage in arithmetic
    reasoning encompassing both tabular and textual data, rather than merely converting
    the superficial representation of table contents. In the data generation stage,
    we utilise LLMs to generate table-based reasoning and consistency statements based
    on the input table, employing a one-shot CoT methodology. . Subsequently, in the
    fine-tuning phase, we employ the distilled CoT data generated by LLMs to imbue
    smaller models with table reasoning proficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Our experimental results underscore that fine-tuning smaller models with table-based
    reasoning data distilled from LLMs leads to significant performance enhancements
    compared to baseline models in the context of scientific table-to-text generation
    tasks. Distillation empowers models with as few as 0.22 billion parameters to
    outperform larger student models and even surpass the 175 billion-parameter teacher
    model in certain metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf4210818965f33e315b832748607bac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The overview of our framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table-based Reasoning.   Table-based reasoning tasks require the ability to
    reason over both natural language and structured tables. Traditional table-based
    reasoning involves employing semantic parsing to execute commands on tables, several
    benchmarks including WikiTableQuestions Pasupat and Liang ([2015](#bib.bib33)),
    WikiSQL Zhong et al. ([2017](#bib.bib47)), and Spider Yu et al. ([2018](#bib.bib42)).
    These models are designed to produce SQL for interacting with tables. However,
    the machine languages impose strict criteria on tables and make these method cannot
    understand the semantics of text segments in the tables. Some works proposed to
    learn joint representation by pre-training table and text (Herzig et al., [2020](#bib.bib17);
    Liu et al., [2021](#bib.bib26); Zhao et al., [2022](#bib.bib45)). Through pre-training
    the model on extensive synthesized data, they can achieve desirable performance
    on table-related tasks. Recent works Chen ([2023](#bib.bib5)); Ye et al. ([2023](#bib.bib41));
    Nan et al. ([2023](#bib.bib32)) have shown the ability of LLMs in table reasoning
    tasks by in-context learning. Lu et al. ([2023](#bib.bib27)) use LLMs to perform
    reasoning in scientific table fact-checking task. This task require compositional
    reasoning using scientific tables as evidence. BINDER (Cheng et al., [2023](#bib.bib9))
    use Codex to synthesize SQL language to execute logical forms against tables in
    question answering task.
  prefs: []
  type: TYPE_NORMAL
- en: Chian-of-thought Reasoning. CoT prompting encourages LLMs to break down a reasoning
    task into a series of intermediate steps, therefore enhances the reasoning ability
    across various tasks (Wei et al., [2022](#bib.bib40)). With a few CoT reasoning
    examples, the LLMs can achieve the state-of-the-art performance on complex math
    problem sovling. Self-consistency CoT (Wang et al., [2023](#bib.bib38)) involves
    sampling multiple CoTs and selecting the most consistent one by beam searching.
    Kojima et al. ([2022](#bib.bib22)) proposed zero-shot CoT by first generating
    CoT template and produce the final answer with LLMs in zero-shot setting.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation.  Distillation has demonstrated its effectiveness in
    transferring valuable capabilities from a larger model to a smaller one (Hinton
    et al., [2015](#bib.bib18); Sanh et al., [2019](#bib.bib35); Zeng et al., [2022](#bib.bib43)).
    Recent works have shown that synthetic data generated by the teacher model can
    effectively transfer the specialized abilities, i.e. numerical reasoning, to the
    student model. Chung et al. ([2022](#bib.bib12)) use manually generated CoT data
    to finetune a FLAN-based version of PaLM Chowdhery et al. ([2022](#bib.bib11)).
    Fu et al. ([2023](#bib.bib14)) employed enriched chain-of-thought data to specialize
    the small model. Ho et al. ([2023](#bib.bib19)) propose diverse CoT to sample
    different reasoning outputs from LLMs to fine-tune a smaller model. Magister et al.
    ([2023](#bib.bib29)) propose a two-step pipeline for transferring the reasoning
    capabilities of large models to smaller models. Hsieh et al. ([2023](#bib.bib20))
    extracted rationales from LLMs and integrated such data in the smaller model instruction
    tuning framework. Zhu et al. ([2023](#bib.bib49)) distill the program produced
    by LLMs to specialize reasoning ability into small models. We extend the above
    ideas to table-based reasoning task, specifically in scientific table-to-text
    generation task, which the generated CoT data leads to the improved table reasoning
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our proposed framework is illustrated in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Effective Distillation of Table-based Reasoning Ability from LLMs"), which consists
    of two steps: synthesizing data from LLMs and fine-tuning student models with
    the distilled data. The primary purpose of the first stage is to generate table-based
    reasoning and descriptions given the input tables through CoT prompting from LLMs.
    In the second stage, the table-based reasoning ability will be transferred into
    smaller models by fine-tuning with the distilled data.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Task Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the task as follows: The input serialized tabular data is denoted
    as $T$. The generated description should be factually consistent with the given
    table, and contain reasoning over the table.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Table-based Reasoning Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data synthesis process of our proposed method is illustrated in the upper
    part of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Effective Distillation
    of Table-based Reasoning Ability from LLMs"), which is based on in-context learning Brown
    et al. ([2020](#bib.bib3)), the emergent ability of LLMs. Different from traditional
    fine-tuning, in-context learning enables the LLMs make predictions based on the
    input context where only a few examples are demonstrated without parameter updating.
  prefs: []
  type: TYPE_NORMAL
- en: 'We utilize a large teacher LLM to generate table-based reasoning through CoT.
    We formulate the data generation process as follows: given a input serialized
    table $T$ are hand-crafted. Finally, we can generate data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R_{i},Y_{i}=$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where we prepend the demonstrated example $C$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Diverse Reasoning The table-to-text task enables the model to produce varied
    descriptions by focusing on different table regions or performing various reasoning
    operations, provided that the generated descriptions are factually consistent
    to the table Zhao et al. ([2023](#bib.bib46)). To maximize the reasoning ability
    distilled from LLMs, we employ the diverse reasoning Ho et al. ([2023](#bib.bib19));
    Zhu et al. ([2023](#bib.bib49)); Zhao et al. ([2023](#bib.bib46)) to generate
    two different reasonings and descriptions for a given scientific table. We do
    not generate more reasoning-description pairs to a table because the maximal context
    limit of the LLMs and the average length of tables and descriptions of the SciGen
    dataset is larger than other table-to-text datasets. Specifically, the data generation
    process is shown as follows: data with LLMs, given a context $C$, the LLMs are
    required to generate'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\{(R_{1},Y_{1}),(R_{2},Y_{2})\}=$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Data Filtering  Besides, the synthesized table-based CoT data might contains
    wrong samples due to the hallucination Zhu et al. ([2023](#bib.bib49)). Therefore,
    we needto filter the wrong generated CoT data. For filtering, we follow Madaan
    et al. ([2023](#bib.bib28)) to employ Self-Fine method. To be specific, when generating
    a new set of data $(R_{i},Y_{i})$. We can filter out incorrect samples to refine
    our generated CoT data. The verification and filtering is crucial as the high
    quality training data will improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Fine-tuning Small Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we obtained the generated table-based reasoning data, we use them to fine-tune
    smaller models and inject the reasoning ability into them. As for the choice of
    smaller model, we select T5 Raffel et al. ([2019](#bib.bib34)) and Flan-T5 Chung
    et al. ([2022](#bib.bib12)). This is because recent works Fu et al. ([2023](#bib.bib14));
    Zhu et al. ([2023](#bib.bib49)); Magister et al. ([2023](#bib.bib29)) have revealed
    that these models and gain remarkable numerical reasoning ability when training
    with CoT data in complex mathematical problem solving field. We fine-tune the
    smaller model with the generated table-based reasoning data. Specifically, we
    concatenate the table $T$ with the loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=-\frac{1}{N}\sum_{n=1}^{N}\log P(Y\mid T,R)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $N$ is the cross entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct scientific table-to-text generaton on the SciGen dataset Moosavi
    et al. ([2021](#bib.bib30)). The statistics of the data are shown in Table [1](#S4.T1
    "Table 1 ‣ 4.3 Experimental Settings ‣ 4 Experiments ‣ Effective Distillation
    of Table-based Reasoning Ability from LLMs"). It consists of three different settings:
    few-show, medium and large. The train/val/test sets of medium setting are split
    into sizes of 13,607/3,452/1,038\. The large setting is split into 39,969/12,129/1,038\.
    we choose medium and large to conduct the experiments. This is because few-shot
    setting only contains 200 training data and is not enough for fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We follow the Moosavi et al. ([2021](#bib.bib30)) and choose T5 Raffel et al.
    ([2019](#bib.bib34)) and BART Lewis et al. ([2020](#bib.bib23)) as baselines.
    For the BART baseline, we use BART-large pre-trained model with 400M parameters.
    For the T5 model, we use T5-base and T5-large with 220M, and 770M parameters,
    respectively. For the teacher models, we choose gpt-3.5-turbo as baseline. For
    the one shot setting, e follow the previous works Chen ([2023](#bib.bib5)); Zhao
    et al. ([2023](#bib.bib46)), which prepend one demonstration example to the input
    table. We compared with two variants gpt-3.5-turbo, called 1-shot direct and 1-shot
    CoT. For the prompt formulation of 1-shot direct, we follow the setting of Moosavi
    et al. ([2021](#bib.bib30)) to linearize the table and concatenate it with the
    gold description as demonstration. As for the prompt of 1-shot CoT, we prepend
    the input table to two hand-crafted table-based reasonings and descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To use the above text-to-text generation baselines, We follow the setting in Moosavi
    et al. ([2021](#bib.bib30)) and convert tables into the text sequences. To preserve
    and help the model better learn the table structure, we add four special tokens
    to specify the beginning of rows, cells, table caption, and CoT reasoning with
    tokens “<R>”, “<C>”, “<CAP>”, “<CoT>” respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | Text | Train | Val | Test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | 116 | 200 | 100 | 1,038 |'
  prefs: []
  type: TYPE_TB
- en: '| Medium | 124 | 13,607 | 3,452 | 1,038 |'
  prefs: []
  type: TYPE_TB
- en: '| Large | 133 | 39,969 | 12,129 | 1,038 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: SciGen dataset statistics. Text indicates the average length of descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[b] Models #Params Faithfulness-level Surface-level TAPAS-Acc TAPEX-Acc Meteor
    BERTScore BLEURT Teacher Model gpt-3.5-turbo (1-shot direct) 175B 72.34 70.48
    0.09 0.85 -0.91 gpt-3.5-turbo (1-shot CoT) 175B 82.53 84.99 0.09 0.83 -0.96 Medium
    Setting BART-large 0.40B 57.45 58.41 0.23 0.84 -0.72 T5-base 0.22B 53.27 52.45
    0.15 0.82 -0.89 T5-large 0.77B 56.32 54.78 0.17 0.83 -0.77 Large Setting BART-large
    0.40B 59.69 61.38 0.15 0.82 -0.89 T5-base 0.22B 55.32 53.76 0.15 0.82 -0.85 T5-large
    0.77B 58.21 56.32 0.18 0.83 -0.79 CoT fine tuning T5-base-CoT 0.22B 78.16 82.30
    0.08 0.83 -0.89 T5-large-CoT 0.77B 80.62 81.97 0.07 0.82 -0.89 Flan-T5-base-CoT
    0.22B 78.72 82.75 0.08 0.82 -0.89 Flan-T5-large-CoT 0.77B 79.05 82.53 0.06 0.83
    -0.89'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Performance on the SciGen test set. Medium and large setting indicates
    the datasets’ setting used for training.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Automatic Evaluation Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We utilized a wide range of automatic evaluation metrics from various levels
    to assess the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Surface-level  Following Moosavi et al. ([2021](#bib.bib30)), we choose the
    METEOR Banerjee and Lavie ([2005](#bib.bib2)), BERTScore Zhang et al. ([2020](#bib.bib44)),
    and BLEURT Sellam et al. ([2020](#bib.bib36)) to measure the surface similarity
    of the generated statements to the gold references. However,  Moosavi et al. ([2021](#bib.bib30))
    stated that these metrics are not sufficient as the value range is quite low except
    for BERTScore. In addition, in some cases, the incorrect description have the
    higher metric scores than the correct ones.
  prefs: []
  type: TYPE_NORMAL
- en: Faithfulness-level  Recent works Moosavi et al. ([2021](#bib.bib30)); Liu et al.
    ([2022a](#bib.bib24)) have pointed out that the above surface-level metrics cannot
    measure the factual correctness of the generated descriptions given the corresponding
    tables. The SciGen task requires the model to generate statements which contains
    numerical reasoning over table values. In addition, the generated statements might
    cover a different table region from the gold reference. Therefore, we add two
    faithfulness-level (whether the generated sentence is grounded on the input table)
    metrics, TAPAS-Acc and TAPEX-Acc Liu et al. ([2022a](#bib.bib24)) to evaluate
    the factual consistency and fidelity, which have been widely used for table-to-text
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: TAPAS-Acc fine-tunes TAPAS Herzig et al. ([2020](#bib.bib17)) on the TabFact
    dataset Chen et al. ([2020b](#bib.bib7)) and achieves 81% test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: TAPEX-Acc use TAPEX Liu et al. ([2022b](#bib.bib25)) which is fine-tuned on
    the TabFact dataset and achieves 84% test accuracy. Previous works Liu et al.
    ([2022a](#bib.bib24)); Zhao et al. ([2023](#bib.bib46)) stated that TAPAS-Acc
    is overly positive about the predictions, while TAPEX-Acc is more reliable for
    the evaluation of the faithfulness of generated sentences. Both above reference-free
    metrics score the generated descriptions as 0 for refuted and 1 for entailed given
    the corresponding tables.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we evaluate both the performance of teacher LLMs and the fine-tuned
    smaller models on scientific table-to-text task. We conduct automated evaluation
    on both Surface-level and Faithfulness-level metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Performance of LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our experiments include two in-context learning methods, Direct Prompt and CoT
    Prompt on the SciGen dataset. As shown in Table [2](#S4.T2 "Table 2 ‣ 4.3 Experimental
    Settings ‣ 4 Experiments ‣ Effective Distillation of Table-based Reasoning Ability
    from LLMs"), on surface-level metrics, both Direct Prompt and CoT Prompt cannot
    achieve the best performance, except for the Direct Prompt achieves the best BERTScore.
    However, the surface-level metrics are unable to accurately measure the faithfulness
    and accuracy of the models’ generated outputs. In terms of the faithfulness-level
    metrics, Direct Prompt can already achieve over 70% accuracy on both TAPAS-Acc
    and TAPEX-Acc, which outperform the traditional fine-tuned baseline models (i.e.
    BART and T5). When combined with CoT reasoning, the accuracy of CoT Prompt increases
    by at least 10% on both metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Performance of Fine-tuned Smaller Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As for the surface-level metrics, all the smaller models, whether they are fine-tuned
    with CoT data or not, can only show a low value range on them. The experimental
    results is consistent with the statements in SciGen’s paper that surface-level
    metrics are not sufficient to reflect models’ abilities on this complex task.
  prefs: []
  type: TYPE_NORMAL
- en: Small models with traditional fine-tuning perform not well on faithfulness-level
    metrics. In terms of the Smaller Model fine-tuned without CoT data, BART-large
    fine-tuned with medium dataset achieves the best on surface-level metrics. However,
    in terms of faithfulness-level, all the BART and T5 baselines can only achieve
    an accuracy slightly higher than random guess. Besides, we investigate the impact
    of dataset size, ranging from the Medium Setting to the Large Setting. Although
    the size of Large Setting dataset is three times than the Medium Setting one,
    performance improvements are not significant, i.e., only around 2% increase on
    the faithfulness-level metrics. However, for the surface-level metrics, models
    that are trained with Medium datasets can achieves better overall performances,
    especially in Meteor and BLEURT metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Small models fine-tuned with CoT data achieve a significant performance improvement.
    On the other hand, the T5 and Flan-T5 models with CoT fine-tuning can achieve
    the best overall performance on the faithfulness-level metrics among all the small
    models. All the performances of CoT fine-tuning models are on par with the teacher
    model, i.e. gpt-3.5-turbo with one shot CoT, on the faithfulness-level metrics.
    For instance, T5-large-CoT and Flan-T5-base-CoT achieve the highest TAPAS-Acc
    (80.62%) and TAPEX-Acc (82.75%), and only underperform the teacher model with
    the best performance by a margin of 2%. These results indicate that fine-tuning
    with CoT data distilled from LLMs can transfer the table-based reasoning ability
    into smaller models. Furthermore, our experiments also investigate the impact
    of the model size for CoT fine-tuning, ranging from the base to the large variant.
    While it is intuitive to expect performance improvements with larger models, the
    experimental results on TAPEX-Acc metric reveal that models with larger parameters,
    such as T5-large and Flan-T5-large, do not consistently outperform their smaller
    counterparts, T5-base and Flan-T5-base. However, as for the TAPAS-Acc, the performance
    improvement is consistent, with the model size increasing from base (0.22B) to
    large (0.77B).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduce a two-stage distillation framework that distills
    table-based CoT data from LLMs. Our experiments illustrate that this method effectively
    transfer table reasoning abilities to smaller models in scientific table-to-text
    generation task. The performance improvement can even outperform certain teacher
    LLMs (e.g., gpt-3.5-turbo with one shot). Our proposed method achieves comprehensive
    superiority while utilizing less data and smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: 7 References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aly et al. (2021) Rami Aly, Zhijiang Guo, M. Schlichtkrull, James Thorne, Andreas
    Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021.
    [Feverous: Fact extraction and verification over unstructured and structured information](https://api.semanticscholar.org/CorpusID:235391052).
    *ArXiv*, abs/2106.05707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
    An automatic metric for mt evaluation with improved correlation with human judgments.
    In *IEEvaluation@ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://api.semanticscholar.org/CorpusID:218971783).
    *ArXiv*, abs/2005.14165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2022) Zefeng Cai, Xiangyu Li, Binyuan Hui, Min Yang, Bowen Li,
    Binhua Li, Zhen Cao, Weijie Li, Fei Huang, Luo Si, and Yongbin Li. 2022. [Star:
    Sql guided pre-training for context-dependent text-to-sql parsing](https://api.semanticscholar.org/CorpusID:253080452).
    *ArXiv*, abs/2210.11888.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen (2023) Wenhu Chen. 2023. [Large Language Models are few(1)-shot Table
    Reasoners](https://aclanthology.org/2023.findings-eacl.83). In *Findings of the
    Association for Computational Linguistics: EACL 2023*, pages 1120–1130, Dubrovnik,
    Croatia. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang
    Wang. 2020a. [Logical Natural Language Generation from Open-Domain Tables](https://doi.org/10.48550/arXiv.2004.10404).
    ArXiv:2004.10404 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong
    Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020b. TABFACT: A LARGE-SCALE
    DATASET FOR TABLE- BASED FACT VERIFICATION.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020c) Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai
    Zhang, Sairam Sundaresan, and William Yang Wang. 2020c. [Logic2Text: High-Fidelity
    Natural Language Generation from Logical Forms](http://arxiv.org/abs/2004.14579).
    *arXiv:2004.14579 [cs]*. ArXiv: 2004.14579.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2023) Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul
    Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
    Noah A. Smith, and Tao Yu. 2023. [Binding Language Models in Symbolic Languages](http://arxiv.org/abs/2210.02875).
    ArXiv:2210.02875 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2019) Minseok Cho, Gyeongbok Lee, and Seung won Hwang. 2019. [Explanatory
    and actionable debugging for machine learning: A tableqa demonstration](https://api.semanticscholar.org/CorpusID:197928300).
    *Proceedings of the 42nd International ACM SIGIR Conference on Research and Development
    in Information Retrieval*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran,
    Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
    Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
    Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson,
    Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
    Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
    Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
    Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
    Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.
    Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. [Palm:
    Scaling language modeling with pathways](https://api.semanticscholar.org/CorpusID:247951931).
    *ArXiv*, abs/2204.02311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping
    Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob
    Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. [Scaling instruction-finetuned
    language models](https://api.semanticscholar.org/CorpusID:253018554). *ArXiv*,
    abs/2210.11416.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drozdov et al. (2022) Andrew Drozdov, Jiawei Zhou, Radu Florian, Andrew McCallum,
    Tahira Naseem, Yoon Kim, and Ramon Fernandez Astudillo. 2022. [Inducing and Using
    Alignments for Transition-based AMR Parsing](http://arxiv.org/abs/2205.01464).
    ArXiv:2205.01464 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.
    2023. [Specializing Smaller Language Models towards Multi-Step Reasoning](http://arxiv.org/abs/2301.12726).
    ArXiv:2301.12726 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemmell and Dalton (2023) Carlos Gemmell and Jeffrey Stephen Dalton. 2023.
    [Generate, transform, answer: Question specific tool synthesis for tabular data](https://api.semanticscholar.org/CorpusID:257622721).
    *ArXiv*, abs/2303.10138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2020) Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar.
    2020. [Infotabs: Inference on tables as semi-structured data](https://api.semanticscholar.org/CorpusID:218614095).
    In *Annual Meeting of the Association for Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herzig et al. (2020) Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller,
    Francesco Piccinno, and Julian Martin Eisenschlos. 2020. [TAPAS: Weakly Supervised
    Table Parsing via Pre-training](http://arxiv.org/abs/2004.02349). *arXiv:2004.02349
    [cs]*. ArXiv: 2004.02349.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
    [Distilling the knowledge in a neural network](https://api.semanticscholar.org/CorpusID:7200347).
    *ArXiv*, abs/1503.02531.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2023) Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023. [Large Language
    Models Are Reasoning Teachers](https://doi.org/10.48550/arXiv.2212.10071). ArXiv:2212.10071
    [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander J. Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. [Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes](https://api.semanticscholar.org/CorpusID:258461606).
    *ArXiv*, abs/2305.02301.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2022) Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and
    Weizhu Chen. 2022. [Omnitab: Pretraining with natural and synthetic data for few-shot
    table-based question answering](https://api.semanticscholar.org/CorpusID:250390443).
    In *North American Chapter of the Association for Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://api.semanticscholar.org/CorpusID:249017743).
    *ArXiv*, abs/2205.11916.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension](https://doi.org/10.18653/v1/2020.acl-main.703).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7871–7880, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and Dongmei
    Zhang. 2022a. [PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation](https://doi.org/10.18653/v1/2022.emnlp-main.373).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 5531–5546, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin,
    Weizhu Chen, and Jian-Guang Lou. 2022b. TAPEX: TABLE PRE-TRAINING VIA LEARNING
    A NEURAL SQL EXECUTOR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Tianyu Liu, Xin Zheng, Baobao Chang, and Zhifang Sui. 2021.
    [Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric
    View](http://arxiv.org/abs/2102.08585). *arXiv:2102.08585 [cs]*. ArXiv: 2102.08585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen
    Kan. 2023. [SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim
    Verification on Scientific Tables](http://arxiv.org/abs/2305.13186). ArXiv:2305.13186
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2023. Self-refine: Iterative refinement with self-feedback. *arXiv preprint
    arXiv:2303.17651*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magister et al. (2023) Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek,
    Eric Malmi, and Aliaksei Severyn. 2023. [Teaching Small Language Models to Reason](http://arxiv.org/abs/2212.08410).
    ArXiv:2212.08410 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi et al. (2021) N. Moosavi, Andreas Rücklé, D. Roth, and Iryna Gurevych.
    2021. [SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables](https://www.semanticscholar.org/paper/SciGen%3A-a-Dataset-for-Reasoning-Aware-Text-from-Moosavi-R%C3%BCckl%C3%A9/1893f9875fe6a5b40b82838aa3a4259f5763d7f0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nan et al. (2022) Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin,
    Neha Verma, Rui Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru
    Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham,
    Caiming Xiong, Dragomir Radev, and Dragomir Radev. 2022. [FeTaQA: Free-form Table
    Question Answering](https://doi.org/10.1162/tacl_a_00446). *Transactions of the
    Association for Computational Linguistics*, 10:35–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nan et al. (2023) Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung
    Tae, Ellen Zhang, Arman Cohan, and Dragomir Radev. 2023. Enhancing few-shot text-to-sql
    capabilities of large language models: A study on prompt design strategies. *arXiv
    preprint arXiv:2305.12586*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasupat and Liang (2015) Panupong Pasupat and Percy Liang. 2015. [Compositional
    semantic parsing on semi-structured tables](https://api.semanticscholar.org/CorpusID:9027681).
    In *Annual Meeting of the Association for Computational Linguistics*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2019) Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine
    Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.
    [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/3cfb319689f06bf04c2e28399361f414ca32c4b3).
    *ArXiv*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. [Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter](https://api.semanticscholar.org/CorpusID:203626972). *ArXiv*, abs/1910.01108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sellam et al. (2020) Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020.
    Bleurt: Learning robust metrics for text generation. In *ACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://api.semanticscholar.org/CorpusID:257219404).
    *ArXiv*, abs/2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. [Self-Consistency Improves
    Chain of Thought Reasoning in Language Models](https://doi.org/10.48550/arXiv.2203.11171).
    ArXiv:2203.11171 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai
    hsin Chi, and Denny Zhou. 2022. [Self-consistency improves chain of thought reasoning
    in language models](https://api.semanticscholar.org/CorpusID:247595263). *ArXiv*,
    abs/2203.11171.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai
    hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. [Chain of thought prompting elicits
    reasoning in large language models](https://api.semanticscholar.org/CorpusID:246411621).
    *ArXiv*, abs/2201.11903.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023) Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and
    Yongbin Li. 2023. [Large Language Models are Versatile Decomposers: Decompose
    Evidence and Questions for Table-based Reasoning](http://arxiv.org/abs/2301.13808).
    ArXiv:2301.13808 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Tao Yu, Rui Zhang, Kai-Chou Yang, Michihiro Yasunaga, Dongxu
    Wang, Zifan Li, James Ma, Irene Z Li, Qingning Yao, Shanelle Roman, Zilin Zhang,
    and Dragomir R. Radev. 2018. [Spider: A large-scale human-labeled dataset for
    complex and cross-domain semantic parsing and text-to-sql task](https://api.semanticscholar.org/CorpusID:52815560).
    *ArXiv*, abs/1809.08887.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan
    Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang.
    2022. [Glm-130b: An open bilingual pre-trained model](https://api.semanticscholar.org/CorpusID:252715691).
    *ArXiv*, abs/2210.02414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. *ArXiv*,
    abs/1904.09675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022) Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, and Dragomir R.
    Radev. 2022. [Reastap: Injecting table reasoning skills during pre-training via
    synthetic reasoning examples](https://api.semanticscholar.org/CorpusID:253097905).
    In *Conference on Empirical Methods in Natural Language Processing*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru
    Tang, and Arman Cohan. 2023. [Large Language Models are Effective Table-to-Text
    Generators, Evaluators, and Feedback Providers](http://arxiv.org/abs/2305.14987).
    ArXiv:2305.14987 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    [Seq2sql: Generating structured queries from natural language using reinforcement
    learning](https://api.semanticscholar.org/CorpusID:25156106). *ArXiv*, abs/1709.00103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and
    Ed Chi. 2023. LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE
    MODELS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen
    Zhou. 2023. [PaD: Program-aided Distillation Specializes Large Models in Reasoning](https://doi.org/10.48550/arXiv.2305.13888).
    ArXiv:2305.13888 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
