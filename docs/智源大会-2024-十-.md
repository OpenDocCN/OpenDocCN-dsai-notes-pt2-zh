# 智源大会 2024（十）



# 2024北京智源大会-生成模型 - P4：视觉生成中的若干问题；古纾旸 - 智源社区 - BV1DS411w7hz

这个怎么控，好可以好，行那就这样吧，嗯好，大家好，我是谷舜阳，今天这个标题的title写的有点大，我一直在想我要不要起这个title，最后还是起了这个title，主要有两个原因，第一个原因是。

其实最近跟很多朋友，包括在读博的学生，包括一些研究员，年轻的研究员，甚至还包括一些很资深的研究员在聊，大家都觉得，现在这些大模型出来，我们做research的没什么好做的了，我自己其实不是太同意。

所以来阐明一下自己的观点，然后第二个事情是，其实这个title很大，正常应该是那些大佬说的，不应该有我这样的小卡拉咪来，在这边说这些事，说这个事主要是，也是阐述一下自己的观点，希望抛砖引玉。

希望大家能多argue我，我们在讨论中，获得一些新的认识，所以我会尽量讲快一点，多留一些时间我们来QA一下，好，title叫。

Several questions for visual generation，但我做完这个slides我发现，其实，其实它好像动不了，ok，可以动，其实只有一个问题，对。

就所有的问题都是围着一个问题的，这个问题叫做visual signal decomposition，视觉信号拆解，我个人认为这个是，目前这个视觉生成率，几乎最重要的问题了，我后面来看一下这个问题。

它的这个原因，以及我们现在做的事情，和它到底有什么关系，好我们来第一个问题，第一个问题是引述它的，是我们做这个生成模型，生成模型的目标是什么，生成模型的，我自己理解，生成模型的目标就是这个。

我们把这个计算机所得，这个用户在想什么，我们就把它生成出来，that's ok，所以其实第一步做的事情，其实是一个人机交互的事情，就是我们怎么把用户想的这些事情，变成这个计算机可以理解的。

这些语言或者instruction，或者一些什么样的东西，第二部分呢，就是好计算机知道了，我们要生成什么，那我们就把这个事情也生成出来，我们也知道这个生成模型，其实是一个sampling模型。

所以其实我们要生成的是，我们从manifold learning的角度来说，我们要生成的是p-target，这个distribution里面的一个样本，但p-target的distribution。

我们是拿不到的，所以我们希望我们能生成一个，可采样的数据分布，叫做p-generated，然后它和p-target，两个distribution是一致的，这样我们从可采样的p-generated。

里面做sample，其实就相当于是，我们在这个p-target里面做sample，这个东西其实就是我们的生成过程，但是问题来了，这个p-target的数据分布，其实有可能非常复杂，我们实在拟合不了。

我们想用p-generated，拟合这个p-target，但p-target太复杂了，我们找不到一个p-generated，可以拟合它，我们其实除了第一部分，在做这个和人机交互相关的。

剩下我觉得这个生成模型，这边所有的问题，其实都是这个问题，因此这么多年来，我们做生成模型，不管是人机视觉，还是language，还是什么其他的，Digital signal processing。

这边做generation，其实一直在追求，我们怎么能提升，这个模型的modeling capacity，让模型能力增强，所以这个生成模型的发展过程，其实就是一个，模型能力增强的过程。

从energy based model，到GAN VAE，到Diffusion model，都做的是一样的事，然后在这个问题之后，下一个问题就是，我们怎么样做得更efficient。

怎么样做得更explainable，涉及到security等一系列问题，这个问题也很重要，但我今天不太会涉及这个方面，我今天主要提的就是第二个问题，就是distribution modeling里面。

这个数据分布太复杂了，怎么办呢，针对这个问题，其实大部分人的想法都一样，你的一个数据分布太复杂了，那我们就把这个复杂问题拆分，拆分成多个简单的问题，That's ok，所以回到我们，所以回到我们的。

视觉生成领域，问题就是我们怎么样，把我们的视觉信号做拆分，这样我们把一个复杂的任务，拆成多个简单的任务，然后我们再对，这个简单的任务，逐一做modeling，就好了，我们现在来看看。

这个language领域，是怎么做这个事情，language领域非常细腻，因为language非常，在这个问题上，language领域是非常简单，首先language的distribution。

也非常复杂，是吧，大规模的语料库，这个数据量非常大，但其实他们的数据拆分非常直观，比如说举个例子，我喜欢吃苹果，它一个字是一个字，一个token是一个token，那我们简单的就把它。

按照这个token来进行拆分就好了，是吧，所以我们把一个复杂的数据，Px，这边有激光笔吗，没关系，对，就是我们把一个复杂的数据分布，Px的问题，直接拆分成了，next token prediction。

每次根据前面的这些token，来预测后面的token，所以我拆分成了n个task，其中第2个task，就是我在预测si这个token，其中是依赖于i前面的这些token，来进行预测的。

我们来举一个再具体一点的例子，假设我们的语调库里面有两个句子，一个叫I love swimming，一个叫You love playing basketball，I love swimming。

所以我们在做next token prediction的时候，我们如果拿第1个句子，那它拆成了第2个task，是我根据I love来predict，这个swimming，然后根据这个第2个句子。

如果我这个，也一样的方式做拆分，那它的第7个task是，根据You love playing basketball，I love来predict这个swimming的事情。

事实上当这个Corpus非常大的时候，这个不同的任务之间，他们在这个第2个任务，第7个任务第5个任务，其实他们之间是没有冲突的，就是，意思就是当我学习第2个任务，它并不会影响我学习第7个任务。

当我这个学习，You love playing basketball，I love来predict swimming这个任务的时候，它对于我做这个。

有了I love来predict swimming这个任务来说，其实它是有帮助的，或者至少来说它是没有冲突的，他们甚至可以看成是一种，互相是一种augmentation，当我数据量足够大的情况下。

所以我们把这个东西，叫这个equivalent，我自己起的名字，我们把它叫做这个等价性，不同任务之间它没有冲突，他们是这个互相等价的互相帮助的，这个东西对于language来说特别重要。

我们来看看在这个vision里面是什么样，对，就就是因为前面前面的这个的成功，所以呢我们能把这个language，这个很复杂的任务，拆分成各个简单的任务，拆分成简单的任务。

然后我们这个就能很自然的scale up，并且这个直接少了很多这个不必要的麻烦，所以呢这个visionguides，看到了这个languageguides，做的这么成功。

就想我们是不是可以这个照葫芦画瓢，把这个事情也抄一遍，那这个抄的方式也很简单，这个就是我们先把这个图像，他们做的tokenization，分成了一个一个离散token，是不是离散这个事情其实不重要。

但至少分成了一个个token，那我们这边也干一样的事情，我们先把它这个切成块，切成这个16块，这个32块或者更多的块，然后呢我们再来一个这个auto-regressive model。

来这个progressive的来做生成，是吧这里面有很多这个代表性工作，比如这个Igpt，比如Darling，比如这个Party，比如这个Tarmin Transformer，就是VQ干了。

对做的都挺好的挺好的，但是呢他们之间其实，这个其实都都有这个。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_1.png)

unequivalent的这个问题，是吧我们来举个例子，这个是他们的这个信号拆解方式，我们把其中的一些task拿出来，就比如说这个上面是这个sk-1，sk这个这个token，下面是sk-1，sk。

 sk+1，所以这个Decade task做的事情，其实是根据这个，K前面的这些token来预测Decade token，这个J是J前面的预测J，但但其实这个我们我们这边只是为了说明简单。

我们稍微做个简化，他其实把K之前呢我们就当成是，这个他前面一个，因为他这个相关性最高，其实这个不对，我只是为了把这个声音给，讲的稍微更clear一些，对所以我们把它近似当成是。

我们从K-1在predict K，从J-1在predict J，下面一个task就是从这个J来predict，这个J+1这么个task，然后我们发现我们把这些这个condition，画在了这个后面。

根据哪个图像块来预测哪个图像块，哎我们发现这个其实，对于某些他是continuous的，这个对于另一些他不是continuous的，是吧所以你你当你用一个需要。

parameter的一个model来做这个事情的时候，他其实是在学习两个conflict的事情，他到底要不要学习这个前后token之间的，这个连续性能，当然其实我用连续性只是举个例子。

连续性其实不是最重要的事情，其实这个conflict的事情出现在，出现在所有的所有的事情里面都有，对continuous只是一个相对最容易理解的，再打个比方，比如说我们做这个，做一些和human相关的。

是吧其实大部分我们connect的数据集，都有都是有很多bios的，比如我的人脸在中间，对那那对于像这种情况，这个你其实在不同task你学习的，其实也是不一样的，比如我有的task。

fixed resolution的这些task，有的task这个我学习的，我可能尽可能的学习眼睛学习一些别的，是吧那对于其他那些其实你学的，根本不是这样的东西。

这就会导致另一个这个not equivalent的，这个事情，对所以这个因为就patch之外，这个是不是还有其他的一些拆分方式，能让我们更equivalent一些呢。

一个典型的做法是用这个depth来做拆分，是吧depth这个很好理解，我们对于同一个space位置，我们可以这个拆成多个channel，拆成多个什么样最简单的就是这个RGB，但RGB也可以再拆分吗。

因为这个这个这个一个一个R通道，也是8个bit，我们也可以把这个RGB进一步拆分成24个，这个这个bit来怎么样，对吧理论上来说这样我们也是把一个复杂的，24个channel的这个。

不说channel吧，就三个channel的这个复杂的分布，我们也拆成了三个简单的分布，我们是不是可以利用他们之间的一些关系啊，来做一些事，当然RGB这个只是for example。

这个其实大家还会做得更优雅一些，比如说用这个VQ-VAE-2我们做这个层级化，再比如说我们用这个RQ-VAE，我们每次quantize一次，再这个做一些resizer，再quantize一次。

然后这个它也是一个，这个这个在depth维度上做了一个信号拆分，但actually我想说的是，这个depth上的这些decomposition，他们也做不到这个equivalent的这一点。

对下面下面我以这个，当sample up sample，算是另一种这个depth的decomposition，也举了个例子吧，就是你在不同depth来做的情况下，你也不能保证你做的事情一样了。

比如前面第一个task啊，他做了一点这个虚模窝，再做一些这个低频信号，后面再做一些高频信号，你们之间其实也是有conflict的，所以这边我其实有更好的一些例子，后面等一下来讲的更清楚一些。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_3.png)

对所以所以对于这些not equivalent的问题，我们怎么来解决这个问题呢，其实其实无非两类做法，就第一我们现有model来做，第二我们不现有model来做，刚刚江老师也说了，这个现有model。

其实这个VAR就是一个典型的现有model的，就是我知道我在不同reolution上，我我我从这个2×2，你和4×4和这个8×8，你和16×16，其实不一样，没关系，这个这个我我有钱，这个我模型够大。

我不在意性价比，那我就那我就做就好了，是吧那那那他这个导致的问题，就是我在不同task上面，可能是有conflict，对这个影响的是什么，影响的是当我们的数据分布，真正非常复杂。

比如我们做一些真正inner wide的，或者超越inner wide的一些，这个distribution的时候，这个你不在意性价比不行了，这个这个地主家也没有余粮了，我就这么多这个参数嘛是吧。

就这么多参数就这么多flow数，你的这个任务拆解不合理的话，这个再复杂数据分布我也做不了，这是一种，第二种是这个not shell model，对我我提一下，就是这个这个每个都是有很多很多工作。

这个我我我只是for example，对肯定会miss很多，对那not shell model这边，我举的一个例子是这个muse的例子，muse就是典型的利用这个VKBE做的拆分。

然后先把这个low resolution，这个distribution我先modeling好，然后根据这个low resolution distribution。

我来modeling这个high resolution distribution，是吧，然后这种这种问题是第一，第一，它的这个parameter数，当你的这个拆分数真的足够多的时候。

你这个parameter数，你是一个O(n)这个数量级的，你其实很难做，你比如说这个这个一个是一个两B，那你这个拆分成10个就是20B了，20B不是不能做，但你真正做拆分的时候。

这个这个参数量和这个flow数，你完全不shell，其实是很痛苦的一个事情，而第二个事情是叫这个invalid encoding issue，就是我们实际在做的过程中，这个大部分人不会跟你们说。

这个但实际做的过程中，你们会发现这个这个你很难真正的把它拆的又长，每个每个token每个depth上面，或者叫depth上面每个位置，它都是一个有效的信息，这个东西我放在后面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_5.png)

我会这个专门来说这个问题，所以除了这个depth的decomposition以外，现在最火的这个这个这个拆分方式，其实是diffusion。

diffusion叫做noise intensity decomposition，其实问题是一样的，我们要拟合这个PX0，PX0太复杂了，那我们就用这个，这应该是全概率公式。

我们用全概率公式对它进行拆分，拆分成了啪啪啪啪task，一个task从X1到X0，第二个从X2到X1，然后这样依次往后做拆分，在这样子的情况下，我们的每一个xt是我们predefined的。

通过这个后面的这个加招公式，xt=αt x0+c8t，我们来拿到xt的这个数据分布，这样我们就define好了，每个sub task，它到底是在做一个什么样分布。

到什么样分布的一个distribution mapping，然后下面画一个这个diffusion的这么一个，简单的一个示意图。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_7.png)

是吧，在这个情况下，我们再来想一想，这种信号拆分方式，它是equivalent的吗，其实它其实也不是equivalent的，它有非常多的问题，它在不同time step。

或者叫不同的noisy testing。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_9.png)

他们这个学习到的信息，其实totally不一样，是吧，我们也举几个例子，第一个这个edify，这个TerraCurrents，他们做的这个工作，他们在这个inference过程中。

前后选了不同的这个prompt，然后发现这个有的prompt，在前面后面起的作用也不一样，这个在diffusion的这个早期阶段，他可能更学一些这个low level的structure。

这个后面的阶段，可能更学一些high frequency的一些，这个detail啊或者其他的一些信息，然后第二个是我们自己做的一个工作，mini snr，这个所有事情就更直白了，是吧。

我们这个diffusion大部分，比如拆成1000个time step，我们就对着中间某一个time step，给我猛tune，看我tune这个time step，你其他time step。

对我来说到底是不是有好处的，但讲到底其实就是我对着这个，这个造成强度某个训练，我看我对其他造成强度来说，到底是有帮助的还是有坏处的，结果是有坏处的，结果是有坏处的，坏处的其实就告诉我们。

这个这个它不同task之间，其实是有冲突的，我把这个task做好了，其他task做差了，是吧，按了葫芦起来调，对，因此这个diffusion model，它其实也是，也是这个。

这个不equivalent的，所以针对这个问题，我们怎么来解呢，这也是两类嘛，这个这个，其实问题都是两类，这个现象model和不现象model，这个不现象model的典型，其实就是EDFI。

他们自己做的，这个，这个这个我们，我们把这个expert，我们用这个多个expert来做，每个expert专门做一个，这个造成强度，然后不同的这个造成强度，我直接这个，拆分成不同的这个模型来做就好了。

所以这边的问题，其实是我的这个模型太多了，我该怎么办，所以这个EDFI的做法也很简单，我们做了一些这个凹差数，来做一些规定，来让它这个数量别这么多，然后另一类呢，就是现象model，现象model这边。

其实我想提一个，提一个事情，其实大家可能没有意识到，特别重要的事情，叫做reparameterization，从参数化，其实对于diffusion的成功来说，从参数化非常重要。

因为diffusion原则上，它做的每个sum of time，做的diffusion mapping，是从xt到xt-1，所以我比如说分成100个，那我一个做的是100到99，一个是98到97。

一个是79到78，这些其实非常diverse，reparameterization做的事情是，OK我不care你input是什么，你input是100就199，就99，98就98，我把你的output。

这个unify成一个diffusion，所以这个reparameterization，这个会把output有的predict，这个noise的data，有的predict。

有的predict clean data，有的predict noise，有的predict v，predict x1-x0，predict everything，that's ok 是吧，因为我总之。

我的input distribution不一样，但我output distribution不一样，output distribution一样，这样我让我的多个task的冲突减弱了。

其实这是一个非常重要的事情，这其实当这个事情，现在已经well known，大家都在用这样的事情，那第二件事情，其实就是我们可以做一些，loss weighting design。

我们这个有些这个像我们min-snr，这个sable division 3，其实都做了一些这样的一些事情，我们这个大家出发点不太一样，我们做min-snr的出发点，其实是这个，这个大家既然有冲突。

好那我们就找一个这个，大家不冲突的方向，这个求同存异，我们一起来这个，一起来进步，是吧，所以这个min-snr，其实做的是找了一个，派对托最优的方向，来做这样的事情，但是这个这些事情，说实话支标不支本。

就是他本，因为他本身上，他还是有这些冲突的，我们只是这个这个这个，这个尽量的去减少这些冲突，因为我的这些冲突，其实其实他客观是存在的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_11.png)

因此我们来想一想，他的这个冲突为啥存在，这个这边是他拆分的，这个sub task，然后呢这个每个sub task，是由上面的这个xt来给定的，然后xt里面的这个又有两个参数。

alpha t跟sigma t，alpha t跟sigma t，probably不一样，但是无所谓，这个事情在，在我们一般提到，这个definition model里面，他们都是predefined的。

所以当我predefined好了，alpha t和sigma t，我的中间每一个数据分布，其实已经给定了，所以给定了这个数据分布以后，他到底有没有冲突。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_13.png)

有多大冲突，这个事情也已经给定了，因此我们想，这个这个问题都出在你这，都出在你这个alpha t，sigma t给定了，然后这些这个没deliver，是吧，我们是不是可以找一种更好的。

这个方式来让你这个减少冲突，所以一个一个一个做法，是用nullable的方式来做，我让alpha t跟sigma t变得可以学习，这样我们是不是可以学一种，让他这个冲突更小的，这么一种方式吗。

这里面的一个典型工作，是variational definition model，这个Keyma他们的工作，对，然后这边的想法就是好，就非常简单，就是这个，就我们说是alpha t sigma t。

但其实alpha t跟sigma t合在一起是snr，平方一下是snr，对snr我们也可以这个，做一些这个log呀什么的，所以他们其实是用一个网络，我们在这个predict这个log snr。

然后predict这个log snr以后，我们就可以拿到这个alpha t和sigma t，然后我们让这个，相当于是家造过程里面的，这两个参数是可学的，我们是不是能学一种更好的，对。

这是one solution，所以所以这个solution，我们把它叫做nullable noise schedule，我们在学习这个家造里面的，这些参数，然后第二个事情，做的就更极端了。

我只学这两个参数不太够，是吧我们是不是可以这个，我们让整个家造过程也是可学的，然后我这边举个例子，这个F alpha n，表示的是一个neural network，所以我们家造的每一个状态。

拿到的这个数据分布，是由我们一个neural network决定的，这边的典型工作是这个，薛定谔桥相关的一些工作，包括这个diffusion shedding bridge。

simplify diffusion shedding bridge，diffusion shedding bridge matching，其实都是这边的一些代表性工作，这里面的核心在于。

我们让整个家造过程和驱造过程，都是可以学习的，家造过程来告诉我们这个，这个每一步来define，我们的这个sub task是什么，驱造过程来对它进行一个measure，我们这个不同的task。

到底是不是这个equivalent，或者怎么怎么样的，对，然后这个，其实neural network，其实也不一定一定是我写的这个形式。

我只是以这个simplify diffusion shedding bridge，因为这个我们自己的工作相对熟悉一些，以这个来举个例子，对，对，然后这种叫做learn a network。

to add noise，我们学了这个，用网络来学习家造过程，对，然后所以总结一下，这个learnable decomposition，这边其实又分为两种，一种是我们学习这个家造里面的参数。

一种是我们学习这个，用一个网络来学习家造，但从我们视角来看，他们其实做的事情都是啊，我们怎么能找一种这个更好的这个信号拆分方式，来让我们的这个视觉信号啊。

当我们这边其实特指的都是invice signal，来让我们的这个这个图像信号，它这个拆分的更合理，大家的conflict更小一些，对，然后这边其实大家看上去很美好，但其实大家做过程中。

其实又有一堆问题，比如这个前面这个VDM的问题啊，这个我们放在后面的时候，后面会专门说，对，比如说这个Bridge这边的问题，其实在于当我们有了前面的那个，难易的加高公式，XT等于αT。

这个等于这个X0和noise，这个αT SigmaT的这个加权的情况，它有个这么simplified的情况，这样我们就能用我前面说的特别重要的，这个reparameterization。

当你没有这个式子的情况下，你每一步都是通过这个neural network来学的时候，你其实很难拿到一个这个很难易的，这个这个这个这个，可以可以直接传递的过程。

这样你就损失了利用这个Diffusion model里面，最重要的这个reparameterization的这个check，所以它进一步又会导致其他的这个冲突，对，所以在实际过程中，这个这边怎么做。

其实还是有一些这个值得研究的空间的，好，然后下面这个来说这个，第三个问题，Tokenization的问题，对，说Tokenization的问题，我们先这个稍微站远一点，说我们现在这个生成模型，生成模型。

我们现在做这些数据生成的时候，其实大部分都分两步来做，第一第一部分呢，我们先做这个signal的这个embedding，来把这个信号做压缩，第二步我们在这个压缩的信号上。

第二步我们在在这个压缩好的这个信号上，再来做这个modeling的过程，对，比如一些这个典型的一些工作，其实其实都是这么做，大家有没有想过为什么要做这个compressing，其实其实事情是一样。

就是这个px太复杂了，是吧，我们要有些办法，能把这个px纬度降低，数据分布变得简单，那我们这个就就就做它就好了，那我们后面这个压力就降低了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_15.png)

对，然后我们来想想这个compressing是怎么做的，language里面很简单，是吧，假如我有一个这个language，一个一个一个sentence。

叫做I like playing basketball，那我们把它做这个做这个这个编码啊，我这个编码成这个里三头三一七四，然后我们再对它做这个decoder，哎，它直接就出来。

I like playing basketball，这个是因为它无损的，很漂亮，是吧，然后压缩成了这个漂亮漂亮的，每个是自己的一个token，英语机里面不是，英语机里面这个这个我们常常用的这些。

不管IEVAE，其实或多或少你都是一个这个有损的一个过程，但有损无损真的重要吗，其实也不是我们这个这个这个越无损越好，这个事情对于这个视觉信号来说肯定是不对的。

我我这边是以这个lookup free quantization，举的个例子，他发现这个压缩的越多，然后这个重建重重建的越好，但后面做modeling的这个越难，这个事情很make sense。

我我我觉得很make sense，不知道大家是不是觉得也make sense，其实是这样，就是当你当你这个重建的越好的时候，你的这个数据分布有可能是越复杂的，就比如说这个大家有想过我们。

比如说做stable diffusion，为什么里面用的是一个VAE而不是用AE吗，因为AE你出来的这个数据分布可能很长很复杂，其实大家根本做不了modeling。

但你如果加一个regularization，不管是KL是quantization的一些regularization，那我的数据分布相对可控一些，这样我后面做modeling的时候就会容易很多。

对其实就是这个原因，对我这边反正是这个local free quantization，来举个例子，所以这个这个事情重建，这个事情就是越无损越好，这个事情不对的，这个命题其实这是一个general的命题。

这个在speech啊在image里面，这个这个其实是well known的一个事情，对然后这个大家的解决方法也很很统一，就是我们把那些没用的这些token也丢了就好了，这个在speech里面很多的这些。

这个不知道大家知不知道，speech的这个信号处理，我也这个略知一二，这个大概就是这个我们有一段这个音频，然后把音频做这个离散幅列变换，然后可能变成16个，可能变成32个，但这个里面有很多这种鱼的。

所以大家做的事情就是，没关系我不care，我只用我前面的这个8个来做拟合就好了，他们重建的也挺好的，因为这里面是一样的，因为这里面我就举一个这个，这个最最拿义务的这个例子。

其实最喜欢干这个事情的人也是Keyma，Keyma他们之前做这个glow，或者他们后面做这个，Volitional Diffusion Model的这些后续工作的时候，他们都是。

然后他们说这个我我拟合这个灰度图，灰度图是8个bit，8个bit太复杂了，这个后面也是没用了，我们就拟合5个bit，诶5个bit出来的结果，这个这个其实比8个bit来拟合，其实还更好一些。

对这个其实是一些well known的一些事情，所以所以general来说，他的solution就是我们把一些这个后面的一些，没用的token也直接给丢了就好了，所以这边我们在想。

我们既然做tokenization，你做了以后我还要丢了，那我是不是这个这个这些就可以不要做，或者我可以做一些这个实验来适应这些的，所以一个直接的想法就是我们可以。

是不是可以做一个这个变长的这个embedding，是吧，RQAE就是一个典型的变长embedding，这个很直观，听名字也很直观，因为他这个RQAE，我做一点留了一点这个渣子，然后再做一次再做一次。

所以这个自然就是这样，但是RQAE其实也有一堆问题，对我们自己其实做了一些实验，发现这个RQAE这个他很容易做了就做了，做到后面，结果又变差了，这个前面这个大家这个重建的越来越好，或者RFID越来越好。

然后做到后面又变差了，尤其随着你这个，尤其是随着你这个debs的增加，他这个出现这些这个这个往上翘，或者这个不往不变好的这些情况，其实比例其实是越来越高的，对我们先把这个东西先把它命名。

叫做embedded embedding的这个问题，这个问题其实是兼而存在的，当我们做这个debs是4的时候，他有30%的可能性，做8的时候他有40%可能性，这个debs做16的时候。

他有50%多的可能性遇到这样的情况，大家如果不相信，可以自己去做一下这个实验，强烈推荐大家做一下，不管你用什么codebase都会浮现这个结果，然后我们把这个我们就以8为例，把这个图打出来看一下。

打出来看一下，发现他在中间某一个stage，他会dominate，到后面他也不怎么会变好了，当然这个还不是naive的RKVAE，因为naiveRKVAE从第一个开始就已经差不多了。

就我们这个这个做了一些，这个information bottleneck，来让这个东西相对可视化的更明显一些，我放这个图，我先这个先先是正好想提一下，另一个问题，大家看我们这个图，我们先把这个图打出来。

然后我们把这个图打出来，然后我们先想提一下，另一个问题，大家看我们在做这个不同task，我们在做这个modeling的时候，你第二个task到第三个task，你显然补了一些这个比较比较有效的一些信息。

当你在这个dominate stage以后，你再去做这个，做这个modeling的时候，其实你，Visually我自己肉眼，其实都看不出有什么变化，对所以其实这个也很直观的。

可以看到你有一个这个不等价的这个问题，是吧，这个也尽致了我们前面说的这个，在depth上说拆分，你要你要注意一下，你到底是不是有这个问题，然后呢，这个图其实还能说明，另一个就是我们前面也是。

来一次说的问题，就是invalid embedding的问题，是吧，你既然四个五个这个token，你已经invalid的结果，跟这个最后都差不多了，你后面那些要他干啥呢，这个也也没也没啥用。

直接丢了就好了，是吧，那我们为什么要干这个事情，所以这个事情其实挺讨厌的，你做一个AQAE，这个这个做到最后，你你给我出了一大堆这个没用的东西，这个为什么会这样呢，哦对前面忘了说了。

这个这个这个talk里面很多东西，都是我自己这个拍脑袋随便想的，可能有80%90%的东西都错的，大家这个听一听就好了，这个欢迎来喷我，对真的欢迎来喷我，对这边也是这个这个就就自己的一些想法。

对这个可能的一个原因是这样，就是我们还以8为例啊，以8为例，所以我们在做这个重建的时候，其实第一个是对这个X0过一个decoder，然后跟ground choose来拿。

然后第二个就是这个X0和X1加在一起，这个过decoder和ground choose来拿，然后巴拉巴拉巴拉，反正8个是这样，所以所以我们这个在train这个AQAE的时候。

其实我们是把这些loss给combine在一起，我当然知道这个这个AQAE的那个中间还有一些loss，但跟这个问题没关系，我们先暂时我们先不讨论它，对然后在这种情况下，我们再做一个这个假设。

我我我我也知道，我我真的知道这个decoder，其实是一个很强的一个非线影音设，但但其实这个如果我们先做分析，把它做个简化，假设它是个linear mapping的话，那你其实把这些loss。

我们也对对这个其实无所谓，就是你到底是不是有一个loss weight的加权，我们先suppose这个loss weight是equal。

所以所以其实你在优化这8个loss的combination的时候，actually你在做的事情是这个，对后面这个loss的优化，当然condition我已经写在这了，所以当你优化这个事情的时候。

你发现你找到的argmin，其实是下面这个十字，下面这个十字问题又来了，那我既然argmin x是后面这个十字的时候，那我凭啥能保证最后一个是最低的呢，大家明白我说的意思了吧。

是吧就是既然我找到了argmin是这个十字，那我并不能保证我的这个x0加到x7，8个token合在一起，哪怕是一个最低的，哎这个就导致了embedded，embedding的问题。

尤其我们可以再做一个分析，假设是吧，你你用这个CL codebook了，x0到x7你这个codebook用的一样的，然后又假设他们这个是绝对同分布的，那你能得到一个更有意思的事情，你看这个这个。

那其实这个你左边的这个argmin x，你可能是跟第五个左右可能是最接近的，你取一个expectation对吧，这就这就是你后面三个就没用了，甚至还可能往上翘，他很痛苦，这边是有些研究空间了。

大家可以这个这个想想这里面的一些问题啊，对当我我说这些只是为了这个去说这个这里面有这些问题啊，我们来讨论下一个问题，对那个时间有点紧，我尽量说快一点，那个下个问题是。

Is diffusion model a maximum likelihood model，这个扩散模型，它是最大自然模型吗，这个问题是怎么来的，这问题其实其实他有你很多地方都可以来，我自己只选了一个。

我们这个dpm里面这个他最开始推的时候，他从这个最大自然来进行一些推导，然后推导出来，理论上哎他应该是这么一个形式，但实际中我们也不care，他前面这些loss weight。

我们把这个loss weight给这个丢了，然后这个再一般的呢，这个他其实training loss，其实是下面这个式子，我直接摘了这个vdm++里面的这个这个这个式子，对，然后vdm++他告诉我们。

哎当我的这个wNamla是一个单调函数的情况下，那么呢我的这个整个的这个training loss，他可以看成是这个一个最大自然学习啊，当然加了一些alimentation。

但不幸的是我们这个我们其实在做的过程中，我们其实也不是这个wNamla的选择，也不是单调的，所以所以其实他跟这个最大自然是有gap的，当然这个是从training的角度来说，是吧。

我们之所以讨论这个问题是因为这个，因为其他的这些success model，很多都是一个最大自然模型，比如VAE，比如autoregressive model，是吧。

我们definition model到底是不是这样，至少从training的角度来看有一个gap，对，从influence的角度来看，问题更大了。

那个因为我们前面modeling的是这个从xt到xt-1的disability mapping，但我们做influence的时候，我们一般也不拿xt到xt-1。

我们做的事情是用了一个叫classifier free guidance的东西，是吧，classifier free guidance其实干的事情，这边也有些推导我们就不说了。

其实他干的事情是把你的这个自然函数，跟你的这个后延 posterior，做了一个combination，combination还有个系数，是吧，这个就明晃晃的在告诉你，你最大自然不好。

我要给你加上一个后延，你才能拿到一个更好的结果，所以这个一直在动摇我，这个definition model到底是不是最大自然模型，看上去又不是，在evaluation的时候，这个问题更明显了，是吧。

这个大家一直在聊这个scaling law，这个language model scaling law是因为这个越压缩越好，什么叫压缩，压缩就是negative。

这个negative log likelihood，是吧，但是我们在这边你直接拿这个，negative log likelihood，也不能衡量你的这个结果。

你的这个negative log likelihood，结果越低，你的这个结果也不一定越好，我的这个图是也是直接从那个DDPM里面来扣的，其实你这个，这个这个你自己去做一些，Visualization。

你随便做点什么，你都能发现这个，这个NLL loss其实没啥用，所以这个training inference，evaluation，它都有问题，它到底这个有用吗，所以其实这个问题变成了这个。

为什么maximum likelihood of diffusion model，它并不能拿到一个最好的结果呢，这边也是我personal的一些理解，这个非常有可能是错的，对。

那个从training的角度来说，training角度其实就是这个，我在这个talk刚开始，一直在强调的这个问题，这个信号拆解的不对，Diffusion model的信号拆解它不具有等价性，所以呢。

它在这个不同的noise intensity上，它的这个importance和difficulty是不一样的，稍微注意一下，这个importance和difficulty这两个事情，这两个词。

它们代表的意思也不一样，就我们今天可能没时间了，后面以后有时间我们再说，对，然后这个，所以这个，所以这个最大的这个difficulty，其实往往是在这个中间的，这个信号比属于中间的这个状态。

这个是SD3，这个它的这个发现，其实在它之前，我们很多人都有这样的发现，对，所以这个中间的这个likelihood，往往学的不是太好，因此呢，我们这个没有按照这个ELBO的方式来进行学习。

然后第二个在inference的时候，inference的时候我的理解是这样，这个classified free guidance，可以看成是对这个最大自然学习的一个纠正。

这个其实说的是difficulty的事情，就是它在不同的这个noise intensity的情况下，它这个学习的这个难度啊压力不一样，学习的好坏程度也不一样，尤其是在中间，那个最难的部分学的最差。

这个是下面引的这篇文章，TerraCurse他们前两个月做的，他们发现这个classified free guidance，我好像放的太靠下了，这个看不到，下次下次这个学习进步一点。

TerraCurse他们发现这个classified free guidance，这个在noise intensity在中间的时候是最重要的，前后的时候没什么重要，没那么重要。

这个事情其实反过来可以验证我们的这个想法啊，因为它中间的时候难，学的差，所以我们需要classified free guidance，来对我们的这个自然模型做一个纠正，然后第三个evaluation。

evaluation其实这个更直观了，这个其实对应的上面是importance的事情，你把他们这个equalize进行加权，这个事情压根就不合理，他们这个重要性其实不是一样的。

重要性不一样的原因还是来自于我们前面反复在说的，这个视觉信号拆分它的不等加性啊，所以你的这个重要性也是不等加的，所以呢你equalize加权不合理，所以呢你的这个NRLoss不行。

然后这边其实可以引申到一些其他非常有意思的一些话题，比如说这个视觉信号的这个evaluation model的这个scaling loss，真正的scaling loss，对吧，这个今天也就先不说了。

对然后，最后一个问题要不说了吧，我们那个留留些时间我们来这个进行一些QA吧，对大家有没有什么问题，好，你前面提到那个decomposition。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/839f9dfbf25622d0073dcf82ff161be3_17.png)

如果是一个三维空间的话，那么怎么做呢，比如像我们这个会场里面三维空间怎么做那个分解，我要比如说我要关注某个人比如说我要关注陈建飞，我怎么就是假设我是一个那个AI对不对，我怎么关注陈建飞，怎么怎么做分解。

对这是一个非常好的问题，我也不知道答案，今天我提的大部分问题我自己都不知道答案，我只觉得这个信号拆解的问题应该这个再深入的进行研究，对这个至于你是二维信号三维信号，三维信号微信号还是什么信号。

这个probably是不一样的，我我自己其实确实没有明确的答案，就是在小模型的时候是有一个分解定理的，就叫三维空间的博弈的，就是说Helmholtz-Holtz增加分解定理。

那么这个增加分解定理如何的跟你这个刚才讲的那个，整个的那个分解那种思想能够结合起来，我觉得这说不定是将来是一个重大突破的一个一个契机吧，对这是一个很好的问题，其实分解包括两种方式。

一种分解方式是我分解成这个互相相关的，比如这个language就是典型的分解成相互相关的，前后它有同样的依赖关系，这样我把我的这个依赖关系这个进行这个这个这个equivalent的建模，这是一种好方式。

对还有第二种很好的方式，就我我我不是这个领域的专家，我不知道理解的对不对，就是我们把这个px分解成independent的这个simple distribution。

我们对每个independent的simple distribution进行建模，probably你不是完全independent的，我们之间有些这个弱的这个correlation，我们就独立建模。

然后在你和我们这个这个joint distribution，对可能可能可能在3D里面我们有些这个独特的一些cellar啊什么，我们可以利用上这些东西来做到更好的这个，对我我觉得probably很有可能。

好谢谢，那后面还有，我说这个，请您往前走一点，谢谢非常感谢你今天讲了非常有意思的话题，然后我对那个tokenization比较感兴趣，然后因为这个我是这周吧周三看了一篇paper，其实是自己他们发的。

就是他们说之前我们比如说做VAE和VQAE，我们是把这个图片用CN给压缩到一个2D的一个结构上面，然后去做KL或者是VQ之类操作，然后那篇paper他就说，因为我们考虑到其实可能会有一些冗余。

他就说能不能把直接加到1D上面，然后在1D上面比如说用一些transformer技术往下去做生成，我刚才那篇文章其实做了一个比较好的探索了，因为比如说我们256的输入，假设我们压缩比是8。

其实它最后还是一个32×32，大概是1024的一个token length，但如果我们用那个1D的方式，他们最终是可以压缩到一个32的token length，所以我感觉压缩的这个比例是非常高的。

而且他们最终的效果其实也差别没有那么大，所以说我其实就比较好奇，就我们去做生成我们的图像去做压缩，然后去做生成的时候一定要保持这个2D结构吗，或者说这个1D的这个去做的话。

是不是更符合用语言模型或者transformer的结构，对我们对这个比较感兴趣，好这个谢谢你给我分享了一篇这个，我还没看过title的这篇paper，我的看法是这样，我前面其实也提到了。

我们在做这个embedding的时候，其实在K有两个事情，第一个事情是你重建的到底有多好，第二个事情是你压缩的到底有，其实也不叫你压缩到底有多狠，很多人这么说，但其实这个观点其实不对。

其实关键在于我压缩的这个分布到底有多好去拟合，这两个事情稍微有点gap，所以至于你是2D的还是这个one dimension的这个data，其实无所谓，这个我觉得是没有明确的这个答案的。

不是说一定要怎么样好，或者不要怎么样好，因为这个现在的这些tokenization，不一定能做到真正的这个global tokenization。

因为你做1D的你基本上是global tokenization，这个我自己其实在这边也做过一些工作，其实这个global tokenization，其实中间有些独特的挑战，我这边没有去说这个事情。

这是第一个事情，第二个事情是我觉得，我觉得我们不应该去，也个人观念，我觉得我们不应该去将就这个Large-scale model，他们是1D的data，所以我们的这些signal都要压缩成1D的。

我自己觉得这个观点是不对的，我们从research的角度，从工业的角度都没必要做这样的事情，对这是我的看法，还有一个其实就是，因为他们那种想法，其实之前我们可能做detection。

或者是做那个believe-to里面，我们会有一个Q-formal，或者是那种1D的query vector，所以他们想之前那些1D的query vector，是用来做理解。

或者是一些Bounding box的生成任务，所以他们想能不能把这个拿去做，这个图片的生成和重建，对我觉得就是这种想法，因为从他那个实验结果来看，或者是我们自己去算。

其实这种token length能降低，对于我们生成的速度，以及我们模型的尺度大小，其实会有一个比较明显的一个降降，可能会对学术界来说，这种春年的cost，是相对来说比较可以接受的，对对对。

我觉得你说的这个点很对，这个首先它是一个balance，然后第二个事情我想还稍微提一下，就是那个1D的和global的还是不一样的，我也可以变成一个2D的，然后我直接做reshape。

你里面其实precision binding，其实你还蕴含了很多其他的一些东西，所以这边其实要深入去思考一下，对其实就是global的这种，能不能去把global的特征，给比较好的提供出来，对是的。

谢谢你，謝謝妳 謝謝。

# 2024北京智源大会-生成模型 - P5：大模型的高效并行推理方法：邓志杰 - 智源社区 - BV1DS411w7hz

好嗯感谢主持人啊，感谢智园组委会的邀请，让我有这样一个机会来给大家啊，分享我们在啊大模型的高效推理方面，的一些初步的工作啊，以及一些比较粗浅的想法啊，我是邓志杰啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_1.png)

来自上海交通大学清源研究院，那么我们本次报告的背景呢，其实就是啊，这两年我们所遇到的这个时代的背景啊，大语言模型啊，已经成为了广泛的现有任务的基石，然后也激激发了这个广大的啊。

我们的这个研究的兴趣以及工业界的兴趣。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_3.png)

那另一方面呢就是以大的扩散模型为代表的啊，这这类模型在图像尤其是视频生成上啊，所产生的带来的巨大的影响力和价值，那典型的工作就包括我们这个耳熟能详的，open AI做出来的这个SORA。

还有我们这个啊清华的和声树公司，联合做的这个维度啊，这样的一类模型，那么我个人呢是觉得这样，这两类模型其实可以统一在啊AIGC大模型，这样的一个架构下面，并且呢啊对于这两类模型。

或者说对于AIGC大模型，我们也渐渐的发现了啊，他们之间的一些趋势啊，发展的一些趋势，那么第一个就是呃架构啊趋于统一了啊，这transformer啊，在大模型的这个是架构使用上。

具有绝对的这个优势的地位，那么尽管有一些后来的挑战者啊，近期也会有些新的工作，像曼巴two啊等等啊，但是啊目前来说，transformer的地位还是啊无法撼动的，那么在学习方式上呢。

刚才各位老师也讨论的，我们存在多个主流的方向，那么一类，这就是以啊在语言上，以next token prediction为代表的啊，这种自回归的模型，那另一类呢就是在图像上，我们对图像做扩散啊。

扩散建模啊，扩散这样的模型学习方式上啊，目前来说还没有统一，但可能也会有一个统一的趋势，那么呃第三点呢，就是啊我们使我们相当于大一部分的人呢，还是比较相信scaling NO的啊。

然后呢scaling node也就是说啊，我们持续的给我们的模型增加算力啊，增加数据啊，增加啊它的参数量啊，那可以带来更好的这个推理呃，生生成的效果，那么我们作为普通的研究人员啊，或者是很多是学生同学。

那我们面对这样的这个时代，我们可可能也也会想玩一玩这些大模型，我们可以把模型下到我们本地来啊，我们不说训一训的，我们可以把它跑一跑推理一下，但是我们会发现我们会碰到非常多的挑战啊，比如一开始啊。

我我自己弄了一个3090的显卡，我又下了一个模型，发现加载都加载不进来，直接OOM了，爆了啊，这后面我们就加了很多trick给它，把它这个搞好，搞好之后呢，我们就拿模型来生成生存之后。

发现他的这个token一个一个一个的往外蹦啊，生成一段长的回复，他可能会需要一分钟的时间，那么这是比较早在去年的时候，那么这样的一种啊低效的推理啊，是会导致啊非常差的用户体验的啊，所以说从那时候开始呢。

我们就在考虑啊，如何的啊，把大模型的推理低效这个问题给解决了，那这个问题呢实际上来源于两个方面啊，第一个方面就是现在的模型，它本身也是越来越大啊，因为随着skin node的这个不断的扩展。

模型越做越大，那么另一方面呢，我觉得更重要的就是我们从算法的角度分析，我们发现啊，像大圆模型或者扩散模型，它都会依赖一个顺序推理的过程，很像语言模型里面，我们生成的词儿是一个一个自回归的往外蹦。

那么在扩散模型里面呢，我们也是从纯噪声呢啊出发啊，不断的把这个噪声里面的这个呃，把这个图像上的这个噪声给去除掉，生成出一张图像，比如说我们需要一个顺序的啊，漫长的推理过程啊，才能实现啊一次推理，那么由。

那么这个过程呢，会进一步放大我们模型自身大所带来的，这个开销啊，导致整个啊高部署的成本，还以及比较差的用户体验，所以说呢围绕着这个问题啊，我们呃也做了相关的一些工作啊，那么这里呢也会讨论一些啊。

这个领域里面的一些新的进展，那么我会主要分三个方面啊，第一个方面呢是围绕着大语言模型啊，我们会考虑把大圆模型的这个顺序推理啊，改成并行的推理啊，当然不是纯粹的并行，只是呢我们在算法上做一些调整。

用一些新的机制啊，那么另一方面呢就是会讨论，对于大的扩散模型啊，我们如何对它进行低步式的推理啊，那么这里面会涉及两部分，也是啊，一方面是对于扩散模型采样器的设计，另一方面呢我们就从蒸馏的角度出发啊。

把多步数的模型整流到低步数啊，最后呢也会花简短的花一点时间来介绍一下，我们在模型结构以及缓存优化等方面的，一些啊进展，呃那么呃可能大家都对大圆模型的这个，推理过程有一定的了解啊，可能比较熟悉。

但是呢我在这呢给大家简单回顾一下，那么比如说啊我有这样的一句prompt3个词，那我想输到某语言模型里面去，让它往后生成，往后帮我填充，那么首先呢我就把它放到模型里面去，它会生成出后一个词。

那么这一个阶段呢我们一般一个比较啊，大家认可的术语叫做prefer啊，就是我把prompt，PREFILL到我们的这个模型里面去啊，这样一个阶段，那后面呢其实就是一个不断重复的。

把刚才生成的词儿啊接到我们的输入后面去，哎，往后生成下一个词这样的一个过程，那么这个过程呢，我们比较通用的把它叫做这个啊decode face啊，这个decode face呢啊是自回归式的啊。

一个一个的来，那么prefile face呢是一蹴而就的，是并行的，就这三个词呃，Artificial intelligence is，这三个词是同时处理的啊，是同时处理的，那什么时候这个过程停止了啊。

基本基本上有两个准则，一个准则就是它生成出结束符US啊，另一个准则就是它到达了这个模型的生成上限，比方说2048啊，他截止了，那我们分析一下这个呃这个这个推理，或者说这个生成过程里面。

它的这个计算开销啊，时间花在哪里，那么我们说这个prefer这个face呢，它是并行的，是一蹴而就的，所以说啊他的时间我们先啊可以认为是比较小，那如果说我们要生成出来的这个呃，生成出来的话。

也就是decode face是很长的，那么整个的这个推理过程就会很慢，所以很多时候我们的计算开要花在decode face上，那么另一方面呢就是这个decode face我们会注意到。

比方说我们要生成feature啊这个词，那他会看什么呢，他会看前面的这四个词都会看啊，这can用的是这个transformer里面的，这个自注意力机制，对吧啊，这个是要生成的，这个词呢会看前面的词啊。

所以呃它这个地方就要看四个，那么我再看下一个词，它要生成of呢，它就会看前面那五个词，就是说它不断的往后生成我们自注意力机制，它的复杂度是不断的提高的，也就是说到后面它的开销是越来越大的啊。

那么这一点呢啊，是可能会导致我们的语言模型生成，越越往后越慢啊，带来比较大的开销是比较显著的一个地方，那么解决这个问题，一个典型的技术就叫k k v catch，或者说现在已经是一个啊标准的技术了。

也就是说呢我用呃这个空间换时间，因为什么呢，呃我在后面生成，不管是分生成future呃，future还是成off，我其实都是用到前面的这个artificial intelligence is，这个词。

这三个词我都要复用的，就是我要相当相当来说我要把它算很多遍，那没必要，我就可以算一遍之后呢，把它把它那个对应的啊，自助义里面的一些状态存下来，存下来之后呢，我在后面需要啊用到这个地方的时候。

需要看他们的时候呢，我就用他的状态，我就不重新算了，那么这样的一种方式呢就可以啊，相当于以空间换时间，节省了我们的这个flops，节省了计算量，那么对于刚才新生冲出来这个词呢，我把这个there。

我把他的这个呃，这个这个K和V的状态，也可以再丢到kv catch里，不断的把我这个kv catch变大啊，填填填充的更大，那再往后生成，那么这样的一种有k v catch的啊。

大圆模型的推理呢有两个主要特点，那第一个特点就是它的prefer face，其实是计算密集的啊，计算密集的，因为如果说我的prom很长的话啊，其实呃我就可以很充分的把我这个GPU打满。

把它的并行能力给激发出来，但是呢呃大语言模型，它的decode face是memory I/O bd的啊，比如说受限于我们GPU内部的这个memory I/O啊，其实它是flops是跑不满的啊。

主要是把它带宽memory I/O的带宽给跑满了，那这个带宽其实是在我们的这个GPU的储存，和它的一个这个啊高速计算的这个这个存储，可以就叫做HBM，储存和HBM之间的这个通讯啊，这个是比较慢的。

这是比较慢的，相对来说比较慢的，所以呢实际上真正局是限制，我们的这个大圆模型推理的其实是这个带宽啊，储存和HBM之间的带宽，那么我们还有一个这个特点，或者说是一个观察吧。

就说我PREFILL几个token啊，我我我prefer几个token所花的时间啊，可以认为是跟啊我decode一个token的时间，差不多的啊，差不多的，这就是因为我们的这个GPU呢。

它这个flops是够的啊，它并因计算能力够，那么基于这样两个观察呢，我们就在想啊，我们能不能通过某种方式啊，把我们的这个decode face里面的这个memory I/O啊。

这个这个这个需要的量给降下来，比方说我现在生成十个词，需要十次memory I/O，那我能不能比方说给它降成三次两次啊，如果降下来的话，这就可以显著的提高我们这个啊，生成过程的这个速度啊。

解码过程的速度，那么一个典型的这个做法叫做这个投机解码啊，他就是为了实现这个目标，那他的一个想法呢啊是这样，就是说啊比方说我要生成出后面那一段，我喜欢做什么样的，我我有什么爱好，这样的一段这个话。

一段token，他有个假设，他说我要生成的所有token，不是每个token都很难的，不是每个token都有很强的语意的，肯定是有些废话，token对吧，或者是就相当于占位符一样的token。

那么我是不是可以把啊这样的一些token，我放到我的一个小模型上去生成啊，小模型我把它叫做草稿模型啊，Drift model，那这个草稿模型呢，比方说我就先让我这个草稿模型啊，去对我这个问题做回答啊。

生成出六个五个词，生成五个词儿之后呢，我这大模型用来干嘛呢，我大模型用来去这个评判啊，我我去判断我是不是要接收啊，小模型生成出来的这个提议，他这个proposal，那如果说不接收呢。

我这个大模型还可以给他提供一个啊，对于他犯的第一个错的改正哎，也就是他这儿的一个playing，那这里面用的一套这个准则呢，是基于这个啊拒绝性采样的啊，有有严格的理论证明，可以说啊。

我们这个啊通过啊聚云采样啊，得出来的，这个呃采样出来的，这个token的分布是符合原来的啊，自遵循自回归的啊，这个token分布的，然后呢，我们这个大模型去对小模型的proposal。

做验证的这个过程其实是啊并行验证的，这就等价于是做一次啊prefer face啊，去做一次prefer，所以说呢嗯从这个角度来理解，我们将生成三个词啊，它所对应的decode face的时间呢。

转变成啊做疑似PREFU的这个时间，is prefu和三次，这个呃和一个一次PREFILL和三次，这个啊decode它们对应的memory I/O那个交换，我就要少三倍。

所以说因为因为我这个生成过程是被memory I/O，BD的啊，所以说我这个速度呢就可以成倍的提高啊，当然这是理理想情况下，实际呢我们也会呃碰到一些这个啊，不理想的问题，包括什么呢。

包括我们在这会谈论到的，我们投机解码要工作需要哪几个必要的条件嗯，那么一个关键条件呢，就是我们这个drift model小模型够不够小，它得够小啊，才能够快对吧。

另一方面呢就是我我一次这个proposal多长，我一次proposal几个词，我propose一个词呢基本上就没什么意义，把我propose太长了，那我可能前面他接收的也没有，到不了那么长对吧。

这也是我们一个可以调的，可以选择的地方，那么还有一个呢比较关键的，就是我这个小模型propose出来的啊，这个token被大模型接受的概率高不高啊，也就是说我这个小模型。

能不能很好的猜中我们大模型的这个心理啊，猜中它的分布，那这三个因素是比较关键的啊，这是我们自己画的一些，这个它所带来的期望加速比的图啊，对于这三个影响因子来说。

所以呢呃其实对于呃propose的这个长度K啊，我们是可以做调参调的，然后模型的大小呢也是可以我们来设定的，那你这里面其实比较关键的，就是我们要提高我们的小模型，猜中大模型的这个啊这个分布的准确率。

也就是这个token acceptance rate啊，我们要来提高这个东西，那我们就观察了一下，现在的这个投机解码的一些这个部署的系统，我们会发现两个机会啊。

就是来提高token acceptance re的两个机会，那么一个机会呢，就是我们发现我们的这个呃，这个投机解码过程里面啊，我们的这个小模型给出来了一个proposal之后。

其实我们大模型会检测出它的第一个错误，然后呢白送他一个啊，这个应该怎么生成这件事，告诉他白送他一个正确token，那这个token实际上嗯在这个投机解码里，就是被用来了，我们往后接着所生成的。

但我们是我转过来想，这个信号实际上可以拿很好的拿过来啊，帮助我们来做小模型的校正，那么我就教小模型，你下次不能再犯这个错了对吧，那那我这个小模型相当于就在不断的啊，这个过程里面提升自己。

也就我这也也就我这里所说的，这个我们大模型可以免费的提供信息啊，告诉小模型怎么改进它的token acceptance rate，那另一方面呢，就是啊，我们刚才一直强调的这个投机解码的系统里。

有很多这个空的FLOS啊，很多count flops，那我这个有flops，我可以来train模型吗，所以呢我们就啊做了这样一件事情啊，叫做这个在线的投机解码啊，OSD嗯。

那么我们就是把两部分做一个结合，我们在投机解码的这里面呢，做了一个draft model的在线的蒸馏啊，从这个比较直观的角度来说，我们就做了这样一件事情，那去去做法上也非常简单啊，我们可以就是在这个啊。

online serving的这个过程里面呢，我就不断的记录小模型是哪个地方犯了错，大模型给它的一个校正是什么，我用一个buffer来存啊，然后每过一段时间，或者说我的buffer满了之后呢。

我就跑一次啊，蒸馏的一个一个过程啊，蒸馏一个过程，这个蒸馏其实跟我们的这个做做语言模型的，这个teacher forcing的training是差不多的啊，north north是差不多的。

好吹一下啊就可以了，那么它有一个很显著的好处，一会我们在这个这个实验结果里会发现，就是呃我们假设是一个open domin的情况，然后呢。

我们会有一个嗯比较稳定的一个query distribution，就是说啊，我这个我作为一个用户，来用这个大模型的时候呢，实际上我往往倾向于问一个，特定范围内的问题啊，有有可能今天下午我在改论文。

我就可能问的就是怎么叫他帮我改语法错误啊，或者是叫我帮他翻，帮我翻译啊，把有的有的有的人他从事的是金融啊，或者数学相关的这个工作的话，有可能会一直问的是数学的这种问题，那也就是说我们的这个用户。

其实会有一个query distribution啊，这个query discretion是比较窄的，不会像原模型学习的那个distribution那么大，所以说在这种情况下。

我们这个OSD呢就可以快速的适配到啊，用户的这个queries distribution，更好的猜中用户的心理啊，有这样一种感觉啊，那么我们首先就是模拟了一些，在线部署的场景啊，比方说我们的会假设啊。

我们的这个系统会呃会用到这个spider啊，或者GSN8K啊这样的一些benchmark上去，那么随着我们对这个这个这个呃，投机解码系统的不断的query啊，不断的跟他交互啊，我们的模型呢。

这个蓝色的线表示我们的这个模型的，这个猜中的概率，它就会不断的提高啊，这就是很符合我们预期的一个啊，一个是因为我们做了在线蒸馏，那如果base line呢就是用offline种模型，就是不做蒸馏啊。

不做在线，那它就是一个比较静态稳定的一个准确率，不会有提高，那么这个工作里面，我们还做了一个比较有意思的一个呃一个探索，就是我们发现呢我们这个drift model也没有必要，一定是一个啊。

我可以是多个，尤其是在一些复杂的这个呃，query g g g ution的场景下，比方说我这个模型，可能将来面临被多种语言的人来访问，那我就可以其实给每一种语言的人呢。

我部署一个drift model啊，如果说大家会问多个主题的问题，我可以为不同的主题啊，不属于一个这个drift mod，不属于一个小模型啊，然后呢我们再也模拟了这样的一些场景啊。

我们会发现呢在这种混合的场景下，不同的小模型，不同的drift model，他的这个准确率呢也会啊持续的提升啊，这也会持续的提升，那么最终我想可能我们可以拓展为一种，基于用户的这个路由。

我我为每一个用户呢，我可以部署一个draft model，那最终可以实现就是嗯嗯在这个这个用户，他的手机上部署的这份漏洞，更能猜中这个用户的心，啊当然我们也跟一些啊。

比较这个大家公认的BASSLINE做了一些比较啊，包括这个美杜莎啊，然后我们也其实也可以跟美杜莎做结合，然后我们做了一些这个观察，就发现呢这个小模型啊对哪些词能猜对，能猜对嗯，这个提升比较大。

猜对的概率提升比较大呢，我们发现跟这个任务其实是特别相关的，比方说spider，它是一个文本到SQL语句的这样的一个任务，那你就会发现他能猜对的词，好多就变成了select呀啊。

或者是就是跟这个任务特别相关的，然后包括GSN8K呢，它就会很容易猜对一些符号，所以说这个小模型确实是在猜对一些这个啊，就是可能是一些啊，相对来说信息密度比较低的这种token啊。

这样的话我就可以释放这个大模型的，生成的压力啊，帮他做的快一点，那么在刚才这样的一个工作里面呢，我们仍然考虑模型呃，模原模型自身仍然是顺序解码器，然后尽管我们用了一个小模型。

然后用大模型去做啊一个并行的验证，但是小模型呢和大模型在这个里面，它仍然是顺序的，那么我们会呃这是受限于这类模型啊，原模型它是从自回归的这种方式里面去学的，那么我们可能就有一个比较跳脱的想法，就是啊。

我们的语言模型能不能一次预测出多个token，我跳出刚才这种范式，当然这个里面就有一些啊初步的探索，那个这个探索叫做jakobe decoding，这里面其实有苏阳博士的这个身影，这这个些工作。

那他是说什么呢，我们如果想同时啊，从一个大圆模型里面解码出N个token啊，多个token n大于一，那实际上等价于我同时求解呃，一个呃有N个方程的方程组啊，尽管说这个方程组就长这样啊。

这样说这个方程组它的呃第一个方程的解呢啊，第二个方程的解呢依赖于第一个的解啊，第二第三个呢又依赖于第一个和第二个，但是呢我们仍然可以用啊并行推呃，并行的这个求解器啊，并行的不动点迭代求解器来求解。

然后呢可以从理论上证明啊，我们的步数可以不超过N，也就是说我要解我要生成出N个token，我可以求解的步数是小于N，我就可以把它求解出来，小于等于N，并且呢我求解出来的这个token呢。

严格会服从于啊我们想要的这个分布，比方说这个这个如果取的是arg max，我们就是跟GRADY，跟这个语言模型，GRADY生成的这个分布是一致的，那这个可能从公式上比较难理解，比较晦涩。

我们可以看这个直观的图，就是对于我们有一个prompt，有一个prefix输到模型里面来呢，我们呢先随机的猜N个token，随机的把后面N个token给猜出来，猜出来之后呢。

我们就把它们一起丢到语言模型里面去，做一次迭代，得到一个输出，然后呢输出里面跟输入的这个token如果是一样，我就给它固定下来啊，然后呢我就把后面的token呢，我再再再输到前面去啊。

再输到语言模型里面去再做一次迭代啊，最后呢就会得到一个不动点啊，这就是其实做起来就很简单啊，做起很简单，然后呢他用的时间呢其实是呃，那类似于我们之前说的这个prefer一次的时间啊，相对来说它对它。

它跟这个decode一次的时间呢也差不了多少，所以说总共呢他的时间也不会啊，也不会引起太大的开销，但是呢呃2013年的一个工作啊，这个做出来之后说在上面可以用这个方法啊，可以相对于AR有零点呃，有1。

05倍的这个速度提升不是很理想，不是很理想，那么他的原因主要就是因为模型呢，其实在训练的时候，他没有学过怎么预测预测多个token，比方说它前面的词儿啊，前面有两个词没预测对，它后面的词它不可能预测对。

就是概率非常小啊，非常小，那模型没有这个能力，我们就想我们得把模型这个能力给他给他，给他习，得给他学会啊，我们可能就要需要调这个模型，那么我们就想设计怎么样的一种学习目标，来调这个模型呢。

我们还是从这个角cobe decoding，这个不动点迭代的这个角度出发，我们如果看这个图上，右边是jo be decoding的一个轨迹啊，它是不动点迭代的一个轨迹，其实很类似于我们的这个呃。

这个diffusion model里面这个ODE采样的轨迹啊，很像啊，确定性的，我们就想，那我们的终极目标，其实就是让模型直接从随机的初始化映射到它，最后那个不动点，就学这个映射，就学这个映射。

但这个映射呢，就如果直接以这个作为north function去训练，训不出来，就是我们也做了一点尝试，因为这个问题太难了，你一次往后猜十个词，十个词，这还得了，就是这个很难猜不对。

那么我们就想有没有一些折中的方案啊，我们就从这个consistent model啊，也是孙杨博士做的这个啊，对于defer model加速的工作里面啊，得到了一些灵感，我们就想嗯。

我能不能把这个轨迹上的任意一个点啊，都映射到他的这个fix point上去啊，我们这样的话我就可以定义一组啊损失函数，一组学习目标，那么这一组学习目标呢，它就有一个很好的这个性质，就是说嗯。

我我我从最后我就要往fix point的这个预测的话，这是很简单的，其实我就往后预测一个词，两个词就够不了，这是很简单，但是越往前越难，它就有一个从易到难的这样的一个变化，那我觉得可能这这个变化呢。

对于啊模原模型来说，或者说对于大模型的训练来说呢，其实是可以给它起到111点的，这个引导作用的，可能会有一些啊课程学习这样的一种感觉，所以最后呢我们就用了这样的一个呃学习目标，来来来学习啊。

我们就定义了啊，我们有两种选择来定义这个一致性损失函数啊，Consistent north consistency laws，那一个呢就是我刚刚说的，直接就从中间的任意一个状态啊。

不动点迭代状态去预测不动点，那另一个呢就是很类似于consistent model里面的，这个loss，就说我找两个相邻的不动点啊，然后呢我的目标就是把它俩预测到一起啊，让他俩的预测一致啊。

这是我们的两种north function，但我们发现呢其实还有一个比较关键的点，就是这里面呢我要把自回归的损失我要加上，我如果不加上自回归的损失吧，模型很有可能会这个崩塌啊。

他这个就就就全生成出同样的一个一个，一个token，或者说就什么可能找到一些捷径在这里面，所以说我要用自回归的损失呢来矫正它。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_5.png)

这是最后我们能够达到的效果啊，在几个我们这是选了几个case，当然我们还有做了其他的一些case，基本上来说我们对于现有的模型，比方说VIKA呀，或者是deep deep sick coder啊。

拿过来我调一下好调一下，不需要调很久，后面我们有时间啊，简单调一下就可以达到，用我们的那个loss function来调，调完之后就可以达到，基本上223倍的这个加速。

然后呢同时他的这个生成质量是啊不会下降，不会明显下降的，哎就是这个10%到5%以内的下降啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_7.png)

这个表里就有我们的这个啊，加速时间和这个啊性能啊，这一个系统性的这个比较啊，至多来说是有3。6倍的加速，我们知道今现在这个市面上，可能在大圆模型加速上，比较这个啊权威的一个方法就是美杜莎啊。

美杜莎呃他的第二个版本啊，那美杜莎他的名字就是呃，那这这个名字我们就能猜出来，他这模型是有多个头的，多个输出的头，那他就需要对模型架构上做改变，然后呢这个头其实很重，然后他也要训要训很久。

那我们来说的话，我们就不需要改模型，我们需要改一下模型的这个训练的objective，就可以了，在生成质量上，我们也可以达到基本上不怎么下降，比方说原始的我们的这个呃希尔GBT呃，这个MT奔驰上是6。

5，我们修完之后也就6。4啊，也就一点点啊，这是我们训练的开销，我们训练的开销，我们是啊已预训练这个模型，预训练它用的token数啊作为基准，我们我们我们做的微调相对于预训练啊。

我我这个token说我吃他的那个比例是多少啊，基本上都是小于百分之0。10。2的，最后我们也分析了一下，就是呃这个一致性，我们做的这个一致性大圆模型呢，它所带来加速的根源是什么啊。

我们就做找了很多case来看，我们发现其实主要有两个根源，一根源呢我们叫做这个啊，Fast forwarding，也就是说我一次可以预测出预测对多个词啊，预测对多个token啊。

这这在这右这个这个图的右边，大家可以看到，我可以一次至多可以预测出三个三个token对吧，那另一个呢就是呃叫做stationary token，也就是说呢在前面还有预测不对的情况下。

我可以提前把后面的某个词预测对，但这个情况下这个情况是比较少的，相对于fast forwarding是比较少的啊，因为这个任务这这样的一件事情确实是很难，就是很难，OK那我们第二部分呢。

我们就会介绍我们在这个大语言模型啊，啊以及这个我我们的这相关进展啊，包括我们做的一些工作，哎，在大扩散模型的这个地步地，部署重流方面做一些工作，那扩散模型呢呃这个他的这个推理慢。

每一个去造部都需要一次模型前传。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_9.png)

我想我也不需要给大家重复了啊，之前呢大家啊会比较关注的一个点呢，就是从呃这个呃设计扩散模型的采样器，这个角度出发来加速扩散模型的推理，当然我这边用的其实是啊，李钟元老师的这个一个PPT啊。

因为因为呃有有有一个视角，就是我们的这个扩散模型呢，它有一个啊长它的这个反向的SD过程，有等价的一个啊常微分方程，有个常微分方程，那叫做这个概率啊，概率流产原方程啊，这也是孙杨博士。

在他这个12021年的文章里面，这个给出来的，我们发现呢这个长元方程和啊，它对应的这个啊随机分为随机分微分方程，它们的这个边缘分布是一样的啊，但是呢这个常用方程它是更加平滑的啊，它平滑，那就给我们做啊。

在它上面的一个快速采样提供了机会，我们可以用非常这个快的呃，或者说一个比较大的跳跃步啊，在这个这个啊超越方式上去走，所以呢现在呃也有很多这个啊基于呃长呃，长扩散模型的采样。

从这个长元方程离散化角度来做的一些工作啊，那么在21年的时候，就是大家发现我们就即便是用传统的o d server啊，其实就可以啊，相对于sd server啊，有有一个很很大的这个啊速度上的提升啊。

可以减少这个推理步数啊，减少两倍。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_11.png)

那甚至说我那后面呢我们就可以呃，为专专门为这个default model设计，适合于它的这个o d e server，那么这里面代表性的工作就是啊，清华大学做出来的这个DP m server这个工作。

那它就是面向啊扩散概率模型的常微方程，离散化啊，它利用了这个啊扩散概率模型它半线性的特点，然后呢基于呃，基于泰勒展开等一些这个技术来设计它等价的，长元方程的啊离散化的解析形式。

然后最后呢啊再可以用一些差分，来近似里面的一些计算项啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_13.png)

最后得到d pn server，d pn server它的效果特别好，然后主要是也被我们的这个社区广泛的采用，像这些啊主流的这个SLIABLEDEFUSION啊，啊或者是搭理啊。

可能都会用到这样的一个呃，d pm server这样的一个高效的采样算法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_15.png)

那我就以DP m sorry为代表啊，介绍了这个呃这个从这个采样器设计的角度啊，来加速扩散模型的方案，那我们更多会讨论的就是从模型蒸馏的角度啊，我们怎么啊实现啊低步低步数的这个推理啊。

实现扩散模型采样的加速，那么我们最早这个出现相关的工作呢，叫做这个渐进蒸馏啊，Progressive distillation，那他呢也很直观，比较好理解，就说我原来比方说我需要做四步采样。

才能从噪声啊恢复出X的话，那我现在呢我就可以把它中间的每两步，蒸馏到另一个模型里面去啊，直接让另一个模型基于EPSAL来预测出两步之后，他的状态哎，或者说啊从那个状态到EPP。

epp one之间那个差值啊，就是他要走的这个啊走多少，然后呢，我我做了这样的一次这个蒸馏过程之后呢，我就可以把四部的一个啊模型啊，四部的一个采样过程，蒸馏为一个两步采样过程啊，如果说我再重复一次的话。

就可以把两步蒸馏成一步，这样的话我就可以渐进的啊，减少扩散模型的这个蒸馏的这个呃，呃渐进的采呃，减少它的这个采样的时间啊，呃之后呢CVPR2013上的这个工作呢，就啊进一步的改改进了啊。

这样的一个啊禁忌蒸馏的方式，为他引入了CFG，然后做到了更大的模型，做到了这个文生图的场景里啊，做到了隐空间的这个扩散模型上啊等等，把这些必要的这个技巧都引入进来了。

但是呢这类方法它还是有很大的一个问题，就是它需要证流多个模型，就是啊对于不同的这个采样部署，比方说四部或者八部啊，它一般都是训练，专门的一个八部模型和一个四部模型，就是它有多个模型，然后呢它的这个蒸馏。

这整个成本啊还是比较高的啊，可能需要上千张显卡来做这个蒸馏，那么有一个这个比较好的解决方案啊，就是从这个呃，就从这个共享这这这这几这几个啊，不同步数的模型的角度来说。

有个比较好的解决方案叫做一一致性蒸馏啊，那么一致性蒸馏呢它是从这个角度来理解的，他是想我们如果考虑从defer model里的采样，那实际上我们是在对我们的这个概率流，ODE啊做离散化对吧。

然后这个概率流ODE呢，它实际上定义了一个，从噪声到我们的这个观测数据的一个一一映射，一个一映射，我们之前呢是需要用一个流程，来把这个映射给跑完的，用一个sampler来把这个跑完。

那我现在就想我能不能直接用一个模型来建模，这个映射，我知道这个模型的输入是啊，这个呃这个ODE轨迹上的一个状态，让它的输出直接是一张干净的图像啊，在这个ODE轨迹末端的这个干净图像。

如果说我能构造出这样一个模型，那我就可以实现啊，一步采样的这样的一个目标啊，就是把采样步数尽可能的降到了最低的，那它从形式上来说呢，我们就会发现我们需要定义这样的一个模型啊，FC它，然后它的输入呢是啊。

我们的这一个一个呃OD轨迹上的一个状态，那么理想情况下，我们只需要把最终那个状态给给给给输进去，那实际上呢为了我们的这个模型啊，可以训练，然后呢可以是服务于多步材呀，我们也可以考虑把OD轨迹上。

中间的一个XT给输进去，反正呢我们就是让这个轨迹上的所有点，都能映射到X0，映射到一张干净图像上去，然后这个呃FC它这样的一个函数呢啊，它的参数化呃有有一个比较有意思的技巧，因为我们会发现。

如果说这个f theta它的输入这个T是零的话，那其实我们就可以直接把它同时输进来的，这个XT给return出去啊，因为就表示呃，我已经就是在这个OD轨迹的末端了啊，就是最后那个时刻了。

所以呢为了实现这一点啊，就是呃一阵与蒸馏呢正从模型构建的角度啊，就做到了这样一个事情啊，他就是做了我们的这个数据输入数据X啊，和我们额外的一个建模模型，FC的大FC塔之间的一个差值。

那么它们之间的这个差值的VT呢，就可以保证在时间不是零的时候情况下，我就把X输出出来，那这个模型的训练呢是呃，可能初看它的形式有点复杂，但实际上也啊比较好理解，就是我们先呢把一张干净的图像，X0啊。

降噪啊，加到XT撇这个时刻上去，然后我我我现在不就是考虑XT撇，在哪个ODE轨轨迹上吗，那为了确定这个ODE，我就要拿一个预训练好的defer model来走啊。

因为这个预训练好的dev model就确定了一个OD，那就让它呢往前走一步啊，走到XT这个位置上去，然后呢我的学习目标就简单了，我就只需要让我们的这个XT撇和XT，放到我们的模型里面去。

都能预测到同一个初始状态啊，那就说明他们是在同一个ODE轨迹上了，所以呢这就定义了一致性蒸馏的这个损失，一定蒸流损失，那么这个结果其实是去年来说，是啊一步生成的SOTA的结果啊。

在saver ten和这个image led上，EB站64上，然后呢它也有很多的这个应用场景，那么今年呢就哎也是清华大学做了一个工作，就把它给拓展到了隐形空间啊，在影空间上做了一个一致性的蒸馏。

我们把它叫做这个这个latent latent consent model呃，然后它最后能实现的目标呢，就是它四部就已经生成的很好了，然后呢在一部和二部的情况下呢，仍然可以生成，仍然可以生成。

但那效果我们也测了一下啊，这个一一部和二部的情况下没有那么理想啊，所以呢我们也做了个工作，也是延续隐空隐空间的一部一一致性蒸馏，我们就把它拓展成了多部，我们就想其实我们直接呃让我们的模型学习。

从噪声到数图像的一致性映射呢，这个问题本身也挺难的，那我可以考虑呢，我首先将我们要建模的这个OD轨迹呢，我给它切段啊，比方说我一开始给它切成十段，16段，那么在每一段里面，我让他学习一个一致性的映射。

然后呢我不断的通过减少段数，比方说我一次让这个16段变成八段，然后训一会儿之后我再让它变成四段，这样的话呢，我就可以有一个嗯，相当于热启动的这样一种感觉啊，可以我们发现这样的一种方式。

可以提升模型的这个收敛啊，改进改进模型的收敛，然后呢，我们还还这个啊使用了一种这个training data free啊，就是我不需要training data来做蒸馏的一种方案啊。

就是其实说白了也简单，就是我们从啊教师模型的采样啊，从他出发来做蒸馏，最后呢我们引引入了一些偏好学习，因为现在大家会比较强调符合美学，符合这个啊一些这个伦理道德啊，这样的一些呃要求。

所以我们引入了偏好学习的一些损失，兼顾了这个加速和对齐两个方面呃，这是我们最后得到的效果啊，然后呢我们是啊，不需要训练图片统一的一个多步生成模型啊，然后在两部的情况下就可以实现高质量的啊。

1024分辨率的图片的生成啊，比SD叉L和hyper sd要好很多，然后也赋能到了一些产业界里面去呃，在这个这个模型的拓展方面呢，也和control做了一些这个结合，发现效果也不错啊。

然后还我们还可以做到中文的这个生成，包括对于中文的一些语境的理解，就是我给他一些这个中文的诗句啊什么的，像蓝水箱啊这种感觉，那这种可能就不不单单是汉字层面的，就是我对这个语境啊有一些理解，那说到蒸馏呢。

其实我们也啊绕不开一个话题，叫做这个得分得分蒸馏啊，他也是就是啊我们从这个模型加速的角度也好，从2D到3D这种理的角度也好，就是绕不开的一个算法，然后呢以及它的一个改进算法叫做变分，这个得分正流啊。

它是以通过引入变分推断的方式，缓解了原始的得分正流的一个巨大的问题，然后呢我们会发现这个变分得分蒸馏里面呢，它有两个模块，一个就是原始的我已有的一个扩散模型分布。

另一个呢就是我需要额外引入一个用LAURA啊，加持的啊，一个拟和渲染图像分布的呃这样的一个模型，这两个模型呢呃呃就是这个NORA模型呢，和我们的这个3D的这个状态交替优化。

我们发现呃这样的一个优化过程呢，在在实际里面，我跑代码发现它就是存在一个比较empirical，就比较现实的问题就是收敛比较慢，然后呢也需要多个阶段的训练，我们就在想，能不能把这个多阶段。

我给它归归归约到一个阶段啊，让他一个阶段就能做的很好呢，那么我们发明了一种就是啊向前看一步啊，这样的一个呃，我把我发现把这个简单的这个思想给引入进去，我就先把我们的这个呃nora model呢。

在我们当前的3D state上我都多训一步，我发现确实可以改善啊，这个呃变分得分蒸馏的一个这个这个收敛，但是呢它会存在一个过拟合的风险，所以呢我们又提出把这个nora model呢，我做一个线性化啊。

给它线性化一下，它变成一个线性模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_17.png)

就不容易过拟合，最后呢我们有一些trick来实现高速，高高效地实现这种线性化，那最后我们得到的结果就是，我们基本上一个嗯系统决断就可以啊，比较明显的就是也优于已有的方法啊，包括VSD。

还有EESD这些方法。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_19.png)

最后呢我们在呃讨论一下，在模型结构以及啊学习状态等等，这些其他方面的优一些优化方法啊，这里面呢很多就是啊比较啊，出名或者经典的一些工作啊，也不也就是给大家这个简单的分享一下，那模型结结构上呢。

其实我们一直都在想提高模型的这个表达能力，但是呢现在你你你你单纯的把模型加深，加大之后呢，它很慢，所以呢有比较出名的一比较近，这个这个大家通用的一个机制叫做专家混合啊，专家混合，那它里面就是关键。

就是我比较稀疏的激活这些专家，我在保证模型的这个能力的同时，我可以降低它的计算开销，这里面的代表就是IMMIXTURE，然后千问啊，然后deep sick啊，他们为代表的一些MOE的模型。

那么深度混合呢是对于MOE的一个改进啊，他是认为呢MOE他这样，因为他这样可能会导致不同专家的node啊，不同专家的负载不均衡不均衡，我们怎么呃，这个就就可能会拖慢整个系统的速度啊。

所以说我我我这个MOD呢，就从从专家的视角来出发来做这个柔体，来做那个token的这个呃，只拿到token的这个选选择，选定它保证负载均衡，但是呢它有一个问题就是比较难做。

cole model0就是来做我们通常所说的language model ning啊，从前到后，从左到右这样的一个因果顺序比较难建模，那我们做了一个很初步的工作，我们发现什么呢。

我我在MOE里面我就加一组空专家啊，加一组空转夹，然后呢我就把MOE选top k这个key我加大一点，我就可以保证对于不同的token，我可以自适应的选择专家术，如果他全选了公众专家。

那这个那相当于这个token呢，我就需要零个专家来处理，零个真实专家来处理，如果他是全选的真实专家呢，那就需要嗯比方说四个真实专家来处理，那从这个角度来说呢，我们就可以极大地增加模型的灵活灵活度。

然后呢呃我我们再通过一些负载均衡的损失，来保证控专家的使用率啊，最终可以就是这个在呃，保证我们优于这个呃MOE啊，在这个这个这个负载利用率上是优于MOE，然后呢我们的建活力模呃。

建模灵活性是由于MOD的，因为它不能做啊，cos l的这种建模，对那个mixture87B，我们简单的微调了一下啊，就可以在性能不降低的情况下降低呃，这个20%的FLOS，influence的FLOS。

那么在token序列上的优化呢是呃，这也是一些像呃已有的工作啊，一个典型的工作叫做to me啊，它就是呃把啊基于相似度，对我们的这个token序列做合并，来降低我们transformer里面。

这个token序列的长度，从而提高它的推理效率，那么近期吧有一个比较这个呃出圈的工作，就是这个virotoken withdraw啊，他是说我在VOM啊视觉语言模型里面。

在后面我其实可以把那些视觉token全扔掉，尤其是16层之后全扔掉之后不影响性能啊，这也是比较有意思的一些现象，那在k v catch上的优化呢，我们也呃，就是从这就这都是比较这个出名的工作。

包括我们可以对啊这个query呢做分组啊，降低KV开始需要存储的这个内容的量啊，存储的状态的量，那么还有就是啊，deep sick他们做的这个啊多头的啊，latent attention啊。

它就是通过啊低质映射，把k v catch投影到低维的向量上去，存这些低维向量啊，从而是有效的降低缓存，那么其他的一些方法还包括啊，streaming l o m和H2O啊。

这也是啊今年做出来的比较好的一些工作，那么以上呢基本上就是我啊，想分享的一些内容啊，对于未来的一些研究方向呢，我觉得在AIGC大模型的高效推理方面啊，我们还可以做很多基础上的这个呃这个研究。

尤其是从模型架构啊，模型的学习或者训练方法啊，以及采样算法上啊，其实还有很多工作可以做，那么我个人认为可能比较重要的两个场景，就一个就是回到多模态这样的一个场景里面，那另一个就是啊视频生成啊。

世界是学习世界模型这样的一个目标。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/bd6e5853bbff1b0b165f7af1f7a876b7_21.png)

好以上就是我的分享，然后谢谢各位专家啊。

# 2024北京智源大会-生成模型 - P6：圆桌讨论 - 智源社区 - BV1DS411w7hz

好我们今天这个报告courage还是比较全，这个文本图片和视频模态都覆盖了，那难得也这么多这个同行聚在一起啊，呵呵探讨一些本质一点的问题，对我们之前也准备了几个问题，首先呃就一上来可能就是一些灵魂发问。

首先是这个泛式的问题，我们现在至少有两种范式，这个扩散和这个自回归，还有很多可能登上舞台的范式，比如说MANA，然后那那后面这个范式会怎么样转变，嗯想想请这个各位老师谈一下看法。

或者各位嘉宾也可以结合自己的领域，谈一下这个看法，对我想要不从咱们按报告就按报告顺序，就是说从罗老师开始这么难的问题，对嗯，嗯这个方式方面，就是说我只能说通过我的经验吧，就是这个肯定不是结论啊。

嗯就是我我自己做多模态大模型做的特别多啊，里面嗯就是就理解模型就是输入是多模态的，但是输入是文字的，嗯在这个做的过程中间，我们同时也其实特别想把呃，视觉的生产结合进去，反正发现特别困难啊。

所以这种我我觉得啊，至少如果从一个有一个统一的，大一统的多模态大模型的话，嗯我们想生产的部分，如果嗯如果是比如说都是一个transformer，或者怎么样的话，反正至少现阶段我感觉还是有点困难。

嗯明白啊，这是我的经验啊，不是结论嗯，明白对对，我们多收集一些输入，对啊，姜老师，您的看法是什么样呃，我我的看法是呃，在算力相对没有那么多，或者是呃model size没有那么大的情况下。

我比较倾向于DE呃，就是扩散模型可能会能快速达到一个，还不错的效果，那随着算力的增加和model赛增加，我是比较相信那个AR的这个这条路线，能够达到一个更高天花板。

当然前提是他的code book做的足够好，对包括他的那个嗯这个TOGANIZATION的话，可能是一个视频和图像上联合建模的，对包括他的code book的呃，对这是我的看法，这样的差异在什么。

来自于哪呢，为什么就是这个scale会成为我我我我的，我的意思是说，就是呃因为那个呃就是AR这条路线的话，就是它呃就未来的话，它的scanning的空间会比diffusion会更高一些，对啊。

随着就是它的，就随着算力的增加和model赛增加的话，就是它的呃，就是后面会追赶上diffusion在某个节点，并且表现出更好的效果，嗯好的明白，对我感觉这也是很很有意思的观点，对孟孟老师有什么看法。

谢谢我其实也不是相关领域的专家，就抛砖引玉一下，就是如果从文本这个角度来说，其实这一直是NOP，还有ml这个领域的一个一个大家关心的，就是怎么做生成文本对吧，这个事情其实做了这么多年了。

在深度学习的时候，就最开始应该是RNASTM这种，到现在呢，可能就是以transformer为这个主流了啊，基本上是主流了，那其实他的学习的这个机制，本质上大家还是以自自回归为多啊。

auto regressive为多，然后架构上也也在transform为主流，但其实这两方面其实也有新的挑战呀对吧，其实你从学习的目标上，现在其实我关注到有这个BFN啊，这样他也可以做很多。

就是basin flow network对吧，他也可以做这种离散的数据的这个建模，包括呃之前有什么diffusion bird啊，还有就是类似的就是应该是有很多，就是建模上大家其实也在尝试不同的手段嗯。

模型架构上更不用说了，就是像MANA，还有这个啊，前两天看到这个新闻是嗯，RN又重出江湖了什么的，就嗯在架构上我想大家肯定也会啊，接着探讨，因为transformer它这个复杂度。

还有它就是为了解决它高复杂度，大家用的这些trick虽然说发展的很多了，但是如果说我们真的能把它这个推翻的话，其实你你你这些trick其实都没有必要的，就是我如果能真的搞一个这个线性的这种，这种机。

这种模型的话，对，所以我觉得其实嗯，这当然你要证明这件事情也很难，因为嗯因为sky nore这个问题，你必须搞搞到那么大规模之后，别人看到东西可能才会白银，但是我觉得。

这正是因为有这些不确定性或者质疑吧，就是呃大家才有这个去去做这些东西的机会，不然大家都看到了之后，那又那又就是可能会比较卷对吧，诶好谢谢，好封建有什么看吧好呃，这个我我我我我我我也是一个自己的想法。

不一定对，就是呃我我我最早思考这个问题的时候，是从纯纯数学的那些角度或者怎么样，然后发现其实没有什么结果啊，就是呃虽然刚才那个呃谷老师质疑了，这个diffusion是最大自然的这个。

但但我之前一直是从最大自然的角度去，理解它的对，然后然后如果你认为他差不多在做自然，自然的话，其实它和呃子回归其实真的很像啊，或者很相关，然后或者说之前有很多人说什么压缩及智能，其实从信息论角度。

各个角度去理解，它都有压缩的这个感觉，你做做压私人估计都是在压缩，所以所以然后包括就是我们也做一些，transformer和diffusion的结合，如果你相信啊，skating的这个能力。

有可能来自于transformer的话，所以这个事情说不好，然后但是这是数学，那纯数学，所以我怎么也想不明白，为什么比如说视觉的模型现在都这么小，做不大，对不对，为什么。

就比如说像基于diffusion那些像像文道图，我们都是在10亿左右，然后扩大之后效果不明显，所以它解释不了这个问题，它它一定是一个哦，我觉得跟数据或者跟工程或者是跟呃。

我不知道一些knowledge就很相关啊，就就就然后从这个角度去想，你的其实变量很多，它不光是diffusion和呃这个自回归的变量，其实还有语言和图像，它本身模态的这个变量对。

然后我们呃呃最近有些尝试啊，然后我除了刚才智杰提到basin flow network，它是一个连续化的这个diffusion之外，我们也做一些mask discution，就是很离散的。

然后但他们都是defusion model去做语言，然后现在有些进展，比如在g p t two大桥上，其实没有比传送门差的多，所以在它它可能是一个skating，开始探索的起点啊，所以从这个角度来讲。

可能还有很多的这个可研究的空间啊，也有可能是一个很负面的，就是发现语言上你也不能skin up，那就是那那就是自富贵了，对不对，那就那就是自富贵了，对但但但也不是很确定啊，对啊对好我。

我基本上就是有一些这种看法啊，对好的，顾老师有什么观点对我，我对我也谈一下我的看法，呃其实跟我今天talk讲的一样，我觉得这个什么范式其实不重要，根本问题是我怎么做信号拆分是吧。

当我把language你拆分成一个一个token的时候，它天然就适合用auto aggressive model来做，当你把这个image拆分成了这个noise intensity的。

这个每个肯定向这个噪声更小一点的情况下，你天然就应该用diffusion来做，因为这个你每一种这个拆分方式，你其实会带来自己的inductive bias，然后你每一个自己的这个范式。

其实也有自己的inductive BIOS，当你的这个范式的inductive BIOS，跟你的拆分方式的index tive BIOS一致的时候，那你自然就有些好的结果。

所以所以我觉得这个呃个人的看法，我觉得这个问题其实不是那么重要的问题，重要的问题是我们真的想清楚了，怎么拆分，自然而然就会有更更合适的这个办事的出现，对谢谢对我感，我感觉这个郭郭老师的这个看法。

也是比比较犀利对，就是呃确实是这样，是有INDUCTIVICE，包括我想就是像姜老师VR做的很成功，是不是也有这种刚才也讲了，对这种这个视觉的层次化的，这种indux space的考虑。

所以所以所以或许这是一个指引，下一步的这种方向好，那我们的第二个问题可能探讨一下skin law，就是你们刚刚的报告都都出现过skin la，对我我一直有一个特别想不明白的事情。

比如说我们的语言它在scale up的时候，大家现在对他的能力的提升是有一个刻画，比如说它增加了这种推理的能力，增加了word knowledge，增加了比如说去做数学题的能力。

但是我们比如说这个视觉的模型，不管是图的还是视频的，它这个模型大了以后，确实生成的图片质量变好了，FID也变好了，但是就是呃这这具体发生了什么，到底大的比小的是强在哪了，怎么去进更进一步的去刻画。

这个他的能力的提升，以及有什么问题是scaling可能解决不了的，就这这个问题想跟几位嘉宾探讨一下，对啊，我想想我们这菜按什么顺序来对对，我来反过来好嗯对好行，没问题，这个我我我上来就是暴论了。

我觉得这个呃，我觉得是呃，这个scaling law还和和刚才那个还不太一样，skin law是基于这个范式来存在的，就是我，所以所以我猜这个我们现在主要想讨论一下。

diffusion model相关的斯SK领了，对我觉得这个diffusion model没有skalling out，只有一个原因是大家没有一个好的evaluation，Metric。

我们来想想language model为什么有valuation呃，language model为什么有这个skin law，是因为这个有有一个很天才的人，站出来说这个压缩机智能。

所以我们用这个n l l loss是吧，Negative log likelihood，我们就能衡量你的这个language model。

training这个inference以外的这个performance是吧，然后呢他又有这个等价性，哎所以更好了，我们直接把这个SUBTASK直接做一个average，哎我的这个NL了。

这个negative log likelihood的loss，就可以就可以用来度量我们的这个performance，到底怎么样，到defasion model，这这个出问题了，你你没有办法度量。

你有啥办法度量，第一就是我，我今天也提到这个，这个如果你按diffusion的方式做拆分，你的这个importance，你其实是不中不知道的，你不知道到底谁比较重要，所以你不知道怎么把这个每个每每个。

SUBTASK的这个negative log likelihood来进行加权，所以这个首先这边这条路我记了第二条路，这个你用什么，你用FIDFID的问题就更多了是吧。

你首先你你FID你到底用什么抽feature，你抽feature带来多大的BIOS，第二你FID做的这个高斯假设是吧，你这个这个这个那么那么复杂的数据分布，你真的能满足高斯假设吗，你这个度量一阶局二阶。

举这个这个这个这个说出来都好笑是吧，所以所以我我都不知道到底用什么，这个evaluation metric，所以我我觉得主要问题在这，当我们能把evaluation metric搞定。

这些问题其实就就就迎刃而解了，对对您的那个个人简介里面写着，您做那个evaluation那个是啥呀，是跟这个有关的吗，这个没有没有关系，对我我我自己其实也在做一些这个呃，生生成模型的一些这个质量评估。

相关的一些事情，对对这些事情我都挺感兴趣的，对我我这个也也不代表我研究出来了是吧啊，明白明白，好的好的好，李老师有什么看法，好这个对对，其实我我我这个这个问题，还还挺超出我的知识范围的。

因为我其实接触大的很真的很少，就我可能做的都非常非常小，或者是在在大的上面做一些加速啊，之类这种很前期的工作，呃，我自己是这样去感受的，就是从呃你你如果从生成的这个任务上来讲哈，就是其实理想上。

它是有一个非常非常好的生成模型啊，它能够去呃一方面达到最好的like you，另外一方面同时达到最好的视觉效果，或者人的感知啊，这个是有的，然后但是确实有evaluation的很多问题。

导致我们没有办法去量化，它到底走到了什么程度，但但从那个我我我我从，比如从总喜欢从概率角度去讲的话，每一个模型，不管包括干或者包括之前我们淘汰的一些模型，它其实都是一致的，一致是什么意思。

就是如果有无穷的数据，有无穷的优化的能力和无穷大的模型，它原理上就是会收敛到ground true state distribution，对不对，所以从这个角度来讲，从那个极端上来讲。

大家都有skinning了，对的，所以我们关心的是什么呢，关心的是在我们现在的这个资源下，或未来一段资源的情况下，他大家的这个la的曲线，或者所谓的是是超线性还是什么，线性或者次线性是什么样的一个性质。

然后然后这种情况下，就就跟其古老师刚才说的很有关系，就是你从你换metric，你的law就可能不一样，对不对，然后你你你考虑不同的这种贝塔的这种啊，某杂的题，它可能唠又不一样啊，所以所以从这个角度来讲。

就它会非常非常复杂，然后我们imperial里，现在感受就是自回归的这个特性是最好的对，所以这就是我的这种看法，然后但是呢呃怎么才能就是是不是这就是对的，也有可能就是就是这样的啊。

然后或者说如果没有的话，我们怎么发现，比如说diffusion有可能哪个地方没做对，或其他的还有更新的，那那这个就我觉得就很困难，就是我也没有一些什么呃，一些好的这种指导。

能够让我去意识到是什么样的东西，但我确实同意很多古老师这种看法，就是所谓的skin law，它会和metric有很大关系，然后和我们现在处的这个所谓的data的regime。

就比如说或者model size的regime，比如说是在百亿到千亿之前或怎么样的，这个有很大的关系，它它是一个应该是一个非常非常复杂的，然后我们empirical里只能看到的一个。

非常非常狭隘的一个地方，对然后大家也不太会有钱的人，也不会太去乐于去探索其他的，他会强化自强化我们对子回归的这种印象，因为大家会从这个角度去去做，比较安全对啊，这就是我的看法。

啊呃刚谷老师和李老师分享的，其实我有有几个点我是特别的同意，就是从MATRIC呀，还有从一些模态的一些角度，其实嗯是这样，那就是我个人其实在这一块呢，没有特别多的理解。

就DEPUSION的skin nor，但是我有一些这个经验性的这个呃看法，然后呢也想听听各位各位的这个呃想法，就是我发现可能是不是图像的这个defence model。

或者是文道图的defence model上，它的这个呃skinning nore里面啊，我的这个training flops是最有用的啊，就是不是说狂加图片就有用，而是说我要加一些。

就用一些高质量图片好像就够了，然后狂加training flos，然后对吧，然后是不是呃这个我现在也比较关心，就是这个比方说文到视频这样的一个场景里面，那你嗯这个它里面，我们到底嗯想把性能拉上去啊。

比方说就是人的视觉性能，而最后人的观感，那我们到底应该在哪些方面投注呃，倾注比较大的这个呃呃资源啊，到底是标数据呢，还是嗯搞显卡，来来来来串，其实这这些我我也一直在思考，我我也我也不太懂啊，可以嗯。

如果后面几位老师有想法的话，也可以啊，帮我解答解答，我就这些，谢谢，嗯呃我我说一下我看法呃，就是前面几位老师说已经表比较多了啊，包括那个呃metric对，然后我说下我看法。

就是说我认为所有的生成和理解任务，都是围绕着人的，就所有的任务都是人定义出来的，对包括就是生成和理解，包括视频中的，所以我认为语义是最重要的，那语义怎么来呢，就是轮廓还是通过人来。

所以我认为啊diffusion model的PRETRADING，它实际上语义相对呃，呃就是语言模型要差很多对，所以我认为语言模型的语义才是未来，所以我觉得呃我我我的看法就是，嗯语义是最关键的。

如如果diffusion，未来如果能够啊formulation出一套PRE，就PRETRAINING这个语义的这套框架的话，也许它将来会是一个比较完美的框架，但就现在而言的话，就是呃。

就是如果围绕着我们这些人的所谓的一些概念，或者视频中的一些场景，包括文字渲染，包括一些相关的一些task，这些都是最关键的，然后这些语义才是最重要的，对这是我看法，老师好，我来谈谈我的认识啊。

其实我部分观点跟李老师很认很认同啊，呃因为我做多模态做了好久啊，做了45年，我是觉得语言跟就是视觉的这两个东西，差别其实特别大，我我成天还在死磕他们之间的差别，你比如说我最近做了很多事啊。

嗯就是比如说嗯我们的大大模型啊，嗯把语言的那MOE啊，就是套到多模态这边就会发现它的效果很差，其实其实很差的对，很多时候就是因为这个模态变化导致的，你看起来好像token化是一样的，案例的观点都一样。

但是它就是不一样啊，然后比如说嗯你语言模型的幻觉我觉得还好吧，但是在视觉这边他又特别严重，就导致我们很多时候，比如说要把那样多么的大模型，用到跟摄像头结合呀，他是在一些比较严肃的场景。

你的识别率不准或者幻觉比较严重，别人根本不会让你去试的，所以呃所以我觉得这个视觉，这个模特是特别奇怪的一个模态啊，我我我也懵懵懂懂有一点感觉，但是我也说不清楚，我是觉得吧呃语言我是觉得他是人定义出来的。

它是个离散的，甚至我觉得不客气的说，基本上可以穷尽呃，但是好像视觉吧它是个连续的嗯，如果从比如说假设背后有个物理的东西，能表示清楚，它可能能用一个物理的模型表示出来，所以他需要的参数量可能不需要那么大。

但是呢你这么理解，好像觉得SSKINNER不是那么严重的个问题啊，但实际上发现你做在应用的过程中间，又发现，视觉这个东西特别容易出现各种极端的情况，就是啊可令呃。

就是他的那种什么bad case又特别多，导致应用各种困难吧，所以我也不知道，就是朦朦胧胧感觉到了，他可能跟落在这个视觉模态上，好像不是那么严重，但是好像把它做好，我也不知道怎么做好。

比如说好像做设计的人特别讨论吧，比如说VIT那个模型，他们把它加大好像没有太大的意义，就是没有表现出很好的效果来，就是说你可能费了很多事，他提升很有限，这个原因到现在也没搞清楚，这是我的经验哦。

对是不是结论又是不是结论，对我感觉刚才的一些看法都很有启发性，对我自己也朦朦胧胧懂，当然当然我自己也没真正做过大的skill up，但是从大家模型的结果来看，我觉得好像呃就能力的提升还是有一些刻画。

比如说当年做干的时候，大家可能在追求这种呃一些consistency，比如说你生成一只狗，这只狗到底是不是两只眼，那它或者是它有八只眼，就当当时大家是做不到的，后来包括那个那个SD3。

大家看到了一大堆那个奇怪的case，就他他理不理解这个事儿该长什么样，然后到最近这些视频的模型，大家开始谈论说他的一个什么物物理规律，他知不知道这个物理规律长长什么样。

然后呃今天我又看到了那个那个叫dream dream是吗，是啊，那个是刚开完那个视频生成，然后说他那个呃他那个合影，然后OpenAI的那个几个人的合影，然后他变成视频以后，这合影这里的人就开始打架。

然后说这个这个社会学规律，就我们说如果他说作为一个word model的话，他可能会有一些理解，就是说这几个人前一秒还在微笑合影，下一秒他该不该打架，就是嗯我感觉这3号还是能能有一些客观。

你好像观察到就是sell up以后，他3号他他能学到，然后这个幻幻觉，我总感觉可能有可能是大语言模型，scp up也不一定能解决的问题，但你说这个视频里面，他或者图像里面他有没有幻觉，就比如说对。

这就是可能人的这个期待，就比如说你大语言模型，你问他中国这个球队是哪年得到世界杯冠军的，然后详细描述这是怎么夺冠的过程呢，如果给你输出一个夺冠过程，你说这个是不好的，这是事实性的错误。

然后但是你这个视频模型，你全生成一些什么宇航员，骑着马，反而大家觉得这是好的，就其实我感觉这其实也是幻幻觉了，对吧嗯对来做一个，对我就简单的就插一句哈，我感觉幻觉这个事情可能呃，就是伴随着这种概率的。

他就与生俱来的，就就相当于就是说你给他一个什么样的一个，条件的输入，哪怕他没见过，他也要把一的概率分配到后边的所有token上，对不对，所以从这种情况下，他他就一定会有这种啊不好的这种情况出现。

包括我们早期机器学习理论，不也是这个近似概率正确吗，所以从这个角度来讲，你你扩大会缓解你的幻觉，但是它它可能还是会存在，所以可能会有一些比卢老师提到，有些严肃的场场合下，我们可能还是很难去用。

所以即使是图或者视频也也有这种情况，但感觉这还是个比较重要的问题，对对对，是的是的，感觉现在这个方面研究其实还是比较少，对好我们按预设的这个时间还有10分钟，我还准备了一个问题，但是我我也想听一下对。

就是台下有没有什么问题，没有，有没有人能帮忙递个话筒，或者我来递吧，嗯其实就像老师们刚才提到这个幻觉问题，然后我想问一下说可不可以利用说思维链，然后去嗯的利用提供一些结构化的推理路径。

然后引导模型产生一些扎实和实质性的输出，就是刚才提到这个幻觉效应，然后去利用思维链，然后去引导模型，产生一些扎实和事事实性的输出，包括这种有没有一些就是说方向的应用，简单回答下就行。

这其实我也不认识这方面专家，但应该是有相关的工作，那个在benchmark上能证明他把就是通过思维链，或者是一些什么prompt的手段，可以缓解那个呃缓解这个幻觉的问题。

然后我记得我我们我们上海交通大学，清源剧院的那个刘鹏飞教授，他也做了这个事实性检查的一些工具啊，也就是说借助这些辅助的外外部工具，我觉得幻觉肯定是一定程度上，我们很很很多可以这个消除掉的，对对是的。

谢谢我多多说一句哈，我感觉就是你思维链这些，它其实相当于就是呃输入不同的条件，然后呢呃让他去用对应的条件模型来回答，你虽然都是一个模型，但是你的prompt这个条件的改变。

会让你后边的这个条件概率有一个显著的改变，所以它会有区别，然后呢，你合适的prompt，可能会激发他合适的语料的能力，比如说什么，我之前上课经常举例子啊，就比如说谢邀，人在美国刚下飞机。

如果你输入这个problem，他显然会用知乎的语料，去相关的这种东西去回答，它会激活那个地方，然后你用一些其他的可能激活其他的，所以这会有区别，然后呢但它就是它可能会在某些特定的任务上。

你用特定prompt，它应该会有好的表现，但sam how呃，未见得会有一个universal的好的一个东西，他一定会对所有的任务都会好啊，这可能是比较困难的，对对对，但但对。

但anyway就是很多现在这种探索，我觉得也都是有益的，比如思维链啊，或其他的一些对，当然可能我们还缺乏一些，就是呃呃更深的理解吧，对对好，也也谢谢这个问题啊，还有没有什么呃嗯听众们想讨论的问题好。

我们还有几分钟时间，我们再稍微简单的讨论一下吧，反正因为我们做研究，我们还是想关注这个有什么问题还没有被解决，所以可能现在比较哦有一个哦好好好，哎我我想我们现在实际上嗯，在研究这个问题当中。

我们肯定是没有你们那么高端，也没有你们那么基层，但是国际的应用层在十几年前，我们已经在筹划怎么样要结合思维念，写出一篇授课稿，是我要给大家讲一个讲一堂的课，思想政治教育课，那么那个稿子我以往有模板。

有十几20篇，那个好的模板了，有些是讲故事的，有些是跟讲道理的，有些是举数字来跟你说话啊，说话的一我的性格是什么，我这个人就喜欢讲故事，我这个人最喜欢讲哲学，那么你就选你的模板么，选了模板以后。

但是呢最新的事例还要根据我的思维念来来走，视力要换，不断的要换，那这里呢我在半个小时之内，人机结合人机互动，我要写出一篇很有就说感染力的授课岗，这个思维链呢要是跟着我走的，它不论是说你这个大模型。

你想我这几个条件除了什么就是什么，如果我们现在就说，这几天我们已经进行了一些训练，但是大模型那个可能还还达不到，但是呢就是他怎么样跟着我结合起来，哎就这个，这个属于您，您的这个算是一个呃不问题。

我现在就是如果是有需要，我们马上肯定就能课项目就能立起来，这个问题是说怎么做这种，这个人在环路的这种生成，是是我不知道我理解的是正确的吗，嗯反正这个问题，我可能就说现在大模型它写出来的东西啊。

就说大语言模型写出来的东西，一说多一幅画，写一篇小说，它没有标准，但是我要讲课的时候，我是部队出来的，我还是在认为我就是讲思想政治教育课的时候，他现在这个信息瞬息万变了，变得很多了。

人比大脑他处理不了这么多信息了，我们最早给下面讲课的时候，我是占据信息的绝对的主动权，我拿一张参考消息，他们有，我就可以跟他讲，现在下面听课的都比我掌握的信息要快了，那么这么远，我就要怎么样迅速地。

按照原来一些好的讲课稿装，说，说白了就是旧瓶装新酒装进去，在我喜欢的模板之下写出一篇稿子出来，感感感谢您，您您我您，您用一句话能不能提一个问题，谢谢谢您一句话，谢谢你提提一个问题。

就是您一句话总结一下您的问题，然后我们方便我们理解，然后我们是不是在回答还是什么对，一句话就说就是要跟着我的思维，去协助我写出一篇很生动的思想，政治教育授课稿，我我总觉得这是一个长上下文的问题。

就是呃我想着那个之前有一个叫模世界模拟的，那么一个英勇，说一堆那个小人儿，他们在一个小镇里面，然后让让他们去那个互动对吧，然后呃每个人他有自己的记忆，他他记得自己说过什么话，然后他跟别人说过什么话。

他自己的背景是什么样，然后呃他这样的话，它就能生成的更加这个符合这个人的特点，然后我总觉得这位老师的问题，可能是一个长上下文的问题。

就是如果我们有infinite lg的这个context length，那我就能把你对吧，你出生以来，所有的这个哼发生跟世界发生交互的内容，都记下来，放到你的长上下上下文里面，然后再再去记忆这个生成。

它可能就是像你说的这个什么呃，符符合要求的这种思想政治课，对之之前好像有一个那个数字分身，我不知道是那个那个是谁的CEO，跟自己数字分身对话，是一个个性化的GPT，把他的所有采访访谈。

写过的书都输给那个GPT训一下，然后他就来来模仿自己，就跟自己对话，好像有一个那个video可以可以可以可以看一看，但我忘记叫什么名字了啊，对对对对好，那当然了，还有几个更深层的，我不帮平那手。

我下来以后，我还可以再跟你们讨论几个，更更是我们可以还线下还可以交流，谢谢谢谢，我们对我们还稍微有个35分钟，要不我们再在这个再再跟讨论一个，各位老师好，呃，就是我这边的就是两个问题的话。

我是从应用应用的那个角度来提，提的问题就是第一个的话是嗯，因为我们一直在找一些那个场景嘛，就是现在这个扩散模型的话，嗯就是我们有有这样一种场景，就是想在比如说我呃，我在我要在一件T恤上对吧。

我还要印上一个啊，就说比如说是圆桌讨论这样四个中文字对吧，但是现在的那个呃很多模型的话，我看他是就是就是我们去找找这样的模型，就没有，好像我不知道是可能是我不知道对吧，但是我就想请问老师呃。

国外老师就是我如果要达到这样一个效果，我是已经有现成的这个技术了，还是说有什么途径去呃，找到这样的一个呃这样这样的一个呃模型对吧，嗯这是第一个问题，您您能再重复一下要达到什么样的效果，但啊就是很简单。

比如说呃我要在我的这个，我我说说出一个提示对吧，我要在我这个T学上啊，打印出呃呃呃圆桌讨论四个字啊，这个扩散模型里面就是那个，比如说SSDX2嘛，对不对哦，这这这文文本生成文本线染哦，纹身图对。

就理解成纹身图啊哈哈，因为啊因为就是我看现在最新出那个SD是呃，三对吧，它其实是可以支持呃，比如英文喊我对这支持的挺好，但是中文其实是没法支持的，然后在chat g p呢。

它其实也基本上是不支持这个东西的，对其实其实大力三做的真的很不错，那个现在stable diffusion3，这个相对于SDXL在这方面提升也很多，然后呃我们我们有些同事在这边。

其实也做过一些很不错的一些探索，但他们做的是这个专有化模型，大部分走的到技术路线是这个有一个这个foundation model。

然后在foundation model的基础上做post training，就拿这些字专门做post training，然后将train的这个专业化模型，其实做的结果还可以啊。

就是要自己训练个专业模型是吧，是的啊，就是不能在现在现在这个这个呃，开源的预训练模型上其实是达不到的对吧，开源有的预训练模型也还可以啊，比如stable depension3也还不错呃。

但是中文是我试了一下，好像是不行的，因为他刚不是开源出来了吗，我不知道各位老师有没有试试过这个问题，对我我其实没有试过中文那个呃，但也可以理解嘛，这个中文数据量毕竟相对少一些，yes8C一些。

这是因为呃是呃他训练的时候，这个语调的问题是吧，对我觉得是的啊，谢谢，这还有一个问题的话是呃就是现在这个SD的，他对这个就是位置是不是也不是太敏感，比如说我想在这个呃一个呃一张图片。

就说我想生成一张左下角对吧，我左下角生成呃啊，就说生成一只小狗，对不对啊，就是我的提示语对吧，我左下左下角生成一只可爱的小狗，那这个的话基本上我试了一下呃，也很就是现在这个这这些模型基本上都呃。

很难做到这个事情，那这个的话是有没有什么解决的一个途径啊，对那我回答一下那个对我，我觉得我觉得这些这个foundation model之所以做不好的，原因还是因为数据量的欠缺。

他们的这个decomposition double，不好意思说错，他们的这个disc entanglement做的不够好，所以针对这些问题，我我自己认为general的这个最最简单实用的解法。

就是做post training，你不管是用RL的方式做，还是用这个其他类似SFT的方式来做，就collect这B系列data，然后这个对着它在to就好了啊，就是还是就是去训练自己的专业模型是吧。

对是的哦好的，就是现在是这个就是开源的好，这这这这方面有限，有做的比较好的吗，还是说都是还是得自己去训练，因为这个我觉得它其实是一个通用的，一个应该是个通用的需求，对吧啊，还不是说一个行业。

说专业行业的需求还是什么的，这个对，因为因为这个这个，这个其实其实这个问题其实很多，这个你说这个position也是问题，你说这个一些什么一致性也是问题，然后包括您说的这个字体都是问题对。

就是一个general模型，这个拟合不了这么多复杂的数据分布，其实也是可以理解的，因为你看像我们做language model，其实也不是这个真的一个模型，他就把everything都做好了。

有人这个针对读读论文去做了一些fine too，做针对这个其他一些task做了一些发音to，我觉得在目前阶段是可以理解的对，所以目前阶段呢我觉得这个做一些specific model。

是一些相对更靠谱的路，对啊好的，谢谢好，我看我们的时间也差不多了。

# 2024北京智源大会-视觉大模型 - P1：论坛背景与嘉宾介绍：颜水成 - 智源社区 - BV13x4y1t7sb

各位同学，各位老师，各位朋友，这个早上好，那个呃大家知道就是说其实计算机视觉的话，在这个研究的领域的话，曾经一直是这个研究的最前沿，当时的状况如果是在5年前或者10年前的状况，是说，所有的最新的算法。

全部从计算机视觉里面出来以后的话，再把它转移到比如说语音NIP，或者说再到这个其他的，比如说甚至甚至来说就是这些视觉的话，可能还会转移到比如说medical image analysis。

也就是说一直来说的话，其实计算机视觉的话是这个是算法的，这个就是最源头，但是自从transformer出来之后的话，然后的话风水轮流转。

然后的话变成了everything comes from AI p，然后MMP出来之后，比如说像transformer出来之后啊，不比如说像那个那个VIT的这种这种结构，出结构出来之后的话。

要不就是视觉里面出来之后的话，才从啊才从APE出来之后的话，才慢慢的转移到这个视觉里面，那么包括最近出来的这个man宝，也是先有MANA再有展宝的是吧，再有这个就是相当于是是再有再有呃，呃WEMBER。

相当于是就是说现在AIP里面的话，在产生出一些新的一些成果，然后再慢慢的往这个事业里面转，但是今年的状况又有一个变化，自从年初的时候的话，这个lama出来之后的话以后，大家看到就是说好像计算机视觉的话。

好像要重回第一赛道的那种感觉，所以我们今天的这个计算机，这个论坛的这个主题，是说我们的计算机视视觉的话，是不是在技术领域的话，又重新回到了这个第一赛道对，所以为此的话我们也邀请了。

就是说在计算机视觉在过去这一年里面的话，就是说做了非常不多啊，非常好的这个contribution的一些学者，和这个工业界的一些呃成果，相当于是所以今天我们有四位的这个啊，讲者第一位啊。

我就不按这个顺序来，按照我这上面的那个顺序来讲，就是呃第一位是鲍凡，来自申诉科技的CTO，欢迎，第二位的话是open solar的负责人，也就是由洋老师团队的这个呃。

这个open solar的负责人声称会欢迎，第三位的话是这个原来在cf force，然后他在这个多模态的，这个就是clip的这个维度的话，做了非常好的这个contribution是底俊蓝，对啊。

还有这个论坛的话，然后那个来自南开大学的杰出教授，也是计算机系的几个主任，陈敏敏教授对，陈明明赵州应该是啊，在这个孙健和那个何凯明之后的话，华人里面的话有大批量这个SITATION过千，论文的这个作者。

也是我们这个青年这一代的，这个杰出的这个代表，所以啊所以我们这次论坛的这个组织的话，也是陈敏敏老师做了非常多的这个贡献，好谢谢嗯，好那啊，一般的情况下的话，在这个论坛的时候的话，在呃在大家演讲的时候。

一般会出现一个情况，就是讲着讲着时间就过去了，所以我的开场白就不讲太多，我们就直接步入我们今天的这个presentation好吧，那么第一个的话是呃，由来自这个open solar的负责人。

声称会给我们带来的这个呃talk叫open solar，高效低成本视频生成模型。

# 2024北京智源大会-视觉大模型 - P2：OPEN-SORA高效低成本视频生成模型：申琛惠 - 智源社区 - BV13x4y1t7sb

各位朋友大家好呃，我今天非常荣幸可以来到这里呃，为大家分享我们open sora的这个模型，嗯这个幻灯片，它右侧展示的呢是我们第一个开源版本，也是今年3月刚刚公开嗯，所展示的生成的这个海滩。

然后嗯我们希望可以通过这个模型呢，让更多的中小企业可以以比较低的成本，参与到这个视频生成模型这个嗯当中，那嗯open sa这个项目呢，是在由洋教授的指导下进行的，呃，嗯就是这张照片是有阳教授。

在伯克利答辩的时候拍的，然后他身边是几位伯克利，在高性能计算和视觉领域的教授，那可以说这两个领域呢，对我们open sa的项目都是有非常密切的关系，呃先简单讲一下我们今天的大纲。

就是我会首先简单的介绍一下SORA，然后为为什么我们需要嗯，想要去做这个open sara的项目，然后接下来是对open sora的一些技术解析，然后当然就是要嗯做到低成本高效率的训练呢。

嗯当中也用到了路程科技，这个CLOSALAI的一项加速工作，然后最后呢我们会嗯给大家展示一下，我们最新版本，也就是其实是下周即将公开的这个V1。2，版本的的这个推理资源的测速。

然后最后是嗯与大家分享一下我们的模型效果，在这边呢嗯也是主要是想嗯抛砖引玉，因为我们认为就是说在视频生成哦，目前其实还是处于一个非常早期的阶段，就是说虽然市场上，我们已经有了一些商业化的产品。

但是要做到一个非常成熟的，这个嗯生成的模型还是有一段距离，目前可能还是离不开大量的剪辑和编辑工作，那嗯希望嗯通过我们这个open sora的，这个嗯初步探索的成果呢，就是嗯能够激发大家兴趣。

然后进行深入的讨论，那首先我简单介绍一下SARA，我相信在座的各位嗯，对open aid sora都已经非常的熟悉了，就是他是一个OpenAI开发的，生成式文本到视频生成的模型，然后嗯其实在他之前的话。

市场上呢，就是嗯已经市场上已经有了一些呃模型，像比如说软尾m l p custable video，但是在SARA出现以后，就是他从这个视频的生成时长，甚至质量来说都已经远超嗯当时的模型。

然后达到非常令人惊艳的效果，嗯然后嗯大视频这个嗯模型生成这个呢，其实嗯它的应用前景也是非常的广泛，就包括到游戏艺术媒体制作药物研发广告，教育各种嗯这个行业，然后我们可以看这个幻灯片。

右边这张图呢是扎克库科夫，他在twitter上面的一则发言，就是说，嗯视频模型使我们以嗯低成本制作这个电影，也是成为一种可能，那嗯虽然说嗯open sa它的效果非常的好。

但是嗯就是OpenAI的SORA，不好意思，它效果非常的好，但是SORA目前是没有公开的版本的，一方面呢可能说明这个嗯SORA，他这个大视频的技术并没有完全的成熟。

另一方面可能也是说他的使用的消耗的成本，是非常的高昂的，那即使它的功能非常的强大呢，它封闭的特性也导致说呃，目前我们不能对它的应用场景，或者这个模型进行进一步的用途的拓展。

因此呢我们就开发了open sora这个模型呃，首先需要声明一点，就是open sara跟OpenAI的SORA，它是完全不同的两个模型，然后我们做的是嗯，根据这个OpenAI sora。

它的技术报告中的一些嗯技术，来做了一个类SA的模型，然后希望呢也可以帮助大家更好的理解，如何去复制这样的模型，或者或者就是说在自己自己的产品中，可以体验类似于SARA这样模型的效果功能。

open sora呢是首个开源的类SORA视频生成模型，然后我们主要目标就是说用低成本，完全开源的方案把它的模型引入社区，那我们公开了模型结构，已经训练好的模型参数嗯，完整的训练流程。

我们的数据预处理流程，也提供了一些视频生成的教程，方便大家可以直接使用我们的模型进行体验，接下来我来简单的讲解一下，我们这个open sora第一个版本中用到一些技术嗯，分为以下三个框架。

就首先我会分析一下模型的架构设计，以及我们的模型的背后的原理，然后在第二就是来讲解一下，我们的LISA的训练方案，第三也是非常重要的一个部分，就是数据的预处理，因为数据对于视频模型的训练来说。

是非常的重要的，那首先模型架构我们也是用了基于DIT的架构，当我们因为需要降低这个模型训练的成本，所以我们首个版本直接用到了pixel alpha，作为模型的初始化，那大家可以看这个幻灯片。

左边的这张图是我们第一个版本的模型架构嗯，可以看到我们嗯除了这个，因为pixel alpha它是一个纹身图的模型，但是我们在这个special attention，就是空间的这个自助力。

后面呢多加了一层时间的自注意力机制，通过完全将空间和时间嗯进行分开处理呢，我可以大幅度的降低这个模型的成本，那嗯右边的这张图是我们对于两个嗯，对于di it和我们提出呃，和这个我们的SDDIT的架构。

进行了一些测速，可以看到就是当这个token数量增加呢，就是这个STDIT，它在吞吐量上是有非常大的优势哦，那接下来呢呃会讲一下open sa的应用原理，嗯相信大家都看过SORA的这个技术报告。

那其实我们的原理呢嗯大家应该也不会陌生，就是我们会对视频和这个文本的控制信息，都会进行一个encoder进行编码，然后呢把这些编码的信息传入到STDIT中，嗯进行训练，这是我们的训练阶段。

然后推理阶段呢，就是嗯和diffusion model就非常相似，我们就是从这个嗯编码器的这个潜在空间中，进行随机的采样，然后嗯再把这个采样到的噪声，输入到STDIT中，让它进行降噪，然后嗯制造以后呢。

这个特征再经过我们的VAE的这个解码器，来生成我们最终的这个视频嗯，嗯接下来是训练细节的部分，嗯其实嗯就是open s呃，就是SORA的这个方案呢，他们的训练成本可能推测是在数千万到，数亿美元。

这是这个，使得其实大部分企业都没有办法参与到这个视，嗯视频的模型开发中，那我们的目标呢是说，要将成本控制在1万美元左右，嗯我们其实这个训练呢主要分为三个阶段吧，就第一个阶段就是说大规模的图像预训练。

第二个阶段呢是说我们进行视频的预训练，但这个阶段就是说我们的视频，它的分辨率会更低一些，然后可能它的质量也没有那么好，就让模型简单的具有一个嗯视频对视频的理解，然后第三阶段才是在进行高质量的那个视频。

数据进行微调，让模型在视频上面的表现进行显著提升，嗯其实第一个阶段的话，我们是之前也已经提到过，直接把这个已有的纹身图模型作为初始化，这样可以大大降低我们这个模型训练的成本。

然后当然也是嗯目前也是有很多不少新的工作，有不少新的这个纹身图的模型，大家如果想要做这个LSORA的开源方案的话，也是可以直接拿过来用的，然后当时呢是嗯我们看完第一个版本的时候。

并没有可用的高质量时空的VIE，所以我们就是把stable diffusion，这个嗯空间上面的VIE能拿来直接进行使用，这个也帮助我们降低成本，然后第二个阶段呢就是我们在这个阶段。

大概嗯第一个版本是经过了2800多个嗯，H800的GPU小时的训练，然后成本大概在7000美金的样子，主要就是说嗯我们的SDDIT的架构，就是引入了时间注意力的模块，那可以说就是纹身图模型呢。

我们嗯之前已经有了这个special self attention，他的能力假设哦，我们可以把它看成是一个初中生的水平，那我们新引入的这个temporal of attention。

其实嗯是需要完全从头开始训练的，它类似是一个婴儿的水平，那我们目标就是说在这个一起进行训练以后呢，让两者都可以达到大学生的水平，然后嗯其实我们在这个时候，就是直接进行了大规模这个数据的混讯。

然后就是说我们发现这样子模型，学习的速度还是非常的快的，然后由此也可以看出，就是说这个嗯这种大模型，它在学习上面就是尽管只是说它不同的框架，这个能力是不太对等，但是也可以进行快速的学习。

达到水平的快速增长，当我们也用了多样化的数据训练，来增强这个模型的泛化能力，稍后讲数据的时候，我也会分享我们用到的数据集，然后嗯我们对于这个不同的分辨率呢，也进行了优化。

以达到就是说我们在有限的计算资源，可以这个情况下，可以更加有效的训练我们的视频模型，阶段三呢，就是说我们用这个更高质的质量的视频数据，进行微调，这边呢我们也用了将近2000个GPU小时的样子。

然后这一步是跟第二部的最主要的区别，就是说我们这个视频的质量，它的这个像素以及时间都会嗯更更高更长，然后嗯在这一步呢，我们就可以嗯达到这个视频从短到长，从低分辨率到高分辨率。

然后从低保真到高保真度的这个视频的学习，Now on，其实我们借鉴于ul two这个技术，这个也是自然语言中嗯一个技术，就是说嗯把这个transformer的训练呢，也是灵活的运用了不同的掩码策略。

其中嗯可以说嗯对比如说前K帧末末尾的K帧，或者其中任意的K帧进行掩码来训练，我们的模型，这样子呢，也使得说我们的模型可以嗯，非常灵活的应用到不同的场景，包括就是说嗯基于一个图像来生成一段视频。

或者生成一个循环视频，嗯或者视频到视频的生成，以及就是说一些视频方面的剪辑的性能，然后我们用了一个五元组，就是你可以看这个最下方这个示例，就是我们五元组来定义，就是不同的应用场景。

就是使得我们这个在推理阶段，有非常高的灵活度，嗯那这边呢嗯我也再来讲一下，就说我们如何去支持这个不同分和高宽比的，这个视频视频训练，因为SORA他的那个技术报告中提到。

就是说用原视频的这个分辨率宽高比来训练呢，可以非常有效的嗯改善这个画面构图，因此呢我们其实用了一种嗯，分统训练的这个策略，其中就是分桶，主要是嗯根据这三个看，大家看左边这边resolution。

Non frame，as sporal就是分辨率，然后视频的帧数以及它的高宽比，来将视频进行哦一定概率的分类，然后我们提出两个参数，就是说keep probability嗯，这些视频它都有一定的概率。

用它的原本的这个分辨率宽高比来进行训练，然后同时batch size呢也有助于我们更好的平衡，就是不同这个resolution他的这个训练时间，来更好的利用我们这个GPU进行训练嗯，第三嗯。

接下来第三个板块，我要讲的就是我们的数据处理嗯，这边是我们open sara嗯，这个第一个版本所用到的一些数据集，然后嗯其实我们的数据总量是非常的大，大概在100tb的样子，这对于我们的存储而言。

其实是一个非常大的挑战，嗯我来简单分享一下我们这个数据的收集流程，大家可以看，从图的最左边就是我们起始点是一个视频，一个video，然后我们会对这个视频的场景呢进行检测，把它分割成不同的这个短的视频。

然后我们会它对它进行美学打分，就比如说你看这个esthetic score，4。5或者6。5，以及我们会对它进行这个光流分数的打分，以及视频中这个嗯有的这个文，文本信息进行检测，就当这些处理。

就是我们都觉得它的质量令人满意之后呢，我们才会进行下一步，也就是说对于这个视频描述进行标注，然后当有了标注呢，我们也会再进一步去看，就是我们这个视频描述的这个标注的内容，是否和视频嗯近有一个有效的对齐。

然后在之后我们也会去检测这个镜头的移动嗯，以及一些嗯，这就是那种各类各类的这个嗯，我们数据清理的流程，然后以达到说我们的训练数据嗯，它是具有高美学分数嗯，大的近镜头运动以及这个强的语义一致性。

这样子的特点嗯，来训练出更好质量的模型，那这个对视频的这个描述的标注呢，其实open呃，其实SORA就是a OpenAI的SORA呢，他们是用到了g p t four v，这个模型来进行生成。

但是嗯如果我们也用这个的话，这个成本肯定是非常的高昂的，所以我们用的是一个开源的lava1。6，来进行一个自动标注，然后嗯后续的话我们也是通过这个match sc，这个分数，来确保说。

我们这个生成视频描述的质量是过关的，然后接下来嗯，我来分享一些我们这个数据上面遇到的挑战，嗯其实最典型的就是这四个，就是一个是高节点负担，据说我们训练的数据，可能单个数据集里面都是上达上万条嗯。

简短的这个视频文件嗯，小文件，但是数量非常的大，就会造成这个高节点的负担，然后此外呢就是是不断增高的嗯需求，因为我们嗯在也在快速的进行这个模型的迭代，目前呢我们已经开源了两个版本，然后也是将在下周开源。

我们的第三个也就是version1。2的版本，其实我们发现说我们这个数据的规模，在以每个月50TP的级别进行增长，然后第三点就是高性能需求，就是我们训练模型呢需要非常快速的去读取，这个大量的视频数据。

所以我们需要这个低延迟的性能，同时呢我们每训练一定的阶段，都会对这个模型的checkpoint进行一个存储，这个写入存储呢也是需要高带宽样子的性能，然后最后一点就是高切换成本。

就是假设说嗯我们在这个我们的存储，如果想要在多云或者多肌群训练场景下，进行数据同步或者数据迁移的话，因为我们这个数据数据量是巨大的，所以我们的时间成本也是非常的高，嗯那其实嗯介绍完整体的模型算法。

这个整个的流程以后呢，就是我们要考虑如何把这个成本降到最低，那嗯其实我们如果说要去租借一台H800呢，就是每个月的成本，可能会达到8万到10万人民币，那假设如果用到吧台呢，可能租金就要高达80万。

所以我们如果想要一次性的，把试验成本尽可能的减小呢，其实我们第一个版本只用到了八台，H800是64个，这个GPU进行了非常短的这个嗯训练时间，就是把速度和效率提到最高，然后我们的这个低成本的训练。

其实离不开classical AI的这个加速的，close AI呢，也是嗯，路程呢团队在高性能计算领域的一项优秀工作，它是一个专门为嗯大规模的AI模型的训，练和推理而进行深嗯设计的一个深度学习系统。

它的目标就是说我们要最大化这个计算效率，而且最小化部署成本，就是说嗯再不用大幅度改动，就是已有的代码，其实我们只需要进行嗯几行的代码的改变，就可以很便捷的运用到这个系统，然后他现在嗯大家可以看右边。

就是支持这个不同的平台嗯，那它主要是分成这样子的三层，第一层就是高效内存系统，第二层是N维的并行系统，第三层就是这个低延迟的推理系统，这样我们就可以最小化我们的部署成本，好我们先嗯来看一下。

就是第一层的结构，其实我们用到的是一个异构内存的这个系统，那嗯简单的来说就是说当模型特别大的时候呢，因为我们这个GPU内存空间非常的有限，所以我们可以灵活的利用到CPU或者硬盘中的空。

间来进行实时的这个数据交换，然后让这个单卡训练比较大的模型，也成为一种可能，然后呃我们的高效训练策略呢，其中也用到了非常多的并行策略，就比如说前面的这个左边流水线并行，多维张量并行嗯。

都是模型并行的一些方法，当然我们的这个多维张量其实也是有，就是对于数据的这个激活的一些优化的方法，然后最后面的序列并行呢，其实嗯这也是这个视频训练的一个难点吧，就是说嗯常见的一个困难。

就是我们视频随着这个分辨率的增加，以及训练时长的增加，他的token数量可以达到百万级别，那就是要如何兼容这个非常长的序列嗯，序列在模型上进行训练呢，可能就需要用到这个序列并行的策略嗯，诶不好意思。

当然就是这些嗯各种变形策略，就是嗯大家都不用去担心，因为其实这个CLOSALAI的系统已经它在内部，就是把这些优化都已经做好了，所以研究人员其实只需要去关注这个模型，它的设计和训练就可以。

就不用关注这些分布式计算的内容，嗯那这边是我们open sora在close AI上面的加速效果，就close AI，其实我们嗯这边左边的图呢，其实对我们的文本编码器和这个视频的编码器。

也是进行了一些加速，提升了它的吞吐量，然后也把TFI所占用的计算资源呢进行了嗯，这个份额让它显著的下降，我们可以在这个64帧，五百十二的分辨率视频的训练给它达到加速嗯，55%。

然后在单台服务器上面也可以训练，长达一分钟的高清视频，然后这个提速也达到了16%，接下来嗯想要给大家分享一下，我们最新的这个V1。2版本的嗯，推理资源的测速嗯，这边是我们的一点一比领大小的模型。

然后如果大家看右边的这个表格，就可以看到说这个1080P高清视频，如果我们想要生成16秒的视频的话，其实这个我们的token数量就已经达到了一个，mil领，就是百万级别的这个长度。

而且这个其实已经是基于我们的via e，对于时间上已经进行压缩了，就是如果我们没有压缩的话，这个序列还会更长，然后左边呢是我们这个在单台机器上面进行推，算的一个延迟，其实说这个时间还是非常的惊人的。

如果大家看这个八秒嗯，最高清的这个视频生成，它甚至可以达到800秒，而这种非常长的延迟呢，也是为这个视频投入应用带来了非常大的挑战，但是也可以看出就是说嗯我们在相同的分辨率，这个视频生成长度的增加。

和这个它实际生成所需要的时间嗯，我们自己的测试发现，它其实是一个线性的关系啊，嗯然后这边是我们这个对于存储需求的测速，测速呃，这个存储需求这个测验的效果，然后接下来这边呢是我们嗯三比零大小的。

模型的这个测试效果，左图是它的这个推理就是时间的延迟，而右图是他对这个存储的消耗，当然就是说随着模型的增大呢，就是如果我们用这个单单的一个H800的GPU嗯，需要注意一下。

这边其实我们测试只用到了一个H800的GPU嗯，它其实是没有办法容纳，就是我们特别高清的长时间的生成的，这也就是说嗯我们就是在这个视频生成，用到多卡并行进行生成部署是非常的重要的。

嗯那嗯有了之前的这些观察呢，其实嗯我们这边如果就是做出一个假设，就是说我们这个视频生成的所需要花费的时间，假设与我们这个视频本身的时间长度，它的这个space也就是他的这个视频的像素。

以及这个我们模型的参数量大小，假设它是呈线性的关系，也就是说我们之前所观测观察到的这些现象呢，那嗯就是说如果我们做出这个假设，如果OpenAI的SORA它是一个30比脸大小的模型。

如果他想要生成一分钟的高清视频，那它可能需要12个小时，但是根据现在就是网上一些信息说，它生成一分钟的视频，大概需要一小时到两小时之间呢，那我们推测说它SORA这个模型参数量。

很可能是在三比领到七比零之间，但嗯也是在这个假设成立的前提下，好就是，那接下来呢我展示一下我们的demo呃，我们的demo其实是跟这个OpenAI是没有办法去比的，那主要是以下两点原因。

就说我们是在一个低成本的条件下完成的训练，open air呢，它使用到了大概2000到4000个H100的GPU嗯，花费了可能将近5000万美元，到2亿美元的这个训练成本。

而我们其实用了只是不到1万美嗯，大概1万美金这样子的一个，嗯范围的成本进行试验，所以在这样非常有效成本下呢，就是我觉得我们取得的效果是可以接受的，而且通常说这个训练数据它的质量越高。

那生成的视频呢质量也会非常的好，嗯我们之前采用的分阶段的训练呢，其实也是一个降低成本的一个方法，就是说先用一些嗯质量没有那么高，可能甚至就是这个像素也更低的，这个数据进行训练，然后最后再进行高清。

来拉高他的这个视频生成效果，但这一套方法其实在这个自然语言大模，那个模型训练领域呢也是已经实行多年了，就比如说当年这个BT的训练也是分为两个阶段，先用128个token进行生成。

然后后面再把它提升到五百十二个token来，这样增强它这个模型的生成效果，OK就是说嗯，我们目前呢这个最新版本就是可以生成，单次生成大概是20秒的视频，那其实我们针对于我们最初版本，这个只能生成两秒了。

有了显著的提升，然后用到说，如果我们视频基于嗯之前视频生成的，这个延续性生成的可以长达数分钟，所以嗯OK，那我们先接下来直接看一下这个演示吧，这个是我们嗯已经公开的最新版本的，这个不同比例的生成。

然后我们可以生成较为高的分辨率，当时是可以生成720P，但这个是我们之后嗯，下周将要开源的这个模型的一些生成效果，然后我们嗯之前的模型也可以做视频编辑，然后可以基于图像来生成视频。

然后当然也是可以直接进行图像的生成，那嗯其实我们在GITHUB上面呢，就是获得新标数量的增长也非常快，自从我们今年3月开园到目前呢，只是已经获得了大概1。10000新标用户呢，群体遍布全球。

然后这个是我们市社区的一些使用案例，是我们最初的版本的一些效果，虽然只有短短的两秒，其实嗯我觉得当时在这个低成本的前提下，这效果还是不错的，然后这个也是这个社区用户这个一个使用案例。

就说只是经过一些简单简单的这个文本提示，也可以达到一定的生成效果，那我们的未来计划就说首先在模型方面嗯，我们肯定是进一步想要增加模型的大小，来提高生存质量，然后开发这个时域的变分自编码器。

就是我们下周这个版本应该会直接公开，这个已经嗯最新的版本投入使用，然后接下来就是说我们的数据呢，因为我们就是之前几个版本，它在人像上面的生成质量，只是嗯不是非常的好，因为主要也是因为这方面数据的欠缺。

所以目标是收集更多像这人像数据，来完善这个生成人类相关内容的这个能力，然后会进行更精确的美学评分，或者嗯也是进进一步的优化，来加强它这个视频的生成的强度，以及它的质量效果，那非常感谢大家的聆听。

嗯也欢迎大家在我们的GITHUB上面自由探索，就是目前呢我们也在招聘实习生和全职人员，如果大家对这个项目或者路程感兴趣呢，也欢迎加入大嗯加入我们，祝大家有快乐的一天，谢谢好。

非常感谢那个我们再回答几个问题可以吗，非常感谢那个陈慧，那么呃我觉得我觉得就是说呃，哦像开源的这种project的话，跟公司project其实一个最大的一个差别，就是说所有的细节都可以分享，对不对是吧。

然后就是如果是企业界的话，因为呃其他的因素考虑的话，一般有些东西是没有办法去分享的，所以在这的话我大概给两个问题的机会吧，对诶给那给成话筒，最左边那个，喂您好，想向您请教一下，就是在做的过程之中哈。

呃就是我们对于呃不同的图像，因为刚才最后您有提到说，增强人像这块的主要能力，那这个其实就是代表的是，我们对于图像内容的不一样，导致我们对于图像最后所形成出来的生成结果，是有不同差异的。

那么我们是不是有做过，针对于不同类型的图像去做一个评估，就是从什么样的情况下，我们需要什么样的资源，在什么样的架构的情况下，我是可以去启动起这样的一个，围绕着这种图像内容的一个四。

skin in law的一个起点的，呃，在这个没有没有来之前，比如说我55G的人像数据没有来之前，我这个实际上是没有spin了，LOD一点一点意义或者是概念，那这个实际上其实对于我们再去准备一些专业。

垂直领域里头的一些这个这个图像，或者是这个这个视视视频的这个这个呃，这个生成上面可能会有一些指导意义，所以我就想请教一下，说这个事情在我们open sora在去做这个过程之中。

有没有针对于具体的某些内容，然后对它去进行一些这种呃资源计算的，一个是稍微简单一点，嗯对好嗯嗯好，非常感谢，就是您的问题是说我们的在数据处理方面，有没有就是对他的不同的这个数据的嗯内容，来进行分类。

来进行这样一个处理是吗，对于不同的内容进行呃分类，然后基于这个分类来去进行资源，使用水平的评估啊，嗯其实我们如果您说到这个资源使用水平的话，嗯差别最大的，其实他的意思可能是不是说，就你把数据分成几堆。

就是每一堆的话就是可能用的就是用特用，就是就是相当于是数据的SAMPRING的ratio，是不有有一个这样的一个处理，其实数据sampling ratio呢，就是呃我之前诶可以再分享一下。

我之前的slides吗，我们有一个这个数据的这个bucketing，configuration的一个嗯，或者我就简单讲一下，就是说其实还没做是吧，就直接拿着那个，就就直接拿这个数据就直接用了，对不对。

我们对，就是说我们嗯也做一下简单的处理，就是说这个根据他的这个不同的resolution，然后会让他有一定的概率用这个，当然就是说我们用更高的resolution的时候。

我们也会耗费就是更加高的计算资源，所以在这一方面呢，我们是也是进行了一些测速，就是说确定说我们哦每一个不同的resolution，我们会用到不同的batch size来说。

我们训练不同的resolution，它可以达到一个计算资源的平衡，然后嗯在对数据质量的评估呢，就是我们其实现在是一个非常笼统的一套流程，就是进行了这个美学分数啊，就是光流分数的这一部分的统计对。

因为时间问题的话，我我我就我就不啊等，因为等下还有pa环节还可以问，但我觉得有个问题可能会带来比较感兴趣，就是说啊，因为你前面的模型train的时候，是用2D v e train的嘛。

但是你现在这个V1。2的时候，你是用那个就是啊相当于3DVAE做的，那么这时候的话，你其实你这个continue training就是以前的checkpoint的话，他就转过来时候的话。

其实他的那个就是latent space，它就变了嘛，你这个地方在做continue training的时候，有一些什么trick没有，这个嗯这个其实嗯我来讲一下，我们最新版本的这个嗯VAE的训练呢。

就是直接做了一个VAE的adaptation，然后这个方法其实我们是受这个pixel sigma，这篇论文的启发，他当中就是也是做了一些实验，然后发现说这个嗯diffusion模型。

它对这个VAE的adaptation其实是非常快，然后适应性非常好的，当然就是说先把把那个那个模型的参数那边先，比如说有一些地方先fix主先做一，做一段时间adaptation是吧。

就是说其实我们比如说我们已有这个模型对，然后我们新训练这个VAE，那我们可以把这个VAE拿过去以后，做一个这个模型和VAE在一起的混讯，然后让他做达到一些adaptation。

就是适应这个VAE的这个嗯，这个嗯他的这个效果OK对，然后嗯就是说因为目前嗯，其实我们也没有太多的这个计算资源，去做很多的试验，所以我们的这个策略，其实也是基于说对于一些论文。

他提出的方法就觉得可能不错，然后就直接投入到我们的训练当中，OK这可能是一个非常关键的一个细节，好那我们再次感谢春慧。



# 2024北京智源大会-视觉大模型 - P3：高保真 4D 重构模型 Vidu4D：鲍 凡 - 智源社区 - BV13x4y1t7sb

啊喂大家好，今天和大家分享一下，这个咱们生成式AI的一些实践，就是特别是啊我们视频大模型飞度呃，在这种私立上面的一个生成的潜力哦，对然后介绍一下我自己，我是申树科技的鲍凡呃，还是先从这个维度的一个。

整体的技术路线开始讲吧，呃就从他的一个比较底层一个架构开始呃，当时我们是在两年前左右吧，可能两年前呃，4月份开始做这个UVIT的架构呃，他应该是比这个OpenAI solo。

后面DIT还要更早发表一些呃，大概是比他早发表三个月左右吧，然后他其实也是一个扩散模型和transformer，融合的一个架构，呃对他类似，就是把呃也是那种一致的思路，就把这个图像打成patch。

然后patch加造，然后再用transformer去对这些patch做一个去造，对然后我们的架构其实是一个非常简洁的架构，因为你可以看到呃，他是把所有的输入都给统一化成一个token。

就是无论是扩散模型里面的这个条件condition，还是说扩散模型面它这个独有的时间TT，哎我们都把它呃不做区分对待，就统一处统一处理了，这呃陈哲token，然后把他们就一起拼起来。

送到这个transformer里面去，对，这其实对transformer的架构，几乎不需要任何改变，就是如果大家翻DIT论文，可以看到，他对transformer里面还是做了不少的呃。

针对扩散模型的一些特殊的设计的，比方说那种呃在呃把各个参数变得跟时间相关，就是一个叫做adaptive ler normalization的设计，但在我们架构里面，这些通通都没有。

所以基本上你随便拿一个transformer，就可以在很快的时间把它转换成一个diffusion，transformer呃，然后然后并且我们实验表明，就这种非常简洁的设计，它的效果其实也是非常好的。

呃然后值得一提的是，就是我们也加入了一些这种独特的呃设计，比方说这种长连接，就他把这种底层的broke和这种高层broke，做一个这种跨层的连接，然后使得它呃训练上能有更快收敛。

这在我们大量的实验里面都是被验证有效的，呃对然后呃在做v do之前的话，其实我们已经有了一些大规模的尝试了，在去年1月份左右，我们呃呃发表这个un defer这个工作，然后他是一个11参数上的模型。

然后当时是想啊把这个由VIT架构啊，然后去做成这么一个多模态的，一个统一的一个生成式建模呃，他大概能做呃，就他用一个模型，用一个扩散模型，然后可以处理同时处理呃若干个不同任务。

比方说你有这个文本和图像的模态，然后在他们上进行一个训练之后，然后这个模型就能完成，这两个模态之间的任意变化，以及说这样个模态的独立的生成，或者说是联合的生成呃，值得一提的是呃，这个架构呢。

我们只对扩散模型本身的这种FORMERATION，做一些最小的改动，呃，其实就是从一个单模态的输入，变成双模态的输入，然后从单模态的时间T，把它拓展成这种双模态的时间T。

然后并且呃同时预测这样个模态上面的噪声，然后也刚刚好，就是UVIT非常适合处理这种任务，因为它其实已经把这种所有的输入。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/fdf33260e7d18dd97f8360d7bbc6dd16_1.png)

都统一成了序列，对，然后呃当时这个架构的话，基本上是能够对标stable diffusion，然后无论是这个数据量，还是说这个模型的参数量上，然后并且也验证了这种应该说是首次验证吧。

这种纯transformer的一个diffusion，能够有非常好的图像生成的效果，呃这笔就是sorrow，或者说最近开源的那个pixel art，pixel art呃，都会都要更早一些对。

然后我们也可以看一下，就是把这un defer架构去呃，进一步在更高质量的数据上去学习的一些效果，就它也可以支持多分辨率的生成，无论是竖屏还是横屏，然后这种呃美学多元的风格，他都能非常非常好的。

这么一个掌握嗯，所以他其实也是在工业界和在实践中被充分论，证有效的这么一个架构，对，然后并且这种架构其实也可以有，非常好的语义理解能力，就他能够把这个所有prompt里面的每一个细节，都刻画到。

嗯对然后基于这种架构的话，就是他可以在上面去搭一些3D的东西，比方说这种纹身3D图，神3D和这种呃贴图生成，然后他们来说是呃，相对于是一个paper里面的两个环节吧，呃比方说在这种呃扩散模型基础上。

用一些那种呃VSD那种蒸馏技术。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/fdf33260e7d18dd97f8360d7bbc6dd16_3.png)

然后可以从这个模型里面去蒸馏出一些呃，这种3D的表示。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/fdf33260e7d18dd97f8360d7bbc6dd16_5.png)

对然后在后面的话，其实呃除了这种图像的模态，我们也做了挺多一些3D和4D的工作呃，比方说热的话，就是你给任意的一段呃真实的视频，你可以把里面这种物体的一个4D的表示，提出来。

然后你可以在这个4D表示的基础上，对它做任意的编辑，其实就呃等效的呃，我们就对这段输入视频做了一些非常精确，可控的一些编辑，比方说把视频里面的主体呃，换成一些别的小动物之类的。

得把这个把这个猫也可以做这种任意的编辑，然后把它变成这个戴红色帽子的，一个小狗或北极熊之类的，然后并且呢也可以，因为它是一个4D的表示，所以你可以从任意的角度对它做这么一个渲染。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/fdf33260e7d18dd97f8360d7bbc6dd16_7.png)

呃除了这种4D的这种物体编辑，然后我们对这种场景的编辑也是可以支持的，非常好对，就是他是把这个3D的场景表示，用这个高悬spreading这个表示呃，对来进行一个表示，然后它有非常好的这种可编辑性。

然后你可以在这场景里面给它添加任意的物体，然后或者说删除任意的物体，这些都是可以做到，比方这case里面，我们就可以在桌子上给它添加一个花瓶，然后其实它背后都是这个高悬PREADING的。

这种3D的表示，也就是可以看到我们在这个生成式建模上，还是呃有比较充足的耕耘的啊，无论是从这个底层的呃基础理论上啊，比方说我们一些关于基础理论的工作呃，入选过这种杰出论文奖。

或者说被open AI的一些项目，以及说呃stable diffusion这种项目呃被采用呃，然后后面的一些这种关于网络架构的一些工作，然后再到后面这种大规模工程落地实践。

我们都有一些呃比较深厚的基础对，然后这些之前基础也就是支撑，我们可以做呃后面这个维度这个工作，然后他是呃关于这种底层的理论，以及说呃网络架构，以说这种工程实践和数据的一个呃，共同的这么一个长处吧。

对然后对大家可以看一下，就是我们V度在最近也是能够支持到，32秒的视频，然后他也是呃完全从头的这么单次的生成，对他就是基于一个diffusion transformer，然后一次性的把这个32秒视频。

给它生成出来呃，对，然后其实我们可以在这个生成的视频的，基础上呃，再给它加上这种audio模态，就比方说你通过video然后去做一个video to audio，或者说从test to audio。

然后你可以给这个视频，就是做一个比较自然的配音，当然我们现在这管线还是多带来的，就我们后面也会探索，就是那种一次性的视频和呃，音频的这种联合的生成，这种时候可能他就生出来的音效，会比现在还要更加自然。

更加符合这个场景一些，呃包括这种在画室里面的什么，然后这个例子大家应该也都挺熟悉的，就是OPENAI，他也展示过这个开车的例子呃，然后我们可以进一步的在这个粒子基础上，把这个背景音。

然后和这个汽车的轰鸣声音呃，都通过呃，这个video to audio或者说test audio方式给它补充上，还有这也是open i展示的，那个在馆里的一堆电视机的例子，然后我们再最后再提一下这个呃。

比方说我们有这么一个video的高质量的视频模型，之后，我们后面能进一步做一些什么事情呃，对就视频生成模型，它其实有这种大量的真实的想象力，然后其实它可以进一步的去去增强这种三呃。

3D4D重建的一致性啊，有着这么一种作为世界模拟器的潜力，对这个地方我们想解决一个什么问题呢，就是我们给定了一个生成视频之后，我们想把这整段视频里面的这么一个，3D的表示给它提出来啊。

那实际上这种呃带有序信息的3D表示，它其实就是呃可以被称为一个4D，因为就3D的表示，再加上一个时序的维度，它就被叫做4D，然后类似于这种NF的重构吧，我们是希望把给定一段视频之后。

把里面这种带有时序的3D给他，也给它提取出来呃，对然后在这个工作里面的话，就是呃他有这么一个核心的技术吧，叫做动态高斯曲面DJS呃，怎么理解这个事情呢，就首先嗯我们要去对这种4D的表示。

做一个良好的呃建模就是一种比较粗糙方法，就是你每一帧你都可以给他一个3D的表示，那这样子显然是一个比较低效的做法，这意味着你每一帧的3D表示都是独立的呃，那所以的话。

其实对于这种连续视频背后的这种3D呃，表示我们可以通过呃，对于首帧的这么一个3D表示，及说再加后面每一帧的这个，3D的一个变化量啊，去代表这么一个CD表示，那这个变化量的话啊。

在这个场景下面就叫做working呃，可以这么直观理解吧，就是你对这个3D的表示做一些扭曲啊，比方说你呃右下角那只猫，然后它在时时间中他做了一些呃方向的转换，那其实我们是可以把这些转化这种变化量。

用这么一种空间扭曲去做一个表示，然后这种空间的扭曲，在这里就叫就被称作是working这个事情对，然后呃这个呃4D的这个重构技术，它其实跟3D也比较类似，它分为呃它有一个最基本的这种呃重构的rose。

这个rose其实就是可以类比为这种NERF里面，对于这种基于这种像素的一个重构，然后它这个里面其实就是对于视频里面，每个点像素的这么一个重构，就说你先从这个4D的表示，然后通过体渲染去渲染出呃。

这么一个预测的视频吧，然后再把这个预测视频和ground tru的视频啊，去做一个这种呃回归的lose，然后这是一个rose，然后另外一个rose的话，你是呃要去做这种一些关于这个表示的。

正则化rose，就你希望这个表示能够呃4D的表示，能够有一些华的性质啊，有一些这种比方说它在时间上呃，有这种连续，也说在空间上有这种啊被分布在表面上，这种平滑性质。

然后所以他需要一个额外的正则化rose，那这正正则化的loss的话，它是通过那种呃你可以这么理解，它就希望这个loss的效果，就促进它能够达到这种呃，把这种呃3D的这个高选的这个点。

然后被比较均匀光滑的，分布在某一个物体的表面，对，这就是这个这个工作里面所提出的，两个比较重要的rose，然后呃基于这两个rose，我们可以呃对这种3D的视频啊，做这么一个重构。

比方说这个左上角的它是呃，它其实就是一个4D的表示的，就它里它有一个输入是一个3D的视频呃，它有个输入是一个维度生成的视频，然后这个维度生成的视视频呢，我们可以把它提取出这种呃4D的表示。

就是左下角里面这个呃，可以看到这个几何的这么一个变化的几何，它其实就是那次节表示，然后通过这4D表示，我们可以把它渲染成上面的左上角这个猫，然后其实是进一步的呢，我们可以对这4D去做任意的一些编辑。

就这编辑这编辑的，你可以放到这种游戏引擎里面，去做一些手动的操作，比方说把它一只耳朵给它弄大一些，比方这眼睛弄大一些，然后他其实都可以呃，后面可以进一步的去做渲染，然后渲染出一些你所需要的一些新的形态。

对这是一些更多的例子，对，所以就是我们从这个维度4D这工作里面看到，就是这种视频大模型有非常深刻的，这种作为一个世界模型的这么一个潜力，我们呃真的可能把这个世界上的各种物理规律，都给模拟出来。

然后可能后续再结合上一些呃，3D或者4D的技术，然后把这些具体物理规律给它，提取出相关的一些表示，嗯对，然后就是也介绍一些关于公司的一些东西吧，就是我们对我们在这个多模态大模型上，有比较深厚的基础。

就是应该说是对现在所有主流模态都有，都有这种全面的底层的呃，乘上自研的技术，无论是图像还是视频还是3D，然后从这个基本理论到这个网络架构，到后面的这种工程优化到数据上啊。

都有一些这个自己的这个独门经验在里面对，然后也欢迎大家来关注我们对，这是我们的一个呃图像的一个平台吧，然后，以及说后面我们也有这么一个3D的平台，就大家可以在上面去呃，感受一些我们的这个技术能力。

对然后最后欢迎大家关注我们，就我们是神树科技啊，谢谢大家，好的谢谢那个包环呃，我们给呃，我看时间啊，我们可以给那个一个问题这么多这么多问题啊，好那这边才开始吧，对老师您好，我想请教一下，在长期看来。

4D重建和4D生成有多大意义，或者说您刚才提到的那个模拟世界的一个建立，就是这个事情有多么的重要，我想问一下这个点呃，我觉得世界模型还是一个很重要的事情，然后目前看起来它有两条路。

第一条你是通过大院模型，先去把这个抽象知识给它构建好，然后再把它拓展成一个世界模型，然后第二条可能就是，我们把这种物理规律给它构建好，然后再把它拓展成这种世界模型。

然后我觉得这两条道路目前都是在探索状态呃，但是我觉得他们各自都有可能，成为这个最终世界模型的这么一个，呃收敛的方向吧，跟那个您呃video4D，那个4D生成这个部分的关联是在哪里啊，你觉得呃。

我感觉这样子就是呃飞度4D，相当于是把这视频背后的4D表示，都给提取了出来，对就比方说我们给这么一个场景，然后我们可以把场景里面所有人物的背后的，这个呃这种3D的表示带有时序，3D表示都提取出来。

然后并且呢它也是呃有这种时序的信息，然后基于这4D表示，比方说诶我们有这些4D场景之后呃，我们可以比方说把它放入到这种一些，工业的管线里面，就工业管线里面，可能它已经在背后给你用代码。

制定好了一些物理的规则，然后它其实就是一个非常好的那种，强化学习的场景，我们可以在这场景里面，就它其实可以提供大量这种reword1些反馈吧，然后在这反馈里面，我们可以去训练我们的智能体。

对他这种呃呃v two CD，它实际上提供了这么一个模比，非常好的一个模拟的环境，就他可以从大量的视频里面呃，把这种模拟的环境给它提取出来，然后为后续这个呃在这环境里面。

智能体的训练提供有意义的这种反馈信息，好的谢谢我插一个插一个问题，就是说我看你现在的话那个ford那块的话，主要还是重点是在那个objects啊，对于环境的这个这个forty重建这一块的话。

有什么计划吗，呃对我们是有呃计划在里面的，因为对于环境的4D重建它呃，我觉得它在短期的意义更大，比方说这种多机位的拍摄，那你可能以后只需要拍一个机位就可以了，对那别的机位都可以给你重建出来。

对然后我们确实是有探索在里面啊，其实也是会基于呃刚刚那种那个维度4D技术，进一步把它拓展到这种场景级别的，4D重建里面去，K好那我们可以再来再来一个问题，好吧嗯啊好吧好吧，可以再来两个再来呃。

这个再来一个吧对呃我虽然是一个外行，但是我很受鼓舞，看来这个东西因为我是研究骨骼的，嗯目前呢骨骼呢除非拍片子，但是再做调整以后，再让它重新排编的，对人体伤害很大，刚才我看到你这个东西。

就是它既然能生成给你一个图片，或者是给你个什么素材，你能生成4D好，我现在反过来讲，当你拍了这个人体的后背的情况，或者做一个简单的扭曲，你捕捉到这个信号以后，你倒过来，你能给我。

它现在的骨骼调理前后的这样一个模拟东西吗，如果能需要多久啊，这个实际上在这篇论文里面已经做到的事情了，就是他在把4D表示提出来，同时它也可以自动的把这谷歌给绑定上对，所以这位是已经呃已经做到一个事情。

对这个会不会比较难呢，因为你要真正骨骼的话，是在肉体里面，对不对，你拍照片只能拍那个外表，估计可能这个还是有一定的gap，是对对是的，就是他这个骨骼可能是一个呃，不像我们真正的那种实际人体的骨骼啊。

哈哈哈哈哎，谢谢老师，谢谢老师，好好好，那这个这个这啊，谁您您吗啊，可以没问题，这样啊，因为我也是在那个做医疗大模型这个领域的哈，就是您刚才提到的这个嗯，好像是从中医入手去做这个事。

我觉得取决于说呃现在用X光照的那个，比如说胫骨也好，或者是脊柱也好，它的那个数据数量，如果数量在足够多的情况下，呃，我我我理解是通过他现在的这个呃，4D的这个模型，它是可以去做一个相应的一个预测的。

也就是说当下一次我给单个图像之后，它最后它是不是可以有一个360度全息的，这个这个我觉得是可以的，但前提是有足够的数据数量，能够支撑到那个我们这边从技术角度上给它，去去进行这个模型的学习啊。

这个其其实可能是个挑战，所以所以他的意思是说还是有一张是吧，然后的话要重建出这个谷歌的三维的，那个那个结，那个结构出来，对我觉得这应该是可是可是可能的是吧，嗯嗯好的，那我们最后一个问题对。

两位跟刚才两位提的有点相似，我个人感觉因为我的年龄嘛，我对你4D这个定义呢有点问题，因为你的没有一个你现在还是视频，视频看的还是图像，就是两维的，只是加了一个是三维，是时间，你所说的是4D呢。

那个3D呢并没有距离的感觉，你比如说我是搞自动驾驶的激光雷达，是给你4D告诉你距离你这里没有距离的信息，这就跟他刚才说的，我测量骨头，这有个距离的信息，我这地上长个包，它多出来了，不是喜不是样子。

是怎么样，3D包括空间的信息，你这里边呢，我建议你尽好尽量把这个4D呢用一个别的词，否则会跟传统的3D和4D定义呢有冲突啊，谢谢老师的建议，对，可以再来一个问题，哎我请问一下。

其实跟前几个问题都有一些关联度，其实我就想问一下，目前您这边看到的一个最大的应用场景是什么，因为你比如说谷歌，谷歌也好，还是3D也还是自动驾驶，但是您这边看到的最佳的应用场景是什么，然后再补充一下啊。

为什么问这个问题，因为你从技术来讲的话，我们和美国肯定还是有一定区别，但我们的优势是应用落地，所以还是看您这个设计的当初到底干什么用的，所以包华你这个得回答，你毕竟是公司对，如果是如果是那个陈老师的话。

可以不回答这个问题，对K我觉得我也可以回答一下吧，其实呃在短期内，一个最直接的应用就是这种相机多视角拍摄，就你看我们这里有这么多相机，它其实都是在拍摄同一个场景，比方说我们能够做到这种4D的重构呃。

那他已经把这内在表示完全给它建模出来了，那后续我们从任意一个角度去拍摄，其实都是可行的，就我们其实不需要这种多相机的，所以说VR和XR类似，这种应用，对是就大家可能在看电视剧的时候都会看到。

比方说诶有一个场景是两个人在对话，然后后面呢这个镜头呢又转成了，比方说针对某一个人特写，其实他们背后呢都是对应着同一个场景，那如果我们把这个场景的4D表示，能够建模出来。

那其实我们只需要诶有这个视力表示，那对于任意的相机角度，这种拍摄我们都能够自动的生成出来，对，然后智慧是对这种影视行业创作，带来这种极大便利，好诶呃因为嗯你这个接触比较快，我觉得可以再回答一个。

再来一个问题，就说你现在的话做这个私立，这个的话是是说是把视频重建出来了以后的话，我再在重建的时候去考虑，就是说每一帧的那个three d的，那个coin spring这样的这种constrain。

然后有没有可能就是直接就从输入呃，就从文本的这个输入，condition的输入直接就能生成，就是直接是直接生成这个three d的，就是这种就也是一直说把three egon writing。

作later valuable，是不是有可能性嗯，对现在实际上是有一些工作，但是用的表示还不是高升spreading，就是有一些那种NERF或者mesh，或者一些表示他们确实也做一些非常类似。

stable diffusion的工作，就直接把那种原声的三列表，那比如说直接VOXEL以后的话，雷种value对我，我们在我其实我们在昆仑湾尾那边的话，也是这么做的，对质量还非常不错，是的是的对。

行好的，那我们再次感谢鲍怀好吧，谢谢嗯。

# 2024北京智源大会-视觉大模型 - P4：视觉和语言：多模态模型的发展：李俊男 - 智源社区 - BV13x4y1t7sb

好谢谢啊，很高兴，今天有这个机会跟大家分享一下，我们的之前的一些工作呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_1.png)

我这里列了一个大概的呃，大概从2021年，整个多模态大模型的一些重要的工作，历史的脉络，然后呃这个里面有我们呃在sales forts，做的一些工作呃，包括最早的我们做的这个l BF。

是相当于第一个基于transformer的，一个多模态的呃encoder呃，然后之后我们做了这个blip series呃，就是把这encoder呢转成了既可以做理解，也可以做这个图像到语言的生成呃。

然后之后我们blip two呢，相当于在GBE思维出来之前，我们是第一个可以做这种zero shot的一个呃，基于大语言模型的这样一个多模态的大模型呃，然后其中我们还有一个拉维斯这样一个。

开源的library呃，然后再之后呢，我们把blip two又扩展成了instruct blip，包括我们在图像生成领域，我们也基于blip的思路去做了一个工作，叫blip diffusion，呃。

所以我今天会基于我们这个脉络去讲一下，我们中间的一些工作，然后会穿插一些相关的其他呃，呃的工作的一些背景好。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_3.png)

那我们先看一下呃，抱歉，我这个可能大部分是用英文的，我我我会用中文来讲呃，我们先看一下我们所谓这个大的，多模态的foundation model啊，它有一些什么挑战呢。

呃那首先我们要做这种PRETRAINING的foundation model，的目的就是说我们希望这样的视觉语言的模型，它可以用非常低成本，甚至zero shot的办法。

去泛化到一些其他的下游任务上面啊，那我这里主要列了三个挑战，第一个挑战就是这个语言和视觉的对齐，因为说这两个模态是完全不同的，两种信息的信号，所以我们怎么把他们呃，这两种不一样的数据对齐。

在一个模型里面是非常有挑战性的，第二个挑战是从数据层面来讲呃，我们经常会有这种呃很大量的有噪声的呃，是图片和呃语言的数据，那怎么从这种数据里去有效的学习，是第二个挑战，第三个是从计算资源来讲啊。

这种scale up的PRETRAINING是非常耗费计算资源的，那我们也有一些方案去解决这个问题呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_5.png)

先介绍一下背景呃，最开始的这个vision transformer，也就是VIT的出现呃，相当于把呃transformer这个architecture，领入到视觉领域呃。

然后他通过scaling now实现了很好的效果啊，它的基本的框架就是我们把一个图片，分成这一个一个patch。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_7.png)

然后呃通用transformer去处理它呃，然后之后open i又做出了这个clip，这个工作相当于是一个呃，一直到现在还非常被广泛使用的，一个transformer的啊。

vision encoder的框架，他的做法就是用一个叫contrast si learning，这样一个loss呃，它会有这种pair的图片和文本的数据，然后他通过让相似的呃。

图片和文本之间有更大的这种相似度，来预训练它的这个模型。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_9.png)

呃然后这里我们提出了这个align before fuse呃，也叫ABETH呃，这样一个工作呃，他是第一个能够把图片跟文本一起，encode在同一个空间里面的，这样一个transformer啊。

相比于clip，Clip，它是一个单独的VIT，还有一个单独的text transformer，我们是一个更融合的这样一个结构啊，那我们这里提出的一个概念。

就是我们先希望把图片和文本做一个alignment，然后我们再另外有一个encoder，去把它们做一个joint的这样一个fusion呃，我们这里提出了三个PRETRAINING的objective啊。

第一个就是跟clip相似的这样一个，contrastive learning啊，他的目的就是把单独模态的图片跟呃，文本去做一个alignment，然后呢。

接下来我们会把这个feature再过过一个额外的，我们叫multi model encoder里面啊，他会用cross attention去把图片的信息啊，把它融合到文本里面啊。

这里我们提出了额外两个loss去训练这个啊，Multi mode encoder，第一个我们叫image text matching，它是一个二分类的任务，也就是说呃。

当你给定一个图片和文本的这样一个pair之呃，的时候，他会去判断这个pair是一个呃positive，就是他俩是不是对应的还是一个negative。

然后这里我们额外有一个hard negative mining的办法，就是说我们会去选非常难的这种negative呃，就是它看起来很像，但实际上他俩并不是呃，来自同样的一个信息源呃。

然后第三个loss我们是基于这个bird呃，改成了一个多模态的这样一种maslanguage modeling，也就是说我们会掩盖掉一些词，然后让模型去基于图片和文本，共同预测这个词啊。

我们在这个论文里面呃，也理论上证明了这三个objective可以共同去呃，最大化这个文本跟图像之间的一个互信息啊，从而使得这个我们最后得到一个非常好的，多模态的融合的效果。

呃那接下来我们这个blip的工作呢，是把l BF的思路继续延伸下去啊，把这个encoder呃，把它改成了一个可以做呃，文本生成的这样一个decoder啊，所以我们这里叫一个unified的模型。

它既可以做encoding，也可以做decoding啊，然后呃我们还是用同样的三个loss啊，只不过最后一个这个mask这个掩码的呃，Language model，我们把它改成了一个outer。

aggressive的这样这样一个办法啊，这样他就从一个encoder，变成了可以去不断的生成，下一个词的这样一个decoder。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_11.png)

呃，然后在blip里面呢，我们还提出了这样一个对数据处理的办法啊，这个我们叫caption and filtering啊，它具体来讲就是由两个比较简单的模块组成啊，第一个就是我们会去根据图片去呃。

人工的生成很多的这样一这样一些image caption呃，然后另外呢我们还会有一个呃filter的的模型，去从呃你生成的呃caption，包括从网上原始得到的caption里面。

去筛选出来质量比较高的这些啊，然后这两个模型其实都是可以，通过我们这个blip的loss训练得到的，所以我们可以通过这样迭代的方式去，不断的迭代我们的模型和数据呃，实现一个不断的增强的效果。

呃然后我们这里还有一个比较关键的发现，就是我们当我们生成这种啊，人工的caption数据的时候啊，我们需要有一些trick去保证它的啊，多样性是足够的啊，因为就算它质量很高，但如果它多样性不够的话。

我们发现它对于这个模型的训练啊，也是起一个起不到一个很强的正面的效果的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_13.png)

呃然后接下来呢在我们blip这个工作之后呢，呃large large model就迎来了一波比较呃，关键的发展嘛，然后这个里面呃，其中呃GP3就是最关键的一个模型嘛，它是第一个展现出来。

当你用大规模的数据去训练的时候，你的这个模型是有一定的这个，zero shot的泛化能力的，你只要用合适的这种prompting的提示词去prompt，你的模型的话啊，他就可以去解决很多。

你呃没有去fine t过的这种下游的任务。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_15.png)

所以我们接下来的一个思路就是，我们怎么利用这种大语言模型，去增强我们多模态模型的能力，呃，然后我们第一篇工作呢，其实是用了一个比较简单的办法，我们并没有去呃训练这个大语言模型啊，我们做的是，我们想看。

能不能直接把图像的信号转化成语言的信号，然后直接让语言模型去理解，所以我们这里做一个工作叫plug and play v q a，这个字的思路，就是说我们想把一个图片转成一些caption。

然后让这些caption给到一个大圆模型，然后让他直接去做一些任务，这里我们提出一个比较关键的技术，就是呃我们会根据你用户提出的问题，去生成一些跟问题最相关的caption啊。

我们会用到我们blip模型里面，这个matching的这个模块，我们会去找到跟你这个问题最相关性最高的，一些图片的这个区域，然后去采样这些区域去生成一些caption啊，这里面能看到。

当你问题是问到一个特殊的物体的时候，我们这caption也会基于这个特殊的物体去描述，然后当我们把这样一些caption，给到一个大语言模型啊，比如说GP3。

或者我们这里用的一些t five的模型的时候，它就可以根据纯文本的信息去回答呃你的问题，所以这个算是我们首次验证了大语言模型在呃，多模态理解里面的一些呃它的这种力量。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_17.png)

那接下来我们这个blip two的工作呢，就是我们能不能做一个end to end的模型，我们把直接把视觉的模块跟大语言模型的模块，连在一起，呃然后这里blip two解决了四点啊，关键的挑战啊。

第一个就是我们可以让任何这种呃被冻住的，就是我们不需要打开去训练这个大语言模型，但是我们可以让他去理解呃视觉信息，然后同时呢，我们还可以很高效地利用到任何已经被PRETRAIN，好的开源的这些啊。

Vision transformer，比如说clip，比如说呃智源的一些呃VIT，我们都可以利用啊，第三点呢是我们提出了一个算法，去，使得我们可以在不打开这个VIT和语言，模型的情况下呢。

使得它们之间的这个模态的这个差异，能够被弥补啊，然后通过这些呢，我们能够实现的就是一个zero shot的一个图片，到文字的这样一个生成呃，然后他也可以利用到大语言模型的能力去啊。

follow1些这种自然语言的指令呃，然后最后呢我们这个算法是非常高效的啊，我们可能只需要16张呃，A100的卡就可以呃，利用到很多这些billion scale的呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_19.png)

大质量位置model，那这个里面我们提出的一个关键的技术点，就是说呃我们有这样一个q former的模块啊，它是在一个呃vision的encoder，就是一个VIT，和在一个LM之间的这样一个模块啊。

它是参数量不大，只有不到200个million啊，但是它可以实现的就是把视觉的信息呃，很有效地转化成语言模型能够理解的信息，然后呃，并且呢能够在语言模型给定不同prom的情况下。

利用到这些视觉信息去给一些输出呃，我们这里会有两个的训练阶段啊，第一个训练阶段呢，我们只拿这个VIT和就image encoder和q former啊，一起去训练他们两个呃。

然后我们在冻住这个VIT的情况下，我们希望让q former能够提取出来，这个最和语言相关的啊，这这部分的视觉信息，这是第一个阶段，然后第二个阶段呢，我们会让这个q former跟语言模型也连接起来啊。

然后我们让这个我们去entto and train这个模型啊，但是这个过程中，我们语言模型是会不不会打开训练的呃，然后第二个阶段，我们相当于让q farmer通过很高效的办法。

就能把他第一个阶段学到的这些信息啊，让语言模型能够啊理解到。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_21.png)

那这个我们第一阶段的训练的这个objective呢，其实是跟我们之前LBETH或者blip，是呃一脉相承的，呃，就是我们有三个这样比较有效的训练的loss啊，就是contrast you loss啊。

Matching loss，还有通过呃image去生成text，这样一个image captioning的loss呃，然后这里可能我就不会讲它特别具体的，这个框架呃。

但是我们通过一种attention mask的设计方法呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_23.png)

使得我们可以同时优化呃这这三种loss，然后我们也在实验中发现呢，其实每一种loss对于它q former学习这个文呃，图片信息都是比较有帮助的，呃然后经过第一阶段呢，我们实现的一个目标。

就是这个q former已经能很好的把图片的信息，转化成语言空间的这样一种表征啊，那么经过第二第一阶段的训练呢，我们在第二阶段就可以纳入任何的这种呃，大语言模型啊，比如说这个里面当时可能还没有说。

现在这些拉玛这些模型我们用了一个是呃，当时FACEBOOK的一个OPT模型，包括google的一个flt five的模型呃，他们两个一个是decoder only的。

一个是这个encoder decoder的模型呃，但是我们都可以用同样的方法去处理，就是我们会把q former呃的输出啊，它这在这个里面，比如说它是32个QY的话。

那么每一个QY都会有对应的一个一个输出啊，那你可以把这个它的输出，理解成某种视觉的token，那这个token呢和可以和语言的text token，合并在一起啊，然后直接喂给这个语言模型。

然后我们通过一个比较快速的第二阶段训练，就可以让这个Q方面，很快速的适应到不同的语言模型上面，使得它的输出能够被这个语言模型理解，然后我们这个里面training的这个loss。

就是比较简单的给一个图片，我们会生成它的这个描述。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_25.png)

呃那呃这个就是我们当时实现的一些demo。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_27.png)

这个当然可能现在看起来啊，不是不是特别impressive，但是在当时在思维出现之前，我们是第一个能够实现这样能力的，就是给定一个图片，我们在呃不同的prompt下面。

能够啊follow这个指令去输出一些问题，包括我们也可以做这种多轮的呃，针对图片的这种问答，呃然后这个是我们当时科技界其他模型，包括可能比较出名的，就是DeepMind的这个flamingo呃。

它同样也是一个这种多模态的大模型啊，的一些比较嗯，你可以看到非常明显的一个比较，就是我们所需要的训练的参数量是非常少的，只有100多个million呃，但是我们能够很有效的利用到，这个总模型的参数量啊。

虽然我们不需要训练它啊，但是我们通过这种呃我们比较高效的办法，能够实现在相对少的计算资源的情况下呃，在这种不同的downstream的呃，benchmark的性能上面都会超过flamingo。

包括一些当时其他的一些模型呃，然后blip two呢也嗯我们发也也是开源出去了，然后有呃一些其他工作也呃，也在follow我们这个去进去进做进一步的提升。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_29.png)

呃那接下来呢在语言模型这个空间里面，又发生了一件事情，就是这个instruction tuning，可能现在大家会叫他SFT或者post training呃，他这个最开始被提出的概念就是。

当我们有一个预训练好的语言模型的时候，呃，我们可以通过提示词的办法，但是我们也可以通过构建一些不同的，这种任务呃，然后当我们把这些任务集合在一起的时候，它就形成了这样一个呃有输入。

有输出的这样一个instruction tuning的数据呃，然后当我们用这样的数据去进一步，训练模型的时候，我们会发现它对指令遵循的能力，会得到非常大的提升啊。

这里面代表性的工作有就是instruct g p t，还有google的这个flank five啊，他们有提出一些不同的这种instruction tuning，的数数据库。

那我们多模态这边我们就有同样一个思路。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_31.png)

那我们能不能也用这种instruction tunning的办法，去增强我们这个blip to模型，它的呃指令遵循的能力，包括在不同场景上的泛化能力。

所以这里我们就啊做了一个这种vision language，的instruction tuning啊，具体的做法就比较简单，我们就是构建了这样一些图片，问题和答案的数据啊。

我们构建的办法就是从开源的数据里面去整理，去收集去筛选啊。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_33.png)

形成了这样最终一个数据库呃，这个是我们当时呃构构建的一个数数据的办法，然后我们用这样的数据去继续指令微调，我们的模型的话，我们会发现，确实它在很多任务上面都会得到一个，性能的提升啊。

这个里面我们可以在同等呃语言模型的情况下，跟blip two包，包括跟flamingo做一个对比啊，这个当时V呃已经出了这个拉玛一代，然后基于拉玛一代有一个VIKA的模型，所以我们也把这个VIKA呃。

作为decoder语言模型的代表加入了进来，我们能看到在不同的这种呃，zero shot的这种呃泛化场景上，我们的这个instruct BP，是会取得进一步的一个提升的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_35.png)

呃，然后包括呢，我们这个模型不光是可以做这种zero shot的生成，我们也可以在一些呃比较小的下游任务上面。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_37.png)

通过比较快速的微调取得一个搜它的效果，然后这个里面是当时我们做了一个可能，instruct clip和其他的这些包括GPT4呃，和一些其他开源模型的一些效果的对比，呃，我们在一些呃这种呃例子上面。

是能够达到接近于GB4V的一个一个效果，好那接下来我可能会再讲一下，我们在从呃语言到视觉生成的这样一些工作啊。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_39.png)

那这里呃稍微提一下这个背景，就是diffusion model的出现啊，就是极极大地增强了这种这个领域，研究的一个一个热度和模型的能力，这个diffusion model。

他做的事情其实就是一个去噪音的事情啊，他会学到一个呃，从噪声到一个图片的这样一个过程呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_41.png)

然后最开始引领这个领域的一个论文，就是这个latent diffusion model啊，它是通过把图片转成到呃这种影视的空间，然后在影视的空间去做diffusion啊，这样能极大的提升这个图片呃。

diffusion这个过程的一个效率和最终的结果。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_43.png)

然后基于此呢就出现了separty fusion这个工作啊，这个开源的模型它通过大量的数据，把latent diffusion model啊，scale up起来，取得了非常好的效果啊。

那在这个里面我们发现siberty fusion有一个弱点，就是说它只能通过文本的提示词去生成图片呃，但是我们想能不能有办法，让多模态的提示提示啊，比如说我们用文本和图片一起去控制。

我们要生成的这个图片啊，所以我们这里blip diffusion这篇工作呢，就是想把这个图片，图像的控制信号引入到生成里面啊，然后可以去控制这个生成图片的主体，长成什么样子呃，那这个里面我们采用了一个。

也是一个两个阶段的训练方法啊，第一个阶段呢我们沿用了blip two啊，我们希望学到一个这种multi model的encoder，它能够把图片跟文本做一个共同的表征，表征到同样一个空间里面。

所以我们沿用了BELIETO里面，我们学到的这个q former啊，它既可以输入文本，也可以输入图片，然后可以得到一个这种多模态的表征，然后第二个阶段呢，我们就会把这个多模态的表征给到一个啊。

diffusion的模型里面啊，然后把它跟呃本身的这个文本的prompt，结合在一起，呃，然后我们的训练办法呢，就是说我们会把一张图啊，它原本的图，比如说是长长成右边那个火车的样子。

我们会把里面的主体把它拿出来，然后替换掉另外一个背景，那这样的话我们学到的一个就是这个啊，Defusion model，会根据你啊输入图像的主体，去生成一张新的图像，然后我们发现通过这种训练方式呢。

它能够学会很好的这种泛化性啊，有很大的原因呢，也是因为我们在第一阶段有一个非常好的，这种泛化性很强的这种多模态的encoder，去表征这个图片和文本共同的这样一个空间。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_45.png)

那这里我们有一些功能呃，可以被我们的模模型给开发出来，那第一个就是一个呃，zero shot的一个一个个个性化的生成啊，比如说我们给一张这个猫的图啊，我们就可以给不同的文本的提示词啊。

它就可以通过这张图去生成，这个猫在不同情况下的一些其他的呃一些图片，这个完全是没有经过训练的，全都是zero shot的一个效果啊，然后包括车或者玩具也都可以，那我们还可以把它跟一些其他的技术。

比如说control net结合在一起啊，这里我们输入的图，比如说是这个茶壶，我们额外有一个control net的这个输入，是这个沙发，那这个沙发我们提取出来它的这个深度图。

我们可以去控制这个输输出的一个深度呃，和这个结构，但是整个的这个输出的这种呃，style是通过我们这个茶壶来控制的，呃包括我们这个输入也可以，那这个control net输入不不不局限于深度。

我们可以用不同的，比如说边，或者是一些其他办法去控制这个呃，输出的结构啊，但是整个它输出的这个风格和这个主体，是可以通过我们的呃，是另外一张图片去实现控制的，嗯然后这个里面呃可能有一些其他的例子。

就是说做style呃，这种风格迁移啊，我们左边比如说有不同的这种结构化的控制，我们上面有不同的这种风格的控制，我们其实可以把它用各种方式去组合在一起，呃然后我们还可以跟另外一个技术叫做pigs。

to pigs去结合起来，去做一些这种呃，通过主体去控制的一种编辑功能啊，比如说我们现在有两张图，一张是这个左下角的这个机器狗的图，然后我还有一张我是我自己的狗的图，我想把我自己的狗换到这个机器狗上面。

我们就可以用我们的技术去，实现这样一个功能啊，同时右边比如说我们可以把这个蛋糕啊，换换到这个呃汉堡这个上面，就是通过这种办法，我们可以实现一种可控的去编辑图像的功能，呃然后最后呢。

我们还可以实现一些主体之间的这种，interpolation啊，就是呃一些融合啊，这里我们比如说我们有四个不一样的小动物啊，我们通过一个线性差值的办法就可以啊，取得它们之间任意两种动物的融合啊。

这个也可以把这个动物换成人的话，我们也可以实现人脸的这种这种融合功能，好那这个就是今天我要讲的全部内容了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/399882bea44b37a9ddec16616439575c_47.png)

谢谢大家，好的谢谢君兰，那呃我们有足够的时间呃，也不是足够多，大概能两个问题吧，有没有问那个，呃就后后面讲那个那个技术哈，它还和我们那个CVTFO里面那个control net。

它又有什么原因上的不同呢，嗯对非常好的问题啊，呃control net它控制的，你可以看到它是结构化的一控一种控制，比如说用深度图或者用边去控制，我们这个控制是更偏这种风格或者主体，就是你一个图像。

你要生成的东西是什么的这样一个控制，这种控制是可以跟control net进行一个互补的，呃你可以想象，比如说我用CTRLNE去控制它，没法特别精细的把我这个颜色，包括我的这个纹理控制出来。

而用我们这个办法，是不是可以实现这样的功能的呃，也就说是以前我们用那个control t，要做组合才能实现的功能，你可能这样，你一个技术就可以实现，是这样嗯，对是这样的。

就是我们可以通过一个技术是呃覆盖control net，但同时呢control net有一些一些它实现不了的，也可以通过我们这个去实现嗯，挺好的啊，谢谢好，我们再一个问题，后面那个小伙子吧对呃。

我想向您请教一下，就是这个框架下的图像的理解与生成的统一，有没有未来的可能性，呃，谢谢对呃，我觉得这个是一个现在非常热门的一个话题，就是我们能不能做出一个统一的这种，多模态模型，既能做生成，也能做理解。

呃，其实有一些工作也基于我们这个blip里面，这个q former去做过一些类似的探索啊，我觉得这个里面可能比较关键的一个点，就是我们怎么把图片的信息呃，用一种离散化的方方法token ize出来呃。

然后我们这样我们的模型就可以继续理解它，也能够生成呃，我觉得这个还需要很多探索吧，因为现有的一些工作可能表明呃，理解和生成是一个会有一些互斥的一些任务呃，在某种程度上面。

所以它可能需要更多的资源和数据啊，去做样的，去做这样的一些探索啊，对呃我其实我这边还有一个问题啊，就是呃其实我一直呃q formal的话，我一直觉得就是说是就是意思，就是说这个图像的话。

去提取出它的那个token的话，到那个language model里面再插进去的话，其实是应该给跟那个prompt是相关的，恰好你的instruct，instruction那个那个q former的话。

就相当于说大概是做这件事情，就这块你有没有一个visualization，会不会就是说对于同一张图片不同的问题，它回溯到原始图片的那个位置的话，是不是有一些就是自适应的一个过程，是的是的。

其实我们instruct clip里面，就是提取这个图像特征时候，是会把instruction加进去，然后这个是首先是为提升性能的，然后我们有试过做一些这种attention的呃。

Visualization，我们发现在某些场景下确实能看到不同的问题，它会对应激活图片里面不同的区域的，但是因为本身VIT的这个attention map比较复杂，所以不是所有情况下。

我们都能找到这样一个一个一个pattern出来，KK然后再一个问题，诶那个呃这位这位先生第一排的对对诶，话筒我想问我想问一下，就是说这个你你这个one shot。

就是直接就把图像按照patch变成token了是吗，嗯对是的，这个昨天我听那个就是sorry那个组织者，他就是讲到了一个，就是说呃这个token的这个作用和怎么生成token。

他用了一个很关键的一个词叫做压缩，那么我觉得呢呃当时就是借助于语言大模型，直接把图像变成256个词，或者又又又有小块，然后就按照语言来做呢，是一个只是检测的的能力，但是对于图像压缩这个方面呢。

并没有任何这个的贡献，所以我想我问想问句话，你为什么走回，就是说骚扰出来以后，为什么往回走这么一步，我觉得这个是特别有深度的一个问题，我觉得这个里面的压缩，可能跟我们传统的这种图像的像素的压缩。

有一点区别，就取决于我们压缩之后，我们希望得到什么样的一种信号，呃因为我我们在这个里面，我们更多是想得到一种跟语言最相关的信号，换句话说我们的任务是给定一些问题，用语言表述的一些问题。

我们能不能通过图像去回答这个问题，所以这里我们最在乎的我们要压缩出来的信号，就是图像里面比较抽象的这些这些概念，这些物体这这些东西的信号呃，然后SORA呢从另外一个程度上讲，他需要这些。

但是他可能额外还需要一些比较呃，low level的这样一些信号，比如说这个东西的一些非常具体的呃，纹理呀之类的，呃我觉得从这这个这种程度是不一样的，压缩可能跟本身图像这种像素的压缩，也是会有区别。

对不起啊，可能是我刚才呃呃没有讲清楚，我想说这个呢我就拿中国中文的汉字，我可以说呢语言大模型适合西文，就是26个字母，然后呃十个字符对不对，然后你就可以去去去提任何这个语言，就是像英文什么的。

但是中文是象形文字，中文的字呢就是一个图像，他这个图像呢它抽出来一个呢，然后中文是以这个整个这个图像来组词组文的，是不是，那么我个人认为呢，就是我们现在搞这个视觉模型呢，应该针对图像本身。

那么他现在sorry做那个路子呢，就是把它看成一个现代象形文字，这个我不知道，就是说因为前几年北大搞了一个叫CDVA，现在已经被呃国际上作为标准的，叫VCM就是一个新的。

就是说过去的文字用20×20的像素，黑白两个颜色就可以表示了，这信息量也很大，但只有6000多个汉字，但是可有各种不同的体，你可以草书，你可以这个这个行书，你也可以印刷体，各个不不一样的。

但它都能识别出来，图像将来就是下一代的象形文字，所以说他现在把它变成一个symbol呢，就是为了把图像，根据图像变成新一代的机器视觉的文字，所以我感觉就是说呢用patch这套方法呢。

实际上只是利用了语言，就是这个时序的模型，但是它没有真正考虑到图像本身的特征，好吧嗯谢谢谢你的建议，我确实没有太多思考这个方向，我我我会那个再想一想好啊，我你再回答一个问题好吧。

嗯就是说其实就是说对于这个东模态的话，就是说其实我们可能期待的事情是说，图文结合的时候，一方面是对这个图像相关的任务的话，他的能力可能会提升，但另外一个维度，其实我们应该还是期望。

就是说对language mol化这块的话也是有增强的，但是现在其实大部分的这个啊vision language model的话，其实一般的情况下的话，它的那个language的部分。

如果你做翻to或者说做做一个lama上去的话，可能都会有一些这个性能的损失，你觉得就是说如何让这个vision language model的话，在这个重新确认了之后的话。

能够让language对vision啊，不sorry，vision对language的话能有帮助，这种可能性有没有嗯，我觉得这个是特别值得研研究的一个问题，呃。

然后我觉得这个也是现在很多组在比较active，去去做的一个事情，而这里可能有几个点，就是我觉得最关键的还是数据的问题，就是我们这种多模态的数据呃，它到底有多少有效的信息，是能够提升模型的智能的呃。

我觉得这个目前来讲，大部分的这种网上的这种图片和文字，是不太能够做到这一点，所以我们能不能找到去更好的数据，比如说视频数据去data的问题，对对我，我我个人是是这样理解。

OK嗯因为其实人也学习也是一样嘛，其实我们language的这种提升，其实也是通过reading的形式，慢慢把这个能力逐步提升上来，对不对是吧，好的。



# 2024北京智源大会-视觉大模型 - P5：高效能个性化图像生成：程明明 - 智源社区 - BV13x4y1t7sb

嗯非常高兴有这样一个机会给大家汇报一下，我们在呃高效能个性化图像生成，这方面的一些工作，呃其实提到这个图像生成呃，我们其实想去做这个事情，时间已经非常久了。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_1.png)

嗯在早在大概十多年以前，我们曾经就呃试图去做一些关于呃，怎么样去用文字去生成一些图像，呃，之前跟杨老师沟通，好像杨老师那时候也做过一些类似的这个探索，嗯其实在在十多年以前，这个问题还是挺难的。

因为那时候没有大量的这个GPU，也没有海量的数据，呃当时呢我们去做的是用这个呃，后来发现这个实在是没法弄，然后就变成这个呃画一些简单的这些sketch，然后变成用这个搜索，从互联网上搜索图像呃。

最后进一步的去合成，然后当时那个这个工作就受到了很多的关注，当时很短的时间内，在这个VIVO那个平台上，然后就有100多万的这个浏览，然后来我们还拿到了法国政府办的，办的一个论坛的一个奖。

当时还以为是个诈骗电话呢，反正后来真是邀请我们去法，法国巴黎参与领一个奖啊，当然这个事情的话呢，呃可能和现在当然可能对，现在的这样一些文生图呢，只是说呃我们早期试图去做一些探索，当然可能跟现在从结构呀。

从这个方法上有非常大的区别，呃现在的这些方法大家也都比较熟悉，基本上都是基于diffusion的这样一个模型，呃，由于刚才其实几位呃老师在介绍过程中，已经呃讲的比较多了，然后这里我就不赘述，呃。

我今天给大家汇报的其实是呃，关于这个我们在高效能，个性化图像生成方面的三个工作啊，一个是说嗯目前来讲，这个其实好几个报告的讲者都提到了，说呃我们现在的这样一个diffusion。

这一系列的这些模型训练起来，这个整个的这个对资源的消耗嗯，是非常非常之巨大的，呃这个我们有一些呃跟杨老师合作的，做这个呃训练加速的这一块的一些工作，呃，另外一块呢就是呃，我们也希望我们的这个图像的。

这个生成的模型呃，它不光是说哎生成一个比较有呃可爱的，或者说生成一些这个呃可能看着比较高质量的，另外呢，我们也希望他有比较强的这种个性化的能力，呃因为呃我个人感觉从应用的角度来讲。

现阶段呃我们的这个图像的这个生成的模型，可能要去做一些非常严肃的事情，或者说一些非常精确的一些符合物理规律啊，之类的事情，可能还有一定的困难呃，更多的我个人感觉在应用的层面，可能更多的还是泛娱乐化的。

这些应用，可能会呃更贴近于可能近期的一个实现，所以呢在这种情况下，可能用户的这种参与感，或者用户本身在里边能够起的这个个性化的，这些东西可能会比较关键，那呃这是一个我们的一个大致的一个判断。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_3.png)

然后从这一点来讲的话呢，其实呃我们去尝试去理解现在的这样一个，图像生成的模型之呃的时候呢，我们其实会看到，其实啊现在很多的这些视觉模型，有一部分呢是呃，呃早期当我们可能想想学习各种各样的表征。

比如说通过这个V呃VE啊，通过各种各样的一些呃方法，我们想是去学习各种各样的表征去理解图像，然后近近期的话呢，可能更多的人呃在尝试说，我们通过这个文字或者通过多模态的信息，然后去做这个生成啊。

这个理解和生成呢，我这两个呢应该是原则上来讲，这两个应该是能够去相互促进的，嗯基于这个观察的话呢，我们会发现，其实呃现在的这样一个呃，呃diffusion这一系列的模型，比如说这个DIT呃。

diffusion transformer的这样一个模型，我们会发现它去当我们去利用这样的一个模型，去训练呃，它如何去生成一个高质量的图像的过程中，呃，我们会发现，其实它需要的这个迭代是非常非常之多的。

呃比如说这是这个这里边有一个简单的例子，就是我们试图用呃，生图像的生成模型去生成一个去去训练它，去生成各种各样的一些图像物体，比如说这是一个生成这个小狗的一个例子，会发现经过好多次的这个迭代。

DIT的话呢，依然虽然它的细节是很容易搞清楚的，就像最开始我们给大家展示的那个图一样，哎他其实define的模型特别擅长去处理，独立的这样一个主点的这样一个噪声，它去去除这样一个局部的噪声。

是非常非常容易的啊，它可能最开始设计的时候，就是呃是考考虑的那种场景，所以呢可以看到经过非常少的迭代呢，这个小狗的图像里边的各种各样的细节，各种各样的局部其实都挺好的。

但是呢嗯你可以看到对于人来讲的话呢，它其实有很大的问题，比如说它呃少了一只眼睛或者少一个鼻子，那这个的话呢就是它缺少了一些结构化的信息，呃，所以呢我们呃就是说他观察到它缺少这种context。

的这样一个RERELATION的这样一个建模的能力，那因此呢我们希望说呃，既然由于这个嗯缺少这样一个建模的能力，导致他可能这个训练需要的这个时长，会非常的长，那有没有可能我们去强化它这块的建模的能力。

进而呢去呃增强它的这样一个训练的一个效率，迭代的这个收敛的一个速度，那从这个角度来讲的话呢，我们就在想说呃，有没有可能把这个mask的auto encoder，MAE的这样的一些呃呃能力哎。

给它引入进来，去强化它的这样一个呃结构化建模的一个能力，MAE呢大家可能都比较熟悉，他是何凯明提出的一个去呃，做这个语义表征学习的这样一个工作，然后呢这样的一个工作，从他的这个呃。

反正从他这个里边最重要的这个图，就大家就很容易能够看出来，就是呃它是一种这个无监督的，而对于输入的图像，它随机的mask掉一部分，然后他试图去通过这个呃，没有被mask的这些部分去提取一些特征。

然后进而去恢复被mask的这样一些区域呃，从这个示意图大概大家不难看出，就是呃这样的一个模型呢，在学习图像的视觉表征的过程中呃，肯定需要去建模这个图像里边，不同patch之间的相互的关系，呃。

如果没有这种对PH之间，上下文信息的这样一个建模，它是不可能把中间的其他的空洞给补起来的，那所以呢我们认为这样的一种能力呢，它能够很大程度上去嗯，强化我们对这个patch之间呃。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_5.png)

上下文关系的一种建模，那基于此的话呢，呃我们就提出了一种新的一个工作，就是说呃我们希望能够把刚才我们的MAE，mask auto encoder的这样一个机制呃，引入到我们这个diffusion的这个。

训练的过程中来啊，希望呢呃在这个training的过程中，我们希望把这个mask modeling的机制引入进来，哎，这样的话呢它能够去加速define模型，在训练过程中对上下文建模的能力。

进而加速它的这样一个模型的收敛，在inference过程中呢，我们也希望说这个过程呢尽量的平顺，跟以前DEFENCE一样，然后它不额外的增加这个学呃这个推理的速度，那个左边呢是我们的这样一个左。

下角呢是我们这样一个方法的一个，大概的一个流程嗯，对于一个这个LYY的一个latent的表达，然后我们做做一个mask的这样一个机制，然后mask剩下的这样部分。

没有被mask这样一些latent的表达，然后我们可以去把它送到一个一个非对称的，一个diffusion transformer啊，为什么提它是个非对称的呢。

是因为它在training和inference阶段，是有点不太一样的，呃在training的阶段呢，我们不给它完整的这个latent，然后我们希望通过MAE。

也就是说通过mask auto encoder的机制，让他去做一些推理，哎这样的话呢，加强它表征里边可能相互之间的这种联系呃，contest的这种联系啊，在这个推理的阶段呢。

我们就像一个正常的一个full latent一样，整个整个把这个latent的表达整个全部送给他，然后呢，希望他正常的去能够去根据这样的一个latent，做这样一个图像的生成。

嗯对于这个ASSEMMETRIC的这样一个diffusion，transformer来讲的话呢，它整嗯刚才我们也提到了，它主要是在这个training和inference阶段不太一样，呃。

training的阶段呢，就是呃整个它可能呃他是需要去做这样一个呃，对mask调区域的这样一个inference。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_7.png)

然后再在这个呃infer阶段就直接就过了，相当于是它还是像原来的那个DEFENCE那样，一个一个结果呃，当然在这个过程中呢，我们会发现说呃这个呃可能需要的这个呃，在这个我们刚才讲的这个就是重构它的。

这样一个mask调的区域的这个过程，我们把它叫这个set intepreter，然后会需要发现它不需要太多的这个block，然后就可一个一个一个不不太大的这样，一个block啊。

去尝试去建模它的这样一个上下文的关系呃，更多的是引导他的这个视觉表征训练的过程中，关注可能其他的这样一些PH之间的信息，关注patch之间的这种一致性，嗯这样的话呢就构成了我们刚才所说的这个呃。

mask的diffusion，transformer的这样一个一个工作，呃，进一步我们其实也发现说嗯，这个因为我们刚才提到的很多次，这个所谓的这个上下文的这个信息的建模啊，一提到上下文。

大家可能会很容易联想到在视觉里边，我们经常一提到上下文呢，就就就不不免跟另外一个词会强相关，就是跟这个多尺度会强相关，哎我们通常来讲可能很一一说，上下文就会需要有个特别大尺度的信息。

然后去理解这个所谓的这个上下文的信息。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_9.png)

那因此呢我们近期对这个呃模型，MDT的这样一个模型做了进一步的提升，就是把这个是多尺度的建模的能力，通过一些skip collection啊，通过这些单词的这种collection啊。

把这个多尺度的建模的能力，给它再进一步的加入进去呃，加入进去之后呢，我们会发现整个的这样一个MDT的，这个模型呃，相对于之前的这个DIT的这样一个模型，我们去呃去做这个图像生成过程中。

我们会发现它的这个训练的这个速度，不论是迭代的这个次数还是实际的耗时呃，都会得到非常大的这样一个增强，呃，比如说这个呃MMDT的这样一个V1版本，我们相对于这个DIT增加了这个三倍的这样。

一个速度的这样一个提升，然后另一方面呢，就是说呃，这个如果我们进一步的去考虑这样一个多嗯，多尺度的这样一个呃连接哎，能够进一步的将模型的训练的这样一个效率，提升大概五倍啊，整体上来讲。

比如说我们要达到同样的这个quality嗯，基本上我们可以提速提速大约十倍以上，哎，这个还是一个非常大的这样一个，速度的一个提升，这样利用这样的一个呃这样一个方法呢，我们也在这个呃EMNE呃。

在paper with code上，对，做这个EMNET这样一个呃，图像的这个生成的这个任务上，我们也嗯拿到了这个呃比较高的分数，拿到第一的这样一个分数呃，刷新了之前的一个记录呃。

可以看到就说有了这样的一个呃mask diffusion，transformer呢，它能够在很大程度上加快模型的这样一个，收敛的速度，能够让让这个context建模的能力啊。

must auto encoder这样一个context建模能力，和这个diffusion呃，非常强大的，对细节的这样一个呃生成的这样一个能力，进行一个结合，嗯另外呢就是说呃也可以看到。

就是说呃整个这个deep呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_11.png)

这个呃只需要一个非常小的这样一个模型，就可以完成。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_13.png)

另外一个就是我们会发现这个position information啊等等，也做了一些这个abolition study吧，呃详细的这个细节我就不展示展示了，就说这里边有不是呃。

有挺多的关于这个结构的设计的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_15.png)

一些细节的信息信息嗯，在论文里边也有啊，另外一个就是呃我给大家想汇报的，一个是关于这个很个性化的图像的生成呃，关于个性化的图像生成呢，我们嗯近期呃今年年初的时候做了两个工作呃，就是开开呃开源了两个工作。

一个是这个叫做photo maker，这个是我们跟这个呃腾讯合作的。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_17.png)

另外一个是这个story feeling，是我们跟那个头条去合作做的一个工作，嗯这个这样一个工作呢，这个被了困，然后转了之后呢，非非常火，大家好多人去去尝试，就是曾经一度在哈根face上。

第一和第二的都是我们这个方法呃，第一是我们这个方法生成自然图像的，第二个是我们这个是这个方法，生成风格化图像的，然后在这个GITHU呀，还有在这个paper with code上。

都呃有比较呃多的这样一个呃信号啊，或者是大家的这个呃关注呃，这个这个工作的话呢，其实受启发于我们这个CPR，去年的best student paper叫dream booth呃。

他是做这个个性化图像生成的，一个非常重要的一个工作，哎，它需要嗯它可以给定一些少量的一个example，然后呢能够让这个模型在生成的过程中，能够去更多的生成个性化的呃。

跟这个实物图sample相关的这样一个图像呃，当然刚才居然也演示了这个zero shot这个能力啊，当然这个工作可能有了这样一个图像的话，它可能跟这个个性化的和这个个性的，这个信息可能会更多一些，呃。

当然针对这样的一个呃dream boss这样一个方法呢，它需要对这个呃输入的ID图像呢，进行很多的这样一个迭代啊，需要的这个时间呢是非常长的，就是它它的这个方法的话呢，他直接去呃。

从这个sample图像里边去抽取个性化的信息，然后再去呃，某种意义上需要微调整个这样一个呃生成的嗯，啊，所以呢它这个呃整个的这个时间呢。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_19.png)

是非常长的啊，虽然后呃也有很多的些方法尝试去加速呃，但是呢毕竟你需要微调整个的这样一个模型，它的这个时间呢还是很难避免的。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_21.png)

这个事情呢还是在过去的一段时间，受到了非常多的关注，呃，在GITHUB上也开源了很多特别相关的一些工作，哎，可以看到这些这些工作呢都有上千个，star的这个新标也受到了大家的一个广泛的关注，呃。

另外一些的就说呃，这个工业界也是生成了很多相关的一些应用呃，比如说加这个呃，利用大家利用这个dream boost这样一些能力和呃，结合这个LAURA等等，催生了巨量的这样一些应用。

嗯这个国内比较的就是这个妙压相机，它可能需要人去上传，大概呃20来张，这个比较高清的高质量的一些图像，然后有了这样一些图像之后呢，它就会给你生成一个这样一个模型，呃，这样的模型之后呢。

你可以去定制化的通过做一些这个纹身图，比如说生成跟自己相关的哎这个纹身图嗯，当然我们看到一方面呢是有很多这样的工作，但另一方面呢，这样的工作呢依然存在着呃不少的挑战，呃。

这些挑战呢其实最主要的一个呃表现呢，就是它的这个资源消耗呃，这个如果我们希望去微调整个模型的那一类的，这些方法呢，它的这个自然消耗呢通常是呃比较多的，呃，比如说在这个刚才说的那个妙压相机，他就提到说嗯。

你建议用户上传20多张，这个高质量的人脸的照片，然后呢可能需要等半个小时之后再再来看，结果啊，它背后呢需要很多张这个呃高大显存的显卡，然后呢而且等的时间呢通常按小时半小时算，或者至少几10分钟算。

然后我们当时就希望说做一个这个呃速度，用户可以接受吧，就是比如说我们十秒钟的时间，这也是为什么那个在HAGPACE上会那么火，因为大家可能大部分用户，如果说你要去等时半个小时才能出结果的话。

大部分人是等不起的啊，另外呢我们这个方法呢，它对这个显存的消耗呢也不大。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_23.png)

可能一个消费级的显卡就能做，然后另外还有一个就是这个方法呢，对这个呃输入素材的这个质量啊，它要求并不是很高，当然同期呢也有很多其他的一些工作啊，试图去呃避免这个test time。

Opera optimization，测试时对整个模型的这样一个优化，呃，包括这个adobe啊，OPPO啊，google啊，m i it啊等等，但同期有很多的这样一些工作呃。

但是呢这些工作里边开源的相对比较少，另外呢呃它生成的质量相对会比较低，然后它这姿姿态呢比较单一，嗯后面我给大家介绍方法的时候，也会详细的去汇报，为什么会产生这样的一些呃这样的一些问题呃。

大家可以看一些这些example，大概也能理解说这个问题里边，我说的这个呃生成质量比较低，姿态比较单一的这样一个表现，表观上的一些特点，一些一些一些样例吧，呃这类工作的话呢，他之所以有这样的一些问题呢。

是因为嗯它输入图像呃，和target的图像呢往往来自于同一个图像呃，比如说你看这个输入的这个图像，哎，我们从这个输入的图像里边呢，去提取一些微弱的这个表征，然后呢根据这些微弱的表征在训练的过程中呢。

我们希望它去生成一个呃，根据这个微弱表征，去生成一个target的图像的过程中呢，其实这个微弱的这个呃这个表征和这个target图像，它本质上来源于同一个图像，单一的这样一个embedding。

这个单一的embedding呢可能会存在一些问题啊，就是说我们是希望他去记住个性化的信息，就是它它把它去能够去用到这个图像里边，呃，我们所表现出来的这些所谓的个性化信息，具体来说。

像这里边我们就想应利用这个人的特点，但事实上呢，其实整个模型其实它很难去去确定出来，你到底是想要的是这个人呢，还是想要这个人的姿态呢，还是说这个人的这个图像照的很不清晰，你想要这个很不清晰的这种风格呢。

其实他是很难去把这个事情搞清楚的，所以呢他这个方法呢，在很大程度上缺乏这个可嗯变化性，也很难去改变人脸相关的一些姿态呀，一些其他的一些属性，因此呢生成的这个结果呢相对比较单一，然后呢我们的一个方法呢。

就是说我们希望他能够去呃处理，就是输入的这个图像啊，就是我们输入的这个图像embedding啊，我们同的里边不是一个图像，而是一组图像得到一个stack的i d embedding。

我们希望从这一组图像里边，他关注到跟这个人个性化的信息，比如说人脸的属性等等的，这样一些个性化的信息，然后进而呢输出的这个图像呢，呃这可不完全是刚才那个一一对应的那个图像。

这样的话避免了这个对这个姿态呀，或者图像退化的一个记忆，然后更多的去关注这个人脸的一个属性，那我先给大家看一下这个结果啊，就说啊有了这样的一些呃模型之后呢，就是有了这样一个能力之后呢。

我们可以很容易的呃，去根据少量的这个sample的image呃，去定制化的去生成跟这个人相关的，或者说具有这个人的人脸属性的呃，一些这个图像呃，可以向右边呢我们就可以生成说。

i hen带着一个a man，Wearing a christmas hat，然后戴着一个声带帽的一个场景，甚至可以生成一个a boy，或者我们关键词换成一个boy，然后呢。

呃这个生成他小时候的个小孩的这样一个照片，然后我个人觉得这个还是蛮像的啊，我估计亨森自己也没有这样的一个图片，嗯小时候也没没拍过这样的图片，嗯就是呃这是另外一个例子，就是就是说呃这样的一个方法呢。

因为呃我们是从多张图像里边，去关注人脸的属性，呃所以的话呢他没有去记住，可以看到这个例子里边，他没有记住这个图像的退化呃，嗯当然这个这个是梵高的图像啊，梵高我们也不可能要求他去拍一个，高清的照片了。

然后高质量像像这个其他的一些app里边，要求他拍高质量的照片，那可以看到就说这里边没有记住更多的这个呃，这个图像退化的信息，像油画里边，你可以认为是一个非常低质量的这个呃，这个自然的图像嗯，然后呢。

我们可以说哎这个呃在电脑前面写代码呀，或者骑着摩托车呀，可以生成很多的这个具有个性化的这样一个，图像的信息啊，甚至不一定得是图像啊，甚至是一个三维的模型啊，一个石膏的像，然后石雕像嗯。

拍一个照片也能够去体现这个人的个性化的，一些信息哎生成，比如说这个柏拉图呃，穿着宇航服呀，或者戴着耳机的这样一些图像，而且呢当我们去呃，给定就是我们有这个呃multiple的这样一个图像。

去表征呃这个人的这个ID的时候呢，呃很多时候呢也嗯训练的时候，我们当然是用同一个人的一组图像去做的，希望他能够关注人的这个ID的属性，但是在测试的时候呢，可以不可以。

不是仅仅只有一个图像的这样一个一个ID的，对应的这个图像在测试的时候呢可能是多个，比如说像把这个演员和某一个卡通的人物，做一个合成呃，就能够呃比如说呃合成出来这个人去演。

比如说白雪公主的时候的这样一个照片，那就从方法方法层面来讲的话呢，就是说呃我们的这样一个方法呢，是呃这个多张图像迭代，送入到一个stack ADD embedding呃，就给到给定多张图像，然后呢。

从这个通过image encoder去提取这个image embedding，然后呢进一步呢把这样的一个in beiimage，embedding呢，跟这个文本里边的这个这样一个关于图像的。

这个这样一个描述，结合而得到一个stack id embedding，这样的一个stack id embedding呃，替呃就说呃替换掉里边，原来我们就就往那个文本里面插入的。

这个普通的这样一个视觉的表征，哎，就能够去让它去影响这个DEFASION的模型啊，我们这个呃这样的话呢，我们就不需要说每一次给到一组图像，我们都要微调整个defer模型，而是说哎，我们是我们希望说。

我们的这个关于图像里边的信息，都成为这个stack edd embedding呃，输出的里边的包含这些信息，不用每个人再去根据这个图像，再去微调整个模型，因而呢它的这个速度可以呃，得到很大的这个提升。

嗯可以嗯包括这个测试的时候呢，就可以用多张的图像去做成不同人人的这个，ID的混合呃，很多时候呢呃另外就说呃，我们还需要有一个这个自动的生成，数据的这样一个流程，为什么呢。

因为呃我们希望得到很多关于同一个ID的，不同的图像的这样一个数据，呃，当然现有的这些数据集并不足以去支撑，我们去做这样一件事情啊，因此呢我们搞了一个数据数据组装的一个流程，以人为主体嗯。

具体来说呢我们就从互联网上下载，很用人名去下载很多的图像，做一些过滤。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_25.png)

然后做一些人脸的ID的验证，然后做一些这个分割裁剪，并添加这个文本的描述，呃这样的一些工作呢都还比较常规吧，就是利用现有的这些工具啊，我们去组合出来一个数据集，让这个数据集是以人物ID为中心的。

呃里边大概有1万多，这呃一呃1万多的这个呃，1。3万张的这样一个人物的ID，大概11万的这个图像也不是特别大吧，啊用这样一个东西去呃，训练，我们刚才所说这个方法去得到这个人物属性的。

ID的这个呃stack的i d embedding，哎这样的话呢我们能够在保证快速的同时呢。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_27.png)

生成的结果具有高的生成质量，ID的保真度，还有这个人脸的这样一个多样性，呃这是一些评测的一些结果和user study，我就不详细的去展示了，哎看看到就说有了这样一个ID，这呃有了这样一个呃呃放之后呢。

相当于说他能够去记住这个人的这个，关键的一些属性啊，我觉得感觉跟这个呃，跟这个呃，我们人对这个人物的这个关键属性的理解，还是比较一致的，然后甚至可以去换一些这个提示词，然后呢。

可以甚至可以去只有一两张的这样一个图像，不用太高的质量，也能生成非常高质量的这个图像嗯，可以看到就说比如说像这里边，像梵高的这个头像啊，这一基本上只有少量的这个人物，ID的这样一个信息。

然后我们就能够去生成非常高质量的呃，牛顿的一些图像呀，甚至更多的呃，由于是呃我看好像时间嗯不剩的不算太多了啊，我就讲的快一点啊，这里边甚至可以通过这个prompt，waiting的机制啊。

我们可以把多个图像，比如说20%的奥巴马，百分之这个80的拜登等等去，可可以做一些混合多ID的混合啊，有了这样一个混合之后呢，也可以玩出更多的花样，呃，这个整个在ID的表征度呀等等。

我们会有一些嗯嗯有一些优势啊，除了速度和资源的这样一个消耗以外呢，这样的方法呢，还对这个输入图像的质量和清晰度，没有太严格的限制，没有把这些图像的退化给学进去明，嗯我们在这个实验的过程中。

没有发现特别明显的这个OVERFITTING。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_29.png)

然后也能去做一些很强的一些，风格化的这个生成呃，这是一些风格化生成的节点，我给大家就简单的展示一些事例就可以了，那最后一个工作呢是这个story diffusion。

嗯这个story diffusion呢，我们也试图去做一些这个呃，视频的生成的一些结果，在这个嗯，就是我们也能希望能够生成一个呃，具有一定一致性的这样一个动画，或者长的长一些的这个视频。

哎嗯啊写那个这个这个工作，这次又被乐坤给看到了，然后又帮我们转发了一次，然后特别感谢啊，免费的帮我们做一个广告，然后这个也是在非常短的时间内，这个545月4号的时候他转化了一下。

很快可能一个一周多就大数超过了3000，然后呢，那我们是期望说用较少的资源做比较长的视频，呃。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_31.png)

刚才几位也做了详细的分析，我就这里就不展开了啊，我们大概的这样一个方案呢，对对已有的方法我就不展开了，然后我们大概的方案呢，就是一个两阶段的视频的生成方案，期望呢能够用较少的资源去生成。

比较长的一个视频，嗯这里边大概包含两个部分，我们第一步呢是希望能够生成一些关键帧，然后呢把这些关键帧呢串起来，给它得成一个视频，呃，当然呃要从关键帧能够串成视频呢，你这个关键帧肯定得具有一定的一致性。

否则的话呢你可能A图还是张三隔了五针，隔了20帧就变成李四了，那基本上没法串起来嗯，所以呢我们需要有一个比较强的这样一个，一致性的图像生成的一个策略呃，另外呢就是说关键帧去串成一个视频呢。

它不光是说时间维度上的一个超分呃，以前我们一般讲这个关键帧的，这个超时间维度的超分呢，比如从20帧超到30帧，这个图像生成的，那可能相当于说从这个呃个位数的，这个从从一帧两呃。

一一帧两帧超到这个什么60帧，或这种或者甚至更长更夸张，所以呢我们需要建模。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_33.png)

更强大的这个插针的一个能力，那简单来讲就是说比如说给定一段故事，一段文字来描述一个一个视频，然后我们可以把这段文字呢生成，分割成若干段话，若干句话，每一句话呢它会生成一个关键帧。

当然在这个关键帧的生成过程中呢，我们提出了一种叫做特征，保持自注意力的机制呃，相当于说根据其他针呃，它同期生成的其他针得到一种特征，保持的自助意机制呃，使得我们整个生成的不同帧之间。

具有比较好的这样一个相似性和相关性，呃，另外一方面呢，就是说即使有比较还不错的这个相关性，比如说像左边哎，我们用这个坑嗯，这个一致的这样一个attention，生成的一些两个图像。

这两个图像呢如果我们要通过插值的方式，把它变成一个视频的话呢，呃如果用传统相对来说，基于类似于光流的这样一方法，它结果会是非常差的，是因为呃其实左边这两个图像呢，虽然应该算是同一个人。

还比较一致的一个图像呃，但是呢呃它没法做到像素级的光流的，这样级别的这样一个对应，因为它可能很多地方遮遮挡了，很多地方看看不见了，所以你试图去用这些方法去插针的时候呢，他肯定那个视频就没法看了。

哎我们就提出了一种新的方法叫motion prediction，A predictor，那这个mamotion predictor呢就是它的，其实呃希望他能够去建模这个呃，大幅度的一些动作的变化。

嗯比如说在现有的一些视频生成模型中呢，其实呃是超值呃，超分的过程中呢，很多时候我们通过这个TEMPORATTENTION啊等等，去在每一个空间位置呢独立的去计算，然后呢，呃它其实很难建模一些全局的信息。

和大的一些动作，哎，我们这个方法呢，相当于说直接在语义空间里边做插针，然后语义空间中插值，插针完之后呢，我们把这个表征呢再投射到这个图像空间，哎，这样就避免了对限速级的这样一个这个呃。

对应关系的一个建模呃，能够去建模比较嗯，比较大大的这样一个幅度的一个动作。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_35.png)

然后整个的这样一个方法呢，呃这个呃就是可以利用原来的一些很多的这个。

![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_37.png)

数据去做一个训练，呃，这是一些结果的一些展示，就说可以看到，就是它生成的这个人的这些动作呀，人脸呀没有一个非常大的这个扭曲，然后能够做比较呃，不错的这样一个视频的这个生成哎。

这是更多的一些转场视频的这样一个，生成的能力，哎，它可以把不同的这个关键帧给它插起来，变成一段这个视频啊，当然可以看到也有一些不尽如人意的地方，比如说右上角那个图像，就是虽然可能两个关键帧之间。

看着还比较这个一致，然后但是呢在转场过程中，他那个发型也好像也发生了一些变化，呃但是呢如果说是我们要求不高的话，如果我们拿它来做一个这个呃，这个比如说连环画生成的话，那这个一致性就非常好了，就是我们呃。

从如果从娱乐化的这个体验来讲的话呢，如果用用它去生成一个连环画，唉这个还是结果非常惊艳的，然后呢比如说做一些卡通的风格呀，甚至引入一个reference image，做一些定制化的这个连环画的生成呃。

甚至可以用它来讲个故事，比如说以前去创作漫画，这个漫画是需要一帧一帧的一个图，一个图的去画，其实挺麻烦的，那我们现在可以去嗯自动生成一段文字，然后再自动给这段文字配一段这个啊图像。

然后呢就生成一个连环画啊，相信这两个连环画呢，大家可能大家大概瞄一眼也知道是什么故事啊，这个这个会非常的这个图文并茂啊，最后呢做一个简单总结，就是呃我们这个我感觉啊。

图像的生成可能最早是通过文娱的行业呃，走进普通的用户，然后大家觉得这个事情有趣好玩，然后呢嗯这个要解决两个最主要的核心的问题，一个是这个高效能的问题，因为你要面向很多的用户，你这个效能太低了。

这个卡受不了嗯，钱包受不了啊，另外一个就是个性化，就是让大家觉得好玩，你得跟这个人有关系，你不能说是只是看别人玩，自己参与不进去，这个可能会是一个比较麻烦的事情，另外就是说我们在视频和连环画的这个。

高效的生成的时候呢，也要关注这个一致性，然后呢另外就说呃大的这个动作的之间的插针，然后感觉这个AIGC呢，有望在很大的很多的领域，比如说这个漫画的创作领域，能够很大程度上的解放生产力。

但另一方面呢也给社会治理啊带来了很多挑战，以前我们经常说这个有图有真相啊，未来现在可能有视频都没真相了，也还是挺挺吓人的好。



![](https://gitee.com/OpenDocCN/dsai-notes-pt2-zh/raw/master/docs/baai24/img/a05d3b53f9b3021898c92601255f173e_39.png)

最后感谢大家，我们的这个工作呢都已经开源了，谢谢，好的谢谢呃，那个你们可以开始准备了，好那呃，我们给时间一到两个问题吧，哦对那个我我这还有那个可能屏幕给切了，再把那个代码对对对。

那个第一个工作非常有用啊，就是嗯我知道有很多公司的话，已经在用这个东西去去做这个视频的，这个生成了，对是是是那个极大的能减少对卡的需求，对对而且效果确实很好，我们大概两个问题啊，好吧，这位戴墨镜的吧。

诶呃谢谢教授，您的分享很精彩，呃我有个很好奇的问题哦，就是你提到了你们的motion predictor，刚才那个呃，包总他们也提到了他们做的那个4D的那个，但我看到你们的训练的数据。

好像主要都还是2D和2D结合文字为主，为什么没有大量的使用3D的数据来训练，这个这个对于motion的prediction不是应该更自然吗，哎我们有很多视频的数据在训练，是3D的吗，还是只是视频。

就你们会经过像mono depth，或者说俄罗都是视频的，嗯对我就很好奇，为什么不会直接引入3D的数据呢，嗯可能嗯这个事可能也跟这个就是呃，是不是开源的也不多，对对对对对。

就是相相当于说其实我们在高校吧，可能一般来说也不太倾向于去做一个特，别大的系统，把所有的部件都弄得说，这个事情最好我们就一定要用这个事情，我们可能更多的是希望呃找到一两个突破点。

从这两个突破点去展示某种呃技术，可能对这个事情会有帮助，呃，不一定会试图去尝试说这个流程里边，比如说ABC3个步骤，每个步骤呢都有更好的解决方案，然后呢我们都去做这个，从资源消耗上来讲。

我们从高校的角度来讲，其实对我们来讲是不友好的，然后工作量也会相对再大一些，所以我们更多的是找一个这个，大家觉得还不错的一个baseline，在这个baseline的基础上呃，我们去把我们的想法加进去。

然后看我们这想法，最后对整个base ne的这样一个改变呃，另外从高校的角度来讲的话呢，从这个比如说最终你去论述，你这个成果的过程中，如果你引入了太多的，你把这个步骤也变了，那个步骤也变了。

最后大家说不清楚，你到底整个这个最终的性能的提升，是因为哪个原因提升的，所以这个也是我们可能，高校去做一些工作的时候，可能跟企业界，企业界是说我这产品质性能越好越好，那我可能不用去论述清楚。

我这个东西到底是因为哪个不用搞得那么清楚，到底是因为哪个原因搞得那么好，对呃那最后一点，那那那或者换个方向问这个问题，可以可以请您评论一下，就是如果说我们有充分的3D的数据。

会对会对我们这样的模型有什么样的改善吗，哎我个人觉得如果有更多的这样一些数据，然后包括你也有足够的这个训练的资源，应该是有很多的这样一个呃改善的，呃，比如说像这个呃我们右上角那个图像。

那个视频的生成的过程中，大家可以看到，就是说呃，可能是因为我们没有太多的这个，3D的一些信息在里边，可以导致你看那个人在变化的过程中呃，我看一下啊，这个再播一下，就是这个人在变化的过程中呢。

你会明显的感觉到他的这个3D不太对对。